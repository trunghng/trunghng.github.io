[{"content":"Notes on using Gumbel-Softmax \u0026amp; Concrete Distribution in Categorical sampling.\nGumbel distribution Gumbel distribution, denoted $\\text{Gumbel}(\\mu,\\beta)$, is a continuous probability distribution whose cumulative density function (CDF) is given by \\begin{equation} F(x)=\\exp\\left(-\\exp\\left(-\\frac{x-\\mu}{\\beta}\\right)\\right), \\end{equation} which implies that the probability density function (PDF) is given as \\begin{equation} f(x)=F\u0026rsquo;(x)=\\frac{1}{\\beta}e^{-(e^{-z}+z)}, \\end{equation} where \\begin{equation} z=\\frac{x-\\mu}{\\beta} \\end{equation} The standard Gumbel distribution, denoted $\\text{Gumbel}(0,1)$, is specified at location $\\mu=0$ and unit scale $\\beta=1$, whose density functions, i.e. CDF and PDF, are then explicitly given as \\begin{align} F(x)\u0026amp;=e^{-e^{-x}} \\\\ f(x)\u0026amp;=e^{-(e^{-x}+x)}\\label{eq:gd.1} \\end{align} Below are some illustrations of Gumbel distribution.\nFigure 1: Gumbel distribution $\\text{Gumbel}(\\mu,\\beta)$. The code can be found here Since the quantile function, i.e. inverse of the CDF, of Gumbel r.v $\\text{Gumbel}(\\mu,\\beta)$ is referred as \\begin{equation} Q(p)=\\mu-\\beta\\log(-\\log p), \\end{equation} which implies that the standard Gumbel random variable $X\\sim\\text{Gumbel}(0,1)$ can be sampled using inverse transform sampling by first drawing $U\\sim\\text{Unif}(0, 1)$ and then computing \\begin{equation} X=−\\log(−\\log U) \\end{equation}\nOptimizing Stochastic Computation Graph Consider the following Stochastic Computation Graph\nwhere\n$w,\\phi,\\theta$ denote input nodes. $X$ is a stochastic node, which is given by sampling according to $p_\\phi(x\\vert w)$. $f$ is a deterministic node, i.e. $f_\\theta(x)$ is a deterministic function at $X$. The graph corresponds to the objective function \\begin{equation} L(\\theta,\\phi)=\\mathbb{E}_{X\\sim p_\\phi(x)}\\big[f_\\theta(X)\\big],\\label{eq:oscg.1} \\end{equation} where without loss of generality, we have considered $w$ as a constant.\nConsider the backpropagation through the computation graph, we have that the gradient w.r.t $\\theta$ of the cost function is given by \\begin{equation} \\nabla_\\theta L(\\theta,\\phi)=\\nabla_\\theta\\mathbb{E}_{X\\sim p_\\phi(x)}\\big[f_\\theta(X)\\big]=\\mathbb{E}_{X\\sim p_\\phi(x)}\\big[\\nabla_\\theta f_\\theta(X)\\big],\\label{eq:oscg.2} \\end{equation} which, as an expectation, can be estimated using Monte Carlo method. In particular, let $X_1,\\ldots,X_s$ be $s$ i.i.d samples drawn from $p_\\phi(x)$, the gradient given in \\eqref{eq:oscg.2} can be estimated with the unbiased \\begin{equation} \\nabla_\\theta L(\\theta,\\phi)\\approx\\frac{1}{s}\\sum_{i=1}^{s}\\nabla_\\theta f_\\theta(X_i) \\end{equation} On the other hands, taking the gradient w.r.t parameters $\\phi$ gives us \\begin{equation} \\nabla_\\phi L(\\theta,\\phi)=\\nabla_\\phi\\int p_\\phi(x)f_\\theta(x)dx=\\int f_\\theta(x)\\nabla_\\phi p_\\phi(x)dx,\\label{eq:oscg.3} \\end{equation} which can not be estimated directly using Monte Carlo sampling.\nScore Function Estimators Score function estimator utilizes the log-likelihood trick to rewrite the gradient in \\eqref{eq:oscg.3} in an expectation form \\begin{align} \\nabla_\\phi L(\\theta,\\phi)\u0026amp;=\\int f_\\theta(x)\\nabla_\\phi p_\\phi(x)dx \\\\ \u0026amp;=\\int f_\\theta(x)p_\\phi(x)\\nabla_\\phi\\log p_\\phi(x)dx \\\\ \u0026amp;=\\mathbb{E}_{X\\sim p_\\phi(x)}\\big[f_\\theta(X)\\nabla_\\phi\\log p_\\phi(X)\\big], \\end{align} which analogously can be estimated by $s$ samples $X_1,\\ldots,X_s\\overset{\\text{i.i.d}}{\\sim} p_\\phi(x)$ \\begin{equation} \\nabla_\\phi L(\\theta,\\phi)\\approx\\frac{1}{s}\\sum_{i=1}^{s}f_\\theta(X_i)\\nabla_\\phi\\log p_\\phi(X_i) \\end{equation}\nReparameterization Trick In some circumstances, it could be helpful that instead of sampling from $p_\\phi(x)$, we first sample $Z$ according to some fixed distribution $q(z)$ and then transform the sample to $x$ using some function $x=g_\\phi(z)$.\nFor instance, by properties of the Normal distribution, a Gaussian sample $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ can always be obtained through a standard Normal $Z\\sim\\mathcal{N}(0,1)$ by computing $X=g_{\\mu,\\sigma}(Z)=\\mu+\\sigma Z$.\nThis reparameterization trick, $x=g_\\phi(z)$, let us transfer the dependence on $\\phi$ from $p$ into $f$ by writing \\begin{equation} f_\\theta(x)=f_\\theta(g_\\phi(z)), \\end{equation} which enables the possibility of reducing the problem of estimating the gradient of w.r.t parameters of a distribution into a more trivial task of estimating the gradient w.r.t parameters of a deterministic function.\nApplying this reparameterization trick allows us to rewrite the objective function given in \\eqref{eq:oscg.1} as \\begin{equation} L(\\theta,\\phi)=\\mathbb{E}_{X\\sim p_\\phi(x)}\\big[f_\\theta(X)\\big]=\\mathbb{E}_{Z\\sim q(z)}\\big[f_\\theta(g_\\phi(Z))\\big], \\end{equation} which has the gradient w.r.t $\\phi$ given by \\begin{align} \\nabla_\\phi L(\\theta,\\phi)\u0026amp;=\\nabla_\\phi\\mathbb{E}_{Z\\sim q(z)}\\big[f_\\theta(g_\\phi(Z))\\big] \\\\ \u0026amp;=\\mathbb{E}_{Z\\sim q(z)}\\big[\\nabla_\\phi f_\\theta(g_\\phi(Z))\\big] \\\\ \u0026amp;=\\mathbb{E}_{Z\\sim q(z)}\\big[f_\\theta\u0026rsquo;(g_\\phi(Z))\\nabla_\\phi g_\\phi(Z)\\big] \\end{align}\nGumbel-Max Trick Using the idea of reparameterization trick, Gumbel-Max trick refers to an approach that allows us to sample from a categorical distribution1 through sampling according to Gumbel distribution.\nFirst let $D$ be a categorical variable with class probabilities $\\alpha_1,\\alpha_2,\\ldots,\\alpha_k$ for $\\sum_{i=1}^{k}\\alpha_i=1$ and without loss of generality we can assume that zero category probability excluded, i.e. $\\alpha_i\u0026gt;0$. Thus, we can express each sample drawn from the distribution as a $k$-dimensional one-hot vector lying in the corner (or vertex) of a $(k-1)$-dimensional probability simplex $\\Delta^{k-1}$2. In particular, each categorical sample is in form of \\begin{equation} D=\\left[\\begin{matrix}D_1 \\\\ \\vdots \\\\ D_k\\end{matrix}\\right], \\end{equation} where $\\sum_{i=1}^{k}D_i=1$; for $i=1,\\ldots,k$ we have $D_i\\in\\{0,1\\}$ and $P(D_i=1)=\\alpha_i$.\nUsually, we rewrite each class probability as a softmax function \\begin{equation} \\alpha_i=\\frac{\\exp(\\pi_i)}{\\sum_{j=1}^{k}\\exp(\\pi_j)} \\end{equation} where $\\pi_i\\in(-\\infty,0)$.\nGumbel-max trick provides us another way to get samples following this discrete distribution through samples drawn from Gumbel distribution. The trick is described as follow.\nConsider $k$ unit-scaled Gumbel random variables $G_1,\\ldots,G_k$ where $G_i\\sim\\text{Gumbel}(\\pi_i,1)$. Thus, the density functions corresponds to $\\text{Gumbel}(\\pi_i,1)$ are given by \\begin{equation} P(G_i\\leq x)=F_{G_i}(x)=\\exp(-\\exp(-x+\\pi_i)), \\end{equation} and also its PDF \\begin{equation} f_{G_i}(x)=\\exp(-\\exp(-x+\\pi_i)-x+\\pi_i) \\end{equation} We have that the probability that $G_m$ taking the maximal value across $k$ samples can be computed as \\begin{align} P\\left(G_m=\\max_{i=1,\\ldots,k}G_i\\Bigg\\vert G_m=x\\right)\u0026amp;=P\\big(G_1\\leq G_m,\\ldots, G_k\\leq G_m\\big\\vert G_m=x\\big) \\\\ \u0026amp;=\\prod_{i=1,i\\neq m}^{k}P(G_i\\leq G_m\\vert G_m=x) \\\\ \u0026amp;=\\prod_{i=1,i\\neq m}^{k}F_{G_i}(x) \\\\ \u0026amp;=\\prod_{i=1,i\\neq m}^{k}\\exp(-\\exp(-x+\\pi_i)) \\end{align} Therefore, integrating over sample space of $G_m$, the probability that an arbitrary index $m$ corresponds to the largest sample $G_m$, i.e. $m=\\text{argmax}_i G_i$ is computed by \\begin{align} \u0026amp;P\\left(m=\\underset{i=1,\\ldots,k}{\\text{argmax}}G_i\\right)\\nonumber \\\\ \u0026amp;=\\int f_m(x)\\left(G_m=\\max_{i=1,\\ldots,k}G_i\\Bigg\\vert G_m=x\\right)dx \\\\ \u0026amp;=\\int\\exp(-\\exp(-x+\\pi_m)-x+\\pi_m)\\prod_{i=1,i\\neq m}^{k}\\exp(-\\exp(-x+\\pi_i))dx \\\\ \u0026amp;=\\int\\exp(-x+\\pi_m)\\prod_{i=1}^{k}\\exp(-\\exp(-x+\\pi_i))dx \\\\ \u0026amp;=\\exp(\\pi_m)\\int\\exp(-x)+\\exp\\Bigg(-\\exp(-x)\\sum_{i=1}^{k}\\exp(\\pi_i)\\Bigg)dx \\\\ \u0026amp;=\\frac{\\exp(\\pi_m)}{\\sum_{i=1}^{k}\\exp(\\pi_i)}\\int\\exp(-\\exp(-x)-x)dx\\label{eq:gmt.1} \\\\ \u0026amp;=\\frac{\\exp(\\pi_m)}{\\sum_{i=1}^{k}\\exp(\\pi_i)}, \\end{align} where the last step is due to that the integrand in \\eqref{eq:gmt.1} is the PDF of a standard Gumbel distribution, as defined in \\eqref{eq:gd.1}, which therefore integrates to $1$. Hence, we have that \\begin{equation} P\\left(m=\\underset{i=1,\\ldots,k}{\\text{argmax}}G_i\\right)=\\pi_m \\end{equation} Since a $\\text{Gumbel}(\\mu,\\beta)$ can always be obtained from a standard $\\text{Gumbel}(0,1)$ by scaling it with $\\beta$ then translationing with $\\mu$, then $G_i\\sim\\text{Gumbel}(\\pi_i,1)$ can be computed as \\begin{equation} G_i=g+\\pi_i, \\end{equation} where $g\\sim\\text{Gumbel}(0,1)$, which as mentioned above, can be obtained with \\begin{equation} g=-\\log(-\\log u), \\end{equation} where $u$ is drawn from an Uniform distribution, $u\\sim\\text{Unif}(0,1)$.\nTo summarize this, the Gumbel-max trick proceeds as: let \\begin{equation} U_1,\\ldots,U_k\\overset{\\text{i.i.d}}{\\sim}\\text{Unif}(0,1),\\label{eq:gmt.2} \\end{equation} and let \\begin{equation} m=\\underset{i=1,\\ldots,k}{\\text{argmax}}\\log\\alpha_i-\\log(-\\log U_i),\\label{eq:gmt.3} \\end{equation} where $\\alpha=(\\alpha_1,\\alpha_2\\ldots,\\alpha_k)$ with $\\alpha_i\\in(0,\\infty)$ is an unnormalized parameterization of a discrete distribution $D\\sim\\text{Discrete}(\\alpha)$. Then each sample $D$ can be expressed as a one-hot vector \\begin{equation} D=\\left[\\begin{matrix}D_1 \\\\ \\vdots \\\\ D_k\\end{matrix}\\right], \\end{equation} where $D_m=1$ and $D_i=0$ for $i=1,\\ldots,k$ and $i\\neq m$. Then \\begin{equation} P(D_m=1)=\\frac{\\alpha_m}{\\sum_{i=1}^{k}\\alpha_i} \\end{equation} The below figure illustrates the reparameterization sampling process.\nFigure 2: (taken from the paper) Graph for sampling process according to the Categorical (Discrete) distribution $D\\sim\\text{Discrete}(\\alpha)$, where $\\alpha=(\\alpha_1,\\alpha_2,\\alpha_3)$. White operations are deterministic, blue stochastic, rounded continuous, and square discrete. For $i=1,2,3$, the samples $G_i$ are computed as $G_i=-\\log(-\\log U_i)$; adding them with $\\log\\alpha_i$, we obtain $x_i$, i.e. $x_i=\\log\\alpha_i+G_i=\\log\\alpha_i-\\log(-\\log U_i)$; and finally which are given in \\eqref{eq:gmt.2} and \\eqref{eq:gmt.3}. The top node denotes one-hot representation of $D$. Gumbel-Softmax \u0026amp; Concrete Distribution Since $\\text{argmax}$ function is non-differentiable (as illustrated by a discrete node in Figure 2), thus in order to apply the reparameterization in SCG, we use a continuous approximation of the function, which is the $\\text{softmax}$ function, as visualized in Figure 3.\nFigure 3: (taken from the paper) Results taken from sampling process (top node) are now probability vectors, i.e. vectors with elements between $0$ and $1$ and summing to $1$. Rather than unit vectors (lying at the corners of $\\Delta^{k-1}$), the resulting samples now are described in stochastic form (staying inside the probability simplex $\\Delta^{k-1}$ instead). They follow to a distribution called Concrete, or Gumbel-Softmax. In particular, as illustrated above, to sample a Concrete random variable $X\\in\\Delta^{k-1}$ at temperature $\\lambda\\in(0,\\infty)$ with parameter vector $\\alpha=(a_1,\\ldots,a_k)$ where $a_i\\in(0,\\infty)$, we first sample $G_i\\overset{\\text{i.i.d}}{\\sim}\\text{Gumbel}(0,1)$ and set \\begin{equation} X_i=\\frac{\\exp((\\log\\alpha_i+G_i)/\\lambda)}{\\sum_{j=1}^{k}\\exp((\\log\\alpha_j+G_j)/\\lambda)}\\label{eq:gsc.1} \\end{equation} The $\\text{softmax}$ computation of \\eqref{eq:gsc.1} smoothly approaches the discrete $\\text{argmax}$ computation as $\\lambda\\to 0$ while preserving the relative order of the Gumbels $\\log\\alpha_i+G_i$. At higher temperature, the Concrete samples are no longer one-hot, and as $\\lambda\\to\\infty$, they become uniform.\nThe probability density function of a Concrete random variable $X\\sim\\text{Concrete}(\\alpha,\\lambda)$ with location vector $\\alpha\\in(0,\\infty)^k$ and temperature $\\lambda\\in(0,\\infty)$ is given as \\begin{equation} f_X(x)=\\Gamma(k)\\lambda^{k-1}\\prod_{i=1}^{k}\\frac{\\alpha_i x_i^{-\\lambda-1}}{\\sum_{j=1}^{k}\\alpha_j x_j^{-\\lambda}} \\end{equation}\nProposition 1 (Properties of Concrete r.v.s) Let $X\\sim\\text{Concrete}(\\alpha,\\lambda)$ with location $\\alpha\\in(0,\\infty)^k$ and temperature $\\lambda\\in(0,\\infty)$. Then\nReparameterization. If $G_i\\overset{\\text{i.i.d}}{\\sim}\\text{Gumbel}(0,1)$, then \\begin{equation} X_i\\overset{\\text{d}}{=}\\frac{\\exp((\\log\\alpha_i+G_i)/\\lambda)}{\\sum_{j=1}^{k}\\exp((\\log\\alpha_j+G_j)/\\lambda)}, \\end{equation} where $A\\overset{\\text{d}}{=}B$ denotes that random variables $A$ and $B$ follow the same distribution. Rounding. $P(X_i\u003eX_m,i\\neq m)=\\frac{\\alpha_i}{\\sum_{j=1}^{k}\\alpha_j}$ Zero temperature. $P\\left(\\lim_{\\lambda\\to 0}X_i=1\\right)=\\frac{\\alpha_i}{\\sum_{j=1}^{k}\\alpha_j}$ Convex eventually. If $\\lambda\\leq(k-1)^{-1}$, then $f_X(x)$ is log-convex in $x$. Proof\nLet $Y_i=\\log\\alpha_i+G_i$ for $i=1,\\ldots,k$, then $Y_i$ are unit-scaled Gumbel r.v.s with locations $\\log\\alpha_i$, which have the PDFs given as \\begin{align} f_{Y_i}(y_i)\u0026=\\exp(-\\exp(-y_i+\\log\\alpha_i)-y_i+\\log\\alpha_i) \\\\ \u0026=\\alpha_i\\exp(-y_i)\\exp(-\\alpha_i\\exp(-y_i))\\label{eq:gsc.2} \\end{align} Also, let \\begin{equation} Z_i=\\frac{\\exp((\\log\\alpha_i+G_i)/\\lambda)}{\\sum_{j=1}^{k}\\exp((\\log\\alpha_j+G_j)/\\lambda)}=\\frac{\\exp(Y_i/\\lambda)}{\\sum_{j=1}^{k}\\exp(Y_j/\\lambda)}, \\end{equation} which implies that the r.v.s $Z_1,\\ldots,Z_k$ relate to each other with the dependence \\begin{equation} \\sum_{i=1}^{k}Z_i=1 \\end{equation} We continue by considering the invertible transformation \\begin{equation} T(y_1,\\ldots,y_k)=(z_1,\\ldots,z_{k-1},c), \\end{equation} where \\begin{align} z_i\u0026=\\frac{1}{c}\\exp\\left(\\frac{y_i}{\\lambda}\\right) \\\\ c\u0026=\\sum_{j=1}^{k}\\exp\\left(\\frac{y_j}{\\lambda}\\right) \\end{align} Hence, its inverse is given by \\begin{align} \\hspace{-1cm}T^{-1}(z_1,\\ldots,z_{k-1},c)\u0026=(y_1,\\ldots,y_k) \\\\ \u0026=(\\lambda\\log(z_1 c),\\ldots,\\lambda\\log(z_{k-1}c),\\lambda\\log(z_k c)) \\\\ \u0026=(\\lambda(\\log z_1+\\log c),\\ldots,\\lambda(\\log z_{k-1}+\\log c),\\lambda(\\log z_k+\\log c)), \\end{align} where $z_k=1-\\sum_{j=1}^{k-1}z_j$. Therefore, by change of variables \\begin{align} f_{Z,C}(z_1,\\ldots,z_k,c)\u0026=f_{Z,C}(z_1,\\ldots,z_{k-1},c) \\\\ \u0026=f_Y(y_1,\\ldots,y_k)\\left\\vert\\text{det}\\left(\\frac{\\partial Y}{\\partial Z}\\right)\\right\\vert\\label{eq:gsc.3} \\end{align} Let us consider the determinant of Jacobian matrix $\\frac{\\partial Y}{\\partial Z}$, which can be computed by \\begin{align} \\hspace{-0.5cm}\\text{det}\\left(\\frac{\\partial Y}{\\partial Z}\\right)\u0026=\\text{det}\\left[\\begin{matrix}\\lambda z_1^{-1}\u00260\u00260\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ 0\u0026\\lambda z_2^{-1}\u00260\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ 0\u00260\u0026\\lambda z_3^{-1}\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ \\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots \\\\ -\\lambda z_k^{-1}\u0026-\\lambda z_k^{-1}\u0026-\\lambda z_k^{-1}\u0026-\\lambda z_k^{-1}\u0026\\ldots\u0026-\\lambda z_k^{-1}\u0026\\lambda c^{-1}\\end{matrix}\\right] \\\\ \u0026=\\text{det}\\left[\\begin{matrix}\\lambda z_1^{-1}\u00260\u00260\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ 0\u0026\\lambda z_2^{-1}\u00260\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ 0\u00260\u0026\\lambda z_3^{-1}\u00260\u0026\\ldots\u00260\u0026\\lambda c^{-1} \\\\ \\vdots\u0026\\vdots\u0026\\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots\u0026\\vdots \\\\ 0\u00260\u00260\u00260\u0026\\ldots\u00260\u0026\\lambda(z_k c)^{-1}\\end{matrix}\\right] \\\\ \u0026=\\frac{\\lambda^k}{c\\prod_{j=1}^{k}z_j} \\end{align} Using the definition of PDFs of $Y_i$ given in \\eqref{eq:gsc.2}, we can continue to derive \\eqref{eq:gsc.3} as \\begin{align} \u0026f_{Z,C}(z_1,\\ldots,z_k,c)\\nonumber \\\\ \u0026=f_Y(y_1,\\ldots,y_k)\\left\\vert\\text{det}\\left(\\frac{\\partial Y}{\\partial Z}\\right)\\right\\vert \\\\ \u0026=\\left(\\prod_{i=1}^{k}f_{Y_i}(y_i)\\right)\\frac{\\lambda^k}{c\\prod_{j=1}^{k}z_j} \\\\ \u0026=\\left(\\prod_{i=1}^{k}f_{Y_i}\\big(\\lambda(\\log z_i+\\log c)\\big)\\right)\\frac{\\lambda^k}{c\\prod_{j=1}^{k}z_j} \\\\ \u0026=\\frac{\\lambda^k\\prod_{i=1}^{k}\\alpha_i\\exp(-\\lambda(\\log z_i+\\log c))\\exp(-\\alpha_i\\exp(-\\lambda(\\log z_i+\\log c)))}{c\\prod_{j=1}^{k}z_j} \\\\ \u0026=\\frac{\\lambda^k\\prod_{i=1}^{k}\\alpha_i}{c\\prod_{j=1}^{k}z_j^{\\lambda+1}}\\exp(-k\\lambda\\log c)\\exp\\left(-\\sum_{i=1}^{k}\\alpha_i\\exp(-\\lambda\\log z_i-\\lambda\\log c)\\right) \\\\ \u0026=\\frac{\\lambda^k\\prod_{i=1}^{k}\\alpha_i}{c\\prod_{j=1}^{k}z_j^{\\lambda+1}}\\exp(-k\\lambda\\log c)\\exp\\left(-\\exp(-\\lambda\\log c)\\sum_{i=1}^{k}\\alpha_i z_i^{-\\lambda}\\right) \\end{align} Integrating the joint density $f_{Z,C}$ over $c$ and letting $\\gamma=\\log\\sum_{i=1}^{k}\\alpha_i z_i^{-\\lambda}$ gives us the marginal distribution \\begin{align} \u0026f_Z(z_1,\\ldots,z_k)\\nonumber \\\\ \u0026=\\int_{0}^{\\infty}\\frac{\\lambda^k\\prod_{i=1}^{k}\\alpha_i}{c\\prod_{j=1}^{k}z_j^{\\lambda+1}}\\exp(-k\\lambda\\log c)\\exp(-\\exp(-\\lambda\\log c+\\gamma))dc \\\\ \u0026=\\int_{0}^{\\infty}\\frac{\\lambda^k\\prod_{i=1}^{k}\\alpha_i}{c\\prod_{j=1}^{k}z_j^{\\lambda+1}\\exp(\\gamma)}\\exp(-k\\lambda\\log c+k\\gamma)\\exp(-\\exp(-\\lambda\\log c+\\gamma))dc \\end{align} Let $u=\\lambda\\log c-\\gamma$, so \\begin{equation} du=d(\\lambda\\log c-\\gamma))=\\frac{\\lambda}{c}dc, \\end{equation} the above integration thus can be rewritten as \\begin{equation} f_Z(z_1,\\ldots,z_k)=\\frac{\\lambda^{k-1}\\prod_{i=1}^{k}\\alpha_i}{\\prod_{j=1}^{k}z_j^{\\lambda+1}\\exp(\\gamma)}\\int_{-\\infty}^{\\infty}\\exp(-ku)\\exp(-\\exp(-u))du \\end{equation} Let $v=\\exp(-u)$, then \\begin{equation} dv=d(\\exp(-u))=-\\exp(-u)du, \\end{equation} which implies that \\begin{align} f_Z(z_1,\\ldots,z_k)\u0026=\\frac{\\lambda^{k-1}\\prod_{i=1}^{k}\\alpha_i}{\\prod_{j=1}^{k}z_j^{\\lambda+1}\\exp(\\gamma)}\\int_{0}^{\\infty}v^{k-1}\\exp(-v)dv \\\\ \u0026=\\frac{\\lambda^{k-1}\\prod_{i=1}^{k}\\alpha_i}{\\prod_{j=1}^{k}z_j^{\\lambda+1}\\exp(\\gamma)}\\Gamma(k) \\\\ \u0026=\\Gamma(k)\\lambda^{k-1}\\prod_{i=1}^{k}\\frac{\\alpha_i z_i^{-\\lambda-1}}{\\sum_{j=1}^{k}\\alpha_j z_j^{-\\gamma}}, \\end{align} which claims that $Z\\overset{\\text{d}}{=}X$. This follows directly from (i) and the Gumbel-Max trick This follows directly from (i) and the Gumbel-Max trick Let $\\lambda\\leq(k-1)^{-1}$. The density of $X$ can be rewritten as \\begin{align} f_X(x)\u0026=\\Gamma(k)\\lambda^{k-1}\\prod_{i=1}^{k}\\frac{\\alpha_i x_i^{-\\lambda-1}}{\\sum_{j=1}^{k}\\alpha_j x_j^{-\\gamma}} \\\\ \u0026=\\Gamma(k)\\lambda^{k-1}\\prod_{i=1}^{k}\\frac{\\alpha_i x_i^{\\lambda(k-1)-1}}{\\sum_{j=1}^{k}\\alpha_j\\prod_{l\\neq j}x_l^\\lambda} \\end{align} Thus, the log density of $X$ is given as \\begin{align} \\log f_X(x)\u0026=\\log\\left(\\Gamma(k)\\lambda^{k-1}\\prod_{i=1}^{k}\\frac{\\alpha_i x_i^{\\lambda(k-1)-1}}{\\sum_{j=1}^{k}\\alpha_j\\prod_{l\\neq j}x_l^\\lambda}\\right) \\\\ \u0026=\\log(\\Gamma(k)\\lambda^{k-1})+\\sum_{i=1}^{k}(\\lambda(k-1)-1)\\log x_i-k\\log\\left(\\sum_{j=1}^{k}\\alpha_j\\prod_{l\\neq j}x_l^\\lambda\\right), \\end{align} which can be easily observed to be convex with $\\lambda\\leq(k-1)^{-1}$ due to $-\\log$ function is convex and non-increasing and $\\prod_{l\\neq j}x_l^\\lambda$ is concave, while the first term $\\log(\\Gamma(k)\\lambda^{k-1})$ is a constant. References [1] Eric Jang, Shixiang Gu, Ben Poole. Categorical Reparameterization with Gumbel-Softmax. ICLR 2017.\n[2] Chris J. Maddison, Andriy Mnih, Yee Whye Teh. The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables. ICLR 2017.\n[3] John Schulman, Theophane Weber, Nicolas Heess, Pieter Abbeel. Gradient Estimation Using Stochastic Computation Graphs. NIPS, 2015.\n[4] Wikipedia. Gumbel distribution.\n[5] Chris J. Maddison, Daniel Tarlow, Tom Minka. A$^*$ Sampling. NIPS, 2014.\nFootnotes The generalization of Bernoulli distribution into $k$ dimensions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is due to the constraint that the probabilities associated with each category sum to $1$ reduces the dimensionality by $1$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/","summary":"\u003cp\u003eNotes on using Gumbel-Softmax \u0026amp; Concrete Distribution in Categorical sampling.\u003c/p\u003e","title":"Categorical Reparameterization with Gumbel-Softmax \u0026 Concrete Distribution"},{"content":" Notes on Maximum Entropy Reinforcement Learning via SQL \u0026amp; SAC.\nEntropy-Regularized Reinforcement Learning Consider an infinite-horizon Markov Decision Process (MDP), defined as a tuple $(\\mathcal{S},\\mathcal{A},p,r,\\gamma)$, where\n$\\mathcal{S}$ is the state space. $\\mathcal{A}$ is the action space. $p:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ is the transition probability distribution, i.e. $p(s,a,s\u0026rsquo;)=p(s\u0026rsquo;\\vert s,a)$ denotes the probability of transitioning to state $s\u0026rsquo;$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function, and let us denote $r_t\\doteq r(s_t,a_t)$ for simplicity. $\\gamma\\in(0,1)$ is the discount factor. To consider entropy regularization setting, we first recall some basics in standard RL, then extend them into the maximum entropy framework.\nObjective Function Regularly, with discounted infinite-horizon MDP, our objective is to maximize the expected cumulative rewards \\begin{equation} J_\\text{std}(\\pi)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t\\right] \\end{equation} In **Entropy-Regularized RLd, or Maximum Entropy RL framework, we wish to maximize the expected entropy-augmented return \\begin{equation} J_\\text{MaxEnt}(\\pi)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\right],\\label{eq:mr.1} \\end{equation} where $\\alpha\u0026gt;0$ is the temperature parameter determines the relative importance of the entropy with the rewards, and thus controls the stochasticity of the optimal policy. The corresponding optimal policy of the maximum entropy objective is then given by \\begin{align} \\pi_\\text{MaxEnt}^*\u0026amp;=\\underset{\\pi}{\\text{argmax}}J_\\text{MaxEnt}(\\pi) \\\\ \u0026amp;=\\underset{\\pi}{\\text{argmax}}\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\right] \\end{align}\nSoft Value Functions In standard RL, value functions are referred to be the expected returns. Thus, the state-value function and state-action value function in maximum entropy framework could be defined as the expected entropy-augmented returns. Specifically, by adding an entropy term, the state-value function is then referred as soft state value function given by \\begin{equation} V_\\pi(s)=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\Big\\vert s_0=s\\right], \\end{equation} and analogously the soft state-action value function, or soft Q-function is given as \\begin{equation} Q_\\pi(s,a)=\\mathbb{E}_{\\pi,p}\\left[r_0+\\sum_{t=1}^{\\infty}\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\Big\\vert s_0=s,a_0=a\\right] \\end{equation} It is worth remarking that those definitions imply that \\begin{align} V_\\pi(s)\u0026amp;=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\Big)\\label{eq:svf.1} \\\\ \u0026amp;=\\mathbb{E}_{a\\sim\\pi}\\big[Q_\\pi(s,a)-\\alpha\\log\\pi(a\\vert s)\\big], \\end{align} and \\begin{align} Q_\\pi(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p}\\big[V_\\pi(s\u0026rsquo;)\\big]\\label{eq:svf.2} \\end{align}\nSoft Bellman Backup Operators In standard RL, let $\\mathcal{T}_\\pi$ be the Bellman operator1, with which we can compute the expected returns by one-step lookahead, i.e. \\begin{align} (\\mathcal{T}_\\pi V_\\pi)(s)\u0026amp;=\\sum_a\\pi(a\\vert s)\\Big[r(s,a)+\\gamma V_\\pi(s\u0026rsquo;)\\Big] \\\\ \u0026amp;=\\mathbb{E}_{a\\sim\\pi}\\Big[r(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p}\\big[V_\\pi(s\u0026rsquo;)\\big]\\Big] \\\\ \u0026amp;=\\mathbb{E}_{a\\sim\\pi,s\u0026rsquo;\\sim p}\\Big[r(s,a)+\\gamma V_\\pi(s\u0026rsquo;)\\Big]\\label{eq:sbo.1} \\end{align} and \\begin{align} (\\mathcal{T}_\\pi Q_\\pi)(s,a)\u0026amp;=r(s,a)+\\gamma\\sum_{s\u0026rsquo;,a\u0026rsquo;}p(s\u0026rsquo;\\vert s,a)\\pi(a\u0026rsquo;\\vert s\u0026rsquo;)Q_\\pi(s\u0026rsquo;,a\u0026rsquo;) \\\\ \u0026amp;=r(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p,a\u0026rsquo;\\sim\\pi}\\Big[Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Big[r(s,a)+\\gamma\\mathbb{E}_{a\u0026rsquo;\\sim\\pi}\\big[Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\big]\\Big] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p,a\u0026rsquo;\\sim\\pi}\\Big[r(s,a)+\\gamma Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\Big] \\end{align} Repeatedly applying $\\mathcal{T}_\\pi$ operator $n$ times yields the $n$-step Bellman operator, denoted $\\mathcal{T}_\\pi^{(n)}$, which allows us to compute the expected returns with $n$-step lookahead, i.e. \\begin{align} (\\mathcal{T}_\\pi^{(n)}V_\\pi)(s)\u0026amp;=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t r_t+\\gamma^n V_\\pi(s_n)\\Bigg\\vert s_0=s\\right], \\\\ (\\mathcal{T}_\\pi^{(n)}Q_\\pi)(s,a)\u0026amp;=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t r_t+\\gamma^n Q_\\pi(s_n,a_n)\\Bigg\\vert s_0=s,a_0=a\\right] \\end{align} We can generalize the Bellman operator $\\mathcal{T}_\\pi$ to the maximum entropy setting to get a recursive relationship between successive value functions. Specifically, by adding an entropy term, \\eqref{eq:sbo.1} can be rewritten as \\begin{equation} (\\mathcal{T}_\\pi V_\\pi)(s)=\\mathbb{E}_{a\\sim\\pi,s\u0026rsquo;\\sim p}\\Big[r(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)+\\gamma V_\\pi(s\u0026rsquo;)\\Big], \\end{equation} and analogously we have \\begin{align} (\\mathcal{T}_\\pi Q_\\pi)(s,a)\u0026amp;=r(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Big[\\mathbb{E}_{a\u0026rsquo;\\sim\\pi}\\big[Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s\u0026rsquo;)\\big)\\Big]\\label{eq:sbo.2} \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Big[r(s,a)+\\gamma\\Big(\\mathbb{E}_{a\u0026rsquo;\\sim\\pi}\\big[Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s\u0026rsquo;)\\big)\\Big)\\Big] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p,a\u0026rsquo;\\sim\\pi}\\Big[r(s,a)+\\gamma\\Big(Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)+\\alpha H\\big(\\pi(\\cdot\\vert s\u0026rsquo;)\\big)\\Big)\\Big] \\end{align} And also, the $n$-step Bellman operators generalize for entropy regularization framework are given by \\begin{equation} (\\mathcal{T}_\\pi^{(n)}V_\\pi)(s)=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t\\Big(r_t+\\gamma H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)+\\gamma^n V_\\pi(s_n)\\Bigg\\vert s_0=s\\right],\\label{eq:sbo.3} \\end{equation} and \\begin{align} \\hspace{-0.8cm}\u0026amp;(\\mathcal{T}_\\pi^{(n)}Q_\\pi)(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\nonumber \\\\ \\hspace{-0.8cm}\u0026amp;=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)+\\gamma^n\\Big(Q_\\pi(s_n,a_n)+\\alpha H\\big(\\pi(\\cdot\\vert s_n)\\big)\\Big)\\Bigg\\vert s_0=s,a_0=a\\right] \\end{align} The above $n$-step Bellman operator for state-action value can also be deduced by combining \\eqref{eq:sbo.3} with the result \\eqref{eq:svf.1}.\nGreedy Policy Recall that in standard setting, the greedy policy for state-action value function $Q$ are defined as a deterministic policy that selects the greedy action in the sense that maximizes the state-action value function, i.e. \\begin{equation} \\pi_\\text{g}(s)\\doteq\\underset{a}{\\text{argmax}}Q(s,a) \\end{equation} With entropy-regularized, the greedy policy instead maximizes the entropy-augmented value function, and thus given in stochastic form that for some $s\\in\\mathcal{S}$ \\begin{align} \\pi_\\text{g}(\\cdot\\vert s)\u0026amp;\\doteq\\underset{\\pi}{\\text{argmax}}\\mathbb{E}_{a\\sim\\pi}\\Big[Q(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\label{eq:gp.1} \\\\ \u0026amp;=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q(s,\\cdot)\\right)}{\\mathbb{E}_{a\u0026rsquo;\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a\u0026rsquo;)\\right)\\right]}, \\end{align} where $\\tilde{\\pi}$ is some \u0026ldquo;reference\u0026rdquo; policy, and thus the denominator is acting as a normalizing constant since it is independent of $\\pi$.\nTo verify this, we begin by considering2 \\begin{align} \\hspace{-1.2cm}H\\big(\\pi(\\cdot\\vert s)\\big)\u0026amp;=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\mathbb{E}_{a\\sim\\pi}\\big[\\log\\pi_\\text{g}(a\\vert s)\\big] \\\\ \u0026amp;=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\mathbb{E}_{a\\sim\\pi}\\left[\\frac{1}{\\alpha}Q(s,a)-\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right]\\right] \\\\ \u0026amp;=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\frac{1}{\\alpha}\\mathbb{E}_{a\\sim\\pi}\\big[Q(s,a)\\big]+\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right],\\label{eq:gp.2} \\end{align} where $D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)$ denotes the KL divergence between $\\pi(\\cdot\\vert s)$ and $\\pi_\\text{g}(\\cdot\\vert s)$. The result \\eqref{eq:gp.2} implies that \\begin{equation} \\hspace{-1.2cm}\\mathbb{E}_{a\\sim\\pi}\\big[Q(s,a)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)=-\\alpha D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)+\\alpha\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right] \\end{equation} Since $\\alpha\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right]$ does not depend on $\\pi$ and $\\alpha\\in[0,1]$, the policy $\\pi$ that maximizes LHS is the one that minimizes $D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)$, which proves our claim due to the fact that KL divergence between two distributions is $\\geq 0$ with equality holds when they are identical.\nBackup Operators for Greedy Policy It has been shown that we can also define backup operators for value functions corresponding to greedy policy $\\pi_\\text{g}$. In particular, we have \\begin{align} \\hspace{-1.2cm}(\\mathcal{T}Q_{\\pi_\\text{g}})(s,a)\u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Big[r(s,a)+\\gamma\\Big(\\mathbb{E}_{a\u0026rsquo;\\sim\\pi_\\text{g}}\\big[Q_{\\pi_\\text{g}}(s\u0026rsquo;,a\u0026rsquo;)\\big]+\\alpha H\\big(\\pi_\\text{g}(\\cdot\\vert s\u0026rsquo;)\\big)\\Big)\\Big] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\left[r(s,a)+\\gamma\\left(\\mathbb{E}_{a\u0026rsquo;\\sim\\pi_\\text{g}}\\big[Q_{\\pi_\\text{g}}(s\u0026rsquo;,a\u0026rsquo;)\\big]-\\alpha\\sum_{a\u0026rsquo;}\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)\\log\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)\\right)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\left[r(s,a)+\\gamma\\sum_{a\u0026rsquo;}\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)\\big(Q_{\\pi_\\text{g}}(s\u0026rsquo;,a\u0026rsquo;)-\\alpha\\log\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)\\big)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\left[r(s,a)+\\gamma\\sum_{a\u0026rsquo;}\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)\\alpha\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s\u0026rsquo;,\\tilde{a})\\right)\\right]\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Bigg[r(s,a)+\\gamma\\alpha\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s\u0026rsquo;,\\tilde{a})\\right)\\right]\\Bigg] \\\\ \u0026amp;=\\mathbb{E}_{s\u0026rsquo;\\sim p}\\Bigg[r(s,a)+\\gamma\\alpha\\log\\mathbb{E}_{a\u0026rsquo;\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s\u0026rsquo;,a\u0026rsquo;)\\right)\\right]\\Bigg]\\label{eq:bogp.1} \\end{align} where in the fifth step, we use the fact that $\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s\u0026rsquo;,\\tilde{a})\\right)\\right]$ is independent of $a\u0026rsquo;$, which allows us to do the summation of $\\pi_\\text{g}$ over the action space $\\mathcal{A}$, i.e. \\begin{equation} \\sum_{a\u0026rsquo;}\\pi_\\text{g}(a\u0026rsquo;\\vert s\u0026rsquo;)=1 \\end{equation}\nSoft Policy Iteration In standard RL, the Bellman operator provides useful facts to apply the dynamic programming method - policy iteration, which alternates between policy evaluation and policy improvement processes, and eventually we end up with the optimal policy. More importantly, it has been proved that we can also apply the method to entropy-regularized RL.\nSoft Policy Evaluation In policy evaluation process, we wish to compute the value of a given policy according to the entropy regularization objective \\eqref{eq:mr.1}. This can be found by an iterative method.\nLemma 1 (Soft Policy Evaluation). Consider the Bellman backup operator $\\mathcal{T}_\\pi$ specified in \\eqref{eq:sbo.2} and a mapping $Q^{(0)}:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ with $\\vert\\mathcal{A}\\vert\\lt\\infty$, and define $Q^{(k+1)}=\\mathcal{T}_\\pi Q^{(k)}$. The resulting sequence $\\{Q^{(k)}\\}_{k=0,1\\ldots,}$ will converge as $k\\to\\infty$.\nProof\nLet $r_\\pi(s,a)\\doteq r(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p}\\big[\\alpha H\\big(\\pi(\\cdot\\vert s\u0026rsquo;)\\big)\\big]$ denote the entropy augmented reward, the update rule can be rewritten as \\begin{equation} Q^{(k+1)}(s,a)=r_\\pi(s,a)+\\gamma\\mathbb{E}_{s\u0026rsquo;\\sim p,a\u0026rsquo;\\sim\\pi}\\big[Q^{(k)}(s\u0026rsquo;,a\u0026rsquo;)\\big] \\end{equation} Since $\\vert\\mathcal{A}\\vert\u0026lt;\\infty$, we have that $r_\\pi(s,a)$ is bounded. Analogy to the (standard) policy evaluation, we then can prove that $\\mathcal{T}_\\pi$ is a contraction mapping and then by using the Banach\u0026rsquo;s fixed point theorem, we can show that $\\{Q^{(k)}\\}_{k=0,1,\\ldots}$ eventually converges to a fixed point, which we call it the soft Q-value of $\\pi$.\nSoft Policy Improvement Analogously, the (standard) policy improvement step can be generalized to entropy regularizing as:\nLemma 2 (Soft Policy Improvement) Let $\\Pi$ be some set of policies, $\\pi_\\text{old}\\in\\Pi$ be some policy and for each $s_t$ let $\\pi_\\text{new}$ be defined as \\begin{equation} \\pi_\\text{new}(\\cdot\\vert s_t)=\\underset{\\pi\u0026rsquo;\\in\\Pi}{\\text{argmin}}D_\\text{KL}\\left(\\pi\u0026rsquo;(\\cdot\\vert s_t)\\Bigg\\Vert\\frac{\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,\\cdot)\\right)}{\\mathbb{E}_{a_t\\sim\\tilde{\\pi}_\\text{old}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,a_t)\\right)\\right]}\\right) \\end{equation} Then $Q_{\\pi_\\text{new}}(s_t,a_t)\\geq Q_{\\pi_\\text{old}}(s_t,a_t)$ for all $(s_t,a_t)\\in\\mathcal{S}\\times\\mathcal{A}$ with $\\vert\\mathcal{A}\\vert\\lt\\infty$.\nProof\nAs KL divergence between two distribution reaches its minimum when those two distributions are identical, we then have that for each $s_t\\in\\mathcal{S}$ \\begin{equation} \\pi_\\text{new}(\\cdot\\vert s_t)=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,\\cdot)\\right)}{\\mathbb{E}_{a_t\\sim\\tilde{\\pi}_\\text{old}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,a_t)\\right)\\right]}, \\end{equation} which is the greedy policy $\\pi_\\text{g}$, and thus by \\eqref{eq:gp.1}, for all $s_t\\in\\mathcal{S}$, we have \\begin{align} \\mathbb{E}_{a_t\\sim\\pi_\\text{new}}\\big[Q_{\\pi_\\text{old}}(s_t,a_t)\\big]+\\alpha H\\big(\\pi_\\text{new}(\\cdot\\vert s_t)\\big)\u0026amp;\\geq\\mathbb{E}_{a_t\\sim\\pi_\\text{old}}\\big[Q_{\\pi_\\text{old}}(s_t,a_t)\\big]+\\alpha H\\big(\\pi_\\text{old}(\\cdot\\vert s_t)\\big) \\\\ \u0026amp;=V_{\\pi_\\text{old}}(s_t) \\end{align} Therefore, combined with \\eqref{eq:svf.2} we obtain \\begin{align} Q_{\\pi_\\text{old}}(s_t,a_t)\u0026amp;=r_t+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\big[V_{\\pi_\\text{old}}(s_{t+1})\\big] \\\\ \u0026amp;\\leq r_t+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\Big[\\mathbb{E}_{a_{t+1}\\sim\\pi_\\text{new}}\\big[Q_{\\pi_\\text{old}}(s_{t+1},a_{t+1})\\big]+\\alpha H\\big(\\pi_\\text{new}(\\cdot\\vert s_{t+1})\\big)\\Big] \\\\ \u0026amp;\\hspace{0.3cm}\\vdots \\\\ \u0026amp;\\leq Q_{\\pi_\\text{new}}(s_t,a_t) \\end{align}\nNow we are ready to specify the policy iteration for entropy-regularized RL.\nTheorem 3 (Soft Policy Iteration) Repeated application of soft policy evaluation and soft policy improvement to any $\\pi\\in\\Pi$ converges to a policy $\\pi^*$ such that $Q_{\\pi^∗}(s_t,a_t)\\geq Q_\\pi(s_t,a_t)$ for all $\\pi\\in\\Pi$ and for all $(s_t,a_t)\\in\\mathcal{S}\\times\\mathcal{A}$ , assuming $\\vert\\mathcal{A}\\vert\\lt\\infty$.\nProof\nThis can be easily proved since the soft Bellman operator $\\mathcal{T}_\\pi$ defined in \\eqref{eq:sbo.2} and backup operator $\\mathcal{T}$ given in \\eqref{eq:bogp.1} both are contractions.\nSoft Actor-Critic In large scale problems, it is impractical to run either policy evaluation or policy improvement until convergence. It is then necessary to use an approximation version of soft policy iteration, which we call Soft Actor-Critic, or SAC. SAC instead uses function approximators (neural network function approximation is our choice) for both the policy and the soft Q-function, and rather than alternating between evaluation and improvement to convergence, using SGD to optimize both networks.\nThese following are key components of SAC method:\nA policy $\\pi_\\phi$ and two soft Q-functions $Q_{\\theta_1},Q_{\\theta_2}$. Utilizes shared target Q-networks, $Q_{\\overline{\\theta}_1},Q_{\\overline{\\theta}_2}$, whose parameters are soft updated due to \\begin{equation} \\bar{\\theta}_i\\leftarrow\\tau\\theta_i+(1-\\tau)\\bar{\\theta}_i,\\hspace{2cm}i=\\{1,2\\} \\end{equation} where $\\tau\\in(0,1]$ and close to $0$. Off-policy training with samples from a replay buffer $\\mathcal{D}$ to minimize correlations between samples. The rollout phase is given as for each step $t$ \\begin{align*} \u0026a_t\\sim\\pi_\\phi(a_t\\vert s_t) \\\\ \u0026s_{t+1}\\sim p(s_{t+1}\\vert s_t,a_t) \\\\ \u0026\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{s_t,a_t,r(s_t,a_t),s_{t+1},d_t\\}, \\end{align*} where $d_t$ informs whether $s_{t+1}$ is the terminal state. The Q-function parameters are trained to minimize the Mean Square Bellman Error (MSBE) \\begin{equation} J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim\\mathcal{D}}\\left[\\frac{1}{2}\\Big(y_t-Q_\\theta(s_t,a_t)\\Big)^2\\right], \\end{equation} where $y_t$ is the TD target at step $t$, which is given by \\begin{align} y_t\u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\big[V_\\overline{\\theta}(s_{t+1})\\big] \\\\ \u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p,a_{t+1}\\sim\\pi_\\phi}\\Big[\\gamma\\Big(Q_\\overline{\\theta}(s_{t+1},a_{t+1})+\\alpha H\\big(\\pi_\\phi(\\cdot\\vert s_{t+1})\\big)\\Big)\\Big] \\\\ \u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p,a_{t+1}\\sim\\pi_\\phi}\\Big[Q_\\overline{\\theta}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\Big], \\end{align} which, as an expectation, can be approximated with samples from replay buffer $\\mathcal{D}$ with current policy $\\pi_\\phi$: \\begin{equation} y_t\\approx r(s_t,a_t)+\\gamma\\big(Q_\\phi(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\big) \\end{equation} Therefore, the loss function for Q-networks at step $t$ is given by \\begin{equation} J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t,r,s_{t+1},d_t)\\sim\\mathcal{D},a_{t+1}\\sim\\pi_\\phi}\\left[\\frac{1}{2}\\big(y(r,s_{t+1},a_{t+1},d_t)-Q_\\theta(s_t,a_t)\\big)^2\\right],\\label{eq:sac.1} \\end{equation} where \\begin{equation} y(r,s_{t+1},a_{t+1},d_t)=r+\\gamma(1-d_t)\\big(Q_\\overline{\\theta}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\big)\\label{eq:sac.2} \\end{equation} The loss function $J_Q(\\theta)$ in \\eqref{eq:sac.1} then can be optimized according to SGD using \\begin{equation} \\hat{\\nabla}_\\theta J_Q(\\theta)=\\nabla_\\theta Q_\\theta(s_t,a_t)\\big(y_t-Q_\\theta(s_t,a_t)\\big),\\label{eq:sac.3} \\end{equation} where $y_t$ is the TD target at time-step $t$, which can be computed according \\eqref{eq:sac.2}. The policy, in each state, acts greedily as \\eqref{eq:gp.1}, which maximizes the expected entropy-augmented return, which is $V_\\pi(s)$ \\begin{align} V_\\pi(s)\u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\Big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)-\\alpha\\log\\pi(a\\vert s)\\Big] \\end{align} Hence, the policy parameters $\\phi$ can be learned by directly maximizing \\begin{align} J_\\pi(\\phi)\u0026=\\mathbb{E}_{s_t\\sim\\mathcal{D}}\\Big[\\mathbb{E}_{a_t\\sim\\pi_\\phi}\\big[Q_\\theta(s_t,a_t)-\\alpha\\log\\pi_\\phi(a_t\\vert s_t)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s_t\\sim\\mathcal{D},a_t\\sim\\pi_\\phi}\\Big[Q_\\theta(s_t,a_t)-\\alpha\\log\\pi_\\phi(a_t\\vert s_t)\\Big]\\label{eq:sac.4} \\end{align} Since Q-function is represented by a neural network and can be differentiated, in SAC paper, the authors make use of the reparameterization trick to reduce variance. In particular, samples are obtained according to \\begin{equation} a_\\phi(s_t,\\epsilon_t)=\\text{tanh}(\\mu_\\phi(s_t)+\\sigma_\\phi(s_t)\\odot\\epsilon_t) \\end{equation} where $\\epsilon_t\\sim\\mathcal{N}(0,I)$ is a spherical Gaussian noise, $\\mu_\\phi$ and $\\sigma_\\phi$ are defined as given in the next key point. The loss function in \\eqref{eq:sac.4} then can be rewritten as \\begin{equation} J_\\pi(\\phi)=\\mathbb{E}_{s_t\\sim\\mathcal{D},\\epsilon_t\\sim\\mathcal{N}(0,I)}\\Big[Q_\\theta\\big(s_t,a_\\phi(s_t,\\epsilon_t)\\big)-\\alpha\\log\\pi_\\phi\\big(a_\\phi(s_t,\\epsilon_t)\\vert s_t\\big)\\Big], \\end{equation} which rather than taking the expectation over actions ($a_t\\sim\\pi_\\theta$, depends on $\\theta$), computing over noise ($\\epsilon_t\\sim\\mathcal{N}(0,I)$, depends on nothing). This function can be optimized with SGD with \\begin{equation} \\hspace{-0.9cm}\\hat{\\nabla}_\\phi J_\\pi(\\phi)=\\big(\\nabla_{a_t}Q_\\theta(s_t,a_t)-\\alpha\\nabla_{a_t}\\log\\pi_\\phi(a_t\\vert s_t)\\big)\\nabla_\\phi a_\\phi(s_t,\\epsilon_t)-\\alpha\\nabla_\\phi\\log\\pi_\\phi(a_t\\vert s_t),\\label{eq:sac.5} \\end{equation} where $a_t$ is evaluated at $a_\\phi(s_t,\\epsilon_t)$. For continuous action space tasks, a stochastic policy is usually given in form of a diagonal Gaussian, i.e. \\begin{equation} \\pi_\\phi(\\cdot\\vert s)=\\mathcal{N}(\\mu_\\phi(s),\\Sigma_\\phi(s))=\\mathcal{N}(\\mu_\\phi(s),\\sigma_\\theta(s)^2 I)=\\mu_\\phi(s)+\\sigma_\\phi(s)\\mathcal{N}(0,I) \\end{equation} hence, when sampling from $\\pi_\\theta$, let $\\epsilon_t\\sim\\mathcal{N}(0,I)$ be a vector of spherical Gaussian noise, an action $a_t\\sim\\mathcal{N}(\\mu_\\theta(s),\\sigma_\\phi(s)^2 I)$ can be computed as \\begin{equation} a_t=\\mu_\\phi(s_t)+\\sigma_\\phi(s_t)\\odot\\epsilon_t, \\end{equation} where $\\odot$ denotes the elementwise product of two vectors.\nSince the normal distribution taking range of $(-\\infty,\\infty)$, it is necessary to bound the policy to a finite interval, which can be performed by applying a squashing function (e.g. $\\text{tanh}$,sigmoid,etc) to the Gaussian samples. For instance, the $\\text{tanh}$ function converts support of $(-\\infty,\\infty)$ into $(-1,1)$. Two soft Q-functions $Q_{\\theta_1},Q_{\\theta_2}$ are trained independently to optimize $J_Q(\\theta_1),J_Q(\\theta_2)$ respectively. Also, the minimum of the soft Q-functions is used in \\eqref{eq:sac.3} and \\eqref{eq:sac.5} instead, i.e. \\begin{align} \\hat{\\nabla}_\\theta J_Q(\\theta)\u0026=\\nabla_\\theta Q_\\theta(s_t,a_t)\\big(y_t-Q_\\theta(s_t,a_t)\\big), \\\\ \\hat{\\nabla}_\\phi J_\\pi(\\phi)\u0026=\\big(\\nabla_{a_t}\\min_{i=1,2}Q_{\\theta_i}(s_t,a_t)-\\alpha\\nabla_{a_t}\\log\\pi_\\phi(a_t\\vert s_t)\\big)\\nabla_\\phi a_\\phi(s_t,\\epsilon_t)\\nonumber \\\\ \u0026\\hspace{2cm}-\\alpha\\nabla_\\phi\\log\\pi_\\phi(a_t\\vert s_t) \\end{align} where \\begin{equation} y_t=r+\\gamma(1-d_t)\\left(\\min_{i=1,2}Q_{\\overline{\\theta}_i}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\right) \\end{equation} Instead of considering entropy coefficient $\\alpha$ as a constant, in the newer version of SAC, authors treated it as a parameter and can be optimized due to the loss function \\begin{equation} J(\\alpha)=\\mathbb{E}_{a_t\\sim\\pi_t}\\big[-\\alpha\\log\\pi(a_t\\vert s_t)-\\alpha\\bar{H}\\big] \\end{equation} where $\\pi_t$ denotes the current policy at time-step $t$ and $\\bar{H}$ is target entropy value, usually is set as $-\\text{dim}(\\mathcal{A})$. Pseudocode for our final algorithm is given below.\nSoft Q-learning References [1] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies. ICML, 2017.\n[2] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv preprint, arXiv:1812.05905, 2018.\n[3] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine. Soft Actor-Critic Algorithms and Applications. arXiv preprint, arXiv:1812.05905, 2019.\n[4] Brian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD Thesis, Carnegie Mellon University, 2010.\n[5] John Schulman, Xi Chen, Pieter Abbeel. Equivalence Between Policy Gradients and Soft Q-Learning. arXiv preprint arXiv:1704.06440, 2018.\n[6] Csaba Szepesvári. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2010.\n[7] Richard S. Sutton, Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[8] Josh Achiam. Spinning Up in Deep Reinforcement Learning. SpinningUp2018, 2018.\nFootnotes With an abuse of notation, $\\mathcal{T}_\\pi$ implicitly represents two mappings $\\mathcal{T}_\\pi:\\mathcal{S}\\to\\mathcal{S}$ and $\\mathcal{T}_\\pi\u0026rsquo;:\\mathcal{S}\\times\\mathcal{A}\\to\\mathcal{S}\\times\\mathcal{A}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe formula for greedy policy can also be derived by another way. First off, consider the objective we have \\begin{align*} J(\\pi(\\cdot\\vert s))\u0026amp;=\\mathbb{E}_{a\\sim\\pi}\\Big[Q(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big) \\\\ \u0026amp;=\\sum_{a\\sim\\pi}\\pi(a\\vert s)Q(s,a)-\\sum_{a\\sim\\pi}\\pi(a\\vert s)\\log\\pi(a\\vert s) \\\\ \u0026amp;=\\sum_{a}\\pi(a\\vert s)\\big(Q(s,a)-\\alpha\\log\\pi(a\\vert s)\\big) \\end{align*} Thus, the partial derivative of the $J(\\pi(\\cdot\\vert s))$ w.r.t $\\pi(\\bar{a}\\vert s)$ for some $\\bar{a}\\in\\mathcal{A}$ is given as \\begin{align*} \\nabla_{\\pi(\\bar{a}\\vert s)}J(\\pi(\\cdot\\vert s))\u0026amp;=\\nabla_{\\pi(\\bar{a}\\vert s)}\\sum_a\\pi(a\\vert s)\\Big[Q(s,a)-\\alpha\\log\\pi(a\\vert s)\\Big] \\\\ \u0026amp;=Q(s,\\bar{a})-\\alpha\\log\\pi(\\bar{a}\\vert s)-\\alpha\\pi(\\bar{a}\\vert s)\\cdot\\frac{1}{\\pi(\\bar{a}\\vert s)} \\\\ \u0026amp;=Q(s,\\bar{a})-\\alpha\\log\\pi(\\bar{a}\\vert s)-\\alpha \\end{align*} Setting the derivative to zero yields \\begin{equation*} \\pi(\\bar{a}\\vert s)=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q(s,\\bar{a})\\right)}{\\exp\\alpha} \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on Maximum Entropy Reinforcement Learning via SQL \u0026amp; SAC.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Maximum Entropy Reinforcement Learning via Soft Q-learning \u0026 Soft Actor-Critic"},{"content":"Notes on Representation in PGMs.\nGraphs A graph, denoted $\\mathcal{K}$ is a tuple of $\\mathcal{X}$ and $\\mathcal{E})$ where $\\mathcal{X}=\\{X_1,\\ldots,X_n\\}$ is the sets of nodes (or vertices) and $\\mathcal{E}$ is the set of edges. \\begin{equation} \\mathcal{K}=(\\mathcal{X},\\mathcal{E}) \\end{equation}\nNodes, Edges Any pair of nodes $X_i,X_j$, for $i\\neq j$ is connected by either a directed edge $X_i\\rightarrow X_j$ or an undirected edge $X_i-X_j$1. We use the notation $X_i\\rightleftharpoons X_j$ to denote that $X_i$ is connected to $X_j$ via some edge, whether directed (in any direction) or undirected.\nIf the graph contains directed edges only, we call it a directed graph, denoted $\\mathcal{G}$, else if the graph established by undirected edge only, it is referred as undirected graph, denoted $\\mathcal{H}$.\nFigure 1: (taken from the PGM book) Example of a partially directed graph $\\mathcal{K}$ Following are some necessary notations:\nIf $X_i\\rightarrow X_j\\in\\mathcal{E}$, we say that $X_i$ is the parent of $X_j$ while $X_j$ is the child of $X_i$.\nE.g. node $I$ is a child of nodes $C,E$ and $H$ while $D$ is a parent of $G$. If $X_i-X_j\\in\\mathcal{E}$, we say that $X_i$ is a neighbor of $X_j$, and vice versa.\nE.g. node $F$ is a neighbor of $G$. If $X\\rightleftharpoons Y\\in\\mathcal{E}$, we say that $X$ and $Y$ are adjacent.\nE.g. nodes $A$ and $C$ are adjacent, while $D$ is adjacent to $E$. We use $\\text{Pa}_X$ to denote the set of parents of $X$, $\\text{Ch}_X$ to denote the set of its children and $\\text{Nb}_X$ to denote its neighbors. The set $\\text{Boundary}_X\\doteq\\text{Pa}_X\\cup\\text{Nb}_X$ is known as the boundary of $X$.\nE.g. $\\text{Pa}_I=\\{C,E,H\\}$ and $\\text{Boundary}_F=\\text{Pa}_F\\cup\\text{Nb}_F=\\{C\\}\\cup\\{G\\}=\\{C,G\\}$. The degree of a node $X$ is the number of edges in which it participates; its indegree is the number of directed edges $Y\\rightarrow X$. The degree of a graph is the maximal degree of a node in the graph.\nE.g. node $D$ has degree of $3$, indegree of $0$; the graph $\\mathcal{K}$ has degree of $3$. Subgraphs Consider the graph $\\mathcal{K}=(\\mathcal{X},\\mathcal{E})$ and let $\\mathbf{X}\\subset\\mathcal{X}$ be a subset of nodes in $\\mathcal{K}$. Then:\nThe induced subgraph of $\\mathcal{K}$, denoted $\\mathcal{K}[\\mathbf{X}]$ is defined as the graph $(\\mathbf{X},\\mathcal{E}')$ where \\begin{equation} \\mathcal{E}'=\\{X\\rightleftharpoons Y:X,Y\\in\\mathbf{X}\\} \\end{equation} A subgraph over $\\mathbf{X}$ is complete if every two nodes in $\\mathbf{X}$ are connected via some edges. The set $\\mathbf{X}$ is known as a clique; or even a maximal clique if for any set of nodes $\\mathbf{Y}\\supset\\mathbf{X}$, $\\mathbf{Y}$ is not a clique, i.e. \\begin{equation} \\{\\mathbf{Y}\\text{ clique}:\\mathbf{Y}\\supset\\mathbf{X}\\}=\\emptyset \\end{equation} The set $\\mathbf{X}$ is called upward closed in $\\mathbf{K}$ if for any $X\\in\\mathbf{X}$, we have that \\begin{equation} \\text{Boundary}_X\\subset\\mathbf{X} \\end{equation} The upward closure of $\\mathbf{X}$ is the minimal closed subset $\\mathbf{Y}$ covering $\\mathbf{X}$, i.e. \\begin{equation} \\mathbf{Y}=\\sup\\{\\bar{\\mathbf{Y}}\\text{ upward closed in }\\mathcal{K}:\\bar{\\mathbf{Y}}\\supset\\mathbf{X}\\} \\end{equation} The upwardly closed subgraph of $\\mathbf{X}$, denoted $\\mathcal{K}^+[\\mathbf{X}]$, is the induced subgraph over $\\mathbf{Y}$, $\\mathcal{K}[\\mathbf{Y}]$. Paths, Trails Consider the graph $\\mathcal{K}=(\\mathcal{X},\\mathcal{E})$, the basic notion of edges gives rise to following definitions:\n$X_1,\\ldots,X_k$ form a path in $\\mathcal{K}$ if for every $i=1,\\ldots,k-1$, we have that either $X_i\\rightarrow X_{i+1}$ or $X_i-X_{i+1}$. A path is directed if there exists a directed edge $X_i\\rightarrow X_{i+1}$. $X_1,\\ldots,X_k$ form a trail in $\\mathcal{K}$ if for every $i=1,\\ldots,k-1$, we have that $X_i\\rightleftharpoons X_{i+1}$. $\\mathcal{K}$ is connected if for every pair $X_i,X_j$ there is a trail between $X_i$ and $X_j$. $X$ is an ancestor of $Y$ and correspondingly $Y$ is a descendant of $X$ in $\\mathcal{K}$ if there exists a directed path $X_1,\\ldots,X_k$ with $X_1=X$ and $X_k=Y$. An ordering of nodes $X_1,\\ldots,X_n$ is a topological ordering relative to $\\mathbf{K}$ if whenever we have $X_i\\rightarrow X_j\\in\\mathcal{E}$, then $i\\lt j$. This gives rise to critical results that for each node $X_i$, we have that \\begin{equation} \\text{Pa}_{X_i}\\subset\\{X_1,\\ldots,X_{i-1}\\}, \\end{equation} and \\begin{equation} \\text{Ch}_{X_i}\\subset\\{X_{i+1},\\ldots,X_n\\} \\end{equation} Cycles, Loops A cycle in $\\mathcal{K}$ is a directed path $X_1,\\ldots,X_k$ where $X_1=X_k$. $\\mathcal{K}$ is acyclic if it contains no cycles. $\\mathcal{K}$ is a directed acyclic graph (or DAG) if it is both directed and acyclic. An acyclic graph containing both directed and undirected edges is known as a partially directed acyclic graph (or PDAG).\nLet $\\mathcal{K}$ be a PDAG over $\\mathcal{X}$ and let $\\mathbf{K}_1,\\ldots,\\mathbf{K}_\\ell$ be a disjoint partition of $\\mathcal{X}$ such that: the induced subgraph over $\\mathbf{K}_i$ contains no directed edges; for any pair of nodes $X\\in\\mathbf{K}_i$ and $Y\\in\\mathbf{K}_j$ for $i\\lt j$, an edge between $X$ and $Y$ can only be a directed edge $X\\rightarrow Y$. Each component $\\mathbf{K}_i$ is called a chain component, while $\\mathcal{K}$ is referred as a chain graph. A loop in $\\mathcal{K}$ is a trail $X_1,\\ldots,X_k$ where $X_1=X_k$. A graph is singly connected if it contains no loops. A node in a singly connected graph is called a leaf if it has exactly one adjacent node. A singly connected directed graph is called a polytree, while a singly connected undirected graph is known as a forest; if it is also connected, it is called a tree. A directed graph is a forest if each node has at most one parent. A directed forest is a tree if it is also connected. Directed Graphical Model A Directed Graphical Model (or Bayesian network) is a tuple $\\mathcal{B}=(\\mathcal{G},P)$ where\n$\\mathcal{G}$ is a Bayesian network structure, $P$ factorizes according to $\\mathcal{G}$, $P$ is specified as a set of CPDs associated with $\\mathcal{G}$\u0026rsquo;s nodes. Bayesian Network Structure A Bayesian network structure (or Bayesian network graph, BN graph) is a DAG, denoted $\\mathcal{G}=(\\mathcal{X},\\mathcal{E})$ with $\\mathcal{X}=\\{X_1,\\ldots,X_n\\}$ where\nEach node $X_i\\in\\mathcal{X}$ represents a random variable. Each node $X_i\\in\\mathcal{X}$ is associated with a conditional independencies assumption, called local independencies, denoted $\\mathcal{I}_\\ell(\\mathcal{G})$, which says that $X_i$ is conditionally independent of its non-descendants given its parent, i.e. \\begin{equation} (X_i\\perp\\text{NonDescendants}_{X_i}\\vert\\hspace{0.1cm}\\text{Pa}_{X_i}), \\end{equation} where $\\text{NonDescendants}_{X_i}$ denotes the set of non-descendant nodes of $X_i$. I-Maps Let $P$ be a distribution over $\\mathcal{X}$, we define $\\mathcal{I}(P)$ to be the set of independence assertions of the form $(X\\perp Y\\hspace{0.1cm}\\vert Z)$ that hold in $P$, i.e. \\begin{equation} P\\models(X\\perp Y\\vert Z), \\end{equation} or \\begin{equation} P(X,Y\\vert Z)=P(X\\vert Z)P(Y\\vert Z) \\end{equation} Let $\\mathcal{K}$ be a graph associated with a set of independencies $\\mathcal{I}(\\mathcal{K})$, then $\\mathcal{K}$ is an I-map (for independence map) for a set of independencies $\\mathcal{I}$ if $\\mathcal{I}(\\mathcal{K})\\subset\\mathcal{I}$.\nHence, if $P$ satisfies the local dependencies associated with $\\mathcal{G}$, we have \\begin{equation} \\mathcal{I}_\\ell(\\mathcal{G})\\subset\\mathcal{I}(P), \\end{equation} which implies that $\\mathcal{G}$ is an I-map for $\\mathcal{I}(P)$, or simply an I-map for $P$.\nFactorization Let $\\mathcal{G}$ is a BN graph over $X_1,\\ldots,X_n\\in\\mathcal{X}$. A distribution $P$ over $\\mathcal{X}$ is said to factorize according to $\\mathcal{G}$ if $P$ can be expressed as a product \\begin{equation} P(X_1,\\ldots,X_n)=\\prod_{i=1}^{n}P(X_i\\vert\\text{Pa}_{X_i}) \\end{equation} This equation is known as the chain rule for Bayesian networks. Each individual factor $P(X_i\\vert\\text{Pa}_{X_i})$, which is a conditional probability distribution (CPD), is called the local probabilistic model.\nI-Map - Factorization Connection Theorem 1: Let $\\mathcal{G}$ be a BN graph over a set of random variables $\\mathcal{X}$ and let $P$ be a joint distribution over $\\mathcal{X}$. Then $\\mathcal{G}$ is an I-map for $P$ if and only if $P$ factorizes over $\\mathcal{G}$.\nProof\nI-map $\\Rightarrow$ Factorization\nWithout loss of generality, let $X_1,\\ldots,X_n$ be a topological ordering of the variables in $\\mathcal{X}$.\nLet us consider an arbitrary $X_i$ for $i\\in\\{1,\\ldots,n\\}$. As mentioning above, the topological ordering implies that \\begin{align} \\text{Pa}_{X_i}\u0026\\subset\\{X_1,\\ldots,X_{i-1}\\}, \\\\ \\text{Ch}_{X_i}\u0026\\subset\\{X_{i+1},\\ldots,X_n\\} \\end{align} Consequently, none of descendants of $X_i$ is in $\\{X_1,\\ldots,X_{n-1}\\}$. Thus, if we denote the set of all non-descendant nodes of $X_i$ as $\\text{NonDescentdants}_{X_i}$ and let $\\mathbf{Z}\\subset\\text{NonDescentdants}_{X_i}$, then \\begin{equation} \\mathbf{Z}\\cup\\text{Pa}_{X_i}=\\{X_1,\\ldots,X_{i-1}\\} \\end{equation} Moreover, the local independence for $X_i$ implies that \\begin{equation} (X_i\\perp\\mathbf{Z}\\vert\\text{ Pa}_{X_i}) \\end{equation} Therefore, since $\\mathcal{G}$ is an I-map for $P$ we obtain \\begin{equation} P(X_i\\vert X_1,\\ldots,X_{i-1})=P(X_i\\vert\\text{Pa}_{X_i}) \\end{equation} Thus, by the chain rule for probabilities, we have \\begin{equation} P(X_1,\\ldots,X_n)=\\prod_{i=1}^{n}P(X_i\\vert X_1,\\ldots,X_{i-1})=\\prod_{i=1}^{n}P(X_i\\vert\\text{Pa}_{X_i}) \\end{equation} I-map $\\Leftarrow$ Factorization\nTo prove that $\\mathcal{G}$ is an I-map according to $P$, we need to show that $\\mathcal{I}_\\ell(\\mathcal{G})$ holds in $P$. Consider an arbitrary node $X_i$ and the local independencies $(X_i\\perp\\text{NonDescendants}_{X_i}\\vert\\text{Pa}_{X_i})$, our problem remains to prove that \\begin{equation} P(X_i\\vert\\mathcal{X}\\backslash X_i)=P(X_i\\vert\\text{Pa}_{X_i}) \\end{equation} since \\begin{equation} P(X_i\\vert\\mathcal{X}\\backslash X_i)=P(X_i\\vert\\text{NonDescendants}_{X_i}\\cup\\text{Pa}_{X_i}) \\end{equation} By factorization, we have that \\begin{align} P(\\mathcal{X}\\backslash X_i)\u0026=\\sum_{X_i}P(X_1,\\ldots,X_n) \\\\ \u0026=\\sum_{X_i}\\prod_{j=1}^{n}P(X_j\\vert\\text{Pa}_{X_j}) \\\\ \u0026=\\left(\\prod_{j=1,j\\neq i}^{n}P(X_j\\vert\\text{Pa}_{X_j})\\right)\\sum_{X_i}P(X_i\\vert\\text{Pa}_{X_i}) \\\\ \u0026=\\prod_{j=1,j\\neq i}^{n}P(X_j\\vert\\text{Pa}_{X_j}), \\end{align} where in the last step, we use the fact that the conditional probability function $P(X_i\\vert\\text{Pa}_{X_i})$ sum to $1$ over the sample space of $X_i$. This implies that by Bayes rules \\begin{align} P(X_i\\vert\\mathcal{X}\\backslash X_i)\u0026=\\frac{P(X_1,\\ldots,X_n)}{P(\\mathcal{X}\\backslash X_i)} \\\\ \u0026=\\frac{\\prod_{j=1}^{n}P(X_j\\vert\\text{Pa}_{X_j})}{\\prod_{j=1,j\\neq i}^{n}P(X_j\\vert\\text{Pa}_{X_j})} \\\\ \u0026=P(X_i\\vert\\text{Pa}_{X_i}) \\end{align} Independencies in Bayesian Network D-separation Let $\\mathcal{G}$ be a BN structure, $X_1\\rightleftharpoons\\ldots\\rightleftharpoons X_n$ be a trail in $\\mathcal{G}$ and let $\\mathbf{Z}$ be a subset of observed variables. The trail $X_1\\rightleftharpoons\\ldots\\rightleftharpoons X_n$ is active if\nWhenever we have a v-structure $X_{i-1}\\rightarrow X_i\\leftarrow X_{i+1}$, $X_i$ or one of its descendants are in $\\mathbf{Z}$; No other node along the trail are in $\\mathbf{Z}$. Figure 2: (taken from the PGM book) The four possible two-edge trails from $X$ to $Y$ via $Z$: (a) Causal trail; (b) Evidential trail; (c) Common cause trail; (d) Common effect trail Consider the trails forming from two edges as illustrated above:\nThe trail $X\\rightarrow Z\\rightarrow Y$ is active $\\Leftrightarrow$ $Z$ is not observed. The trail $X\\leftarrow Z\\leftarrow Y$ is active $\\Leftrightarrow$ $Z$ is not observed. The trail $X\\leftarrow Z\\leftarrow Y$ is active $\\Leftrightarrow$ $Z$ is not observed. The trail $X\\rightarrow Z\\leftarrow Y$ is active $\\Leftrightarrow$ either $Z$ or one of its descendants is observed. Let $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ be sets of nodes in $\\mathcal{G}$. Then $\\mathbf{X}$ and $\\mathbf{Y}$ are said to be d-separated given $\\mathbf{Z}$, denoted $\\text{d-sep}_\\mathcal{G}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})$, if there is no active trail between any node $X\\in\\mathbf{X}$ and $Y\\in\\mathbf{Y}$ given $\\mathbf{Z}$.\nWe define the global Markov independencies associated with $\\mathcal{G}$, denoted $\\mathcal{I}(\\mathcal{G})$, to be the set of all independencies that correspond to d-separation in $\\mathcal{G}$ \\begin{equation} \\mathcal{I}(\\mathcal{G})=\\big\\{(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z}):\\text{d-sep}_\\mathcal{G}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})\\big\\} \\end{equation}\nSoundness, Completeness Theorem 2 (Soundness of d-separation) If a distribution $P$ factorizes according to $\\mathcal{G}$, then \\begin{equation} \\mathcal{I}(\\mathcal{G})\\subset\\mathcal{I}(P) \\end{equation} The soundness property says that if $\\text{d-sep}_\\mathcal{G}(X;Y\\vert\\mathbf{Z})$ then they are conditional independent given $\\mathbf{Z}$, or $(X\\perp Y\\vert\\mathbf{Z})$.\nTheorem 3 (Completeness of d-separation) If two variables $X$ and $Y$ are independent given $\\mathbf{Z}$, then they are d-separated.\nThe completeness property says that d-separation detects all possible independencies.\nTheorem 4: Let $\\mathcal{G}$ be a BN graph. If $X$ and $Y$ are not d-separated given $\\mathbf{Z}$, then $X$ and $Y$ are dependent given $\\mathbf{Z}$ in some distribution $P$ that factorizes over $\\mathcal{G}$.\nTheorem 5: For almost all distributions $P$ that factorize over $\\mathcal{G}$, i.e. for all distributions except for a set of measure zero in the space of CPD parameterizations, we have that \\begin{equation} \\mathcal{I}(\\mathcal{G})=\\mathcal{I}(P) \\end{equation} These results state that for almost all parameterizations $P$ of the graph $\\mathcal{G}$, the d-separation test precisely characterizes the independencies that hold for $P$.\nI-Equivalence Two graph $\\mathcal{K}_1$ and $\\mathcal{K}_2$ over $\\mathcal{X}$ are said to be I-equivalent if they encode the same set of conditional independencies assertions, i.e. \\begin{equation} \\mathcal{I}(\\mathcal{K}_1)=\\mathcal{I}(\\mathcal{K}_2) \\end{equation} This implies that any distribution $P$ that factorizes over $\\mathcal{K}_1$ also factorizes according to $\\mathcal{K}_2$ and vice versa.\nThe skeleton of a BN graph $\\mathcal{G}$ over $\\mathcal{X}$ is an undirected graph over $\\mathcal{X}$ containing an edge $\\{X,Y\\}$ for every edge $(X,Y)$ in $\\mathcal{G}$.\nTheorem 6 (skeleton + v-structures $\\Rightarrow$ I-equivalence) Let $\\mathcal{G}_1$ and $\\mathcal{G_2}$ be two graphs over $\\mathcal{X}$. If $\\mathcal{G}_1,\\mathcal{G}_2$ both have the same skeleton and the same set of v-structures then they are I-equivalent.2\nFigure 3: (taken from the PGM book) Two graphs have the same skeleton and set of v-structures, i.e. $\\{X\\rightarrow Y\\leftarrow Z\\}$, and thus are I-equivalent Immorality A v-structure $X\\rightarrow Z\\leftarrow Y$ is an immorality if there is no direct edge between. If there is such an edge, it is called a covering edge for the v-structure.\nIt is easily seen that not every v-structure is an immorality, which implies that two networks with the same set of immoralities do not necessarily have the same set of v-structures.\nTheorem 7 (skeleton + immoralities $\\Leftrightarrow$ I-equivalence) Let $\\mathcal{G}_1$ and $\\mathcal{G_2}$ be two graphs over $\\mathcal{X}$. If $\\mathcal{G}_1,\\mathcal{G}_2$ both have the same skeleton and the same set of immoralities iff they are I-equivalent.\nProof\nTo prove the theorem, we first introduce the notion of minimal active trail and triangle.\nDefinition (Minimal active trail) An active trail $X_1,\\ldots,X_m$ is minimal if there is no other active trail from $X_1$ to $X_m$ that shortcuts some of the nodes, i.e. there is no active trail \\begin{equation} X_1\\rightleftharpoons X_{i_1}\\rightleftharpoons\\ldots X_{i_k}\\rightleftharpoons X_m\\hspace{1cm}\\text{for }1\\lt i_1\\lt\\ldots\\lt i_k\\lt m \\end{equation} Definition (Triangle) Any three consecutive nodes in a trail $X_1,\\ldots,X_m$ are called a triangle if their skeleton is fully connected, i.e. forms a 3-clique.\nOur attention now is to prove that the only possible triangle in minimal active trail is the one having form of $X_{i-1}\\leftarrow X_i\\rightarrow X_{i+1}$ and either $X_{i-1}\\rightarrow X_{i+1}$ or $X_{i-1}\\leftarrow X_{i+1}$.\nConsider a two-edge trail from $X_{i-1}$ to $X_{i+1}$ via $X_i$, which as being mentioned above, has four possible forms $X_{i-1}\\rightarrow X_i\\rightarrow X_{i+1}$\nIt is easily seen that $X_i$ has to be not observed to make the trail active. If $X_{i-1}$ is connected to $X_{i+1}$ via $X_{i-1}\\rightarrow X_{i+1}$, this gives rise to a shortcut. On the other hand, if they are connected by $X_{i-1}\\leftarrow X_{i+1}$, the triangle now induces a cycle. $X_{i-1}\\leftarrow X_i\\leftarrow X_{i+1}$\nThis case is symmetrically identical to the previous one, and thus is not viable. $X_{i-1}\\leftarrow X_i\\rightarrow X_{i+1}$\nThe first observation is that $X_i$ has to be not given. The second observation is $X_{i-1}$ and $X_{i+1}$ are symmetric through $X_i$, so we only need to consider some specific cases of $X_{i-1}$ and the same logic is applied to $X_{i+1}$ analogously.\nLet us examine the two-edge trail $X_{i-2},X_{i-1},X_i$. On the one hand, if we have $X_{i-2}\\rightarrow X_{i-1}$, $X_{i-1}$ then has to be given, which implies that If $X_{i-1}\\leftarrow X_{i+1}$ exists, it will create a shortcut, which is not allowed. If $X_{i-1}\\rightarrow X_{i+1}$ exists, no shortcut appears, $X_{i-1},X_i,X_{i+1}$ satisfies the condition of a triangle in the minimal active trail $X_1,\\ldots,X_m$. On the other hand, if we have $X_{i-2}\\leftarrow X_{i-1}$, then $X_{i-1}$ is not observed, analogously, we instead have If $X_{i-1}\\leftarrow X_{i+1}$ exists, no shortcut is formed, $X_{i-1},X_i,X_{i+1}$ create a triangle. If $X_{i-1}\\rightarrow X_{i+1}$ exists, $X_{i-1},X_i,X_{i+1}$ is do not form a triangle due to the appearance of a shortcut through $X_{i-1}$ to $X_{i+1}$. $X_{i-1}\\rightarrow X_i\\leftarrow X_{i+1}$\nIn this case, $X_i$ or one of its descendant is observed. Using the similar procedure to previous case gives us no viable triangle formed by $X_{i-1},X_i,X_{i+1}$. Given these results, we are now ready for the main part. Let us begin with the forward path. Skeleton + Immoralities $\\Rightarrow$ I-equivalence\nAssume that there exists node $X,Y,Z$ such that \\begin{align} (X\\perp Y\\vert Z)\u0026\\in\\mathcal{I}(\\mathcal{G}_1), \\\\ (X\\perp Y\\vert Z)\u0026\\not\\in\\mathcal{I}(\\mathcal{G}_2), \\end{align} which implies that there is an active trail through $X,Y$ and $Z$ in the graph $\\mathcal{G}_2$. Let us consider the minimal one and continue by examining two cases that whether $Z$ is observed. If $Z$ is observed, in $\\mathcal{G}_1$, we have $X\\rightarrow Z\\rightarrow Y$, or $X\\leftarrow Z\\leftarrow Y$, or $X\\leftarrow Z\\rightarrow Y$, while we have $X\\rightarrow Z\\leftarrow Y$ in $\\mathcal{G}_2$, which is a v-structure. To assure that both graphs have the same set of moralities, there exist an edge that directly connects $X$ and $Y$, or in other words, $X,Y,Z$ form a triangle. This contradicts to the claim we have proved in the previous part. If $Z$ is not observed, thus in $\\mathcal{G}_1$, $X,Y,Z$ now must form a v-structure $X\\rightarrow Z\\leftarrow Y$. And also, to guarantee that both graphs have the same moralities, there exists an edge, without loss of generality, we assume $X\\rightarrow Y$. However, this edge will active the trail $X,Y,Z$, or in other words, in $\\mathcal{G}_1$, we now have $(X\\not\\perp Y\\vert Z)$, which is a contradiction of our assumption. Skeleton + Immoralities $\\Leftarrow$ I-equivalence\nConsider two I-equivalent graphs $\\mathcal{G}_1$ and $\\mathcal{G}_2$. First assuming that they do not have that same skeleton. This implies without loss of generality that there exists a trail in $\\mathcal{G}_1$ that does not appear in $\\mathcal{G}_2$, which induces a conditional independence in $\\mathcal{G}_1$ but not in $\\mathcal{G}_2$, contradicts to the fact that they two graphs are I-equivalent. Now assuming that two graphs do not have the same set of moralities. From Distributions to Graphs Given a distribution $P$, how can we represent the independencies of $P$ with a graph $\\mathcal{G}$?\nMinimal I-Maps A graph $\\mathcal{K}$ is a minimal I-map for a set of independencies $\\mathcal{I}$ if it is an I-map for $\\mathcal{I}$, and removing one edge from $\\mathcal{K}$ makes it no longer be an I-map.\nPerfect Maps A graph $\\mathcal{K}$ is a perfect map (or P-map) for a set of independencies $\\mathcal{I}$ if we have that $\\mathcal{I}(\\mathcal{K})=\\mathcal{I}$; and if $\\mathcal{I}(\\mathcal{K})=\\mathcal{I}(P)$, $\\mathcal{K}$ is said to be a perfect map for $P$.\nUndirected Graphical Model Similar to the directed case, each node in an undirected graphical model represents a random variable. However, as indicated from the name, each edge that connects two nodes is now undirected, and thus can not describe causal relationship between those nodes as in Bayesian network.\nMarkov Random Fields An Undirected Graphical Model (or Markov Random Field, or Markov network) represents a probability distribution $P$ over variables $X_1,\\ldots,X_n$ defined by an undirected graph $\\mathcal{H}$ in which each node correspond to a variable $X_i$, and a set of positive potential functions $\\psi_c$ associated with the cliques of $\\mathcal{H}$ such that \\begin{equation} P(X_1,\\ldots,X_n)=\\frac{1}{Z}\\prod_{c\\in C}\\psi_c(\\mathbf{X}_c),\\label{eq:mrf.1} \\end{equation} where\n$P$ is also called a Gibbs distribution $C$ denotes the set of cliques of $\\mathcal{H}$ $\\mathbf{X}_c$ represents the set of nodes within clique $c$ $Z$ is known as the partition function, given by \\begin{equation} Z=\\sum_{X_1,\\ldots,X_n}\\prod_{c\\in C}\\psi_c(\\mathbf{X}_c) \\end{equation} From the above definition, we say that a distribution $P$ is a Gibbs distribution that factorizes over $\\mathcal{H}$ if it can be expressed as a normalized product over all potential functions defined on the cliques of $\\mathcal{H}$, as in \\eqref{eq:mrf.1}.\nIndependencies in Markov Network Analogy to Bayesian network, the graph structure in a Markov Random Field can be viewed as encoding a set of independence assumptions, which can be specified by considering the undirected paths through nodes.\nSeparation Let $\\mathcal{H}$ be a Markov network structure and let $X_1-\\ldots-X_k$ be a path in $\\mathcal{H}=(\\mathcal{X},\\mathcal{E})$. Let $\\mathbf{Z}\\subset\\mathcal{X}$ be a set of observed variables. Then $X_1-\\ldots-X_k$ is active given $\\mathbf{Z}$ if none of $X_1,\\ldots,X_k$ is in $\\mathbf{Z}$.\nLet $\\mathbf{X},\\mathbf{Y}$ be set of nodes in $\\mathcal{H}$. We say that $\\mathbf{Z}$ separates $\\mathbf{X}$ and $\\mathbf{Y}$, denoted $\\text{sep}_\\mathcal{H}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})$ if there is no active path between any node $X\\in\\mathbf{X}$ and $Y\\in\\mathbf{Y}$ given $\\mathbf{Z}$.\nGlobal Markov Independencies As in the case of Bayesian network, we define the global Markov independencies associated with $\\mathcal{H}$, denoted $\\mathcal{I}(\\mathcal{H})$, to be the set of all independencies correspond to separation in $\\mathcal{H}$ \\begin{equation} \\mathcal{I}(\\mathcal{H})=\\big\\{(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z}):\\text{sep}_\\mathcal{H}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})\\big\\} \\end{equation}\nLocal Markov Independencies Let $\\mathcal{H}$ be a Markov network. We define the pairwise independencies associated with $\\mathcal{H}$ to be \\begin{equation} \\mathcal{I}_p(\\mathcal{H})=\\big\\{(X\\perp Y\\vert\\mathcal{X}\\backslash\\{X,Y\\}):X-Y\\notin\\mathcal{H}\\big\\} \\end{equation} Or in other words, the pairwise independencies states that $X$ and $Y$ are independent given all other nodes in $\\mathcal{H}$.\nFor a given graph $\\mathcal{H}$ and for an arbitrary node $X$ of $\\mathcal{H}$, the set of neighbors of $X$ is also called the Markov blanket of $X$, denoted $\\text{MB}_\\mathcal{H}(X)$. With this notion, we define the local independencies, or Markov local independencies, associated with $\\mathcal{H}$ to be \\begin{equation} \\mathcal{I}_\\ell(\\mathcal{H})=\\big\\{\\big(X\\perp\\mathcal{X}\\backslash(\\{X\\}\\cup\\text{MB}_\\mathcal{H}(X))\\vert\\text{MB}_\\mathcal{H}(X)\\big):X\\in\\mathcal{X}\\big\\} \\end{equation} Or in other words, the Markov local independencies says that $X$ is independent of the rest of the nodes in $\\mathcal{H}$ given its neighbors.\nThe definition of Markov blanket can also be rewritten using independencies assertions:\nA set $\\mathbf{U}$ is a Markov blanket of $X$ in a distribution $P$ if $X\\notin\\mathbf{U}$ and if $\\mathbf{U}$ is a minimal set of nodes such that \\begin{equation} \\big(X\\perp\\mathcal{X}\\backslash(\\{X\\}\\cup\\mathbf{U})\\vert\\mathbf{U}\\big)\\in\\mathcal{I}(P) \\end{equation}\nMarkov Independencies Relationships Theorem 8: Let $\\mathcal{H}$ be a Markov network and $P$ be a positive distribution. The following three statement are then equivalent:\n$P\\models\\mathcal{I}_\\ell(\\mathcal{H})$. $P\\models\\mathcal{I}_p(\\mathcal{H})$. $P\\models\\mathcal{I}(\\mathcal{H})$. Proof\n(i) $\\Rightarrow$ (ii)\nConsider an arbitrary node $X$ in $\\mathcal{H}$. Let $Y\\in\\mathcal{X}$ such that $X-Y\\notin\\mathcal{H}$, then $Y\\notin\\text{MB}_\\mathcal{H}(X)$, or in other words \\begin{equation} Y\\in\\mathcal{X}\\backslash(\\{X\\}\\cup\\text{MB}_\\mathcal{H}(X)) \\end{equation} Moreover, since $P\\models\\mathcal{I}_\\ell(\\mathcal{H})$, we have that $P$ satisfies \\begin{equation} \\big(X\\perp\\mathcal{X}\\backslash(\\{X\\}\\cup\\text{MB}_\\mathcal{H}(X))\\vert\\text{MB}_\\mathcal{H}(X)\\big), \\end{equation} which implies that \\begin{equation} \\big(X\\perp Y\\vert\\text{MB}_\\mathcal{X}\\cup\\mathcal{X}\\backslash(\\{X,Y\\}\\cup\\text{MB}_\\mathcal{H}(X))\\big) \\end{equation} holds for $P$. Or in other words, for any $X,Y$ such that $X-Y\\notin\\mathcal{H}$, we have \\begin{equation} P\\models(X\\perp Y\\vert\\mathcal{X}\\backslash\\{X,Y\\}), \\end{equation} which proves our claim. (ii) $\\Rightarrow$ (iii) (iii) $\\Rightarrow$ (i)\nThis follows directly from the fact that if two nodes $X$ and $Y$ are not connected, then they are necessarily separated by all remaining nodes of the graph. Soundness, Completeness Theorem 9 (Soundness of separation) Let $\\mathcal{H}=(\\mathcal{X},\\mathcal{E})$ be a Markov network structure and let $P$ be a distribution over $\\mathcal{X}$. If $P$ is a Gibbs distribution that factorizes over $\\mathcal{H}$, then $\\mathcal{H}$ is an I-map for $P$.\nProof\nLet $\\mathbf{X},\\mathbf{Y}$ and $\\mathbf{Z}$ be any disjoint subsets of $\\mathcal{X}$ such that $\\text{sep}_\\mathcal{H}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})$. We need to show that \\begin{equation} P\\models(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z}) \\end{equation} We begin by considering the case that $\\mathbf{X}\\cup\\mathbf{Y}\\cup\\mathbf{Z}=\\mathcal{X}$. Since $\\mathbf{Z}$ separates $\\mathbf{X}$ and $\\mathbf{Y}$, then for any $X\\in\\mathbf{X}$ and for any $Y\\in\\mathbf{Y}$, there is no direct edge between $X,Y$. This implies that any clique in $\\mathcal{H}$ is either in $\\mathbf{X}\\cup\\mathbf{Z}$ or in $\\mathbf{Y}\\cup\\mathbf{Z}$.\nLet $C_\\mathbf{X}$ denote the set of cliques within $\\mathbf{X}\\cup\\mathbf{Z}$ and let $C_\\mathbf{Y}$ represent the set of cliques in $\\mathbf{Y}\\cup\\mathbf{Z}$. Thus, as $P$ factorizes over $\\mathcal{H}$, we have that \\begin{align} P(X_1,\\ldots,X_n)\u0026amp;=\\frac{1}{Z}\\left(\\prod_{c\\in C_\\mathbf{X}}\\psi(X_c)\\right)\\left(\\prod_{c\\in C_\\mathbf{Y}}\\psi(X_c)\\right) \\\\ \u0026amp;=\\frac{1}{Z}\\psi_\\mathbf{X}(\\mathbf{X},\\mathbf{Z})\\psi_\\mathbf{Y}(\\mathbf{Y},\\mathbf{Z}), \\end{align} where $\\psi_\\mathbf{X},\\psi_\\mathbf{Y}$ are some factor with scopes $\\mathbf{X}\\cup\\mathbf{Z}$ and $\\mathbf{Y}\\cup\\mathbf{Z}$ respectively. Hence, it follows that \\begin{equation} P\\models(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z}) \\end{equation} Now consider the case that $\\mathbf{X}\\cup\\mathbf{Y}\\cup\\mathbf{Z}\\subset\\mathcal{X}$. Let $\\mathbf{U}=\\mathcal{X}\\backslash(\\mathbf{X}\\cup\\mathbf{Y}\\cup\\mathbf{Z})$. We can divide $\\mathbf{U}$ into two disjoint sets $\\mathbf{U}_1$ and $\\mathbf{U}_2$ such that \\begin{equation} \\text{sep}_\\mathcal{H}(\\mathbf{X}\\cup\\mathbf{U}_1;\\mathbf{Y}\\cup\\mathbf{U}_2\\vert\\mathbf{Z}) \\end{equation} And since $(\\mathbf{X}\\cup\\mathbf{U}_1)\\cup(\\mathbf{Y}\\cup\\mathbf{U}_2)\\cup\\mathbf{Z}=\\mathcal{X}$, we can follow the previous procedure to conclude that \\begin{equation} P\\models(\\mathbf{X}\\cup\\mathbf{U}_1\\perp\\mathbf{Y}\\cup\\mathbf{U}_2\\vert\\mathbf{Z}), \\end{equation} which implies that \\begin{equation} P\\models(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z}) \\end{equation}\nTheorem 10 (Hammersley-Clifford) Let $\\mathcal{H}=(\\mathcal{X},\\mathcal{E})$ be a Markov network structure and let $P$ be a positive distribution over $\\mathcal{X}$. If $\\mathcal{H}$ is an I-map for $P$, then $P$ is a Gibbs distribution that factorizes over $\\mathcal{H}$.\nCorollary 11: The positive distribution $P$ factorizes a Markov network $\\mathcal{H}$ iff $\\mathcal{H}$ is an I-map for $P$.\nTheorem 12 (Completeness of separation) Let $\\mathcal{H}$ be a Markov network structure. If $X$ and $Y$ are not separated given $\\mathbf{Z}$ in $\\mathcal{H}$, then $X$ and $Y$ are dependent given $\\mathbf{Z}$ in some distribution $P$ that factorizes over $\\mathcal{H}$.\nFrom Distributions to Graphs As mentioned above, the notion of minimal I-map lets us encode the independencies of a given distribution $P$ using a graph structure.\nIn particular, for a distribution $P$, we can construct the minimal I-map based on either the pairwise independencies or the local independencies.\nTheorem 13: Let $P$ be a positive distribution and $\\mathcal{H}$ be a Markov network defined by including an edge $X-Y$ for all $X,Y$ such that $P\\not\\models(X\\perp Y\\vert\\mathcal{X}\\backslash\\{X,Y\\}) $. Then $\\mathcal{H}$ is the unique minimal I-map for $P$.\nTheorem 14: Let P be a positive distribution and let $\\mathcal{H}$ be a Markov network defined by including an edge $X-Y$ for all $X$ and all $Y\\in\\text{MB}_\\mathcal{H}(X)$. Then $\\mathcal{H}$ is the unique minimal I-map for $P$.\nRemark: Not every distribution has a perfect map as UGM (proof by contradiction).\nFactor Graphs A factor graph $\\mathcal{F}$ is an undirected graph whose nodes are divided into two groups: variable nodes (denoted as ovals) and factor nodes (denoted as squares) and whose edges only connect each factor (potential function) $\\psi_c$ to its dependent nodes $X\\in X_c$.\nFigure 4: (based on figure from the PGM book) Different factor graphs for the same Markov network (a) A Markov network consists of nodes $X_1,X_2,X_3$; (b) A factor graph with a factor $\\psi_{1,2,3}$ connected to each $X_1,X_2,X_3$; (c) A factor graph with three pairwise factors $\\psi_{1,2}$ (connected to $X_1,X_2$), $\\psi_{1,3}$ (connected to $X_1,X_3$) and $\\psi_{2,3}$ (connected to $X_2,X_3$) Log-Linear Models A distribution $P$ is a log-linear model over a Markov network $\\mathcal{H}$ if it is associated with\na set of features $\\mathcal{F}=\\{\\phi_1(\\mathbf{X}_1),\\ldots,\\phi_k(\\mathbf{X}_k)\\}$ where each $\\mathbf{X}_i$ is a complete subgraph in $\\mathcal{H}$, a set of weight $w_1,\\ldots,w_k$, such that \\begin{equation} P(X_1,\\ldots,X_n)=\\frac{1}{Z}\\exp\\left[-\\sum_{i=1}^{k}w_i\\phi_i(\\mathbf{X}_i)\\right] \\end{equation} The function $\\phi_i$ are called energy functions.\nConditional Random Fields A conditional random field, or CRF, is an undirected graph $\\mathcal{H}$ whose nodes correspond to $\\mathbf{X}\\cup\\mathbf{Y}$ where $\\mathbf{X}$ is a set of observed variables and $\\mathbf{Y}$ is a (disjoint) set of target variables which specifies a conditional distribution (instead of a joint distribution) \\begin{equation} P(\\mathbf{Y}\\vert\\mathbf{X})=\\frac{1}{Z(\\mathbf{X})}\\prod_{c\\in C}\\psi_c(\\mathbf{X}_c,\\mathbf{Y}_c), \\end{equation} where the partition function $Z(\\mathbf{X})$ now depends on $\\mathbf{X}$ (rather than being a constant) \\begin{equation} Z(\\mathbf{X})=\\sum_\\mathbf{Y}\\prod_{c\\in C}\\psi_c(\\mathbf{X}_c,\\mathbf{Y}_c) \\end{equation}\nExample - Naive Markov Consider a CRF over the binary-value variables $\\mathbf{X}=\\{X_1,\\ldots,X_k\\}$ and $\\mathbf{Y}=\\{Y\\}$, and a pairwise potential between $Y$ and each $X_i$. The model is referred as a naive Markov model. Assume that the pairwise potentials defined via the log-linear model \\begin{equation} \\psi_i(X_i,Y)=\\exp\\big[w_i\\mathbf{1}\\{X_i=1,Y=1\\}\\big] \\end{equation} Additionally, we also have a single-node potential \\begin{equation} \\psi_0(Y)=\\exp\\big[w_0\\mathbf{1}\\{Y=1\\}\\big] \\end{equation} We then have that \\begin{equation} P(Y=1\\vert x_1,\\ldots,x_k)=\\frac{1}{1+\\exp\\left[-\\left(w_0+\\sum_{i=1}^{k}w_i x_i\\right)\\right]} \\end{equation}\nLocal Probabilistic Models Tabular CPDs For a space of discrete-valued random variables only, we can encode the CPDs $P(X\\vert\\text{Pa}_X)$ as a table where each entry corresponds to a pair of $X,\\text{Pa}_X$.\nIt is easily seen that this representation raises a disadvantage that the number of parameters required grows exponentially in the number of parents. Also, it is impossible to store the conditional probability corresponding to a continuous-valued random variables.\nTo avoid these problems, instead of viewing CPDs as tables listing all of the conditional probabilities $P(x\\vert\\text{pa}_X)$, we should consider them as functions that given $\\text{pa}_X$ and $x$, return the conditional probability $P(x\\vert\\text{pa}_X)$.\nDeterministic CPDs The simplest type of non-tabular CPD corresponds to a variable $X$ being a deterministic function of its parents $\\text{Pa}_X$. It means, there exists $f:\\text{Val}(Pa_X)\\mapsto\\text{Val}(X)$ such that \\begin{equation} P(x\\vert\\text{pa}_X)=\\begin{cases}1\u0026amp;\\hspace{1cm}x=f(\\text{pa}_X) \\\\ 0\u0026amp;\\hspace{1cm}\\text{otherwise}\\end{cases} \\end{equation} Deterministic variables are denoted as double-line ovals, as illustrated in the following example\nFigure 5: (taken from the PGM book) A network with $C$ being a deterministic function of $A$ and $B$ Consider the above figure, as $C$ being a deterministic function of $A$ and $B$, we can deduce that $C$ is fully observed if $A$ and $B$ are both observed. In other words, we have that \\begin{equation} (D\\perp E\\vert A,B) \\end{equation}\nTheorem 15: Let $\\mathcal{G}$ be a network structure, and let $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ be sets of variables, $\\mathbf{D}$ be set of deterministic variables. If $\\mathbf{X}$ is deterministically separated from $\\mathbf{Y}$ given $\\mathbf{Z}$3, then for all distributions $P$ such that $P\\models\\mathcal{I}_\\ell(\\mathcal{G})$ and where, for each $X\\in\\mathbf{D}$, $P(X\\vert\\text{Pa}_X)$ is a deterministic CPD, we have that $P\\models(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z})$.\nTheorem 16: Let $\\mathcal{G}$ be a network structure, and let $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ be sets of variables, $\\mathbf{D}$ be set of deterministic variables. If $\\mathbf{X}$ is not deterministically separated from $\\mathbf{Y}$ given $\\mathbf{Z}$, then there exists a distribution $P$ such that $P\\models\\mathcal{I}_\\ell(\\mathcal{G})$ and where, for each $X\\in\\mathbf{D}$, $P(X\\vert\\text{Pa}_X)$ is a deterministic CPD, but we instead have $P\\not\\models(\\mathbf{X}\\perp\\mathbf{Y}\\vert\\mathbf{Z})$.\nIt is worth remarking that particular deterministic CPD might imply additional independencies. For instance, let us consider the following examples\nExample 1: Consider the following Bayesian network\nFigure 6: (taken from the PGM book) Another Bayesian network with $C$ being a deterministic function of $A$ and $B$ In the above figure, if $C=A\\text{ XOR }B$, we have that $A$ is fully determined given $C$ and $B$. In other words, we have that \\begin{equation} (D\\perp E\\vert B,C) \\end{equation} Example 2: Consider the network given in Figure 5, with $C=A\\text{ OR }B$. Assume that we are given $A=a^1$, it is then immediately that $C=c^1$ without taking into account the value of $B$. Or in other words, we have that \\begin{equation} P(D\\vert B,a^1)=P(D\\vert a^1) \\end{equation} On the other hand, given $A=a^0$, the value of $C$ is not fully determined, and still depend the value of $B$.\nContext-Specific Independence Let $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ be pairwise disjoint sets of variables, let $\\mathbf{C}$ be a set of variable (which might overlap with $\\mathbf{X}\\cup\\mathbf{Y}\\cup\\mathbf{Z}$), and let $\\mathbf{c}\\in\\text{Val}(\\mathbf{C})$. We say that $X$ and $\\mathbf{Y}$ are contextually independent given $\\mathbf{Z}$ and the context $\\mathbf{C}$ denoted $(\\mathbf{X}\\perp_c\\mathbf{Y}\\vert\\mathbf{Z},\\mathbf{c})$ if \\begin{equation} P(\\mathbf{Y},\\mathbf{Z},\\mathbf{c})\u0026gt;0\\Rightarrow P(\\mathbf{X}\\vert\\mathbf{Y},\\mathbf{Z},\\mathbf{c})=P(\\mathbf{X}\\vert\\mathbf{Z},\\mathbf{c}) \\end{equation} Independence statements of this form is known as the context-specific independencies (CSI).\nGiven this definition, let us examine some examples.\nExample 3: Given the Bayesian network in Figure 5 with $C$ being a deterministic function $\\text{OR}$ of $A$ and $B$. By properties of $\\text{OR}$ function, we can conclude some independence assertions \\begin{align} \u0026amp;(C\\perp_c B\\hspace{0.1cm}\\vert\\hspace{0.1cm}a^1), \\\\ \u0026amp;(D\\perp_c B\\hspace{0.1cm}\\vert\\hspace{0.1cm}a^1), \\\\ \u0026amp;(A\\perp_c B\\hspace{0.1cm}\\vert\\hspace{0.1cm}c^0), \\\\ \u0026amp;(D\\perp_c E\\hspace{0.1cm}\\vert\\hspace{0.1cm}c^0), \\\\ \u0026amp;(D\\perp_c E\\hspace{0.1cm}\\vert\\hspace{0.1cm}b^0,c^1) \\end{align} Example 4: Given the Bayesian network in Figure 6 with $C$ being the exclusive or of $A$ and $B$. We can also conclude some independence assertions using properties of $\\text{XOR}$ function \\begin{align} \u0026amp;(D\\perp_c E\\hspace{0.1cm}\\vert\\hspace{0.1cm}b^1,c^0), \\\\ \u0026amp;(D\\perp_c E\\vert\\hspace{0.1cm}b^0,c^1) \\end{align}\nContext-specific CPDs Tree-CPDs A tree-CPD representing a CPD for variable $X$ is a rooted tree, where:\neach leaf node is labeled with a distribution $P(X)$; each internal node is labeled with some variable $Z\\in\\text{Pa}_X$; each edge from an internal node, which is labeled as some $Z$, to its child nodes corresponds to a $z_i\\in\\text{Val}(Z)$. Figure 7: (taken from the PGM book) A tree-CPD for $P(J\\vert A,S,L)$ The structure is common in cases where a variable can depend on a set of r.v.s but we have uncertainty about which r.v.s it depends on. For example, in the above tree-CDP representing $P(J\\vert A,S,L)$, we have that \\begin{align} \u0026amp;(J\\perp_c L,S\\vert\\hspace{0.1cm}a^0), \\\\ \u0026amp;(J\\perp_c L\\vert\\hspace{0.1cm}a^1,s^1), \\\\ \u0026amp;(J\\perp_c L\\vert\\hspace{0.1cm}s^1) \\end{align}\nMultiplexer CPD A CPD $P(Y\\vert A,Z_1,\\ldots,Z_k)$ is said to be a multiplexer CPD if $\\text{Val}(A)=\\{1,\\ldots,k\\}$ and \\begin{equation} P(Y\\vert a,Z_1,\\ldots,Z_k)=\\mathbf{1}\\{Y=Z_a\\}, \\end{equation} where $a$ is the value of $A$. The variable $A$ is referred as the selector variable for the CPD.\nFigure 8: (based on figure from the PGM book) (a) A Bayesian network for $P(J,C,L_1,L_2)$; (b) Tree-CPD for $P(J\\vert C,L_1,L_2)$; (c) Modified network with additional variable $L$ acting as a multiplexer CPD Rule CPDs A rule $\\rho$ is a pair $(\\mathbf{c},p)$ where $\\mathbf{c}$ is an assignment to some subset of variables $\\mathbf{C}$ and $p\\in[0,1]$. $\\mathbf{C}$ is then referred as the scope of $\\rho$, denoted $\\mathbf{C}=\\text{Scope}(\\rho)$.\nThis representation decomposes a tree-CPD into its most basic elements.\nExample 5: Consider the tree-CPD given in Figure 7. The tree defines eight rules \\begin{Bmatrix}(a^0,j^0;0.8), \\\\ (a^0,j^1;0.2), \\\\ (a^1,s^0,l^0,j^0;0.9), \\\\ (a^1,s^0,l^0,j^1;0.1), \\\\ (a^1,s^0,l^1,j^0;0.4), \\\\ (a^1,s^0,l^1,j^1;0.6), \\\\ (a^1,s^1,j^0;0.1), \\\\ (a^1,s^1,j^1;0.9)\\end{Bmatrix} It is necessary that each conditional distribution $P(X\\vert\\text{Pa}_X)$ is specified by exactly one rule. Or in other words, the rules in a tree-CPD must be mutually exclusive and exhaustive.\nRule-based CPD A rule-based CPD $P(X\\vert\\text{Pa}_X)$ is a set of rules $\\mathcal{R}$ such that\nFor each $\\rho\\in\\mathcal{R}$, we have that \\begin{equation} \\text{Scope}(\\rho)\\subset\\{X\\}\\cup\\text{Pa}_X \\end{equation} For each assignment $(x,\\mathbf{u})$ to $\\{X\\}\\cup\\text{Pa}_X$, we have exactly one rule $(\\mathbf{c};p)\\in\\mathcal{R}$ such that $\\mathbf{c}$ is compatible with $(x,\\mathbf{u})$. And we say that \\begin{equation} P(X=x\\vert\\text{Pa}_X=\\mathbf{u})=p \\end{equation} $\\sum_x P(x\\vert\\mathbf{u})=1$. Example 6: Let $X$ be a variable with $\\text{Pa}_X=\\{A,B,C\\}$ with $X$\u0026rsquo;s CPD is defined by sets of rules \\begin{Bmatrix}\\rho_1:(a^1,b^1,x^0;0.1), \\\\ \\rho_2:(a^1,b^1,x^1;0.9), \\\\ \\rho_3:(a^0,c^1,x^0;0.2), \\\\ \\rho_4:(a^0,c^1,x^1;0.8), \\\\ \\rho_5:(b^0,c^0,x^0;0.3), \\\\ \\rho_6:(b^0,c^0,x^1;0.7), \\\\ \\rho_7:(a^1,b^0,c^1,x^0;0.4), \\\\ \\rho_8:(a^1,b^0,c^1,x^1;0.6), \\\\ \\rho_9:(a^0,b^1,c^0;0.5)\\end{Bmatrix} The tree-CPD corresponds to the above rule-based CPD $P(X\\vert A,B,C)$ is given as:\nIt is worth noticing that both CPD entries $P(x^0\\vert a^0,b^1,c^0)$ and $P(x^1\\vert a^0,b^1,c^0)$ are determined by rule $\\rho_9$ only. This kind of rule only works for uniform distribution. Independencies in Context-specific CPDs If $\\mathbf{c}$ be a context associated with a branch in the tree-CPD for $X$, then $X$ is independent of the remaining parents, $\\text{Pa}_X\\backslash\\text{Scope}(\\mathbf{c})$, given $\\mathbf{c}$. Moreover, there might exist CSI statements conditioned on contexts which are not induced by complete branches.\nExample 7: Consider the tree-CPD given in Figure 7, as mentioned above, we have that \\begin{equation} (J\\perp_c L\\hspace{0.1cm}\\vert\\hspace{0.1cm}s^1), \\end{equation} where $s^1$ is not the full assignment associated with a branch.\nAlso, consider the tree-CPD given in Figure 8(b), we have that \\begin{align} \u0026amp;(J\\perp_c L_2\\hspace{0.1cm}\\vert\\hspace{0.1cm}c^1), \\\\ \u0026amp;(J\\perp_c L_1\\hspace{0.1cm}\\vert\\hspace{0.1cm}c^2), \\end{align} where neither $c^1$ nor $c^2$ is the full assignment associated with a branch.\nReduced Rule Let $\\rho=(\\mathbf{c}\u0026rsquo;;p)$ be a rule and $\\mathbf{c}$ be a context. If $\\mathbf{c}\u0026rsquo;$ is compatible with $\\mathbf{c}$, we say that $\\rho\\sim\\mathbf{c}$.\nIn this case, let $\\mathbf{c}\u0026rsquo;\u0026rsquo;$ be the assignment in $c\u0026rsquo;$ to the variables in $\\text{Scope}(\\mathbf{c}\u0026rsquo;)\\backslash\\text{Scope}(\\mathbf{c})$. We then define the reduced rule $\\rho[\\mathbf{c}]=(\\mathbf{c}\u0026rsquo;\u0026rsquo;;p)$. If $\\mathcal{R}$ be a set of rules, we define the reduced rule set \\begin{equation} \\mathcal{R}[\\mathbf{c}]=\\{\\rho[\\mathbf{c}]:\\rho\\in\\mathcal{R},\\rho\\sim\\mathbf{c}\\} \\end{equation}\nExample 8: Consider the rule set $\\mathcal{R}$ given in Example 6, we have that the reduced set corresponding to context $a^1$ is \\begin{equation} \\mathcal{R}[a^1]=\\begin{Bmatrix}\\rho_1\u0026rsquo;:(b^1,x^0;0.1), \\\\ \\rho_2:(b^1,x^1;0.9), \\\\ \\rho_5:(b^0,c^0,x^0;0.3), \\\\ \\rho_6:(b^0,c^0,x^1;0.7), \\\\ \\rho_7:(b^0,c^1,x^0;0.4), \\\\ \\rho_8\u0026rsquo;:(b^0,c^1,x^1;0.6),\\end{Bmatrix} \\end{equation} which is obtained by selecting rules compatible with $a^1$, i.e. $\\{\\rho_1,\\rho_2,\\rho_5,\\rho_6,\\rho_7,\\rho_8\\}$, then canceling out $a^1$ from all the rules where it appeared.\nProposition 17: Let $\\mathcal{R}$ be the rules in the rule-based CPD for a variable $X$, and let $\\mathcal{R}_\\mathbf{c}$ be the rules in $\\mathcal{R}$ compatible with $\\mathbf{c}$. Let $\\mathbf{Y}\\subset\\text{Pa}_X$ be a subset such that $\\mathbf{Y}\\cap\\text{Scope}(\\mathbf{c})=\\emptyset$. If for all $\\rho\\in\\mathcal{R}[\\mathbf{c}]$, we have that $\\mathbf{Y}\\cap\\text{Scope}(\\rho)=\\emptyset$, then \\begin{equation} (X\\perp_c\\mathbf{Y}\\hspace{0.1cm}\\vert\\hspace{0.1cm}\\text{Pa}_X\\backslash\\mathbf{Y},\\mathbf{c}) \\end{equation}\nSpurious Edge Let $P(X\\vert\\text{Pa}_X)$ be a CPD, $Y\\in\\text{Pa}_X$ be a set and let $\\mathbf{c}$ be a context. The edge $Y\\rightarrow X$ is said to be spurious in the context $\\mathbf{c}$ if $P(X\\vert\\text{Pa}_X)$ satisfies $(X\\perp_c Y\\hspace{0.1cm}\\vert\\hspace{0.1cm}\\text{Pa}_X\\backslash\\{Y\\},\\mathbf{c})$, where $\\mathbf{c}\u0026rsquo;$ is the restriction of $\\mathbf{c}$ to variables in $\\text{Pa}_X$.\nHence, by examining the reduced rule set, we can specify whether an edge is spurious, i.e. if $\\mathcal{R}$ be the rule-based CPD for $P(X\\vert\\text{Pa}_X)$, then $Y\\rightarrow X$ is spurious in context $\\mathbf{c}$ if $Y$ does not appear in $\\mathcal{R}[\\mathbf{c}]$.\nTheorem 18: Let $\\mathcal{G}$ be a network structure, $P$ be a distribution such that $P\\models\\mathcal{I}_\\ell(\\mathcal{G})$, $\\mathbf{c}$ be a context, and $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ be sets of variables. If $\\mathbf{X}$ is CSI-separated from $\\mathbf{Y}$ given $\\mathbf{Z}$ in the context $\\mathbf{c}$4, then we have that $P\\models(\\mathbf{X}\\perp_c\\mathbf{Y}\\hspace{0.1cm}\\vert\\hspace{0.1cm}\\mathbf{Z},\\mathbf{c})$.\nIndependence of Causal Influence Noisy-Or Model Let $Y$ be a binary-valued r.v with $k$ binary-valued parents $X_1,\\ldots,X_k$. The CPD $P(Y\\vert X_1,\\ldots,X_k)$ is a noisy-or if there are $k+1$ noise parameters $\\lambda_0,\\lambda_1,\\ldots,\\lambda_k$ such that \\begin{align} P(y^0\\vert X_1,\\ldots,X_k)\u0026amp;=(1-\\lambda_0)\\prod_{i:X_i=x_i^1}(1-\\lambda_i)\\label{eq:nom.1} \\\\ P(y^1\\vert X_1,\\ldots,X_k)\u0026amp;=1-(1-\\lambda_0)\\prod_{i:X_i=x_i^1}(1-\\lambda_i) \\end{align} If we interpret $x_i^0$ as $0$ and $x_i^1$ as $1$, \\eqref{eq:nom.1} can be rewritten as \\begin{equation} P(y^0\\vert X_1,\\ldots,X_k)=(1-\\lambda_0)\\prod_{i=1}^{k}(1-\\lambda_i)^{x_i} \\end{equation}\nGeneralized Linear Models Binary-valued Variables Multivalued Variables Continuous Variables Conditional Bayesian Networks A conditional Bayesian network $\\mathcal{B}$ over $\\mathbf{Y}$ given $\\mathbf{X}$ is defined as a DAG $\\mathcal{G}$ whose nodes are $\\mathbf{X}\\cup\\mathbf{Y}\\cup\\mathbf{Z}$ where $\\mathbf{X},\\mathbf{Y},\\mathbf{Z}$ are disjoint. The variables in $\\mathbf{X}$ are called inputs, the ones in $\\mathbf{Y}$ are referred as outputs and the others in $\\mathbf{Z}$ are known as encapsulated.\nThe variables in $\\mathbf{X}$ have no parents in $\\mathcal{G}$, while the variables in $\\mathbf{Y}\\cup\\mathbf{Z}$ are associated with a CPD. The network defines a CPD using chain rule \\begin{equation} P_\\mathcal{B}(\\mathbf{Y},\\mathbf{Z}\\vert\\mathbf{X})=\\prod_{T\\in\\mathbf{Y}\\cup\\mathbf{Z}}P(T\\vert\\text{Pa}_T) \\end{equation} The distribution $P_\\mathcal{B}(\\mathbf{Y}\\vert\\mathbf{X})$ is defined as the marginal of $P_\\mathcal{B}(\\mathbf{Y},\\mathbf{Z}\\vert\\mathbf{X})$ \\begin{equation} P_\\mathcal{B}(\\mathbf{Y}\\vert\\mathbf{X})=\\sum_\\mathbf{Z}P_\\mathcal{B}(\\mathbf{Y},\\mathbf{Z}\\vert\\mathbf{X}) \\end{equation} The conditional Bayesian network is the directed version of CRF mentioned above.\nEncapsulated CPD Let $Y$ be a r.v with $k$ parents $X_1,\\ldots,X_k$. The CPD $P(Y\\vert X_1,\\ldots,X_k)$ is an encapsulated CPD if it is represented using a conditional Bayesian network over $Y$ given $X_1,\\ldots,X_k$.\nTemplate-based Representations Temporal Models In a temporal model, for each $X_i\\in\\mathcal{X}$, we let $X_i^{(t)}$ denote its instantiation at time $t$. The variables $X_i$ are referred as template variables.\nConsider a distribution over trajectories sampled over time $t=0,1,\\ldots,T$ - $P(\\mathcal{X}^{(0)},\\mathcal{X}^{(1)},\\ldots,\\mathcal{X}^{(T)})$, or $P(\\mathcal{X}^{(0:T)})$ where $\\mathcal{X}^{(t)}=\\{X_i^{(t)}\\}$. Using the chain rule for probabilities, we have that \\begin{equation} P(\\mathcal{X}^{(0:T)})=P(\\mathcal{X}^{(0)},\\mathcal{X}^{(1)},\\ldots,\\mathcal{X}^{(T)})=P(\\mathcal{X}^{(0)})\\prod_{t=0}^{T-1}P(\\mathcal{X}^{(t+1)}\\vert \\mathcal{X}^{(0:t)}),\\label{eq:tm.1} \\end{equation} where $\\mathcal{X}^{(t_1:t_2)}\\doteq\\{\\mathcal{X}^{(t_1)},\\mathcal{X}^{(t_1+1)},\\ldots,\\mathcal{X}^{(t_2-1)},\\mathcal{X}^{(t_2)}\\}$ for $t_1\u0026lt;t_2$. In other words, the distribution over trajectories is the product of conditional distribution, over the variables in each time step $t$ given the preceding one.\nMarkovian System A dynamic system over the template variables $\\mathcal{X}$ is referred as Markovian if it satisfies the Markov assumption, in the sense that \\begin{equation} (\\mathcal{X}^{(t+1)}\\perp\\mathcal{X}^{(0:t-1)}\\vert\\mathcal{X}^{(t)}) \\end{equation} In other words, in such systems, we have a more compact form of \\eqref{eq:tm.1}, which is \\begin{equation} P(\\mathcal{X}^{(0)},\\mathcal{X}^{(1)},\\ldots,\\mathcal{X}^{(T)})=P(\\mathcal{X}^{(0)})\\prod_{t=0}^{T-1}P(\\mathcal{X}^{(t+1)}\\vert\\mathcal{X}^{(t)}) \\end{equation} A Markovian dynamic system is said to be stationary (or time invariant, or homogeneous) if $P(\\mathcal{X}^{(t+1)}\\vert\\mathcal{X}^{(t)})$ is the same for all $t$. In this case, we can represent the process using a transition model $P(\\mathcal{X}\u0026rsquo;\\vert\\mathcal{X})$, so that for any $t\\geq0$, we have \\begin{equation} P(\\mathcal{X}^{(t+1)}=\\xi\u0026rsquo;\\vert\\mathcal{X}^{(t)}=\\xi)=P(\\mathcal{X}\u0026rsquo;=\\xi\u0026rsquo;\\vert\\mathcal{X}=\\xi) \\end{equation}\nDynamic Bayesian Networks A 2-time-slice Bayesian network (2-TBN) for a process over $\\mathcal{X}$ is a conditional Bayesian network over $\\mathcal{X}\u0026rsquo;$ given $\\mathcal{X}_I$, where $\\mathcal{X}_I\\subset\\mathcal{X}$ is a set of interface variables.\nHence, as mentioned above, we have\nOnly the variables $\\mathcal{X}'$ are associated with CPDs (i.e. having parents). The interface variables $\\mathcal{X}_I$ are variables whose values at time $t$ have a direct effect on the variables at time $t+1$. Thus, only the variables in $\\mathcal{X}_I$ can be parents of variables in $\\mathcal{X}'$. The 2-TBN represents the distribution \\begin{equation} P(\\mathcal{X}'\\vert\\mathcal{X})=P(\\mathcal{X}'\\vert\\mathcal{X}_I)=\\prod_{i=1}^{n}P(X_i'\\vert\\text{Pa}_{X_i'}) \\end{equation} For each template variable $X_i$, the CPD $P(X_i'\\vert\\text{Pa}_{X_i'})$ is a template factor, i.e. it will be instantiated multiple times within the model, for multiple variables $X_i^{(t)}$ (and their parents). In a 2-TBN, edges that go between time slices are called inter-time-slice, while the ones connecting variables in the same slices are known as intra-time-slice. Additionally, inter-time-slice edges having the form of $X\\rightarrow X\u0026rsquo;$ are referred as persistence. The variable $X$ for which we have an edge $X\\rightarrow X\u0026rsquo;$ is also called persistent variable.\nFigure 9: (based on figure from the PGM book) A $2-TBN$ Based on the stationary property, a 2-TBN defines the probability distribution $P(\\mathcal{X}^{(t+1)}\\vert\\mathcal{X}^{(t)})$ for any $t$. Given a distribution over the initial states, we can unroll the network over sequences of any length, to define a Bayesian network that induces a distribution over trajectories of that length.\nA dynamic Bayesian network (or DBN) is a tuple $(\\mathcal{B}_0,\\mathcal{B}_\\rightarrow)$, where\n$\\mathcal{B}_0$ is a Bayesian network over $\\mathcal{X}^{(0)}$ representing the initial distribution over states; $\\mathcal{B}_\\rightarrow$ is a 2-TBN for the process. For any time span $T\\geq0$, the distribution over $\\mathcal{X}^{(0:T)}$ is defined as an unrolled Bayesian network, where, for any $i=1,\\ldots,n$:\nThe structure and CPDs of $X_i^{(0)}$ are the same as those for $X_i$ in $\\mathcal{B}_0$. The structure and CPDs of $X_i^{(t)}$ for $t\u003e0$ are the same as those for $X_i'$ in $\\mathcal{B}_\\rightarrow$. Or in other words, $\\mathcal{B}_0$ is the initial state, while $\\mathcal{B}_\\rightarrow$ represents the transition model.\nRemark: Hence, we can view a DBN as a compact representation from which we can generate an infinite set of Bayesian networks (one for every $T\u0026gt;0$).\nFigure 10: (based on figure from the PGM book)\n(a) $\\mathcal{B}_\\rightarrow$; (b) $\\mathcal{B}_0$; (c) 3-step unrolled DBN In DBNs, we can partition the variables $\\mathcal{X}$ into disjoint subsets $\\mathbf{X}$ and $\\mathbf{O}$ such that variables in $\\mathbf{X}$ are always hidden, while ones in $\\mathbf{O}$ are always observed. This introduces us to an another way of representing temporal process, which is the state-observation model.\nState-Observation Models A state-observation model utilizes two independent assumptions:\nMarkov assumption: \\begin{equation} (\\mathbf{X}^{(t+1)}\\perp\\mathbf{X}^{(0:t-1)}\\vert\\mathbf{X}^{(t)}) \\end{equation} Observations depend on current state only: \\begin{equation} (\\mathbf{O}^{(t)}\\perp\\mathbf{X}^{(0:t-1)},\\mathbf{X}^{(t+1:\\infty)}\\vert\\mathbf{X}^{(t)}) \\end{equation} Therefore, we can view our probabilistic model containing two components: the transition model, $P(\\mathbf{X}\u0026rsquo;\\vert\\mathbf{X})$, and the observation model, $P(\\mathbf{O}\\vert\\mathbf{X})$. This corresponds to a 2-TBN structure where the observation variables $\\mathbf{O}\u0026rsquo;$ are all leaves, and have parents only in $\\mathbf{X}\u0026rsquo;$. For instance, as considering Figure 9, Observation' are acting as $\\mathbf{O}\u0026rsquo;$.\nHidden Markov Models A Hidden Markov model, or HMM, is the simplest example of a state-observation model, and also a special case of a simple DBN, which has a sparse transition model $P(S\u0026rsquo;\\vert S)$. Thus, HMMs are often represented using a different graphical notation which visualizes this sparse transition model.\nSpecifically, in the is representation, the transition model is encoded using a directed graph, where\nNodes represent the different states of the system, i.e. the values in $\\text{Val}(S)$. Each directed edge $s\\rightarrow s'$ represents a possible transitioning from $s$ to $s'$, i.e. $P(s'\\vert s)\u003e0$. Example 9: Consider an HMM with state variable $S$ that takes 4 possible values $s_1,s_2,s_3,s_4$ and with a transition model\n$s_1$ $s_2$ $s_3$ $s_4$ $s_1$ 0.3 0.7 0 0 $s_2$ 0 0 0.4 0.6 $s_3$ 0.5 0 0 0.5 $s_4$ 0 0.9 0 0.1 where the rows correspond to states $s$, while the columns to next states $s\u0026rsquo;$. On other words, the $i$-th row represents the CPD $P(s\u0026rsquo;\\vert s=s_i)$, and thus must sum to $1$. Its transition graph is shown below\nLinear Dynamical Systems A linear dynamical system, or Kalman filter represents a system of one or more real-valued variable that evolve linearly over time, with some Gaussian noise.\nSuch systems can be viewed as DBNs where the variables are all continuous and all of the dependencies are linear Gaussian.\nThey are traditionally represented as a state-observation model, where the state and the observation are both vector-valued r.v.s; and where the transition model and observation model are both encoded using matrices. In more specific, the model is generally defined via the set of equations \\begin{align} P(\\mathbf{X}^{(t)}\\vert\\mathbf{X}^{(t-1)})\u0026amp;=\\mathcal{N}(A\\mathbf{X}^{(t-1)};Q), \\\\ P(O^{(t)}\\vert\\mathbf{X}^{(t)})\u0026amp;=\\mathcal{N}(H\\mathbf{X}^{(t)};R), \\end{align} where\n$\\mathbf{X}\\in\\mathbb{R}^n$ is the vector of state variables; $O\\in\\mathbb{R}^m$ is the vector of observation variables; $A\\in\\mathbb{R}^{n\\times n}$ (precisely $A\\in[0,1]^{n\\times n}$) is the transition matrix, defines the linear transition model; $H\\in\\mathbb{R}^{n\\times m}$ (also $H\\in[0,1]^{n\\times m}$ to be exact) defines the linear observation model; $R\\in\\mathbb{R}^{m\\times m}$ defines the Gaussian noise associated with the observations. Extended Kalman Filters A nonlinear variant of the linear dynamical system, known as extended Kalman filter, is a system where the state transition and observation model are nonlinear functions, i.e. \\begin{align} P(\\mathbf{X}^{(t)}\\vert\\mathbf{X}^{(t-1)})\u0026amp;=f(\\mathbf{X}^{(t-1)},\\mathbf{U}^{(t-1)}) \\\\ P(O^{(t)}\\vert\\mathbf{X}^{(t)})\u0026amp;=g(\\mathbf{X}^{(t)},\\mathbf{W}^{(t)}), \\end{align} where\n$f,g$ are nonlinear functions; $\\mathbf{U}^{(t)},\\mathbf{W}^{(t)}$ are Gaussian r.v.s. Template Variables \u0026amp; Template Factors As viewing the world as a set of objects, each of which can be divided into a set of mutually exclusive and exhaustive classes $\\mathcal{Q}=Q_1,\\ldots,Q_k$.\nAn attribute $A$ is a function $A(U_1,\\ldots,U_k)$ whose range is some set $\\text{Val}(A)$ and where each argument $U_i$ is known as a logical variable associated with a particular class $Q[U_i]\\doteq Q_i$. The tuple $(U_1,\\ldots,U_k)$ is called the argument signature of the attribute $A$, denoted $\\alpha(A)$ \\begin{equation} \\alpha(A)\\doteq(U_1,\\ldots,U_k) \\end{equation} Example 10: The argument signature of Grade attribute would have two logical variables $S,C$ where $S$ is of class Student, and where $C$ is of class Course.\nLet $\\mathcal{Q}$ be a set of classes, and $\\aleph$ be a set of attributes over $\\mathcal{Q}$. An object skeleton $\\kappa$ specifies a fixed, finite set of objects $\\mathcal{O}^\\kappa[Q]$ for every $Q\\in\\mathcal{Q}$. We also define \\begin{equation} \\mathcal{O}^\\kappa[\\alpha(A)]=\\mathcal{O}^\\kappa[U_1,\\ldots,U_k]\\doteq\\mathcal{O}^\\kappa[Q[U_1]]\\times\\ldots\\times\\mathcal{O}^\\kappa[Q[U_k]] \\end{equation} By default, we let $\\Gamma_\\kappa[A]=\\mathcal{O}^\\kappa[\\alpha(A)]$ to be the set of possible assignments to the logical variables in the argument signature of $A$. However, an object skeleton might also specify a subset of legal assignments, i.e. $\\Gamma_\\kappa[A]\\subset\\mathcal{O}^\\kappa[\\alpha(A)]$.\nFor an object skeleton $\\kappa$ over $\\mathcal{Q},\\aleph$. We define sets of ground random variables \\begin{align} \\mathcal{X}_\\kappa[A]\u0026amp;\\doteq\\{A(\\gamma):\\gamma\\in\\Gamma_\\kappa[A]\\} \\\\ \\mathcal{X}_\\kappa[\\aleph]\u0026amp;\\doteq\\bigcup_{A\\in\\aleph}\\mathcal{X}_\\kappa[A] \\end{align} Here, we are abusing notation, identifying an argument $\\gamma=(U_1\\mapsto u_1,\\ldots,U_k\\mapsto u_k)$ with the tuple $(u_1,\\ldots,u_k)$.\nA template factor $\\xi$ is a function defined over a tuple of template attributes $A_1,\\ldots,A_l$ where each $A_i$ has a range $\\text{Val}(A_i)$. It defines a mapping $\\text{Val}(A_1)\\times\\ldots\\times\\text{Val}(A_l)\\mapsto\\mathbb{R}$. Given r.v.s $X_1,\\ldots,X_l$ such that $\\text{Val}(X_i)=\\text{Val}(A_i)$ for all $i=1,\\ldots,j$, we define $\\xi(X_1,\\ldots,X_l)$ to be the instantiated factor from $\\mathbf{X}$ to $\\mathbb{R}$.\nDirected Probabilistic Models for Object-Relational Plate Models A plate model $\\mathcal{M}_\\text{plate}$ defines for each template attribute $A\\in\\aleph$ with argument signature $U_1,\\ldots,U_k$:\na set of template parents \\begin{equation} \\text{Pa}_A=\\{B_1(\\mathbf{U}_1),\\ldots,B_l(\\mathbf{U}_l)\\}, \\end{equation} such that for each $B_i(\\mathbf{U}_i)$, we have that $\\mathbf{U}_i\\subset\\{U_1,\\ldots,U_k\\}$. The variables $\\mathbf{U}_i$ are the argument signature of the parent $B_i$. a template CPD $P(A\\vert\\text{Pa}_A)$. Figure 11: (based on figure from the PGM book)\nPlate models and induced ground Bayesian networks\n(a) Single plate: for any student $s$, $P(I(s))$ and $P(G(s)\\vert I(s))$ are the same;\n(b) Nested plates: for any (student, course) pair $(s,c)$, $\\textit{Grade}(s,c)$ depends on $\\textit{Difficulty}(c)$ and on $\\textit{Intelligence}(s,c)$;\n(c) Intersecting plates: for any (student, course) pair $(s,c)$, $\\text{Grade}(s,c)$ depends on $\\textit{Difficulty}(c)$ and on $\\textit{Intelligence}(s)$. Ground Bayesian Networks for Plate Models A plate model $\\mathcal{M}_\\text{plate}$ and object skeleton $\\kappa$ define a ground Bayesian network $\\mathcal{B}_\\kappa^{\\mathcal{M}_\\text{plate}}$ as follows. Let $A(U_1,\\ldots,U_k)$ be any template attribute in $\\aleph$. Then, for any $\\gamma=(U_1\\mapsto u_1,\\ldots,U_k\\mapsto u_k)\\in\\Gamma_\\kappa[A]$, we have a variable $A(\\gamma)$ in the ground network, with parents $B(\\gamma)$ for all $B\\in\\text{Pa}_A$, and the instantiated CPD $P(A(\\gamma)\\vert\\text{Pa}_A(\\gamma))$.\nFor instance, consider the Figure 11(c), without loss of generality, we have that:\nThe plate model $\\mathcal{M}_\\text{plate}$ is defined over a set $\\aleph=\\{\\textit{Grade},\\textit{Difficulty},\\textit{Intelligence}\\}$ of template attributes, for each of which: $\\alpha(\\textit{Grade})=(S,C)$ and $\\text{Pa}_\\textit{Grade}=\\{\\textit{Difficulty}(C),\\textit{Intelligence}(S)\\}$; $\\alpha(\\textit{Difficulty})=(C)$ and $\\text{Pa}_\\textit{Difficulty}=\\emptyset$; $\\alpha(\\textit{Intelligence})=(S)$ and $\\text{Pa}_\\textit{Intelligence}=\\emptyset$, where $S$ is logical variable of class $\\textit{Student}$, and $C$ is a logical variable of class $\\textit{Course}$. Let $(S\\mapsto s_1,C\\mapsto c_1)$, $(C\\mapsto c_2)$ and $(S\\mapsto s_3)$ be some assignment to some logical variables $S,C$ where $S$ is of class $\\textit{Student}$, $C$ is of class $\\textit{Course}$. We then have instantiated CPDs: \\begin{align} P(G(s_1,c_1)\\vert\\text{Pa}_G(s_1,c_1))\u0026=P(G(s_1,c_1)\\vert D(s_1,c_1),I(s_1,c_1)) \\\\ \u0026=P(G(s_1,c_1)\\vert D(c_1),I(s_1)); \\\\ P(D(c_2)\\vert\\text{Pa}_D(c_2))\u0026=P(D(c_2)); \\\\ P(I(s_3)\\vert\\text{Pa}_I(s_3))\u0026=P(I(s_3)) \\end{align} Probabilistic Relational Models For a template attribute $A$, we define a contingent dependency model as a tuple containing:\nA parent argument signature $\\alpha(\\text{Pa}_A)$, which is a tuple of typed logical variables $U_i$ such that $\\alpha(\\text{Pa}_A)\\supset\\alpha(A)$. A guard $\\Gamma$, which is a binary-valued formula defined in terms of a set of template attributes $\\text{Pa}_A^\\Gamma$ over the argument signature $\\alpha(\\text{Pa}_A)$. A set of template parents \\begin{equation} \\text{Pa}_A=\\{B_1(\\mathbf{U}_1),\\ldots,B_l(\\mathbf{U}_l)\\}, \\end{equation} such that for each $B_i(\\mathbf{U}_i)$, we have that $\\mathbf{U}_i\\subset\\alpha(\\text{Pa}_A)$. Ground Bayesian Networks for PRMs References [1] Daphne Koller, Nir Friedman. Probabilistic Graphical Models. The MIT Press.\n[2] Michael I. Jordan. An Introduction to Probabilistic Graphical Models. In preparation.\n[3] Eric P. Xing. 10-708: Probabilistic Graphical Model. CMU Spring 2020.\n[4] Stefano Ermon. CS228: Probabilistic Graphical Model. Stanford Winter 2017-2018.\nFootnotes Note that $X_i\\rightarrow X_j\\equiv X_j\\leftarrow X_i$ but $X_i\\rightarrow X_j\\not\\equiv X_i\\leftarrow X_j$, while $X_i-X_j\\equiv X_j-X_i$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nNote that the inverse is not true.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis can be specified by doing the procedure\nLet $\\mathbf{Z}^+\\leftarrow\\mathbf{Z}$\nWhile $\\exists X_i$ such that $X_i\\in\\mathbf{D}$ and $\\text{Pa}_{X_i}\\subset\\mathbf{Z}^+$\n$\\hspace{1cm}\\mathbf{Z}^+\\leftarrow\\mathbf{Z}\\cup\\{X_i\\}$\nreturn $\\text{d-sep}(\\mathbf{X};\\mathbf{Y}\\vert\\mathbf{Z})$\n\u0026#160;\u0026#x21a9;\u0026#xfe0e; \u0026#160;\u0026#x21a9;\u0026#xfe0e; ","permalink":"https://trunghng.github.io/posts/machine-learning/pgm-representation/","summary":"\u003cp\u003eNotes on Representation in PGMs.\u003c/p\u003e","title":"Probabilistic Graphical Models - Representation"},{"content":" Notes on Deterministic Policy Gradient Algorithms\nPreliminaries Consider a (infinite-horizon) Markov Decision Process (MDP), defined as a tuple of $(\\mathcal{S},\\mathcal{A},p,r,\\rho_0,\\gamma)$ where\n$\\mathcal{S}$ is the state space. $\\mathcal{A}$ is the action space. $p:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ is the transition probability distribution, i.e. $p(s,a,s\u0026rsquo;)=p(s\u0026rsquo;\\vert s,a)$ denotes the probability of transitioning to state $s\u0026rsquo;$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function, and let us denote $r_{t+1}\\doteq r(S_t,A_t)$. $\\rho_0:\\mathcal{S}\\to\\mathbb{R}$ is the distribution of the initial state $s_0$. $\\gamma\\in(0,1)$ is the discount factor. Within an MPD, a policy parameterized by a vector $\\theta\\in\\mathbb{R}^n$ can be given as\nStochastic policy. $\\pi_\\theta:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$, or Deterministic policy. $\\mu_\\theta:\\mathcal{S}\\to\\mathcal{A}$. (Stochastic) Policy Gradient Theorem We continue with an assumption that the action space $\\mathcal{A}=\\mathbb{R}^m$ and the state space $\\mathcal{S}\\subset\\mathbb{R}^d$ and $\\mathcal{S}$ is compact.\nStart-state formulation Recall that in the stochastic case, $\\pi_\\theta$, the Policy Gradient Theorem states that1 \\begin{align} \\nabla_\\theta J(\\pi_\\theta)\u0026amp;=\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\hspace{0.1cm}da\\hspace{0.1cm}ds\\label{eq:spgt.1} \\\\ \u0026amp;=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big]\\label{eq:spgt.2} \\end{align} where\n$J(\\pi_\\theta)$ is the performance objective function, which we are trying to maximize. It is defined as the expected cumulative discounted reward from the start state \\begin{align} J(\\pi_\\theta)\u0026\\doteq\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\big[r_0^\\gamma\\big]\\label{eq:spgt.6} \\\\ \u0026=\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1}\\right] \\\\ \u0026=\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\big[G_0\\big] \\\\ \u0026=\\mathbb{E}_{s_0\\sim\\rho_0}\\Big[\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s_0\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\pi(s_0)\\big],\\label{eq:spgt.3} \\end{align} where $r_t^\\gamma$ is defined as the total discounted reward from time-step $t$ onward, which is thus the return at that step \\begin{equation} G_t=r_t^\\gamma\\doteq\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\end{equation} The function $\\rho_\\pi(s)$ is defined as the discounted weighting of states encountered when starting at $s_0$ and following $\\pi_\\theta$ thereafter \\begin{align} \\rho_\\pi(s)\u0026\\doteq\\sum_{t=0}^{\\infty}\\gamma^t P(S_t=s\\vert s_0,\\pi_\\theta) \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})\\left(\\sum_{t=0}^{\\infty}\\gamma^t P(S_t=s\\vert\\pi_\\theta)\\right)d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\sum_{t=0}^{\\infty}\\gamma^t\\rho_0(\\bar{s})p(\\bar{s}\\to s,t,\\pi_\\theta)d\\bar{s},\\label{eq:spgt.4} \\end{align} where $p(\\bar{s}\\to s,t,\\pi_\\theta)$ is defined as the the probability of transitioning to $s$ after $t$ steps starting from $\\bar{s}$ under $\\pi_\\theta$, which implies that \\begin{equation} p(\\bar{s}\\to s,t,\\pi_\\theta)=P(S_t=s\\vert\\pi_\\theta) \\end{equation} In fact, $\\rho_\\pi(s)$ can be seen as a density function since integrating $\\rho_\\pi$ over state space $\\mathcal{S}$ gives us \\begin{align} \\int_\\mathcal{S}\\rho_\\pi(s)d s\u0026=\\int_{s\\in\\mathcal{S}}\\int_{\\bar{s}\\in\\mathcal{S}}\\sum_{t=0}^{\\infty}\\gamma^t\\rho_0(\\bar{s})p(\\bar{s}\\to s,t,\\pi_\\theta)d\\bar{s}\\hspace{0.1cm}d s \\\\ \u0026=\\int_{\\bar{s}\\in\\mathcal{S}}\\rho_0(\\bar{s})\\int_{s\\in\\mathcal{S}}\\sum_{t=0}^{\\infty}\\gamma^t p(\\bar{s}\\to s,t,\\pi_\\theta)d s\\hspace{0.1cm}d\\bar{s} \\\\ \u0026=\\int_{\\bar{s}\\in\\mathcal{S}}\\rho_0(\\bar{s})\\sum_{t=0}^{\\infty}\\gamma^t\\underbrace{\\int_{s\\in\\mathcal{S}}p(\\bar{s}\\to s,t,\\pi_\\theta)d s}_{=1}d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})\\underbrace{\\sum_{t=0}^{\\infty}\\gamma^t}_{=1}d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})d\\bar{s} \\\\ \u0026=1 \\end{align} Thus, this definition of $\\rho_\\pi$ lets us write \\eqref{eq:spgt.1} as an expectation, combined with using the log-likelihood trick, we end up with \\eqref{eq:spgt.2}. Proof\nThe definition of $J(\\pi_\\theta)$ given in \\eqref{eq:spgt.3} suggests us begin by considering the gradient of the state value function w.r.t $\\theta$. For any $s\\in\\mathcal{S}$, we have \\begin{align} \\hspace{-1cm}\\nabla_\\theta V_\\pi(s)\u0026amp;=\\nabla_\\theta\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da \\\\ \u0026amp;=\\underbrace{\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da}_{\\psi(s)}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)da \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\left(r(s,a)+\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\vert s,a)V_\\pi(s\u0026rsquo;)d s\u0026rsquo;\\right)da \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da \\\\ \u0026amp;\\overset{\\text{(i)}}{=}\\psi(s)+\\int_\\mathcal{S}\\left(\\int_\\mathcal{A}\\gamma\\pi_\\theta(a\\vert s)p(s\u0026rsquo;\\vert s,a)da\\right)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo;\\label{eq:spgt.9} \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\pi_\\theta)\\left(\\psi(s\u0026rsquo;)+\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\to s\u0026rsquo;\u0026rsquo;,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s\u0026rsquo;\u0026rsquo;)d s\u0026rsquo;\u0026rsquo;\\right)d s\u0026rsquo; \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\pi_\\theta)\\psi(s\u0026rsquo;)d s\u0026rsquo;\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\int_\\mathcal{S}\\int_\\mathcal{S}\\gamma^2 p(s\\to s\u0026rsquo;,1,\\pi_\\theta)p(s\u0026rsquo;\\to s\u0026rsquo;\u0026rsquo;,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s\u0026rsquo;\u0026rsquo;)d s\u0026rsquo;\u0026rsquo;\\hspace{0.1cm}d s\u0026rsquo; \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\pi_\\theta)\\psi(s\u0026rsquo;)d s\u0026rsquo;+\\int_\\mathcal{S}\\gamma^2 p(s\\to s\u0026rsquo;\u0026rsquo;,2,\\pi_\\theta)\\nabla_\\theta V_\\pi(s\u0026rsquo;\u0026rsquo;)d s\u0026rsquo;\u0026rsquo; \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\pi_\\theta)\\psi(s\u0026rsquo;)d s\u0026rsquo;+\\int_\\mathcal{S}\\gamma^2 p(s\\to s\u0026rsquo;\u0026rsquo;,2,\\pi_\\theta)\\psi(s\u0026rsquo;\u0026rsquo;)d s\u0026rsquo;\u0026rsquo;\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\int_\\mathcal{S}\\gamma^3 p(s\\to s\u0026rsquo;\u0026rsquo;\u0026rsquo;,3,\\pi_\\theta)\\nabla_\\theta V_\\pi(s\u0026rsquo;\u0026rsquo;\u0026rsquo;)d s\u0026rsquo;\u0026rsquo;\u0026rsquo; \\\\ \u0026amp;\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026amp;=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\pi_\\theta)\\psi(\\tilde{s})d\\tilde{s} \\\\ \u0026amp;=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert\\tilde{s})Q_\\pi(\\tilde{s},a)da\\hspace{0.1cm}d\\tilde{s}\\label{eq:spgt.5} \\end{align} where\nIn this step, we have used the Fubini's theorem to exchange the order of integrals. We have once again used the Fubini's theorem to exchange the order of integration combined with the identity \\begin{equation} \\int_\\mathcal{S}p(s\\to s',1,\\pi_\\theta)p(s'\\to s'',1,\\pi_\\theta)d s'=p(s\\to s'',2,\\pi_\\theta) \\end{equation} Combining \\eqref{eq:spgt.3},\\eqref{eq:spgt.4} and \\eqref{eq:spgt.5} together allows us to obtain \\begin{align} \\hspace{-1cm}\\nabla_\\theta J(\\pi_\\theta)\u0026amp;=\\nabla_\\theta\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\pi(s_0)\\big] \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_0(s_0)\\nabla_\\theta V_\\pi(s_0)d s_0 \\\\ \u0026amp;=\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\int_{s\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s\\hspace{0.1cm}d s_0 \\\\ \u0026amp;\\overset{\\text{(i)}}{=}\\int_{s\\in\\mathcal{S}}\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s_0\\hspace{0.1cm}d s \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}\\int_{s\\in\\mathcal{S}}\\int_\\mathcal{A}\\left(\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)d s_0\\right)\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\int_\\mathcal{A}\\rho_\\pi(s)\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s, \\end{align} where in two steps (i) and (ii), we have exchanged the order of integration by respectively applying the Fubini\u0026rsquo;s theorem.$\\tag*{$\\Box$}$\nThe theorem lets us rewrite the policy gradient $\\nabla_\\theta J(\\pi_\\theta)$ in terms of which does not depend the gradient of state distribution, $\\nabla_\\theta\\rho_\\pi(s)$, despite of the fact that $J(\\pi_\\theta)$ depends on $\\rho_\\pi(s)$.\nThe above policy gradient theorem is explicitly known as the start-state policy gradient theorem (since it is given in terms of the start state distribution $\\rho_0$) defined by the policy objective function $J(\\pi_\\theta)$, given in \\eqref{eq:spgt.6}, for episodic and discounted tasks. To extend the theorem in case of continuing problems, in which the interaction between agent and the environment lasts forever, we start by giving a new definition for the $J(\\pi_\\theta)$.\nAverage-reward formulation The performance objective function in such continuing tasks is defined as the average rate of reward, or average reward, denoted $r(\\pi_\\theta)$2, while following policy $\\pi_\\theta$ \\begin{align} J(\\pi_\\theta)\\doteq r(\\pi_\\theta)\u0026amp;\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=0}^{h}\\mathbb{E}\\big[r_{t+1}\\vert S_0,A_{0:t}\\sim\\pi_\\theta\\big] \\\\ \u0026amp;=\\lim_{t\\to\\infty}\\mathbb{E}\\big[r_{t+1}\\vert S_0,A_{0:t}\\sim\\pi_\\theta\\big] \\\\ \u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)r(s,a)da\\hspace{0.1cm}d s, \\end{align} where $\\bar{\\rho}_\\pi(s)\\doteq\\lim_{t\\to\\infty}P(S_t=s\\vert A_{0:t}\\sim\\pi_\\theta)$ is the stationary distribution3 of states under policy $\\pi_\\theta$ which is assumed to exist and does not depend on the start state $S_0$ despite of the fact that the expectations are conditioned on $S_0$.\nAn MDP with this assumption is referred as an ergodic MDP, in which the MDP starts or any early decision made by the agent can have only a temporary effect; in the long run the expectation of being in a state depends only on the policy and the MDP transition probabilities.\nAnalogously, in continuing problems, the return at time-step $t$, $G_t$, is instead called differential return and is defined in terms of differences between rewards and the average reward $r(\\pi_\\theta)$ \\begin{equation} G_t\\doteq\\sum_{k=1}^{\\infty}\\big[r_{t+k}-r(\\pi_\\theta)\\big] \\end{equation} The value functions, which are defined as the expected return, therefore are known as differential value functions and are respectively given by \\begin{align} V_\\pi(s)\u0026amp;\\doteq\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s\\big] \\\\ \u0026amp;=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{k=1}^{\\infty}\\big(r_{t+k}-r(\\pi_\\theta)\\big)\\Big\\vert S_t=s\\right] \\\\ \u0026amp;=\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\big[r(s,a)-r(\\pi_\\theta)+V_\\pi(s\u0026rsquo;)\\big]d s\u0026rsquo; da \\\\ \u0026amp;=\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)r(s,a)da-r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da \\end{align} and \\begin{align} Q_\\pi(s,a)\u0026amp;\\doteq\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s,A_t=a\\big] \\\\ \u0026amp;=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{k=1}^{\\infty}\\big(r_{t+k}-r(\\pi_\\theta)\\big)\\Big\\vert S_t=s,A_t=a\\right] \\\\ \u0026amp;=\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\left[r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a\u0026rsquo;\\vert s\u0026rsquo;)Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)d a\u0026rsquo;\\right]d s\u0026rsquo; \\\\ \u0026amp;=r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\int_\\mathcal{A}\\pi_\\theta(a\u0026rsquo;\\vert s\u0026rsquo;)Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)d a\u0026rsquo; d s\u0026rsquo; \\\\ \u0026amp;=r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)V_\\pi(s\u0026rsquo;)d s' \\end{align} Now we are ready for the policy gradient theorem specified for continuing tasks, and hence is called average-reward policy gradient theorem. The theorem states that \\begin{align} \\nabla_\\theta J(\\pi_\\theta)\u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\hspace{0.1cm}da\\hspace{0.1cm}ds\\label{eq:spgt.8} \\\\ \u0026amp;=\\mathbb{E}_{\\bar{\\rho}_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big] \\end{align} Proof\nAnalogy to the episodic case, we start with the gradient of the state value function w.r.t $\\theta$. For any $s\\in\\mathcal{S}$, we have \\begin{align} \\nabla_\\theta V_\\pi(s)\u0026amp;=\\nabla_\\theta\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da \\\\ \u0026amp;=\\underbrace{\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da}_{\\psi(s)}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)da \\\\ \u0026amp;=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\left[r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)V_\\pi(s\u0026rsquo;)d s\u0026rsquo;\\right]da \\\\ \u0026amp;=\\psi(s)-\\nabla_\\theta r(\\pi_\\theta)\\underbrace{\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)da}_{=1}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da \\\\ \u0026amp;=\\psi(s)-\\nabla_\\theta r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da, \\end{align} which implies that the policy gradient can be obtained as \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\nabla_\\theta r(\\pi_\\theta)=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da-\\nabla_\\theta V_\\pi(s)\\label{eq:spgt.7} \\end{equation} Using the identity \\begin{equation} \\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta J(\\pi_\\theta)d s=\\nabla_\\theta J(\\pi_\\theta)\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)d s=\\nabla_\\theta J(\\pi_\\theta), \\end{equation} we can continue to derive \\eqref{eq:spgt.7} as \\begin{align} \\hspace{-0.5cm}\\nabla_\\theta J(\\pi_\\theta)\u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s+\\int_{s\\in\\mathcal{S}}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_{s\u0026rsquo;\\in\\mathcal{S}}p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo; da\\hspace{0.1cm}d s\\nonumber \\\\ \u0026amp;\\hspace{2cm}-\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta V_\\pi(s)d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s+\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s\u0026rsquo;)\\nabla_\\theta V_\\pi(s\u0026rsquo;)d s\u0026rsquo;-\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta V_\\pi(s)d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\end{align} where the second step is due to \\eqref{eq:fn.1}.$\\tag*{$\\Box$}$\nIt can be seen that the stochastic policy gradient theorem specified in both discounted episodic and continuing settings have the same formulation. In particular, if we replace the state distribution $\\rho_\\pi$ in start-state formulation \\eqref{eq:spgt.1} by the stationary distribution $\\bar{\\rho}_\\pi$ (also with new definition of the value functions), we obtain average-reward formulation \\eqref{eq:spgt.8}. Thus, in the remaining of this note, we will be considering the episodic and discounted setting.\n(Stochastic) Actor-Critic Based on the policy gradient theorem, a (stochastic) actor-critic algorithm consists of two elements:\nActor learns a parameter $\\theta$ of the stochastic policy $\\pi_\\theta$ by iteratively update $\\theta$ by SGA using the policy gradient in \\eqref{eq:spgt.2}. Critic estimates the value function $Q_\\pi(s,a)$ by an state-action value function approximation $Q_w(s,a)$ parameterized by a vector $w$. Policy Gradient with Function Approximation Let $Q_w(s,a)$ be a function approximation parameterized by $w\\in\\mathbb{R}^n$ of the state-action value function $Q_\\pi(s,a)$ for a stochastic policy $\\pi_\\theta$ parameterized by $\\theta\\in\\mathbb{R}^n$. Then if $Q_w(s,a)$ is compatible with the policy parameterization in the sense that\n$Q_w(s,a)=\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)^\\text{T}w$. The parameters $w$ are chosen to minimize the mean-squared error (MSE) \\begin{equation} \\epsilon^2(w)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\big(Q_w(s,a)-Q_\\pi(s,a)\\big)^2\\Big] \\end{equation} then $Q_w(s,a)$ \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big], \\end{equation}\nProof\nTaking the gradient w.r.t $w$ of both sides of the equation given in property (i) gives us \\begin{equation} \\nabla_w Q_w(s,a)=\\nabla_\\theta\\log\\pi_\\theta(a\\vert s) \\end{equation} On the other hand, consider the gradient of the MSE, $\\epsilon^2(w)$, w.r.t $w$, we have \\begin{align} \\nabla_w\\epsilon^2(w)\u0026amp;=\\nabla_w\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\big(Q_w(s,a)-Q_\\pi(s,a)\\big)^2\\Big] \\\\ \u0026amp;=\\nabla_w\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]^2 da\\hspace{0.1cm}d s \\\\ \u0026amp;=2\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]\\nabla_w Q_w(s,a)da\\hspace{0.1cm}d s \\\\ \u0026amp;=2\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)da\\hspace{0.1cm}d s \\\\ \u0026amp;=2\\left(\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)da\\hspace{0.1cm}d s-\\nabla_\\theta J(\\pi_\\theta)\\right) \\\\ \u0026amp;=2\\left(\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big]-\\nabla_\\theta J(\\pi_\\theta)\\right) \\end{align} Moreover, property (ii) claims that this gradient w.r.t $w$ must be zero due to the fact that $w$ minimizes $\\epsilon^2(w)$. And thus, we obtain \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big] \\end{equation}\nOff-policy Actor-Critic Consider off-policy methods, which learn a target policy $\\pi_\\theta$ using trajectories sampled according to a behavior policy $\\beta(a\\vert s)\\neq\\pi_\\theta(a\\vert s)$. In this setting, the performance objective is given as the value function of the target policy, averaged over $\\beta$, as \\begin{align} J_\\beta(\\pi_\\theta)\u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)V_\\pi(s)d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\end{align} The off-policy policy gradient then be given by utilizing importance sampling \\begin{align} \\nabla_\\theta J_\\beta(\\pi_\\theta)\u0026amp;=\\nabla_\\theta\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\Big(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)+\\color{red}{\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)}\\Big)da\\hspace{0.1cm}d s\\label{eq:offpac.1} \\\\ \u0026amp;\\overset{\\text{(i)}}{\\approx}\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)d a\\hspace{0.1cm}d s \\\\ \u0026amp;=\\mathbb{E}_{\\rho_\\beta,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big] \\\\ \u0026amp;=\\mathbb{E}_{\\rho_\\beta,\\beta}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\beta(a\\vert s)}\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\right], \\end{align} where to get the approximation in step (i), we have removed the $\\color{red}{\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)}$ part in \\eqref{eq:offpac.1}. This approximation is good enough to guarantee the policy improvement and eventually achieve the true local optima due to justification proofs proposed in Off-PAC paper, which stands for Off-policy Actor-Critic.\nDeterministic Policy Gradient Theorem Now let us consider the case of deterministic policy $\\mu_\\theta$, in which the performance objective function is also defined as the expected return of the start state. Thus we also have that \\begin{equation} J(\\mu_\\theta)=\\mathbb{E}_{\\rho_0,\\mu_\\theta}\\big[r_0^\\gamma\\big]=\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\mu(s_0)\\big]\\label{eq:dpgt.1} \\end{equation} The Deterministic Policy Gradient Theorem thus states that \\begin{align} \\nabla_\\theta J(\\mu_\\theta)\u0026amp;=\\int_\\mathcal{S}\\rho_\\mu(s)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\big\\vert_{a=\\mu_\\theta(s)}d s \\\\ \u0026amp;=\\mathbb{E}_{\\rho_\\mu}\\Big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\big\\vert_{a=\\mu_\\theta(s)}\\Big]\\label{eq:dpgt.2} \\end{align} where along with the assumption we have made in stochastic case, we additionally assume that $\\nabla_a p(s\u0026rsquo;\\vert s,a),\\mu_\\theta(s),\\nabla_\\theta\\mu_\\theta(s),\\nabla_a r(s,a)$ are continues for all $\\theta$ and $s,s\u0026rsquo;\\in\\mathcal{S},a\\in\\mathcal{A}$. These also imply the existence of $\\nabla_a Q_\\mu(s,a)$.\nProof\nThis proof will be quite similar to what we have used in the stochastic case. Specifically, also starting with the gradient of the value function w.r.t $\\theta$, we have \\begin{align} \u0026amp;\\hspace{-0.5cm}\\nabla_\\theta V_\\mu(s)\\nonumber \\\\ \u0026amp;\\hspace{-0.5cm}=\\nabla_\\theta Q_\\mu(s,\\mu_\\theta(s)) \\\\ \u0026amp;\\hspace{-0.5cm}=\\nabla_\\theta\\left[r(s,\\mu_\\theta(s))+\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\vert s,\\mu_\\theta(s))V_\\mu(s\u0026rsquo;)d s\u0026rsquo;\\right] \\\\ \u0026amp;\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a r(s,a)\\vert_{a=\\mu_\\theta(s)}+\\nabla_\\theta\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\vert s,\\mu_\\theta(s))V_\\mu(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a r(s,a)\\vert_{a=\\mu_\\theta(s)}\\nonumber \\\\ \u0026amp;\\hspace{1.5cm}+\\int_\\mathcal{S}\\gamma\\nabla_\\theta\\mu_\\theta(s)\\nabla_a p(s\u0026rsquo;\\vert s,a)\\vert_{a=\\mu_\\theta(s)}V_\\mu(s\u0026rsquo;)+\\gamma p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\mu(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a\\Big(\\underbrace{r(s,a)+\\int_\\mathcal{S}p(s\u0026rsquo;\\vert s,a)V_\\mu(s\u0026rsquo;)d s\u0026rsquo;}_{Q_\\mu(s,a)}\\Big)\\Big\\vert_{a=\\mu_\\theta(s)}+\\int_\\mathcal{S}\\gamma p(s\u0026rsquo;\\vert s,a)\\nabla_\\theta V_\\mu(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;\\hspace{-0.5cm}=\\underbrace{\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}}_{\\psi(s)}+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;\\hspace{-0.5cm}=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s\u0026rsquo;)d s' \\end{align} which is in the same form as equation \\eqref{eq:spgt.9}. Thus after repeated unrolling, we also end up with \\begin{align} \\nabla_\\theta V_\\mu(s)\u0026amp;=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s\u0026rsquo;,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s\u0026rsquo;)d s\u0026rsquo; \\\\ \u0026amp;=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\mu_\\theta)\\psi(\\tilde{s})d\\tilde{s} \\\\ \u0026amp;=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\mu_\\theta)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d\\tilde{s}\\label{eq:dpgt.3} \\end{align} Consider the definition of $J(\\mu_\\theta)$ given in \\eqref{eq:dpgt.1}, taking gradient of both sides w.r.t $\\theta$, combined with \\eqref{eq:dpgt.3} gives us \\begin{align} \u0026amp;\\nabla_\\theta J(\\mu_\\theta)\\nonumber \\\\ \u0026amp;=\\nabla_\\theta\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\mu(s_0)\\big] \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_0(s_0)\\nabla_\\theta V_\\mu(s_0)d s_0 \\\\ \u0026amp;=\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\int_{s\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\mu_\\theta)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s\\hspace{0.1cm}d s_0 \\\\ \u0026amp;=\\int_{s\\in\\mathcal{S}}\\left(\\int_{s_0\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k\\rho_0(s_0)p(s_0\\to s,k,\\mu_\\theta)d s_0\\right)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\mu(s)\\nabla_\\theta\\mu_\\theta(s)\\nabla a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s, \\end{align} where in the forth step, the Fubini\u0026rsquo;s theorem has helped us exchange the order of integration.$\\tag*{$\\Box$}$\nIt is worth remarking that we can consider a deterministic policy $\\mu_\\theta$ as a special case of the stochastic policy, in which $\\pi_\\theta(\\cdot\\vert s)$ becomes the Kronecker delta function, which takes the value of $1$ at only one point $a\\in\\mathcal{A}$ and $0$ elsewhere.\nTo be more specific, in the original paper, the authors have shown that by rewriting the stochastic policy as $\\pi_{\\mu_\\theta,\\sigma}$, which is parameterized by a deterministic policy $\\mu_\\theta:\\mathcal{S}\\to\\mathcal{A}$ and a variance parameter $\\sigma$ such that for $\\sigma=0$, we have that $\\pi_{\\mu_\\theta,0}\\equiv\\mu_\\theta$; then as $\\sigma\\to 0$, they have proved that the stochastic policy gradient converges to the deterministic one.\nThis critical result allows us to apply the deterministic policy gradient to common policy gradient frameworks, for example actor-critic approaches.\nDeterministic Actor-Critic On-policy Deterministic Actor-Critic Analogous to the stochastic approach, the deterministic actor learns a parameter $\\theta$ by using SGA to iteratively update the parameter vector according to the deterministic policy gradient direction \\eqref{eq:dpgt.2} while the critic estimates the state-action value function by a function approximation $Q_w(s,a)$ using a policy evaluation method such as TD-learning.\nFor instance, a deterministic actor-critic method with a Sarsa critic has the following update in each time-step $t$ \\begin{align} \\delta_t\u0026amp;=r_{t+1}+\\gamma Q_w(s_{t+1},a_{t+1})-Q_w(s_t,a_t) \\\\ w_{t+1}\u0026amp;=w_t+\\alpha_w\\delta_t\\nabla_w Q_w(s_t,a_t) \\\\ \\theta_{t+1}\u0026amp;=\\theta_t+\\alpha_\\theta\\nabla_\\theta\\mu_\\theta(s_t)\\nabla_a Q_w(s_t,a_t)\\vert_{a=\\mu_\\theta(s)}, \\end{align} where $\\delta_t$ as specified before, are known as TD errors.\nOff-policy Deterministic Actor-Critic Analogy to stochastic methods, let $\\beta(a\\vert s)$ denote the behavior policy that generates trajectories used for updating the deterministic target policy $\\mu_\\theta(s)$, the performance objective $J(\\mu_\\theta)$ is then given as \\begin{align} J_\\beta(\\mu_\\theta)\u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)V_\\mu(s)d s \\\\ \u0026amp;=\\int_\\mathcal{S}\\rho_\\beta(s)Q_\\mu(s,\\mu_\\theta(s))d s \\end{align} It is noticeable that the deterministic policy allows us to explicitly replace the integration over action space $\\mathcal{A}$ by $Q_\\mu(s,\\mu_\\theta(s))$, and thus we do not need to use importance sampling in the actor. Hence, we have that \\begin{align} \\nabla_\\theta J_\\beta(\\mu_\\theta)\u0026amp;=\\nabla_\\theta\\int_\\mathcal{S}\\rho_\\beta(s)Q_\\mu(s,\\mu_\\theta(s))d s \\\\ \u0026amp;\\approx\\int_\\mathcal{S}\\rho_\\beta(s)\\nabla_\\theta\\mu_\\theta(a\\vert s)Q_\\mu(s,a)d s \\\\ \u0026amp;=\\mathbb{E}_{\\rho_\\beta}\\Big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\Big] \\end{align} We can also avoid using importance sampling in critic by using Q-learning as our policy evaluation In particular, the off-policy deterministic actor-critic with a Q-learning critic has the form of \\begin{align} \\delta_t\u0026amp;=r_{t+1}+\\gamma Q_w(s_{t+1},\\mu_\\theta(s))-Q_w(s_t,a_t)\\label{eq:opdac.1} \\\\ w_{t+1}\u0026amp;=w_t+\\alpha_w\\delta_t\\nabla_w Q_w(s_t,a_t) \\\\ \\theta_{t+1}\u0026amp;=\\theta_t+\\alpha_\\theta\\nabla_\\theta\\mu_\\theta(s_t)\\nabla_a Q_w(s_t,a_t)\\vert_{a_t=\\mu_\\theta(s_t)}, \\end{align} where the greedy policy, $\\underset{a}{\\text{argmax}}Q_w(s,a)$, in the usual Q-learning update has been replaced by the newly-updated deterministic policy, $\\mu_\\theta(s)$, in \\eqref{eq:opdac.1}, i.e. $\\mu_\\theta\\equiv\\mu_{\\theta_k}$.\nCompatible Function Approximation with Deterministic Policy From what we have mentioned in the stochastic case, we can also define an appropriate form of function approximation $Q_w$ which preserves the direction of true gradient.\nIn particular, A $w$-parameterized $Q_w(s,a)$ is referred as a compatible function approximator of the state-action value function $Q_\\mu$ for deterministic policy $\\mu_\\theta$ in the sense that\n$\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}=\\nabla_\\theta\\mu_\\theta(s)^\\text{T}w$. Parameters $w$ minimize the mean-squared error \\begin{equation} \\text{MSE}(\\theta,w)=\\mathbb{E}\\big[\\epsilon(\\theta,w,s)^\\text{T}\\epsilon(\\theta,w,s)\\big], \\end{equation} where \\begin{equation} \\epsilon(\\theta,w,s)\\doteq\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)} \\end{equation} then \\begin{equation} \\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}\\big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big]\\label{eq:cfad.1} \\end{equation}\nProof\nWe follow the procedure used in stochastic case. Specifically, starting with the property (i) and by definition of $\\epsilon$, we have that \\begin{align} \\nabla_w\\epsilon(\\theta,w,s)\u0026amp;=\\nabla_w\\big[\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\big] \\\\ \u0026amp;=\\nabla_w\\big(\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big) \\\\ \u0026amp;=\\nabla_w\\big(\\nabla_\\theta\\mu_\\theta(s)^\\text{T}w\\big) \\\\ \u0026amp;=\\nabla_\\theta\\mu_\\theta(s) \\end{align} Using this result, the gradient of $\\text{MSE}(\\theta,w)$ w.r.t $w$ is thus given as \\begin{align} \\nabla_w\\text{MSE}(\\theta,w)\u0026amp;=\\nabla_w\\mathbb{E}\\big[\\epsilon(\\theta,w,s)^\\text{T}\\epsilon(\\theta,w,s)\\big] \\\\ \u0026amp;=\\mathbb{E}\\big[2\\epsilon(\\theta,w,s)\\nabla_w\\epsilon(\\theta,w,s)\\big] \\\\ \u0026amp;=2\\mathbb{E}\\Big[\\big(\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\big)\\nabla_\\theta\\mu_\\theta(s)\\Big] \\\\ \u0026amp;=2\\Big[\\mathbb{E}\\big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big]-J(\\mu_\\theta)\\Big], \\end{align} which lets our claim, \\eqref{eq:cfad.1}, follows due to the property (ii), which means that $\\nabla_w\\text{MSE}(\\theta,w)=0$.$\\tag*{$\\Box$}$\nDeep Deterministic Policy Gradient References [1] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller. Deterministic Policy Gradient Algorithms. JMLR 2014.\n[2] Richard S. Sutton, Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[3] Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. NIPS 1999.\n[4] Elias M. Stein, Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007.\n[5] Thomas Degris, Martha White, Richard S. Sutton. Off-Policy Actor-Critic. ICML 2012.\n[6] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra. Continuous control with deep reinforcement learning. ICLR 2016.\nFootnotes To simplify the notation, we have let $\\rho_\\pi\\doteq\\rho_{\\pi_\\theta}$ and $Q_\\pi\\doteq Q_{\\pi_\\theta}$ implicitly. As a result, we will also denote $V_{\\pi_\\theta}$ by $V_\\pi$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe notation $r(\\pi)$ of the average reward is just a notation-abused and should not be confused with notation $r$ of the reward function.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis means that if we keep selecting action according to $\\pi_\\theta$, we remains in the same state distribution $\\bar{\\rho}_\\pi$, i.e. \\begin{equation} \\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)p(s\u0026rsquo;\\vert s,a)da\\hspace{0.1cm}d s=\\bar{\\rho}_\\pi(s\u0026rsquo;)\\label{eq:fn.1} \\end{equation}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on Deterministic Policy Gradient Algorithms\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Deterministic Policy Gradients"},{"content":" Notes on TRPO.\nBasic definitions \u0026amp; notations We begin by recalling definition of MDPs, coupling and total variation distance.\nMarkov Decision Processes An infinite-horizon discounted Markov Decision Process (MDP) is defined as the tuple $(\\mathcal{S},\\mathcal{A},P,r,\\rho_0,\\gamma)$, where\n$\\mathcal{S}$ is a finite set of states, or state space. $\\mathcal{A}$ is a finite set of actions, or action space. $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to\\mathbb{R}$ is the transition probability distribution, i.e. $P(s,a,s\u0026rsquo;)=P(s\u0026rsquo;\\vert s,a)$ denotes the probability of transitioning to state $s\u0026rsquo;$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function. $\\rho_0:\\mathcal{S}\\to\\mathbb{R}$ is the distribution of the initial state $s_0$. $\\gamma\\in(0,1)$ is the discount factor. A policy, denoted $\\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to\\{0,1\\}$ (or $\\pi:\\mathcal{S}\\to\\mathcal{A}$) or stochastic $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$. Here, we consider the stochastic policy only.\nWe continue by letting $\\eta(\\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\\pi$ thereafter \\begin{equation} \\eta(\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right], \\end{equation} where \\begin{equation} s_0\\sim\\rho_0(s_0),\\hspace{1cm}a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t) \\end{equation} For a policy $\\pi$, the state value function, denoted as $V_\\pi$, of a state $s\\in\\mathcal{S}$ measures how good it is for the agent to be in $s$, and the action value function, referred as $Q_\\pi$, of a state-action pair $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as \\begin{align} V_\\pi(s_t)\u0026amp;=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\\\ Q_\\pi(s_t,a_t)\u0026amp;=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\end{align} where \\begin{equation} a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t)\\hspace{1cm}t\\geq 0 \\end{equation} Along with these value functions, we will also define the advantage function for $\\pi$, denoted $A_\\pi$, given as \\begin{equation} A_\\pi(s_t,a_t)=Q_\\pi(s_t,a_t)-V_\\pi(s_t) \\end{equation}\nCoupling \u0026amp; Total variation distance Consider two probability measures $\\mu$ and $\\nu$ on a probability space $(\\Omega,\\mathcal{F},P)$. One refers a coupling of $\\mu$ and $\\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\\mu$ and $\\nu$.\nSpecifically, if $p$ is a joint distribution of $X,Y$ on $\\Omega$, then it implies that \\begin{align} \\sum_{y\\in\\Omega}p(x,y)\u0026amp;=\\sum_{y\\in\\Omega}P(X=x,Y=y)=P(X=x)=\\mu(x) \\\\ \\sum_{x\\in\\Omega}p(x,y)\u0026amp;=\\sum_{x\\in\\Omega}P(X=x,Y=y)=P(Y=y)=\\nu(y) \\end{align} For probability distributions $\\mu$ and $\\nu$ on $\\Omega$ as above, the total variation distance between $\\mu$ and $\\nu$, denoted $\\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}$, is defined by \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}\\doteq\\sup_{A\\subset\\Omega}\\big\\vert\\mu(A)-\\nu(A)\\big\\vert \\end{equation} Proposition 1\nLet $\\mu$ and $\\nu$ be probability distributions on $\\Omega$, we then have \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\frac{1}{2}\\sum_{x\\in\\Omega}\\big\\vert\\mu(x)-\\nu(x)\\big\\vert \\end{equation} Proof\nLet $B=\\{x:\\mu(x)\\geq\\nu(x)\\}$ and let $A\\subset\\Omega$. We have \\begin{align} \\mu(A)-\\nu(A)\u0026amp;=\\mu(A\\cap B)+\\mu(A\\cap B^c)-\\nu(A\\cap B)-\\nu(A\\cap B^c) \\\\ \u0026amp;\\leq\\mu(A\\cap B)-\\nu(A\\cap B) \\\\ \u0026amp;\\leq\\mu(B)-\\nu(B) \\end{align} Analogously, we also have \\begin{equation} \\nu(A)-\\mu(A)\\leq\\nu(B^c)-\\mu(B^c) \\end{equation} Hence, combining these results gives us \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\frac{1}{2}\\left(\\mu(B)-\\nu(B)+\\nu(B^c)-\\mu(B^c)\\right)=\\frac{1}{2}\\sum_{x\\in\\Omega}\\big\\vert\\mu(x)-\\nu(x)\\big\\vert \\end{equation} This proof also implies that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\sum_{x\\in\\Omega;,\\mu(x)\\geq\\nu(x)}\\mu(x)-\\nu(x) \\end{equation} Proposition 2\nLet $\\mu$ and $\\nu$ be two probability measures defined in a probability space $\\Omega$, we then have that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\inf_{(X,Y)\\text{ coupling of }\\mu,\\nu}P(X\\neq Y) \\end{equation} Proof\nFor any $A\\subset\\Omega$ and for any coupling $(X,Y)$ of $\\mu$ and $\\nu$ we have \\begin{align} \\mu(A)-\\nu(A)\u0026amp;=P(X\\in A)-P(Y\\in A) \\\\ \u0026amp;=P(X\\in A,Y\\notin A)+P(X\\in A,Y\\in A)-P(Y\\in A) \\\\ \u0026amp;\\leq P(X\\in A,Y\\notin A) \\\\ \u0026amp;\\leq P(X\\neq Y), \\end{align} which implies that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\sup_{A\u0026rsquo;\\subset\\Omega}\\big\\vert\\mu(A\u0026rsquo;)-\\nu(A\u0026rsquo;)\\big\\vert\\leq P(X\\neq Y)\\leq\\inf_{(X,Y)\\text{ coupling of }\\mu,\\nu}P(X\\neq Y) \\end{equation} Thus, it suffices to construct a coupling $(X,Y)$ of $\\mu$ and $\\nu$ such that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=P(X\\neq Y) \\end{equation}\nPolicy improvement We begin by proving an identity that expresses the expected return $\\eta(\\tilde{\\pi})$ of a policy $\\tilde{\\pi}$ in terms of the advantage over another policy $\\pi$, accumulated over time steps.\nLemma 3\nGiven two policies $\\pi,\\tilde{\\pi}$, we have \\begin{equation} \\eta(\\tilde{\\pi})=\\eta(\\pi)+\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t A_\\pi(s_t,a_t)\\right]\\label{eq:pi.1} \\end{equation} Proof\nBy definition of advantage function $A_\\pi$ of policy $\\pi$, we have \\begin{align} \\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t A_\\pi(s_t,a_t)\\right]\u0026amp;=\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left(Q_\\pi(s_t,a_t)-V_\\pi(s_t)\\right)\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\big(r(s_t,a_t)+\\gamma V_\\pi(s_{t+1})-V_\\pi(s_t)\\big)\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\tilde{\\pi}}\\left[-V_\\pi(s_0)+\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right] \\\\ \u0026amp;=-\\mathbb{E}_{s_0}\\big[V_\\pi(s_0)\\big]+\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right] \\\\ \u0026amp;=-\\eta(\\pi)+\\eta(\\tilde{\\pi}), \\end{align} where in the third step, since $\\gamma\\in(0,1)$ as $t\\to\\infty$, we have that $\\gamma^t V_\\pi(s_{t+1})\\to 0$.\nLet $\\rho_\\pi$ be the unnormalized discounted visitation frequencies for state $s$: \\begin{equation} \\rho_\\pi(s)\\doteq P(s_0=s)+\\gamma P(s_1=s)+\\gamma^2 P(s_2=s)+\\ldots \\end{equation} where $s_0\\sim\\rho_0$ and the actions are chosen according to $\\pi$. This allows us to rewrite \\eqref{eq:pi.1} as \\begin{align} \\eta(\\tilde{\\pi})\u0026amp;=\\eta(\\pi)+\\sum_{t=0}^{\\infty}\\sum_{s}P(s_t=s\\vert\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\ \u0026amp;=\\eta(\\pi)+\\sum_{s}\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s\\vert\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a) \\\\ \u0026amp;=\\eta(\\pi)+\\sum_{s}\\rho_\\tilde{\\pi}(s)\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\label{eq:pi.2} \\end{align} This result implies that any policy update $\\pi\\to\\tilde{\\pi}$ that has a nonnegative expected advantage at every state $s$, i.e. $\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\geq 0$, is guaranteed to make an improvement on $\\eta$ (or unchanged in case the expected advantage take the value of zero for every $s$). By letting $\\tilde{\\pi}$ be the deterministic policy that \\begin{equation} \\tilde{\\pi}(s)=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}A_\\pi(s,a), \\end{equation} we obtain the policy improvement result used in policy iteration.\nHowever, there are cases when \\eqref{eq:pi.2} is difficult to be optimized, especially when the expected advantage is negative, i.e. $\\sum_a\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)$, due to estimation and approximation error in the approximate setting. We instead consider a local approximation to $\\eta$: \\begin{equation} L_\\pi(\\tilde{\\pi})=\\eta(\\pi)+\\sum_s\\rho_\\pi(s)\\sum_a\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\label{eq:pi.5} \\end{equation}\nIf $\\pi$ is a policy parameterized by $\\theta$, in which $\\pi_\\theta(a\\vert s)$ s differentiable w.r.t $\\theta$, we then have for any parameter value $\\theta_0$ \\begin{align} L_{\\pi_{\\theta_0}}(\\pi_{\\theta_0})\u0026amp;=\\eta(\\pi_{\\theta_0}) \\\\ \\nabla_\\theta L_{\\pi_{\\theta_0}}(\\pi_\\theta)\\big\\vert_{\\theta=\\theta_0}\u0026amp;=\\nabla_\\theta\\eta(\\pi_\\theta)\\big\\vert_{\\theta=\\theta_0},\\label{eq:pi.6} \\end{align} which suggests that a sufficiently small step $\\pi_{\\theta_0}\\to\\tilde{\\pi}$ that leads to an improvement on $L_{\\pi_{\\theta_\\text{old}}}$ will also make an improvement on $\\eta$.\nTo measure the improvement on updating $\\pi_\\text{old}\\to\\pi_\\text{new}$, we choose the total variance distance metric, as defined above with an observation that each policy $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$ can be viewed as a distribution function defined on $\\mathcal{S}\\times\\mathcal{A}$. Thus, those results and definitions mentioned above for probability measures $\\mu$ and $\\nu$ defined on $\\Omega$ can also be applied to policies $\\pi$ and $\\tilde{\\pi}$ specified on $\\mathcal{S}\\times\\mathcal{A}$.\nIn addition, we need to define some notations:\nLet \\begin{equation} \\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_{\\text{TV}}^{\\text{max}}\\doteq\\max_s\\big\\Vert\\pi(\\cdot\\vert s)-\\tilde{\\pi}(\\cdot\\vert s)\\big\\Vert_\\text{TV} \\end{equation} A policy pair $(\\pi,\\tilde{\\pi})$ is referred as $\\alpha$-coupled if it defines a joint distribution $(a,\\tilde{a})\\vert s$ such that \\begin{equation} P(a\\neq\\tilde{a}\\vert s)\\leq\\alpha,\\hspace{1cm}\\forall s \\end{equation} $\\pi$ and $\\tilde{\\pi}$ will respectively denote the marginal distributions of $a$ and $\\tilde{a}$.\nProposition 4\nLet $(\\pi,\\tilde{\\pi})$ be $\\alpha$-coupled policy pair, for all $s$, we have \\begin{equation} \\big\\vert\\bar{A}(s)\\big\\vert\\leq 2\\alpha\\max_{s,\\tilde{a}}\\big\\vert A_\\pi(s,\\tilde{a})\\big\\vert, \\end{equation} where $\\bar{A}(s)$ is the expected advantage of $\\tilde{\\pi}$ over $\\pi$ at state $s$, given as \\begin{equation} \\bar{A}(s)=\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})\\big] \\end{equation} Proof\nBy definition of the advantage function, it is easily noticed that $\\mathbb{E}_{a\\sim\\pi}\\big[A_\\pi(s,a)\\big]=0$, which lets us obtain \\begin{align} \\bar{A}(s)\u0026=\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})\\big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi,\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})-A_\\pi(s,a)\\big] \\\\ \u0026=P(a\\neq\\tilde{a}\\vert s)\\mathbb{E}_{a\\sim\\pi,\\tilde{a}\\sim\\tilde{\\pi}\\vert a\\neq\\tilde{a}}\\big[A_\\pi(s,\\tilde{a})-A_\\pi(s,a)\\big], \\end{align} which by definition of $\\alpha$-coupling implies that \\begin{equation} \\big\\vert\\bar{A}(s)\\big\\vert\\leq\\alpha\\cdot 2\\max_{s,\\tilde{a}}\\big\\vert A_\\pi(s,\\tilde{a})\\big\\vert \\end{equation} Theorem 5\nLet $\\alpha=\\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_\\text{TV}^\\text{max}$. The following holds \\begin{equation} \\eta(\\tilde{\\pi})\\geq L_\\pi(\\tilde{\\pi})-\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\end{equation} where \\begin{equation} \\epsilon=\\max_{s,a}\\big\\vert A_\\pi(s,a)\\big\\vert \\end{equation} Proof\nOn the other hand, by Pinsker\u0026rsquo;s inequality, which bounds the total variation distance in terms of the Kullback-Leibler divergence, denoted $D_\\text{KL}$, we have that \\begin{equation} \\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_\\text{TV}^2\\leq\\frac{1}{2}D_\\text{KL}(\\pi\\Vert\\tilde{\\pi})\\leq D_\\text{KL}(\\pi\\Vert\\tilde{\\pi}),\\label{eq:pi.3} \\end{equation} since $D_\\text{KL}(\\cdot\\Vert\\cdot)\\geq 0$. Thus, let \\begin{equation} D_\\text{KL}^\\text{max}(\\pi,\\tilde{\\pi})\\doteq\\max_s D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\tilde{\\pi}(\\cdot\\vert s)\\big), \\end{equation} with the result \\eqref{eq:pi.3} and by Theorem 5, we have \\begin{equation} \\eta(\\tilde{\\pi})\\geq L_\\pi(\\tilde{\\pi})-CD_\\text{KL}^\\text{max}(\\pi,\\tilde{\\pi}),\\label{eq:pi.4} \\end{equation} where \\begin{equation} C=\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2} \\end{equation} The policy improvement bound \\eqref{eq:pi.4} allows us to specify a policy iteration, as given in the following pseudocode\nIt is worth noticing that \\eqref{eq:pi.4} allows the policy iteration above to guarantee to generating a sequence of non-decreasing expected returns \\begin{equation} \\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\eta(\\pi_2)\\leq\\ldots \\end{equation} To see this, let \\begin{equation} M_i(\\pi)\\doteq L_{\\pi_i}(\\pi)-CD_\\text{KL}^\\text{max}(\\pi_i,\\pi), \\end{equation} by \\eqref{eq:pi.4}, we then have \\begin{equation} \\eta(\\pi_{i+1})\\geq M_i(\\pi_{i+1}), \\end{equation} which implies that \\begin{equation} \\eta(\\pi_{i+1})-\\eta(\\pi_i)=\\eta(\\pi_{i+1})-M_i(\\pi_i)\\geq M_i(\\pi_{i+1})-M_i(\\pi_i) \\end{equation} Parameterized Policy Optimization by Trust Region We now consider the policy optimization problem in which the policy is parameterized by $\\theta$.\nWe begin by simplifying notations. In particular, let $\\eta(\\theta)\\doteq\\eta(\\pi_\\theta)$, let $L_\\theta(\\tilde{\\theta})\\doteq L_{\\pi_\\theta}(\\pi_\\tilde{\\theta})$ and $D_\\text{KL}(\\theta\\Vert\\tilde{\\theta})\\doteq D_\\text{KL}(\\pi_\\theta\\Vert\\pi_\\tilde{\\theta})$, which allows us to represent \\begin{equation} D_\\text{KL}^\\text{max}(\\theta,\\tilde{\\theta})\\doteq D_\\text{KL}^\\text{max}(\\pi_\\theta,\\pi_\\tilde{\\theta})=\\max_s D_\\text{KL}\\big(\\pi_\\theta(\\cdot\\vert s)\\Vert\\pi_\\tilde{\\theta}(\\cdot\\vert s)\\big) \\end{equation} Also let $\\theta_\\text{old}$ denote the previous policy parameters that we want to improve. Hence, by the previous section, we have \\begin{equation} \\eta(\\theta)\\geq L_{\\theta_\\text{old}}(\\theta)-CD_\\text{KL}^\\text{max}(\\theta_\\text{old},\\theta), \\end{equation} where the equality holds at $\\theta=\\theta_\\text{old}$. This means, we get a guaranteed improvement to the true objective function $\\eta$ by solving the following optimization problem \\begin{equation} \\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\big[L_{\\theta_\\text{old}}(\\theta)-CD_\\text{KL}^\\text{max}(\\theta_\\text{old},\\theta)\\big] \\end{equation} To speed up the algorithm, we make some robust modification. Specifically, we instead solve a trust region problem: \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026amp;\\hspace{0.2cm}L_{\\theta_\\text{old}}(\\theta)\\nonumber \\\\ \\text{s.t.}\u0026amp;\\hspace{0.2cm}\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\leq\\delta,\\label{eq:ppo.1} \\end{align} where $\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}$ is the average KL divergence, given as \\begin{equation} \\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big] \\end{equation} Let us pay attention to our objective function, $L_{\\theta_\\text{old}}(\\theta)$, for a while. By the definition of $L$, given in \\eqref{eq:pi.5}, combined with using an importance sampling estimator, we can rewrite the objective function of \\eqref{eq:ppo.1} as \\begin{align} L_{\\theta_\\text{old}}(\\theta)\u0026amp;=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\sum_a\\pi_\\theta(a\\vert s)A_{\\theta_\\text{old}}(s,a) \\\\ \u0026amp;=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{align} where $A_{\\theta_\\text{old}}\\doteq A_{\\pi_{\\theta_\\text{old}}}$; and $q$ represents the sampling distribution. The trust region problem now is given as \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026amp;\\hspace{0.2cm}\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\\\ \\text{s.t.}\u0026amp;\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big]\\leq\\delta \\end{align} which is thus equivalent to12 \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026amp;\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\\\ \\text{s.t.}\u0026amp;\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big]\\leq\\delta\\label{eq:ppo.2} \\end{align}\nSolving the optimization problem Let us take a closer look on how to solve this trust region constrained optimization problem. We begin by letting \\begin{equation} \\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\pi_{\\theta_\\text{old}}(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{equation} Consider Taylor expansion of the objective function $\\mathcal{L}_{\\theta_\\text{old}}(\\theta)$ about $\\theta=\\theta_\\text{old}$ to the first order, we thus can linearly approximate the objective function by the policy gradient, $\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})$, as \\begin{align} \\mathcal{L}_{\\theta_\\text{old}}(\\theta)\u0026amp;\\approx\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]+(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026amp;\\overset{\\text{(i)}}{=}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}(\\theta-\\theta_\\text{old})^\\text{T}\\left[\\frac{1}{1-\\gamma}\\nabla_\\theta L_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}}\\right] \\\\ \u0026amp;\\overset{\\text{(iii)}}{=}\\frac{1}{1-\\gamma}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026amp;\\underset{\\max_\\theta}{\\propto}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}},\\label{eq:st.1} \\end{align} where\nThis step is due to definition of advantage function for a policy $\\pi$, we have that $\\mathbb{E}_{a\\sim\\pi}\\big[A_\\pi(s,a)\\big]=0$, which implies that \\begin{align} \\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]\u0026=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\mathbb{E}_{a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\big[0\\big]=0 \\end{align} This step uses the same logic as we have used in \\eqref{eq:fn.1}. This step is due to \\eqref{eq:pi.6}. To get an approximation of the constraint, we fist consider the Taylor expansion of the KL divergence $D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)$ about $\\theta=\\theta_\\text{old}$ to the second order, which, given a state $s$, gives us a quadratic approximation: \\begin{align} \u0026amp;\\hspace{-0.7cm}D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\nonumber \\\\ \u0026amp;=\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)-\\log\\pi_\\theta(\\cdot\\vert s)\\Big] \\\\ \u0026amp;\\approx\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Bigg[\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)-\\Big(\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)+(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\left.\\frac{1}{2}(\\theta_\\text{old}-\\theta)^\\text{T}\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}(\\theta_\\text{old}-\\theta)\\right)\\Bigg] \\\\ \u0026amp;\\overset{\\text{(i)}}{=}-\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\left[\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}(\\theta-\\theta_\\text{old})\\right] \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right),\\label{eq:st.2} \\end{align} where\nBy chain rule, we have \\begin{align} \\hspace{-0.7cm}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\left[(\\theta_\\text{old}-\\theta)^\\text{T}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\right]\u0026=\\sum_s\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)(\\theta_\\text{old}-\\theta)^\\text{T}\\frac{\\nabla_\\theta\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}}{\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\sum_s\\nabla_\\theta\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\left.\\left(\\nabla_\\theta\\sum_s\\pi_\\theta(\\cdot\\vert s)\\right)\\right\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}(\\nabla_\\theta 1)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\mathbf{0}=0 \\end{align} This step goes with same logic as we have used in Natural evolution strategies, which let us claim that \\begin{equation} -\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\Big]=\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big] \\end{equation} Given the Taylor series approximation \\eqref{eq:st.2}, we can locally approximate $\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)$ as \\begin{align} \u0026amp;\\hspace{-0.5cm}\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\nonumber \\\\ \u0026amp;\\approx\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right)\\right] \\\\ \u0026amp;=\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right) \\\\ \u0026amp;=\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbf{F}(\\theta-\\theta_\\text{old}),\\label{eq:st.3} \\end{align} where the matrix \\begin{equation} \\mathbf{F}\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big] \\end{equation} is referred as the Fisher information matrix, which, worthy remarking, is symmetric.\nAs acquired results \\eqref{eq:st.1} and \\eqref{eq:st.3}, we yield an approximate problem \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026amp;\\hspace{0.2cm}\\Delta\\theta^\\text{T}g=\\tilde{\\mathcal{L}}(\\theta)\\nonumber \\\\ \\text{s.t.}\u0026amp;\\hspace{0.2cm}\\frac{1}{2}\\Delta\\theta^\\text{T}\\mathbf{F}\\Delta\\theta\\leq\\delta,\\label{eq:st.4} \\end{align} where we have let $\\Delta\\theta\\doteq\\theta-\\theta_\\text{old}$ and $g\\doteq\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}}$ denote the policy gradient to simplify our notations.\nNatural policy gradient Consider the problem \\eqref{eq:st.4}, we have the Lagrangian3 associated with the our constrained optimization problem is given by \\begin{equation} \\bar{\\mathcal{L}}(\\Delta\\theta,\\lambda)=-\\Delta\\theta^\\text{T}g+\\lambda\\left(\\frac{1}{2}\\Delta\\theta^\\text{T}\\mathbf{F}\\Delta\\theta-\\delta\\right) \\end{equation} which can be minimized w.r.t $\\Delta\\theta$ by taking the gradient of the Lagrangian w.r.t $\\Delta\\theta$ \\begin{equation} \\nabla_{\\Delta\\theta}\\overline{\\mathcal{L}}(\\Delta\\theta,\\lambda)=-g+\\lambda\\mathbf{F}\\Delta\\theta, \\end{equation} and setting this gradient to zero, which yields \\begin{equation} \\Delta\\theta=\\frac{1}{\\lambda}\\mathbf{F}^{-1}g\\label{eq:np.1} \\end{equation} The dual function4 then is given by \\begin{align} \\overline{g}(\\lambda)\u0026amp;=-\\frac{1}{\\lambda}g^\\text{T}\\mathbf{F}^{-1}g+\\frac{1}{2\\lambda}g^\\text{T}\\mathbf{F}^{-1}\\mathbf{F}\\mathbf{F}^{-1}g-\\lambda\\delta \\\\ \u0026amp;=-\\frac{1}{2\\lambda}g^\\text{T}\\mathbf{F}^{-1}g-\\lambda\\delta, \\end{align} Letting the gradient of dual function w.r.t $\\lambda$ \\begin{equation} \\nabla_\\lambda\\overline{g}(\\lambda)=\\frac{1}{2}g^\\text{T}\\mathbf{F}^{-1}g\\cdot\\frac{1}{\\lambda^2}-\\delta \\end{equation} be zero and solving for $\\lambda$ gives us the Lagrange multiplier that maximizes $\\overline{g}$, which is \\begin{equation} \\lambda=\\sqrt{\\frac{g^\\text{T}\\mathbf{F}^{-1}g}{2\\delta}} \\end{equation} The vector $\\Delta\\theta$ given in \\eqref{eq:np.1} that solves the optimization problem \\eqref{eq:st.4} defines the a search direction, which is the direction of the natural policy gradient, i.e. $\\tilde{\\nabla}_\\theta\\tilde{\\mathcal{L}}(\\theta)=\\mathbf{F}^{-1}g$.\nHence, using this gradient to iteratively update $\\theta$ gives us \\begin{equation} \\theta_{k+1}:=\\theta_k+\\lambda^{-1}\\tilde{\\nabla}_\\theta\\tilde{\\mathcal{L}}(\\theta)=\\theta_k+\\sqrt{\\frac{2\\delta}{g^\\text{T}\\mathbf{F}^{-1}g}}\\mathbf{F}^{-1}g\\label{eq:np.2} \\end{equation}\nLine search A problem with the above algorithm is that there exist approximation errors because of the Taylor expansion we have used. This consequently might not give us an improvement of the objective or the updated $\\pi_\\theta$ may not satisfy the KL constraint due to taking large steps.\nTo overcome this, we use a line search by adding an exponential decay to the update rule \\eqref{eq:np.2} that \\begin{equation} \\theta_{k+1}:=\\theta_k+\\alpha^j\\sqrt{\\frac{2\\delta}{g^\\text{T}\\mathbf{F}^{-1}g}}\\mathbf{F}^{-1}g,\\label{eq:ls.1} \\end{equation} where $\\alpha\\in(0,1)$ is the decay coefficient and $j$ is the smallest nonnegative integer that make an improvement on the objective, while let $\\pi_{\\theta_{k+1}}$ satisfy the KL constraint as well.\nCompute $\\mathbf{F}^{-1}g$ Since both the step size and direction of the update \\eqref{eq:ls.1} relate to $\\mathbf{F}^{-1}g$, it is then necessary to take into account the computation of this product.\nRather than computing the inverse $\\mathbf{F}^{-1}$ of the Fisher information matrix, then multiply it with the gradient vector $g$ to obtain the natural gradient $\\mathbf{F}^{-1}g$, we find a vector $x$ such that \\begin{equation} \\mathbf{F}x=g,\\label{eq:cfig.1} \\end{equation} which implies that $x=\\mathbf{F}^{-1}g$.\nThe problem now remains to solve the linear equation \\eqref{eq:cfig.1}, which can be approximately solved by Conjugate gradient method with a predefined number of iterations.\nSampled-based estimation The objective and constraint functions of \\eqref{eq:ppo.2} can be approximated using Monte Carlo simulation. Following are two possible sampling approaches to construct the estimated objective and constraint functions.\nSingle path This sampling scheme has the following procedure\nSample $s_0\\sim\\rho_0$ to get a set of $m$ start states $\\mathcal{S}_0=\\{s_0^{(1)},\\ldots,s_0^{(m)}\\}$. For each $s_0^{(i)}\\in\\mathcal{S}_0$, generate a trajectory $\\tau^{(i)}=\\big(s_0^{(i)},a_0^{(i)},s_1^{(i)},a_1^{(i)},\\ldots,s_{T-1}^{(i)},a_{T-1}^{(i)},s_T^{(i)}\\big)$ by rolling out the policy $\\pi_{\\theta_\\text{old}}$ for $T$ steps. Thus $q(a^{(i)}\\vert s^{(i)})=\\pi_{\\theta_\\text{old}}(a^{(i)}\\vert s^{(i)})$. At each state-action pair $(s_t^{(i)},a_t^{(i)})$, compute the action-value function $Q_{\\theta_\\text{old}}(s,a)$ by taking the discounted sum of future rewards along $\\tau^{(i)}$. Vine This sampling approach follows the following process\nSample $s_0\\sim\\rho_0$ and simulate the policy $\\pi_{\\theta_i}$ to generate $m$ trajectories. Choose a rollout set, which is a subset $s_1,\\ldots,s_N$ of $N$ states along the trajectories. For each state $s_n$ with $1\\leq n\\leq N$, sample $K$ actions according to $a_{n,k}\\sim q(\\cdot\\vert s_n)$, where $q(\\cdot\\vert s_n)$ includes the support of $\\pi_{\\theta_i}(\\cdot\\vert s_n)$. For each action $a_{n,k}$, estimate $\\hat{Q}_{\\theta_i}(s_n,a_{n,k})$ by performing a rollout starting from $s_n$ and taking action $a_{n,k}$ Given the estimated action-value function, $\\hat{Q}_{\\theta_i}(s_n,a_{n,k})$, for each state-action pair $(s_n,a_{n,k})$, compute the estimator, $L_n(\\theta)$, of $L_{\\theta_\\text{old}}$ at state $s_n$ as: For small, finite action spaces, in which generating a rollout for every possible action from a given state is possible, thus \\begin{equation} L_n(\\theta)=\\sum_{k=1}^{K}\\pi_\\theta(a_k\\vert s_n)\\hat{Q}(s_n,a_k), \\end{equation} where $\\mathcal{A}=\\{a_1,\\ldots,a_K\\}$ is the action space. For large or continuous state spaces, use importance sampling \\begin{equation} L_n(\\theta)=\\frac{\\sum_{k=1}^{K}\\frac{\\pi_\\theta(a_{n,k}\\vert s_n)}{\\pi_{\\theta_\\text{old}}(a_{n,k}\\vert s_n)}\\hat{Q}(s_n,a_{n,k})}{\\sum_{k=1}^{K}\\frac{\\pi_\\theta(a_{n,k}\\vert s_n)}{\\pi_{\\theta_\\text{old}}(a_{n,k}\\vert s_n)}}, \\end{equation} assuming that $K$ actions $a_{n,1},\\ldots,a_{n,K}$ are performed from state $s_n$. Average over $s_n\\sim\\rho(\\pi)$ to obtain an estimator for $L_{\\theta_\\text{old}}$, as well the policy gradient. Final algorithm References [1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. Trust Region Policy Optimization. ICML'15, pp 1889–1897, 2015.\n[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. Markov chains and mixing times. American Mathematical Society, 2009.\n[3] Sham Kakade, John Langford. Approximately optimal approximate reinforcement learning. ICML'2, pp. 267–274, 2002.\n[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv:1707.06347, 2017.\n[5] Stephen Boyd \u0026amp; Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[6] Sham Kakade. A Natural Policy Gradient. NIPS 2001.\nFootnotes To be more specific, by definition of the advantage, i.e. $A_{\\theta_\\text{old}}(s,a)=Q_{\\theta_\\text{old}}(s,a)-V_{\\theta_\\text{old}}(s)$, we have: \\begin{align} \\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\u0026amp;=\\mathbb{E}_{s\\sim\\rho_{\\text{old}}}\\left[\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\right]\\nonumber \\\\ \u0026amp;\\underset{\\max_\\theta}{\\propto}\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\right]\\label{eq:fn.1} \\\\ \u0026amp;=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\end{align} where we have used the notation \\begin{equation} \\text{LHS}\\underset{\\max_\\theta}{\\propto}\\text{RHS}\\nonumber \\end{equation} to denote that the problem $\\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\text{LHS}$ is equivalent to $\\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\text{RHS}$. Also, the second step comes from definition of $\\rho_\\pi$, i.e. for $s_0\\sim\\rho_0$ and the actions are chosen according to $\\pi$, we have \\begin{equation*} \\rho_\\pi(s)=P(s_0=s)+\\gamma P(s_1=s)+\\gamma^2 P(s_2=s)+\\ldots, \\end{equation*} which implies that by summing across all $s$, we obtain \\begin{align*} \\sum_{s}\\rho_\\pi(s)\u0026amp;=\\sum_s P(s_0=s)+\\gamma\\sum_s P(s_1=s)+\\gamma^2\\sum_s P(s_2=s)+\\ldots \\\\ \u0026amp;=1+\\gamma+\\gamma^2+\\ldots \\\\ \u0026amp;=\\frac{1}{1-\\gamma} \\end{align*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn the original TRPO paper, the authors used the state-action value function $Q_{\\theta_\\text{old}}$ rather than the advantage $A_{\\theta_\\text{old}}$ since by definition, $A_{\\theta_\\text{old}}(s,a)=Q_{\\theta_\\text{old}}(s,a)-V_{\\theta_\\text{old}}(s)$, which lets us obtain \\begin{align*} \u0026amp;\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\text{old}}(s,a)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}\\big(A_{\\theta_\\text{old}}(s,a)+V_{\\theta_\\text{old}}(s)\\big)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]+\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[V_{\\theta_\\text{old}}(s)\\sum_{a}\\pi_\\theta(a\\vert s)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]+\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\big[V_{\\theta_\\text{old}}(s)\\big] \\\\ \u0026amp;\\underset{\\max_\\theta}{\\propto}\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{align*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe Lagrangian, $\\bar{\\mathcal{L}}$, here should not be confused with the objective function $\\mathcal{L}$ due to their notations. It was just a notation-abused problem, in which normally the Lagrangian is denoted as $\\mathcal{L}$, which has been already used.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe dual function is usually denoted by $g$, which has unfortunately been taken by the policy gradient. Thus, we abuse the notation once again by representing it as $\\overline{g}$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/trpo/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on TRPO.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Trust Region Policy Optimization"},{"content":" Notes on DQN.\nQ-value iteration Recall that in the note Markov Decision Processes, Bellman equations, we have defined the state-value function for a policy $\\pi$ to measure how good the state $s$ is, given as \\begin{equation} V_\\pi(s)=\\sum_{a}\\pi(a\\vert s)\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\big[R(s,a,s\u0026rsquo;)+\\gamma V_\\pi(s\u0026rsquo;)\\big] \\end{equation} From the definition of $V_\\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$: \\begin{equation} V^*(s)=\\max_{a}\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\big[R(s,a,s\u0026rsquo;)+\\gamma V^*(s\u0026rsquo;)\\big],\\label{eq:qvi.1} \\end{equation} which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s\u0026rsquo;$.\nThen, with Dynamic programming, we can solve \\eqref{eq:qvi.1} by an iterative method, called value iteration, given as \\begin{equation} V_{t+1}(s)=\\max_{a}\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\big[R(s,a,s\u0026rsquo;)+\\gamma V_t(s\u0026rsquo;)\\big]\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\\{V_t\\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the Banach\u0026rsquo;s fixed point theorem, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.\nDetails for value iteration method can be seen in the following pseudocode.\nRemember that along with the state-value function $V_\\pi(s)$, we have also defined the action-value function, or Q-values for a policy $\\pi$, denoted $Q$, given by \\begin{align} Q_\\pi(s,a)\u0026amp;=\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\left[R(s,a,s\u0026rsquo;)+\\gamma\\sum_{a\u0026rsquo;}\\pi(a\u0026rsquo;\\vert s\u0026rsquo;)Q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\right] \\\\ \u0026amp;=\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\big[R(s,a,s\u0026rsquo;)+\\gamma V_\\pi(s\u0026rsquo;)\\big] \\end{align} which measures how good it is to be in state $s$ and take action $a$.\nAnalogously, we also have the Bellman equation for the optimal action-value function, given as \\begin{align} Q^*(s,a)\u0026amp;=\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\left[R(s,a,s\u0026rsquo;)+\\gamma\\max_{a\u0026rsquo;}Q^*(s\u0026rsquo;,a\u0026rsquo;)\\right]\\label{eq:qvi.2} \\\\ \u0026amp;=\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\big[R(s,a,s\u0026rsquo;)+\\gamma V^*(s\u0026rsquo;)\\big]\\label{eq:qvi.3} \\end{align} The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\\pi^*$, thereafter.\nEquation \\eqref{eq:qvi.3} allows us to write \\begin{equation} V^*(s)=\\max_a Q^*(s,a) \\end{equation} Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \\eqref{eq:qvi.2}, called Q-value iteration. The method is given by the update rule \\begin{equation} Q_{t+1}(s,a)=\\sum_{s\u0026rsquo;}P(s\u0026rsquo;\\vert s,a)\\left[R(s,a,s\u0026rsquo;)+\\gamma\\max_{a\u0026rsquo;}Q_t(s\u0026rsquo;,a\u0026rsquo;)\\right]\\label{eq:qvi.4} \\end{equation} This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.\nQ-learning The update formula \\eqref{eq:qvi.4} can be rewritten as an expected update \\begin{equation} Q_{t+1}(s,a)=\\mathbb{E}_{s\u0026rsquo;\\sim P(s\u0026rsquo;\\vert s,a)}\\left[R(s,a,s\u0026rsquo;)+\\gamma\\max_{a\u0026rsquo;}Q_t(s\u0026rsquo;,a\u0026rsquo;)\\right]\\label{eq:ql.1} \\end{equation} It is noticeable that the above update rule requires the transition model $P(s\u0026rsquo;\\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \\eqref{eq:ql.1} can be approximated by sampling, as\nAt a state, taking (sampling) action $a$ (e.g. due to an $\\varepsilon$-greedy policy), we get the next state: \\begin{equation} s'\\sim P(s'\\vert s,a) \\end{equation} Consider the old estimate $Q_t(s,a)$. Consider the new sample estimate (target): \\begin{equation} Q_\\text{target}=R(s,a,s')+\\gamma\\max_{a'}Q_t(s',a')\\label{eq:ql.2} \\end{equation} Append the new estimate into a running average to iteratively update Q-values: \\begin{align} Q_{t+1}(s,a)\u0026=(1-\\alpha)Q_t(s,a)+\\alpha Q_\\text{target} \\\\ \u0026=(1-\\alpha)Q_t(s,a)+\\alpha\\left[R(s,a,s')+\\gamma\\max_{a'}Q_t(s',a')\\right] \\end{align} This update rule is in form of a stochastic process, and thus, is guaranteed to converge to the optimal $Q^*$, under the stochastic approximation conditions for the learning rate $\\alpha$. \\begin{equation} \\sum_{t=1}^{\\infty}\\alpha_t(s,a)=\\infty\\hspace{1cm}\\text{and}\\hspace{1cm}\\sum_{t=1}^{\\infty}\\alpha_t^2(s,a)\u0026lt;\\infty,\\label{eq:ql.3} \\end{equation} for all $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$.\nThe method is so called Q-learning, with pseudocode given below.\nNeural networks with Q-learning As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an approximated solution.\nIn particular, we have tried to find an approximated action-value function $Q_\\boldsymbol{\\theta}(s,a)$, parameterized by a learnable vector $\\boldsymbol{\\theta}$, of the action-value function $Q(s,a)$, as \\begin{equation} Q_\\boldsymbol{\\theta}(s,a) \\end{equation} Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\\boldsymbol{\\theta}$ so as to minimize the loss function \\begin{equation} L(\\boldsymbol{\\theta})=\\mathbb{E}_{s,a\\sim\\mu(\\cdot)}\\Big[\\big(Q(s,a)-Q_\\boldsymbol{\\theta}(s,a)\\big)^2\\Big] \\end{equation} The resulting SGD update had the form \\begin{align} \\boldsymbol{\\theta}_{t+1}\u0026amp;=\\boldsymbol{\\theta}_t-\\frac{1}{2}\\alpha\\nabla_\\boldsymbol{\\theta}\\big[Q(s_t,a_t)-Q_\\boldsymbol{\\theta}(s_t,a_t)\\big]^2 \\\\ \u0026amp;=\\boldsymbol{\\theta}_t+\\alpha\\big[Q(s_t,a_t)-Q_\\boldsymbol{\\theta}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.1} \\end{align} However, we could not perform the exact update \\eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $y_t$, which can be any approximation of $Q(s_t,a_t)$1: \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[y_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.2} \\end{equation}\nLinear function approximation Recall that, we have applied linear methods as our function approximators: \\begin{equation} Q_\\boldsymbol{\\theta}(s,a)=\\boldsymbol{\\theta}^\\text{T}\\mathbf{f}(s,a), \\end{equation} where $\\mathbf{f}(s,a)$ represents the feature vector, (or basis functions) of the state-action pair $(s,a)$. Linear function approximation allowed us to rewrite \\eqref{eq:nql.2} in a simplified form \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[y_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\mathbf{f}(s_t,a_t)\\label{eq:nql.3} \\end{equation} The corresponding SGD method for Q-learning and Q-learning with linear function approximation are respectively given in form of \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\left[R(s_t,a_t,s_{t+1})+\\gamma\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a\u0026rsquo;)-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\right]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.4} \\end{equation} and \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\left[R(s_t,a_t,s_{t+1})+\\gamma\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a\u0026rsquo;)-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\right]\\mathbf{f}(s_t,a_t),\\label{eq:nql.5} \\end{equation} which both replace the $Q_\\text{target}$ in \\eqref{eq:ql.2} by the one parameterized by $\\boldsymbol{\\theta}$ \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a\u0026rsquo;) \\end{equation} However, in updating $\\boldsymbol{\\theta}_ {t+1}$, these methods both use the bootstrapping target: \\begin{equation} R(s_t,a_t,s_{t+1})+\\gamma\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a\u0026rsquo;), \\end{equation} which depends on the current value $\\boldsymbol{\\theta}_t$, and thus will be biased. As a consequence, \\eqref{eq:nql.4} does not guarantee to converge2.\nSuch methods are known as semi-gradient since they take into account the effect of changing the weight vector $\\boldsymbol{\\theta}_t$ on the estimate, but ignore its effect on the target.\nDeep Q-learning On the other hands, we have already known that a neural network with particular settings for hidden layers and activation functions can approximate any continuous functions on a compact subsets of $\\mathbb{R}^n$, so how about using it with the Q-learning algorithm?\nSpecifically, we will be using neural network with weight $\\boldsymbol{\\theta}$ as a function approximator for Q-learning update. The network is referred as Q-network, as the whole algorithm is so-called Deep Q-learning, and the agent is known as DQN in short for Deep Q-network.\nThe Q-network can be trained by minimizing a sequence of loss function $L_t(\\boldsymbol{\\theta}_t)$ that changes at each iteration $t$: \\begin{equation} L_t(\\boldsymbol{\\theta}_t)=\\mathbb{E}_{s,a\\sim\\rho(\\cdot)}\\Big[\\big(y_t-Q_{\\boldsymbol{\\theta}_t}(s,a)\\big)^2\\Big],\\label{eq:dqn.1} \\end{equation} where \\begin{equation} y_t=\\mathbb{E}_{s\u0026rsquo;\\sim\\mathcal{E}}\\left[R(s,a,s\u0026rsquo;)+\\gamma\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_{t-1}}(s\u0026rsquo;,a\u0026rsquo;)\\vert s,a\\right] \\end{equation} is the target in iteration $t$, which follows as in \\eqref{eq:nql.3}; and where $\\rho(s,a)$ is referred as the behavior policy.\nThe TD target $y_t$ can approximated as \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\max_{a\u0026rsquo;}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a\u0026rsquo;) \\end{equation} To stabilize learning, DQN applies the following mechanisms.\nExperience replay Along with Q-network, the authors of deep-Q learning also introduce a technique called experience replay, which utilizes data efficiency and at the same time reduces the variance of the updates.\nIn particular, at each time step $t$, the experience, $e_t$, defined as \\begin{equation} e_t=(s_t,a_t,r_t,s_{t+1}) \\end{equation} is added into a set $\\mathcal{D}$ of size $N$, which is sampled uniformly at the training time to apply Q-learning updates. This method provides some advantages:\nEach experience $e_t$ can be used in many weight updates. Uniformly sampling from $\\mathcal{D}$ cancels out the correlations between consecutive experiences, i.e. $e_t, e_{t+1}$. Target network DQN introduces a target network $\\hat{Q}$ parameterized by $\\boldsymbol{\\theta}^-$to generate the TD target $y_t$ in \\eqref{eq:dqn.1} as \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\max_{a\u0026rsquo;}\\hat{Q}_{\\boldsymbol{\\theta}_t^-}(s_{t+1},a\u0026rsquo;)\\label{eq:tn.1} \\end{equation} The target network $\\hat{Q}$ is cloned from $Q$ every $C$ Q-learning update steps, i.e. $\\boldsymbol{\\theta}^-\\leftarrow\\boldsymbol{\\theta}$.\nImproved variants Double deep Q-learning As stated before that the Q-learning method could lead to over optimistic value estimates. Moreover, Q-learning with function approximation, such as DQN, has also been proved to induce maximization bias. These results are due to that in Q-learning and DQN, the $\\max$ operator uses the same values to both select and evaluate an action.\nTo reduce the overoptimism effect due to overestimation in DQN, we use a double estimator version of deep Q-learning, called Double Deep-Q learning, as we have used double Q-learning to mitigate the maximization bias in Q-learning.\nThe double DQN agent is similar to DQN, except that it replaces the target \\eqref{eq:tn.1}, which can be rewritten as: \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\hat{Q}_{\\boldsymbol{\\theta}_t^-}\\left(s_{t+1},\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\hat{Q}_{\\boldsymbol{\\theta}_t^-}(s_{t+1},a)\\right), \\end{equation} with \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\hat{Q}_{\\boldsymbol{\\theta}_t^-}\\left(s_{t+1},\\underset{a}{\\text{argmax}}\\hspace{0.1cm}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a)\\right) \\end{equation}\nPrioritized replay Dueling network Rainbow References [1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. On the Convergence of Stochastic Iterative Dynamic Programming Algorithms. A.I. Memo No. 1441, 1993.\n[2] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[3] Pieter Abbeel. Foundations of Deep RL Series, YouTube, 2021.\n[4] Vlad Mnih, et al. Playing Atari with Deep Reinforcement Learning, 2013.\n[5] Vlad Mnih, et al. Human Level Control Through Deep Reinforcement Learning. Nature, 2015.\n[6] Hado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning. AAAI16, 2016.\n[7] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. arXiv:1511.06581, 2015.\n[8] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. Prioritized Experience Replay. arXiv:1511.05952, 2016.\n[9] Taisuke Kobayashi, Wendyam Eric Lionel Ilboudo. t-Soft Update of Target Network for Deep Reinforcement Learning. arXiv:2008.10861, 2020.\n[10] Zhikang T. Wang, Masahito Ueda. Convergent and Efficient Deep Q Network Algorithm. arXiv:2106.15419, 2022.\nFootnotes In Monte Carlo control, the update target $y_t$ is chosen as the full return $G_t$, i.e. \\begin{equation*} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[G_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{equation*} and in (episodic on-policy) TD control methods, we use the TD target as the choice for $y_t$, i.e. for one-step TD methods such as one-step Sarsa, the update rule for $\\boldsymbol{\\theta}$ is given as \\begin{align*} \\boldsymbol{\\theta}_{t+1}\u0026amp;=\\boldsymbol{\\theta}_t+\\alpha\\big[G_{t:t+1}-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t) \\\\ \u0026amp;=\\boldsymbol{\\theta}_t+\\alpha\\big[R(s_t,a_t,s_{t+1})+\\gamma Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a_{t+1})-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{align*} and for $n$-step TD method, for instance, $n$-step Sarsa, we instead have \\begin{equation*} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[G_{t:t+n}-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{equation*} where \\begin{equation*} G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1}R_{t+n}+\\gamma^n Q_{\\boldsymbol{\\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\\hspace{1cm}t+n\u0026lt;T \\end{equation*} with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$ and where $R_{t+1}\\doteq R(s_t,a_t,s_{t+1})$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe semi-gradient TD methods with linear function approximation, e.g. \\eqref{eq:nql.5}, are guaranteed to converge to the TD fixed point due to the result we have proved.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on DQN.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Deep Q-learning"},{"content":" Natural Evolution Strategies, or NES, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.\nSearch gradients Usually when working on Evolution Strategy methods, we select some candidate solutions, which generate better fitness values than the other ones, to be parents of the next generation. This means, majority of solution samples have been wasted since they may contain some useful information.\nTo utilize the use all fitness samples, the NES uses search gradients in updating the parameters for the search distribution.\nLet $\\mathbf{z}\\in\\mathbb{R}^n$ denote the solution sampled from the distribution $\\pi(\\mathbf{z},\\theta)$ and let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be the fitness (or objective) function. The expected fitness value is then given by \\begin{equation} J(\\theta)=\\mathbb{E}_\\theta[f(\\mathbf{z})]=\\int f(\\mathbf{z})\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z}\\label{eq:sg.1} \\end{equation} Taking the gradient of the above function w.r.t $\\theta$ using the log-likelihood trick as in REINFORCE gives us \\begin{align} \\nabla_\\theta J(\\theta)\u0026amp;=\\nabla_\\theta\\int f(\\mathbf{z})\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026amp;=\\int f(\\mathbf{z})\\nabla_\\theta\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026amp;=\\int f(\\mathbf{z})\\nabla_\\theta\\pi(\\mathbf{z}\\vert\\theta)\\frac{\\pi(\\mathbf{z}\\vert\\theta)}{\\pi(\\mathbf{z}\\vert\\theta)}\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026amp;=\\int\\left[f(\\mathbf{z})\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\right]\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026amp;=\\mathbb{E}_\\theta\\left[f(\\mathbf{z})\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\right] \\end{align} Using Monte Carlo method, given samples $\\mathbf{z}_1,\\ldots,\\mathbf{z}_\\lambda$ from the population of size $\\lambda$, the search gradient is then can be approximated by \\begin{equation} \\nabla_\\theta J(\\theta)\\approx\\frac{1}{\\lambda}\\sum_{k=1}^{\\lambda}f(\\mathbf{z}_k)\\nabla_\\theta\\log\\pi(\\mathbf{z}_k\\vert\\theta)\\label{eq:sg.2} \\end{equation} Given this gradient w.r.t $\\theta$, we then can use a gradient-based method to repeatedly update the parameter $\\theta$ in order to give us a more desired search distribution. In particular, we can use such as SGD method \\begin{equation} \\theta\\leftarrow\\theta+\\alpha\\nabla_\\theta J(\\theta),\\label{eq:sg.3} \\end{equation} where $\\alpha$ is the learning rate.\nSearch gradients for MVN Consider the case that our search distribution $\\pi(\\mathbf{z}\\vert\\theta)$ is in form of a Multivariate Normal distribution, $\\mathbf{z}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, where $\\boldsymbol{\\mu}\\in\\mathbb{R}^n$ and $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{n\\times n}$.\nIn this case $\\theta=(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ denotes a tuple of parameters for the search distribution, which is given by \\begin{equation} \\pi(\\mathbf{z}\\vert\\theta)=\\frac{1}{(2\\pi)^{n/1}\\left\\vert\\boldsymbol{\\Sigma}\\right\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)\\right] \\end{equation} Taking natural logarithm of both sides then gives us \\begin{align} \\log\\pi(\\mathbf{z}\\vert\\theta)\u0026amp;=\\log\\left(\\frac{1}{(2\\pi)^{n/1}\\left\\vert\\boldsymbol{\\Sigma}\\right\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)\\right]\\right) \\\\ \u0026amp;=-\\frac{n}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\vert\\boldsymbol{\\Sigma}\\vert-\\frac{1}{2}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right) \\end{align} We continue by differentiating the above log-likelihood w.r.t $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$. Starting with $\\boldsymbol{\\mu}$, the gradient is given by \\begin{align} \\nabla_\\boldsymbol{\\mu}\\log\\pi(\\mathbf{z}\\vert\\theta)\u0026amp;=\\nabla_\\boldsymbol{\\mu}\\left(-\\frac{n}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\vert\\boldsymbol{\\Sigma}\\vert-\\frac{1}{2}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)\\right) \\\\ \u0026amp;=-\\frac{1}{2}\\nabla_\\boldsymbol{\\mu}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right) \\\\ \u0026amp;=\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}-\\boldsymbol{\\mu}) \\end{align} And the gradient w.r.t $\\boldsymbol{\\Sigma}$ is computed as \\begin{align} \\nabla_\\boldsymbol{\\Sigma}\\pi(\\mathbf{z}\\vert\\theta)\u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\left(-\\frac{n}{2}\\log(2\\pi)-\\frac{1}{2}\\log\\vert\\boldsymbol{\\Sigma}\\vert-\\frac{1}{2}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)\\right) \\\\ \u0026amp;=-\\frac{1}{2}\\nabla_\\boldsymbol{\\Sigma}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right) \\\\ \u0026amp;=\\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{z}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}-\\frac{1}{2}\\boldsymbol{\\Sigma}^{-1} \\end{align} The SGD update \\eqref{eq:sg.3} now is applied for each of $\\boldsymbol{\\mu}$ and $\\boldsymbol{\\Sigma}$ as \\begin{align} \\boldsymbol{\\mu}\u0026amp;\\leftarrow\\boldsymbol{\\mu}+\\alpha\\nabla_\\boldsymbol{\\mu}J(\\theta) \\\\ \u0026amp;\\leftarrow\\boldsymbol{\\mu}+\\alpha\\frac{1}{\\lambda}\\sum_{k=1}^{\\lambda}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}_k-\\boldsymbol{\\mu}\\right)f(\\mathbf{z}_k) \\end{align} and \\begin{align} \\boldsymbol{\\Sigma}\u0026amp;\\leftarrow\\boldsymbol{\\Sigma}+\\alpha\\nabla_\\boldsymbol{\\Sigma}J(\\theta) \\\\ \u0026amp;\\leftarrow\\boldsymbol{\\Sigma}+\\alpha\\frac{1}{\\lambda}\\sum_{k=1}^{\\lambda}\\left[\\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\\left(\\mathbf{z}_k-\\boldsymbol{\\mu}\\right)\\left(\\mathbf{z}_k-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}-\\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\\right]f(\\mathbf{z}_k) \\end{align}\nNatural gradient The natural gradient searches for the direction based on the distance between distributions $\\pi(\\mathbf{z}\\vert\\theta)$ and $\\pi(\\mathbf{z}\\vert\\theta\u0026rsquo;)$. One natural measure of distance between probability distributions is the Kullback-Leibler divergence, or KL divergence.\nIn other words, our work is to look for the direction of updating gradient, denoted as $\\delta\\theta$, such that \\begin{align} \\max_{\\delta\\theta}\u0026amp;\\hspace{0.1cm}J(\\theta+\\delta\\theta)\\approx J(\\theta)+\\delta\\theta^\\text{T}\\nabla_\\theta J \\\\ \\text{s.t.}\u0026amp;\\hspace{0.1cm}D_\\text{KL}(\\theta\\Vert\\theta+\\delta\\theta)=\\varepsilon, \\end{align} where $J(\\theta)$ is given as in \\eqref{eq:sg.1}; $\\varepsilon$ is a small increment size; and where $D_\\text{KL}(\\theta\\Vert\\theta+\\delta\\theta)$ is the KL divergence of $\\pi(\\mathbf{z}\\vert\\theta)$ from $\\pi(\\mathbf{z}\\vert\\theta+\\delta\\theta)$, defined as \\begin{align} D_\\text{KL}(\\theta\\Vert\\theta+\\delta\\theta)\u0026amp;=\\int\\pi(\\mathbf{z}\\vert\\theta)\\log\\frac{\\pi(\\mathbf{z}\\vert\\theta)}{\\pi(\\mathbf{z}\\vert\\theta+\\delta\\theta)}\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026amp;=\\mathbb{E}_{\\theta}\\big[\\log\\pi(\\mathbf{z}\\vert\\theta)-\\log\\pi(\\mathbf{z}\\vert\\theta+\\delta)\\big]\\label{eq:ng.1} \\end{align} As $\\delta\\theta\\to 0$, or in other words, consider the Taylor expansion of \\eqref{eq:ng.1} about $\\delta\\theta=0$, we have \\begin{align} \u0026amp;D_\\text{KL}(\\theta\\Vert\\theta+\\delta\\theta)\\nonumber \\\\ \u0026amp;=\\mathbb{E}_{\\theta}\\big[\\log\\pi(\\mathbf{z}\\vert\\theta)-\\log\\pi(\\mathbf{z}\\vert\\theta+\\delta\\theta)\\big] \\\\ \u0026amp;\\approx\\mathbb{E}_\\theta\\left[\\log\\pi(\\mathbf{z}\\vert\\theta)-\\left(\\log\\pi(\\mathbf{z}\\vert\\theta)+\\delta\\theta^\\text{T}\\frac{\\nabla_\\theta\\pi(\\mathbf{z}\\vert\\theta)}{\\pi(\\mathbf{z}\\vert\\theta)}+\\frac{1}{2}\\delta\\theta^\\text{T}\\frac{\\nabla_\\theta^2\\pi(\\mathbf{z}\\vert\\theta)}{\\pi(\\mathbf{z}\\vert\\theta)}\\delta\\theta\\right)\\right] \\\\ \u0026amp;=-\\mathbb{E}_\\theta\\left[\\delta\\theta^\\text{T}\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)+\\frac{1}{2}\\delta\\theta^\\text{T}\\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)\\delta\\theta\\right] \\\\ \u0026amp;=-\\mathbb{E}_\\theta\\Big[\\delta\\theta^\\text{T}\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\Big]-\\mathbb{E}_\\theta\\left[\\frac{1}{2}\\delta\\theta^\\text{T}\\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)\\delta\\theta\\right] \\\\ \u0026amp;\\overset{\\text{(i)}}{=}-\\frac{1}{2}\\delta\\theta^\\text{T}\\mathbb{E}_\\theta\\Big[\\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)\\Big]\\delta\\theta \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}\\frac{1}{2}\\delta\\theta^\\text{T}\\mathbb{E}_\\theta\\Big[\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}\\Big]\\delta\\theta \\\\ \u0026amp;\\overset{\\text{(iii)}}{=}\\frac{1}{2}\\delta\\theta^\\text{T}\\mathbf{F}\\delta\\theta\\label{eq:ng.2} \\end{align} where\nIn this step, we have used \\begin{align} \\mathbb{E}_\\theta\\Big[\\delta\\theta^\\text{T}\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\Big]\u0026=\\delta\\theta^\\text{T}\\int\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026=\\delta\\theta^\\text{T}\\int\\pi(\\mathbf{z}\\vert\\theta)\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\nabla_\\theta\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026=\\delta\\theta^\\text{T}\\nabla_\\theta\\int\\pi(\\mathbf{z}\\vert\\theta)\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026=\\delta\\theta^\\text{T}\\nabla_\\theta 1=0 \\end{align} In this step, let $\\theta_j,\\theta_k$ denote the $j$-th and $k$-th element of $\\theta$ respectively. The $(j,k)$ element of the Hessian $\\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)$ thus, by chain rule, can be computed as \\begin{align} \\hspace{-1.7cm}\\frac{\\partial^2}{\\partial\\theta_j\\partial\\theta_k}\\log\\pi(\\mathbf{z}\\vert\\theta)\u0026=\\frac{\\partial}{\\partial\\theta_j}\\left(\\frac{\\partial\\log\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}\\right) \\\\ \u0026=\\frac{\\partial}{\\partial\\theta_j}\\left(\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}\\right) \\\\ \u0026=\\frac{\\partial}{\\partial\\theta_j}\\left(\\frac{1}{\\pi(\\mathbf{z} \\vert\\theta)}\\right)\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}+\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial^2\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j\\partial\\theta_k} \\\\ \u0026=\\left(\\frac{\\partial\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}}{\\partial\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j}\\right)\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}+\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial^2\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j\\partial\\theta_k} \\\\ \u0026=-\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)^2}\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j}\\cdot\\frac{\\partial\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}+\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial^2\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j\\partial\\theta_k} \\\\ \u0026=-\\frac{\\partial\\log\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j}\\cdot\\frac{\\partial\\log\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_k}+\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\cdot\\frac{\\partial^2\\pi(\\mathbf{z}\\vert\\theta)}{\\partial\\theta_j\\partial\\theta_k}, \\end{align} which implies that \\begin{equation} \\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)=-\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}+\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\nabla_\\theta^2\\pi(\\mathbf{z}\\vert\\theta) \\end{equation} Taking expectation on both sides gives us \\begin{align} \\hspace{-1cm}\\mathbb{E}_\\theta\\Big[\\nabla_\\theta^2\\log\\pi(\\mathbf{z}\\vert\\theta)\\Big]\u0026=-\\mathbb{E}_\\theta\\Big[\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}\\Big]+\\mathbb{E}_\\theta\\left[\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\nabla_\\theta^2\\pi(\\mathbf{z}\\vert\\theta)\\right]\\label{eq:ng.5} \\\\ \u0026=-\\mathbb{E}_\\theta\\Big[\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}\\Big], \\end{align} where the latter expectation in \\eqref{eq:ng.5} has been absorbed due to \\begin{align} \\mathbb{E}_\\theta\\left[\\frac{1}{\\pi(\\mathbf{z}\\vert\\theta)}\\nabla_\\theta^2\\pi(\\mathbf{z}\\vert\\theta)\\right]\u0026=\\int\\nabla_\\theta^2\\pi(\\mathbf{z}\\vert\\theta)\\,d\\mathbf{z} \\\\ \u0026=\\nabla_\\theta^2\\int\\pi(\\mathbf{z}\\vert\\theta)\\,d\\mathbf{z} \\\\ \u0026=\\nabla_\\theta^2 1=\\mathbf{0} \\end{align} The matrix $\\mathbf{F}$ is referred as the Fisher information matrix of the given parametric family of search distributions, defined as \\begin{align} \\mathbf{F}\u0026=\\mathbb{E}_\\theta\\Big[\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}\\Big] \\\\ \u0026=\\int\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)\\nabla_\\theta\\log\\pi(\\mathbf{z}\\vert\\theta)^\\text{T}\\hspace{0.1cm}d\\mathbf{z} \\end{align} Hence, we have the Lagrangian of our constrained optimization problem is \\begin{align} \\mathcal{L}(\\theta,\\delta\\theta,\\lambda)\u0026amp;=J(\\theta)+\\delta\\theta^\\text{T}\\nabla_\\theta J(\\theta)+\\lambda\\big(\\varepsilon-D_\\text{KL}(\\theta\\Vert\\theta+\\delta\\theta)\\big) \\\\ \u0026amp;=J(\\theta)+\\delta\\theta^\\text{T}\\nabla_\\theta J(\\theta)+\\lambda\\left(\\varepsilon-\\frac{1}{2}\\delta\\theta^\\text{T}\\mathbf{F}\\delta\\theta\\right), \\end{align} where $\\lambda\u0026gt;0$ is the Lagrange multiplier.\nIt is easily seen that $\\mathbf{F}$ is symmetric, thus taking the gradient of the Lagrangian w.r.t $\\delta\\theta$ and setting it to zero gives us \\begin{equation} \\lambda\\mathbf{F}\\delta\\theta=\\nabla_\\theta J(\\theta) \\end{equation} If the Fisher information matrix $\\mathbf{F}$ is invertible, the solution for $\\delta\\theta$ that maximizes $\\mathcal{L}$ then can be computed as \\begin{equation} \\delta\\theta=\\frac{1}{\\lambda}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta),\\label{eq:ng.3} \\end{equation} which defines the direction of the natural gradient $\\tilde{\\nabla}_\\theta J(\\theta)$. Since $\\lambda\u0026gt;0$ we therefore obtain \\begin{equation} \\tilde{\\nabla}_\\theta J(\\theta)=\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta) \\end{equation} Continue with the value of $\\delta\\theta$ given in \\eqref{eq:ng.3}, the dual function of our optimization is given as \\begin{align} g(\\lambda)\u0026amp;=J(\\theta)+\\frac{1}{\\lambda}\\nabla_\\theta J(\\theta)^\\text{T}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta)-\\frac{1}{2}\\frac{\\lambda}{\\lambda^2}\\nabla_\\theta J(\\theta)^\\text{T}\\mathbf{F}^{-1}\\mathbf{F}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta)+\\lambda\\varepsilon \\\\ \u0026amp;=J(\\theta)+\\frac{1}{2}\\lambda^{-1}\\nabla_\\theta J(\\theta)^\\text{T}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta)+\\lambda\\varepsilon \\end{align} Taking the gradient of $g$ w.r.t $\\lambda$ and setting it to zero and since $\\varepsilon\u0026gt;0$ small gives us the solution for $\\lambda$, which is \\begin{equation} \\lambda=\\sqrt{\\frac{\\nabla_\\theta J(\\theta)^\\text{T}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta)}{\\varepsilon}}, \\end{equation} Hence, the SGD update for the parameter $\\theta$ using natural gradient is \\begin{equation} \\theta\\leftarrow\\theta+\\eta\\tilde{\\nabla}_\\theta J(\\theta)=\\theta+\\eta\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta),\\label{eq:ng.4} \\end{equation} where $\\eta$ is the learning rate, given as \\begin{equation} \\eta=\\lambda^{-1}=\\sqrt{\\frac{\\varepsilon}{\\nabla_\\theta J(\\theta)^\\text{T}\\mathbf{F}^{-1}\\nabla_\\theta J(\\theta)}} \\end{equation} This learning rate can also be replaced by a more desirable one without changing the direction of our update. With this update rule for natural gradient, we obtain the general formulation of NES, as described in the following pseudocode.\nRobustness techniques Fitness shaping NES uses the so-called fitness shaping technique, which helps to avoid early convergence due to the possible affection of outliers fitness value in \\eqref{eq:sg.2}, e.g. there may exist an outlier whose fitness value, says $f(\\mathbf{z}_i)$, is much greater than other solutions\u0026rsquo; ones, $\\{f(\\mathbf{z}_k)\\}_{k\\neq i}$.\nRather than using fitness values $f(\\mathbf{z}_k)$ in approximating the gradient in \\eqref{eq:sg.2}, fitness shaping instead applies a rank-based transformation of $f(\\mathbf{z}_k)$.\nIn particular, let $\\mathbf{z}_{k:\\lambda}$ denote the $k$-th best sample out of the population of size $\\lambda$, $\\mathbf{z}_1,\\ldots,\\mathbf{z}_\\lambda$, i.e. $f(\\mathbf{z}_{1:\\lambda})\\geq\\ldots\\geq f(\\mathbf{z}_{\\lambda:\\lambda})$, the gradient estimate \\eqref{eq:sg.2} now is rewritten as \\begin{equation} \\nabla_\\theta J(\\theta)=\\sum_{k=1}^{\\lambda}u_k\\nabla_\\theta\\log\\pi(\\mathbf{z}_{k:\\lambda}\\vert\\theta),\\label{eq:fs.1} \\end{equation} where $u_1\\geq\\ldots\\geq u_\\lambda$ are referred as utility values, which are preserved-order transformations of $f(\\mathbf{z}_{1:\\lambda}),\\ldots,f(\\mathbf{z}_{\\lambda:\\lambda})$.\nThe choice for utility function $u$ is a free parameter of the algorithm. In the original paper, the author proposed \\begin{equation} u_k=\\frac{\\max\\left(0,\\log\\left(\\frac{\\lambda}{2}+1\\right)-\\log k\\right)}{\\sum_{j=1}^{\\lambda}\\max\\left(0,\\log\\left(\\frac{\\lambda}{2}+1\\right)-\\log j\\right)}-\\frac{1}{\\lambda} \\end{equation}\nAdaption sampling Beside fitness shaping, NES also applies another heuristic, called adaption sampling, to make the performance more robustly. This technique lets the algorithm determine the appropriate hyperparameters (in this case, NES chooses the learning rate $\\eta$ be the one to adapt) more quickly.\nIn particular, for a successive parameter $\\theta\u0026rsquo;$ of $\\theta$, the corresponding learning rate $\\eta$ used in its update \\eqref{eq:ng.4} will be determined by comparing samples $\\mathbf{z}\u0026rsquo;$ sampled from $\\pi_\\theta\u0026rsquo;$ with samples $\\mathbf{z}$ sampled from $\\pi_\\theta$ according to a Mann-Whitney U-test.\nRotationally-symmetric distributions The rotationally-symmetric distributions, or radial distributions refer to class of distributions $p(\\mathbf{x})$ such that \\begin{equation} p(\\mathbf{x})=p(\\mathbf{U}\\mathbf{x}),\\label{eq:rsd.1} \\end{equation} for all $\\mathbf{x}\\in\\mathbb{R}^n$ and for all orthogonal matrices $\\mathbf{U}\\in\\mathbb{R}^{n\\times n}$.\nLet $Q_\\boldsymbol{\\tau}(\\mathbf{z})$ be a family of rotationally-symmetric distributions in $\\mathbb{R}^n$ parameterized by $\\boldsymbol{\\tau}$. The property \\eqref{eq:rsd.1} allows us to represent $Q_\\boldsymbol{\\tau}(\\mathbf{z})$ as \\begin{equation} Q_\\boldsymbol{\\tau}(\\mathbf{z})=q_\\boldsymbol{\\tau}(\\Vert\\mathbf{z}\\Vert^2), \\end{equation} for some family of functions $q_\\boldsymbol{\\tau}:\\mathbb{R}_+\\to\\mathbb{R}_+$.\nConsider the classes of search distributions in a form of \\begin{align} \\pi(\\mathbf{z}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Sigma},\\boldsymbol{\\tau})\u0026amp;=\\frac{1}{\\vert\\mathbf{A}\\vert}q_\\boldsymbol{\\tau}\\left(\\left\\Vert(\\mathbf{A}^{-1})^\\text{T}(\\mathbf{z}-\\boldsymbol{\\mu})\\right\\Vert^2\\right) \\\\ \u0026amp;=\\frac{1}{\\left\\vert\\mathbf{A}^\\text{T}\\mathbf{A}\\right\\vert^{1/2}}q_\\boldsymbol{\\tau}\\left((\\mathbf{z}-\\boldsymbol{\\mu})^\\text{T}(\\mathbf{A}^\\text{T}\\mathbf{A})^{-1}(\\mathbf{z}-\\boldsymbol{\\mu})\\right),\\label{eq:rsd.2} \\end{align} with additional transformation parameters $\\boldsymbol{\\mu}\\in\\mathbb{R}^n$ and invertible matrices $\\mathbf{A}\\in\\mathbb{R}^{n\\times n}$.\nIt can be seen that Gaussian and its multivariate form, MVN, can be written in form of $\\eqref{eq:rsd.2}$, and thus are members of these classes of distributions.\nExponential parameterization By \\eqref{eq:ng.4}, the natural gradient update for a multivariate Gaussian search distribution, denoted $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, is \\begin{align} \\boldsymbol{\\mu}\u0026amp;\\leftarrow\\boldsymbol{\\mu}+\\eta\\mathbf{F}^{-1}\\nabla_\\boldsymbol{\\mu} J(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}), \\\\ \\boldsymbol{\\Sigma}\u0026amp;\\leftarrow\\boldsymbol{\\Sigma}+\\eta\\mathbf{F}^{-1}\\nabla_\\boldsymbol{\\Sigma} J(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}) \\end{align} Thus, in updating the covariance matrix $\\boldsymbol{\\Sigma}$ as above, we have to ensure that $\\boldsymbol{\\Sigma}+\\eta\\mathbf{F}^{-1}\\nabla_\\boldsymbol{\\Sigma} J(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ is symmetric positive definite.\nTo accomplish this, we may represent the covariance matrix using the exponential parameterization for symmetric matrices. In particular, let \\begin{equation} \\mathcal{S}_n\\doteq\\{\\mathbf{M}\\in\\mathbb{R}^{n\\times n}:\\mathbf{M}=\\mathbf{M}^\\text{T}\\} \\end{equation} denote the set of symmetric matrices of $\\mathbb{R}^{n\\times n}$ and let \\begin{equation} \\mathcal{P}_n\\doteq\\{\\mathbf{M}\\in\\mathcal{S}_n:\\mathbf{M}\\succ 0\\} \\end{equation} represent the cone of symmetric positive definite matrices of $\\mathbb{R}^{n\\times n}$.\nUsing Taylor expansion for the exponential function, we then have the exponential map $\\exp:\\mathcal{S}_n\\to\\mathcal{P}_n$ can be written as \\begin{equation} \\exp(\\mathbf{M})=\\sum_{i=0}^{\\infty}\\frac{\\mathbf{M}^i}{i!},\\label{eq:ep.1} \\end{equation} which is diffeomorphism, i.e. the map is bijective, plus the map and its inverse map, $\\log:\\mathcal{P}_n\\to\\mathcal{S}_n$, both are differentiable.\nTherefore, we can represent the covariance matrix $\\boldsymbol{\\Sigma}\\in\\mathcal{P}_n$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\exp(\\mathbf{M}),\\hspace{2cm}\\mathbf{M}\\in\\mathcal{S}_n \\end{equation} This representation lets the gradient update always end up as a valid covariance matrix. However, the computation for the Fisher information matrix $\\mathbf{F}$ is consequently more complicated due to require partial derivatives of matrix exponential \\eqref{eq:ep.1}.\nExponential local coordinates It is noticeable from \\eqref{eq:rsd.2} that the dependency of the distribution on $\\mathbf{A}$ is only in terms of $\\mathbf{A}^\\text{T}\\mathbf{A}$, which is a symmetric positive semi-definite matrix since for all non-zero vector $\\mathbf{x}\\in\\mathbb{R}^n$ we have \\begin{equation} \\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{A}\\mathbf{x}=\\Vert\\mathbf{A}\\mathbf{x}\\Vert^2\\geq 0 \\end{equation} In the case of MVN, this matrix corresponds to the covariance matrix.\nTherefore, rather than using exponential mapping in updating the positive definite matrices $\\mathbf{A}^\\text{T}\\mathbf{A}$, we repeatedly linear transform the coordinate system in each iteration to a coordinate system in which the calculation for $\\mathbf{F}$ is trivial.\nSpecifically, let the current search distribution be given by $(\\boldsymbol{\\mu},\\mathbf{A})$, we use exponential local coordinates \\begin{equation} (\\boldsymbol{\\delta},\\mathbf{M})\\mapsto(\\boldsymbol{\\mu}_\\text{new},\\mathbf{A}_\\text{new})=\\left(\\boldsymbol{\\mu}+\\mathbf{A}^\\text{T}\\boldsymbol{\\delta},\\mathbf{A}\\exp\\left(\\frac{1}{2}\\mathbf{M}\\right)\\right) \\end{equation} This coordinate system is local in the sense that the coordinates $(\\boldsymbol{\\delta},\\mathbf{M})=(\\mathbf{0},\\mathbf{0})$ is mapped to $(\\boldsymbol{\\mu},\\mathbf{A})$.\nFor the case that $\\tau\\in\\mathbb{R}^{n\u0026rsquo;}$, $\\boldsymbol{\\delta}\\in\\mathbb{R}^n$ and $\\mathbf{M}\\in\\mathbb{R}^{n(n+1)/2}$, the Fisher information matrix $\\mathbf{F}$ in this coordinate system is an $m\\times m$ matrix, where \\begin{equation} m=n+\\frac{n(n+1)}{2}+n\u0026rsquo;=\\frac{n(n+3)}{2}+n\u0026rsquo;, \\end{equation} and is given as \\begin{equation} \\mathbf{F}=\\left[\\begin{matrix}\\mathbf{I}\u0026amp;\\mathbf{V} \\\\ \\mathbf{V}^\\text{T}\u0026amp;\\mathbf{C}\\end{matrix}\\right],\\label{eq:ec.1} \\end{equation} where \\begin{equation} \\mathbf{V}=\\frac{\\partial^2\\log\\pi(\\mathbf{z})}{\\partial(\\boldsymbol{\\delta},\\mathbf{M})\\partial\\boldsymbol{\\tau}}\\in\\mathbb{R}^{(m-n\u0026rsquo;)\\times n\u0026rsquo;},\\hspace{1cm}\\mathbf{C}=\\frac{\\partial^2\\log\\pi(\\mathbf{z})}{\\partial\\boldsymbol{\\tau}^2}\\in\\mathbb{R}^{n\u0026rsquo;\\times n\u0026rsquo;} \\end{equation} Using the Woodbury identity for $\\mathbf{F}$ gives us its inverse \\begin{equation} \\mathbf{F}^{-1}=\\left[\\begin{matrix}\\mathbf{I}\u0026amp;\\mathbf{V} \\\\ \\mathbf{V}^\\text{T}\u0026amp;\\mathbf{C}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{I}+\\mathbf{H}\\mathbf{V}\\mathbf{V}^\\text{T}\u0026amp;-\\mathbf{H}\\mathbf{v} \\\\ -\\mathbf{H}\\mathbf{V}^\\text{T}\u0026amp;\\mathbf{H}\\end{matrix}\\right], \\end{equation} where $\\mathbf{H}=(\\mathbf{C}-\\mathbf{V}^\\text{T}\\mathbf{V})^{-1}$, and thus $\\mathbf{H}$ is symmetric.\nOn the other hands, the gradient w.r.t each parameter of $\\log\\pi(\\mathbf{z})$ are given as \\begin{equation} \\nabla_{\\boldsymbol{\\delta},\\mathbf{M},\\boldsymbol{\\tau}}\\log\\pi(\\mathbf{z}\\vert\\boldsymbol{\\mu},\\mathbf{A},\\boldsymbol{\\tau},\\boldsymbol{\\delta},\\mathbf{M})\\big\\vert_{\\hspace{0.1cm}\\boldsymbol{\\delta}=\\mathbf{0},\\mathbf{M}=\\mathbf{0}}=\\mathbf{g}=\\left[\\begin{matrix}\\mathbf{g}_\\boldsymbol{\\delta} \\\\ \\mathbf{g}_\\mathbf{M} \\\\ \\mathbf{g}_\\boldsymbol{\\tau}\\end{matrix}\\right], \\end{equation} where \\begin{align} \\mathbf{g}_\\boldsymbol{\\delta}\u0026amp;=-2\\frac{q_\\boldsymbol{\\tau}\u0026rsquo;(\\Vert\\mathbf{s}\\Vert^2)}{q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2)}\\mathbf{s},\\label{eq:ec.2} \\\\ \\mathbf{g}_\\mathbf{M}\u0026amp;=-\\frac{1}{2}\\mathbf{I}-\\frac{q_\\boldsymbol{\\tau}\u0026rsquo;(\\Vert\\mathbf{s}\\Vert^2)}{q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2)}\\mathbf{s}\\mathbf{s}^\\text{T},\\label{eq:ec.3} \\\\ \\mathbf{g}_\\boldsymbol{\\tau}\u0026amp;=\\frac{1}{q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2)}\\nabla_\\boldsymbol{\\tau}q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2), \\end{align} where \\begin{equation} q_\\boldsymbol{\\tau}\u0026rsquo;=\\frac{\\partial}{\\partial(r^2)}q_\\boldsymbol{\\tau} \\end{equation} denotes the derivative of $q_\\boldsymbol{\\tau}$ w.r.t $r^2$, while $\\nabla_\\boldsymbol{\\tau}q_\\boldsymbol{\\tau}$ represents the gradient w.r.t $\\boldsymbol{\\tau}$.\nThe natural gradient for a sample $\\mathbf{s}$ is then can be computed as \\begin{equation} \\tilde{\\nabla}J=\\mathbf{F}^{-1}\\mathbf{g}=\\mathbf{F}^{-1}\\left[\\begin{matrix}\\mathbf{g}_\\boldsymbol{\\delta} \\\\ \\mathbf{g}_\\mathbf{M} \\\\ \\mathbf{g}_\\boldsymbol{\\tau}\\end{matrix}\\right]=\\left[\\begin{matrix}\\left(\\mathbf{g}_\\boldsymbol{\\delta},\\mathbf{g}_\\mathbf{M}\\right)-\\mathbf{H}\\mathbf{V}\\left(\\mathbf{V}^\\text{T}\\left(\\mathbf{g}_\\boldsymbol{\\delta},\\mathbf{g}_\\mathbf{M}\\right)-\\mathbf{g}_\\boldsymbol{\\tau}\\right) \\\\ \\mathbf{H}\\left(\\mathbf{V}^\\text{T}\\left(\\mathbf{g}_\\boldsymbol{\\delta},\\mathbf{g}_\\mathbf{M}\\right)-\\mathbf{g}_\\boldsymbol{\\tau}\\right)\\end{matrix}\\right], \\end{equation} where \\begin{equation} \\left(\\mathbf{g}_\\boldsymbol{\\delta},\\mathbf{g}_\\mathbf{M}\\right)=\\left[\\begin{matrix}\\mathbf{g}_\\boldsymbol{\\delta} \\\\ \\mathbf{g}_\\mathbf{M}\\end{matrix}\\right] \\end{equation}\nSampling from rotationally symmetric distributions To sample from this class of distributions, we first draw a sample $\\mathbf{s}$ according to the standard density \\begin{equation} \\mathbf{s}\\sim\\pi(\\mathbf{s}\\vert\\boldsymbol{\\mu}=\\mathbf{0},\\mathbf{A}=\\mathbf{I},\\boldsymbol{\\tau}), \\end{equation} We continue to transform this sample into \\begin{equation} \\mathbf{z}=\\boldsymbol{\\mu}+\\mathbf{A}^\\text{T}\\mathbf{s}\\sim\\pi(\\mathbf{z}\\vert\\boldsymbol{\\mu},\\mathbf{A},\\boldsymbol{\\tau}) \\end{equation} In general, sampling $\\mathbf{s}$ can be decomposed into sampling $r^2$ according to \\begin{equation} r^2\\sim\\tilde{q}_\\boldsymbol{\\tau}(r^2)=\\int_{\\Vert\\mathbf{z}\\Vert^2=r^2}Q_\\boldsymbol{\\tau}\\hspace{0.1cm}d\\mathbf{z}=\\frac{2\\pi^{n/2}}{\\Gamma(n/2)}(r^2)^{(d-1)/2}q_\\boldsymbol{\\tau}(r^2) \\end{equation} and a unit vector $\\mathbf{u}\\in\\mathbb{R}^n$.\nExponential Natural Evolution Strategies Recall that the Multivariate Gaussian can be expressed in form of a radial distribution \\eqref{eq:ep.1}. In this case, we have that \\begin{equation} q_\\boldsymbol{\\tau}(r^2)=\\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}r^2\\right),\\label{eq:xnes.1} \\end{equation} which does not depend on $\\boldsymbol{\\tau}$. This lets the Fisher information matrix in \\eqref{eq:ec.1} be simplified to the most trivial form, which is the identity matrix $\\mathbf{I}$.\nDifferentiating \\eqref{eq:xnes.1} w.r.t $r^2$ then gives us \\begin{equation} q_\\boldsymbol{\\tau}\u0026rsquo;(r^2)=\\frac{\\partial}{\\partial(r^2)}\\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}r^2\\right)=-\\frac{1}{2}\\frac{1}{(2\\pi)^{n/2}}\\exp\\left(-\\frac{1}{2}r^2\\right)=-\\frac{1}{2}q_\\boldsymbol{\\tau}(r^2), \\end{equation} which by \\eqref{eq:ec.2} and \\eqref{eq:ec.3} implies that \\begin{equation} \\mathbf{g}_\\boldsymbol{\\delta}=-2\\frac{q_\\boldsymbol{\\tau}\u0026rsquo;(\\Vert\\mathbf{s}\\Vert^2)}{q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2)}\\mathbf{s}=\\mathbf{s} \\end{equation} and \\begin{equation} \\mathbf{g}_\\mathbf{M}=-\\frac{1}{2}\\mathbf{I}-\\frac{q_\\boldsymbol{\\tau}\u0026rsquo;(\\Vert\\mathbf{s}\\Vert^2)}{q_\\boldsymbol{\\tau}(\\Vert\\mathbf{s}\\Vert^2)}\\mathbf{s}\\mathbf{s}^\\text{T}=\\frac{1}{2}(\\mathbf{s}\\mathbf{s}^\\text{T}-\\mathbf{I}) \\end{equation} Hence, the natural gradient is then given as \\begin{align} \\nabla_\\boldsymbol{\\delta}J\u0026amp;=\\sum_{k=1}^{\\lambda}f(\\mathbf{z}_k)\\mathbf{s}_k \\\\ \\nabla_\\mathbf{M}J\u0026amp;=\\sum_{k=1}^{\\lambda}f(\\mathbf{z}_k)(\\mathbf{s}_k\\mathbf{s}_k^\\text{T}-\\mathbf{I}), \\end{align} which can be improved with fitness shaping using the update formula \\eqref{eq:fs.1} as \\begin{align} \\nabla_\\boldsymbol{\\delta}J\u0026amp;=\\sum_{k=1}^{\\lambda}u_k\\mathbf{s}_{k:\\lambda}, \\\\ \\nabla_\\mathbf{M}J\u0026amp;=\\sum_{k=1}^{\\lambda}u_k(\\mathbf{s}_{k:\\lambda}\\mathbf{s}_{k:\\lambda}^\\text{T}-\\mathbf{I}), \\end{align} where $\\mathbf{s}_{k:\\lambda}$ denotes the $k$-th best sample in local coordinates. The resulting algorithm is thus known as Exponential Natural Evolution Strategies, or xNES, with the corresponding pseudocode shown below.\nTesting on Rastrigin function Analogy to CMA-ES, let us test NES on the Rastrigin function, which is, recall that, given by the formula \\begin{equation} f(\\mathbf{x})=10 n+\\sum_{i=1}^{n}x_i^2-10\\cos\\left(2\\pi x_i\\right) \\end{equation} $f(\\mathbf{x})$ reaches its global minimum $0$ at $\\mathbf{x}=\\mathbf{0}$. The experimental setup we are going to use are provided in Wierstra et al. 2014. Similar to our test with CMA-ES, each function evaluation is counted as success when it reaches $f_\\text{stop}=10^{-10}$.\nThe result after running our experiment is illustrated in the figure below.\nFigure 1: Success rate to reach $f_\\text{stop}=10^{-10}$ versus population size for Rastrigin function.\nThe code can be found here References [1] Daan Wierstra, Tom Schaul, Jan Peters, Jürgen Schmidhuber. Natural Evolution Strategies. IEEE World Congress on Computational Intelligence, 2008.\n[2] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jürgen Schmidhuber. Natural Evolution Strategies. arXiv:1106.4487, 2011.\n[3] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, Jürgen Schmidhuber. Natural Evolution Strategies. Journal of Machine Learning Research 15, 2014.\n[4] Jan Reinhard Peters. Machine Learning of Motor Skills for Robotics. PhD thesis, 2007.\n[5] Stephen Boyd \u0026amp; Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[6] Ha, David. A Visual Guide to Evolution Strategies. blog.otoro.net, 2017.\nFootnotes","permalink":"https://trunghng.github.io/posts/evolution-strategy/nes/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003eNatural Evolution Strategies\u003c/strong\u003e, or \u003cstrong\u003eNES\u003c/strong\u003e, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Natural Evolution Strategies"},{"content":" Notes on Policy gradient methods.\nPreliminaries Consider an finite-horizon undiscounted Markov Decision Process (MDP), which is a tuple of $(\\mathcal{S},\\mathcal{A},P,r,\\rho_0,H)$ where\n$\\mathcal{S}$ is the state space. $\\mathcal{A}$ is the action space. $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ is the transition probability, i.e. $P(s\u0026rsquo;\\vert s,a)$ denotes the probability of being at state $s\u0026rsquo;$ by taking action $a$ from state $s$. $r:\\mathcal{\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}}\\to\\mathbb{R}$ is the reward function. $\\rho_0$ is the distribution of the start state $s_0$. $H$ is the horizon time. To start an episode, the agent is given an initial state $s_0$, which is sampled from $\\rho_0$, i.e. $s_0\\sim\\rho_0$. At each time step $t$, from state $s_t$, until reaching the terminal state, the agent takes action $a_t$, according to a policy $\\pi$, where $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$, which lets agent end up at state $s_{t+1}$ due to the dynamics $P$, and is given a corresponding reward $r_t=r(s_t,a_t,s_{t+1})$. The process gives rise to a sequence, called a trajectory, defined by: \\begin{equation} \\tau=(s_0,a_0,s_1,a_1,s_2,a_2\\ldots) \\end{equation} For a policy $\\pi$, let $V_\\pi:\\mathcal{S}\\to\\mathbb{R}$ denote the state value function, $Q_\\pi:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ represent the state-action value function and let $A_\\pi:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ be the advantage function: \\begin{align} V_\\pi(s_t)\u0026amp;\\doteq\\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}\\left[\\sum_{k=0}^{H-1}r_{t+k}\\right] \\\\ Q_\\pi(s_t,a_t)\u0026amp;\\doteq\\mathbb{E}_{s_{t+1:H-1},a_{t+1:H-1}}\\left[\\sum_{k=0}^{H-1}r_{t+k}\\right] \\\\ A_\\pi(s_t,a_t)\u0026amp;\\doteq Q_\\pi(s_t,a_t)-V_\\pi(s_t), \\end{align} where the expectation notation $\\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}$ denotes that the expected value is computed by integrated over $s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t),a_t\\sim\\pi(a_t\\vert s_t)$.\nAs in DQN, here we will be working with a policy $\\pi_\\theta$ parameterized by a vector $\\theta$. Let $R(\\tau)\\doteq\\sum_{t=0}^{H-1}r_t$ denote the return, or the total reward along trajectory $\\tau$. Our goal is to maximize the expected return: \\begin{equation} \\eta(\\pi_\\theta)\\doteq\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\big[R(\\tau)\\big]=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}r_t\\right]\\label{eq:pre.1} \\end{equation}\n(Vanilla) Policy Gradient In (vanilla) policy gradient method, we are trying to optimize the expected total reward \\eqref{eq:pre.1} by repeatedly estimating the gradient \\begin{equation} \\nabla_\\theta\\eta(\\pi_\\theta)=\\nabla_\\theta\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\big[R(\\tau)\\big]\\label{eq:vpg.1} \\end{equation} To continue our derivation, we will be using the probability of a trajectory $\\tau\\sim\\pi_\\theta$, computed by \\begin{equation} P(\\tau;\\theta)=\\rho_0(s_0)\\prod_{t=0}^{H-1}P(s_{t+1}\\vert s_t,a_t)\\pi_\\theta(a_t\\vert s_t), \\end{equation} Given this definition, \\eqref{eq:vpg.1} can be written in a form that does not require a dynamics model: \\begin{align} \\hspace{-0.8cm}\\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\nabla_\\theta\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\big[R(\\tau)\\big] \\\\ \u0026amp;=\\nabla_\\theta\\sum_\\tau P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\sum_\\tau\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}\\nabla_\\theta P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\sum_\\tau P(\\theta;\\tau)\\nabla_\\theta\\log P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\big[\\nabla_\\theta\\log P(\\tau;\\theta)R(\\tau)\\big] \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\nabla_\\theta\\left(\\sum_{t=0}^{H-1}\\log\\rho_0(s_0)+\\log P(s_{t+1}\\vert s_t,a_t)+\\log\\pi_\\theta(a_t\\vert s_t)\\right)R(\\tau)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)R(\\tau)\\right] \\end{align} Since the gradient is now an expectation, we can approximate it with the empirical estimate from $m$ sample trajectories, as \\begin{equation} \\nabla_\\theta\\eta(\\pi_\\theta)=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})R(\\tau^{(i)}) \\end{equation}\nVariance reduction Reward-to-go To reduce the variance, we first notice that the total reward along a trajectory $\\tau$, $R(\\tau)$, can be expressed as sum of total reward from step $t$, called reward-to-go from $t$, and total preceding rewards w.r.t $t$, which has the expected value of zero but non-zero variance. In particular, we can simplify the policy gradient to be indepedent to the reward-to-go only, as: \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)R(\\tau)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\left(\\sum_{k=0}^{t-1}r_k+\\sum_{k=t}^{H-1}r_k\\right)\\right] \\\\ \u0026amp;=\\sum_{t=0}^{H-1}\\mathbb{E}_{s_{0:t},a_{0:t}}\\left[\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{k=0}^{t-1}r_k\\right]\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{k=t}^{H-1}r_k\\right] \\\\ \u0026amp;\\overset{\\text{(i)}}{=}\\sum_{t=0}^{H-1}\\left(\\mathbb{E}_{s_{0:t},a_{0:t-1}}\\left[\\mathbb{E}_{a_t}\\big[\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big]\\cdot\\sum_{k=0}^{t-1}r_k\\right]\\right)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{k=t}^{H-1}r_k\\right]\\label{eq:vr.1} \\\\ \u0026amp;\\overset{\\text{(ii)}}{=}\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\sum_{k=t}^{H-1}r_k\\right] \\\\ \u0026amp;\\overset{\\text{(iii)}}{=}\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\hat{r}_t\\right],\\label{eq:vr.2} \\end{align} where\nThe (i) step is due to that the total past reward is independent of the current action $t$. In the (ii) step, we have used \\begin{align} \\mathbb{E}_{a_t}\\big[\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big]\u0026amp;=\\sum_{a_t}\\pi_\\theta(a_t\\vert s_t)\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t) \\\\ \u0026amp;=\\sum_{a_t}\\nabla_\\theta 1=\\mathbf{0}\\label{eq:vr.3} \\end{align} In the (iii) step, the $\\hat{r}_t\\doteq\\sum_{k=t}^{H-1}r_k$ is referred as the reward-to-go from step $t$. Baseline It is worth remarking that we can furtherly reduce the variance of the estimator by adding an baseline, denoted $b$, as \\begin{equation} \\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\left(\\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\\right)\\right]\\label{eq:vrb.1} \\end{equation}\nUnbiased estimator First we will prove that the estimator with baseline \\eqref{eq:vrb.1} is still an unbiased with \\eqref{eq:vr.2}. Specifically \\begin{align} \u0026amp;\\hspace{-1cm}\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\left(\\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\\right)\\right]\\nonumber \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\hat{r}_t\\right]\\nonumber \\\\ \u0026amp;\\hspace{0.5cm}-\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)b_t(s_{0:t},a_{0:t-1})\\right], \\end{align} which makes our claim follow if the latter expectation of the RHS is zero. In fact, using the logic as in \\eqref{eq:vr.1} and the result \\eqref{eq:vr.3}, we have \\begin{align} \u0026amp;\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)b_t(s_{0:t},a_{0:t-1})\\right]\\nonumber \\\\ \u0026amp;=\\sum_{t=0}^{H-1}\\mathbb{E}_{s_{0:t},a_{0:t-1}}\\Big[\\mathbb{E}_{a_t}\\big[\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)b_t(s_{0:t},a_{0:t-1})\\big]\\Big] \\\\ \u0026amp;=\\sum_{t=0}^{H-1}\\mathbb{E}_{s_{0:t},a_{0:t-1}}\\Big[\\mathbb{E}_{a_t}\\big[\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big]\\cdot b_t(s_{0:t},a_{0:t-1})\\Big] \\\\ \u0026amp;=\\sum_{t=0}^{H-1}\\mathbb{E}_{s_{0:t},a_{0:t-1}}\\Big[\\mathbf{0}\\cdot b_t(s_{0:t},a_{0:t-1})\\Big]=\\mathbf{0} \\end{align}\nHow can a baseline reduce variance? By definition of the variance of a r.v $X$ \\begin{equation} \\text{Var}(X)=\\mathbb{E}\\big[X^2\\big]-\\mathbb{E}\\big[X\\big]^2, \\end{equation} combined with the claim that \\eqref{eq:vrb.1} is an unbiased estimator of the policy gradient $\\nabla_\\theta\\eta(\\pi_\\theta)$, and letting $b_t\\doteq b_t(s_{0:t},a_{0:t-1})$ to simplify the notation, we have \\begin{align} \\text{Var}\u0026amp;=\\text{Var}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big(\\hat{r}_t-b_t\\big)\\right] \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\left(\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big(\\hat{r}_t-b_t\\big)\\right)^2\\right]\\nonumber \\\\ \u0026amp;\\hspace{1.5cm}-\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big(\\hat{r}_t-b_t\\big)\\right]^2 \\\\ \u0026amp;=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\left[\\left(\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big(\\hat{r}_t-b_t\\big)\\right)^2\\right]-\\big(\\nabla_\\theta\\eta(\\pi_\\theta)\\big)^2, \\end{align} which suggests us that for each $t$, by finding $b_t$ that minimizes the former expectation, we can also reduce the variance $\\text{Var}$.\nDifferentiating the variance w.r.t $b_t$ gives us \\begin{equation} \\frac{\\partial\\text{Var}}{\\partial b_t}=\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\Big[\\big(\\nabla_\\theta\\log\\pi_\\theta(a_t\\vert s_t)\\big)^2(2b_t-2\\hat{r}_t)\\Big] \\end{equation} Set the derivative to zero and solve for $b_t$, we obtain the optimal baseline \\begin{equation} b_t=\\frac{\\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\\Big[\\big(\\nabla_\\theta\\pi_\\theta(a_t\\vert s_t)\\big)^2\\hat{r}_t\\Big]}{\\mathbb{E}_{a_t}\\Big[\\big(\\nabla_\\theta\\pi_\\theta(a_t\\vert s_t)\\big)^2\\Big]} \\end{equation}\nTypes of baseline Discount factor Preferences [1] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel. High-Dimensional Continuous Control Using Generalized Advantage Estimation. ICLR 2016.\nFootnotes","permalink":"https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on Policy gradient methods.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Policy Gradient"},{"content":" Notes on CMA - Evolution Strategy.\nPreliminaries The condition number of a matrix $\\mathbf{A}$ is defined by \\begin{equation} \\kappa(\\mathbf{A})\\doteq\\Vert\\mathbf{A}\\Vert\\Vert\\mathbf{A}^{-1}\\Vert, \\end{equation} where $\\Vert\\mathbf{A}\\Vert=\\sup_{\\Vert\\mathbf{x}\\Vert=1}\\Vert\\mathbf{Ax}\\Vert$.\nFor $\\mathbf{A}$ that is non-singular, $\\kappa(\\mathbf{A})=\\infty$.\nFor $\\mathbf{A}$ which is positive definite, we thus have $\\Vert\\mathbf{A}\\Vert=\\lambda_\\text{max}$ where $\\lambda_\\text{max}$ denotes the largest eigenvalue of $\\mathbf{A}$, correspondingly $\\lambda_\\text{min}$ denotes the smallest eigenvalue of $\\mathbf{A}$. The condition number of $\\mathbf{A}$ therefore can be written as \\begin{equation} \\kappa(\\mathbf{A})=\\frac{\\lambda_\\text{max}}{\\lambda_\\text{min}}\\geq 1, \\end{equation} since corresponding to each eigenvalue $\\lambda$ of $\\mathbf{A}$, the inverse matrix $\\mathbf{A}^{-1}$ takes $1/\\lambda$ as its eigenvalue.\nBasic equation In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\\ldots$ \\begin{equation} \\mathbf{x}_k^{(t+1)}\\sim\\boldsymbol{\\mu}^{(t)}+\\sigma^{(t)}\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}^{(t)})\\sim\\mathcal{N}\\left(\\boldsymbol{\\mu}^{(t)},{\\sigma^{(t)}}^2\\boldsymbol{\\Sigma}^{(t)}\\right),\\hspace{1cm}k=1,\\ldots,\\lambda\\label{eq:be.1} \\end{equation} where\n$\\mathbf{x}_k^{(t+1)}\\in\\mathbb{R}^n$: the $k$-th sample at generation $t+1$. $\\boldsymbol{\\mu}^{(t)}\\in\\mathbb{R}^n$: mean of the search distribution at generation $t$. $\\sigma^{(t)}\\in\\mathbb{R}$: step-size at generation $t$. $\\boldsymbol{\\Sigma}^{(t)}$: covariance matrix at generation $t$. ${\\sigma^{(t)}}^2\\boldsymbol{\\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$. $\\lambda\\geq 2$: sample size. Updating the mean The mean $\\boldsymbol{\\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\\gamma$ selected points from the sample $\\mathbf{x}_1^{(t+1)},\\ldots,\\mathbf{x}_\\lambda^{(t+1)}$: \\begin{equation} \\boldsymbol{\\mu}^{(t+1)}=\\sum_{i=1}^{\\gamma}w_i\\mathbf{x}_{i:\\lambda}^{(t+1)},\\label{eq:um.1} \\end{equation} where\n$\\sum_{i=1}^{\\gamma}w_i=1$ with $w_1\\geq w_2\\geq\\ldots\\geq w_{\\gamma}\u0026gt;0$. $\\gamma\\leq\\lambda$: number of selected points. $\\mathbf{x}_{i:\\lambda}^{(t+1)}$: $i$-th best sample out of $\\mathbf{x}_1^{(t+1)},\\ldots,\\mathbf{x}_\\lambda^{(t+1)}$ from \\eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have \\begin{equation} f(\\mathbf{x}_{1:\\lambda}^{(t+1)})\\geq f(\\mathbf{x}_{2:\\lambda}^{(t+1)})\\geq\\ldots\\geq f(\\mathbf{x}_{\\lambda:\\lambda}^{(t+1)}) \\end{equation} We can rewrite \\eqref{eq:um.1} as an update rule for the mean $\\boldsymbol{\\mu}$ \\begin{equation} \\boldsymbol{\\mu}^{(t+1)}=\\boldsymbol{\\mu}^{(t)}+\\alpha_\\boldsymbol{\\mu}\\sum_{i=1}^{\\gamma}w_i\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right), \\end{equation} where $\\alpha_\\boldsymbol{\\mu}\\leq 1$ is the learning rate, which is usually set to $1$.\nWhen choosing the weight values $w_i$ and population size $\\gamma$ for recombination, we take into account the variance effective selection mass, denoted as $\\gamma_\\text{eff}$, given by \\begin{equation} \\gamma_\\text{eff}\\doteq\\left(\\frac{\\Vert\\mathbf{w}\\Vert_1}{\\Vert\\mathbf{w}\\Vert_2}\\right)=\\frac{\\Vert\\mathbf{w}\\Vert_1^2}{\\Vert\\mathbf{w}\\Vert_2^2}=\\frac{1}{\\sum_{i=1}^{\\gamma}w_i^2} \\end{equation} where $\\mathbf{w}$ is defined as the weight vector \\begin{equation} \\mathbf{w}=(w_1,\\ldots,w_\\gamma)^\\text{T} \\end{equation}\nAdapting the covariance matrix The covariance matrix can be estimated from scratch using the population of the current generation or can be estimated with covariance matrix from previous generations.\nEstimating from scratch Rather than using the empirical covariance matrix as an estimator for $\\boldsymbol{\\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation \\begin{equation} \\boldsymbol{\\Sigma}_\\lambda^{(t+1)}=\\frac{1}{\\lambda{\\sigma^{(t)}}^2}\\sum_{i=1}^{\\lambda}\\left(\\mathbf{x}_i^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)\\left(\\mathbf{x}_i^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)^\\text{T}\\label{eq:es.1} \\end{equation} Notice that in the above estimation \\eqref{eq:es.1}, we have used all of the $\\lambda$ samples. We thus can estimate a better covariance matrix by select some of the best individual out of $\\lambda$ samples, which is analogous to how we update the mean $\\boldsymbol{\\mu}$.\nIn particular, we instead consider the estimation \\begin{equation} \\boldsymbol{\\Sigma}_{\\gamma}^{(t+1)}=\\frac{1}{{\\sigma^{(t)}}^2}\\sum_{i=1}^{\\gamma}w_i\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)^\\text{T},\\label{eq:es.2} \\end{equation} where $\\gamma\\leq\\lambda$ is the number of selected points; the weights $w_i$ and selected points $\\mathbf{x}_{i:\\lambda}^{(t+1)}$ are defined as given in the update for $\\boldsymbol{\\mu}$.\nRank-$\\gamma$ update In order to ensure that \\eqref{eq:es.2} is a reliable estimator, the selected population must be large enough. However, to get a fast search, the population size $\\lambda$ must be small, which lets the selected sample size consequently small also. Thus, we can not get a reliable estimator for a good covariance matrix from \\eqref{eq:es.2}. However, we can use the history as a helping hand.\nIn particular, if we have experienced a sufficient number of generations, the mean of the $\\boldsymbol{\\Sigma}_\\gamma$ from all previous generations \\begin{equation} \\boldsymbol{\\Sigma}^{(t+1)}=\\frac{1}{t+1}\\sum_{i=0}^{t}\\boldsymbol{\\Sigma}_\\gamma^{(i+1)}\\label{eq:rlmu.1} \\end{equation} would be a reliable estimator.\nIn addition, it is reasonable that the recent generations will have more affection to the current generation than the distant ones. Hence, rather than assigning estimated covariance matrices $\\boldsymbol{\\Sigma}_\\gamma$ from preceding generations the same weight as in \\eqref{eq:rlmu.1}, it would be a better choice to give the more recent generations the higher weight.\nSpecifically, starting with an initial $\\boldsymbol{\\Sigma}^{(0)}=\\mathbf{I}$, we consider the update, called rank-$\\gamma$ update, for the covariance matrix at generation $t+1$ using exponential smoothing1 as \\begin{align} \\boldsymbol{\\Sigma}^{(t+1)}\u0026amp;=(1-\\alpha_\\gamma)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_\\gamma\\boldsymbol{\\Sigma}_\\gamma^{(t+1)} \\\\ \u0026amp;=(1-\\alpha_\\gamma)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_\\gamma\\frac{1}{{\\sigma^{(t)}}^2}\\sum_{i=1}^{\\gamma}w_i\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)^\\text{T} \\\\ \u0026amp;=(1-\\alpha_\\gamma)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_\\gamma\\sum_{i=1}^{\\gamma}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)}{\\mathbf{y}_{i:\\lambda}^{(t+1)}}^\\text{T},\\label{eq:rlmu.2} \\end{align} where\n$\\alpha_\\gamma\\leq 1$: learning rate. $w_1,\\ldots,w_\\gamma$ and $\\mathbf{x}_{1:\\lambda}^{(t+1)},\\ldots,\\mathbf{x}_{\\lambda:\\lambda}^{(g+1)}$ are defined as usual. $\\mathbf{y}_{i:\\lambda}^{(t+1)}=(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)})/\\sigma^{(t)}$. The update \\eqref{eq:rlmu.2} can be generalized to $\\lambda$ weights values which neither necessarily sum to $1$, nor be non-negative anymore, as \\begin{align} \\boldsymbol{\\Sigma}^{(t+1)}\u0026amp;=\\left(1-\\alpha_\\gamma\\sum_{i=1}^{\\lambda}w_i\\right)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_\\gamma\\sum_{i=1}^{\\lambda}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)}{\\mathbf{y}_{i:\\lambda}^{(t+1)}}^\\text{T}\\label{eq:rlmu.3} \\\\ \u0026amp;={\\boldsymbol{\\Sigma}^{(t)}}^{1/2}\\left[\\mathbf{I}+\\alpha_\\gamma\\sum_{i=1}^{\\lambda}w_i\\left(\\mathbf{z}_{i:\\lambda}^{(t+1)}{\\mathbf{z}_{i:\\lambda}^{(t+1)}}^\\text{T}-\\mathbf{I}\\right)\\right]{\\boldsymbol{\\Sigma}^{(t)}}^{1/2}, \\end{align} where\n$w_1\\geq\\ldots\\geq w_\\gamma\u0026gt;0\\geq w_{\\gamma+1}\\geq\\ldots\\geq w_\\lambda\\in\\mathbb{R}$, and usually $\\sum_{i=1}^{\\gamma}w_i=1$ and $\\sum_{i=1}^{\\lambda}w_i\\approx 0$. $\\mathbf{z}_{i:\\lambda}^{(t+1)}={\\boldsymbol{\\Sigma}^{(t)}}^{1/2}\\mathbf{y}_{i:\\lambda}^{(t+1)}$ is the mutation vector. Rank-one-update We first consider a method that produces an $n$-dimensional normal distribution with zero mean. Specifically, let $\\mathbf{y}_1,\\ldots,\\mathbf{y}_{t_0}\\in\\mathbb{R}^n$, for $t_0\\geq n$ be vectors span $\\mathbb{R}^n$. We thus have that \\begin{align} \\mathcal{N}(0,1)\\mathbf{y}_1+\\ldots+\\mathcal{N}(0,1)\\mathbf{y}_{t_0}\u0026amp;\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{y}_1\\mathbf{y}_1^\\text{T})+\\ldots+\\mathcal{N}(\\mathbf{0},\\mathbf{y}_{t_0}\\mathbf{y}_{t_0}^\\text{T}) \\\\ \u0026amp;\\sim\\mathcal{N}\\left(\\mathbf{0},\\sum_{i=1}^{t_0}\\mathbf{y}_i\\mathbf{y}_i^\\text{T}\\right) \\end{align} The covariance matrix $\\mathbf{y}_i\\mathbf{y}_i^\\text{T}$ has rank one, with only one eigenvalue $\\Vert\\mathbf{y}_i\\Vert^2$ and a corresponding eigenvector within the form $\\alpha\\mathbf{y}_i$ for $\\alpha\\in\\mathbb{R}$. Using the above equation, we can generate any MVN distribution.\nConsider the update \\eqref{eq:rlmu.3} with $\\gamma=1$ and let $\\mathbf{y}_{t+1}=\\left(\\mathbf{x}_{1:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)/\\sigma^{(t)}$, the rank-one update for the covariance matrix $\\boldsymbol{\\Sigma}^{(t+1)}$ is given by \\begin{equation} \\boldsymbol{\\Sigma}^{t+1}=(1-\\alpha_1)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_1\\mathbf{y}_{t+1}\\mathbf{y}_{t+1}^\\text{T} \\end{equation} The latter summand in the RHS has rank one and adds the maximum likelihood term for $\\mathbf{y}_{t+1}$ into the covariance matrix $\\boldsymbol{\\Sigma}^{(t)}$, which makes the probability of generating $\\mathbf{y}_{t+1}$ in the generation $t+1$ increase.\nWe continue by noticing that to update the covariance matrix $\\boldsymbol{\\Sigma}^{(t+1)}$, in \\eqref{eq:rlmu.3}, we have used the selected steps \\begin{equation} \\mathbf{y}_{i:\\lambda}^{(g+1)}=\\frac{\\mathbf{x}_{i:\\lambda}^{(g+1)}-\\boldsymbol{\\mu}^{(g)}}{\\sigma^{(g)}} \\end{equation} However, since \\begin{equation} \\mathbf{y}_{i:\\lambda}^{(g+1)}{\\mathbf{y}_{i:\\lambda}^{(g+1)}}^\\text{T}=-\\mathbf{y}_{i:\\lambda}^{(g+1)}\\left(-\\mathbf{y}_{i:\\lambda}^{(g+1)}\\right)^\\text{T}, \\end{equation} which means the sign information is lost when computing the covariance matrix. To track the sign information to the update rule of $\\boldsymbol{\\Sigma}^{(t+1)}$, we use evolution path, which defined as a sequence of successive steps over number of generations.\nIn particular, analogy to \\eqref{eq:rlmu.3}, we use exponential smoothing to establish the evolution path, $\\mathbf{p}_c\\in\\mathbb{R}^n$, which starting with an initial value $\\mathbf{p}_c^{(0)}=\\mathbf{0}$ and being updated with \\begin{align} \\mathbf{p}_c^{(t+1)}\u0026amp;=(1-\\alpha_c)\\mathbf{p}_c^{(t)}+\\sqrt{(1-(1-\\alpha_c)^2)\\mu_\\text{eff}}\\sum_{i=1}^{\\gamma}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)} \\\\ \u0026amp;=(1-\\alpha_c)\\mathbf{p}_c^{(t)}+\\sqrt{\\alpha_c(2-\\alpha_c)\\gamma_\\text{eff}}\\sum_{i=1}^{\\gamma}\\frac{w_i\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)}{\\sigma^{(t)}} \\\\ \u0026amp;=(1-\\alpha_c)\\mathbf{p}_c^{(t)}+\\sqrt{\\alpha_c(2-\\alpha_c)\\gamma_\\text{eff}}\\frac{1}{\\sigma^{(t)}}\\left[\\left(\\sum_{i=1}^{\\gamma}w_i\\mathbf{x}_{i:\\lambda}^{(t+1)}\\right)-\\boldsymbol{\\mu}^{(t)}\\sum_{i=1}^{\\gamma}w_i\\right] \\\\ \u0026amp;=(1-\\alpha_c)\\mathbf{p}_c^{(t)}+\\sqrt{\\alpha_c(2-\\alpha_c)\\gamma_\\text{eff}}\\frac{\\boldsymbol{\\mu}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}}{\\sigma^{(t)}}, \\end{align} where\n$\\mathbf{p}_c^{(t)}\\in\\mathbb{R}^n$ is the evolution path at generation $t$. $\\alpha_c\\leq 1$ is the learning rate. $\\sqrt{\\alpha_c(2-\\alpha_c)\\gamma_\\text{eff}}$ is a normalization factor for $\\mathbf{p}_c^{(t+1)}$ such that \\begin{equation} \\mathbf{p}_c^{(t+1)}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}), \\end{equation} since by $\\mathbf{y}_{i:\\lambda}^{(t+1)}=(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)})/\\sigma^{(t)}$ we have that \\begin{equation} \\mathbf{p}_c^{(t)}\\sim\\mathbf{y}_{i:\\lambda}^{(t+1)}\\sim\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}),\\hspace{1cm}\\forall i=1,\\ldots,\\gamma \\end{equation} which by $\\gamma_\\text{eff}=\\left(\\sum_{i=1}^{\\gamma}w_i^2\\right)^{-1}$ implies that \\begin{equation} \\sum_{i=1}^{\\gamma}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)}\\sim\\frac{1}{\\sqrt{\\gamma_\\text{eff}}}\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}) \\end{equation} The rank-one update for the covariance matrix $\\boldsymbol{\\Sigma}^{(t)}$ via the evolution path $\\mathbf{p}_c^{(t+1)}$ then given as \\begin{equation} \\boldsymbol{\\Sigma}^{(t+1)}=(1-\\alpha_1)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_1\\mathbf{p}_c^{(t+1)}{\\mathbf{p}_c^{(t+1)}}^\\text{T},\\label{eq:rou.1} \\end{equation} An empirical validated choice for the learning rate $\\alpha_1$ is $\\alpha_1\\approx 2/n^2$.\nFinal update Combining rank-$\\gamma$ update \\eqref{eq:rlmu.3} and rank-one update \\eqref{eq:rou.1} together, we obtain the final update for the covariance matrix $\\boldsymbol{\\Sigma}^{(t+1)}$ as \\begin{equation} \\boldsymbol{\\Sigma}^{(t+1)}=\\left(1-\\alpha_1-\\alpha_\\gamma\\sum_{i=1}^{\\lambda}w_i\\right)\\boldsymbol{\\Sigma}^{(t)}+\\alpha_1\\mathbf{p}_c^{(t+1)}{\\mathbf{p}_c^{(t+1)}}^\\text{T}+\\alpha_\\gamma\\sum_{i=1}^{\\lambda}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)}{\\mathbf{y}_{i:\\lambda}^{(t+1)}}^\\text{T}, \\end{equation} where\n$\\alpha_1\\approx 2/n^2$. $\\alpha_\\gamma\\approx\\min(\\gamma_\\text{eff}/n^2,1-\\alpha_1)$. $\\mathbf{y}_{i:\\lambda}^{(t+1)}=\\left(\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}\\right)/\\sigma^{(t)}$. $\\sum_{i=1}^{\\lambda}w_i\\approx-\\alpha_1/\\alpha_\\gamma$. Controlling the step-size To control the step-size $\\sigma^{(t)}$, similar to how we cumulatively update the covariance matrix by rank-one covariance matrices, we also use an evolution path, which is defined as sum of successive steps $\\boldsymbol{\\mu}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}$.\nHowever, in this step-size adaption, we utilize a conjugate evolution path $\\mathbf{p}_\\sigma$, which begins with an initial value $\\mathbf{p}_\\sigma^{(0)}=\\mathbf{0}$ and is repeatedly updated by \\begin{align} \\mathbf{p}_\\sigma^{(t+1)}\u0026amp;=(1-\\alpha_\\sigma)\\mathbf{p}_\\sigma^{(t)}+\\sqrt{(1-(1-\\alpha_\\sigma)^2)\\gamma_\\text{eff}}{\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}\\sum_{i=1}^{\\gamma}w_i\\mathbf{y}_{i:\\lambda}^{(t+1)} \\\\ \u0026amp;=1-\\alpha_\\sigma)\\mathbf{p}_\\sigma^{(t)}+\\sqrt{\\alpha_\\sigma(2-\\alpha_\\sigma)\\gamma_\\text{eff}}{\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}\\sum_{i=1}^{\\gamma}w_i\\frac{\\mathbf{x}_{i:\\lambda}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}}{\\sigma^{(t)}} \\\\ \u0026amp;=1-\\alpha_\\sigma)\\mathbf{p}_\\sigma^{(t)}+\\sqrt{\\alpha_\\sigma(2-\\alpha_\\sigma)\\gamma_\\text{eff}}{\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}\\frac{1}{\\sigma^{(t)}}\\left[\\left(\\sum_{i=1}^{\\gamma}w_i\\mathbf{x}_{i:\\lambda}^{(t+1)}\\right)-\\boldsymbol{\\mu}^{(t)}\\sum_{i=1}^{\\gamma}w_i\\right] \\\\ \u0026amp;=1-\\alpha_\\sigma)\\mathbf{p}_\\sigma^{(t)}+\\sqrt{\\alpha_\\sigma(2-\\alpha_\\sigma)\\gamma_\\text{eff}}{\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}\\frac{\\boldsymbol{\\mu}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}}{\\sigma^{(t)}}, \\end{align} where\n$\\mathbf{p}_\\sigma^{(t)}\\in\\mathbb{R}^n$ is the conjugate evolution path at generation $t$. $\\alpha_\\sigma\u0026lt;1$ is the learning rate. $\\sqrt{\\alpha_c(2-\\alpha_c)\\gamma_\\text{eff}}$ is a normalization factor for $\\mathbf{p}_\\sigma^{(t+1)}$, which analogously to the covariance matrix adaption, lets \\begin{equation} \\mathbf{p}_\\sigma^{(t+1)}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I}) \\end{equation} The covariance matrix ${\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}$ is defined as \\begin{equation} {\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}\\doteq\\mathbf{Q}^{(t)}{\\boldsymbol{\\Lambda}^{(t)}}^{-1/2}{\\mathbf{Q}^{(t)}}^\\text{T},\\label{eq:cs.1} \\end{equation} where \\begin{equation} \\hspace{-0.8cm}\\boldsymbol{\\Sigma}^{(t)}=\\mathbf{Q}^{(t)}\\boldsymbol{\\Lambda}^{(t)}{\\mathbf{Q}^{(t)}}^\\text{T}=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\mathbf{q}_1^{(t)}\u0026amp;\\ldots\u0026amp;\\mathbf{q}_n^{(t)} \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right]\\left[\\begin{matrix}\\lambda_1^{(t)}\u0026amp;\u0026amp; \\\\ \u0026amp;\\ddots\u0026amp; \\\\ \u0026amp;\u0026amp; \\lambda_n^{(t)}\\end{matrix}\\right]\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\mathbf{q}_1^{(t)}\u0026amp;\\ldots\u0026amp;\\mathbf{q}_n^{(t)} \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right]^\\text{T} \\end{equation} is an eigendecomposition of the positive definite covariance matrix $\\boldsymbol{\\Sigma}^{(t)}$, where $\\mathbf{Q}^{(t)}\\in\\mathbb{R}^{n\\times n}$ is an orthonormal matrix whose columns are unit eigenvectors $\\mathbf{q}_i^{(t)}$ of $\\boldsymbol{\\Sigma}^{(t)}$ and $\\boldsymbol{\\Lambda}^{(t)}\\in\\mathbb{R}^{n\\times n}$ is a diagonal matrix whose diagonal entries are eigenvalues $\\lambda_i^{(t)}$ of $\\boldsymbol{\\Sigma}^{(t)}$.\nMoreover, for each eigenvalue, eigenvector pair $(\\lambda_i^{(t)},\\mathbf{q}_i^{(t)})$ of $\\boldsymbol{\\Sigma}^{(t)}$ we have \\begin{equation} \\lambda_i^{(t)}{\\boldsymbol{\\Sigma}^{(t)}}^{-1}\\mathbf{q}_i^{(t)}={\\boldsymbol{\\Sigma}^{(t)}}^{-1}\\boldsymbol{\\Sigma}^{(t)}\\mathbf{q}_i^{(t)}=\\mathbf{q}_i^{(t)}, \\end{equation} or \\begin{equation} {\\boldsymbol{\\Sigma}^{(t)}}^{-1}\\mathbf{q}_i^{(t)}=\\frac{1}{\\lambda_i^{(t)}}\\mathbf{q}_i^{(t)}, \\end{equation} or in other words, $(1/\\lambda_i^{(t)},\\mathbf{q}_i^{(t)})$ is an eigenvalue, eigenvector pair of ${\\boldsymbol{\\Sigma}^{(t)}}^{-1}$. Therefore, the inverse of $\\boldsymbol{\\Sigma}^{(t)}$, which is also positive definite can be written by \\begin{equation} {\\boldsymbol{\\Sigma}^{(t)}}^{-1}=\\mathbf{Q}^{(t)}\\left[\\begin{matrix}1/\\lambda_1^{(t)}\u0026amp;\u0026amp; \\\\ \u0026amp;\\ddots\u0026amp; \\\\ \u0026amp;\u0026amp; 1/\\lambda_n^{(t)}\\end{matrix}\\right]{\\mathbf{Q}^{(t)}}^\\text{T}=\\mathbf{Q}^{(t)}{\\boldsymbol{\\Lambda}^{(t)}}^{-1}{\\mathbf{Q}^{(t)}}^\\text{T}, \\end{equation} which allows us to obtain the representation \\eqref{eq:cs.1} of ${\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}$. The transformation ${\\boldsymbol{\\Sigma}^{(t)}}^{-1/2}=\\mathbf{Q}^{(t)}{\\boldsymbol{\\Lambda}^{(t)}}^{-1/2}{\\mathbf{Q}^{(t)}}^\\text{T}$ re-scales length of the step $\\boldsymbol{\\mu}^{(t+1)}-\\boldsymbol{\\mu}^{(t)}$ without changing its direction. In more specific:\n${\\mathbf{Q}^{(t)}}^\\text{T}$ transform the original space into the coordinate space with columns of $\\mathbf{Q}^{(t)}$, which is also the eigenvectors of $\\boldsymbol{\\Sigma}^{(t)}$ or the principle axes of $\\mathcal{N}(\\mathbf{0},\\boldsymbol{\\Sigma}^{(t)})$, as its principle axes. ${\\boldsymbol{\\Lambda}^{(t)}}^{-1/2}$ re-scales the principle axes to have the same length. $\\mathbf{Q}^{(t)}$ transforms the coordinate system back to the original space. It means that this transformation makes the expected length of $\\mathbf{p}_\\sigma^{(t+1)}$ independent of its direction.\nWe then update $\\sigma^{(t)}$ by according to the ratio of its length with its expected length $\\Vert\\mathbf{p}_\\sigma^{(t+1)}\\Vert/\\mathbb{E}\\Vert\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\Vert$, given by \\begin{equation} \\log\\sigma^{(t+1)}=\\log\\sigma^{(t)}+\\frac{\\alpha_\\sigma}{d_\\sigma}\\left(\\frac{\\Vert\\mathbf{p}_\\sigma^{(t+1)}\\Vert}{\\mathbb{E}\\Vert\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\Vert}-1\\right), \\end{equation} where $d_\\sigma\\approx 1$ is the damping parameter, which controls the update size. Therefore, since $\\sigma^{(t)}\u0026gt;0$, we have the update rule for $\\sigma^{(t)}$ is given by \\begin{equation} \\sigma^{(t+1)}=\\sigma^{(t)}\\exp\\left(\\frac{\\alpha_\\sigma}{d_\\sigma}\\left(\\frac{\\Vert\\mathbf{p}_\\sigma^{(t+1)}\\Vert}{\\mathbb{E}\\Vert\\mathcal{N}(\\mathbf{0},\\mathbf{I})\\Vert}-1\\right)\\right) \\end{equation}\nTesting on Rastrigin function Let us give the CMA-ES algorithm a try on the Rastrigin function, $f:\\mathbb{R}^n\\to\\mathbb{R}$, which is given by \\begin{equation} f(\\mathbf{x})=10 n+\\sum_{i=1}^{n}x_i^2-10\\cos\\left(2\\pi x_i\\right) \\end{equation} The global minimum of $f(\\mathbf{x})$ is $0$ at $\\mathbf{x}=\\mathbf{0}$. We will be using the experimental settings given in this paper proposed by CMA-ES\u0026rsquo;s original author. Each time we end up with a result less than $f_\\text{stop}=10^{-10}$, we count it a success run.\nThe result obtained is illustrated in the following figure.\nFigure 1: Success rate to reach $f_\\text{stop}=10^{-10}$ versus population size for Rastrigin function.\nThe code can be found here References [1] Nikolaus Hansen. The CMA Evolution Strategy: A Tutorial. arXiv:1604.00772, 2016.\n[2] Nikolaus Hansen, Youhei Akimoto \u0026amp; Petr Baudis. CMA-ES/pycma on Github. Zenodo, DOI:10.5281/zenodo.2559634, February 2019.\n[3] Nikolaus Hansen, Stefan Kern. Evaluating the CMA Evolution Strategy on Multimodal Test Functions. Parallel Problem Solving from Nature - PPSN VIII. PPSN 2004.\n[4] Ha, David. A Visual Guide to Evolution Strategies. blog.otoro.net, 2017.\nFootnotes The simplest form of exponential smoothing is given by the formula \\begin{align*} s_0\u0026amp;=x_0 \\\\ s_t\u0026amp;=\\alpha x_t+(1-\\alpha)s_{t-1},\\hspace{1cm}t\u0026gt;0 \\end{align*} where $0\u0026lt;\\alpha\u0026lt;1$ is referred as the smoothing factor.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/evolution-strategy/cma-es/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on CMA - Evolution Strategy.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"CMA Evolution Strategy"},{"content":" (Temporary being stopped) Note III of the measure theory series. Materials are mostly taken from Tao\u0026rsquo;s book, except for some needed notations extracted from Stein\u0026rsquo;s book.\nIntegration of simple functions Analogy to how the Riemann integral was established by using the integral for piecewise constant functions, the Lebesgue integral is set up using the integral for simple functions.\nSimple function A (complex-valued) simple function $f:\\mathbb{R}^d\\to\\mathbb{C}$ is a finite linear combination \\begin{equation} f=c_1 1_{E_1}+\\ldots+c_k 1_{E_k},\\label{eq:sf.1} \\end{equation} of indicator functions $1_{E_i}$ of Lebesgue measurable sets $E_i\\subset\\mathbb{R}^d$ for $i=1,\\ldots,k$, for natural number $k\\geq 0$ and where $c_1,\\ldots,c_k\\in\\mathbb{C}$ are complex numbers.\nAn unsigned simple function $f:\\mathbb{R}^d\\to[0,+\\infty]$ is given as \\eqref{eq:sf.1} but with the $c_i$ taking values in $[0,+\\infty]$ rather than $\\mathbb{C}$.\nIntegral of a unsigned simple function If $f=c_1 1_{E_1}+\\ldots+c_k 1_{E_k}$ is an unsigned simple function, the integral $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx$ is defined by the formula \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\doteq c_1 m(E_1)+\\ldots+c_k m(E_k), \\end{equation} which means $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\in[0,+\\infty]$.\nWell-definedness of simple integral Lemma 1\nLet $k,k\u0026rsquo;\\geq 0$ be natural, $c_1,\\ldots,c_k,c_1\u0026rsquo;,\\dots,c_{k\u0026rsquo;}\u0026rsquo;\\in[0,+\\infty]$ and $E_1,\\ldots,E_k,E_1\u0026rsquo;,\\ldots,E_{k\u0026rsquo;}\u0026rsquo;\\subset\\mathbb{R}^d$ be Lebesgue measurable sets such that the identity \\begin{equation} c_1 1_{E_1}+\\ldots+c_k 1_{E_k}=c_1\u0026rsquo; 1_{E_1\u0026rsquo;}+\\ldots+c_{k\u0026rsquo;}\u0026rsquo; 1_{E_{k\u0026rsquo;}\u0026rsquo;}\\label{eq:lemma1.1} \\end{equation} holds identically on $\\mathbb{R}^d$. Then we have \\begin{equation} c_1 m(E_1)+\\ldots+c_k m(E_k)=c_1\u0026rsquo; m(E_1\u0026rsquo;)+\\ldots+c_{k\u0026rsquo;}\u0026rsquo; m(E_{k\u0026rsquo;}\u0026rsquo;) \\end{equation}\nProof\nThe $k+k\u0026rsquo;$ sets $E_1,\\ldots,E_k,E_1\u0026rsquo;,\\ldots,E_{k\u0026rsquo;}\u0026rsquo;$ partition $\\mathbb{R}^d$ into $2^{k+k\u0026rsquo;}$ disjoint sets, each of which is an intersection of some of the $E_1,\\ldots,E_k,E_1\u0026rsquo;,\\ldots,E_{k\u0026rsquo;}\u0026rsquo;$ and their complements1.\nRemoving any sets that are empty, we end up with a partition of $R^d$ of $m$ non-empty disjoint sets $A_1,\\ldots,A_m$ for some $0\\leq m\\leq 2^{k+k\u0026rsquo;}$. It easily seen that $A_1,\\ldots,A_m$ are then Lebesgue measurable due to the Lebesgue measurability of $E_1,\\ldots,E_k,E_1\u0026rsquo;,\\ldots,E_{k\u0026rsquo;}\u0026rsquo;$.\nWith this set up, each of the $E_1,\\ldots,E_k,E_1\u0026rsquo;,\\ldots,E_{k\u0026rsquo;}\u0026rsquo;$ are unions of some of the $A_1,\\ldots,A_m$. Or in other words, we have \\begin{equation} E_1=\\bigcup_{j\\in J_i}A_j, \\end{equation} and \\begin{equation} E_{i\u0026rsquo;}\u0026rsquo;=\\bigcup_{j\u0026rsquo;\\in J_{i\u0026rsquo;}\u0026rsquo;}A_j\u0026rsquo;, \\end{equation} for all $i=1,\\ldots,k$ and $i\u0026rsquo;=1,\\ldots,k\u0026rsquo;$, and some subsets $J_i,J_{i\u0026rsquo;}\u0026rsquo;\\subset\\{1,\\ldots,m\\}$. By finite additivity property of Lebesgue measure, we therefore have \\begin{equation} m(E_i)=\\sum_{j\\in J_i}m(A_j) \\end{equation} and \\begin{equation} m(E_{i\u0026rsquo;}\u0026rsquo;)=\\sum_{j\\in J_{i\u0026rsquo;}\u0026rsquo;}m(A_j) \\end{equation} Hence, the problem remains to show that \\begin{equation} \\sum_{i=1}^{k}c_i\\sum_{j\\in J_i}m(A_j)=\\sum_{i\u0026rsquo;=1}^{k\u0026rsquo;}c_{i\u0026rsquo;}\u0026rsquo;\\sum_{j\\in J_{i\u0026rsquo;}\u0026rsquo;}m(A_j)\\label{eq:lemma1.2} \\end{equation} Fix $1\\leq j\\leq m$, we have that at each point $x$ in the non-empty set $A_j$, $1_{E_i}(x)$ is equal to $1_{J_i}(j)$, and similarly $1_{E_{i\u0026rsquo;}\u0026rsquo;}(x)$ is equal to $1_{J_{i\u0026rsquo;}\u0026rsquo;}(j)$. Then from \\eqref{eq:lemma1.1} we have \\begin{equation} \\sum_{i=1}^{k}c_i 1_{J_i}(j)=\\sum_{i\u0026rsquo;=1}^{k\u0026rsquo;}c_{i\u0026rsquo;}\u0026lsquo;1_{J_{i\u0026rsquo;}\u0026rsquo;}(j) \\end{equation} Multiplying both sides by $m(A_j)$ and then summing over all $j=1,\\ldots,m$, we obtain \\eqref{eq:lemma1.2}\nAlmost everywhere and support A property $P(x)$ of a point $x\\in\\mathbb{R}^d$ is said to hold (Lebesgue) almost everywhere in $\\mathbb{R}^d$ or for (Lebesgue) almost every point $x\\in\\mathbb{R}^d$, if the set of $x\\in\\mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure of zero (i.e. $P$ is true outside of a null set).\nTwo functions $f,g:\\mathbb{R}^d\\to Z$ into an arbitrary range $Z$ are referred to agree almost everywhere if we have $f(x)=g(x)$ almost every $x\\in\\mathbb{R}^d$.\nThe support of a function $f:\\mathbb{R}^d\\to\\mathbb{C}$ or $f:\\mathbb{R}^d\\to[0,+\\infty]$ is defined to be the set $\\{x\\in\\mathbb{R}^d:f(x)\\neq 0\\}$ where $f$ is non-zero.\nRemark 2\nIf $P(x)$ holds for almost every $x$, and $P(x)$ implies $Q(x)$, then $Q(x)$ holds for almost every $x$. If $P_1(x),P_2(x),\\ldots$ are an at most countable family of properties, each of which individually holds for almost every $x$, then they will simultaneously holds for almost every $x$, since the countable union of null sets is still a null set. Basic properties of the simple unsigned integral Let $f,g:\\mathbb{R}^d\\to[0,+\\infty]$ be simple unsigned functions.\nUnsigned linearity. We have \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)+g(x)\\hspace{0.1cm}dx=\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} and \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}cf(x)\\hspace{0.1cm}dx=c\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx, \\end{equation} for all $c\\in[0,+\\infty]$. Finiteness. We have $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\u003c\\infty$ iff $f$ is finite almost everywhere, and its support has finite measure. Vanishing. We have $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=0$ iff $f$ is zero almost everywhere. Equivalence. If $f$ and $g$ agree almost everywhere, then \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} Monotonicity. If $f(x)\\leq g(x)$ for almost every $x\\in\\mathbb{R}^d$, then \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\leq\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} Compatibility with Lebesgue measure. For any Lebesgue measurable $E$, we have \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}1_E(x)\\hspace{0.1cm}dx=m(E) \\end{equation} Proof\nSince $f,g:\\mathbb{R}^d\\to[0,+\\infty]$ are simple unsigned functions, we can assume that \\begin{align} f\u0026amp;=c_1 1_{E_1}+\\ldots+c_k 1_{E_k}, \\\\ g\u0026amp;=c_1\u0026rsquo; 1_{E_1\u0026rsquo;}+\\ldots+c_{k\u0026rsquo;}\u0026rsquo; 1_{E_{k\u0026rsquo;}\u0026rsquo;}, \\end{align} where $c_1,\\ldots,c_k,c_1\u0026rsquo;,\\ldots,c_{k\u0026rsquo;}\u0026rsquo;\\in[0,+\\infty]$.\nUnsigned linearity\nWe have \\begin{align} \\hspace{-1cm}\\text{Simp}\\int_{\\mathbb{R}^d}f(x)+g(x)\\hspace{0.1cm}dx\u0026=c_1 m(E_1)+\\ldots+c_k m(E_k)+c_1' m(E_1')+\\ldots+c_{k'}' m(E_{k'}') \\\\ \u0026=\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{align} For any $c\\in[0,+\\infty]$, we have \\begin{align} \\text{Simp}\\int_{\\mathbb{R}^d}cf(x)\\hspace{0.1cm}dx\u0026=c\\left(c_1 m(E_1)+\\ldots+c_k m(E_k)\\right) \\\\ \u0026=c\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx \\end{align} Finiteness\nGiven $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\u003c\\infty$, then for every $i=1,\\ldots,k$ we have that \\begin{equation} c_i m(E_i)\u003c\\infty\\label{eq:bpsui.1} \\end{equation} Suppose that $f$ is not finite almost everywhere, which means that there exists $1\\leq i\\leq k$ such that $E_i$ is a non-null set and $c_i=\\infty$, or \\begin{equation} c_i m(E_i)=\\infty, \\end{equation} which is in contrast with \\eqref{eq:bpsui.1}.\nSuppose that the support of $f$ has infinite measure, or in other word \\begin{equation} c_i\\neq 0,\\hspace{1cm}i=1,\\ldots,k\\label{eq:bpsui.2} \\end{equation} and \\begin{equation} m\\left(\\bigcup_{n=1}^{k}E_n\\right)=\\infty, \\end{equation} Since any $k$ subsets $E_1,\\ldots,E_k$ of $\\mathbb{R}^d$ partition $\\mathbb{R}^d$ into $2^k$ disjoint sets, say $F_1,\\ldots,F_{2^k}$. Hence, by finite additivity property of Lebesgue measure, we have \\begin{equation} \\sum_{n=1}^{2^k}m(F_n)=\\infty, \\end{equation} which implies that there exists $1\\leq n\\leq 2^k$ such that $m(F_n)=\\infty$. And therefore, for a particular $1\\leq i\\leq k$ such that $F_n\\subset E_i$, by monotonicity property of Lebesgue measure \\begin{equation} m(E_i)\\geq m(F_n)=\\infty \\end{equation} Thus, combining with \\eqref{eq:bpsui.2} gives us \\begin{equation} c_i m(E_1)=\\infty, \\end{equation} which again contradicts to \\eqref{eq:bpsui.1}.\nGiven $f$ is finite almost everywhere and its support has finite measure, suppose that its integral is infinite, or \\begin{equation} c_1 m(E_1)+\\ldots+c_k m(E_k)=\\infty, \\end{equation} which implies that there exists $1\\leq i\\leq k$ such that either\n(1) $c_i=\\infty$ and $E_i$ is a non-null set, or\n(2) $c_i\\neq 0$ and $m(E)=\\infty$.\nIf (1) happens, we then have that \\begin{equation} f\\geq c_i 1_{E_i}=\\infty, \\end{equation} which contradicts to our hypothesis.\nIf (2) happens, by monotonicity of Lebesgue measure, the support of $f$ then has infinite measure, which also contradicts to our hypothesis. Vanishing\nGiven $\\text{Simp}\\int_{\\mathbf{R^d}}f(x)\\hspace{0.1cm}dx=0$, we then have \\begin{equation} c_1 m(E_1)+\\ldots+c_k m(E_k)=0, \\end{equation} which implies that for every $1\\leq i\\leq k$, we have that $c_i=0$ or $E_i$ is a null set. Therefore, $f$ is zero almost everywhere because in this case $f$ takes the value of non-zero iff $x$ is in a particular null set $E_j$.\nGiven $f$ is zero almost everywhere, for every $i=1,\\ldots,k$, we have that either\n(1) $c_i=0$, or\n(2) $c_i\\neq 0$ and $x\\notin E_i$ with $E_i$ is a null set, or\n(3) $c_i=0$ and and $x\\notin E_i$ with $E_i$ is a null set.\nTherefore, the integral of $f$ \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=c_1 m(E_1)+\\ldots+c_k m(E_k)=0 \\end{equation} Equivalence\nGiven $f$ and $g$ agree almost everywhere, we have that at any point $x\\in\\mathbb{R}^d$ such that $f(x)=g(x)$, by lemma 1, we obtain \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} For more convenient, let $K=\\{E_i\\cap E_{i'}':1\\leq i\\leq k,1\\leq i'\\leq k'\\}$. The set $K$ then has cardinality of $kk'$. Thus, without loss of generality, we can denote $K$ as \\begin{equation} K=\\{K_1,\\ldots,K_{kk'}\\} \\end{equation} With this definition of $K$, the functions $f$ and $g$ can be rewritten by \\begin{equation} f=a_1 1_{K_1}+\\ldots+a_{kk'}1_{K_{kk'}}\\label{eq:bpsui.3} \\end{equation} and \\begin{equation} g=b_1 1_{K_1}+\\ldots+b_{kk'}1_{K_{kk'}}\\label{eq:bpsui.4} \\end{equation} On the other hand, the set in which $f(x)\\neq g(x)$ is a null set. Thus by \\eqref{eq:bpsui.3} and \\eqref{eq:bpsui.4}, we have $x\\in A$, where some $A\\subset K$ is a null set, and for each $i$ such that $K_i\\subset A$ (thus is also a null set, or $m(K_i)=0$), $a_i\\neq b_i$, otherwise if $K_i\\notin A$, $a_i=b_i$. Therefore, we obtain \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=\\sum_{i,K_i\\notin A}c_i m(K_i) \\end{equation} and \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx=\\sum_{i,K_i\\notin A}c_i m(K_i) \\end{equation} which proves our claim. Monotonicity\nUsing the same procedure as the proof for equivalence, our claim can be proved. Compatibility with Lebesgue measure\nThis follows directly from definition Absolutely convergence simple integral A complex valued simple function $f:\\mathbb{R}^d\\to\\mathbb{C}$ is known as absolutely integrable if \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}\\vert f(x)\\vert\\hspace{0.1cm}dx\u0026lt;\\infty \\end{equation} If $f$ is absolutely integrable, the integral $\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx$ is defined for real signed $f$ by the formula \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\doteq\\text{Simp}\\int_{\\mathbb{R}^d}f_+(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}f_-(x)\\hspace{0.1cm}dx, \\end{equation} where \\begin{align} f_+(x)\u0026amp;\\doteq\\max\\left(f(x),0\\right), \\\\ f_-(x)\u0026amp;\\doteq\\max\\left(-f(x),0\\right), \\end{align} and for complex-valued $f$ by the formula \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\doteq\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}\\hspace{0.1cm}f(x)\\hspace{0.1cm}dx+i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}\\hspace{0.1cm}f(x)\\hspace{0.1cm}dx \\end{equation}\nBasic properties of the complex-valued simple integral Let $f,g:\\mathbb{R}^d\\to\\mathbb{C}$ be absolutely integrable simple functions\n*-linearity. We have \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)+g(x)\\hspace{0.1cm}dx=\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} and \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}cf(x)\\hspace{0.1cm}dx=c\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx, \\end{equation} for all $c\\in\\mathbb{C}$. Also we have \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}\\overline{f}(x)\\hspace{0.1cm}dx=\\overline{\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx} \\end{equation} Equivalence. If $f$ and $g$ agree almost everywhere, then \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx=\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{equation} Compatibility with Lebesgue measure. For any Lebesgue measurable $E$, we have \\begin{equation} \\text{Simp}\\int_{\\mathbb{R}^d}1_E(x)\\hspace{0.1cm}dx=m(E) \\end{equation} Proof\nWe first consider the case of real-valued $f$ and $g$.\n*-linearity\nUsing the identity \\begin{equation} f+g=(f+g)_{+}-(f+g)_{-}=(f_{+}-f_{-})+(g_{+}-g_{-}) \\end{equation} Equivalence\nCompatibility with Lebesgue measure\nFor complex-valued $f$ and $g$ we have: *-linearity\nBy definition of complex-valued simple integral and by linearity of simple unsigned integral we have \\begin{align} \u0026\\text{Simp}\\int_{\\mathbb{R}^d}f(x)+g(x)\\hspace{0.1cm}dx\\nonumber \\\\ \u0026=\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}(f(x)+g(x))\\hspace{0.1cm}dx+i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}(f(x)+g(x))\\hspace{0.1cm}dx \\\\ \u0026=\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}f(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}g(x)\\hspace{0.1cm}dx\\nonumber \\\\ \u0026\\hspace{2cm}+i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}f(x)\\hspace{0.1cm}dx+i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}g(x)\\hspace{0.1cm}dx \\\\ \u0026=\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx+\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx \\end{align} For the complex conjugate $\\overline{f}$, we have its integral can be written as \\begin{align} \\text{Simp}\\int_{\\mathbb{R}^d}\\overline{f}(x)\\hspace{0.1cm}dx\u0026=\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}f(x)\\hspace{0.1cm}dx-\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}f(x)\\hspace{0.1cm}dx \\\\ \u0026=\\overline{\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx} \\end{align} Also, for any $c\\in\\mathbb{C}$, using linearity of simple unsigned integrals once again gives us \\begin{align} \\text{Simp}\\int_{\\mathbb{R}^d}cf(x)\\hspace{0.1cm}dx\u0026=\\text{Simp}\\int_{\\mathbb{R}^d}c\\hspace{0.1cm}\\text{Re}f(x)\\hspace{0.1cm}dx+i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}c\\hspace{0.1cm}\\text{Im}f(x)\\hspace{0.1cm}dx \\\\ \u0026=c\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Re}f(x)\\hspace{0.1cm}dx+c i\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}\\text{Im}f(x)\\hspace{0.1cm}dx \\\\ \u0026=c\\hspace{0.1cm}\\text{Simp}\\int_{\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx \\end{align} Equivalence\nCompatibility with Lebesgue measure\nMeasurable functions Just as how the piecewise constant integral can be extended to the Riemann integral, the unsigned simple integral can be extended to the unsigned Lebesgue integral, by expanding the class of unsigned simple functions to the broader class of unsigned Lebesgue measurable functions.\nUnsigned measurable functions An unsigned function $f:\\mathbb{R}^d\\to[0,+\\infty]$ is unsigned Lebesgue measurable, or measurable, if it is the pointwise limit of unsigned simple functions, i.e. if there exists a sequence $f_1,f_2,\\ldots:\\mathbb{R}\\to[0,+\\infty]$ of unsigned simple functions such that $f_n(x)\\to f(x)$ for every $x\\in\\mathbb{R}^d$.\nEquivalent notions of measurability Lemma 3\nLet $f:\\mathbb{R}\\to[0,+\\infty]$ be an unsigned function. The following are then equivalent:\n$f$ is unsigned Lebesgue measurable. $f$ is the pointwise limit of unsigned simple functions $f_n$ (hence $\\lim_{n\\to\\infty}f_n(x)$ exists and is equal to $f(x)$ for all $x\\in\\mathbb{R}^d$). $f$ is the pointwise almost everywhere limit of unsigned simple function $f_n$ (thus $\\lim_{n\\to\\infty}f_n(x)$ exists and is equal to $f(x)$ for almost every $x\\in\\mathbb{R}^d$). $f(x)=\\sup_n f_n(x)$, where $0\\leq f_1\\leq f_2\\leq\\ldots$ is an increasing sequence of unsigned simple functions, each of which are bounded with finite measure support. For every $\\lambda\\in[0,+\\infty]$, the set $\\{x\\in\\mathbb{R}^d:f(x)\u003e\\lambda\\}$ is Lebesgue measurable. For every $\\lambda\\in[0,+\\infty]$, the set $\\{x\\in\\mathbb{R}^d:f(x)\\geq\\lambda\\}$ is Lebesgue measurable. For every $\\lambda\\in[0,+\\infty]$, the set $\\{x\\in\\mathbb{R}^d:f(x)\u003c\\lambda\\}$ is Lebesgue measurable. For every $\\lambda\\in[0,+\\infty]$, the set $\\{x\\in\\mathbb{R}^d:f(x)\\leq\\lambda\\}$ is Lebesgue measurable. For every interval $I\\subset[0,+\\infty)$, the set $f^{-1}(I)\\doteq\\{x\\in\\mathbb{R}^d:f(x)\\in I\\}$ is Lebesgue measurable. For every (relatively) open set $U\\subset[0,+\\infty)$, the set $f^{-1}(U)\\doteq\\{x\\in\\mathbb{R}^d:f(x)\\in U\\}$ is Lebesgue measurable. For every (relatively) closed set $K\\subset[0,+\\infty)$, the set $f^{-1}(K)\\doteq\\{x\\in\\mathbb{R}^d:f(x)\\in K\\}$ is Lebesgue measurable. Proof\nExamples of measurable function Every continuous function $f:\\mathbb{R}^d\\to[0,+\\infty]$ is measurable. Every unsigned simple function is measurable. The supremum, infimum, limit superior, or limit inferior of unsigned measurable functions is unsigned measurable. An unsigned function that is equal almost everywhere to an unsigned measurable function, is also measurable. If a sequence $f_n$ of unsigned measurable functions converges pointwise almost everywhere to an unsigned limit $f$, then $f$ is also measurable. If $f:\\mathbb{R}^d\\to[0,+\\infty]$ is measurable and $\\phi:[0,+\\infty]\\to[0,+\\infty]$ is continuous, then $\\phi\\circ f:\\mathbb{R}^d\\to[0,+\\infty]$ is measurable. If $f,g$ are unsigned measurable functions, then $f+g$ and $fg$ are measurable. Proof\nComplex measurability An almost everywhere defined complex-valued function $f:\\mathbb{R}^d\\to\\mathbb{C}$ is Lebesgue measurable, or measurable, if it is the pointwise almost everywhere limit of complex-valued simple functions.\nEquivalent notions of complex measurability Let $f:\\mathbb{R}^d\\to\\mathbb{C}$ be an almost everywhere defined complex-valued function. The following are then equivalent:\n$f$ is measurable. $f$ is the pointwise almost everywhere limit of complex-valued simple functions. The (magnitudes of the) positive and negative parts of $\\text{Re}(f)$ and $\\text{Im}(f)$ are unsigned measurable functions. $f^{-1}(U)$ is Lebesgue measurable for every open set $U\\subset\\mathbb{C}$. $f^{-1}(K)$ is Lebesgue measurable for every closed set $K\\subset\\mathbb{C}$. Proof\nUnsigned Lebesgue integrals Lower unsigned Lebesgue integral Let $f:\\mathbb{R}^d\\to[0,+\\infty]$ be an unsigned functions (not necessarily measurable). We define the lower unsigned Lebesgue integral, denoted as $\\underline{\\int_{\\mathbb{R}^d}}f(x)\\hspace{0.1cm}dx$, to be the quantity \\begin{equation} \\underline{\\int_\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\doteq\\sup_{0\\leq g\\leq f;g\\text{ simple}}\\text{Simp}\\int_{\\mathbb{R}^d}g(x)\\hspace{0.1cm}dx, \\end{equation} where $g$ ranges over all unsigned simple functions $g:\\mathbb{R}^d\\to[0,+\\infty]$ that are pointwise bounded by $f$.\nWe can also define the upper unsigned Lebesgue integral as \\begin{equation} \\overline{\\int_\\mathbb{R}^d}f(x)\\hspace{0.1cm}dx\\doteq\\inf_{h\\geq f;h\\text{ simple}}\\text{Simp}\\int_{\\mathbb{R}^d}h(x)\\hspace{0.1cm}dx \\end{equation}\nAbsolute integrability Littlewood\u0026rsquo;s three principles References [1] Terence Tao. An introduction to measure theory. Graduate Studies in Mathematics, vol. 126.\n[2] Elias M. Stein \u0026amp; Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007.\nFootnotes It should be simpler to consider the case of $k=2$, in particular with two sets $E_1,E_2\\subset\\mathbb{R}^d$. These two sets partition $\\mathbb{R}^d$ into four disjoint sets: $E_1\\cap E_2,E_1\\cap E_2^c,E_1^c\\cap E_2,E_1^c\\cap E_2^c$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/measure-theory/measure-theory-p3/","summary":"\u003cblockquote\u003e\n\u003cp\u003e(Temporary being stopped) Note III of the measure theory series. Materials are mostly taken from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#taos-book\"\u003eTao\u0026rsquo;s book\u003c/a\u003e, except for some needed notations extracted from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#steins-book\"\u003eStein\u0026rsquo;s book\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Measure theory - III: the Lebesgue integral"},{"content":" Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.\nPreliminaries Independence, basis in vector space Linear independence The sequence of vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ is said to be linearly independent (or independent) if \\begin{equation} c_1\\mathbf{x}_1+\\ldots+c_n\\mathbf{x}_n=\\mathbf{0} \\end{equation} only when $c_1,\\ldots,c_n$ are all zero.\nConsidering those $n$ vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_n$ as $n$ columns of a matrix $\\mathbf{A}$ \\begin{equation} \\mathbf{A}=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\mathbf{x}_1 \u0026amp; \\ldots \u0026amp; \\mathbf{x}_n \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right] \\end{equation} we have that the columns of $\\mathbf{A}$ are independent when \\begin{equation} \\mathbf{A}\\mathbf{x}=\\mathbf{0}\\hspace{0.5cm}\\Leftrightarrow\\hspace{0.5cm}\\mathbf{x}=\\mathbf{0}, \\end{equation} or in other words, the rank of $\\mathbf{A}$ is equal to the number of columns of $\\mathbf{A}$.\nBasis of a vector space We say that vectors $\\mathbf{v}_1,\\ldots,\\mathbf{v}_k$ span a space $S$ when the space consists of all combinations of those vectors. Or in other words, any vector $\\mathbf{u}\\in S$ can be displayed as linear combination of $\\mathbf{v}_i$.\nIn this case, $S$ is the smallest space containing those vectors.\nA basis for a vector space $S$ is a sequence of vectors $\\mathbf{v}_1,\\ldots,\\mathbf{v}_d$ having two properties:\n$\\mathbf{v}_1,\\ldots,\\mathbf{v}_d$ are independent $\\mathbf{v}_1,\\ldots,\\mathbf{v}_d$ span $S$ In $S$, every basis for that space has the same number of vectors, which is the dimension of $S$. Therefore, there are exactly $n$ vectors in every basis for $\\mathbb{R}^n$. With that definition of a basis $\\mathbf{v}_1,\\dots,\\mathbf{v}_d$ of $S$, for each vector $\\mathbf{u}\\in S$, there exists only one sequence $c_1,\\ldots,c_d$ such that \\begin{equation} \\mathbf{u}=c_1\\mathbf{v}_1+\\ldots+c_d\\mathbf{v}_d \\end{equation}\nLagrange Multipliers Consider the problem of finding the maximum (or minimum) of $w=f(x_1,x_2,x_3)$ subject to a constraint relating $x_1,x_2$ and $x_3$ \\begin{equation} g(x_1,x_2,x_3)=0 \\end{equation} Apart from solving $x_3$ in terms of $x_1$ and $x_2$ in the constraint and substituting into the original function, which now becomes an unconstrained, here we can also solve this problem as a constrained one.\nThe idea is we are using the observation that the gradient vector $\\nabla f(\\mathbf{x})$ and $\\nabla g(\\mathbf{x})$ are parallel, because:\nSuppose $f(\\mathbf{x})$ has a local maximum at $\\mathbf{x}^*$ on the constraint surface $g(\\mathbf{x})=0$.\nLet $\\mathbf{r}(t)=\\langle x_1(t),x_2(t),x_3(t)\\rangle$ be a parameterized curve on the constraint surface such that and $\\mathbf{r}(t)$ has \\begin{equation} (x_1(0),x_2(0),x_3(0))^\\text{T}=\\mathbf{x} \\end{equation} And also, let $h(t)=f(x_1(t),x_2(t),x_3(t))$, then it implies that $h$ has a maximum at $t=0$, which lets \\begin{equation} h\u0026rsquo;(0)=0 \\end{equation} Taking the derivative of $h$ w.r.t, we obtain \\begin{equation} h\u0026rsquo;(t)=\\nabla f(\\mathbf{x})\\big\\vert_{\\mathbf{r}(t)}\\mathbf{r}\u0026rsquo;(t) \\end{equation} Therefore, \\begin{equation} \\nabla f(\\mathbf{x})\\big\\vert_{\\mathbf{x}^*}\\mathbf{r}\u0026rsquo;(0)=0, \\end{equation} which implies that $\\nabla f(\\mathbf{x})$ is perpendicular to any curve in the constraint space that goes through $\\mathbf{x}^*$. And since $\\nabla g(\\mathbf{x})$ perpendicular to the constraint surface $g(x)=0$, then $\\nabla g(\\mathbf{x})$ is also perpendicular to those curves. This implies that $\\nabla f(\\mathbf{x})$ is parallel to $\\nabla g(\\mathbf{x})$.\nWith this property, we can write $\\nabla f(\\mathbf{x})$ in terms of $\\nabla g(\\mathbf{x})$, as \\begin{equation} \\nabla f(\\mathbf{x})=\\lambda\\nabla g(\\mathbf{x}), \\end{equation} where $\\lambda\\neq 0$ is a constant called Lagrange multiplier.\nWith this definition of Lagrange multiplier, we continue to define the Lagrangian function, given as \\begin{equation} \\mathcal{L}(\\mathbf{x},\\lambda)=f(\\mathbf{x})+\\lambda g(\\mathbf{x}) \\end{equation} Then letting the partial derivative of Lagrangian w.r.t $\\lambda$ be zero gives us the constraint \\begin{equation} 0=\\frac{\\partial \\mathcal{L}(\\mathbf{x},\\lambda)}{\\partial\\lambda}=g(\\mathbf{x}) \\end{equation} With Lagrangian, in order to find the maximum of $f(\\mathbf{x})$ that satisfies $g(\\mathbf{x})=0$, we will instead be trying to solve \\begin{equation} \\max_\\mathbf{x}\\min_\\lambda\\mathcal{L}(\\mathbf{x},\\lambda), \\end{equation} This can be solved by letting derivatives of the Lagrangian $\\mathcal{L}$ w.r.t $x_i$ (i.e. components of $\\mathbf{x}$) and $\\lambda$ be zero \\begin{equation} \\frac{\\partial\\mathcal{L}}{\\partial x_i}=0,\\hspace{1cm}\\frac{\\partial\\mathcal{L}}{\\partial\\lambda}=0 \\end{equation} and solve for $x_i$ and $\\lambda$.\nLinear models for Regression Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\\mathbf{x}$ of input variables.\nLinear basis function models The simplest linear model used for regression tasks is linear regression, which is defined as a linear combination of the input variables \\begin{equation} y(\\mathbf{x},\\mathbf{w})=w_0+w_1x_1+\\ldots+w_Dx_D, \\end{equation} where $\\mathbf{x}=(x_1,\\ldots,x_D)^\\text{T}$ is the input variables, while $w_i$\u0026rsquo;s are the parameters parameterizing the space of linear function mapping from the input space $\\mathcal{X}$ of $\\mathbf{x}$ to $\\mathcal{Y}$.\nWith the idea of spanning a space by its basis vectors, we can generalize it to establishing a function space by linear combinations of simpler basis functions. Or in other words, we can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\\mathbf{x}$, as \\begin{equation} y(\\mathbf{x},\\mathbf{w})=w_0+w_1\\phi_1(\\mathbf{x})+\\ldots+w_{M-1}\\phi_{M-1}(\\mathbf{x})=w_0+\\sum_{i=1}^{M-1}w_i\\phi_i(\\mathbf{x}),\\label{eq:lbfm.1} \\end{equation} where $\\phi_i(\\mathbf{x})$\u0026rsquo;s are called the basis functions; $w_0$ is called a bias parameter. By letting $w_0$ be a coefficient corresponding to a dummy basis function $\\phi_0(\\mathbf{x})=1$, \\eqref{eq:lbfm.1} can be written in a more convenient way \\begin{equation} y(\\mathbf{x},\\mathbf{w})=\\sum_{i=0}^{M-1}w_i\\phi_i(\\mathbf{x})=\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}),\\label{eq:lbfm.2} \\end{equation} where $\\mathbf{w}=(w_0,\\ldots,w_{M-1})^\\text{T}$ and $\\boldsymbol{\\phi}=(\\phi_0,\\ldots,\\phi_{M-1})^\\text{T}$, with $\\phi_0(\\cdot)=1$.\nThere are various choices of basis functions:\nPolynomial basis. Each basis function $\\phi_i$ is a powers of a $1$-dimensional input $x$ \\begin{equation} \\phi_i(x)=x^i \\end{equation} An example of polynomial basis functions is illustrated as below Figure 1: Example of polynomial basis functions. The code can be found here Gaussian basis function. Each basis function $\\phi_i$ is a Gaussian function of a $1$-dimensional input $x$ \\begin{equation} \\phi_i(x)=\\exp\\left(-\\frac{(x-\\mu_i)^2}{2\\sigma_i^2}\\right) \\end{equation} An example of Gaussian basis functions is illustrated as below Figure 2: Example of Gaussian basis functions. The code can be found here Sigmoidal basis function. Each basis function $\\phi_i$ is defined as \\begin{equation} \\phi_i(x)=\\sigma\\left(\\frac{x-\\mu_i}{\\sigma_i}\\right), \\end{equation} where $\\sigma(\\cdot)$ is the logistic sigmoid function \\begin{equation} \\sigma(x)=\\frac{1}{1+\\exp(-x)} \\end{equation} An example of sigmoidal basis functions is illustrated as below Figure 3: Example of sigmoidal basis functions. The code can be found here Least squares Assume that the target variable $t$ and the inputs $\\mathbf{x}$ is related via the equation \\begin{equation} t=y(\\mathbf{x},\\mathbf{w})+\\epsilon, \\end{equation} where $\\epsilon$ is an error term that captures random noise such that $\\epsilon\\sim\\mathcal{N}(0,\\sigma^2)$, which means the density of $\\epsilon$ can be written as \\begin{equation} p(\\epsilon)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{\\epsilon^2}{2\\sigma^2}\\right), \\end{equation} which implies that \\begin{equation} p(t|\\mathbf{x};\\mathbf{w},\\beta)=\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left(-\\frac{(t-y(\\mathbf{x},\\mathbf{w}))^2\\beta}{2}\\right),\\label{eq:lsr.1} \\end{equation} where $\\beta=1/\\sigma^2$ is the precision of $\\epsilon$, or \\begin{equation} t|\\mathbf{x};\\mathbf{w},\\beta\\sim\\mathcal{N}(y(\\mathbf{x},\\mathbf{w}),\\beta^{-1}) \\end{equation} Consider a data set of inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\}$ with corresponding target values $\\mathbf{t}=(t_1,\\ldots,t_N)^\\text{T}$ and assume that these data points are drawn independently from the distribution above, we obtain the batch version of \\eqref{eq:lsr.1}, called the likelihood function, given as \\begin{align} L(\\mathbf{w},\\beta)=p(\\mathbf{t}|\\mathbf{X};\\mathbf{w},\\beta)\u0026amp;=\\prod_{i=1}^{N}p(t_i|\\mathbf{x}_i;\\mathbf{w},\\beta) \\\\ \u0026amp;=\\prod_{i=1}^{N}\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left(-\\frac{(t_i-y(\\mathbf{x}_i,\\mathbf{w}))^2\\beta}{2}\\right)\\label{eq:lsr.2} \\end{align} By maximum likelihood, we will be looking for values of $\\mathbf{w}$ and $\\beta$ that maximize the likelihood. We do this by considering maximizing a simpler likelihood, called log likelihood, denoted as $\\ell(\\mathbf{w},\\beta)$, defined as \\begin{align} \\ell(\\mathbf{w},\\beta)=\\log{L(\\mathbf{w},\\beta)}\u0026amp;=\\log\\prod_{i=1}^{N}\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left(-\\frac{(t_i-y(\\mathbf{x}_i,\\mathbf{w}))^2\\beta}{2}\\right) \\\\ \u0026amp;=\\sum_{i=1}^{N}\\log\\left[\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left(-\\frac{(t_i-y(\\mathbf{x}_i,\\mathbf{w}))^2\\beta}{2}\\right)\\right] \\\\ \u0026amp;=\\frac{N}{2}\\log\\beta-\\frac{N}{2}\\log(2\\pi)-\\sum_{i=1}^{N}\\frac{(t_i-y(\\mathbf{x}_i,\\mathbf{w}))^2\\beta}{2} \\\\ \u0026amp;=\\frac{N}{2}\\log\\beta-\\frac{N}{2}\\log(2\\pi)-\\beta E_D(\\mathbf{w})\\label{eq:lsr.3}, \\end{align} where $E_D(\\mathbf{w})$ is the sum-of-squares error function, defined as \\begin{equation} E_D(\\mathbf{w})\\doteq\\frac{1}{2}\\sum_{i=1}^{N}\\left(t_i-y(\\mathbf{x}_i,\\mathbf{w})\\right)^2\\label{eq:lsr.4} \\end{equation} Consider the gradient of \\eqref{eq:lsr.3} w.r.t $\\mathbf{w}$, we have \\begin{align} \\nabla_\\mathbf{w}\\ell(\\mathbf{w},\\beta)\u0026amp;=\\nabla_\\mathbf{w}\\left[\\frac{N}{2}\\log\\beta-\\frac{N}{2}\\log(2\\pi)-\\beta E_D(\\mathbf{w})\\right] \\\\ \u0026amp;\\propto\\nabla_\\mathbf{w}\\frac{1}{2}\\sum_{i=1}^{N}\\big(t_i-y(\\mathbf{x}_i,\\mathbf{w})\\big)^2 \\\\ \u0026amp;=\\nabla_\\mathbf{w}\\frac{1}{2}\\sum_{i=1}^{N}\\left(t_i-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}\\big(\\mathbf{x}_i\\right)\\big)^2 \\\\ \u0026amp;=\\sum_{i=1}^{N}(t_i-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i))\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\text{T} \\end{align} By gradient descent, letting this gradient to zero gives us \\begin{equation} \\sum_{i=1}^{N}t_i\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\text{T}-\\mathbf{w}^\\text{T}\\sum_{i=1}^{N}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\boldsymbol{\\phi}(\\mathbf{x}_i)^\\text{T}=0, \\end{equation} which implies that \\begin{equation} \\mathbf{w}_\\text{ML}=\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t},\\label{eq:lsr.5} \\end{equation} which is known as the normal equations for the least squares problem. In \\eqref{eq:lsr.5}, $\\boldsymbol{\\Phi}\\in\\mathbb{R}^{N\\times M}$ is called the design matrix, whose elements are given by $\\boldsymbol{\\Phi}_{ij}=\\phi_j(\\mathbf{x}_i)$ \\begin{equation} \\boldsymbol{\\Phi}=\\left[\\begin{matrix}-\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_1)^\\text{T}\\hspace{0.1cm}- \\\\ \\hspace{0.1cm}\\vdots\\hspace{0.1cm} \\\\ -\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_N)^\\text{T}\\hspace{0.1cm}-\\end{matrix}\\right]=\\left[\\begin{matrix}\\phi_0(\\mathbf{x}_1)\u0026amp;\\ldots\u0026amp;\\phi_{M-1}(\\mathbf{x}_1) \\\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots \\\\ \\phi_0(\\mathbf{x}_N)\u0026amp;\\ldots\u0026amp;\\phi_{M-1}(\\mathbf{x}_N)\\end{matrix}\\right], \\end{equation} and the quantity \\begin{equation} \\boldsymbol{\\Phi}^\\dagger\\doteq\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\text{T} \\end{equation} is called the Moore-Penrose pseudoinverse of the matrix $\\boldsymbol{\\Phi}$.\nOn the other hand, consider the gradient of \\eqref{eq:lsr.3} w.r.t $\\beta$ and set it equal to zero, we obtain \\begin{equation} \\beta=\\frac{N}{\\sum_{i=1}^{N}\\big(t_i-\\mathbf{w}_\\text{ML}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^2} \\end{equation}\nGeometrical interpretation of least squares As mentioned before, we have applied the idea of spanning a vector space by its basis vectors when constructing basis functions.\nIn particular, consider an $N$-dimensional space whose axes are given by $t_i$, which implies that \\begin{equation} \\mathbf{t}=(t_1,\\ldots,t_N)^\\text{T} \\end{equation} is a vector contained in the space.\nFigure 4: Geometrical interpretation of the least-squares solution. The figure is taken from Bishop's book Each basis function $\\phi_j(\\mathbf{x}_i)$, evaluated at the $N$ data points, then can also be presented as a vector in the same space, denoted by $\\boldsymbol{\\varphi}_j$, as illustrated in Figure 4 above. Therefore, the design matrix $\\boldsymbol{\\Phi}$ can be represented as \\begin{equation} \\boldsymbol{\\Phi}=\\left[\\begin{matrix}-\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_1)\\hspace{0.1cm}- \\\\ \\hspace{0.1cm}\\vdots\\hspace{0.1cm} \\\\ -\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_N)\\hspace{0.1cm}-\\end{matrix}\\right]=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\boldsymbol{\\varphi}_{0}\u0026amp;\\ldots\u0026amp;\\boldsymbol{\\varphi}_{M-1} \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right] \\end{equation} When the number $M$ of basis functions is smaller than the number $N$ of data points, the $M$ vectors $\\phi_j(\\mathbf{x}_i)$ will span a linear subspace $\\mathcal{S}$ of $M$ dimensions.\nWe define $\\mathbf{y}$ to be an $N$-dimensional vector whose the $i$-th element is given by $y(\\mathbf{x}_i,\\mathbf{w})$ \\begin{equation} \\mathbf{y}=\\big(y(\\mathbf{x}_1,\\mathbf{w}),\\ldots,y(\\mathbf{x}_N,\\mathbf{w})\\big)^\\text{T} \\end{equation} Since $\\mathbf{y}$ is a linear combination of $\\boldsymbol{\\varphi}_i$, then $\\mathbf{y}\\in\\mathcal{S}$. Then the sum-of-squares error \\eqref{eq:lsr.4} is exactly (with a factor of $1/2$) the squared Euclidean distance between $\\mathbf{y}$ and $\\mathbf{t}$. Therefore, the least square solution to $\\mathbf{w}$ is the one that makes $\\mathbf{y}$ closest to $\\mathbf{t}$.\nThis solution corresponds to the orthogonal projection of $t$ onto the subspace $S$ spanned by $\\boldsymbol{\\varphi}_i$, because we have that \\begin{align} \\mathbf{y}^\\text{T}(\\mathbf{t}-\\mathbf{y})\u0026amp;=\\left(\\boldsymbol{\\Phi}\\mathbf{w}_\\text{ML}\\right)^\\text{T}\\left(\\mathbf{t}-\\boldsymbol{\\Phi}\\mathbf{w}_\\text{ML}\\right) \\\\ \u0026amp;=\\left(\\boldsymbol{\\Phi}\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}\\mathbf{t}\\right)^\\text{T}\\left(\\mathbf{t}-\\boldsymbol{\\Phi}\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}\\mathbf{t}\\right) \\\\ \u0026amp;=\\mathbf{t}^\\text{T}\\boldsymbol{\\Phi}\\left(\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\right)^\\text{T}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}-\\mathbf{t}^\\text{T}\\boldsymbol{\\Phi}\\left(\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\right)^\\text{T}\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}\\mathbf{t} \\\\ \u0026amp;=\\mathbf{t}^\\text{T}\\boldsymbol{\\Phi}\\left(\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\right)^\\text{T}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}-\\mathbf{t}^\\text{T}\\boldsymbol{\\Phi}\\left(\\left(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\right)^\\text{T}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t} \\\\ \u0026amp;=0, \\end{align}\nThe LMS algorithm The least-means-squares, or LMS algorithm for the sum-of-squares error \\eqref{eq:lsr.4}, which start with some initial vector $\\mathbf{w}_0$ of $\\mathbf{w}$, and repeatedly perform the update \\begin{equation} \\mathbf{w}_{t+1}=\\mathbf{w}_t+\\eta(t_n-\\mathbf{w}_t^\\text{T}\\boldsymbol{\\phi}_n)\\boldsymbol{\\phi}_n, \\end{equation} where $\\boldsymbol{\\phi}_n$ denotes $\\boldsymbol{\\phi}(\\mathbf{x}_n)$, and $\\eta$ is called the learning rate which controls the update amount.\nRegularized least squares To control over-fitting, in the error function \\eqref{eq:lsr.4}, we add an regularization term, which makes the total error function to be minimized take the form \\begin{equation} E_D(\\mathbf{w})+\\lambda E_W(\\mathbf{w}),\\label{eq:rls.1} \\end{equation} where $\\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\\mathbf{w})$ and the regularization term $E_W(\\mathbf{w})$. One simple possible form of regularizer is given as \\begin{equation} E_W(\\mathbf{w})=\\frac{1}{2}\\mathbf{w}^\\text{T}\\mathbf{w} \\end{equation} The total error function \\eqref{eq:rls.1} then can be written as \\begin{equation} E_D(\\mathbf{w})+E_W(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^{N}\\big(t_i-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^2+\\frac{\\lambda}{2}\\mathbf{w}^\\text{T}\\mathbf{w}\\label{eq:rls.2} \\end{equation} Setting the gradient of this error to zero and solving for $\\mathbf{w}$, we have the solution \\begin{equation} \\mathbf{w}= (\\lambda\\mathbf{I}+\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}\\label{eq:rls.3} \\end{equation} This particular choice of regularizer is called weight decay because it encourages weight values to decay towards zero in sequential learning.\nAnother choice of regularizer which is more general lets the regularized error have the form \\begin{equation} E_D(\\mathbf{w})+E_W(\\mathbf{w})=\\frac{1}{2}\\sum_{i=1}^{N}\\big(t_i-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^2+\\frac{\\lambda}{2}\\sum_{j=1}^{M}\\vert w_j\\vert^q, \\end{equation} where $q=2$ corresponds to the regularizer \\eqref{eq:rls.2}.\nMultiple outputs When the target of our model is instead in multiple-dimensional form, denoted as $\\mathbf{t}$, we can generalize our model to be \\begin{equation} \\mathbf{y}(\\mathbf{x},\\mathbf{w})=\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}), \\end{equation} where $\\mathbf{y}\\in\\mathbb{R}^K, \\mathbf{W}\\in\\mathbb{R}^{M\\times K}$ is the matrix of parameters, $\\boldsymbol{\\phi}\\in\\mathbb{R}^M$ with $\\phi_i(\\mathbf{x})$ as the $i$-th element, and with $\\phi_0(\\mathbf{x})=1$.\nWith this generalization, \\eqref{eq:lsr.1} can be also be rewritten as \\begin{equation} p(\\mathbf{t}|\\mathbf{x};\\mathbf{W},\\beta)=\\sqrt{\\frac{\\beta}{2\\pi\\vert\\mathbf{I}\\vert}}\\exp\\left[-\\frac{1}{2}\\left(\\mathbf{t}-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}\\left(\\mathbf{x}\\right)\\right)^\\text{T}\\left(\\mathbf{t}-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}\\left(\\mathbf{x}\\right)\\right)\\beta\\mathbf{I}^{-1}\\right], \\end{equation} or in other words \\begin{equation} \\mathbf{t}|\\mathbf{x};\\mathbf{W},\\beta\\sim\\mathcal{N}(\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}),\\beta^{-1}\\mathbf{I}) \\end{equation} With a data set of inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\ldots,\\mathbf{x}_N\\}$, our target values can also be vectorized into $\\mathbf{T}\\in\\mathbb{R}^{N\\times K}$ given as \\begin{equation} \\mathbf{T}=\\left[\\begin{matrix}-\\hspace{0.1cm}\\mathbf{t}_1^\\text{T}\\hspace{0.1cm}- \\\\ \\vdots \\\\ -\\hspace{0.1cm}\\mathbf{t}_N^\\text{T}\\hspace{0.1cm}-\\end{matrix}\\right], \\end{equation} and likewise with the input matrix $\\mathbf{X}$ vectorized from input vectors $\\mathbf{x}_1,\\ldots,\\mathbf{x}_N$. With these definitions, the multi-dimensional likelihood can be defined as \\begin{align} L(\\mathbf{W},\\beta)\u0026amp;=p(\\mathbf{T}|\\mathbf{X};\\mathbf{W},\\beta) \\\\ \u0026amp;=\\prod_{i=1}^{N}p(\\mathbf{t}_i|\\mathbf{x}_i;\\mathbf{W},\\beta) \\\\ \u0026amp;=\\prod_{i=1}^{N}\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left[-\\frac{\\beta}{2}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^\\text{T}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)\\right] \\end{align} And thus the log likelihood now becomes \\begin{align} \\ell(\\mathbf{W},\\beta)\u0026amp;=\\log L(\\mathbf{W},\\beta) \\\\ \u0026amp;=\\log\\prod_{i=1}^{N}\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left[-\\frac{\\beta}{2}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^\\text{T}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)\\right] \\\\ \u0026amp;=\\sum_{i=1}^{N}\\log\\sqrt{\\frac{\\beta}{2\\pi}}\\exp\\left[-\\frac{\\beta}{2}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^\\text{T}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)\\right] \\\\ \u0026amp;=\\frac{N}{2}\\log\\frac{\\beta}{2\\pi}-\\frac{\\beta}{2}\\sum_{i=1}^{N}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big)^\\text{T}\\big(\\mathbf{t}_i-\\mathbf{W}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_i)\\big) \\end{align} Taking the gradient of the log likelihood w.r.t $\\mathbf{W}$, setting it to zero and solving for $\\mathbf{W}$ gives us \\begin{equation} \\mathbf{W}_\\text{ML}=(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{T} \\end{equation}\nBayesian linear regression Parameter distribution Consider the noise precision parameter $\\beta$ as a constant. From the equation \\eqref{eq:lsr.2}, we see that the likelihood function $L(\\mathbf{w})=p(\\mathbf{t}\\vert\\mathbf{w})$ takes the form of an exponential of a quadratic form in $\\mathbf{w}$. Thus, if we choose the prior $p(\\mathbf{w})$ as a Gaussian, the corresponding posterior will also become a Gaussian due to being computed as a product of two exponentials of quadratic forms of $\\mathbf{w}$. This makes the prior be a conjugate distribution for the likelihood function, and hence be given by \\begin{equation} p(\\mathbf{w})=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{m}_0,\\mathbf{S}_0), \\end{equation} where $\\mathbf{m}_0$ is the mean vector and $\\mathbf{S}_0$ is the covariance matrix.\nBy the result, we have that the corresponding posterior distribution $p(\\mathbf{w}\\vert\\mathbf{t})$, which is a conditional Gaussian distribution, is given by \\begin{equation} p(\\mathbf{w}\\vert\\mathbf{t})=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{m}_N,\\mathbf{S}_N),\\label{eq:pd.1} \\end{equation} where the mean $\\mathbf{m}_N$ and the precision matrix $\\mathbf{S}_N^{-1}$ are defined as \\begin{align} \\mathbf{m}_N\u0026amp;=\\mathbf{S}_N(\\mathbf{S}_0^{-1}\\mathbf{m}_0+\\beta\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}), \\\\ \\mathbf{S}_N^{-1}\u0026amp;=\\mathbf{S}_0^{-1}+\\beta\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi} \\end{align} Therefore, by MAP, we have \\begin{align} \\mathbf{w}_\\text{MAP}\u0026amp;=\\underset{\\mathbf{w}}{\\text{argmax}}\\hspace{0.1cm}\\exp\\Big[-\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_N)^\\text{T}\\mathbf{S}_N^{-1}(\\mathbf{w}-\\mathbf{m}_N)\\Big] \\\\ \u0026amp;=\\underset{\\mathbf{w}}{\\text{argmin}}\\hspace{0.1cm}(\\mathbf{w}-\\mathbf{m}_N)^\\text{T}\\mathbf{S}_N^{-1}(\\mathbf{w}-\\mathbf{m}_N) \\end{align} By this property of the covariance matrix, we have that the precision matrix $\\mathbf{S}_N^{-1}$ and the covariance matrix $\\mathbf{S}_N$ have the same set of eigenvalues, which are non-negative due to the fact that $\\mathbf{S}_N$ is positive semi-definite. This also means that $\\mathbf{S}_N$ is positive semi-definite, and thus \\begin{equation} (\\mathbf{w}-\\mathbf{m}_N)^\\text{T}\\mathbf{S}_N^{-1}(\\mathbf{w}-\\mathbf{m}_N)\\geq0 \\end{equation} Therefore, the maximum posterior weight vector is also the mean vector \\begin{equation} \\mathbf{w}_\\text{MAP}=\\mathbf{m}_N\\label{eq:pd.2} \\end{equation} Consider an infinite broad prior $\\mathbf{S}_0=\\alpha^{-1}\\mathbf{I}$ with $\\alpha\\to 0$, in this case the mean $\\mathbf{m}_N$ reduces to the maximum likelihood value $\\mathbf{w}_\\text{ML}$ given by \\eqref{eq:lsr.5}. And if $N=0$, then the posterior distribution reverts to the prior.\nFurthermore, consider an additional data point $(\\mathbf{x}_{N+1},t_{N+1})$, the posterior given in \\eqref{eq:pd.1} can be regarded as the prior distribution for that data point. If the model is given as \\eqref{eq:lsr.1}, the likelihood function of the newly added data point is then given in form \\begin{equation} p(t_{N+1}\\vert\\mathbf{x}_{N+1},\\mathbf{w})=\\left(\\frac{\\beta}{2\\pi}\\right)^{1/2}\\exp\\left(-\\frac{(t_{N+1}-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_{N+1})\\beta}{2}\\right), \\end{equation} where $\\boldsymbol{\\phi}_{N+1}=\\boldsymbol{\\phi}(\\mathbf{x}_{N+1})$. Therefore, the posterior distribution of the data point $(\\mathbf{x}_{N+1},t_{N+1})$ can be computed as \\begin{align} \u0026amp;\\hspace{0.7cm}p(\\mathbf{w}\\vert t_{N+1},\\mathbf{x}_{N+1},\\mathbf{t}) \\\\ \u0026amp;\\propto p(t_{N+1}\\vert\\mathbf{x}_{N+1},\\mathbf{w})p(\\mathbf{w}\\vert\\mathbf{t}) \\\\ \u0026amp;=\\exp\\Big[-\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_N)^\\text{T}\\mathbf{S}_N^{-1}(\\mathbf{w}-\\mathbf{m}_N)-\\frac{1}{2}(t_{N+1}-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_{N+1})^2\\beta\\Big] \\\\ \u0026amp;=\\exp\\Big[-\\frac{1}{2}\\big(\\mathbf{w}^\\text{T}\\mathbf{S}_N^{-1}\\mathbf{w}+\\beta\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{w}\\big)+\\mathbf{w}^\\text{T}\\big(\\mathbf{S}_N^{-1}\\mathbf{m}_N+t_{N+1}\\beta\\boldsymbol{\\phi}_{N+1}\\big)+c\\Big] \\\\ \u0026amp;=\\exp\\Big[-\\frac{1}{2}\\mathbf{w}^\\text{T}\\big(\\mathbf{S}_N^{-1}+\\beta\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\big)\\mathbf{w}+\\mathbf{w}^\\text{T}\\big(\\mathbf{S}_N^{-1}\\mathbf{m}_N+t_{N+1}\\beta\\boldsymbol{\\phi}_{N+1}\\big)+c\\Big], \\end{align} where $c$ is a constant w.r.t $\\mathbf{w}$, i.e., $c$ is independent of $\\mathbf{w}$, which claims that the posterior distribution is also a Gaussian, given by \\begin{equation} p(\\mathbf{w}\\vert t_{N+1},\\mathbf{x}_{N+1},\\mathbf{t})=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{m}_{N+1},\\mathbf{S}_{N+1})\\label{eq:pd.3} \\end{equation} where the precision matrix $\\mathbf{S}_{N+1}$ is defined as \\begin{equation} \\mathbf{S}_{N+1}^{-1}\\doteq\\mathbf{S}_N^{-1}+\\beta\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}, \\end{equation} and the mean $\\mathbf{m}_{N+1}$ is given by \\begin{equation} \\mathbf{m}_{N+1}\\doteq\\mathbf{S}_{N+1}\\big(\\mathbf{S}_N^{-1}\\mathbf{m}_N+t_{N+1}\\beta\\boldsymbol{\\phi}_{N+1}\\big) \\end{equation} Consider the prior as a Gaussian, defined by \\begin{equation} p(\\mathbf{w}\\vert\\alpha)=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{0},\\alpha^{-1}\\mathbf{I}), \\end{equation} Therefore, the corresponding posterior over $\\mathbf{w}$, $p(\\mathbf{w}\\vert\\mathbf{t})$, will be given as \\eqref{eq:pd.1} with \\begin{align} \\mathbf{m}_N\u0026amp;=\\beta\\mathbf{S}_N\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t} \\\\ \\mathbf{S}_N^{-1}\u0026amp;=\\alpha\\mathbf{I}+\\beta\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi} \\end{align} Taking the natural logarithm of the posterior distribution gives us the sum of the log likelihood and the log of the prior, as a function of $\\mathbf{w}$, given by \\begin{equation} \\log p(\\mathbf{w}\\vert\\mathbf{t})=-\\frac{\\beta}{2}\\sum_{n=1}^{N}\\big(t_n-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}_n)\\big)^2-\\frac{\\alpha}{2}\\mathbf{w}^\\text{T}\\mathbf{w}+c \\end{equation} Therefore, maximizing this posterior is equivalent to minimizing the sum of the sum-of-squares error function with addition of a quadratic regularization term, which is exactly the equation \\eqref{eq:rls.2} with $\\lambda=\\alpha/\\beta$.\nIn addition, by $\\eqref{eq:pd.2}$, we have that \\begin{equation} \\mathbf{w}_\\text{MAP}=\\mathbf{m}_N=\\beta\\left(\\alpha\\mathbf{I}+\\beta\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}=\\left(\\frac{\\alpha}{\\beta}\\mathbf{I}+\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\right)^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}, \\end{equation} which for setting $\\lambda=\\alpha/\\beta$ gives us exactly the solution \\eqref{eq:rls.3} for the regularized least squares \\eqref{eq:rls.2}.\nPredictive distribution The predictive distribution that gives us the information to make predictions $t$ for new values $\\mathbf{x}$ is defined as \\begin{equation} p(t\\vert\\mathbf{x},\\mathbf{t},\\alpha,\\beta)=\\int p(t\\vert\\mathbf{x},\\mathbf{w},\\beta)p(\\mathbf{w}\\vert\\mathbf{x},\\mathbf{t},\\alpha,\\beta)\\hspace{0.1cm}d\\mathbf{w}\\label{eq:pdr.1} \\end{equation} in which $\\mathbf{t}$ is the vector of target values from the training set.\nThe conditional distribution $p(t\\vert\\mathbf{x},\\mathbf{w},\\beta)$ of the target variable is given by \\eqref{eq:lsr.1}, and the posterior weight distribution $p(\\mathbf{w}\\vert\\mathbf{x},\\mathbf{t},\\alpha,\\beta)$ is given by \\eqref{eq:pd.1}. Thus, as a marginal Gaussian distribution, the distribution \\eqref{eq:pdr.1} can be rewritten as \\begin{align} p(t\\vert\\mathbf{x},\\mathbf{t},\\mathbf{w},\\beta)\u0026amp;=\\int\\mathcal{N}(t\\vert\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}),\\beta^{-1})\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{m}_N,\\mathbf{S}_N)\\hspace{0.1cm}d\\mathbf{w} \\\\ \u0026amp;=\\mathcal{N}(t\\vert\\mathbf{m}_N^\\text{T}\\boldsymbol{\\phi}(\\mathbf{x}),\\sigma_N^2(\\mathbf{x})), \\end{align} where the variance $\\sigma_N^2(\\mathbf{x})$ of the predictive distribution is defined as \\begin{equation} \\sigma_N^2(\\mathbf{x})\\doteq\\beta^{-1}+\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}(\\mathbf{x})\\label{eq:pdr.2} \\end{equation} The first term in \\eqref{eq:pdr.2} represents the noise on the data, while the second term reflects the uncertainty associated with the parameters $\\mathbf{w}$.\nIt is worth noting that as additional data points are observed, the posterior distribution becomes narrower. In particular, consider an additional data point $(\\mathbf{x}_{N+1},t_{N+1})$. Therefore, as given by the result \\eqref{eq:pd.3}, its posterior distribution is \\begin{equation} p(\\mathbf{w}\\vert\\mathbf{m}_{N+1},\\mathbf{S}_{N+1}), \\end{equation} where \\begin{align} \\mathbf{m}_{N+1}\u0026amp;=\\mathbf{S}_{N+1}(\\mathbf{S}_N^{-1}\\mathbf{m}_N+t_{N+1}\\beta\\boldsymbol{\\phi}_{N+1}), \\\\ \\mathbf{S}_{N+1}^{-1}\u0026amp;=\\mathbf{S}_{N}^{-1}+\\beta\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T} \\end{align} Therefore, the variance of the corresponding predictive distribution for the newly added data point is then given as \\begin{equation} \\sigma_{N+1}^2(\\mathbf{x})=\\frac{1}{\\beta}+\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\mathbf{S}_{N+1}\\boldsymbol{\\phi}(\\mathbf{x})=\\frac{1}{\\beta}+\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\big(\\mathbf{S}_{N}^{-1}+\\beta\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\big)^{-1}\\boldsymbol{\\phi}(\\mathbf{x})\\label{eq:pdr.3} \\end{equation} Using the matrix identity \\begin{equation} (\\mathbf{M}+\\mathbf{v}\\mathbf{v}^\\text{T})^{-1}=\\mathbf{M}^{-1}-\\frac{(\\mathbf{M}^{-1}\\mathbf{v})(\\mathbf{v}^\\text{T}\\mathbf{M}^{-1})}{1+\\mathbf{v}^\\text{T}\\mathbf{M}^{-1}\\mathbf{v}}, \\end{equation} in the equation \\eqref{eq:pdr.3} gives us \\begin{align} \\sigma_{N+1}^2(\\mathbf{x})\u0026amp;=\\frac{1}{\\beta}+\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\left(\\mathbf{S}_N-\\frac{\\beta\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N}{1+\\beta\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}}\\right)\\boldsymbol{\\phi}(\\mathbf{x}) \\\\ \u0026amp;=\\sigma_N^2(\\mathbf{x})-\\beta\\frac{\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}(\\mathbf{x})}{1+\\beta\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}}\\leq\\sigma_N^2(\\mathbf{x}), \\end{align} since \\begin{equation} \\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}(\\mathbf{x})=\\left\\Vert\\boldsymbol{\\phi}(\\mathbf{x})^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}\\right\\Vert_2^2\\geq0, \\end{equation} and since \\begin{equation} 1+\\beta\\boldsymbol{\\phi}_{N+1}^\\text{T}\\mathbf{S}_N\\boldsymbol{\\phi}_{N+1}\u0026gt;0 \\end{equation} due to $\\mathbf{S}_N$ is the covariance matrix of the posterior distribution $p(\\mathbf{w}\\vert\\mathbf{x},\\mathbf{t},\\alpha,\\beta)$, which implies that it is positive semi-definite.\nIn other words, as $N\\to\\infty$, the second term in \\eqref{eq:pdr.2} goes to zero, and the variance of the predictive distribution solely depends on $\\beta$.\nLinear models for Classification In Machine Learning literature, classification refers the to task of taking an input vector $\\mathbf{x}$ and assigning it to one of $K$ classes $\\mathcal{C}_k$ for $k=1,\\ldots,K$. Usually, each input will be assigned only to a single class. In this case, the input space is divided by the decision boundaries (or decision surfaces) into decision regions.\nTaking an input space of $D$ dimensions, linear models are defined to be linear functions of the input vector $x$, and thus are a $(D-1)$-dimensional hyperplane.\nDiscriminant functions A discriminant is a function that takes an input vector $x$ and assigns it to one of $K$ class, denoted as $\\mathcal{C}_k$\nThe simplest discriminant function is a linear function of the input vector \\begin{equation} y(\\mathbf{x})=\\mathbf{w}^\\text{T}\\mathbf{x}+w_0, \\end{equation} where $\\mathbf{w}$ is called the weight vector, and $w_0$ is the bias.\nIn the case of binary classification, an input $\\mathbf{x}$ is assigned to class $\\mathcal{C}_1$ if $y(\\mathbf{x})\\geq 0$ and otherwise $y(\\mathbf{x})\\lt 0$, it belongs to class $\\mathcal{C}_2$, thus the decision boundary is defined by \\begin{equation} y(\\mathbf{x})=0, \\end{equation} which corresponds to a $(D-1)$-dimensional hyperplane with an $D$-dimensional input space.\nConsider $\\mathbf{x}_A$ and $\\mathbf{x}_B$ lying on the hyperplane, thus $y(\\mathbf{x}_A)=y(\\mathbf{x}_B)=0$, which gives us that \\begin{equation} 0=y(\\mathbf{x}_A)-y(\\mathbf{x}_B)=\\mathbf{w}^\\text{T}\\mathbf{x}_A-\\mathbf{w}^\\text{T}\\mathbf{x}_B=\\mathbf{w}^\\text{T}(\\mathbf{x}_A-\\mathbf{x}_B) \\end{equation} This claims that $\\mathbf{w}$ is perpendicular to any vector within the decision boundary, and thus $\\mathbf{w}$ is a normal vector of the decision boundary itself.\nHence, projecting a point $\\mathbf{x}_0$ into the hyperplane, we have that the distant of $\\mathbf{x}_0$ to the hyperplane is given by \\begin{equation} \\text{dist}(\\mathbf{x}_0,y(\\mathbf{x}))=\\frac{y(\\mathbf{x}_0)}{\\Vert\\mathbf{w}\\Vert}, \\end{equation} which implies that \\begin{equation} \\text{dist}(\\mathbf{0},y(\\mathbf{x}))=\\frac{w_0}{\\Vert\\mathbf{w}\\Vert} \\end{equation} To generalize the binary classification problem into multiple-class ones, we consider a $K$-class discriminant comprising $K$ linear functions of the form \\begin{equation} y_k(\\mathbf{x})=\\mathbf{w}_k^\\text{T}\\mathbf{x}+w_{k,0} \\end{equation} Then for each input $\\mathbf{x}$, it will be assigned to class $\\mathcal{C}_k$ if $y_k(\\mathbf{x})\u0026gt;y_i(\\mathbf{x}),\\forall i\\neq k$, or in other words $\\mathbf{x}$ is assigned to a class $C_k$ that \\begin{equation} k=\\underset{i=1,\\ldots,K}{\\text{argmax}}\\hspace{0.1cm}y_i(\\mathbf{x}) \\end{equation} The boundary between two class $\\mathcal{C}_i$ and $\\mathcal{C}_j$ is therefore given by \\begin{equation} y_i(\\mathbf{x})=y_j(\\mathbf{x}), \\end{equation} or \\begin{equation} (\\mathbf{w}_i-\\mathbf{w}_j)^\\text{T}\\mathbf{x}+w_{i,0}-w_{j,0}=0, \\end{equation} which is an $(D-1)$-dimensional hyperplane.\nLeast squares Recall that in the regression task, we used least squares to find the models in form of linear functions of the parameters. We can also apply least squares approach to classification problems.\nTo begin, we have that for $k=1,\\ldots,K$, each class $\\mathcal{C}_k$ is represented the model \\begin{equation} y_k(\\mathbf{x})=\\mathbf{w}_k^\\text{T}\\mathbf{x}+w_{k,0}\\label{eq:lsc.1} \\end{equation} By giving the bias parameter $w_{k,0}$ a dummy input variable $x_0=0$, we can rewrite \\eqref{eq:lsc.1} in a more convenient form \\begin{equation} y_k(\\mathbf{x})=\\widetilde{\\mathbf{w}}_k^\\text{T}\\widetilde{\\mathbf{x}}, \\end{equation} where \\begin{equation} \\widetilde{\\mathbf{w}}_k=\\left(w_{k,0},\\mathbf{w}_k^\\text{T}\\right)^\\text{T};\\hspace{1cm}\\widetilde{\\mathbf{x}}=\\left(1,\\mathbf{x}^\\text{T}\\right)^\\text{T} \\end{equation} Thus, we can vectorize the $K$ linear models into \\begin{equation} \\mathbf{y}(\\mathbf{x})=\\widetilde{\\mathbf{W}}^\\text{T}\\widetilde{\\mathbf{x}},\\label{eq:lsc.2} \\end{equation} where $\\widetilde{\\mathbf{W}}$ is the parameter matrix whose $k$-th column is the $(D+1)$-dimensional vector $\\widetilde{\\mathbf{w}}_k$ \\begin{equation} \\widetilde{\\mathbf{W}}=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\widetilde{\\mathbf{w}}_1\u0026amp;\\ldots\u0026amp;\\widetilde{\\mathbf{w}}_K \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right] \\end{equation} Consider a training set $\\{\\mathbf{x}_n,\\mathbf{t}_n\\}$ for $n=1,\\ldots,N$, analogy to the parameter matrix $\\widetilde{\\mathbf{W}}$, we can vectorize those input variables and target values into \\begin{equation} \\widetilde{\\mathbf{X}}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\widetilde{\\mathbf{x}}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\widetilde{\\mathbf{x}}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right] \\end{equation} and \\begin{equation} \\mathbf{T}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{t}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{t}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right] \\end{equation} With these definition, the sum-of-squares error function then can be written as \\begin{equation} E_D(\\widetilde{\\mathbf{W}})=\\frac{1}{2}\\text{Tr}\\Big[(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T})^\\text{T}(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T})\\Big] \\end{equation} Taking the derivative of $E_D(\\widetilde{\\mathbf{W}})$ w.r.t $\\widetilde{\\mathbf{W}}$, we obtain \\begin{align} \\nabla_\\widetilde{\\mathbf{W}}E_D(\\widetilde{\\mathbf{W}})\u0026amp;=\\nabla_\\widetilde{\\mathbf{W}}\\frac{1}{2}\\text{Tr}\\Big[(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T})^\\text{T}(\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\mathbf{T})\\Big] \\\\ \u0026amp;=\\frac{1}{2}\\nabla_\\widetilde{\\mathbf{W}}\\text{Tr}\\Big[\\widetilde{\\mathbf{W}}^\\text{T}\\widetilde{\\mathbf{X}}^\\text{T}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\widetilde{\\mathbf{W}}^\\text{T}\\widetilde{\\mathbf{X}}^\\text{T}\\mathbf{T}-\\mathbf{T}^\\text{T}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}+\\mathbf{T}^\\text{T}\\mathbf{T}\\Big] \\\\ \u0026amp;=\\frac{1}{2}\\Big[2\\widetilde{\\mathbf{X}}^\\text{T}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\widetilde{\\mathbf{X}}^\\text{T}\\mathbf{T}-\\widetilde{\\mathbf{X}}^\\text{T}\\mathbf{T}\\Big] \\\\ \u0026amp;=\\widetilde{\\mathbf{X}}^\\text{T}\\widetilde{\\mathbf{X}}\\widetilde{\\mathbf{W}}-\\widetilde{\\mathbf{X}}^\\text{T}\\mathbf{T} \\end{align} Setting this derivative equal to zero, we obtain the least squares solution for $\\widetilde{\\mathbf{W}}$ as \\begin{equation} \\widetilde{\\mathbf{W}}=(\\widetilde{\\mathbf{X}}^\\text{T}\\widetilde{\\mathbf{X}})^{-1}\\widetilde{\\mathbf{X}}^\\text{T}\\mathbf{T}=\\widetilde{\\mathbf{X}}^\\dagger\\mathbf{T} \\end{equation} Therefore, the discriminant function \\eqref{eq:lsc.2} can be rewritten as \\begin{equation} \\mathbf{y}(\\mathbf{x})=\\widetilde{\\mathbf{W}}^\\text{T}\\widetilde{\\mathbf{x}}=\\mathbf{T}^\\text{T}\\big(\\widetilde{\\mathbf{X}}^\\dagger\\big)^\\text{T}\\widetilde{\\mathbf{x}} \\end{equation}\nFisher\u0026rsquo;s linear discriminant One way to view a linear classification model is in terms of dimensional reduction. In particular, given an $D$-dimensional input $\\mathbf{x}$, we project it down to one dimension using \\begin{equation} y=\\mathbf{w}^\\text{T}\\mathbf{x} \\end{equation}\nBinary classification Consider a binary classification in which there are $N_1$ points of class $\\mathcal{C}_1$ and $N_2$ points of class $\\mathcal{C}_2$, thus the mean vectors of those two classes are given by \\begin{align} \\mathbf{m}_1\u0026amp;=\\frac{1}{N_1}\\sum_{n\\in\\mathcal{C}_1}\\mathbf{x}_n, \\\\ \\mathbf{m}_2\u0026amp;=\\frac{1}{N_2}\\sum_{n\\in\\mathcal{C}_2}\\mathbf{x}_n \\end{align} The simplest measure of the separation of the classes, when projected onto $\\mathbf{w}$, is the separation of the projected class means, which suggests us choosing $\\mathbf{w}$ in order to maximize \\begin{equation} m_2-m_1=\\mathbf{w}^\\text{T}(\\mathbf{m}_2-\\mathbf{m}_1), \\end{equation} where for $k=1,\\ldots,K$ \\begin{equation} m_k=\\mathbf{w}^\\text{T}\\mathbf{m}_k \\end{equation} is the mean of the projected data from class $\\mathcal{C}_k$.\nTo make the computation simpler, we normalize the projection simply by making a constraint of $\\mathbf{w}$ being a unit vector, which means \\begin{equation} \\big\\Vert\\mathbf{w}\\big\\Vert_2=\\sum_{i}w_i=1 \\end{equation} Therefore, by Lagrange multiplier, in order to maximize $m_2-m_1$, we have that \\begin{equation} \\mathbf{w}\\propto(\\mathbf{m}_2-\\mathbf{m}_1) \\end{equation} To solve this problem, we use the Fisher\u0026rsquo;s LD approach to minimize the class overlap by maximizing the ratio of the between-class variance to the within-class variance.\nThe within-class variance of projected data from class $\\mathbf{w}_k$ is defined as \\begin{equation} s_k^2\\doteq\\sum_{n\\in\\mathcal{C}_k}(y_n-m_k)^2, \\end{equation} where $y_n=\\mathbf{w}^\\text{T}\\mathbf{x}_n$ is the projected of $\\mathbf{x}_n$. Thus the total within-class variance for the whole data set is $s_1^2+s_2^2$.\nThe between-class variance is simply defined to be the squared of the difference of means, given as \\begin{equation} (m_2-m_1)^2 \\end{equation} Hence, the ratio of the between-class variance to the within-class variance, called the Fisher criterion, can be defined as \\begin{align} J(\\mathbf{w})\u0026amp;=\\frac{(m_2-m_1)^2}{s_1^2+s_2^2} \\\\ \u0026amp;=\\frac{\\big\\Vert\\mathbf{w}^\\text{T}(\\mathbf{m}_2-\\mathbf{m}_1)\\big\\Vert_2^2}{\\sum_{n\\in\\mathcal{C}_1}\\big\\Vert\\mathbf{w}^\\text{T}(\\mathbf{x}_n-\\mathbf{m}_1)\\big\\Vert_2^2+\\sum_{n\\in\\mathcal{C}_2}\\big\\Vert\\mathbf{w}^\\text{T}(\\mathbf{x}_n-\\mathbf{m}_2)\\big\\Vert_2^2} \\\\ \u0026amp;=\\frac{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}}{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}},\\label{eq:flbc.1} \\end{align} where \\begin{equation} \\mathbf{S}_\\text{B}\\doteq(\\mathbf{m}_2-\\mathbf{m}_1)(\\mathbf{m}_2-\\mathbf{m}_1)^\\text{T}, \\end{equation} is called the between-class covariance matrix and \\begin{equation} \\mathbf{S}_\\text{W}\\doteq\\sum_{n\\in\\mathcal{C}_1}(\\mathbf{x}_n-\\mathbf{m}_1)(\\mathbf{x}_n-\\mathbf{m}_1)^\\text{T}+\\sum_{n\\in\\mathcal{C}_2}(\\mathbf{x}_n-\\mathbf{m}_2)(\\mathbf{x}_n-\\mathbf{m}_2)^\\text{T}, \\end{equation} is called the total within-class covariance matrix.\nAs usual, taking the gradient of \\eqref{eq:flbc.1} w.r.t $\\mathbf{w}$, we have \\begin{align} \\nabla_\\mathbf{w}J(\\mathbf{w})\u0026amp;=\\nabla_\\mathbf{w}\\frac{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}}{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}} \\\\ \u0026amp;=\\frac{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}(\\mathbf{S}_\\text{B}+\\mathbf{S}_\\text{B}^\\text{T})\\mathbf{w}-\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}(\\mathbf{S}_\\text{W}+\\mathbf{S}_\\text{W}^\\text{T})\\mathbf{w}}{\\big\\Vert\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}\\big\\Vert_2^2} \\\\ \u0026amp;=\\frac{\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}\\mathbf{S}_\\text{B}\\mathbf{w}-\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}\\mathbf{S}_\\text{W}\\mathbf{w}}{\\big\\Vert\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}\\big\\Vert_2^2} \\\\ \u0026amp;\\propto\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}\\mathbf{S}_\\text{B}\\mathbf{w}-\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}\\mathbf{S}_\\text{W}\\mathbf{w} \\end{align} Setting the gradient equal to zero and solving for $\\mathbf{w}$, we obtain that $\\mathbf{w}$ satisfies \\begin{equation} \\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}\\mathbf{S}_\\text{B}\\mathbf{w}=\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}\\mathbf{S}_\\text{W}\\mathbf{w} \\end{equation} Since $\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{W}\\mathbf{w}$ and $\\mathbf{w}^\\text{T}\\mathbf{S}_\\text{B}\\mathbf{w}$ are two scalars, we then have \\begin{equation} \\mathbf{S}_\\text{W}\\mathbf{w}\\propto\\mathbf{S}_\\text{B}\\mathbf{w} \\end{equation} Multiply both side by $\\mathbf{S}_\\text{W}^{-1}$, we obtain \\begin{align} \\mathbf{w}\u0026amp;\\propto\\mathbf{S}_\\text{W}^{-1}\\mathbf{S}_\\text{B}\\mathbf{w} \\\\ \u0026amp;=\\mathbf{S}_\\text{W}^{-1}(\\mathbf{m}_2-\\mathbf{m}_1)(\\mathbf{m}_2-\\mathbf{m}_1)^\\text{T}\\mathbf{w} \\\\ \u0026amp;\\propto\\mathbf{S}_\\text{W}^{-1}(\\mathbf{m}_2-\\mathbf{m}_1),\\label{eq:flbc.2} \\end{align} since $(\\mathbf{m}_2-\\mathbf{m}_1)^\\text{T}\\mathbf{w}$ is a scalar.\nIf the within-class covariance matrix $\\mathbf{S}_\\text{W}$ is isotropic1, we then have \\begin{equation} \\mathbf{w}\\propto\\mathbf{m}_2-\\mathbf{m}_1 \\end{equation} The result \\eqref{eq:flbc.2} is called Fisher\u0026rsquo;s linear discriminant.\nWith this $\\mathbf{w}$, we can project our data down into one dimension and from projected data, we construct a discriminant by selecting a threshold $y_0$ such that $\\mathbf{x}$ belongs to class $\\mathcal{C}_1$ if $y(\\mathbf{x})\\gg y_0$ and otherwise it belongs to $\\mathcal{C}_1$.\nMulti-class classification To generalize the Fisher discriminant to the case of $K\u0026gt;2$, we first assume that $D\u0026gt;K$ and consider the $D\u0026rsquo;\u0026gt;1$ linear features \\begin{equation} y=\\mathbf{w}_k^\\text{T}\\mathbf{x}, \\end{equation} where $k=1,\\ldots,D\u0026rsquo;$. Thus, as usual we can vectorize these feature values as \\begin{equation} \\mathbf{y}=\\mathbf{W}^\\text{T}\\mathbf{x},\\label{eq:flc.1} \\end{equation} where \\begin{equation} \\mathbf{y}=(y_1,\\ldots,y_k)^\\text{T},\\hspace{2cm}\\mathbf{W}=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\mathbf{w}_1\u0026amp;\\ldots\u0026amp;\\mathbf{w}_{D\u0026rsquo;} \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right] \\end{equation} The mean vector for each class is unchanged, which is given as \\begin{equation} \\mathbf{m}_k=\\frac{1}{N_k}\\sum_{n\\in\\mathcal{C}_k}\\mathbf{x}_n, \\end{equation} where $N_k$ is the number of points in class $\\mathcal{C}_k$ for $k=1,\\ldots,K$.\nThe within-class variance covariance matrix $\\mathbf{S}_\\text{W}$ now can be simply generalized as \\begin{equation} \\mathbf{S}_\\text{W}=\\sum_{k=1}^{K}\\mathbf{S}_k,\\label{eq:flc.2} \\end{equation} where \\begin{equation} \\mathbf{S}_k=\\sum_{n\\in\\mathcal{C}_k}(\\mathbf{x}_n-\\mathbf{m}_k)(\\mathbf{x}_n-\\mathbf{m}_k)^\\text{T} \\end{equation} To find the generalization of the between-class covariance matrix $\\mathbf{S}_\\text{B}$, we first consider the total covariance matrix \\begin{equation} \\mathbf{S}_T=\\sum_{n=1}^{N}(\\mathbf{x}_n-\\mathbf{m})(\\mathbf{x}_n-\\mathbf{m})^\\text{T}, \\end{equation} where \\begin{equation} \\mathbf{m}=\\frac{1}{N}\\sum_{n=1}^{N}\\mathbf{x}_n=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\mathbf{m}_k \\end{equation} is the mean of the whole data set, where $N=\\sum_{k=1}^{K}N_k$ is the number of the data points. The total covariance matrix can be decomposed into the sum of the within-class covariance matrix $\\mathbf{S}_\\text{W}$, as given in \\eqref{eq:flc.2} with a matrix $\\mathbf{S}_\\text{B}$, defined as a measure of the between-class covariance \\begin{equation} \\mathbf{S}_\\text{T}=\\mathbf{S}_\\text{W}+\\mathbf{S}_\\text{B}, \\end{equation} where \\begin{equation} \\mathbf{S}_\\text{B}=\\sum_{k=1}^{K}N_k(\\mathbf{m}_k-\\mathbf{m})(\\mathbf{m}_k-\\mathbf{m})^\\text{T} \\end{equation} Using \\eqref{eq:flc.1}, we project the whole data set into the $D\u0026rsquo;$-dimensional space of $\\mathbf{y}$, the corresponding within-class covariance matrix of the transformed data are given as \\begin{align} \\mathbf{s}_\\text{W}\u0026amp;=\\sum_{k=1}^{K}\\sum_{n\\in\\mathcal{C}_k}\\left(\\mathbf{W}^\\text{T}\\mathbf{x}_n-\\mathbf{W}^\\text{T}\\mathbf{m}_k\\right)\\left(\\mathbf{W}^\\text{T}\\mathbf{x}_n-\\mathbf{W}^\\text{T}\\mathbf{m}_k\\right)^\\text{T} \\\\ \u0026amp;=\\sum_{k=1}^{K}\\sum_{n\\in\\mathcal{C}_k}(\\mathbf{y}_n-\\boldsymbol{\\mu}_k)(\\mathbf{y}_n-\\boldsymbol{\\mu}_k)^\\text{T} \\\\ \u0026amp;=\\mathbf{W}\\mathbf{S}_\\text{W}\\mathbf{W}^\\text{T} \\end{align} and also the transformed between-class covariance matrix \\begin{align} \\mathbf{s}_\\text{B}\u0026amp;=\\sum_{k=1}^{K}N_k(\\mathbf{W}^\\text{T}\\mathbf{m}_k-\\mathbf{W}^\\text{T}\\mathbf{m})(\\mathbf{W}^\\text{T}\\mathbf{m}_k-\\mathbf{W}^\\text{T}\\mathbf{m})^\\text{T} \\\\ \u0026amp;=\\sum_{k=1}^{K}(\\boldsymbol{\\mu}_k-\\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k-\\boldsymbol{\\mu})^\\text{T} \\\\ \u0026amp;=\\mathbf{W}\\mathbf{S}_\\text{B}\\mathbf{W}^\\text{T}, \\end{align} where \\begin{align} \\boldsymbol{\\mu}_k\u0026amp;=\\mathbf{W}^\\text{T}\\mathbf{m}_k=\\mathbf{W}^\\text{T}\\frac{1}{N_k}\\sum_{n\\in\\mathcal{C}_k}\\mathbf{x}_n=\\frac{1}{N_k}\\sum_{n\\in\\mathcal{C}_k}\\mathbf{y}_n, \\\\ \\boldsymbol{\\mu}\u0026amp;=\\mathbf{W}^\\text{T}\\mathbf{m}=\\mathbf{W}^\\text{T}\\frac{1}{N}\\sum_{k=1}^{K}N_k\\mathbf{m}_k=\\frac{1}{N}\\sum_{k=1}^{K}N_k\\boldsymbol{\\mu}_k \\end{align} Analogy to the case of binary classification with Fisher\u0026rsquo;s criterion \\eqref{eq:flbc.1}, we need a new measure that is large when the between-class covariance is large and when the within-class covariance is small. A simple choice of criterion is given as \\begin{equation} J(\\mathbf{W})=\\text{Tr}\\left(\\mathbf{s}_\\text{W}^{-1}\\mathbf{s}_\\text{B}\\right) \\end{equation} or \\begin{equation} J(\\mathbf{w})=\\text{Tr}\\big[(\\mathbf{W}\\mathbf{S}_\\text{W}\\mathbf{W}^\\text{T})^{-1}(\\mathbf{W}\\mathbf{S}_\\text{B}\\mathbf{W}^\\text{T})\\big] \\end{equation} for which the linear basis model follow the same rule as the above\nThe perceptron algorithm Another example of linear discriminant model is the perceptron algorithm\nProbabilistic Generative Models When solving the classification problems, we divide the strategy into two stage\nInference stage. In this stage we use training data to learn a model for $p(\\mathcal{C}_k\\vert\\mathbf{x})$ Decision stage. In this stage we use those posterior probabilities to make optimal class assignments. We can solve both inference and decision problems at the same time by learning a function, which is the discriminant function, maps inputs $\\mathbf{x}$ directly into decisions. When using the generative approach to solve the problem of classification, we first model the class-conditional densities $p(\\mathbf{x}\\vert\\mathcal{C}_k)$ and the class priors $p(\\mathcal{C}_k)$ then apply Bayes\u0026rsquo; theorem to compute the posterior probabilities $p(\\mathcal{C}_k\\vert\\mathbf{x})$.\nConsider the binary case, in which specifically the posterior probability for class $\\mathcal{C}_1$ can be computed as \\begin{align} p(\\mathcal{C}_1\\vert\\mathbf{x})\u0026amp;=\\frac{p(\\mathbf{x}\\vert\\mathcal{C}_1)p(\\mathcal{C}_1)}{p(\\mathbf{x}\\vert\\mathcal{C}_1)p(\\mathcal{C}_1)+p(\\mathbf{x}\\vert\\mathcal{C}_2)p(\\mathcal{C}_2)} \\\\ \u0026amp;=\\frac{1}{1+\\frac{p(\\mathbf{x}\\vert\\mathcal{C}_2)p(\\mathcal{C}_2)}{p(\\mathbf{x}\\vert\\mathcal{C}_1)p(\\mathcal{C}_1)}} \\\\ \u0026amp;=\\frac{1}{1+\\exp(-a)}=\\sigma(a)\\label{eq:pgm.1} \\end{align} where \\begin{equation} a=\\log\\frac{p(\\mathbf{x}\\vert\\mathcal{C}_2)p(\\mathcal{C}_2)}{p(\\mathbf{x}\\vert\\mathcal{C}_1)p(\\mathcal{C}_1)} \\end{equation} and where $\\sigma(\\cdot)$ is the logistic sigmoid function, defined before as \\begin{equation} \\sigma(a)\\doteq\\frac{1}{1+\\exp(-a)} \\end{equation} For the case of multi-class, $K\u0026gt;2$, the posterior probability for class $\\mathcal{C}_k$ can be generalized as \\begin{align} p(\\mathcal{C}_k\\vert\\mathbf{x})\u0026amp;=\\frac{p(\\mathbf{x}\\vert\\mathcal{C}_k)p(\\mathcal{C}_k)}{\\sum_{i=1}^{K}p(\\mathbf{x}\\vert\\mathcal{C}_i)p(\\mathcal{C}_i)}=\\dfrac{\\exp\\Big[\\log\\big(p(\\mathbf{x}\\vert\\mathcal{C}_k)p(\\mathcal{C}_k)\\big)\\Big]}{\\sum_{i=1}^{K}\\exp\\Big[\\log\\big(p(\\mathbf{x}\\vert\\mathcal{C}_i)p(\\mathcal{C}_i)\\big)\\Big]} \\\\ \u0026amp;=\\frac{\\exp(a_k)}{\\sum_{i=1}^{K}\\exp(a_i)}=\\sigma(\\mathbf{a})_k\\label{eq:pgm.2} \\end{align} where \\begin{align} a_k\u0026amp;=\\log\\big(p(\\mathbf{x}\\vert\\mathcal{C}_k)p(\\mathcal{C}_k)\\big), \\\\ \\mathbf{a}\u0026amp;=(a_1,\\ldots,a_K)^\\text{T}, \\end{align} and the function $\\sigma:\\mathbb{R}^K\\to(0,1)^K$, known as the normalized exponential or softmax function - a generalization of sigmoid into multi-dimensional, in which the $k$-th element is defined as \\begin{equation} \\sigma(\\mathbf{a})_k\\doteq\\frac{\\exp(a_k)}{\\sum_{i=1}^{K}\\exp(a_i)}, \\end{equation} for $k=1,\\ldots,K$ and $\\mathbf{a}=(a_1,\\ldots,a_K)^\\text{T}$.\nGaussian Generative models If the class-conditional probabilities are Gaussian, or specifically Multivariate Normal and share the same covariance matrix $\\boldsymbol{\\Sigma}$, then for $k=1,\\ldots,K$, \\begin{equation} \\mathbf{x}\\vert\\mathcal{C}_k\\sim\\mathcal{N}(\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}) \\end{equation} Thus, the density for class $\\mathcal{C}_k$ is defined as \\begin{equation} p(\\mathbf{x}\\vert\\mathcal{C}_k)=\\frac{1}{(2\\pi)^{D/2}\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_k)\\right] \\end{equation} In the binary case, in which the densities above become Bivariate Normal, by \\eqref{eq:pgm.1} we have that \\begin{align} p(\\mathcal{C}_1\\vert\\mathbf{x})\u0026amp;=\\sigma\\left(\\log\\frac{p(\\mathbf{x}\\vert\\mathcal{C}_2)p(\\mathcal{C}_2)}{p(\\mathbf{x}\\vert\\mathcal{C}_1)p(\\mathcal{C}_1)}\\right) \\\\ \u0026amp;=\\sigma\\left(\\log\\frac{\\exp\\Big[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_2)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_2)\\Big]p(\\mathcal{C}_2)}{\\exp\\Big[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_1)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_1)\\Big]p(\\mathcal{C}_1)}\\right) \\\\ \u0026amp;=\\sigma\\Bigg(-\\frac{1}{2}\\Big[-\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2-\\boldsymbol{\\mu}_2^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\boldsymbol{\\mu}_2^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2+\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}-\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1\\Big]-\\log\\frac{p(\\mathcal{C}_2)}{p(\\mathcal{C}_1)}\\Bigg) \\\\ \u0026amp;=\\sigma\\left(\\boldsymbol{\\Sigma}^{-1}\\left(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2\\right)^\\text{T}\\mathbf{x}-\\frac{1}{2}\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1+\\frac{1}{2}\\boldsymbol{\\mu}_2^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2+\\log\\frac{p(\\mathcal{C}_1)}{p(\\mathcal{C}_2)}\\right)\\label{eq:ggm.1} \\end{align} Let \\begin{align} \\mathbf{w}\u0026amp;=\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_2), \\\\ w_0\u0026amp;=-\\frac{1}{2}\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1+\\frac{1}{2}\\boldsymbol{\\mu}_2^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_2+\\log\\frac{p(\\mathcal{C}_1)}{p(\\mathcal{C}_2)}, \\end{align} we have \\eqref{eq:ggm.1} can be rewritten in more convenient form as \\begin{equation} p(\\mathcal{C}_1\\vert\\mathbf{x})=\\sigma\\big(\\mathbf{w}^\\text{T}\\mathbf{x}+w_0\\big) \\end{equation} From the derivation, we see that by making an assumption of having the same covariance matrix $\\boldsymbol{\\Sigma}$ across the densities helped us remove out the quadratic terms of $\\mathbf{x}$, which leads us to ending up with a logistic sigmoid of a linear function of $\\mathbf{x}$.\nFor the multi-dimensional case, $K\u0026gt;2$, by \\eqref{eq:pgm.2}, we have that the density for class $\\mathcal{C}_k$ is \\begin{align} p(\\mathcal{C}_k\\vert\\mathbf{x})\u0026amp;=\\frac{\\exp\\Big[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_k)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_k)+\\log p(\\mathcal{C}_k)\\Big]}{\\sum_{i=1}^{K}\\exp\\Big[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu}_i)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}_i)+\\log p(\\mathcal{C}_i)\\Big]} \\\\ \u0026amp;=\\frac{\\exp\\Big[\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k-\\frac{1}{2}\\boldsymbol{\\mu}_k^\\text{T}\\boldsymbol{\\Sigma}\\boldsymbol{\\mu}_k+\\log p(\\mathbf{w}_k)\\Big]}{\\sum_{i=1}^{K}\\exp\\Big[\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_i-\\frac{1}{2}\\boldsymbol{\\mu}_i^\\text{T}\\boldsymbol{\\Sigma}\\boldsymbol{\\mu}_i+\\log p(\\mathbf{w}_i)\\Big]} \\end{align} Or in other words, we can simplify each element of $\\mathbf{a}$ into a linear function as \\begin{equation} a_k\\doteq a_k(\\mathbf{x})=\\mathbf{w}_k^\\text{T}\\mathbf{x}+w_{k,0}, \\end{equation} where \\begin{align} \\mathbf{w}_k\u0026amp;=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k, \\\\ w_{k,0}\u0026amp;=-\\frac{1}{2}\\boldsymbol{\\mu}_k^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k+\\log p(\\mathcal{C}_k) \\end{align} The simplification we can make also come from the assumption of sharing the same covariance matrix between densities, which is analogous to the binary case that cancelled out the quadratic terms.\nMaximum likelihood solutions Once we have specified a parametric functional form of $p(\\mathbf{x}\\vert\\mathcal{C}_k)$, using maximum likelihood, we can solve for the values of the parameters and also the prior probabilities $p(\\mathcal{C}_k)$.\nBinary classification In particular, first off for the binary case, in which each class-conditional densities $p(\\mathbf{x}\\vert\\mathcal{C}_k)$ is a Bivariate Normal, with a shared covariance matrix, as \\begin{equation} \\mathbf{x}\\vert\\mathcal{C}_k\\sim\\mathcal{N}(\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}) \\end{equation} Consider the data set $\\{\\mathbf{x}_n,t_n\\}$ for $n=1,\\ldots,N$, i.e., $t_n=1$ denotes class $\\mathcal{C}_1$ and $t_n=0$ denotes class $\\mathcal{C}_2$. Let the class prior probability $p(\\mathcal{C}_1)=\\pi$, thus $p(\\mathcal{C}_2)=1-\\pi$. Or \\begin{align} p(\\mathbf{x}_n,\\mathcal{C}_1)\u0026amp;=p(\\mathcal{C}_1)p(\\mathbf{x}_n\\vert\\mathcal{C}_1)=\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}), \\\\ p(\\mathbf{x}_n,\\mathcal{C}_2)\u0026amp;=p(\\mathcal{C}_2)p(\\mathbf{x}_n\\vert\\mathcal{C}_2)=\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\end{align} We also have that \\begin{equation} p(t_n\\vert\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})=p(\\mathbf{x}_n,\\mathcal{C}_1)^{t_n}p(\\mathbf{x}_n,\\mathcal{C}_2)^{1-t_n} \\end{equation} Therefore, the likelihood can be defined as \\begin{align} L(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\u0026amp;=p(\\mathbf{t}\\vert\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\prod_{n=1}^{N}p(t_n\\vert\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\prod_{n=1}^{N}p(\\mathbf{x}_n,\\mathcal{C}_1)^{t_n}p(\\mathbf{x}_n,\\mathcal{C}_2)^{1-t_n} \\\\ \u0026amp;=\\prod_{n=1}^{N}\\Big[\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})\\Big]^{t_n}\\Big[(1-\\pi)\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\\Big]^{1-t_n}, \\end{align} where $\\mathbf{t}=(t_1,\\ldots,t_N)^\\text{T}$. As usual, we continue to consider the log likelihood $\\ell(\\cdot)$ \\begin{align} \u0026amp;\\hspace{0.7cm}\\ell(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\log L(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\sum_{n=1}^{N}t_n\\log\\Big[\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})\\Big]+(1-t_n)\\log\\Big[(1-\\pi)\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\\Big]\\label{eq:gbc.1} \\end{align} Taking the gradient of the log likelihood w.r.t $\\pi$ we have \\begin{align} \u0026amp;\\hspace{0.7cm}\\nabla_\\pi\\ell(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_\\pi\\sum_{n=1}^{N}t_n\\log\\Big[\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})\\Big]+(1-t_n)\\log\\Big[(1-\\pi)\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\\Big] \\\\ \u0026amp;=\\nabla_\\pi\\sum_{n=1}^{N}t_n\\log\\pi+(1-t_n)\\log(1-\\pi) \\\\ \u0026amp;=\\sum_{n=1}^{N}\\left[\\frac{t_n}{\\pi}-\\frac{1-t_n}{1-\\pi}\\right] \\end{align} Setting the derivative to zero and solve for $\\pi$ as usual, we have \\begin{equation} \\sum_{n=1}^{N}t_n-\\pi=0 \\end{equation} Thus, we obtain the solution \\begin{equation} \\pi=\\frac{1}{N}\\sum_{n=1}^{N}t_n=\\frac{N_1}{N}=\\frac{N_1}{N_1+N_2}, \\end{equation} where $N_1,N_2$ denote the total number of data points in class $\\mathcal{C}_1$ and $\\mathcal{C}_2$ respectively.\nOn the other hand, taking the gradient of the log likelihood \\eqref{eq:gbc.1} w.r.t $\\boldsymbol{\\mu}_1$, we have \\begin{align} \u0026amp;\\hspace{0.7cm}\\nabla_{\\boldsymbol{\\mu}_1}\\ell(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_{\\boldsymbol{\\mu}_1}\\sum_{n=1}^{N}t_n\\log\\Big[\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})\\Big]+(1-t_n)\\log\\Big[(1-\\pi)\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\\Big] \\\\ \u0026amp;=\\nabla_{\\boldsymbol{\\mu}_1}\\sum_{n=1}^{N}t_n\\log\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_{\\boldsymbol{\\mu}_1}\\sum_{n=1}^{N}t_n\\left[-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)\\right] \\\\ \u0026amp;\\propto\\nabla_{\\boldsymbol{\\mu}_1}\\sum_{n=1}^{N}t_n\\big(-\\boldsymbol{x}_n^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1-\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}_n+\\boldsymbol{\\mu}_1^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_1\\big) \\\\ \u0026amp;=\\sum_{n=1}^{N}t_n\\Big[\\big(\\boldsymbol{\\Sigma}^{-1}+(\\boldsymbol{\\Sigma}^{-1})^\\text{T}\\big)\\big(\\boldsymbol{\\mu}_1-\\mathbf{x}_n\\big)\\Big] \\\\ \u0026amp;\\propto\\sum_{n=1}^{N}t_n(\\boldsymbol{\\mu}_1-\\mathbf{x}_n) \\end{align} Setting the above gradient to zero and solve for $\\boldsymbol{\\mu}_1$, we obtain the solution \\begin{equation} \\boldsymbol{\\mu}_1=\\frac{1}{N_1}\\sum_{n=1}^{N}t_n\\mathbf{x}_n, \\end{equation} which is simply the mean of all input vectors $\\mathbf{x}_n$ assigned to class $\\mathcal{C}_1$.\nSimilarly, with the same procedure, we have that the maximum likelihood solution for $\\boldsymbol{\\mu}_2$ is the mean of all inputs vectors $\\mathbf{x}_n$ assigned to class $\\mathcal{C}_2$, as \\begin{equation} \\boldsymbol{\\mu}_2=\\frac{1}{N_2}\\sum_{n=1}^{N}(1-t_n)\\mathbf{x}_n \\end{equation} Lastly, taking the gradient of the log likelihood \\eqref{eq:gbc.1} w.r.t $\\boldsymbol{\\Sigma}$, we have \\begin{align} \u0026amp;\\hspace{0.7cm}\\nabla_\\boldsymbol{\\Sigma}\\ell(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}t_n\\log\\Big[\\pi\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})\\Big]+(1-t_n)\\log\\Big[(1-\\pi)\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})\\Big] \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}t_n\\log\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_1,\\boldsymbol{\\Sigma})+(1-t_n)\\log\\mathcal{N}(\\mathbf{x}_n\\vert\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}t_n\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert^{-1/2}+t_n\\left[-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)\\right]\\nonumber \\\\ \u0026amp;\\hspace{2cm}+(1-t_n)\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert^{-1/2}+t_n\\left[-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)\\right] \\\\ \u0026amp;\\propto\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert+t_n(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+(1-t_n)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_2) \\\\ \u0026amp;=N\\nabla_\\boldsymbol{\\Sigma}\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert-\\boldsymbol{\\Sigma}^{-1}\\Big[\\sum_{n=1}^{N}t_n(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}\\nonumber \\\\ \u0026amp;\\hspace{2cm}+(1-t_n)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T}\\Big]\\boldsymbol{\\Sigma}^{-1}\\label{eq:gbc.2} \\end{align} The first term of the gradient can be computed as \\begin{align} \\frac{\\partial\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert}{\\partial\\boldsymbol{\\Sigma}_{ij}}=\\frac{1}{\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert}\\frac{\\partial\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert}{\\partial\\boldsymbol{\\Sigma}_{ij}}=\\frac{1}{\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert}\\text{adj}(\\boldsymbol{\\Sigma})_{ji}=(\\boldsymbol{\\Sigma}^{-1})_{ji}=(\\boldsymbol{\\Sigma}^{-1})_{ij}, \\end{align} since $\\boldsymbol{\\Sigma}$ is symmetric and so is its inverse. This implies that \\begin{equation} \\nabla_\\boldsymbol{\\Sigma}\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert=\\boldsymbol{\\Sigma}^{-1}\\label{eq:gbc.3} \\end{equation} Let $\\mathbf{S}$ be a matrix defined as \\begin{equation} \\mathbf{S}=\\frac{1}{N}\\sum_{n=1}^{N}t_n(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}+(1-t_n)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T} \\end{equation} Therefore, the derivative \\eqref{eq:gbc.2} can be rewritten as \\begin{equation} \\nabla_\\boldsymbol{\\Sigma}\\ell(\\pi,\\boldsymbol{\\mu}_1,\\boldsymbol{\\mu}_2,\\boldsymbol{\\Sigma})=N\\boldsymbol{\\Sigma}^{-1}-N\\boldsymbol{\\Sigma}^{-1}\\mathbf{S}\\boldsymbol{\\Sigma}^{-1} \\end{equation} Setting this gradient to zero and solve for $\\boldsymbol{\\Sigma}$, we obtain the solution \\begin{equation} \\boldsymbol{\\Sigma}=\\mathbf{S}, \\end{equation} where $\\mathbf{S}$ can be continued to derive as \\begin{align} \\mathbf{S}\u0026amp;=\\frac{1}{N}\\sum_{n=1}^{N}t_n(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}+(1-t_n)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T} \\\\ \u0026amp;=\\frac{N_1}{N}\\sum_{n\\in\\mathcal{C}_1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)(\\mathbf{x}_n-\\boldsymbol{\\mu}_1)^\\text{T}+\\frac{N_2}{N}\\sum_{n\\in\\mathcal{C}_2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)(\\mathbf{x}_n-\\boldsymbol{\\mu}_2)^\\text{T}, \\end{align} which is the weighted average of the covariance matrices corresponded to each of the two classes $\\mathcal{C}_1,\\mathcal{C}_2$.\nMulti-class classification To generalize the Gaussian generative binary classification, we consider a model for $K\u0026gt;2$ classes defined by prior class probabilities $p(\\mathcal{C}_k)=\\pi_k$ and Multivariate Normal class-conditional densities with shared covariance matrix, given as \\begin{equation} p({\\boldsymbol{\\phi}}\\vert\\mathcal{C}_k)=\\mathcal{N}(\\boldsymbol{\\phi}\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}), \\end{equation} where $\\boldsymbol{\\phi}$ is the input feature vector.\nGiven a data set $\\{\\boldsymbol{\\phi}_n,\\mathbf{t}_n\\}$ for $n=1,\\ldots,N$ where $\\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\\mathbf{t}_n)_k=1$ denotes class $\\mathcal{C}_k$ and $(\\mathbf{t}_n)_i=0$ for all $i\\neq k$. Therefore, we have that \\begin{equation} p(\\boldsymbol{\\phi}_n,\\mathcal{C}_k)=p(\\mathcal{C}_k)p(\\boldsymbol{\\phi}_n\\vert\\mathcal{C}_k)=\\pi_k\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}) \\end{equation} Analogy to the binary case, we also have that \\begin{equation} p(\\mathbf{t}_n\\vert\\pi_1,\\ldots,\\pi_K,\\boldsymbol{\\phi}_1,\\ldots,\\boldsymbol{\\phi}_K,\\boldsymbol{\\Sigma})=\\prod_{k=1}^{K}p(\\boldsymbol{\\phi}_n,\\mathcal{C}_k)^{(\\mathbf{t}_n)_k} \\end{equation} To simplify the notation, we let $\\mathbf{w}$ denote \\begin{equation} \\pi_1,\\ldots,\\pi_K,\\boldsymbol{\\phi}_1,\\ldots,\\boldsymbol{\\phi}_K,\\boldsymbol{\\Sigma} \\end{equation} And let $\\mathbf{T}$ be a matrix that associates those targets $\\mathbf{t}_n$\u0026rsquo;s together, given as \\begin{equation} \\mathbf{T}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{t}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{t}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right] \\end{equation} Thus, the likelihood is given as \\begin{align} L(\\mathbf{w})\u0026amp;=p(\\mathbf{T}\\vert\\mathbf{w}) \\\\ \u0026amp;=\\prod_{n=1}^{N}p(\\mathbf{t}_n\\vert\\mathbf{w}) \\\\ \u0026amp;=\\prod_{n=1}^{N}\\prod_{k=1}^{K}p(\\boldsymbol{\\phi}_n,\\mathcal{C}_k)^{(\\mathbf{t}_n)_k} \\\\ \u0026amp;=\\prod_{n=1}^{N}\\prod_{k=1}^{K}\\Big[\\pi_k\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma})\\Big]^{(\\mathbf{t}_n)_k} \\end{align} And thus, the log likelihood $\\ell(\\cdot)$ can be computed as \\begin{align} \\ell(\\mathbf{w})\u0026amp;=\\log L(\\mathbf{w}) \\\\ \u0026amp;=\\log\\prod_{n=1}^{N}\\prod_{k=1}^{K}\\Big[\\pi_k\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma})\\Big]^{(\\mathbf{t}_n)_k} \\\\ \u0026amp;=\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k\\Big[\\log\\pi_k+\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma})\\Big]\\label{eq:gc.1} \\end{align} As usual, we continue by using maximum likelihood, which begins by taking gradient of the log likelihood w.r.t to the parameters. However, when maximizing the likelihood w.r.t $\\pi_k$, we have to compute subject to a constraint that \\begin{equation} \\sum_{k=1}^{K}\\pi_k=1 \\end{equation} Therefore, using a Lagrange multiplier $\\lambda$, we instead maximize the Lagrangian w.r.t $\\pi_k$, which is \\begin{equation} \\mathcal{L}(\\pi_1,\\ldots,\\pi_K,\\lambda)=\\ell(\\mathbf{w})+\\lambda\\left(\\sum_{k=1}^{K}\\pi_k-1\\right) \\end{equation} Differentiating $\\mathcal{L}$ w.r.t $\\pi_k$, we have \\begin{align} \u0026amp;\\hspace{0.7cm}\\nabla_{\\pi_k}\\mathcal{L}(\\pi_1,\\ldots,\\pi_K,\\lambda) \\\\ \u0026amp;=\\nabla_{\\pi_k}\\sum_{n=1}^{N}\\sum_{i=1}^{K}(\\mathbf{t}_n)_i\\Big[\\log\\pi_i+\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma})\\Big]+\\nabla_{\\pi_k}\\lambda\\left(\\sum_{i=1}^{K}\\pi_i-1\\right) \\\\ \u0026amp;=\\lambda+\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\nabla_{\\pi_k}\\log\\pi_k \\\\ \u0026amp;=\\lambda+\\frac{\\sum_{n=1}^{N}(\\mathbf{t}_n)_k}{\\pi_k} \\end{align} Setting the derivative equal to zero and solve for $\\pi_k$, we have \\begin{equation} \\pi_k=-\\frac{\\sum_{n=1}^{N}(\\mathbf{t}_n)_k}{\\lambda}=\\frac{N_k}{\\lambda}, \\end{equation} where $N_k$ denotes the number of data points in class $\\mathcal{C}_k$. Moreover, since $\\sum_{k=1}^{K}\\pi_k=1$, we have \\begin{equation} 1=-\\sum_{k=1}^{K}\\frac{N_k}{\\lambda}=\\frac{-N}{\\lambda}, \\end{equation} which implies that \\begin{equation} \\lambda=-N \\end{equation} Hence, the maximum likelihood solution for $\\pi_k$ is \\begin{equation} \\pi_k=-\\frac{N_k}{\\lambda}=\\frac{N_k}{N} \\end{equation} We continue by taking the gradient of the log likelihood \\eqref{eq:gc.1} w.r.t $\\boldsymbol{\\mu}_k$, as \\begin{align} \\nabla_{\\boldsymbol{\\mu}_k}\\ell(\\mathbf{w})\u0026amp;=\\nabla_{\\boldsymbol{\\mu}_k}\\sum_{n=1}^{N}\\sum_{i=1}^{K}(\\mathbf{t}_n)_i\\Big[\\log\\pi_i+\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_i,\\boldsymbol{\\Sigma})\\Big] \\\\ \u0026amp;=\\nabla_{\\boldsymbol{\\mu}_k}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_{\\boldsymbol{\\mu}_k}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\Big[-\\frac{1}{2}(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)\\Big] \\\\ \u0026amp;=-\\frac{1}{2}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\nabla_{\\boldsymbol{\\mu}_k}\\Big[\\boldsymbol{\\mu}_k^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k-2\\boldsymbol{\\mu}_k^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\phi}_n\\Big] \\\\ \u0026amp;=\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\Big[\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}_k-\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\phi}_n\\Big] \\end{align} Setting the above gradient equal to zero and solve for $\\boldsymbol{\\mu}_k$ we obtain the solution \\begin{equation} \\boldsymbol{\\mu}_k=\\frac{1}{\\sum_{n=1}^{N}(\\mathbf{t}_n)_k}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\boldsymbol{\\phi}_n=\\frac{1}{N_k}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k\\boldsymbol{\\phi}_n, \\end{equation} which is the mean of feature vectors assigned to class $\\mathcal{C}_k$.\nFinally, consider the gradient of \\eqref{eq:gc.1} w.r.t $\\boldsymbol{\\Sigma}$, combined with the result \\eqref{eq:gbc.3} we have \\begin{align} \\nabla_\\boldsymbol{\\Sigma}\\ell(\\mathbf{w})\u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k\\Big[\\log\\pi_k+\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k\\boldsymbol{\\Sigma})\\Big] \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k\\log\\mathcal{N}(\\boldsymbol{\\phi}_n\\vert\\boldsymbol{\\mu}_k\\boldsymbol{\\Sigma}) \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\Sigma}\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k\\log\\big\\vert\\boldsymbol{\\Sigma}\\big\\vert^{-1/2}+(\\mathbf{t}_n)_k\\Big[-\\frac{1}{2}(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)\\Big] \\\\ \u0026amp;=-\\frac{N}{2}\\boldsymbol{\\Sigma}^{-1}+\\frac{1}{2}\\boldsymbol{\\Sigma}^{-1}\\Big[\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)^\\text{T}\\Big]\\boldsymbol{\\Sigma}^{-1} \\\\ \u0026amp;\\propto N\\boldsymbol{\\Sigma}^{-1}-\\boldsymbol{\\Sigma}^{-1}\\Big[\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)^\\text{T}\\Big]\\boldsymbol{\\Sigma}^{-1}\\label{eq:gc.2} \\end{align} Let $\\mathbf{S}_k$ be the covariance of the data associated with class $\\mathcal{C}_k$, defined as \\begin{equation} \\mathbf{S}_k=\\frac{1}{N_k}\\sum_{n=1}^{N}(\\mathbf{t}_n)_k(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)(\\boldsymbol{\\phi}_n-\\boldsymbol{\\mu}_k)^\\text{T} \\end{equation} Therefore, letting the derivative \\eqref{eq:gc.2} equal to zero, we have \\begin{equation} N\\boldsymbol{\\Sigma}^{-1}-\\boldsymbol{\\Sigma}^{-1}\\Big[\\sum_{k=1}^{K}N_k\\mathbf{S}_k\\Big]\\boldsymbol{\\Sigma}^{-1}=0 \\end{equation} Solving this equation for $\\boldsymbol{\\Sigma}$, we obtain the solution \\begin{equation} \\boldsymbol{\\Sigma}=\\sum_{k=1}^{K}\\frac{N_k}{N}\\mathbf{S}_k \\end{equation}\nProbabilistic Discriminative Models Logistic Regression Recall that in the previous section of generative approach, in particular for the binary case we knew that the posterior probability for class $\\mathcal{C}_1$ can be defined as the logistic sigmoid of a linear function of the input vector $\\mathbf{x}$ \\begin{equation} p(\\mathcal{C}_1\\vert\\mathbf{x})=\\sigma\\big(\\mathbf{w}^\\text{T}\\mathbf{x}+w_0\\big) \\end{equation} In general, the posterior probabilities can be written as the logistic sigmoid of a linear function of instead feature vector $\\boldsymbol{\\phi}$, as \\begin{equation} p(\\mathcal{C}_1\\vert\\boldsymbol{\\phi})=y(\\boldsymbol{\\phi})=\\sigma\\big(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}+w_0\\big) \\end{equation} This model is called logistic regression, although it is applied for classification tasks. Consider a data set $\\{\\boldsymbol{\\phi}_n,t_n\\}$, where $\\boldsymbol{\\phi}_n=\\boldsymbol{\\phi}(\\mathbf{x}_n)$ and $t_n\\in\\{0,1\\}$, with $n=1,\\ldots,N$. Therefore, \\begin{equation} p(t_n\\vert\\mathbf{w})=y_n^{t_n}(1-y_n)^{1-t_n}, \\end{equation} where $y_n=p(\\mathcal{C}_1\\vert\\boldsymbol{\\phi}_n)$.\nComprise $t_n$\u0026rsquo;s into $\\mathbf{t}\\doteq(t_1,\\ldots,t_N)^\\text{T}$, then we have that the likelihood function can be defined as \\begin{equation} L(\\mathbf{w})=p(\\mathbf{t}\\vert\\mathbf{w})=\\prod_{n=1}^{N}p(t_n\\vert\\mathbf{w})=\\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}\\tag{35}\\label{eq:lr.1} \\end{equation} Taking the negative logarithm of the likelihood gives us the cross-entropy error function, as \\begin{align} E(\\mathbf{w})=-\\log L(\\mathbf{w})\u0026amp;=-\\log\\prod_{n=1}^{N}p(t_n\\vert\\mathbf{w})=\\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \\\\ \u0026amp;=-\\sum_{n=1}^{N}t_n\\log y_n+(1-t_n)\\log(1-y_n)\\label{eq:lr.2} \\end{align} Differentiating the error function $E(\\mathbf{w})$ w.r.t $\\mathbf{w}$ we have that \\begin{align} \\nabla_\\mathbf{w}E(\\mathbf{w})\u0026amp;=\\nabla_\\mathbf{w}-\\sum_{n=1}^{N}t_n\\log y_n+(1-t_n)\\log(1-y_n) \\\\ \u0026amp;=\\sum_{n=1}^{N}\\frac{(1-t_n)\\nabla_\\mathbf{w}y_n}{1-y_n}-\\frac{t_n\\nabla_\\mathbf{w}y_n}{y_n} \\\\ \u0026amp;=\\sum_{n=1}^{N}\\frac{(1-t_n)y_n(1-y_n)\\boldsymbol{\\phi}_n}{1-y_n}-\\frac{t_n y_n(1-y_n)\\boldsymbol{\\phi}_n}{y_n} \\\\ \u0026amp;=\\sum_{n=1}^{N}(1-t_n)y_n\\boldsymbol{\\phi}_n-t_n(1-y_n)\\boldsymbol{\\phi}_n \\\\ \u0026amp;=\\sum_{n=1}^{N}(y_n-t_n)\\boldsymbol{\\phi}_n,\\label{eq:lr.3} \\end{align} where in the third step, we have used the identity of the derivative of the logistic sigmoid function \\begin{equation} \\frac{d\\sigma}{d a}=\\sigma(1-\\sigma) \\end{equation} and the chain rule to compute the gradient of $y_n$ w.r.t $\\mathbf{w}$ as \\begin{align} \\nabla_\\mathbf{w}y_n\u0026amp;=\\nabla_\\mathbf{w}\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0) \\\\ \u0026amp;=\\frac{d\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0)}{d(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0)}\\nabla_\\mathbf{w}(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0) \\\\ \u0026amp;=\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0)\\big(1-\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n+w_0)\\big)\\boldsymbol{\\phi}_n \\\\ \u0026amp;=y_n(1-y_n)\\boldsymbol{\\phi}_n \\end{align}\nSoftmax Regression Analogy to the generalization of the binary case into logistic regression, for the multi-class case, the posterior probability for class $\\mathcal{C}_k$ can be written as the softmax function of a linear function of feature vectors $\\boldsymbol{\\phi}$ as \\begin{equation} p(\\mathcal{C}_k\\vert\\boldsymbol{\\phi})=y_k(\\boldsymbol{\\phi})=\\frac{\\exp(a_k)}{\\sum_{i=1}^{K}\\exp(a_i)}, \\end{equation} where $a_k$\u0026rsquo;s is called the activations, defined as \\begin{equation} a_k=\\mathbf{w}_k^\\text{T}\\boldsymbol{\\phi} \\end{equation} Given a data set $\\{\\boldsymbol{\\phi}_n,\\mathbf{t}_n\\}$ for $n=1,\\ldots,N$ where $\\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\\mathbf{t}_n)_k=1$ denotes class $\\mathcal{C}_k$ and $(\\mathbf{t}_n)_i=0$ for all $i\\neq k$. Similar to the binary case, we also have that \\begin{equation} p(\\mathbf{t}_n\\vert\\mathbf{w}_1,\\ldots,\\mathbf{w}_K)=\\prod_{k=1}^{K}p(\\mathcal{C}_k\\vert\\boldsymbol{\\phi}_n)^{(\\mathbf{t}_n)_k}=\\prod_{k=1}^{K}(y_{n})_k^{(\\mathbf{t}_n)_k}, \\end{equation} where $(y_{n})_k=y_k(\\boldsymbol{\\phi}_n)$.\nLet $\\mathbf{T}$ be a $N\\times K$ matrix comprising $\\mathbf{t}_n$\u0026rsquo;s together as \\begin{equation} \\mathbf{T}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{t}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{t}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right] \\end{equation} Therefore, the likelihood function can be written by \\begin{align} L(\\mathbf{w}_1,\\ldots,\\mathbf{w}_K)\u0026amp;=p(\\mathbf{T}\\vert\\mathbf{w}_1,\\ldots,\\mathbf{w}_K) \\\\ \u0026amp;=\\prod_{n=1}^{N}p(\\mathbf{t}_n\\vert\\mathbf{w}_1,\\ldots,\\mathbf{w}_K) \\\\ \u0026amp;=\\prod_{n=1}^{N}\\prod_{k=1}^{K}(y_{n})_k^{(\\mathbf{t}_n)_k} \\end{align} We also obtain the cross-entropy error function by taking the negative logarithm of the likelihood, as \\begin{align} E(\\mathbf{w}_1,\\ldots,\\mathbf{w}_K)\u0026amp;=-\\log L(\\mathbf{w}_1,\\ldots,\\mathbf{w}_K) \\\\ \u0026amp;=-\\log\\prod_{n=1}^{N}\\prod_{k=1}^{K}(y_{n})_k^{(\\mathbf{t}_n)_k} \\\\ \u0026amp;=-\\sum_{n=1}^{N}\\sum_{k=1}^{K}(\\mathbf{t}_n)_k\\log(y_{n})_k\\label{eq:sr.1} \\end{align} As usual, taking the gradient of the error function $E(\\mathbf{w}_1,\\ldots,\\mathbf{w}_K)$ w.r.t $\\mathbf{w}_k$ we have \\begin{align} \\nabla_{\\mathbf{w}_k}E(\\mathbf{w}_1,\\ldots,\\mathbf{w}_K)\u0026amp;=\\nabla_{\\mathbf{w}_k}-\\sum_{n=1}^{N}\\sum_{i=1}^{K}(\\mathbf{t}_n)_i\\log(y_{n})_i \\\\ \u0026amp;=-\\sum_{n=1}^{N}\\sum_{i=1}^{K}(\\mathbf{t}_n)_i\\frac{(y_n)_i(1\\{i=k\\}-(y_n)_k)\\boldsymbol{\\phi}_n}{(y_n)_i} \\\\ \u0026amp;=\\sum_{n=1}^{N}\\Big[(y_n)_k\\sum_{i=1}^{K}(\\mathbf{t}_n)_i-\\sum_{i=1}^{K}(\\mathbf{t}_n)_i 1\\{i=k\\}\\Big]\\boldsymbol{\\phi}_n \\\\ \u0026amp;=\\sum_{n=1}^{N}\\big[(y_n)_k-(\\mathbf{t}_n)_k\\big]\\boldsymbol{\\phi}_n\\label{eq:sr.2} \\end{align} where in the second step, we have used the identity \\begin{align} \\frac{\\partial y_k}{\\partial a_j}\u0026amp;=\\frac{\\big(\\partial\\exp(a_k)/\\partial\\exp(a_j)\\big)\\sum_{i=1}^{K}\\exp(a_i)-\\exp(a_j)\\exp(a_k)}{\\big(\\sum_{i=1}^{K}\\exp(a_i)\\big)^2} \\\\ \u0026amp;=\\frac{\\exp(a_k)1\\{k=j\\}}{\\sum_{i=1}^{K}\\exp(a_i)}-y_k y_j \\\\ \u0026amp;=y_k(1\\{k=j\\}-y_j) \\end{align} where $1\\{k=j\\}$ is the indicator function, which returns $1$ if $k=j$ and returns $0$ otherwise. Hence, by chain rule, we obtain the gradient of $(y_n)_i$ w.r.t $\\mathbf{w}_k$ given by \\begin{align} \\nabla_{\\mathbf{w}_k}(y_n)_i\u0026amp;=\\frac{\\partial(y_n)_i}{\\partial a_k}\\frac{\\partial a_k(\\mathbf{w}_k,\\boldsymbol{\\phi}_n)}{\\partial\\mathbf{w}_k} \\\\ \u0026amp;=(y_n)_i(1\\{i=k\\}-(y_n)_k)\\boldsymbol{\\phi}_n \\end{align}\nNewton\u0026rsquo;s method Figure 5: Illustration of the Newton's method. The code can be found here \\begin{equation} \\mathbf{w}^{(\\text{new})}=\\mathbf{w}^{(\\text{old})}-\\mathbf{H}^{-1}\\nabla_\\mathbf{w}E(\\mathbf{w}) \\end{equation}\nLinear Regression Consider applying the Newton\u0026rsquo;s method to the sum-of-squares error function \\eqref{eq:lsr.4} for the linear regression model \\eqref{eq:lbfm.2}. The gradient and Hessian of this error function are \\begin{align} \\nabla_\\mathbf{w}E(\\mathbf{w})\u0026amp;=\\nabla_\\mathbf{w}\\sum_{n=1}^{N}\\left(t_n-\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n)\\right)^2 \\\\ \u0026amp;=\\sum_{n=1}^{N}(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n-t_n)\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\mathbf{w}-\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}, \\end{align} and \\begin{equation} \\mathbf{H}=\\nabla_\\mathbf{w}\\nabla_\\mathbf{w}E(\\mathbf{w})=\\nabla_\\mathbf{w}\\big(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\mathbf{w}-\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}\\big)=\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}, \\end{equation} where $\\boldsymbol{\\Phi}$, as defined before, is the $N\\times M$ design matrix \\begin{equation} \\boldsymbol{\\Phi}=\\left[\\begin{matrix}-\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_1)^\\text{T}\\hspace{0.1cm}- \\\\ \\hspace{0.1cm}\\vdots\\hspace{0.1cm} \\\\ -\\hspace{0.1cm}\\boldsymbol{\\phi}(\\mathbf{x}_N)^\\text{T}\\hspace{0.1cm}-\\end{matrix}\\right]=\\left[\\begin{matrix}\\phi_0(\\mathbf{x}_1)\u0026amp;\\ldots\u0026amp;\\phi_{M-1}(\\mathbf{x}_1) \\\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots \\\\ \\phi_0(\\mathbf{x}_N)\u0026amp;\\ldots\u0026amp;\\phi_{M-1}(\\mathbf{x}_N)\\end{matrix}\\right], \\end{equation} Hence, we have that the Newton\u0026rsquo;s update of the model is given by \\begin{align} \\mathbf{w}^{(\\text{new})}\u0026amp;=\\mathbf{w}^{(\\text{old})}-(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\\big(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi}\\mathbf{w}^{(\\text{old})}-\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}\\big) \\\\ \u0026amp;=(\\boldsymbol{\\Phi}^\\text{T}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{t}, \\end{align} which is exactly the standard least-squares solution.\nLogistic Regression Consider using the Newton\u0026rsquo;s method to the logistic regression model with the cross-entropy error function \\eqref{eq:lr.2}. By the result \\eqref{eq:lr.3}, we have the gradient and Hessian of this error function are given as \\begin{equation} \\nabla_\\mathbf{w}E(\\mathbf{w})=\\sum_{n=1}^{N}(y_n-t_n)\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}(\\mathbf{y}-\\mathbf{t}) \\end{equation} and \\begin{align} \\mathbf{H}=\\nabla_\\mathbf{w}\\nabla_\\mathbf{w}E(\\mathbf{w})\u0026amp;=\\nabla_\\mathbf{w}\\sum_{n=1}^{N}(y_n-t_n)\\boldsymbol{\\phi}_n \\\\ \u0026amp;=\\sum_{n=1}^{N}y_n(1-y_n)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\label{eq:nlr.1} \\\\ \u0026amp;=\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi}, \\end{align} where $\\mathbf{R}$ is the $N\\times N$ diagonal matrix with diagonal elements \\begin{equation} \\mathbf{R}_{n n}=y_n(1-y_n) \\end{equation} It is noticeable that hessian matrix $\\mathbf{H}$ is positive definite because for any vector $\\mathbf{v}$ \\begin{equation} \\mathbf{v}^\\text{T}\\mathbf{H}\\mathbf{v}=\\mathbf{v}^\\text{T}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi}\\mathbf{v}\u0026gt;0, \\end{equation} since $\\mathbf{R}$ is positive definite due to $y_n\\in(0,1)$ letting all the diagonal elements of $\\mathbf{R}$ are positive. This positive definiteness claims that the cross-entropy error function is a concave function of $\\mathbf{w}$ and thus has a unique minimum.\nBack to our main attention, the Newton\u0026rsquo;s update of the model then takes the form \\begin{align} \\mathbf{w}^{(\\text{new})}\u0026amp;=\\mathbf{w}^{(\\text{old})}-(\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}(\\mathbf{y}-\\mathbf{t}) \\\\ \u0026amp;=(\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi})^{-1}\\Big[\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi}\\mathbf{w}^{(\\text{old})}-\\boldsymbol{\\Phi}^\\text{T}(\\mathbf{y}-\\mathbf{t})\\Big] \\\\ \u0026amp;=(\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\boldsymbol{\\Phi})^{-1}\\boldsymbol{\\Phi}^\\text{T}\\mathbf{R}\\mathbf{z}, \\end{align} where $\\mathbf{z}$ is an $N$-dimensional vector given by \\begin{equation} \\mathbf{z}=\\boldsymbol{\\Phi}\\mathbf{w}^{(\\text{old})}-\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{t}) \\end{equation} This algorithm is known as iterative reweighted least squares, or IRLS.\nSoftmax Regression Consider applying the Newton\u0026rsquo;s method to the cross-entropy error function \\eqref{eq:sr.1} for the softmax regression model.\nFirst, let $\\mathbf{W}$ be the $M\\times K$ matrix that comprises $\\mathbf{w}_1,\\ldots,\\mathbf{w}_K$ together, as \\begin{equation} \\mathbf{W}=\\left[\\begin{matrix}\\vert\u0026amp;\u0026amp;\\vert \\\\ \\mathbf{w}_1\u0026amp;\\ldots\u0026amp;\\mathbf{w}_K \\\\ \\vert\u0026amp;\u0026amp;\\vert\\end{matrix}\\right] \\end{equation} By the result \\eqref{eq:sr.2}, we have that the $k$-th column of the gradient of this error function is given by \\begin{equation} \\nabla_{\\mathbf{w}_k}E(\\mathbf{W})=\\sum_{n=1}^{N}\\big[(y_n)_k-(\\mathbf{t}_n)_k\\big]\\boldsymbol{\\phi}_n=\\boldsymbol{\\Phi}^\\text{T}(\\mathbf{Y}_k-\\mathbf{T}_k), \\end{equation} where $\\boldsymbol{\\Phi}$ be the $N\\times M$ design matrix, given as \\begin{equation} \\boldsymbol{\\Phi}=\\left[\\begin{matrix}-\\hspace{0.1cm}\\boldsymbol{\\phi}_1^\\text{T}\\hspace{0.1cm}- \\\\ \\hspace{0.1cm}\\vdots\\hspace{0.1cm} \\\\ -\\hspace{0.1cm}\\boldsymbol{\\phi}_N^\\text{T}\\hspace{0.1cm}-\\end{matrix}\\right] \\end{equation} and where $\\mathbf{Y}_k,\\mathbf{T}_k$ are the $k$th columns of the $N\\times K$ matrices \\begin{equation} \\mathbf{Y}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{y}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{y}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right];\\hspace{2cm}\\mathbf{T}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{t}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{t}_N^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right] \\end{equation} Therefore, the gradient of the error function w.r.t $\\mathbf{W}$ can be written as \\begin{equation} \\nabla_\\mathbf{W}E(\\mathbf{W})=\\boldsymbol{\\Phi}^\\text{T}(\\mathbf{Y}-\\mathbf{T}) \\end{equation} Now we consider the hessian matrix $\\mathbf{H}$ of the error function, whose block $(k,j)$ is given by \\begin{align} \\mathbf{H}_{k j}\u0026amp;=\\nabla_{\\mathbf{w}_j}\\nabla_{\\mathbf{w}_k} E(\\mathbf{W}) \\\\ \u0026amp;=\\nabla_{\\mathbf{w}_j}\\sum_{n=1}^{N}\\big[(y_n)_k-(\\mathbf{t}_n)_k\\big]\\boldsymbol{\\phi}_n \\\\ \u0026amp;=\\sum_{n=1}^{N}(y_n)_k\\big(1\\{j=k\\}-(y_n)_j\\big)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T} \\end{align} Analogous to the binary case, the hessian $\\mathbf{H}$ for the multi-class logistic regression model is positive semi-definite. To prove it, since $\\mathbf{H}$ is an $MK\\times MK$ matrix, consider an $MK$-dimensional vector $\\mathbf{u}$. Thus, $\\mathbf{u}$ can be represented as \\begin{equation} \\mathbf{u}=\\left[\\begin{matrix}\\mathbf{u}_1^\\text{T}\u0026amp;\\ldots\u0026amp;\\mathbf{u}_K^\\text{T}\\end{matrix}\\right]^\\text{T}, \\end{equation} where each $\\mathbf{u}_k$ is a vector of length $M$, for $k=1,\\ldots,K$. Therefore, we have \\begin{align} \\mathbf{u}^\\text{T}\\mathbf{H}\\mathbf{u}\u0026amp;=\\sum_{k=1}^{K}\\sum_{j=1}^{K}\\mathbf{u}_k^\\text{T}\\mathbf{H}_{k j}\\mathbf{u}_j \\\\ \u0026amp;=\\sum_{k=1}^{K}\\sum_{j=1}^{K}\\mathbf{u}_k^\\text{T}\\sum_{n=1}^{N}(y_n)_k\\big(1\\{j=k\\}-(y_n)_j\\big)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{u}_j \\\\ \u0026amp;=\\sum_{n=1}^{N}\\left[\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{u}_k-\\sum_{k=1}^{K}\\sum_{j=1}^{K}(y_n)_k(y_n)_j\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{u}_j\\right] \\\\ \u0026amp;=\\sum_{n=1}^{N}\\left[\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{u}_k-\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\sum_{j=1}^{K}(y_n)_j\\mathbf{u}_j\\right]\\label{eq:nsr.1} \\end{align} Consider $f:\\mathbb{R}^M\\to\\mathbb{R}$, defined as \\begin{equation} f(\\mathbf{x})=\\mathbf{x}^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{x} \\end{equation} Thus, it follows immediately from the definition of $f$ that $f$ is convex since \\begin{equation} f(\\mathbf{x})=\\mathbf{x}^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{x}=\\Vert\\mathbf{x}^\\text{T}\\boldsymbol{\\phi}_n\\Vert_2^2\\geq 0 \\end{equation} Let us apply Jensen\u0026rsquo;s inequality2 for $f$ with observing that $\\sum_{k=1}^{K}(y_n)_k=\\sum_{j=1}^{K}(y_n)_j=1$, then \\eqref{eq:nsr.1} can be continued to derive as \\begin{align} \\mathbf{u}^\\text{T}\\mathbf{H}\\mathbf{u}\u0026amp;=\\sum_{n=1}^{N}\\left[\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\mathbf{u}_k-\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k^\\text{T}\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}\\sum_{j=1}^{K}(y_n)_j\\mathbf{u}_j\\right] \\\\ \u0026amp;=\\sum_{n=1}^{N}\\left[\\sum_{k=1}^{K}(y_n)_k f\\left(\\mathbf{u}_k\\right)-f\\left(\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k\\right)\\right] \\\\ \u0026amp;\\geq\\sum_{n=1}^{N}\\left[f\\left(\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k\\right)-f\\left(\\sum_{k=1}^{K}(y_n)_k\\mathbf{u}_k\\right)\\right] \\\\ \u0026amp;=0, \\end{align} which claims the positive semi-definiteness of $\\mathbf{H}$. Therefore, the error function $E(\\mathbf{w})$ is concave and thus has a unique minimum.\nBayesian Logistic Regression When using Bayesian approach for logistic regression model, unlike the case of linear regression \\eqref{eq:pd.1}, the posterior distribution now is no longer Gaussian. This makes the evaluation of posterior be intractable when integrating over the parameter $\\mathbf{w}$.\nTherefore, it is necessary to use some approximation methods.\nThe Laplace approximation The goal of Laplace approximation is to fit a Gaussian distribution to a probability density defined over a set of continuous variables\nWe begin by consider applying Laplace method to one-dimensional variables $z$ with the density function $p(z)$ is defined as \\begin{equation} p(z)=\\frac{1}{Z}f(z), \\end{equation} where $Z=\\int f(z)\\hspace{0.1cm}dz$ is the normalization coefficient, and is unknown.\nThe idea behind Laplace method is to place a Gaussian $q(z)$ on a mode of the distribution $p(z)$. A mode $z_0$ of $p(z)$ is where the distribution reaches its global maximum, which also means the derivative of $p(z)$ at $z_0$ is zero \\begin{equation} \\left.\\frac{d f(z)}{dz}\\right\\vert_{z=z_0}=0 \\end{equation} Therefore, the Taylor expansion of $\\log f(z)$ about $z=z_0$ can be written by \\begin{align} \\log f(z)\u0026amp;\\simeq\\log f(z_0)+\\log f(z)\\left.\\frac{d f(z)}{dz}\\right\\vert_{z=z_0}(z-z_0)+\\frac{1}{2}\\left.\\frac{d^2\\log f(z)}{d^2 z}\\right\\vert_{z=z_0}(z-z_0)^2 \\\\ \u0026amp;=\\log f(z_0)-\\frac{A}{2}(z-z_0)^2, \\end{align} where \\begin{equation} A=-\\left.\\frac{d^2\\log f(z)}{d^2 z}\\right\\vert_{z=z_0} \\end{equation} Thus, taking the exponential gives us \\begin{equation} f(z)\\simeq f(z_0)\\exp\\left(-\\frac{A}{2}(z-z_0)^2\\right), \\end{equation} which is in a form of an unnormalized Gaussian distribution. Hence, we can obtain a Gaussian approximation $q(z)$ of $p(z)$ by adding a normalization parameter to form a Normal distribution, as \\begin{equation} q(z)=\\left(\\frac{A}{2\\pi}\\right)^{1/2}\\exp\\left(-\\frac{A}{2}(z-z_0)^2\\right)=\\mathcal{N}(\\mathbf{z}\\vert z_0,A^{-1}) \\end{equation} We can extend the Laplace approximation into multi-dimensional variable $\\mathbf{z}$, which is finding an Gaussian approximation of distribution \\begin{equation} p(\\mathbf{z})=\\frac{1}{Z}f(\\mathbf{z}), \\end{equation} where $z$ is a vector of length $M\\geq 2$.\nAnalogy to the univariate case, the first step is to consider the Taylor expansion of $\\log f(\\mathbf{z})$ about its stationary point $\\mathbf{z}_0$, which means $\\nabla_\\mathbf{z}f(\\mathbf{z})\\vert_{\\mathbf{z}=\\mathbf{z}_0}=0$. We have \\begin{align} \\log f(\\mathbf{z})\u0026amp;\\simeq f(\\mathbf{z}_0)+\\log f(\\mathbf{z})\\nabla_\\mathbf{z}f(\\mathbf{z})\\vert_{\\mathbf{z}=\\mathbf{z}_0}+\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_0)^\\text{T}\\nabla_\\mathbf{z}\\nabla_\\mathbf{z}\\log f(\\mathbf{z})\\vert_{\\mathbf{z}=\\mathbf{z}_0}(\\mathbf{z}-\\mathbf{z}_0) \\\\ \u0026amp;=\\log f(\\mathbf{z}_0)-\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_0)^\\text{T}\\mathbf{A}(\\mathbf{z}-\\mathbf{z}_0), \\end{align} where \\begin{equation} \\mathbf{A}=-\\nabla_\\mathbf{z}\\nabla_\\mathbf{z}\\log f(z)\\vert_{\\mathbf{z}=\\mathbf{z}_0} \\end{equation} Taking the exponentials of both sides lets us obtain \\begin{equation} f(\\mathbf{z})\\simeq f(\\mathbf{z}_0)\\exp\\left(-\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_0)^\\text{T}\\mathbf{A}(\\mathbf{z}-\\mathbf{z}_0)\\right), \\end{equation} which is in form of an unnormalized multivariate Gaussian. Adding a normalization parameter gives us the Gaussian approximation $q(\\mathbf{z})$ of $p(\\mathbf{z})$ \\begin{equation} q(\\mathbf{z})=\\frac{\\vert\\mathbf{A}\\vert^{1/2}}{(2\\pi)^{M/2}}\\exp\\left(-\\frac{1}{2}(\\mathbf{z}-\\mathbf{z}_0)^\\text{T}\\mathbf{A}(\\mathbf{z}-\\mathbf{z}_0)\\right)=\\mathcal{N}(\\mathbf{z}\\vert\\mathbf{z}_0,\\mathbf{A}^{-1}) \\end{equation}\nApproximation of the posterior Consider the prior to be a Gaussian, which is \\begin{equation} p(\\mathbf{w})=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{m}_0,\\mathbf{S}_0), \\end{equation} where $\\mathbf{m}_0$ and $\\mathbf{S}_0$ are known. Along with this is the likelihood function, which is defined by \\eqref{eq:lr.1}, as \\begin{equation} p(\\mathbf{t}\\vert\\mathbf{w})=\\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}, \\end{equation} where $\\mathbf{t}=(t_1,\\ldots,t_N)^\\text{T}$, and $y_n=\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi}_n)$. Therefore, by Bayes\u0026rsquo; theorem, the posterior is given by \\begin{equation} p(\\mathbf{w}\\vert\\mathbf{t})\\propto p(\\mathbf{w})p(\\mathbf{t}\\vert\\mathbf{w}), \\end{equation} Taking the natural logarithm of both sides gives us \\begin{align} \\log p(\\mathbf{w}\\vert\\mathbf{t})\u0026amp;=-\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_0)^\\text{T}\\mathbf{S}_0^{-1}(\\mathbf{w}-\\mathbf{m}_0)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\sum_{n=1}^{N}\\big[t_n\\log y_n+(1-t_n)\\log(1-y_n)\\big]+c, \\end{align} where $c$ is independent of $\\mathbf{w}$.\nBy Laplace approximation, to find a Gaussian approximation of the posterior, the first step is looking for the point which maximizes the posterior, which is the $\\mathbf{w}_\\text{MAP}$. This point also defines the mean of the approximation. The corresponding covariance matrix $\\mathbf{S}_N$ of the Gaussian is given by \\begin{align} \\mathbf{S}_N\u0026amp;=-\\nabla_\\mathbf{w}\\nabla_\\mathbf{w}\\log p(\\mathbf{w}\\vert\\mathbf{t}) \\\\ \u0026amp;=\\mathbf{S}_0^{-1}+\\sum_{n=1}^{N}y_n(1-y_n)\\boldsymbol{\\phi}_n\\boldsymbol{\\phi}_n^\\text{T}, \\end{align} where the second step is obtained by using the result \\eqref{eq:nlr.1}. Therefore, the Gaussian approximation $q(\\mathbf{w})$ for the posterior distribution is given by \\begin{equation} q(\\mathbf{w})=\\mathcal{N}(\\mathbf{w}\\vert\\mathbf{w}_\\text{MAP},\\mathbf{S}_N)\\label{eq:ap.1} \\end{equation}\nPredictive distribution With the Gaussian approximation \\eqref{eq:ap.1}, the predict distribution for class $\\mathcal{C}_1$, given a new feature vector $\\boldsymbol{\\phi}(\\mathbf{x})$, is then given by marginalizing w.r.t the posterior distribution $p(\\mathbf{w}\\vert\\mathbf{t})$, as \\begin{equation} p(\\mathcal{C}_1\\vert\\boldsymbol{\\phi},\\mathbf{t})=\\int p(\\mathcal{C}_1\\vert\\boldsymbol{\\phi}\\mathbf{w})p(\\mathbf{w}\\vert\\mathbf{t})\\hspace{0.1cm}d\\mathbf{w}\\simeq\\int\\sigma(\\mathbf{w}^\\text{T}\\boldsymbol{\\phi})q(\\mathbf{w})\\hspace{0.1cm}d\\mathbf{w} \\end{equation} And thus, the predictive distribution for class $\\mathcal{C}_2$ is given by \\begin{equation} p(\\mathcal{C}_2\\vert\\boldsymbol{\\phi},\\mathbf{t})=1-p(\\mathcal{C}_1\\vert\\boldsymbol{\\phi},\\mathbf{t}) \\end{equation}\nGeneralized linear models References [1] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer New York, NY, 2006.\n[2] Gilbert Strang. Introduction to Linear Algebra, 5th edition, 2016.\n[3] MIT 18.06. Linear Algebra.\n[4] MIT 18.02. Multivariable Calculus.\n[5] amoeba. What is an isotropic (spherical) covariance matrix?. Cross Validated.\nFootnotes A covariance matrix $\\mathbf{C}$ is isotropic (or spherical) if it is proportional to the identity matrix $\\mathbf{I}$ \\begin{equation*} \\mathbf{C}=\\lambda\\mathbf{I}, \\end{equation*} where $\\lambda\\in\\mathbb{R}$ is a constant.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor positive numbers $p_1,\\ldots,p_n$ such that $\\sum_{i=1}^{n}p_i=1$ and $f$ is a continuous function, if $f$ is convex, then \\begin{equation*} f\\left(\\sum_{i=1}^{n}p_ix_i\\right)\\leq\\sum_{i=1}^{n}p_if(x_i), \\end{equation*} and if $f$ is concave, we instead have \\begin{equation*} f\\left(\\sum_{i=1}^{n}p_ix_i\\right)\\geq\\sum_{i=1}^{n}p_if(x_i), \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/machine-learning/glm/","summary":"\u003cblockquote\u003e\n\u003cp\u003eLinear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Generalized Linear Models"},{"content":" Note II of the measure theory series. Materials are mostly taken from Tao\u0026rsquo;s book, except for some needed notations extracted from Stein\u0026rsquo;s book.\nLebesgue measure Recall that the Jordan outer measure of a set $E\\subset\\mathbb{R}^d$ has been defined as \\begin{equation} m^{*,(J)}(E)\\doteq\\inf_{B\\supset E;B\\text{ elementary}}m(B) \\end{equation} From the finite additivity and subadditivity of elementary measure, we can also write the Jordan outer measure as \\begin{equation} m^{*,(J)}(E)\\doteq\\inf_{B_1\\cup\\dots\\cup B_k\\supset E;B_1,\\dots,B_k\\text{ boxes}}\\vert B_1\\vert+\\dots+\\vert B_k\\vert, \\end{equation} which means the Jordan outer measure is the infimal cost required to cover $E$ by a finite union of boxes. By replacing the finite union of boxes by a countable union of boxes, we obtain the Lebesgue outer measure $m^{*}(E)$ of $E$: \\begin{equation} m^{*}(E)\\doteq\\inf_{\\bigcup_{n=1}^{\\infty}B_n\\supset E;B_1,B_2,\\dots\\text{ boxes}}\\sum_{n=1}^{\\infty}\\vert B_n\\vert, \\end{equation} which is be seen as the infimal cost required to cover $E$ by a countable union of boxes.\nA set $E\\subset\\mathbb{R}^d$ is said to be Lebesgue measurable if, for every $\\varepsilon\u0026gt;0$, there exists an open set $U\\subset\\mathbb{R}^d$ containing $E$ such that $m^{*}(U\\backslash E)\\leq\\varepsilon$. If $E$ is Lebesgue measurable, we refer to \\begin{equation} m(E)\\doteq m^{*}(E) \\end{equation} as the Lebesgue measure of $E$.\nProperties of Lebesgue outer measure Remark 1. (The outer measure axioms)\nEmpty set. $m^*(\\emptyset)=0$. Monotonicity. If $E\\subset F\\subset\\mathbb{R}^d$, then $m^*(E)\\leq m^*(F)$. Countable subadditivity. If $E_1,E_2,\\ldots\\subset\\mathbb{R}^d$ is a countable sequence of sets, then $m^*\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\leq\\sum_{n=1}^{\\infty}m^*(E_n)$. Proof\nThis follows from the definition of Lebesgue outer measure. Since $E\\subset F\\subset\\mathbb{R}^d$, then any set containing $F$ also includes $E$, but not every set having $E$ contains $F$. That means \\begin{equation} \\left\\{\\sum_{n=1}^{\\infty}\\vert B_n\\vert:E\\subset\\bigcup_{n=1}^{\\infty}B_n;B_n\\text{ boxes}\\right\\}\\supset\\left\\{\\sum_{n=1}^{\\infty}\\vert B_n\\vert:F\\subset\\bigcup_{n=1}^{\\infty}B_n;B_n\\text{ boxes}\\right\\} \\end{equation} Thus, \\begin{equation} \\inf\\left\\{\\sum_{n=1}^{\\infty}\\vert B_n\\vert:E\\subset\\bigcup_{n=1}^{\\infty}B_n;B_n\\text{ boxes}\\right\\}\\leq\\inf\\left\\{\\sum_{n=1}^{\\infty}\\vert B_n\\vert:F\\subset\\bigcup_{n=1}^{\\infty}B_n;B_n\\text{ boxes}\\right\\} \\end{equation} or \\begin{equation} m^*(E)\u003c m^*(F) \\end{equation} By the definition of Lebesgue outer measure, for any positive integer $i$, we have \\begin{equation} m^*(E_i)=\\inf_{\\bigcup_{n=1}^{\\infty}B_n\\supset E_i;B_1,B_2,\\ldots\\text{ boxes}}\\sum_{n=1}^{\\infty}\\vert B_n\\vert \\end{equation} Thus, by definition of infimum and by axiom of countable choice, for each $E_i$ in the sequence $(E_n)_{n\\in\\mathbb{N}}$, there exists a family of boxes $B_{i,1},B_{i,2},\\ldots$ in the doubly sequence $(B_{i,j})_{(i,j)\\in\\mathbb{N}^2}$ covering $E_i$ such that \\begin{equation} \\sum_{j=1}^{\\infty}\\vert B_{i,j}\\vert\\lt m^*(E_i)+\\frac{\\varepsilon}{i}, \\end{equation} for any $\\varepsilon\u003e0$, and for $i=1,2,\\ldots$. Plus, we also have \\begin{equation} \\bigcup_{n=1}^{\\infty}E_n\\subset\\bigcup_{i=1}^{\\infty}\\bigcup_{j=1}^{\\infty}B_{i,j} \\end{equation} Moreover, by the Tonelli's theorem for series, we have \\begin{equation} \\bigcup_{i=1}^{\\infty}\\bigcup_{j=1}^{\\infty}B_{i,j}=\\bigcup_{(i,j)\\in\\mathbb{N}^2}B_{i,j} \\end{equation} Therefore once again, by definition of outer measure and definition of infimum, we obtain \\begin{align} m^*\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\u0026=\\inf_{\\bigcup_{(i,j)\\in\\mathbb{N}^2}B_{i,j}}\\sum_{i=1}^{\\infty}\\sum_{j=1}^{\\infty}\\vert B_{i,j}\\vert\\leq\\sum_{i=1}^{\\infty}\\sum_{j=1}^{\\infty}\\vert B_{i,j}\\vert \\\\\\\\ \u0026\\lt\\sum_{i=1}^{\\infty}m^*(E_i)+\\frac{\\varepsilon}{2^i}=\\sum_{i=1}^{\\infty}m^*(E_i)+\\varepsilon \\end{align} And since $\\varepsilon\u003e0$ was arbitrary, we can conclude that \\begin{equation} m^*\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\leq\\sum_{i=n}^{\\infty}m^*(E_n) \\end{equation} Corollary 2\nCombining empty set with countable subadditivity axiom gives us the finite subadditivity property \\begin{equation} m^{*}\\left(E_1\\cup\\ldots\\cup E_k\\right)\\leq m^{*}(E_1)+\\ldots+m^{*}(E_k),\\hspace{1cm}\\forall k\\geq 0 \\end{equation}\nFinite additivity for separated sets Lemma 3 Let $E,F\\subset\\mathbb{R}^d$ be such that $\\text{dist}(E,F)\u0026gt;0$, where \\begin{equation} \\text{dist}(E,F)\\doteq\\inf\\left\\{\\vert x-y\\vert:x\\in E,y\\in F\\right\\} \\end{equation} is the distance between $E$ and $F$. Then $m^*(E\\cup F)=m^*(E)+m^*(F)$.\nProof\nFrom subadditivity property, we have $m^*(E\\cup F)\\leq m^*(E)+m^*(F)$. Then it suffices to prove the inverse, that \\begin{equation} m^*(E\\cup F)\\geq m^*(E)+m^*(F) \\end{equation} Let $\\varepsilon\u0026gt;0$. By definition of Lebesgue outer measure, we can cover $E\\cup F$ by a countable family $B_1,B_2,\\ldots$ of boxes such that \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert\\leq m^*(E\\cup F)+\\varepsilon \\end{equation} Suppose it was the case that each box intersected at most one of $E$ and $F$. Then we could divide this family into two subfamilies $B_1\u0026rsquo;,B_2\u0026rsquo;,\\ldots$ and $B_1'',B_2'',B_3'',\\ldots$, the first of which covered $E$, while the second of which covered $F$. From definition of Lebesgue outer measure, we have \\begin{equation} m^*(E)\\leq\\sum_{n=1}^{\\infty}\\vert B_n\u0026rsquo;\\vert \\end{equation} and \\begin{equation} m^*(F)\\leq\\sum_{n=1}^{\\infty}\\vert B_n''\\vert \\end{equation} Summing up these two equation, we obtain \\begin{equation} m^*(E)+m^*(F)\\leq\\sum_{n=1}^{\\infty}\\vert B_n\\vert \\end{equation} and thus \\begin{equation} m^*(E)+m^*(F)\\leq m^*(E\\cup F)+\\varepsilon \\end{equation} Since $\\varepsilon$ was arbitrary, this gives $m^*(E)+m^*(F)\\leq m^*(E\\cup F)$ as required.\nNow we consider the case that some of the boxes $B_n$ intersect both $E$ and $F$.\nSince given any $r\u0026gt;0$, we can always partition a box $B_n$ into a finite number of smaller boxes, each of which has diameter1 at most $r$, with the total volume of these sub-boxes equal to the volume of the original box $B_n$. Therefore, given any $r\u0026gt;0$, we may assume without loss of generality that the boxes $B_1,B_2,\\ldots$ covering $E\\cup F$ have diameter at most $r$. Or in particular, we may assume that all such boxes have diameter strictly less than $\\text{dist}(E,f)$.\nOnce we do this, then it is no longer possible for any box to intersect both $E$ and $F$, which allows the previous argument be applicable.\nExample 1\nLet $E,F\\subset\\mathbb{R}^d$ be disjoint closed sets, with at least one of $E,F$ being compact. Then $\\text{dist}(E,F)\u0026gt;0$.\nProof\nOuter measure of elementary sets Lemma 4 Let $E$ be an elementary set. Then the Lebesgue outer measure of $E$ is equal to the elementary measure of $E$: \\begin{equation} m^*(E)=m(E) \\end{equation}\nProof\nSince \\begin{equation} m^*(E)\\leq m^{*,(J)}(E)=m(E), \\end{equation} then it suffices to show that \\begin{equation} m(E)\\leq m^*(E) \\end{equation} We first consider the case that $E$ is closed. Since $E$ is elementary, $E$ is also bounded, which implies that $E$ is compact.\nLet $\\varepsilon\u0026gt;0$ be arbitrary, then we can find a countable family $B_1,B_2,\\ldots$ of boxes that cover $E$ \\begin{equation} E\\subset\\bigcup_{n=1}^{\\infty}B_n, \\end{equation} such that \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert\\leq m^*(E)+\\varepsilon \\end{equation} We have that for each box $B_n$, we can find an open box $B_n\u0026rsquo;$ containing $B_n$ such that \\begin{equation} \\vert B_n\u0026rsquo;\\vert\\leq\\vert B_n\\vert+\\frac{\\varepsilon}{2^n} \\end{equation} The $B_n\u0026rsquo;$ still cover $E$ and we have \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\u0026rsquo;\\vert\\leq\\sum_{n=1}^{\\infty}\\left(\\vert B_n\\vert+\\frac{\\varepsilon}{2^n}\\right)=\\left(\\sum_{n=1}^{\\infty}\\vert B_n\\vert\\right)+\\varepsilon\\leq m^*(E)+2\\varepsilon\\label{eq:lemma5.1} \\end{equation} As the $B_n\u0026rsquo;$ are open, apply the Heine-Borel theorem, we obtain \\begin{equation} E\\subset\\bigcup_{n=1}^{N}B_n\u0026rsquo;, \\end{equation} for some finite $N$. Thus, using the finite subadditivity property of elementary measure, combined with the result \\eqref{eq:lemma5.1}, we obtain \\begin{equation} m(E)\\leq\\sum_{n=1}^{N}m(B_n\u0026rsquo;)\\leq m^*(E)+2\\varepsilon \\end{equation} And since $\\varepsilon\u0026gt;0$ was arbitrary, we can conclude that \\begin{equation} m(E)\\leq m^*(E) \\end{equation} Now we turn to considering the case that $E$ is not closed. Then we can write $E$ as the finite union of disjoint boxes \\begin{equation} E=Q_1\\cup\\ldots\\cup Q_k, \\end{equation} which need not be closed.\nAnalogy to before, we have that for every $\\varepsilon\u0026gt;0$ and every $1\\leq j\\leq k$, we can find a closed sub-box $Q_j\u0026rsquo;$ of $Q_j$ such that \\begin{equation} \\vert Q_j\u0026rsquo;\\vert\\geq\\vert Q_j\\vert-\\frac{\\varepsilon}{k} \\end{equation} Then $E$ now contains the finite union of $Q_1\u0026rsquo;\\cup\\ldots\\cup Q_k\u0026rsquo;$ disjoint closed boxes, which is a closed elementary set. By the finite additivity property of elementary measure, the monotonicity property of Lebesgue measure, combined with the result we have proved in the first case, we have \\begin{align} m^*(E)\u0026amp;\\geq m^*(Q_1\u0026rsquo;\\cup\\ldots\\cup Q_k\u0026rsquo;) \\\\ \u0026amp;=m(Q_1\u0026rsquo;\\cup\\ldots\\cup Q_k\u0026rsquo;) \\\\ \u0026amp;=m(Q_1\u0026rsquo;)+\\ldots+m(Q_k\u0026rsquo;) \\\\ \u0026amp;\\geq m(Q_1)+\\ldots+m(Q_k)-\\varepsilon \\\\ \u0026amp;= m(E)-\\varepsilon, \\end{align} for every $\\varepsilon\u0026gt;0$. And since $\\varepsilon\u0026gt;0$ was arbitrary, our claim has been proved.\nCorollary 6\nFrom the lemma above and the monotonicity property, for every $E\\in\\mathbb{R}^d$, we have \\begin{equation} m_{*,(J)}(E)\\leq m^{*}(E)\\leq m^{*,(J)}(E)\\label{eq:cor6.1} \\end{equation}\nCorollary 7\nNot every bounded open set or compact set (bounded closed) is Jordan measurable.\nProof\nConsider the countable set $\\mathbf{Q}\\cap[0,1]$, which we enumerate as $\\{q_1,q_2,\\ldots\\}$. Let $\\varepsilon\u0026gt;0$ be a small number, and consider that \\begin{equation} U\\doteq\\bigcup_{n=1}^{\\infty}(q_n-\\varepsilon/2^n,q_n+\\varepsilon/2^n), \\end{equation} which is a union of open sets and thus is open. On the other hand, by countable subadditivity property of Lebesgue outer measure, we have \\begin{align} m^{*}(U)\u0026amp;=m^{*}\\left(\\sum_{n=1}^{\\infty}\\left(q_n-\\frac{\\varepsilon}{2^n},q_n+\\frac{\\varepsilon}{2^n}\\right)\\right) \\\\ \u0026amp;\\leq\\sum_{n=1}^{\\infty}m^{*}\\left(q_n-\\frac{\\varepsilon}{2^n},q_n+\\frac{\\varepsilon}{2^n}\\right) \\\\ \u0026amp;=\\sum_{n=1}^{\\infty}\\frac{2\\varepsilon}{2^n}=2\\varepsilon \\end{align} As $U$ dense in $[0,1]$ (i.e.,$\\overline{U}$ contains $[0,1]$), we have \\begin{equation} m^{*}(U)=m^{*,(J)}(\\overline{U})\\geq m^{*,(J)}([0,1])=1 \\end{equation} Then for $\\varepsilon\\lt 1$, we have that \\begin{equation} m^{*}(U)\\lt 1\\leq m^{*,(J)}(U) \\end{equation} Combining with \\eqref{eq:cor6.1}, we obtain that the bounded open set $U$ is not Jordan measurable.\nFinite additivity for almost disjoint boxes Two boxes are almost disjoint if their interiors are disjoint, e.g., $[0,1]$ and $[1,2]$ are almost disjoint. If a box has the same elementary as its interior, we see that the finite additivity property \\begin{equation} m(B_1\\cup\\ldots\\cup B_n)=\\vert B_1\\vert+\\ldots+\\vert B_n\\vert\\label{eq:faadb.1} \\end{equation} also holds for almost disjoint boxes $B_1,\\ldots,B_n$.\nOuter measure of countable unions of almost disjoint boxes Lemma 8\nLet $E=\\bigcup_{n=1}^{\\infty}B_n$ be a countable union of almost disjoint boxes $B_1,B_2,\\ldots$. Then \\begin{equation} m^*(E)=\\sum_{n=1}^{\\infty}\\vert B_n\\vert \\end{equation} Thus, for example, $\\mathbb{R}^d$ has an infinite outer measure.\nProof\nFrom countable subadditivity property of Lebesgue measure and Lemma 5, we have \\begin{equation} m^*(E)\\leq\\sum_{n=1}^{\\infty}m^*(B_n)=\\sum_{n=1}^{\\infty}\\vert B_n\\vert, \\end{equation} so it suffices to show that \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert\\leq m^*(E) \\end{equation} Since for each integer $N$, $E$ contains the elementary set $B_1\\cup\\ldots\\cup B_N$, then by monotonicity property and Lemma 5 \\begin{align} m^*(E)\u0026amp;\\geq m^*(B_1\\cup\\ldots\\cup B_N)=m(B_1\\cup\\ldots\\cup B_N) \\end{align} And thus by \\eqref{eq:faadb.1}, we have \\begin{equation} \\sum_{n=1}^{N}\\vert B_n\\vert\\leq m^*(E) \\end{equation} Letting $N\\to\\infty$ we obtain the claim.\nCorollary 9\nIf $E=\\bigcup_{n=1}^{\\infty}B_n=\\bigcup_{n=1}^{\\infty}B_n\u0026rsquo;$ can be decomposed in two different ways as the countable union of almost disjoint boxes, then \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert=\\sum_{n=1}^{\\infty}\\vert B_n\u0026rsquo;\\vert \\end{equation}\nExample 2\nIf a set $E\\subset\\mathbb{R}^{d}$ is expressible as the countable union of almost disjoint boxes, then \\begin{equation} m^{*}(E)=m_{*,(J)}(E) \\end{equation}\nProof\nFor $B_n$\u0026rsquo;s are disjoint boxes, we begin by express $E$ as \\begin{equation} E=\\bigcup_{n=1}^{\\infty}B_n\\label{eq:eg2.1} \\end{equation} Hence, by Lemma 8, we have \\begin{equation} m^{*}(E)=\\sum_{n=1}^{\\infty}\\vert B_n\\vert\\label{eq:eg2.2} \\end{equation} Moreover, \\eqref{eq:eg2.1} can be continued to derive as \\begin{equation} E=\\bigcup_{n=1}^{\\infty}B_n=\\left(\\bigcup_{n=1}^{N}B_n\\right)\\cup\\left(\\bigcup_{n=N+1}^{\\infty}B_n\\right)=\\left(\\bigcup_{n=1}^{N}B_n\\right)\\cup B, \\end{equation} where we have defined $B=\\bigcup_{n=N+1}^{\\infty}B_n$. And thus, we also have that $B_1,\\ldots,B_N,B$ are almost disjoint boxes, which claims that $E$ is an elementary set. Therefore, $E$ is also Jordan measurable. Using finite additivity property of Jordan measurability yields \\begin{equation} m_{*,(J)}(E)=m(E)=\\left(\\sum_{n=1}^{N}\\vert B_n\\vert\\right)+\\vert B\\vert=\\sum_{n=1}^{\\infty}\\vert B_n\\vert\\label{eq:eg2.3} \\end{equation} Combining \\eqref{eq:eg2.2} and \\eqref{eq:eg2.3} together gives us \\begin{equation} m^{*}(E)=m_{*,(J)}(E) \\end{equation}\nOpen sets as countable unions of almost disjoint boxes Lemma 10\nLet $E\\subset\\mathbb{R}^d$ be an open set. Then $E$ can be expressed as the countable union of almost disjoint boxes (and, in fact, as the countable union of almost disjoint closed cubes).\nProof\nWe begin by defining a closed dyadic cube to be a cube $Q$ of the form \\begin{equation} Q=\\left[\\frac{i_1}{2^n},\\frac{i_1+1}{2^n}\\right]\\times\\ldots\\times\\left[\\frac{i_d}{2^n},\\frac{i_d+1}{2^n}\\right], \\end{equation} for some integers $n,i_1,\\ldots,i_d;n\\geq 0$.\nWe have that such closed dyadic cubes of a fixed sidelength $2^{-n}$ are almost disjoint and cover all of $\\mathbb{R}^d$. And also, each dyadic cube of sidelength $2^{-n}$ is contained in exactly one \u0026ldquo;parent\u0026rdquo; of sidelength $2^{-n+1}$ (which, conversely, has $2^d$ \u0026ldquo;children\u0026rdquo; of sidelength $2^{-n}$), giving the dyadic cubes a structure analogous to that of a binary tree.\nAs a consequence of these facts, we also obtain the dyadic nesting property: given any two closed dyadic cubes (not necessarily same sidelength), then either they are almost disjoint, or one of them is contained in the other.\nIf $E$ is open, and $x\\in E$, then by definition there is an open ball centered at $x$ that is contained in $E$. Also, it is easily seen that there is also a closed dyadic cube containing $x$ that is contained in $E$. Hence, if we let $\\mathcal{Q}$ be the collection of all the dyadic cubes $Q$ that are contained in $E$, we see that \\begin{equation} E=\\bigcup_{Q\\in\\mathcal{Q}}Q \\end{equation} Let $\\mathcal{Q}^*$ denote cubes in $\\mathcal{Q}$ such that they are not contained in any other cube in $\\mathcal{Q}$. From the nesting property, we see that every cube in $\\mathcal{Q}$ is contained in exactly one maximal cube in $\\mathcal{Q}^*$, and that any two such maximal cubes in $\\mathcal{Q}^*$ are almost disjoint. Thus, we have that \\begin{equation} E=\\bigcup_{Q\\in\\mathcal{Q}^*}Q, \\end{equation} which is union of almost disjoint cubes. As $\\mathcal{Q}^*$ is at most countable, the claim follows.\nOuter measure of open sets Corollary 11\nThe Lebesgue outer measure of any open set is equal to the Jordan inner measure of that set, or of the total volume of any partitioning of that set into almost disjoint boxes.\nOuter measure of arbitrary sets - Outer regularity Lemma 12.\nLet $E\\subset\\mathbb{R}^d$ be an arbitrary set. Then we have \\begin{equation} m^*(E)=\\inf_{E\\subset U,U\\text{ open}}m^*(U) \\end{equation}\nProof\nFrom monotonicity property, we have \\begin{equation} m^*(E)\\leq\\inf_{E\\subset U,U\\text{ open}}m^*(U) \\end{equation} Then, it suffices to show that \\begin{equation} m^*(E)\\geq\\inf_{E\\subset U,U\\text{ open}}m^*(U), \\end{equation} which is obvious in the case that $m^*(E)$ is infinite. Thus, we now assume that $m^*(E)$ is finite.\nLet $\\varepsilon\u0026gt;0$. By the definition of Lebesgue outer measure, there exists a countable family $B_1,B_2,\\ldots$ of boxes covering $E$ such that \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert\\leq m^*(E)+\\varepsilon \\end{equation} We can enlarge each of these boxes $B_n$ to an open box $B_n\u0026rsquo;$ such that \\begin{equation} \\vert B_n\u0026rsquo;\\vert\\leq\\vert B_n\\vert+\\frac{\\varepsilon}{2^n}, \\end{equation} for any $\\varepsilon\u0026gt;0$. Then the set $\\bigcup_{n=1}^{\\infty}B_n\u0026rsquo;$, being a union of open sets, is itself open, and contains $E$, and \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\u0026rsquo;\\vert\\leq m^*(E)+\\varepsilon+\\sum_{n=1}^{\\infty}\\frac{\\varepsilon}{2^n}=m^*(E)+2\\varepsilon \\end{equation} By countable subadditivity property, it implies that \\begin{equation} m^*\\left(\\bigcup_{n=1}^{\\infty}B_n\u0026rsquo;\\right)\\leq m^*(E)+2\\varepsilon \\end{equation} and thus \\begin{equation} \\inf_{E\\subset U,U\\text{ open}}m^*(U)\\leq m^*(E)+2\\varepsilon \\end{equation} And since $\\varepsilon\u0026gt;0$ was arbitrary, the claim follows.\nLebesgue measurability Existence of Lebesgue measurable sets Lemma 13.\nEvery open set is Lebesgue measurable. Every closed set is Lebesgue measurable. Every set of Lebesgue outer measure zero is measurable. (Such sets are called null sets.) The empty set $\\emptyset$ is Lebesgue measurable. If $E\\subset\\mathbb{R}^d$ is Lebesgue measurable, then so its complement $\\mathbb{R}^d\\backslash E$. If $E_1,E_2,\\ldots\\subset\\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the union $\\bigcup_{n=1}^{\\infty}E_n$ is Lebesgue measurable. If $E_1,E_2,\\ldots\\subset\\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the intersection $\\bigcap_{n=1}^{\\infty}E_n$ is Lebesgue measurable. Proof\nThis follows from definition. We have that every closed set is a the countable union of closed and bounded set, so by (vi), if suffices to verify the claim when $E$ is bounded and closed.\nLet $U\\supset E$ be an open set, we thus have that $U\\backslash E$ is also open due to the compactness of $E$. By lemma 10, we can represent the open set $U\\backslash E$ as a countable union of almost disjoint boxes as \\begin{equation} U\\backslash E=\\bigcup_{n=1}^{\\infty}B_n \\end{equation} The problem remains to prove that for any $\\varepsilon\u003e0$ \\begin{equation} \\sum_{n=1}^{\\infty}\\vert B_n\\vert\u003c\\varepsilon \\end{equation} This follows from definition. This follows from definition. Given $E$ is Lebesgue measurable, for each positive integer $n$, we can find an open set $U_n$ containing $E$ such that \\begin{equation} m^*(U_n\\backslash E)\\leq\\frac{1}{n} \\end{equation} Let $F_n=U_n^c=\\mathbb{R}^d\\backslash U_n$. Thus, we have $F_n\\subset\\mathbb{R}^d\\backslash E$ and \\begin{equation} m^*\\big((\\mathbb{R}^d\\backslash E)\\backslash F_n\\big)=m^*\\big((\\mathbb{R}^d\\backslash E)\\backslash(\\mathbb{R}^d\\backslash U_n)\\big)=m^*(U_n\\backslash E)\\leq\\frac{1}{n}\\label{eq:lemma13.1} \\end{equation} In addition, since $F_n\\subset\\mathbb{R}^d\\backslash E$, the countable union of them, denoted as $F$, is also a subset of $\\mathbb{R}^d\\backslash E$ \\begin{equation} F=\\bigcup_{n=1}^{\\infty}F_n\\subset\\mathbb{R}^d\\backslash E \\end{equation} Moreover, from \\eqref{eq:lemma13.2}, we have \\begin{equation} m^*\\left((\\mathbb{R}^d\\backslash E)\\backslash\\bigcup_{n=1}^{N}F_n\\right)=m^*\\left(\\bigcap_{n=1}^{N}(\\mathbb{R}^d\\backslash E)\\backslash F_n\\right)\\leq\\frac{1}{N} \\end{equation} Let $N$ approaches $\\infty$, we have \\begin{equation} m^*\\left((\\mathbb{R}^d\\backslash E)\\backslash F\\right)=m^*\\left((\\mathbb{R}^d\\backslash E)\\backslash\\bigcup_{n=1}^{\\infty}F_n\\right)\\leq 0 \\end{equation} By non-negativity property, we then have \\begin{equation} m^*\\left((\\mathbb{R}^d\\backslash E)\\backslash F\\right)=0, \\end{equation} Hence, $\\mathbb{R}^d\\backslash E$ is a union of $F$ with a set of Lebesgue outer measure of zero. The set $F$, in the other hand, is a countable union of closed set $F_n$'s (since each $U_n$ is an open set). Therefore, by (ii), (iii) and (vi), we have that $\\mathbb{R}^d\\backslash E$ is also Lebesgue measurable. For each Lebesgue measurable set $E_n$, for any $\\varepsilon\u003e0$ and for $U_n$ is an open set containing $E_n$ we have \\begin{equation} m^{*}(U_n\\backslash E_n)\\leq\\frac{\\varepsilon}{2^n}\\label{eq:lemma13.2} \\end{equation} Moreover, since $E_n\\subset U_n$, then \\begin{equation} \\bigcup_{n=1}^{\\infty}E_n\\subset\\bigcup_{n=1}^{\\infty}U_n, \\end{equation} which is also an open set. Therefore, from \\eqref{eq:lemma13.2} and by countable subadditivity, we have \\begin{equation} m^*\\left(\\left(\\bigcup_{n=1}^{\\infty}U_n\\right)\\backslash\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\right)\\leq\\sum_{n=1}^{\\infty}m^*(U_n\\backslash E_n)\\leq\\sum_{n=1}^{\\infty}\\frac{\\varepsilon}{2^n}=\\varepsilon, \\end{equation} which proves that $\\bigcup_{n=1}^{\\infty}E_n$ is Lebesgue measurable. Given $E_1,E_2,E_3,\\ldots\\subset\\mathbb{R}^d$ are Lebesgue measurable, by (v), the complement of them, \\begin{equation} E_1^c,E_2^c,E_3^c,\\ldots\\subset\\mathbb{R}^d, \\end{equation} are also Lebesgue measurable. By De Morgan's laws, we have \\begin{equation} \\left(\\bigcap_{n=1}^{\\infty}E_n\\right)^c=\\bigcup_{n=1}^{\\infty}E_n^c, \\end{equation} which is Lebesgue measurable by (vi). Thus, $\\left(\\bigcap_{n=1}^{\\infty}E_n\\right)^c$ is also Lebesgue measurable. This means, using (v) once again, we obtain that $\\bigcap_{n=1}^{\\infty}E_n$ is Lebesgue measurable. Criteria for measurability Let $E\\subset\\mathbb{R}^d$, then the following are equivalent:\n$E$ is Lebesgue measurable. Outer approximation by open. For every $\\varepsilon\u003e0$, $E$ can be contained in an open set $U$ with $m^*(U\\backslash E)\\leq\\varepsilon$. Almost open. For every $\\varepsilon\u003e0$, we can find an open set $U$ such that $m^*(U\\Delta E)\\leq\\varepsilon$. ($E$ differs from an open set by a set of outer measure at most $\\varepsilon$.) Inner approximation by closed. For every $\\varepsilon\u003e0$, we can find a closed set $F$ contained in $E$ with $m^*(E\\backslash F)\\leq\\varepsilon$. Almost closed. For every $\\varepsilon\u003e0$, we can find a closed set $F$ such that $m^*(F\\Delta E)\\leq\\varepsilon$. ($E$ differs from a closed set by a set of outer measure at most $\\varepsilon$.) Almost measurable. For every $\\varepsilon\u003e0$, we can find a Lebesgue measurable set $E_\\varepsilon$ such that $m^*(E_\\varepsilon\\Delta E)\\leq\\varepsilon$. ($E$ differs from a measurable set by a set of outer measure at most $\\varepsilon$.) Proof\n(i) $\\Rightarrow$ (ii)\nThis follows from definition (i) $\\Rightarrow$ (iii)\nGiven $E$ is Lebesgue measurable, for any $\\varepsilon\u003e0$, we can find an open set $U$ containing $E$ such that \\begin{equation} m^*(U\\backslash E)\\leq\\varepsilon \\end{equation} And since $E\\subset U$, we have that \\begin{equation} m^(E\\backslash U)=m^*(\\emptyset)=0, \\end{equation} which implies that for any $\\varepsilon\u003e0$ \\begin{equation} m^*(U\\Delta E)=m^*(U\\backslash E)+m^*(E\\backslash U)\\leq\\varepsilon \\end{equation} (i) $\\Rightarrow$ (iv)\nBy the claim (v) in lemma 13, given Lebesgue measurable set $E\\subset\\mathbb{R}^d$, we have that its complement $\\mathbb{R}^d\\backslash E$ is also Lebesgue measurable. Therefore, there exists an open set $U$ containing $\\mathbb{R}^d\\backslash E$ such that for any $\\varepsilon\u003e0$ we have \\begin{equation} m^*\\left(U\\backslash(\\mathbb{R}^d\\backslash E)\\right)\\leq\\varepsilon\\label{eq:cm.1} \\end{equation} Let $F$ denote the complement of $U$, $F=\\mathbb{R}\\backslash U$, thus $F$ is a closed set contained in $E$. Moreover, from \\eqref{eq:cm.1} we also have for any $\\varepsilon\u003e0$ \\begin{equation} m^*(E\\backslash F)=m^*\\left(E\\backslash(\\mathbb{R}^d\\backslash U)\\right)=m^*\\left(U\\backslash(\\mathbb{R}^d\\backslash E)\\right)\\leq\\varepsilon \\end{equation} (i) $\\Rightarrow$ (v)\nGiven Lebesgue measurable set $E\\subset\\mathbb{R}^d$, using the claim (v) in lemma 13 gives us that its complement $\\mathbb{R}^d\\backslash E$ is also Lebesgue measurable.\nFrom claim (iii), for any $\\varepsilon\u003e0$, we can find an open set $U$ such that \\begin{equation} m^*\\left(U\\Delta(\\mathbb{R}^d\\backslash E)\\right)\\leq\\varepsilon\\label{eq:cm.2} \\end{equation} Let $F$ denote the complement of $U$, $F=\\mathbb{R}^d\\backslash$. We then have that $F$ is a closed set. In addition, $U\\Delta(\\mathbb{R}^d\\backslash E)$ can be rewritten by \\begin{align} U\\Delta(\\mathbb{R}^d\\backslash E)\u0026=\\left(U\\backslash(\\mathbb{R}^d\\backslash E)\\right)\\cup\\left((\\mathbb{R}^d\\backslash E)\\backslash U\\right) \\\\ \u0026=\\left(E\\backslash(\\mathbb{R}^d\\backslash U)\\right)\\cup\\left((\\mathbb{R}^d\\backslash U)\\backslash E\\right) \\\\ \u0026=(\\mathbb{R}^d\\backslash U)\\backslash E \\\\ \u0026=F\\Delta E, \\end{align} which lets \\eqref{eq:cm.2} can be written as, for any $\\varepsilon\u003e0$ \\begin{equation} m^*(F\\Delta E)\\leq\\varepsilon \\end{equation} (i) $\\Rightarrow$ (vi)\nGiven $E$ is Lebesgue measurable, by claim (v), for any $\\varepsilon\u003e0$ we can find a closed set $E_\\varepsilon$ such that \\begin{equation} m^*(E_\\varepsilon\\Delta E)\\leq\\varepsilon \\end{equation} While by property (ii) of lemma 13, we have that $E_\\varepsilon$ is Lebesgue measurable, which proves our claim. (vi) $\\Rightarrow$ (i)\nGiven (vi), for any $\\varepsilon\u003e0$, we can find a Lebesgue measurable set $E_\\varepsilon^{(n)}$ such that \\begin{equation} m^*\\left(E_\\varepsilon^{(n)}\\Delta E\\right)\\leq\\frac{\\varepsilon}{2^n} \\end{equation} Therefore, by countable subadditivity property of Lebesgue outer measurability \\begin{equation} m^*\\left(\\bigcup_{n=1}^{\\infty}E_\\varepsilon^{(n)}\\Delta E\\right)\\leq\\sum_{n=1}^{\\infty}m^*\\left(E_\\varepsilon^{(n)}\\Delta E\\right)\\leq\\sum_{n=1}^{\\infty}\\frac{\\varepsilon}{2^n}=\\varepsilon \\end{equation} Remark 14 Every Jordan measurable set is Lebesgue measurable.\nProof\nThis follows directly from corollary 6.\nRemark 15 The Cantor set is compact, uncountable, and a null set.\nProof\nSince $\\mathcal{C}\\subseteq[0,1]$ is closed and bounded, by the Heine-Borel theorem, $\\mathcal{C}$ is then compact. The measure axioms Lemma 16\nEmpty set. $m(\\emptyset)=0$. Countable additivity. If $E_1,E_2,\\ldots\\subset\\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)=\\sum_{n=1}^{\\infty}m(E_n) \\end{equation} Proof\nEmpty set\nWe have that empty set $\\emptyset$ is Lebesgue measurable since for every $\\varepsilon\u003e0$, there exists an open set $U\\subset\\mathbb{R}^d$ containing $\\emptyset$ such that $m^*(U\\backslash\\emptyset)\\leq\\varepsilon$. Thus, \\begin{equation} m(\\emptyset)=m^*(\\emptyset)=0 \\end{equation} Countable additivity\nWe begin by considering the case that $E_n$ are all compact sets. By repeated use of Lemma 12 and Example ?, we have \\begin{equation} m\\left(\\bigcup_{n=1}^{N}E_n\\right)=\\sum_{n=1}^{N}m(E_n) \\end{equation} Thus, using monotonicity property, we have \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\geq\\sum_{n=1}^{N}m(E_n) \\end{equation} Let $N\\to\\infty$, we obtain \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\geq\\sum_{n=1}^{\\infty}m(E_n) \\end{equation} On the other hand, by countable subadditivity, we also have \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\\leq\\sum_{n=1}^{N}m(E_n) \\end{equation} Therefore, we can conclude that \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)=\\sum_{n=1}^{N}m(E_n) \\end{equation} Next, we consider the case that $E_n$ are bounded but not necessarily compact. Let $\\varepsilon\u003e0$. By criteria for measurability, we know that each $E_n$ is the union of a compact set $K_n$ and a set of outer measure at most $\\varepsilon/2^n$. Thus \\begin{equation} m(E_n)\\leq m(K_n)+\\frac{\\varepsilon}{2^n} \\end{equation} And hence \\begin{equation} \\sum_{n=1}^{\\infty}m(E_n)\\leq\\left(\\sum_{n=1}^{\\infty}m(K_n)\\right)+\\varepsilon \\end{equation} From the first case, we know that \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}K_n\\right)=\\sum_{n=1}^{\\infty}m(K_n) \\end{equation} while from monotonicity property of Lebesgue measure \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}K_n\\right)\\leq m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right) \\end{equation} Putting these results together we obtain \\begin{equation} \\sum_{n=1}^{\\infty}m(E_n)\\leq m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)+\\varepsilon, \\end{equation} for every $\\varepsilon\u003e0$. And since $\\varepsilon$ was arbitrary, we have \\begin{equation} \\sum_{n=1}^{\\infty}m(E_n)\\leq m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right) \\end{equation} while from countable subadditivity property we have \\begin{equation} \\sum_{n=1}^{\\infty}m(E_n)\\geq m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right) \\end{equation} Therefore, the claim follows. Finally, we consider the case that $E_n$ are not bounded or closed with the idea of decomposing each $E_n$ as a countable disjoint union of bounded Lebesgue measurable sets. Remark 17\nThe countable additivity also implies the finite additivity property of Lebesgue measure \\begin{equation} m\\left(\\bigcup_{n=1}^{N}E_n\\right)=\\sum_{n=1}^{N}m(E_n), \\end{equation} where $E_1,\\ldots,E_N$ are Lebesgue measurable.\nMonotone convergence theorem for measurable sets Upward monotone convergence. Let $E_1\\subset E_2\\subset\\ldots\\subset\\mathbb{R}^d$ be a countable non-decreasing sequence of Lebesgue measurable sets. Then \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)=\\lim_{n\\to\\infty}m(E_n) \\end{equation} Downward monotone convergence. Let $\\mathbb{R}^d\\supset E_1\\supset E_2\\supset\\ldots$ be a countable non-increasing sequence of Lebesgue measurable sets. If at least one of the $m(E_n)$ is finite, then \\begin{equation} m\\left(\\bigcap_{n=1}^{\\infty}E_n\\right)=\\lim_{n\\to\\infty}m(E_n) \\end{equation} The hypothesis that at least one of the $m(E_n)$ is finite in the downward monotone convergence theorem cannot be dropped. Proof\nUpward monotone convergence\nSince $E_1\\subset E_2\\subset\\ldots\\subset\\mathbb{R}^d$ is a countable non-decreasing sequence of Lebesgue measurable sets, by countable additivity, we have \\begin{align} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)\u0026=m\\left(\\bigcup_{n=1}^{\\infty}E_n\\backslash\\bigcup_{n'=1}^{n-1}E_{n'}\\right) \\\\ \u0026=m\\left(\\bigcup_{n=1}^{\\infty}E_n\\backslash E_{n-1}\\right) \\\\ \u0026=\\left(\\sum_{n=2}^{\\infty}m(E_n)-m(E_{n-1})\\right)+m(E_1) \\\\ \u0026=\\lim_{n\\to\\infty}m(E_n) \\end{align} Downward monotone convergence\nSince $\\mathbb{R}^d\\supset E_1\\supset E_2\\supset\\ldots$ is a countable non-increasing sequence of Lebesgue measurable sets, the sequence of their complement $E_1^c\\subset E_2^c\\subset\\ldots\\subset\\mathbb{R}^d$ is therefore a countable non-decreasing sequence of Lebesgue measurable sets. Using the claim (i) and by De Morgan's laws, we have \\begin{align} m\\left(\\bigcap_{n=1}^{\\infty}E_n\\right)\u0026=m\\left(\\mathbb{R}^d\\backslash\\bigcup_{n=1}^{\\infty}E_n^c\\right) \\\\ \u0026=m(\\mathbb{R}^d)-m\\left(\\bigcup_{n=1}^{\\infty}E_n^c\\right) \\\\ \u0026=m(\\mathbb{R}^d)-\\lim_{n\\to\\infty}m(E_n^c) \\\\ \u0026=m(\\mathbb{R}^d)-m(\\mathbb{R}^d)+\\lim_{n\\to\\infty}m(E_n) \\\\ \u0026=\\lim_{n\\to\\infty}m(E_n) \\end{align} Consider sequence $\\mathbb{R}^d\\supset E_1\\supset E_2\\supset\\ldots$ of non-increasing Lebesgue measurable sets where each $E_n$ is given by \\begin{equation} E_n\\doteq[n,+\\infty) \\end{equation} Therefore, by De Morgan's laws, the Lebesgue measure of their countable intersection is \\begin{align} m\\left(\\bigcap_{n=1}^{\\infty}E_n\\right)\u0026=m\\left(\\mathbb{R}^d\\backslash\\bigcup_{n=1}^{\\infty}E_n^c\\right) \\\\ \u0026=m\\left(\\mathbb{R}^d\\backslash\\bigcup_{n=1}^{\\infty}(-\\infty,n)\\right) \\\\ \u0026=m(\\mathbb{R}^d\\backslash\\mathbb{R}^d) \\\\ \u0026=m(\\emptyset)=0, \\end{align} while for every $n$, we have \\begin{equation} m(E_n)=m\\left([n,+\\infty)\\right)=\\infty \\end{equation} Dominated convergence theorem for measurable sets We say that a sequence $E_n$ of sets in $\\mathbb{R}^d$ converges pointwise to another set $E$ in $\\mathbb{R}^d$ if the indicator function $1_{E_n}$ converges pointwise to $1_E$.\nIf the $E_n$ are all Lebesgue measurable, and converge pointwise to $E$, then $E$ is Lebesgue measurable also. Dominated convergence theorem. Suppose that the $E_n$ are all contained in another Lebesgue measurable set $F$ of finite measure. Then $m(E_n)$ converges to $m(E)$. The dominated convergence theorem fails if the $E_n$'s are not contained in a set of finite measure, even if we assume that the $m(E_n)$ are all uniformly bounded. Proof\nWe have Remark 18\nLet $E\\subset\\mathbb{R}^d$, then $E$ is contained in a Lebesgue measurable set of measure exactly equal to $m^*(E)$.\nProof\nInner regularity Let $E\\subset\\mathbb{R}^d$ be Lebesgue measurable. Then \\begin{equation} m(E)=\\sup_{K\\subset E,K\\text{ compact}}m(K) \\end{equation}\nProof\nBy monotonic we have that \\begin{equation} m(E)\\geq\\sup_{K\\subset E,K\\text{ compact}}m(K), \\end{equation} thus it suffices to show that \\begin{equation} m(E)\\leq\\sup_{K\\subset E,K\\text{ compact}}m(K) \\end{equation} Consider the case that $E$ is bounded. By the criteria for Lebesgue measurability, we have that for any $\\varepsilon\u0026gt;0$, there exist a bounded and closed, and thus compact by the Heine-Borel theorem, set $K\u0026rsquo;$ contained in $E$ such that \\begin{equation} m(E\\backslash K\u0026rsquo;)\\leq\\varepsilon \\end{equation} Moreover, by claim (ii) of lemma 13, we have that $K\u0026rsquo;$ is Lebesgue measurable. Using finite additivity property of Lebesgue measure gives us \\begin{equation} \\varepsilon\\geq m(E\\backslash K\u0026rsquo;)=m(E)-m(K\u0026rsquo;), \\end{equation} which means \\begin{equation} m(E)\\leq m(K\u0026rsquo;)\\leq\\sup_{K\\subset E,K\\text{ compact}}m(K) \\end{equation} Now consider {the case that $E$ is an unbounded set. Let $(K_r)_{r=1,2,\\ldots}$ be the sequence sets in which each $K_r$ is defined as \\begin{equation} K_r\\doteq E\\cap B(\\mathbf{0},r),\\label{eq:ir.1} \\end{equation} where $B(\\mathbf{0},r)$ is a closed ball centered at $\\mathbf{0}\\in\\mathbb{R}^d$ with radius $r$ \\begin{equation} B(\\mathbf{0},r)=\\{\\mathbf{x}:\\vert\\mathbf{x}\\vert\\leq r\\} \\end{equation} which means $K_1\\subset K_2\\subset\\ldots\\subset E$ is an increasing sequence of compact set (since \\eqref{eq:ir.1} also implies that $K_r\\subset B(\\mathbf{0},r)$, and hence bounded and closed, then using the Heine-Borel theorem to obtain the compactness of $K_r$). By the monotone convergence theorem, we have \\begin{equation} m\\left(\\bigcup_{r=1}^{\\infty}K_r\\right)=\\lim_{r\\to\\infty}m(K_r) \\end{equation} On the other hand, the countable union of $K_r$ can be written as \\begin{equation} \\bigcup_{r=1}^{\\infty}K_r=\\bigcup_{r=1}^{\\infty}E\\cap B(\\mathbf{0},r)=E\\cap\\bigcup_{r=1}^{\\infty}B(\\mathbf{0},r)=E\\cap\\mathbb{R}^d=E, \\end{equation} which therefore gives us \\begin{equation} m(E)=\\lim_{r\\to\\infty}m(K_r)\\label{eq:ir.2} \\end{equation} Moreover, by monotonicity property $m(E)\\geq m(K_r),\\forall r$. Hence, \\eqref{eq:ir.2} implies that for any $\\varepsilon\u0026gt;0$, there exists $r\u0026rsquo;$ such that for all $r\\geq r\u0026rsquo;$ \\begin{equation} \\varepsilon\u0026gt;\\vert m(E)-m(K_{r\u0026rsquo;})\\vert=m(E)-m(K_{r\u0026rsquo;}) \\end{equation} This means that \\begin{equation} m(A)\\leq\\sup_{K\\subset E,K\\text{ compact}}m(K) \\end{equation} Our claim then follows.\nCriteria for finite measure Let $E\\subset\\mathbb{R}^d$, then the following are equivalent:\n$E$ is Lebesgue measurable with finite measure. Outer approximation by open. For every $\\varepsilon\u003e0$, we can contain $E$ in an open set $U$ of finite measure with $m^*(U\\backslash E)\\leq\\varepsilon$. Almost open bounded. For every $\\varepsilon\u003e0$, there exists a bounded open set $U$ such that $m^*(E\\Delta U)\\leq\\varepsilon$. (In other words, $E$ differs from a bounded set by a set of arbitrarily small Lebesgue outer measure.) Inner approximation by compact. For every $\\varepsilon\u003e0$, we can find a compact set $F$ contained in $E$ with $m^*(E\\backslash F)\\leq\\varepsilon$. Almost compact. $E$ differs from a compact set by a set of arbitrarily small Lebesgue outer measure. Almost bounded measurable. $E$ differs from a bounded Lebesgue measurable set by a set of arbitrarily small Lebesgue outer measure. Almost finite measure. $E$ differs from a Lebesgue measurable set with finite measure by a set of arbitrarily small Lebesgue outer measure. Almost elementary. $E$ differs from an elementary set by a set of arbitrarily small Lebesgue outer measure. Almost dyadically elementary. For every $\\varepsilon\u003e0$, there exists an integer $n$ and a finite union $F$ of closed dyadic cubes of sidelength $2^{-n}$ such that $m^*(E\\Delta F)\\leq\\varepsilon$. Proof\n(i) $\\Rightarrow$ (ii)\nGiven $E$ is Lebesgue measurable with finite measure, by definition, for any $\\varepsilon\u003e0$, there exists an open set $U$ o such that \\begin{equation} m^*(U\\backslash E)\\leq\\varepsilon \\end{equation} Then, by finite subadditivity property of Lebesgue outer measure \\begin{equation} m^*(U)\\leq m^*(E)+\\varepsilon, \\end{equation} which implies that $m^*(U)$ finite due to finiteness of $m^*(E)$ and $\\varepsilon$, and hence $U$ has finite measure since $m(U)\\leq m^*(U)$. (i) $\\Rightarrow$ (iii)\nCarathéodory criterion, one direction Let $E\\subset\\mathbb{R}^d$, the following are then equivalent:\n$E$ is Lebesgue measurable. For every elementary set $A$ \\begin{equation} m(A)=m^*(A\\cap E)+m^*(A\\backslash E) \\end{equation} For every box $B$, we have \\begin{equation} \\vert B\\vert=m^*(B\\cap E)+m^*(B\\backslash E) \\end{equation} Proof\n(i) $\\Rightarrow$ (ii)\nWe begin with an observation that, by finite additivity property of Lebesgue measure \\begin{equation} m(A)=m(A\\cap E)+m(A\\backslash E)\\leq m^*(A\\cap E)+m^*(A\\backslash E)\\label{eq:cc.1} \\end{equation} Given $A$ is elementary, by [Lemma 10](https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#measure-elementary-set), we can express $A$ as a finite union of disjoint boxes \\begin{equation} A=\\bigcup_{n=1}^{N}B_n \\end{equation} Continuing using finite subadditivity of Lebesgue outer measure and finite additivity of Lebesgue measure, \\eqref{eq:cc.1} then can be continued to derive as \\begin{align} m(A)\u0026\\leq m^*(A\\cap E)+m^*(A\\backslash E) \\\\ \u0026=m^*\\left(\\left(\\bigcup_{n=1}^{N}B_n\\right)\\cap E\\right)+m^*\\left(\\left(\\bigcup_{n=1}^{N}B_n\\right)\\backslash E\\right) \\\\ \u0026=m^*\\left(\\bigcup_{n=1}^{N}B_n\\cap E\\right)+m^*\\left(\\bigcup_{n=1}^{N}B_n\\backslash E\\right) \\\\ \u0026\\leq\\sum_{n=1}^{N}m^*(B_n\\cap E)+m^*(B_n\\backslash E) \\\\ \u0026=\\sum_{n=1}^{N}m^*(B_n)=\\sum_{n=1}^{N}m(B_n)=m\\left(\\bigcup_{n=1}^{N}B_n\\right)=m(A), \\end{align} which implies that \\begin{equation} m(A)=m^*(A\\cap E)+m^*(A\\backslash E) \\end{equation} (i) $\\Rightarrow$ (iii)\nSince every box $B$ is Lebesgue measurable, then given $E$ is also Lebesgue measurable, by Lemma 13, their difference and intersection are also Lebesgue measurable, which means by additivity property of Lebesgue measure we have \\begin{equation} \\vert B\\vert=m(B)=m(B\\cap E)+m(B\\backslash E)=m^*(B\\cap E)+m^*(B\\backslash E) \\end{equation} (ii) $\\Rightarrow$ (i)\nInner measure Let $E\\subset\\mathbb{R}^d$ be a bounded set. The Lebesgue inner measure $m_*(E)$ of $E$ is defined by \\begin{equation} m_*(E)\\doteq m(A)-m^*(A\\backslash E), \\end{equation} for any elementary set $A$ containing $E$. Then\nIf $A,A'$ are two elementary sets containing $E$, then \\begin{equation} m(A)-m^*(A\\backslash E)=m(A')-m^*(A'\\backslash E) \\end{equation} We have that $m_*(E)\\leq m^*(E)$, and that equality holds iff $E$ is Lebesgue measurable. Proof\nExample 3\nLet $E\\subset \\mathbb{R}^d$, and define a $G_\\delta$ set to be a countable intersection $\\bigcap_{n=1}^{\\infty}U_n$ of open sets, and define an $F_\\delta$ set to be a countable union $\\bigcup_{n=1}^{\\infty}F_n$ of closed sets. The following are then equivalent:\n$E$ is Lebesgue measurable. $E$ is a $G_\\delta$ set with a null set removed. $E$ is the union of an $F_\\delta$ set and a null set. Proof\nTranslation invariance Let $E\\subset\\mathbb{R}^d$ be Lebesgue measurable, then $E+x$ is also Lebesgue measurable for any $x\\in\\mathbb{R}^d$, and $m(E+x)=m(E)$.\nProof\nChange of variables Let $E\\subset\\mathbb{R}^d$ be Lebesgue measurable, and $T:\\mathbb{R}^d\\to\\mathbb{R}^d$ be a linear transformation, then $T(E)$ is Lebesgue measurable, and $m(T(E))=\\vert\\text{det}(T)\\vert m(E)$.\nNote\nIf $T:\\mathbb{R}^d\\to\\mathbb{R}^{d\u0026rsquo;}$ is a linear map to a space $\\mathbb{R}^{d\u0026rsquo;}$ of strictly smaller dimension than $\\mathbb{R}^d$, then $T(E)$ need not be Lebesgue measurable.\nProof\nRemark 19\nLet $d,d\u0026rsquo;\\geq 1$ be natural numbers\nIf $E\\subset\\mathbb{R}^d$ and $F\\subset\\mathbb{R}^{d'}$, then \\begin{equation} (m^{d+d'})^*(E\\times F)\\leq(m^d)^*(E)(m^{d'})^*(F) \\end{equation} Let $E\\subset\\mathbb{R}^d,F\\subset\\mathbb{R}^{d'}$ be Lebesgue measurable sets. Then $E\\times F\\subset\\mathbb{R}^{d+d'}$ is Lebesgue measurable, with \\begin{equation} m^{d+d'}(E\\times F)=m^d(E).m^{d'}(F) \\end{equation} Proof\nUniqueness of Lebesgue measure Lebesgue measure $E\\mapsto m(E)$ is the only map from Lebesgue measurable sets to $[0,+\\infty]$ that obeys the following axioms:\nEmpty set. $m(\\emptyset)=0$. Countable additivity. If $E_1,E_2,\\ldots\\subset\\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then \\begin{equation} m\\left(\\bigcup_{n=1}^{\\infty}E_n\\right)=\\sum_{n=1}^{\\infty}m(E_n) \\end{equation} Translation invariance. If $E$ is Lebesgue measurable and $x\\in\\mathbb{R}^d$, then $m(E+x)=m(E)$. Normalisation. $m([0,1]^d)=1$. Proof\nNon-measurable sets Remark 20\nThere exists a subset $E\\subset[0,1]$ which is not Lebesgue measurable.\nRemark 21 (Outer measure is not finitely additive)\nThere exists disjoint bounded subsets $E,F\\subset\\mathbb{R}$ such that \\begin{equation} m^*(E\\cap F)\\neq m^*(E)+m^*(F) \\end{equation}\nRemark 22\nLet $\\pi:\\mathbb{R}^2\\to\\mathbb{R}$ be the coordinate projection $\\pi(x,y)\\doteq x$. Then there exists a measurable $E\\subset\\mathbb{R}^2$ such that $\\pi(E)$ is not measurable.\nReferences [1] Terence Tao. An introduction to measure theory. Graduate Studies in Mathematics, vol. 126.\n[2] Elias M. Stein \u0026amp; Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007. Footnotes The diameter of a set $B$ is defined as \\begin{equation*} \\text{dia}(B)\\doteq\\sup\\{\\vert x-y\\vert:x,y\\in B\\} \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/measure-theory/measure-theory-p2/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNote II of the measure theory series. Materials are mostly taken from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#taos-book\"\u003eTao\u0026rsquo;s book\u003c/a\u003e, except for some needed notations extracted from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#steins-book\"\u003eStein\u0026rsquo;s book\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Measure theory - II: Lebesgue measure"},{"content":" Note I of the measure theory series. Materials are mostly taken from Tao\u0026rsquo;s book, except for some needed notations extracted from Stein\u0026rsquo;s book.\nPreliminaries Points, sets A point $x\\in\\mathbb{R}^d$ consists of a $d$-tuple of real numbers \\begin{equation} x=\\left(x_1,x_2,\\dots,x_d\\right),\\hspace{1cm}x_i\\in\\mathbb{R}, i=1,\\dots,d \\end{equation} Addition between points and multiplication of a point by a real scalar is elementwise.\nThe norm of $x$ is denoted by $\\vert x\\vert$ and is defined to be the standard Euclidean norm given by \\begin{equation} \\vert x\\vert=\\left(x_1^2+\\dots+x_d^2\\right)^{1/2} \\end{equation} We can then calculate the distance between two points $x$ and $y$, which is \\begin{equation} \\text{dist}(x,y)=\\vert x-y\\vert \\end{equation} The complement of a set $E$ in $\\mathbb{R}^d$ is denoted as $E^c$, and defined by \\begin{equation} E^c=\\{x\\in\\mathbb{R}^d:x\\notin E\\} \\end{equation} If $E$ and $F$ are two subsets of $\\mathbb{R}^d$, we denote the complement of $F$ in $E$ by \\begin{equation} E-F=\\{x\\in\\mathbb{R}^d:x\\in E;\\hspace{0.1cm}x\\notin F\\} \\end{equation} The distance between two sets $E$ and $F$ is defined by \\begin{equation} \\text{dist}(E,F)=\\inf_{x\\in E,\\hspace{0.1cm}y\\in F}\\vert x-y\\vert \\end{equation}\nOpen, closed and compact sets The open ball in $\\mathbb{R}^d$ centered at $x$ and of radius $r$ is defined by \\begin{equation} B(x,r)=\\{y\\in\\mathbb{R}^d:\\vert y-x\\vert\u0026lt; r\\} \\end{equation} A subset $E\\subset\\mathbb{R}^d$ is open if for every $x\\in E$ there exists $r\u0026gt;0$ with $B(x,r)\\subset E$. And a set is closed if its complement is open.\nAny (not necessarily countable) union of open sets is open, while in general, the intersection of only finitely many open sets is open. A similar statement holds for the class of closed sets, if we interchange the roles of unions and intersections.\nA set $E$ is bounded if it is contained in some ball of finite radius. A set is compact if it is bounded and is also closed. Compact sets enjoy the Heine-Borel covering property:\nTheorem 1. (Heine-Borel theorem)\nAssume $E$ is compact, $E\\subset\\bigcup_\\alpha\\mathcal{O}_\\alpha$, and each $\\mathcal{O}_\\alpha$ is open. Then there are finitely many of the open sets $\\mathcal{O}_{\\alpha_1},\\mathcal{O}_{\\alpha_2},\\dots,\\mathcal{O}_{\\alpha_N}$, such that $E\\subset\\bigcup_{j=1}^{N}\\mathcal{O}_{\\alpha_j}$.\nIn words, any covering of a compact set by a collection of open sets contains a finite subcovering.\nA point $x\\in\\mathbb{R}^d$ is a limit point of the set $E$ if for every $r\u0026gt;0$, the ball $B(x,r)$ contains points of $E$. This means that there are points in $E$ which are arbitrarily close to $x$. An isolated point of $E$ is a point $x\\in E$ such that there exists an $r\u0026gt;0$ where $B(x,r)\\cap E=\\{x\\}$.\nA point $x\\in E$ is an interior point of $E$ if there exists $r\u0026gt;0$ such that $B(x,r)\\subset E$. The set of all interior points of $E$ is called the interior of $E$.\nThe closure of $E$, denoted as $\\bar{E}$, consists the union of $E$ and all its limit points. The boundary of $E$, denoted as $\\partial E$, is the set of points which are in the closure of $E$ but not in the interior of $E$.\nA closed set $E$ is perfect if $E$ does not have any isolated point.\nThe boundary of $E$, denoted by $\\partial E$, is the set of points in $\\bar{E}$ not belonging to the interior of $E$.\nRemark:\nThe closure of a set is a closed set. Every point in $E$ is a limit point of $E$. A set is closed iff it contains all its limit points. Rectangles, cubes A (closed) rectangle $R$ in $\\mathbb{R}^d$ is given by the product of $d$ one-dimensional closed and bounded intervals \\begin{equation} R\\doteq[a_1,b_1]\\times[a_2,b_2]\\times\\ldots\\times[a_d,b_d], \\end{equation} where $a_j\\leq b_j$, for $j=1,\\ldots,d$, are real numbers. In other words, we have \\begin{equation} R=\\left\\{\\left(x_1,\\ldots,x_d\\right)\\in\\mathbb{R}^d:a_j\\leq x_j\\leq b_j,\\forall j=1,\\ldots,d\\right\\} \\end{equation} With this definition, a rectangle is closed and has sides parallel to the coordinate axis. In $\\mathbb{R}$, the rectangles are the closed and bounded intervals; they becomes the usual rectangles as we usually see in $\\mathbb{R}^2$; while in $\\mathbb{R}^3$, they are the closed parallelepipeds.\nFigure 1: Rectangles in $\\mathbb{R}^d,d=1,2,3$ The lengths of the sides of the rectangle $R$ in $\\mathbb{R}^d$ are $b_1-a_1,\\ldots,b_d-a_d$. The volume of its, denoted as $\\vert R\\vert$, is defined as \\begin{equation} \\vert R\\vert\\doteq(b_1-a_1)\\dots(b_d-a_d) \\end{equation} An open rectangle is the product of open intervals, and the interior of the rectangle $R$ is then \\begin{equation} (a_1,b_1)\\times\\ldots\\times(a_d,b_d) \\end{equation} A cube is a rectangle for which $b_1-a_1=\\ldots=b_d-a_d$.\nA union of rectangles is said to be almost disjoint if the interiors of the rectangles are disjoint.\nLemma 2\nIf a rectangle is the almost disjoint union of finitely many other rectangles, say $R=\\bigcup_{k=1}^{N}R_k$, then \\begin{equation} \\vert R\\vert=\\sum_{k=1}^{N}\\vert R_k\\vert \\end{equation}\nLemma 3\nIf $R,R_1,\\ldots,R_N$ are rectangles, and $R\\subset\\bigcup_{k=1}^{N}R_k$, then \\begin{equation} \\vert R\\vert\\leq\\sum_{k=1}^{N}\\vert R_k\\vert \\end{equation}\nTheorem 4\nEvery open $\\mathcal{O}\\subset\\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals.\nTheorem 5\nEvery open $\\mathcal{O}\\subset\\mathbb{R}^d,d\\geq 1$, can be written as countable union of almost disjoint closed cubes.\nThe Cantor set Let $C_0=[0,1]$ denote the closed unit interval and let $C_1$ represent the set obtained from deleting the middle third open interval from $[0,1]$, as \\begin{equation} C_1=[0,1/3]\\cup[2/3,1] \\end{equation} We repeat this procedure of deleting the middle third open interval for each subinterval of $C_1$. In the second stage we obtain \\begin{equation} C_1=[0,1/9]\\cup[2/9,1/3]\\cup[2/3,7/9]\\cup[8/9,1] \\end{equation} We continue to repeat this process for each subinterval of $C_2$, and so on. The result of this process is a sequence $(C_k)_{k=0,1,\\ldots}$ of compact sets with \\begin{equation} C_0\\supset C_1\\supset C_2\\supset\\ldots\\supset C_k\\supset C_{k+1}\\supset\\ldots \\end{equation} The Cantor set $\\mathcal{C}$ is defined as the intersection of all $C_k$\u0026rsquo;s \\begin{equation} \\mathcal{C}=\\bigcap_{k=0}^{\\infty}C_k \\end{equation} The set $\\mathcal{C}$ is not empty, since all end-points of the intervals in $C_k$ (all $k$) belong to $\\mathcal{C}$.\nOthers Given any sequence $x_1,x_2,\\ldots\\in[0,+\\infty]$. We can always form the sum \\begin{equation} \\sum_{n=1}^{n}x_n\\in[0,+\\infty] \\end{equation} as the limit of the partial sums $\\sum_{n=1}^{N}x_n$, which may be either finite or infinite. An equivalence definition of this infinite sum is as the supremum of all finite subsums: \\begin{equation} \\sum_{n=1}^{\\infty}x_n=\\sup_{F\\subset\\mathbb{N},F\\text{ finite}}\\sum_{n\\in F}x_n \\end{equation} From this equation, given any collection $(x_\\alpha)_{\\alpha\\in A}$ of numbers $x_\\alpha\\in[0,+\\infty]$ indexed by an arbitrary set $A$, we can define the sum $\\sum_{\\alpha\\in A}x_\\alpha$ as \\begin{equation} \\sum_{\\alpha\\in A}x_\\alpha=\\sup_{F\\subset A,F\\text{ finite}}\\sum_{\\alpha\\in F}x_\\alpha\\label{eq:others.1} \\end{equation} Or moreover, given any bijection $\\phi:B\\to A$, we has the change of variables formula \\begin{equation} \\sum_{\\alpha\\in A}x_\\alpha=\\sum_{\\beta\\in B}x_{\\phi(\\beta)} \\end{equation}\nTheorem 6. (Tonelli\u0026rsquo;s theorem for series)\nLet $(x_{n,m})_{n,m\\in\\mathbb{N}}$ be a doubly infinite sequence of extended nonnegative reals $x_{n,m}\\in[0,+\\infty]$. Then \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}=\\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty}x_{n,m}=\\sum_{m=1}^{\\infty}\\sum_{n=1}^{\\infty}x_{n,m} \\end{equation}\nProof\nWe will prove the equality between the first and second expression, the proof for the equality between the first and the third one is similar.\nWe begin by showing that \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}\\leq\\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty}x_{n,m} \\end{equation} Let $F\\subset\\mathbb{N}^2$ be any finite set. Then $F\\subset\\{1,\\ldots,N\\}\\times\\{1,\\ldots,N\\}$ for some finite $N$. Since $x_{n,m}$ are nonnegative, we have \\begin{align} \\sum_{(n,m)\\in F}x_{n,m}\u0026amp;\\leq\\sum_{(n,m)\\in\\{1,\\ldots,N\\}\\times\\{1,\\ldots,N\\}}x_{n,m} \\\\ \u0026amp;=\\sum_{n=1}^{N}\\sum_{m=1}^{N}x_{n,m} \\\\ \u0026amp;\\leq\\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty}x_{n,m}, \\end{align} for any finite subset $F$ of $\\mathbb{R}^2$. Then by \\eqref{eq:others.1}, we have \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}=\\sup_{F\\subset\\mathbb{N}^2,F\\text{ finite}}x_{n,m}\\leq\\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty}x_{n,m} \\end{equation} The problem now remains to prove that \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}\\geq\\sum_{n=1}^{\\infty}\\sum_{m=1}^{\\infty}x_{n,m}, \\end{equation} which will be proved if we can show that \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}\\geq\\sum_{n=1}^{N}\\sum_{m=1}^{\\infty}x_{n,m} \\end{equation} Fix $N$, we have since each $\\sum_{m=1}^{\\infty}$ is the limit of $\\sum_{m=1}^{M}x_{n,m}$, LHS is the limit of $\\sum_{n=1}^{N}\\sum_{m=1}^{M}x_{n,m}$ as $M\\to\\infty$. Thus, it suffices to show that for each finite $M$ \\begin{equation} \\sum_{(n,m)\\in\\mathbb{N}^2}x_{n,m}\\geq\\sum_{n=1}^{N}\\sum_{m=1}^{M}x_{n,m}=\\sum_{(n,m)\\in\\{1,\\ldots,N\\}\\times\\{1,ldots,M\\}}x_{n,m} \\end{equation} which is true for all finite $M,N$. And it concludes our proof.\nAxiom 7. (Axiom of choice)\nLet $(E_\\alpha)_{\\alpha\\in A}$ be a family of non-empty set $E_\\alpha$, indexed by an index set $A$. Then we can find a family $(x_\\alpha)_{\\alpha\\in A}$ of elements $x_\\alpha$ of $E_\\alpha$, indexed by the same set $A$.\nCorollary 8. (Axiom of countable choice)\nLet $E_1,E_2,\\ldots$ be a sequence of non-empty sets. Then we can find a sequence $x_1,x_2,\\ldots$ such that $x_n\\in E_n,\\forall n=1,2,\\ldots$.\nElementary measure Intervals, boxes, elementary sets An interval is a subset of $\\mathbb{R}$ having one of the forms \\begin{align} [a,b]\u0026amp;\\doteq\\{x\\in\\mathbb{R}:a\\leq x\\leq b\\}, \\\\ [a,b)\u0026amp;\\doteq\\{x\\in\\mathbb{R}:a\\leq x\\lt b\\}, \\\\ (a,b]\u0026amp;\\doteq\\{x\\in\\mathbb{R}:a\\lt x\\leq b\\}, \\\\ (a,b)\u0026amp;\\doteq\\{x\\in\\mathbb{R}:a\\lt x\\lt b\\}, \\end{align} where $a\\leq b$ are real numbers.\nThe length of an interval $I=[a,b],[a,b),(a,b],(a,b)$ is denoted as $\\vert I\\vert$ and is defined by \\begin{equation} \\vert I\\vert\\doteq b-a \\end{equation} A box in $\\mathbb{R}^d$ is a Cartesian product $B\\doteq I_1\\times\\ldots\\times I_d$ of $d$ intervals $I_1,\\ldots,I_d$ (not necessarily the same length). The volume $\\vert B\\vert$ of such a box $B$ is defined as \\begin{equation} \\vert B\\vert\\doteq \\vert I_1\\vert\\times\\ldots\\times\\vert I_d\\vert \\end{equation} An elementary set is any subset of $\\mathbb{R}^d$ which is the union of a finite number of boxes.\nRemark 9 (Boolean closure)\nIf $E,F\\subset\\mathbb{R}^d$ are elementary sets, then\nThe union $E\\cup F$ is elementary. The intersection $E\\cap F$ is elementary. The set theoretic difference $E\\backslash F\\doteq\\{x\\in E:x\\notin F\\}$ is elementary. The symmetric difference $E\\Delta F\\doteq(E\\backslash F)\\cup(F\\backslash E)$ is elementary. If $x\\in\\mathbb{R}^d$, then the translate $E+x\\doteq\\{y+x:y\\in E\\}$ is elementary. Proof\nWith their definitions as elementary sets, we can assume that \\begin{align} E\u0026amp;=B_1\\cup\\ldots\\cup B_k, \\\\ F\u0026amp;=B_1\u0026rsquo;\\cup\\ldots\\cup B_{k\u0026rsquo;}\u0026rsquo;, \\end{align} where each $B_i$ and $B_i\u0026rsquo;$ is a $d$-dimensional box. By set theory, we have that:\nThe union of $E$ and $F$ can be written as \\begin{equation} E\\cup F=B_1\\cup\\ldots\\cup B_k\\cup B_1'\\cup\\ldots\\cup B_{k'}', \\end{equation} which is an elementary set. The intersection of $E$ and $F$ can be written as \\begin{align} E\\cap F\u0026=\\left(B_1\\cup\\ldots\\cup B_k\\right)\\cup\\left(B_1'\\cup\\ldots\\cup B_{k'}'\\right) \\\\ \u0026=\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{k'}\\left(B_i\\cap B_j'\\right), \\end{align} which is also an elementary set. The set theoretic difference of $E$ and $F$ can be written as \\begin{align} E\\backslash F\u0026=\\left(B_1\\cup\\ldots\\cup B_k\\right)\\backslash\\left(B_1'\\cup\\ldots\\cup B_{k'}'\\right) \\\\ \u0026=\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{k'}\\left(B_i\\backslash B_j'\\right), \\end{align} which is, once again, an elementary set. With this display, the symmetric difference of $E$ and $F$ can be written as \\begin{align} E\\Delta F\u0026=\\left(E\\backslash F\\right)\\cup\\left(F\\backslash E\\right) \\\\ \u0026=\\Bigg[\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{k'}\\left(B_i\\backslash B_j'\\right)\\Bigg]\\cup\\Bigg[\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{k'}\\left(B_j'\\backslash B_i\\right)\\Bigg], \\end{align} which satisfies conditions of an elementary set. Since $B_i$'s are $d$-dimensional boxes, we can express them as \\begin{equation} B_i=I_{i,1}\\times\\ldots I_{i,d}, \\end{equation} where each $I_{i,j}$ is an interval in $\\mathbb{R}^d$. Without loss of generality, we assume that they are all closed. In particular, for $j=1,\\ldots,d$ \\begin{equation} I_{i,j}=(a_{i,j},b_{i,j}) \\end{equation} Thus, for any $x\\in\\mathbb{R}^d$, we have that \\begin{align} E+x\u0026=\\left\\{y+x:y\\in E\\right\\} \\\\ \u0026=\\Big\\{y+x:y\\in B_1\\cup\\ldots\\cup B_k\\Big\\} \\\\ \u0026=\\Big\\{y+x:y\\in\\bigcup_{i=1}^{k}B_i\\Big\\} \\\\ \u0026=\\left\\{y+x:y\\in\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{d}(a_{i,j},b_{i,j})\\right\\} \\\\ \u0026=\\bigcup_{i=1}^{k}\\bigcup_{j=1}^{d}(a_{i,j}+x,b_{i,j}+x), \\end{align} which is an elementary set. Measure of an elementary set Lemma 10\nLet $E\\subset\\mathbb{R}^d$ be an elementary set.\n$E$ can be expressed as the finite union of disjoint boxes. If $E$ is partitioned as the finite union $B_1\\cup\\ldots\\cup B_k$ of disjoint boxes, then the quantity $m(E)\\doteq\\vert B_1\\vert+\\ldots+\\vert B_k\\vert$ is independent of the partition. In other words, given any other partition $B_1'\\cup\\ldots\\cup B_{k'}'$ of $E$, we have \\begin{equation} \\vert B_1\\vert+\\ldots+\\vert B_k\\vert=\\vert B_1'\\vert+\\ldots+\\vert B_{k'}'\\vert \\end{equation} We refer to $m(E)$ as the elementary measure of $E$.\nProof\nConsider the one-dimensional case, with these $k$ intervals, we can put their $2k$ endpoints into an increasing-order list (discarding repetitions). By looking at the open intervals between these end points, together with the endpoints themselves (viewed as intervals of length zero), we see that there exists a finite collection of disjoint intervals $J_1,\\dots,J_{k'}$, such that each of the $I_1,\\dots,I_k$ are union of some collection of the $J_1,\\dots,J_{k'}$. And since each interval is a one-dimensional box, our statement has been proved with $d=1$.\nIn order to prove the multi-dimensional case, we begin by expressing $E$ as \\begin{equation} E=\\bigcap_{i=1}^{k}B_i, \\end{equation} where each box $B_i=I_{i,1}\\times\\dots\\times I_{i,d}$. For each $j=1,\\dots,d$, since we has proved the one-dimensional case, we can express $I_{1,j},\\dots I_{k,j}$ as the union of subcollections of collections $J_{1,j},\\dots,J_{k',j}$ of disjoint intervals. Taking Cartesian product, we can express the $B_1,\\dots,B_k$ as finite unions of box $J_{i_1,1}\\times\\dots\\times J_{i_d,d}$, where $1\\leq i_j\\leq k_j'$ for all $1\\leq j\\leq d$. Moreover such boxes are disjoint, which proved our argument. We have that the length for an interval $I$ can be computed as \\begin{equation} \\vert I\\vert=\\lim_{N\\to\\infty}\\frac{1}{N}\\#\\left(I\\cap\\frac{1}{N}\\mathbb{Z}\\right), \\end{equation} where $\\#A$ represents the cardinality of a finite set $A$ and \\begin{equation} \\frac{1}{N}\\mathbb{Z}\\doteq\\left\\{\\frac{x}{N}:x\\in\\mathbb{Z}\\right\\} \\end{equation} Thus, volume of the box, say $B$, established from $d$ intervals $I_1,\\dots,I_d$ by taking Cartesian product of them can be written as \\begin{equation} \\vert B\\vert=\\lim_{N\\to\\infty}\\frac{1}{N^d}\\#\\left(B\\cap\\frac{1}{N}\\mathbb{Z}^d\\right) \\end{equation} Therefore, with $k$ disjoint boxes $B_1,\\dots,B_k$, we have that \\begin{align} \\vert B_1\\vert+\\dots+\\vert B_k\\vert\u0026=\\lim_{N\\to\\infty}\\frac{1}{N^d}\\#\\left[\\left(\\bigcup_{i=1}^{k}B_i\\right)\\cap\\frac{1}{N}\\mathbb{Z}^d\\right] \\\\ \u0026=\\lim_{N\\to\\infty}\\frac{1}{N^d}\\#\\left(E\\cap\\frac{1}{N}\\mathbb{Z}^d\\right) \\\\ \u0026=\\lim_{N\\to\\infty}\\frac{1}{N^d}\\#\\left[\\left(\\bigcup_{i=1}^{k'}B_i'\\right)\\cap\\frac{1}{N}\\mathbb{Z}^d\\right] \\\\ \u0026=\\vert B_1'\\vert+\\dots+\\vert B_{k'}'\\vert \\end{align} Properties of elementary measure From the definition of elementary measure, it is easily seen that, for any elementary sets $E$ and $F$ (not necessarily disjoint),\n$m(E)$ is a nonnegative real number (non-negativity), and has finite additivity property: \\begin{equation} m(E\\cup F)=m(E)+m(F) \\end{equation} And by induction, it also implies that \\begin{equation} m(E_1\\cup\\dots\\cup E_k)=m(E_1)+\\dots+m(E_k), \\end{equation} whenever $E_1,\\dots,E_k$ are disjoint elementary sets. $m(\\emptyset)=0$. $m(B)=\\vert B\\vert$ for all box $B$. From non-negativity, finite additivity and Remark 9, we conclude the monotonicity property, i.e., $E\\subset F$ implies that \\begin{equation} m(E)\\leq m(F) \\end{equation} From the above and finite additivity, we also obtain the finite subadditivity property \\begin{equation} m(E\\cup F)\\leq m(E)+m(F) \\end{equation} And by induction, we then have \\begin{equation} m(E_1\\cup\\dots\\cup E_k)\\leq m(E_1)+\\dots+m(E_k), \\end{equation} whenever $E_1,\\dots, E_k$ are elementary sets (not necessarily disjoint). We also have the translation invariance property \\begin{equation} m(E+x)=m(E),\\hspace{1cm}\\forall x\\in\\mathbb{R}^d \\end{equation} Uniqueness of elementary measure Let $d\\geq 1$ and let $m\u0026rsquo;:\\mathcal{E}(\\mathbb{R}^d)\\to\\mathbb{R}^+$ be a map from the collection $\\mathcal{E}(\\mathbb{R}^d)$ of elementary subsets of $\\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity, and translation invariance properties. Then there exists a constant $c\\in\\mathbb{R}^+$ such that \\begin{equation} m\u0026rsquo;(E)=cm(E), \\end{equation} for all elementary sets $E$. In particular, if we impose the additional normalization $m\u0026rsquo;([0,1)^d)=1$, then $m\u0026rsquo;\\equiv m$.\nProof\nSet $c\\doteq m\u0026rsquo;([0,1)^d)$, we then have that $c\\in\\mathbb{R}^+$ by the non-negativity property. Using the translation invariance property, we have that for any positive integer $n$ \\begin{equation} m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\right)=m\u0026rsquo;\\left(\\left[\\frac{1}{n},\\frac{2}{n}\\right)^d\\right)=\\dots=m\u0026rsquo;\\left(\\left[\\frac{n-1}{n},1\\right)^d\\right) \\end{equation} On other hand, using the finite additivity property, for any positive integer $n$, we obtain that \\begin{align} m\u0026rsquo;([0,1)^d)\u0026amp;=m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\cup\\left[\\frac{1}{n},\\frac{2}{n}\\right)^d\\cup\\dots\\cup\\left[\\frac{n-1}{n},1\\right)^d\\right) \\\\ \u0026amp;=m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\right)+m\u0026rsquo;\\left(\\left[\\frac{1}{n},\\frac{2}{n}\\right)^d\\right)+\\dots+m\u0026rsquo;\\left(\\left[\\frac{n-1}{n},1\\right)^d\\right) \\\\ \u0026amp;=n m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\right) \\end{align} Thus, \\begin{equation} m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\right)=\\frac{c}{n},\\hspace{1cm}\\forall n\\in\\mathbb{Z}^+ \\end{equation} Moreover, since $m\\left(\\left[0,\\frac{1}{n}\\right)^d\\right)=\\frac{1}{n}$, we have that for any positive integer $n$ \\begin{equation} m\u0026rsquo;\\left(\\left[0,\\frac{1}{n}\\right)^d\\right)=cm\\left(\\left[0,\\frac{1}{n}\\right)^d\\right) \\end{equation} It then follows by induction that \\begin{equation} m\u0026rsquo;(E)=cm(E) \\end{equation}\nRemark 11\nLet $d_1,d_2\\geq 1$, and let $E_1\\subset\\mathbb{R}^{d_1},E_2\\subset\\mathbb{R}^{d_2}$ be elementary sets. Then $E_1\\times E_2\\subset\\mathbb{R}^{d_1+d_2}$ is also elementary, and $m^{d_1+d_2}(E_1\\times E_2)=m^{d_1}(E_1)\\times m^{d_2}(E_2)$.\nProof\nWithout loss of generality, assume that $d_1\\leq d_2$. With their definitions as elementary sets, we can assume that \\begin{align} E_1\u0026amp;=B_1\\cup\\dots\\cup B_{k_1}, \\\\ E_2\u0026amp;=B_1\u0026rsquo;\\cup\\dots\\cup B_{k_2}\u0026rsquo;, \\end{align} where each $B_i$ is a $d_1$-dimensional box while each $B_i\u0026rsquo;$ is a $d_2$-dimensional box. And using Lemma 5, without loss of generality, we can assume that $B_i$ are disjoint boxes and $B_i\u0026rsquo;$ are also disjoint, which implies that \\begin{align} m^{d_1}(E_1)\u0026amp;=m^{d_1}(B_1)+\\dots+m^{d_1}(B_{k_1}),\\label{eq:remark11.1} \\\\ m^{d_2}(E_2)\u0026amp;=m^{d_2}(B_1\u0026rsquo;)+\\dots+m^{d_2}(B_{k_2}\u0026rsquo;)\\label{eq:remark11.2} \\end{align} By set theory, we have that \\begin{align} E_1\\times E_2\u0026amp;=\\Big(B_1\\cup\\dots\\cup B_{k_1}\\Big)\\times\\Big(B_1\u0026rsquo;\\cup\\dots\\cup B_{k_2}\u0026rsquo;\\Big) \\\\ \u0026amp;=\\bigcup_{i=1}^{k_1}\\bigcup_{j=1}^{k_2}\\left(B_i\\times B_j\u0026rsquo;\\right),\\label{eq:remark11.3} \\end{align} which is an elementary set.\nSince $B_1,\\dots,B_{k_1}$ are disjoint and $B_1\u0026rsquo;,\\dots,B_{k_2}\u0026rsquo;$ are disjoint, the Cartesian products $B_i\\times B_j\u0026rsquo;$ for $i=1,\\dots,k_1$ and $j=1,\\dots,k_2$ are also disjoint. From \\eqref{eq:remark11.3} and using the finite additivity property, we have that \\begin{align} m^{d_1+d_2}(E_1\\times E_2)\u0026amp;=m^{d_1+d_2}\\Bigg(\\bigcup_{i=1}^{k_1}\\bigcup_{j=1}^{k_2}\\left(B_i\\times B_j\u0026rsquo;\\right)\\Bigg) \\\\ \u0026amp;=\\sum_{i=1}^{k_1}\\sum_{j=1}^{k_2}m^{d_1+d_2}\\left(B_i\\times B_j\u0026rsquo;\\right)\\label{eq:remark11.4} \\end{align} On the one hand, using the definition of boxes, and without loss of generality we can express, for each $i=1,\\dots,k_1$, that: \\begin{equation} B_i=(a_{i,1},b_{i,1})\\times\\dots\\times(a_{i,d_1},b_{i,d_1}), \\end{equation} where $a_{i,j},b_{i,j}\\in\\mathbb{R}$ for all $j=1,\\dots,d_1$. Hence, \\begin{equation} m^{d_1}(B_i)=\\prod_{j=1}^{d_1}(b_{i,j}-a_{i,j}),\\hspace{1cm}i=1,\\dots,k_1\\label{eq:remark11.5} \\end{equation} Similarly, we also have that \\begin{equation} m^{d_2}(B_i\u0026rsquo;)=\\prod_{j=1}^{d_2}(d_{i,j}-c_{i,j}),\\hspace{1cm}i=1,\\dots,k_2\\label{eq:remark11.6} \\end{equation} where $c_{i,j},d_{i,j}\\in\\mathbb{R}$ for all $j=1,\\dots,d_2$.\nMoreover, on the other hand, we also have that the $(d_1+d_2)$-dimensional box $B_i\\times B_j\u0026rsquo;$ can be expressed as \\begin{equation} B_i\\times B_j\u0026rsquo;=(e_1,f_1)\\times\\dots\\times(e_{d_1+d_2},f_{d_1+d_2}),\\label{eq:remark11.7} \\end{equation} where $e_k=a_{i,k};f_k=b_{i,k}$ for all $k=1,\\dots,d_1$ and $e_k=c_{j,k-d_1};f_k=d_{j,k-d_1}$ for all $k=d_1+1,\\dots,d_2$.\nFrom \\eqref{eq:remark11.5}, \\eqref{eq:remark11.6} and \\eqref{eq:remark11.5}, for any $i=1,\\dots,k_1$ and for any $j=1,\\dots,k_2$, we have \\begin{align} m^{d_1+d_2}(B_i\\times B_j\u0026rsquo;)\u0026amp;=\\prod_{k=1}^{d_1+d_2}(f_k-e_k) \\\\ \u0026amp;=\\Bigg(\\prod_{k=1}^{d_1}(b_{i,k}-a_{i,k})\\Bigg)\\Bigg(\\prod_{k=1}^{d_2}(d_{j,k}-c_{j,k})\\Bigg) \\\\ \u0026amp;=m^{d_1}(B_i)\\times m^{d_2}(B_j\u0026rsquo;) \\end{align} With this result, combined with \\eqref{eq:remark11.1} and \\eqref{eq:remark11.2}, equation \\eqref{eq:remark11.4} can be written as \\begin{align} m^{d_1+d_2}(E_1\\times E_2)\u0026amp;=\\sum_{i=1}^{k_1}\\sum_{j=1}^{k_2}m^{d_1+d_2}\\left(B_i\\times B_j\u0026rsquo;\\right) \\\\ \u0026amp;=\\sum_{i=1}^{k_1}\\sum_{j=1}^{k_2}m^{d_1}(B_i)\\times m^{d_2}(B_j\u0026rsquo;) \\\\ \u0026amp;=m^{d_1}(E_1)\\times m^{d_2}(E_2), \\end{align} which concludes our proof.\nJordan measure Let $E\\subset\\mathbb{R}^d$ be a bounded set.\nThe Jordan inner measure $m_{*,(J)}(E)$ of $E$ is defined as \\begin{equation} m_{*,(J)}(E)\\doteq\\sup_{A\\subset E,A\\text{ elementary}}m(A) \\end{equation} The Jordan outer measure $m^{*,(J)}(E)$ of $E$ is defined as \\begin{equation} m^{*,(J)}(E)\\doteq\\inf_{B\\supset E,B\\text{ elementary}}m(B) \\end{equation} If $m_{*,(J)}(E)=m^{*,(J)}(E)$, then we say that $E$ is Jordan measurable, and call \\begin{equation} m(E)\\doteq m_{*,(J)}(E)=m^{*,(J)}(E) \\end{equation} the Jordan measure of $E$. Characterisation of Jordan measurability Let $E\\subset\\mathbb{R}^d$ be bounded. These following statements are equivalence\n$E$ is Jordan measurable. For every $\\varepsilon\u003e0$, there exists elementary sets $A\\subset E\\subset B$ such that $m(B\\backslash A)\\leq\\varepsilon$. For every $\\varepsilon\u003e0$, there exists an elementary set $A$ such that $m^{*,(J)}(A\\Delta E)\\leq\\varepsilon$. Proof\nIn order to prove these three statements are equivalence, we will be proving that (1) implies (2); (2) implies (3); and that (2) implies (1).\n(1) implies (2).\nSince $E$ is Jordan measurable, we have that \\begin{equation} m(E)=\\sup_{A\\subset E;A\\text{ elementary}}m(A)=\\inf_{B\\supset E;B\\text{ elementary}}m(B) \\end{equation} By the definition of supremum, there exists an elementary set $A\\subset E$ such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} m(A)\\geq m(E)-\\frac{\\varepsilon}{2}\\label{eq:jmc.1} \\end{equation} In addition, by the definition of infimum, there also exists an elementary set $B\\supset E$ such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} m(B)\\leq m(E)+\\frac{\\varepsilon}{2}\\label{eq:jmc.2} \\end{equation} From \\eqref{eq:jmc.1} and \\eqref{eq:jmc.2}, we have that for any $\\varepsilon\u0026gt;0$ \\begin{equation} m(B\\backslash A)=m(B)-m(A)\\leq\\varepsilon \\end{equation} (2) implies (3).\nWith (2) satisfied, we have that we can find elementary sets $A\\subset E\\subset B$ such that \\begin{equation} m(B\\backslash A)\\leq\\varepsilon,\\hspace{1cm}\\forall\\varepsilon\u0026gt;0 \\end{equation} Since $A\\subset E\\subset B$ and by the definition of symmetric difference, we have \\begin{equation} A\\Delta E=(A\\backslash E)\\cup(E\\backslash A)=(E\\backslash A)\\subset(B\\backslash A) \\end{equation} Hence \\begin{equation} m^{*,(J)}(A\\Delta E)\\leq m(B\\backslash A)\\leq\\varepsilon \\end{equation} (2) implies (1).\nLet $(A_n)_{n\\in\\mathbb{N}}$ and $(B_n)_{n\\in\\mathbb{N}}$ be sequences of elementary sets such that $A_n\\subset E\\subset B_n$ for all $n\\in\\mathbb{N}$. Statement (2) says that for all $\\varepsilon\u0026gt;0$, there exists $i,j\\in\\mathbb{N}$ such that \\begin{equation} m(B_j\\backslash A_i)\\leq\\varepsilon \\end{equation} or \\begin{equation} m(B_j)\\leq m(A_i)+\\varepsilon\\label{eq:jmc.3} \\end{equation} Let $A_\\text{sup}$ and $B_\\text{inf}$ be two sets in the two sequences above with \\begin{align} m(A_\\text{sup})\u0026amp;=\\sup_{n\\in\\mathbb{N}}m(A_n), \\\\ m(B_\\text{inf})\u0026amp;=\\inf_{n\\in\\mathbb{N}}m(B_n), \\end{align} which means \\begin{align} m_{*,(J)}(E)\u0026amp;=m(A_\\text{sup}) \\\\ m^{*,(J)}(E)\u0026amp;=m(B_\\text{inf}) \\end{align} Using the monotonicity property of elementary measure, we have that \\begin{equation} m(A_\\text{sup})\\leq m(B_\\text{inf}) \\end{equation} Assume that $m(B_\\text{inf})\u0026gt;m(A_\\text{sup})$, and consider an $\\varepsilon\u0026gt;0$ such that $\\varepsilon\u0026lt; m(B_\\text{inf})-m(A_\\text{sup})$. We can continue to derive \\eqref{eq:jmc.3} as \\begin{equation} m(B_j)\\leq m(A_i)+\\varepsilon\u0026lt; m(A_i)+m(B_\\text{inf})-m(A_\\text{sup})\u0026lt; m(B_\\text{inf}), \\end{equation} which is false with the definition of $B_\\text{inf}$. Therefore, our assumption is also false, which means \\begin{equation} m(A_\\text{sup})=m(B_\\text{inf}) \\end{equation} or \\begin{equation} m_{*,(J)}(E)=m^{*,(J)}(E), \\end{equation} or in other words, $E$ is Jordan measurable. Corollary 12\nEvery elementary set $E$ is Jordan measurable. On elementary sets, Jordan measure is elementary measure. Jordan measurability also inherits many of the properties of elementary measure.\nProperties of Jordan measurability Let $E,F\\in\\mathbb{R}^d$ be Jordan measurable sets. Then\nBoolean closure. $E\\cup F,E\\cap F,E\\backslash F,E\\Delta F$ are also Jordan measurable sets. Non-negativity. $m(E)\\geq 0$. Finite additivity. If $E,F$ are disjoint, then $m(E\\cup F)=m(E)+m(F)$. Monotonicity. If $E\\subset F$, then $m(E)\\leq m(F)$. Finite subadditivity. $m(E\\cup F)\\leq m(E)+m(F)$. Translation invariance. For any $x\\in\\mathbb{R}^d$, $E+x$ is Jordan measurable, and $m(E+x)=m(E)$. Proof\nBoolean closure. By characterisation of Jordan measurability, we can find elementary sets $A_1\\subset E\\subset B_1$ and $A_2\\subset F\\subset B_2$ such that for any $\\varepsilon\u003e0$ \\begin{align} m(B_1\\backslash A_1)\u0026\\leq\\frac{\\varepsilon}{2}, \\\\ m(B_2\\backslash A_2)\u0026\\leq\\frac{\\varepsilon}{2} \\end{align} Thus, we have that \\begin{equation} \\left(A_1\\cap A_2\\right)\\subset\\left(E\\cap F\\right)\\subset\\left(B_1\\cap B_2\\right) \\end{equation} and \\begin{equation} \\left(A_1\\cup A_2\\right)\\subset\\left(E\\cup F\\right)\\subset\\left(B_1\\cup B_2\\right) \\end{equation} Moreover, for any $\\varepsilon\u003e0$, we have that \\begin{align*} m\\big((B_1\\cup B_2)\\backslash(A_1\\cup A_2)\\big)\u0026=m(B_1\\cup B_2)-m(A_1\\cup A_2) \\\\ \u0026=m(B_1)+m(B_2\\backslash B_1)-m(A_1\\cup A_2) \\\\ \u0026\\leq m(B_1)+m(B_2\\backslash A_1)-m(A_1\\cup A_2) \\\\ \u0026=m(B_1)-m(A_1)+m(B_2\\backslash A_1)+m(A_1)-m(A_1\\cup A_2) \\\\ \u0026=m(B_1)-m(A_1)+m(B_2\\cup A_1)-m(A_1\\cup A_2) \\\\ \u0026=m(B_1\\backslash A_1)+m\\big((B_2\\cup A_1)\\backslash(A_1\\cup A_2)\\big) \\\\ \u0026=m(B_1\\backslash A_1)+m(B_2\\backslash A_2) \\\\ \u0026\\leq\\varepsilon/2+\\varepsilon/2 \\\\ \u0026=\\varepsilon, \\end{align*} which implies that $E\\cup F$ is Jordan measurable. From the result above, and by monotonicity, finite additivity, finite subadditivity properties of elementary measure, for any $\\varepsilon\u003e0$, we also have that \\begin{align*} m\\big((B_1\\cap B_2)\\backslash(A_1\\cap A_2)\\big)\u0026=m(B_1\\cap B_2)-m(A_1\\cap A_2) \\\\ \u0026=m\\Big(\\big(B_1\\cup B_2\\big)\\backslash\\big((B_1\\backslash B_2)\\cup(B_2\\backslash B_1)\\big)\\Big) \\\\ \u0026\\hspace{1cm}-m\\Big(\\big(A_1\\cup A_2\\big)\\backslash\\big((A_1\\backslash A_2)\\cup(A_2\\backslash A_1)\\big)\\Big) \\\\ \u0026=m(B_1\\cup B_2)-m(B_1\\backslash B_2)-m(B_2\\backslash B_1) \\\\ \u0026\\hspace{1cm}-m(A_1\\cup A_2)+m(A_1\\backslash A_2)+m(A_2\\backslash A_1) \\\\ \u0026=m(B_1\\cup B_2)-m(A_1\\cup A_2)+m(A_1\\backslash A_2)-m(B_1\\backslash B_2) \\\\ \u0026\\hspace{1cm}+m(A_2\\backslash A_1)-m(B_2\\backslash B_1) \\\\ \u0026\\leq m(B_1\\cup B_2)-m(A_1\\cup A_2)+m(B_1\\backslash A_2)-m(B_1\\backslash B_2) \\\\ \u0026\\hspace{1cm}+m(B_2\\backslash A_1)-m(B_2\\backslash B_1) \\\\ \u0026\\leq m(B_1\\cup B_2)-m(A_1\\cup A_2) \\\\ \u0026\\leq\\varepsilon, \\end{align*} which also implies that $E\\cap F$ is Jordan measurable. Non-negativity.\nGiven $E$ being Jordan measurable set, we have \\begin{equation} m(E)=\\sup_{A\\subset E,A\\text{ elementary}}m(A)\\geq m(\\emptyset)=0, \\end{equation} by the monotonicity property of elementary measure. Finite additivity.\nSince given $E,F$ being Jordan measurable sets, $E\\cup F$ is also Jordan measurable set. And by the finite additivity property of elementary measure, we have \\begin{align} m(E)+m(F)\u0026=\\sup_{A_1\\subset E,A_1\\text{ elementary}}m(A_1)+\\sup_{A_2\\subset F,A_2\\text{ elementary}}m(A_2) \\\\ \u0026=\\sup_{A_1\\subset E,A_2\\subset F;A_1,A_2\\text{ elementary}}m(A_1)+m(A_2) \\\\ \u0026=\\sup_{A_1\\subset E,A_2\\subset F;A_1,A_2\\text{ elementary}}m(A_1\\cup A_2)=m(E\\cup F) \\end{align} Monotonicity.\nGiven $E\\subset F$ are Jordan measurable sets, the we have \\begin{equation} m(E)\\leq\\sup_{A\\subset F,A\\text{ elementary}}m(A)=m(F) \\end{equation} Finite subadditivity.\nSince given $E,F$ being Jordan measurable sets, $E\\cup F$ is also Jordan measurable set. And by the finite subadditivity property of elementary measure, we have \\begin{align} m(E)+m(F)\u0026=\\sup_{A_1\\subset E,A_1\\text{ elementary}}m(A_1)+\\sup_{A_2\\subset E,A_2\\text{ elementary}}m(A_2) \\\\ \u0026\\geq\\sup_{A_1\\subset E,A_2\\subset F;A_1,A_2\\text{ elementary}}m(A_1\\cup A_2) \\\\ \u0026=m(E\\cup F)=m(E\\cup F) \\end{align} Translation invariance.\nBy the translation invariance property of elementary measure, for any $x\\in\\mathbb{R}^d$, the Jordan inner measure of $E+x$ can be written as \\begin{align} m_{*,(J)}(E+x)\u0026=\\sup_{A\\subset E+x,A\\text{ elementary}}m(A) \\\\ \u0026=\\sup_{A\\subset E+x,A\\text{ elementary}}m(A-x) \\\\ \u0026=\\sup_{A-x\\subset E,A-x\\text{ elementary}}m(A-x)=m(E) \\end{align} Similarly, we also have the Jordan outer measure of $E+x$ is also equal to the Jordan measure of $E$ \\begin{equation} m^{*,(J)}(E+x)=m(E) \\end{equation} Hence, \\begin{equation} m_{*,(J)}(E+x)=m^{*,(J)}(E+x)=m(E), \\end{equation} or in other words, $E+x$ is Jordan measurable with $m(E+x)=m(E)$. Remark 13 (Regions under graphs are Jordan measurable)\nLet $B$ be a closed box in $\\mathbb{R}^d$, and let $f:B\\to\\mathbb{R}$ be a continuous function. Then\nThe graph $\\{(x,f(x)):x\\in B\\}\\subset\\mathbb{R}^{d+1}$ is Jordan measurable in $\\mathbb{R}^{d+1}$ with Jordan measure zero. The set $\\{(x,t):x\\in B;0\\leq t\\leq f(x)\\}\\subset\\mathbb{R}^{d+1}$ is Jordan measurable. Proof\nFor any closed box $C\\in\\mathbb{R}^d$, we have $\\{(x,f(x)):x\\in C\\}\\subset\\mathbb{R}^{d+1}$ with $f:C\\to\\mathbb{R}$ is a compact set. And when $f$ continuous in a compact set we also have $f$ is uniformly continuous[^1], which means for any $\\varepsilon\u003e0$, there exists $\\delta$ such that for every $x,y\\in C$ \\begin{equation} \\vert f(x)-f(y)\\vert\u003c\\varepsilon, \\end{equation} with $\\vert x-y\\vert\u003c\\delta$. Therefore, we can divide $C$ into finitely many almost disjoint boxes $C_1,\\ldots,C_n$ such that $\\vert x_i-y_i\\vert\u003c\\delta$ for every $x_i,y_i\\in C_i$ and for any $\\varepsilon\u003e0$ \\begin{equation} \\vert f(x_i)-f(y_i)\\vert\u003c\\varepsilon \\end{equation} Moreover, for each such box $C_i$ with center of the box $x_i$ we also have \\begin{equation} \\left\\{(x,f(x)):x\\in C_i\\right\\}\\subset C_i\\times\\left(f(x_i)-\\varepsilon,f(x_i)+\\varepsilon\\right) \\end{equation} Therefore \\begin{equation} \\hspace{-0.7cm}\\left\\{(x,f(x)):x\\in C\\right\\}=\\bigcup_{i=1}^{n}\\left\\{(x,f(x)):x\\in C_i\\right\\}\\subset\\bigcup_{i=1}^{n}C_i\\times\\left(f(x_i)-\\varepsilon,f(x_i)+\\varepsilon\\right) \\end{equation} With this result, and by the monotonicity, finite additivity of elementary measure, we have the Jordan outer measure of the graph $\\{(x,f(x)):x\\in B\\}\\subset\\mathbb{R}^{d+1}$ can be written as \\begin{align} m^{*,(J)}\\left(\\{(x,f(x)):x\\in B\\}\\right)\u0026=\\inf_{C\\supset B,C\\text{ closed box}}m\\left(\\left\\{(x,f(x)):x\\in C\\right\\}\\right) \\\\ \u0026\\leq m^{d+1}\\left(\\bigcup_{i=1}^{n}C_i\\times\\left(f(x_i)-\\varepsilon,f(x_i)+\\varepsilon\\right)\\right) \\\\ \u0026=\\sum_{i=1}^{n}m^d(C_i)\\times m^1\\left(\\left(f(x_i)-\\varepsilon,f(x_i)+\\varepsilon\\right)\\right) \\\\ \u0026=2n\\varepsilon m^d(C)\u003c2n\\varepsilon\\delta \\end{align} And since $\\varepsilon\u003e0$ arbitrarily, we finally obtain \\begin{equation} m^{*,(J)}\\left(\\{(x,f(x)):x\\in B\\}\\right)=0 \\end{equation} Plus that, since \\begin{equation} m^{*,(J)}\\left(\\{(x,f(x)):x\\in B\\}\\right)\\geq m_{*,(J)}\\left(\\{(x,f(x)):x\\in B\\}\\right)\\geq 0, \\end{equation} we have that \\begin{equation} m^{*,(J)}\\Big(\\big\\{(x,f(x)):x\\in B\\big\\}\\Big)=m_{*,(J)}\\Big(\\big\\{(x,f(x)):x\\in B\\big\\}\\Big)=0, \\end{equation} or in other words, the graph $\\left(\\{(x,f(x)):x\\in B\\}\\right)$ is Jordan measurable on $\\mathbb{R}^{d+1}$ with Jordan measure zero. Let $E=\\big\\{(x,t):x\\in B;0\\leq t\\leq f(x)\\big\\}$ and let $I$, $O$ be sets defined as for an arbitrary $\\varepsilon\u003e0$ \\begin{align} I\u0026=\\left\\{(x,t):x\\in B,0\\leq t\\leq f(x)-\\frac{\\varepsilon}{2}\\right\\}=B\\times\\left[0,f(x)-\\frac{\\varepsilon}{2}\\right], \\\\ O\u0026=\\left\\{(x,t):x\\in B,0\\leq t\\leq f(x)+\\frac{\\varepsilon}{2}\\right\\}=B\\times\\left[0,f(x)+\\frac{\\varepsilon}{2}\\right] \\end{align} Therefore, it follows immediately that $I\\subset E\\subset O$ and moreover \\begin{align} m^{d+1}(O\\backslash I)\u0026=m^{d+1}\\left(B\\times\\left[0,f(x)+\\frac{\\varepsilon}{2}\\right]\\backslash B\\times\\left[0,f(x)-\\frac{\\varepsilon}{2}\\right]\\right) \\\\ \u0026=m^d(B)\\times m^1\\left(\\left[0,f(x)+\\frac{\\varepsilon}{2}\\right]\\backslash\\left[0,f(x)-\\frac{\\varepsilon}{2}\\right]\\right) \\\\ \u0026=m^d(B)\\times\\varepsilon \\end{align} And since $\\varepsilon\u003e0$ arbitrarily, we can claim that $E$ is Jordan measurable. Remark 14\nAll open and closed Euclidean balls, $B(x,r)\\doteq\\{y\\in\\mathbb{R}^d:\\vert y-x\\vert\u003c r\\}$ and $\\overline{B(x,r)}\\doteq\\{y\\in\\mathbb{R}^d:\\vert y-x\\vert\\leq r\\}$, in $\\mathbb{R}^d$ are Jordan measurable, with Jordan measure $c_dr^d$ for some constant $c_d$ depending only on $d$. Establish the crude bounds \\begin{equation} \\left(\\frac{2}{\\sqrt{d}}\\leq c_d\\leq 2^d\\right) \\end{equation} Jordan null sets A Jordan null set is a Jordan measurable set of Jordan measure zero. We have that any subset of a Jordan null set is also a Jordan null set.\nProof\nLet $E\\subset F$ where F is a Jordan null set. Also let $A\\subset E$, it follows that $A\\subset F$, and hence \\begin{equation} m(A)\\leq m_{*,(J)}(F)=0 \\end{equation} Since $m(E)=0$, we can choose a set $B\\supset F$ such that $m(B)\\leq\\varepsilon$ for $\\varepsilon\u0026gt;0$ arbitrarily. Thus, $E\\subset B$ and moreover \\begin{equation} m(B\\backslash A)\\leq\\varepsilon, \\end{equation} which claims that $E$ is Jordan measurable with measurable of zero since $m(E)\\leq m(F)=0$. Or in other words, $E$ is also a Jordan null set.\nRemark 15\nFor any Jordan measurable set $E\\subset\\mathbb{R}^d$, its Jordan measure can be written as \\begin{equation} m(E)\\doteq\\lim_{N\\to\\infty}\\frac{1}{N^d}\\#\\left(E\\cup\\frac{1}{N}\\mathbb{Z}^d\\right) \\end{equation}\nProof\nMetric entropy formulation of Jordan measurability A dyadic cube is defined to be a half-open box of the form \\begin{equation} \\left[\\frac{i_1}{2^n},\\frac{i_1+1}{2^n}\\right)\\times\\ldots\\times\\left(\\frac{i_d}{2^n},\\frac{i_d+1}{2^n}\\right], \\end{equation} for some integers $n,i_1,\\ldots,i_d$. Let $E\\subset\\mathbb{R}^d$ be a bounded set. For each integer $n$, let $\\mathcal{E}_*(E,2^{-n})$ denote the number of dyadic cubes of sidelength $2^{-n}$ that are contained in $E$, and let $\\mathcal{E}^*(E,2^{-n})$ be the number of dyadic cubes of sidelength $2^{-n}$ that intersect $E$. Then $E$ is Jordan measurable iff \\begin{equation} \\lim_{n\\to\\infty}2^{-dn}(\\mathcal{E}^*(E,2^{-n}))-\\mathcal{E}_*(E, 2^{-n})=0, \\end{equation} in which case we have \\begin{equation} m(E)=\\lim_{n\\to\\infty}2^{-dn}\\mathcal{E}_*(E,2^{-n})=\\lim_{n\\to\\infty}2^{-dn}\\mathcal{E}^*(E,2^{-n}) \\end{equation}\nUniqueness of Jordan measure Let $d\\geq 1$ and let $m\u0026rsquo;:\\mathcal{J}(\\mathbb{R}^d)\\to\\mathbb{R}^+$ be a map from the collection of Jordan measurable subsets of $\\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity and translation invariance properties. Then there exists a constant $c\\in\\mathbb{R}^+$ such that \\begin{equation} m\u0026rsquo;(E)=cm(E), \\end{equation} for all Jordan measurable sets $E$. In particular, if we impose the additional normalization $m\u0026rsquo;([0,1)^d)=1$, then $m\u0026rsquo;\\equiv m$.\nProof\nFollow the same steps as the proof of the uniqueness of elementary measure, the argument above can easily be proved.\nRemark 16\nLet $d_1,d_2\\geq 1$, and let $E_1\\subset\\mathbb{R}^{d_1},E_2\\subset\\mathbb{R}^{d_2}$ be Jordan measurable sets. Then $E_1\\times E_2\\subset\\mathbb{R}^{d_1+d_2}$ is also Jordan measurable, and $m^{d_1+d_2}(E_1\\times E_2)=m^{d_1}(E_1)\\times m^{d_2}(E_2)$.\nProof\nLet $A_1\\subset E_1$ such that $A_1$ is elementary and \\begin{equation} m^{d_1}(A_1)=\\sup_{A\\subset E_1,A\\text{ elementary}}m(A)=m_{*,(J)}(E_1)=m^{d_1}(E_1) \\end{equation} Let $B_1\\supset E_1$ such that $B_1$ is elementary and \\begin{equation} m^{d_1}(B_1)=\\inf_{B\\supset E_1,B\\text{ elementary}}m(B)=m^{*,(J)}(E_1)=m^{d_1}(E_1), \\end{equation} which implies that \\begin{equation} m^{d_1}(A_1)=m^{d_1}(B_1)=m^{d_1}(E_1) \\end{equation} Analogously, we define $A_2\\subset E_2\\subset B_2$ such that \\begin{align} m^{d_2}(A_2)\u0026amp;=\\sup_{A\\subset E_2,A\\text{ elementary}}m(A)=m_{*,(J)}(E_2)=m^{d_2}(E_2) \\\\ m^{d_2}(B_2)\u0026amp;=\\inf_{B\\supset E_2,B\\text{ elementary}}m(B)=m^{*,(J)}(E_2)=m^{d_1}(E_2) \\end{align} And thus, we also have \\begin{equation} m^{d_2}(A_2)=m^{d_2}(B_2)=m^{d_2}(E_2) \\end{equation} On the one hand, with these definitions, we have \\begin{equation} m^{d_1+d_2}(A_1\\times A_2)=\\sup_{A\\subset E_1\\times E_2,A\\text{ elementary}}=m_{*,(J)}(E_1\\times E_2)\\label{eq:remark15.1} \\end{equation} and \\begin{equation} m^{d_1\\times d_2}(B_1\\times B_2)=\\sup_{B\\supset E_1\\times E_2,A\\text{ elementary}}=m^{*,(J)}(E_1\\times E_2)\\label{eq:remark15.2} \\end{equation} On the other hands, By remark 11, we have that $A_1\\times A_2$ and $B_1\\times B_2$ are also elementary sets and \\begin{align} m^{d_1}(A_1)\\times m^{d_2}(A_2)\u0026amp;=m^{d_1+d_2}(A_1\\times A_2)\\label{eq:remark15.3} \\\\ m^{d_1}(B_1)\\times m^{d_2}(B_2)\u0026amp;=m^{d_1+d_2}(B_1\\times B_2)\\label{eq:remark15.4} \\end{align} From \\eqref{eq:remark15.1}, \\eqref{eq:remark15.2}, \\eqref{eq:remark15.3} and \\eqref{eq:remark15.4}, we can claim that $E_1\\times E_2$ is Jordan measurable and \\begin{equation} m^{d_1}(E_1)\\times m^{d_2}(E_2)=m^{d_1+d_2}(E_1\\times E_2) \\end{equation}\nTopological of Jordan measurability Let $E\\subset\\mathbb{R}^d$ be a bounded set\n$E$ and the closure $\\bar{E}$ of $E$ have the same Jordan outer measure. $E$ and the interior $E^\\circ$ of $E$ have the same Jordan inner measure. $E$ is Jordan measurable iff the topological boundary $\\partial E$ of $E$ has Jordan outer measure zero. The bullet-riddled square $[0,1]^2\\backslash\\mathbf{Q}^2$, and set of bullets $[0,1]^2\\cup Q^2$, both have Jordan inner measure zero and Jordan outer measure one. In particular, both sets are not Jordan measurable. Proof\nSince $E\\subset\\overline{E}$, it is easily seen that \\begin{equation} m^{*,(J)}(E)\\leq m^{*,(J)}(\\overline{E}) \\end{equation} Thus, the problem remains to prove that \\begin{equation} m^{*,(J)}(E)\\geq m^{*,(J)(\\overline{E})} \\end{equation} Let $B_1,\\ldots,B_N$ be $N$ disjoint boxes such that Carathéodory type property Let $E\\subset\\mathbb{R}^d$ be a bounded set, and $F\\subset\\mathbb{R}^d$ be an elementary set. Then we have that \\begin{equation} m^{*,(J)}(E)=m^{*,(J)}(E\\cap F)+m^{*,(J)}(E\\backslash F) \\end{equation}\nConnection with the Riemann integral We then consider the relationship between Jordan measure and the Rieman integral, or the equivalent Darboux integral.\nRiemann integrability Let $[a,b]$ be an interval of positive length, and $f:[a,b]\\to\\mathbb{R}$ be a function. A tagged partition \\begin{equation} \\mathcal{P}=\\left(\\left(x_0,x_1,\\dots,x_n\\right),\\left(x_1^{*},\\dots,x_n^{*}\\right)\\right) \\end{equation} of $[a,b]$ is a finite sequence of real numbers $a=x_0\u0026lt; x_1\u0026lt;\\dots\u0026lt; x_n=b$, together with additional numbers $x_{i-1}\\leq x_i^{*}\\leq x_i$ for each $i=1,\\dots,n$. Let $\\delta x_i\\doteq x_i-x_{i-1}$, the quantity \\begin{equation} \\Delta(\\mathcal{P})\\doteq\\sup_{1\\leq i\\leq n}\\delta x_i \\end{equation} is called the norm of the tagged partition. The Riemann sum $\\mathcal{R}(f,\\mathcal{P})$ of $f$ w.r.t the tagged partition $\\mathcal{P}$ is defined as \\begin{equation} \\mathcal{R}(f,\\mathcal{P})\\doteq\\sum_{i=1}^{n}f(x_i^{*})\\delta x_i \\end{equation} we say that $f$ is Riemann integrable on $[a,b]$ if there exists a real number, denoted as $\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$ and referred to as the Riemann integral on $[a,b]$, for which we have \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\lim_{\\Delta\\mathcal{P}\\to 0}\\mathcal{R}(f,\\mathcal{P}), \\end{equation} by which we mean that for every $\\varepsilon\u0026gt;0$ there exists $\\delta\u0026gt;0$ such that \\begin{equation} \\left\\vert\\mathcal{R}(f,\\mathcal{P})-\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\right\\vert\\leq\\varepsilon, \\end{equation} for every tagged partition $\\mathcal{P}$ with $\\Delta(\\mathcal{P})\\leq\\delta$.\nPiecewise constant functions Let $[a,b]$ be an interval. a piecewise constant function $f:[a,b]\\to\\mathbb{R}$ is a function for which there exists a partition of $[a,b]$ into infinitely many intervals $I_1,\\dots,I_n$ such that $f$ is equal to a constant $c_i$ on each of the intervals $I_i$. Then, the expression \\begin{equation} \\sum_{i=1}^{n}c_i\\vert I_i\\vert \\end{equation} is independent of the choice of partition used to demonstrate the piecewise constant nature of $f$. We denote this quantity as $\\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$, and refer it to as piecewise constant integral of $f$ on $[a,b]$.\nProof\nConsider two partitions of the interval $[a,b]$ into finitely many intervals $(I_i)_{i=1,\\ldots,n}=I_1,\\ldots,I_n$ and $(J_i)_{i=1,\\ldots,m}=J_1,\\ldots,J_m$ such that: \\begin{align} f(x)\u0026amp;=c_i,\\hspace{1cm}\\forall x\\in I_i, \\\\ f(x)\u0026amp;=d_i,\\hspace{1cm}\\forall x\\in J_i \\end{align} Thus, we have that: \\begin{equation} c_i=d_j,\\hspace{1cm}\\forall x\\in\\left(I_i\\cap J_j\\right) \\end{equation} With this result, we have: \\begin{align} \\sum_{i=1}^{n}c_i\\vert I_i\\vert\u0026amp;=\\sum_{i=1}^{n}c_i\\left\\vert\\bigcup_{j=1}^{m}\\left(I_i\\cap J_j\\right)\\right\\vert \\\\ \u0026amp;=\\sum_{i=1}^{n}\\sum_{j=1}^{m}c_i\\left\\vert I_i\\cap J_j\\right\\vert \\\\ \u0026amp;=\\sum_{j=1}^{m}\\sum_{i=1}^{n}d_j\\left\\vert I_i\\cap J_j\\right\\vert \\\\ \u0026amp;=\\sum_{j=1}^{m}d_j\\left\\vert\\bigcup_{i=1}^{n}\\left(J_j\\cap I_i\\right)\\right\\vert \\\\ \u0026amp;=\\sum_{j=1}^{m}d_j\\vert J_j\\vert, \\end{align} which claims the independence of the choices of partition of $f$.\nBasic properties of piecewise constant integral Let $[a,b]$ be an interval, and let $f,g:[a,b]\\to\\mathbb{R}$ be piecewise constant functions. Then\nLinearity. For any $c\\in\\mathbb{R}$, $cf$ and $f+g$ are piecewise constant functions, with \\begin{align} \\text{p.c.}\\int_{a}^{b}cf(x)\\hspace{0.1cm}dx\u0026=c\\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\\\\\\\ \\text{p.c.}\\int_{a}^{b}\\left(f(x)+g(x)\\right)\\hspace{0.1cm}dx\u0026=\\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx+\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{align} Monotonicity. If $f\\leq g$ pointwise, i.e., $f(x)\\leq g(x),\\forall x\\in[a,b]$, then \\begin{equation} \\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\leq\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{equation} Indicator. If $E$ is an elementary subset of $[a,b]$, then the indicator function $1_E:[a,b]\\to\\mathbb{R}$ (defined by setting $1_E(x)\\doteq 1$ if $x\\in E$ and 0 otherwise) is piecewise constant, and \\begin{equation} \\text{p.c.}\\int_{a}^{b}1_E(x)\\hspace{0.1cm}dx=m(E) \\end{equation} Proof\nLinearity\nFor any $c\\in\\mathbb{R}$, we have: \\begin{equation} \\text{p.c.}\\int_{a}^{b}cf(x)\\hspace{0.1cm}dx=\\sum_{i=1}^{n}cc_i\\vert I_i\\vert=c\\sum_{i=1}^{n}c_i\\vert I_i\\vert=c\\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\end{equation} From the partitioning independence of piecewise constant functions, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\\ldots,I_n$, such that \\begin{equation} f(x)=c_i,\\hspace{1cm}\\forall x\\in I_i, \\end{equation} and \\begin{equation} g(x)=d_i,\\hspace{1cm}\\forall x\\in I_i, \\end{equation} Thus, we have \\begin{align} \\text{p.c.}\\int_{a}^{b}f(x)+g(x)\\hspace{0.1cm}dx\u0026=\\sum_{i=1}^{n}\\left(c_i+d_i\\right)\\vert I_i\\vert \\\\ \u0026=\\sum_{i=1}^{n}c_i\\vert I_i\\vert+\\sum_{i=1}^{n}d_i\\vert I_i\\vert \\\\ \u0026=\\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx+\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{align} Monotonicity\nAnalogy to the above proof, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\\ldots,I_n$, such that \\begin{align} f(x)\u0026=c_i,\\hspace{1cm}\\forall x\\in I_i, \\\\ g(x)\u0026=d_i,\\hspace{1cm}\\forall x\\in I_i, \\end{align} Since $f\\leq g$ pointwise, in any interval $I_i$, we also have that $c_i=f(x)\\leq g(x)=d_i$. Therefore, \\begin{equation} \\text{p.c.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\sum_{i=1}^{n}c_i\\vert I_i\\vert\\leq\\sum_{i=1}^{n}d_i\\vert I_i\\vert=\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{equation} Indicator\nSince $E\\subset[a,b]\\subset\\mathbb{R}$ is an elementary set, we can represent the elementary measure $m(E)$ of set $E$ as \\begin{equation} m(E)=\\sum_{i=1}^{n}\\vert I_i\\vert \\end{equation} Therefore, for any $x\\in I_i$ for $i=1,\\ldots n$, we have that $1_E(x)=1$; and for any $x\\in[b-a]\\backslash E=\\bigcup_{j=1}^{m}J_j$, we get that $1_E(x)=0$, which lets $1_E$ satisfy the condition of a piecewise constant function.\nMoreover, we have that \\begin{equation} \\text{p.c.}\\int_{a}^{b}1_E(x)\\hspace{0.1cm}dx=\\sum_{i=1}^{n}1\\vert I_i\\vert+\\sum_{j=1}^{m}0\\vert J_j\\vert=\\sum_{i=1}^{n}\\vert I_i\\vert=m(E) \\end{equation} Darboux integral Let $[a,b]$ be an integral, and let $f:[a,b]\\to\\mathbb{R}$ be a bounded function. The lower Darboux integral of $f$ on $[a,b]$, denoted as $\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx$, is defined as \\begin{equation} \\underline{\\int_a^b}f(x)\\hspace{0.1cm}dx\\doteq\\sup_{g\\leq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx, \\end{equation} where $g$ ranges over all piecewise constant functions that are pointwise bounded above by $f$ (the hypothesis that $f$ is bounded ensures that the supremum is over a non-empty set).\nSimilarly, we can define the upper Darboux integral of $f$ on $[a,b]$, denoted as $\\overline{\\int_a^b}f(x)\\hspace{0.1cm}dx$, as \\begin{equation} \\overline{\\int_a^b}f(x)\\hspace{0.1cm}dx\\doteq\\inf_{h\\geq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}h(x)\\hspace{0.1cm}dx \\end{equation} It is easily seen that $\\underline{\\int_a^b}f(x)\\hspace{0.1cm}dx\\leq\\overline{\\int_a^b}f(x)\\hspace{0.1cm}dx$. The equality holds when $f$ is Darboux integrable, and we refer to this quantity as Darboux integral of $f$ on $[a,b]$.\nNote that the upper and lower Darboux integrals are related by \\begin{equation} \\overline{\\int_a^b}-f(x)\\hspace{0.1cm}dx=-\\underline{\\int_a^b}f(x)\\hspace{0.1cm}dx \\end{equation}\nEquivalence of Riemann integral and Darboux integral Let $[a,b]$ be an interval, and $f:[a,b]\\to\\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff it is Darboux integrable, in which case the Riemann integrals and Darboux integrals are the same.\nProof\nGiven $f$ is Riemann integrable on $[a,b]$, we have that for any $\\varepsilon\u0026gt;0$, there exists a tagged partition $((I_1,\\ldots,I_n),(x_1^*,\\ldots,x_n^*))$ of $[a,b]$ with $x_i^*\\in I_i$ such that \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i^*)\\vert I_i\\vert-\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\right\\vert\\leq\\varepsilon \\end{equation} For each interval $I_i$, there exist an $x_i^{(1)}$ such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\inf_{x\\in I_i}f(x)\\leq f(x_i^{(1)})\u0026lt;\\inf_{x\\in I_i}f(x)+\\frac{\\varepsilon}{n} \\end{equation} Thus, for any $\\varepsilon\u0026gt;0$ we obtain \\begin{equation} \\sum_{n=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\leq\\sum_{i=1}^{n}f(x_i^{(1)})\\vert I_i\\vert\u0026lt;\\sum_{i=1}^{n}\\inf_{x\\in I_i}f(x)+\\varepsilon, \\end{equation} which implies that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i^{(1)})\\vert I_i\\vert-\\sum_{n=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\right\\vert\u0026lt;\\varepsilon\\label{eq:erdi.1} \\end{equation} Since $f$ is Riemann integrable on $[a,b]$, as $\\sup_{i=1,\\ldots,n}\\to 0$, we have \\begin{equation} \\sum_{i=1}^{n}f(x_i^{(1)})\\vert I_i\\vert\\to\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\end{equation} Combining with \\eqref{eq:erdi.1}, we have that as $\\sup_{i=1,\\ldots,n}\\vert I_i\\vert\\to 0$ \\begin{equation} \\sum_{n=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\to\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\end{equation} Moreover, we also have that \\begin{equation} \\sum_{n=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\leq\\sup_{g\\leq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx=\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx, \\end{equation} which is the lower Darboux integral of $f$ on $[a,b]$. Thus, \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\leq\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx\\label{eq:erdi.2} \\end{equation} Similarly, applying the same procedure as above, we also have that on each $I_i$ there exists an $x_i^{(2)}$ such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i^{(2)})\\vert I_i\\vert-\\sum_{n=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\right\\vert\u0026lt;\\varepsilon \\end{equation} Since $f$ is Riemann integrable on $[a,b]$, as $\\sup_{i=1,\\ldots,n}\\vert I_i\\vert\\to 0$, we have \\begin{equation} \\sum_{i=1}^{n}f(x_i^{(2)})\\vert I_i\\vert\\to\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\end{equation} Therefore, \\begin{equation} \\sum_{n=1}^{n}\\sup_{x\\in I_i}f(x)\\vert I_i\\vert\\to\\int_{a}^{b}f(x)\\hspace{0.1cm}dx, \\end{equation} as $\\sup_{i=1,\\ldots,n}\\vert I_i\\vert\\to 0$. Additionally, we also have \\begin{equation} \\sum_{i=1}^{n}\\sup_{x\\in I_i}f(x)\\vert I_i\\vert\\geq\\inf_{h\\geq f, \\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}h(x)\\hspace{0.1cm}dx=\\overline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx, \\end{equation} which is the upper Darboux integral of $f$ on $[a,b]$. And hence \\begin{equation} \\overline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx\\leq\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\label{eq:erdi.3} \\end{equation} From \\eqref{eq:erdi.2} and \\eqref{eq:erdi.3}, we end up with \\begin{equation} \\overline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx\\leq\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\leq\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx, \\end{equation} which happens iff \\begin{equation} \\overline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx=\\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx, \\end{equation} which claims that $f$ is Darboux integrable on $[a,b]$, with the Darboux integral is exactly the Riemann integral $\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$. Given $f$ is Darboux integrable on $[a,b]$, we have that the upper and lower Darboux integrals are equal, and are equal to the Darboux integral of $f$ on $[a,b]$ which we denote as $\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\in\\mathbb{R}$. \\begin{equation} \\underline{\\int_a^b}f(x)\\hspace{0.1cm}dx=\\overline{\\int_a^b}f(x)\\hspace{0.1cm}dx=\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\end{equation} By definition of the lower Darboux integral, there exists a piecewise constant function $g(x)$ bounded above by $f$ (i.e., $g\\leq f$ piecewise), such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx\u0026gt;\\underline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx-\\varepsilon=\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx-\\varepsilon\\label{eq:erdi.4} \\end{equation} Likewise, by definition of the upper Darboux integral, there exists a piecewise constant function $h(x)$ bounded below by $f$ (i.e., $h\\geq f$ piecewise), such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\text{p.c.}\\int_{a}^{b}h(x)\\hspace{0.1cm}dx\u0026lt;\\overline{\\int_{a}^{b}}f(x)\\hspace{0.1cm}dx+\\varepsilon=\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx+\\varepsilon\\label{eq:erdi.5} \\end{equation} From the independence of choice of partition of piecewise constant functions $g$ and $h$, there exists a partition $I_1,\\ldots,I_n$ such that \\begin{align} g(x)\u0026amp;=c_i,\\hspace{1cm}\\forall x\\in I_i, \\\\ h(x)\u0026amp;=d_i,\\hspace{1cm}\\forall x\\in I_i \\end{align} and \\begin{align} \\text{p.c.}\\int_{a}^{b}g(x)\\hspace{0.1cm}dx\u0026amp;=\\sum_{i=1}^{n}c_i\\vert I_i\\vert,\\label{eq:erdi.6} \\\\ \\text{p.c.}\\int_{a}^{b}h(x)\\hspace{0.1cm}dx\u0026amp;=\\sum_{i=1}^{n}d_i\\vert I_i\\vert,\\label{eq:erdi.7} \\end{align} then it follows immediately that $c_i\\leq d_i$. And since $g\\leq f\\leq h$ piecewise, on each interval $I_i$, we can find a $x_i^*$ such that $c_i\\leq f(x_i^*)\\leq d_i$. Additionally, combining with \\eqref{eq:erdi.4}, \\eqref{eq:erdi.5}, \\eqref{eq:erdi.6} and \\eqref{eq:erdi.7}, we have that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx-\\varepsilon\u0026lt;\\sum_{i=1}^{n}c_i\\vert I_i\\vert\\leq\\sum_{i=1}^{n}f(x_i^*)\\vert I_i\\vert\\leq\\sum_{i=1}^{n}d_i\\vert I_i\\vert\u0026lt;\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx+\\varepsilon \\end{equation} Therefore, for any $\\varepsilon\u0026gt;0$, we have \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i^*)\\vert I_i\\vert-\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\right\\vert\u0026lt;\\varepsilon, \\end{equation} which claims that $f$ is Riemann integrable on $[a,b]$ with $\\text{d.}\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$ is the Riemann integral of $f$. Example\nAny continuous function $f:[a,b]\\to\\mathbb{R}$ is Riemann integrable. More generally, any bounded, piecewise continuous function1 $f:[a,b]\\to\\mathbb{R}$ is Riemann integrable.\nSolution\nConsider a partition of piecewise continuous f on $[a,b]$ into finitely many intervals $I_1,\\ldots,I_n$. Using the procedure that we used for the above proof, we have that on each interval $I_i$, there exists an $x_i$ such that for any $\\varepsilon\u0026gt;0$ \\begin{equation} \\inf_{x\\in I_i}f(x)\\leq f(x_i)\u0026lt;\\inf_{x\\in I_i}f(x)+\\frac{\\varepsilon}{n} \\end{equation} Hence, \\begin{equation} \\sum_{i=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\leq\\sum_{i=1}^{n}f(x_i)\\vert I_i\\vert\u0026lt;\\sum_{i=1}^{n}\\inf_{x\\in I_i}f(x)+\\varepsilon, \\end{equation} which implies that \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i)\\vert I_i\\vert-\\sum_{i=1}^{n}\\inf_{x\\in I_i}f(x)\\vert I_i\\vert\\right\\vert\u0026lt;\\varepsilon, \\end{equation} which implies that $f$ is Riemann integrable on $[a,b]$.\nBasic properties of Riemann integral Let $[a,b]$ be an interval, and let $f,g:[a,b]\\to\\mathbb{R}$ be Riemann integrable. We then have that\nLinearity. For any $c\\in\\mathbb{R}$, $cf$ and $f+g$ are Riemann integrable, with \\begin{align} \\int_{a}^{b}cf(x)\\hspace{0.1cm}dx\u0026=c\\int_{a}^{b}f(x)\\hspace{0.1cm}dx \\\\ \\int_{a}^{b}\\big(f(x)+g(x)\\big)\\hspace{0.1cm}dx\u0026=\\int_{a}^{b}f(x)\\hspace{0.1cm}dx+\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{align} Monotonicity. If $f\\leq g$ pointwise, then \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\leq\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{equation} Indicator. If $E$ is a Jordan measurable of $[a,b]$, then the indicator function $1_E:[a,b]\\to\\mathbb{R}$ is Riemann integrable, and \\begin{equation} \\int_{a}^{b}1_E(x)\\hspace{0.1cm}dx=m(E) \\end{equation} Proof\nLinearity. Given $f$ Riemann integrable on $[a,b]$, we have that there exists a tagged partition $\\mathcal{P}=((I_1,\\ldots,I_n),(x_1^*,\\ldots,x_n^*));(x_i^*\\in I_i)$ of $[a,b]$ such that for any $\\varepsilon\u003e0$, we have \\begin{equation} \\left\\vert\\sum_{i=1}^{n}f(x_i^*)\\vert I_i\\vert-\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\right\\vert\\leq\\varepsilon \\end{equation} Thus, for any $c\\in\\mathbb{R}$ \\begin{equation} \\left\\vert\\sum_{i=1}^{n}cf(x_i^*)\\vert I_i\\vert-\\int_{a}^{b}cf(x)\\hspace{0.1cm}dx\\right\\vert\\leq\\vert c\\vert\\varepsilon=\\varepsilon', \\end{equation} where $\\varepsilon'\u003e0$ arbitrarily. This implies that $cf$ is Riemann integrable on $[a,b]$ with Riemann integral $\\int_{a}^{b}cf(x)\\hspace{0.1cm}dx=c\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$. Given $f$ Riemann integrable on $[a,b]$, then $f$ is also Darboux integrable on $[a,b]$, which means \\begin{align} \\hspace{-1cm}\\sup_{f_1\\leq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}f_1(x)\\hspace{0.1cm}dx\u0026=\\inf_{f_2\\geq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}f_2(x)\\hspace{0.1cm}dx \\\\ \u0026\\hspace{1cm}=\\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\label{eq:rip.1} \\end{align} Similarly, $g$ Riemann integrable on $[a,b]$ implies that $g$ is also Darboux integrable, or in particular \\begin{align} \\hspace{-1cm}\\sup_{g_1\\leq g,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}g_1(x)\\hspace{0.1cm}dx\u0026=\\inf_{g_2\\geq g,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}g_2(x)\\hspace{0.1cm}dx \\\\ \u0026\\hspace{1cm}=\\int_{a}^{b}g(x)\\hspace{0.1cm}dx\\label{eq:rip.2} \\end{align} By the linearity property of piecewise constant functions, combined with \\eqref{eq:rip.1} and \\eqref{eq:rip.2}, we obtain \\begin{align} \u0026\\sup_{f_1\\leq f,g_1\\leq g,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}f_1(x)+g_1(x)\\hspace{0.1cm}dx \\\\ \u0026\\hspace{2cm}=\\inf_{f_2\\geq f,g_2\\geq g,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}f_2(x)+g_2(x)\\hspace{0.1cm}dx \\\\ \u0026\\hspace{2cm}=\\int_{a}^{b}f(x)+g(x)\\hspace{0.1cm}dx, \\end{align} which claims the Riemann integrability of $f+g$ on $[a,b]$. Monotonicity.\nGiven $f$ and $g$, we obtain two consequential equations \\eqref{eq:rip.1} and \\eqref{eq:rip.2}. And since $f\\leq g$ pointwise we have that \\begin{equation} \\sup_{f_1\\leq f,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}f_1(x)\\hspace{0.1cm}dx\\leq\\sup_{g_1\\leq g,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}g_1(x)\\hspace{0.1cm}dx \\end{equation} or \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx\\leq\\int_{a}^{b}g(x)\\hspace{0.1cm}dx \\end{equation} Indicator.\nGiven $E\\subset [a,b]$ is Jordan measurable, we have \\begin{equation} \\sup_{A\\subset E,A\\text{ elementary}}m(A)=\\inf_{B\\supset E,B\\text{ elementary}}m(B)=m(E)\\label{eq:rip.3} \\end{equation} Recall that we have proved that for any elementary set $E'\\subset[a,b]$, the indicator function $1_{E'}:[a,b]\\to\\mathbb{R}$ is also piecewise constant with \\begin{equation} \\text{p.c.}\\int_{a}^{b}1_{E'}(x)\\hspace{0.1cm}dx=m(E') \\end{equation} Moreover for any $A\\subset E$, we have $1_A(x)\\leq 1_E(x)$; and for any $B\\supset E$, we have $1_B(x)\\geq 1_E(x)$. Therefore the lower Darboux integral of $1_E$ on $[a,b]$ can be defined as \\begin{equation} \\hspace{-0.3cm}\\underline{\\int_{a}^{b}}1_E(x)\\hspace{0.1cm}dx=\\sup_{1_A\\leq 1_E,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}1_A(x)\\hspace{0.1cm}dx=\\sup_{A\\subset E,A\\text{ elementary}}m(A)\\label{eq:rip.4} \\end{equation} And the upper Darboux integral of $1_E$ on $[a,b]$ can also be defined as \\begin{equation} \\hspace{-0.3cm}\\overline{\\int_{a}^{b}}1_E(x)\\hspace{0.1cm}dx=\\inf_{1_B\\geq 1_E,\\text{ piecewise constant}}\\text{p.c.}\\int_{a}^{b}1_B(x)\\hspace{0.1cm}dx=\\inf_{B\\supset E,B\\text{ elementary}}m(B)\\label{eq:rip.5} \\end{equation} Combine \\eqref{eq:rip.3}, \\eqref{eq:rip.4} and \\eqref{eq:rip.5}, we have \\begin{equation} \\underline{\\int_{a}^{b}}1_E(x)\\hspace{0.1cm}dx=\\overline{\\int_{a}^{b}}1_E(x)\\hspace{0.1cm}dx=m(E), \\end{equation} which means $1_E$ is Darboux integrable on $[a,b]$ with the Darboux integrable $m(E)$. By the equivalence of Riemann and Darboux integral, $1_E$ is also Riemann integrable on $[a,b]$ with the Riemann integral \\begin{equation} \\int_{a}^{b}1_E(x)\\hspace{0.1cm}dx=m(E) \\end{equation} These properties uniquely define the Riemann integral, in the sense that the functional $f\\mapsto\\int_{a}^{b}f(x)\\hspace{0.1cm}dx$ is the only map from the space of Riemann integrable functions on $[a,b]$ to $\\mathbb{R}$ which obeys all of these above properties.\nArea interpretation of the Riemann integral Let $[a,b]$ be an interval, and let $f:[a,b]\\to\\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff the sets $E_+\\doteq\\{(x,t):x\\in[a,b];0\\leq t\\leq f(x)\\}$ and $E_-\\doteq\\{(x,t):x\\in[a,b];f(x)\\leq t\\leq 0\\}$ are both Jordan measurable in $R^2$, in which case we have \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx=m^2(E_+)-m^2(E_-), \\end{equation} where $m^2$ denotes two-dimensional Jordan measure.\nProof\nReferences [1] Terence Tao. An introduction to measure theory. Graduate Studies in Mathematics, vol. 126, 2011.\n[2] Elias M. Stein \u0026amp; Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007\nFootnotes A function $f:[a,b]\\to\\mathbb{R}$ is piecewise continuous if we can partition $[a,b]$ into finitely many intervals, such that $f$ is continuous on each interval.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/measure-theory/measure-theory-p1/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNote I of the measure theory series. Materials are mostly taken from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#taos-book\"\u003eTao\u0026rsquo;s book\u003c/a\u003e, except for some needed notations extracted from \u003ca href=\"https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#steins-book\"\u003eStein\u0026rsquo;s book\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Measure theory - I: Elementary measure, Jordan measure \u0026 the Riemann integral"},{"content":" Connection between Likelihood ratio policy gradient method and Importance sampling method.\nPreliminaries An infinite-horizon discounted Markov Decision Process (MDP) is defined as the tuple $(\\mathcal{S},\\mathcal{A},P,r,\\rho_0,\\gamma)$, where\n$\\mathcal{S}$ is a finite set of states, or state space. $\\mathcal{A}$ is a finite set of actions, or action space. $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to\\mathbb{R}$ is the transition probability distribution, i.e. $P(s,a,s\u0026rsquo;)=P(s\u0026rsquo;\\vert s,a)$ denotes the probability of transitioning to state $s\u0026rsquo;$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function. $\\rho_0:\\mathcal{S}\\to\\mathbb{R}$ is the distribution of the initial state $s_0$. $\\gamma\\in(0,1)$ is the discount factor. A (stochastic) policy, denoted $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$, is a mapping from states to probabilities of selecting each possible action.\nLet $\\eta(\\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\\pi$ thereafter \\begin{equation} \\eta(\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right], \\end{equation} where \\begin{equation} s_0\\sim\\rho_0(s_0),\\hspace{1cm}a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t) \\end{equation} For a policy $\\pi$, to measure how good it is to be at a state, or how good it is to take action $a$ at state $s$, we use state value function, denoted $V_\\pi(s)$, and action value function, referred $Q_\\pi(s,a)$. In particular, these value functions are defined as the expected return \\begin{align} V_\\pi(s_t)\u0026amp;=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\\\ Q_\\pi(s_t,a_t)\u0026amp;=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\end{align} where \\begin{equation} a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t)\\hspace{1cm}t\\geq 0 \\end{equation}\nLikelihood Ratio Policy Gradient Let $H$ denote the horizon of an MDP1. Consider likelihood ratio policy gradient problem, in which the policy $\\pi_\\theta$ is parameterized by a vector $\\theta\\in\\mathbb{R}^n$. The expected return of $\\pi_\\theta$ is then given by \\begin{equation} \\eta(\\pi_\\theta)=\\mathbb{E}_{P(\\tau;\\theta)}\\left[\\left.\\sum_{t=0}^{H-1}\\gamma^t r(s_t,a_t)\\right\\vert\\pi_\\theta\\right]=\\sum_{\\tau}P(\\tau;\\theta)R(\\tau),\\label{eq:lrp.1} \\end{equation} where\n$P(\\tau;\\theta)$ is the probability distribution induced by the policy $\\pi_\\theta$, i.e. $s_t$ $\\tau=(s_0,a_0,s_1,a_1,\\ldots,s_H,a_H)$ are trajectories generated by rolls out, i.e. $\\tau\\sim P(\\tau;\\theta)$. $R(\\tau)$ is the discounted cumulative rewards along the trajectory $\\tau$, given as \\begin{equation} R(\\tau)=\\sum_{t=0}^{H-1}\\gamma^t r(s_t,a_t) \\end{equation} The likelihood ratio policy gradient performs a SGA (stochastic gradient ascent) over the policy parameter space $\\Theta$ to find a local optimum of $\\eta(\\pi_\\theta)$ by taking into account the policy gradient \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\nabla_\\theta\\sum_{\\tau}P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\sum_\\tau\\nabla_\\theta P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\sum_\\tau\\nabla_\\theta\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}\\nabla_\\theta P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\sum_{\\tau}P(\\tau;\\theta)\\nabla_\\theta\\log P(\\tau;\\theta)R(\\tau) \\\\ \u0026amp;=\\mathbb{E}_{P(\\tau;\\theta)}\\Big[\\nabla_\\theta P(\\tau;\\theta)R(\\tau)\\Big]\\label{eq:lrp.2} \\end{align} This gradient can be approximated with empirical estimate from $m$ trajectories $\\tau^{(1)},\\ldots,\\tau^{(m)}$ under policy $\\pi_\\theta$ \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\mathbb{E}_{P(\\tau;\\theta)}\\Big[\\nabla_\\theta P(\\tau;\\theta)R(\\tau)\\Big] \\\\ \u0026amp;\\approx\\frac{1}{m}\\sum_{i=1}^{m}\\nabla_\\theta\\log P(\\tau^{(i)};\\theta)R(\\tau^{(i)})=\\hat{g}, \\end{align} which is an unbiased estimate of the policy gradient.\nAdditionally, since \\begin{align} \\nabla_\\theta\\log P(\\tau^{(i)};\\theta)\u0026amp;=\\nabla_\\theta\\log\\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\\vert s_t^{(i)},a_t^{(i)})\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)}) \\\\ \u0026amp;=\\nabla_\\theta\\sum_{t=0}^{H-1}\\log P(s_{t+1}^{(i)}\\vert s_t^{(i)},a_t^{(i)})+\\nabla_\\theta\\sum_{t=0}^{H-1}\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)}) \\\\ \u0026amp;=\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)}), \\end{align} we can rewrite the policy gradient estimate in a form which no longer require the dynamics model \\begin{equation} \\hat{g}=\\frac{1}{m}\\sum_{i=1}^{m}\\nabla_\\theta\\log P(\\tau^{(i)};\\theta)R(\\tau^{(i)})=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})R(\\tau^{(i)}) \\end{equation} Moreover, as \\begin{equation} \\mathbb{E}_{P(\\tau\\vert\\theta)}\\Big[\\nabla_\\theta\\log P(\\tau;\\theta)\\Big]=\\nabla_\\theta\\sum_\\tau P(\\tau;\\theta)=\\nabla_\\theta 1=\\mathbf{0}, \\end{equation} a constant baseline in terms of $\\theta$ (i.e. independent of $\\theta$) $b$ can be inserted into \\eqref{eq:lrp.2} to reduce the variance (where $b$ is a vector which can be optimized to minimize the variance). In particular \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\mathbb{E}_{P(\\tau;\\theta)}\\Big[\\nabla_\\theta\\log P(\\tau;\\theta)(R(\\tau)-b)\\Big] \\\\ \u0026amp;\\approx\\frac{1}{m}\\nabla_\\theta\\log P(\\tau^{(i)};\\theta)\\left(R(\\tau^{(i)})-b\\right) \\\\ \u0026amp;=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})\\left(R(\\tau^{(i)})-b\\right)=\\hat{g} \\end{align} By separating $R(\\tau^{(i)})$ into sum of discounted rewards from the past, which does not depend on the current action $a_t^{(i)}$ (and thus can be removed to reduce the variance), and sum of discounted future rewards, we can continue to decompose the estimator $\\hat{g}$ as \\begin{align} \\hat{g}\u0026amp;=\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})\\left(R(\\tau^{(i)}-b)\\right) \\\\ \u0026amp;=\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})\\Bigg[\\sum_{k=0}^{t-1}r(s_k^{(i)},a_k^{(i)})+\\left(\\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})\\right)-b\\Bigg] \\\\ \u0026amp;=\\sum_{i=1}^{m}\\sum_{t=0}^{H-1}\\nabla_\\theta\\log\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})\\left(\\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})-b\\right) \\end{align} These following are some possible choices of baseline $b$.\nAverage rewards. \\begin{equation} b=\\mathbb{E}\\big[R(\\tau)\\big]\\approx\\frac{1}{m}\\sum_{i=1}^{m}R(\\tau^{(i)}) \\end{equation} Optimal baseline. \\begin{equation} b=\\frac{\\sum_{i=1}^{m}\\left(\\nabla_\\theta\\log P(\\tau^{(i)};\\theta)\\right)^2 R(\\tau^{(i)})}{\\sum_{i=1}^{m}\\left(\\nabla_\\theta\\log P(\\tau^{(i)};\\theta)\\right)^2} \\end{equation} Time-dependent baseline. \\begin{equation} b_t=\\frac{1}{m}\\sum_{i=1}^{m}\\sum_{k=t}^{H-1}R(s_k^{(i)},a_k^{(i)}) \\end{equation} State-dependent baseline. \\begin{equation} b(s_t^{(i)})=\\mathbb{E}_\\pi\\left[\\sum_{k=t}^{H-1}\\gamma^k r(s_k^{(i)},a_k^{(i)})\\right]=V_\\pi(s_t^{(i)}) \\end{equation} Importance Sampling Consider a function $f$ and a probability measure $P$. The expectation of $f$, defined by \\begin{equation} \\mathbb{E}_{P(X)}\\big[f(X)\\big]=\\int_{x}P(x)f(x)\\hspace{0.1cm}dx, \\end{equation} sometimes can be difficult to compute. We can resolve this problem by instead approximating the expectation by sampling method, as \\begin{equation} \\mathbb{E}_{P(X)}f(X)\\approx\\frac{1}{m}\\sum_{i=1}^{m}f(x^{(i)}), \\end{equation} where $x^{(i)}\\sim P$. However, samples generated from $P$ are not always easily obtained. This is where importance sampling plays its role.\nThe idea of importance sampling, or IS is an observation that \\begin{align} \\mathbb{E}_{P(X)}\\big[f(X)\\big]\u0026amp;=\\int_{x}P(x)f(x)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\int_{x}Q(x)\\frac{P(x)}{Q(x)}f(x)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\mathbb{E}_{Q(X)}\\left[\\frac{P(X)}{Q(X)}f(X)\\right], \\end{align} where we have assumed that $Q(x)=0\\Rightarrow P(x)=0$. Hence, we can use a sample-based method on $Q$ to get estimate of the expectation. Specifically, given $x^{(i)}\\sim Q$, for $i=1,\\ldots,m$, we can construct an unbiased estimator \\begin{equation} \\mathbb{E}_{P(X)}\\big[f(X)\\big]=\\mathbb{E}_{Q(X)}\\left[\\frac{P(X)}{Q(X)}f(X)\\right]\\approx\\frac{1}{m}\\sum_{i=1}^{m}\\frac{P(x^{(i)})}{Q(x^{(i)})}f(x^{(i)}) \\end{equation}\nLikelihood Ratio Policy Gradient via IS The IS method suggests us rewrite the expected return of policy $\\pi_\\theta$ as2 \\begin{align} \\eta(\\pi_\\theta)\u0026amp;=\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta)}\\big[R(\\tau)\\big] \\\\ \u0026amp;=\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta\u0026rsquo;)}\\left[\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta\u0026rsquo;)}R(\\tau)\\right] \\end{align} Taking the gradient w.r.t $\\theta$ gives us another representation of the policy gradient \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;=\\nabla_\\theta\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta\u0026rsquo;)}\\left[\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta\u0026rsquo;)}R(\\tau)\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta\u0026rsquo;)}\\left[\\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta\u0026rsquo;)}R(\\tau)\\right], \\end{align} which implies that \\begin{align} \\nabla_\\theta\\eta(\\pi_\\theta)\\big\\vert_{\\theta=\\theta\u0026rsquo;}\u0026amp;=\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta\u0026rsquo;)}\\left[\\frac{\\nabla_\\theta P(\\tau;\\theta)\\big\\vert_{\\theta=\\theta\u0026rsquo;}}{P(\\tau;\\theta\u0026rsquo;)}R(\\tau)\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\tau\\sim P(\\tau;\\theta\u0026rsquo;)}\\big[\\nabla_\\theta\\log P(\\tau;\\theta)\\big\\vert_{\\theta=\\theta\u0026rsquo;}R(\\tau)\\big] \\end{align}\nReferences [1] Jie Tang, Pieter Abbeel. On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient. NIPS 2010.\n[2] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\nFootnotes Any infinite-horizon discounted MDP, as defined in the preceding subsection, can be $\\epsilon$-approximated by a finite horizon MDP, using a horizon \\begin{equation*} H_\\epsilon=\\left\\lceil\\log_\\gamma\\left(\\frac{\\epsilon(1-\\gamma)}{R_\\text{max}}\\right)\\right\\rceil, \\end{equation*} where \\begin{equation*} R_\\text{max}=\\max_s\\big\\vert R(s)\\big\\vert \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWe can also use importance sampling to construct an unbiased estimate of the expected return $\\eta(\\pi_\\theta)$. In particular, the expected return of $\\pi_\\theta$ given in \\eqref{eq:lrp.1} can be approximated by \\begin{equation*} \\eta(\\pi_\\theta)=\\sum_{\\tau\\sim P}P(\\tau;\\theta)R(\\tau)\\approx\\frac{1}{m}\\sum_{i=1}^{m}\\frac{P(\\tau^{(i)};\\theta)}{Q(\\tau^{(i)})}R(\\tau^{(i)}) \\end{equation*} where $\\tau^{(i)}\\sim Q$ and we have assumed that $Q(\\tau^{(i)})=0\\Rightarrow P(\\tau^{(i)};\\theta)=0$.\nIf we choose $Q(\\tau)\\doteq P(\\tau;\\theta\u0026rsquo;)$, this means we are approximating the expected return of $\\pi_\\theta$ from trajectories given according to another parameterized policy $\\pi_{\\theta\u0026rsquo;}$. \\begin{equation*} \\eta(\\pi_\\theta)\\approx\\frac{1}{m}\\sum_{i=1}^{m}\\frac{P(\\tau^{(i)};\\theta)}{P(\\tau^{(i)};\\theta\u0026rsquo;)}R(\\tau^{(i)}) \\end{equation*} Taking the gradient of this estimator w.r.t $\\theta$, we obtain an unbiased estimator for the policy gradient specified at \\eqref{eq:lrp.2} \\begin{align*} \\nabla_\\theta\\eta(\\pi_\\theta)\u0026amp;\\approx\\nabla_\\theta\\frac{1}{m}\\sum_{i=1}^{m}\\frac{P(\\tau^{(i)};\\theta)}{P(\\tau^{(i)};\\theta\u0026rsquo;)}R(\\tau^{(i)}) \\end{align*} Similar to applying IS to off-policy learning, evaluating the importance weights, or importance sampling ratio does not require a dynamics model \\begin{equation*} \\frac{P(\\tau^{(i)};\\theta)}{P(\\tau^{(i)});\\theta\u0026rsquo;}=\\frac{\\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\\vert s_t^{(i)},a_t^{(i)})\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})}{\\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\\vert s_t^{(i)},a_t^{(i)})\\pi_{\\theta\u0026rsquo;}(a_t^{(i)}\\vert s_t^{(i)})}=\\prod_{t=0}^{H-1}\\frac{\\pi_\\theta(a_t^{(i)}\\vert s_t^{(i)})}{\\pi_{\\theta\u0026rsquo;}(a_t^{(i)}\\vert s_t^{(i)})} \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/","summary":"\u003cblockquote\u003e\n\u003cp\u003eConnection between Likelihood ratio policy gradient method and Importance sampling method.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Likelihood Ratio Policy Gradient via Importance Sampling"},{"content":" Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.\nModels \u0026amp; Planning Models A model of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.\nWhen the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.\nIf a model produces a description of all possibilities and their probabilities, we call it distribution model. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin. On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it sample model. Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to simulate the environment in order to produce simulated experience.\nPlanning Planning in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment \\begin{equation} \\text{model}\\hspace{0.5cm}\\xrightarrow[]{\\hspace{1cm}\\text{planning}\\hspace{1cm}}\\hspace{0.5cm}\\text{policy} \\end{equation} There are two types of planning:\nState-space planning is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas: Involving computing value functions as a key intermediate step toward improving the policy. Computing value functions by updates or backup applied to simulated experience. \\begin{equation} \\text{model}\\xrightarrow[]{\\hspace{1.5cm}}\\text{simulated experience}\\xrightarrow[]{\\hspace{0.3cm}\\text{backups}\\hspace{0.3cm}}\\text{backups}\\xrightarrow[]{\\hspace{1.5cm}}\\text{policy} \\end{equation} Plan-space planning is a search through the space of plans. Plan-space planning methods consist of evolutionary methods and partial-order planning, in which the ordering of steps is not completely determined at all states of planning. Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.\nFor instance, following is pseudocode of a planning method, called random-sample one-step tabular Q-planning, based on one-step tabular Q-learning, and on random samples from a sample model.\nDyna Within a planning agent, experience plays at least two roles:\nmodel learning: improving the model; direct reinforcement learning (RL): improving the value function and policy The figure below illustrates the possible relationships between experience, model, value functions and policy.\nFigure 1: The possible relationships between experience, model, values and policy\n(the figure is taken from RL book) Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called indirect RL), which involved in planning.\ndirect RL: simpler, not affected by bad models; indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions. Dyna-Q Dyna-Q is the method having all of the processes shown in the diagram in Figure 1 - planning, acting, model-learning and direct RL - all occurring continually:\nthe planning method is the random-sample one-step tabular Q-planning in the previous section; the direct RL method is the one-step tabular Q-learning; the model-learning method is also table-based and assumes the environment is deterministic. After each transition $S_t,A_t\\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.\nDuring planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.\nFollowing is the general architecture of Dyna methods, of which Dyna-Q is an instance.\nFigure 2: The general Dyna Architecture\n(the figure is taken from [RL book](#rl-book)) In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.\nPseudocode of Dyna-Q method is shown below.\nExample (This example is taken from RL book - example 8.1.)\nConsider a gridworld with some obstacles, called \u0026ldquo;maze\u0026rdquo; in this example, shown in the figure below.\nFigure 3: The maze with some obstacles\n(the figure is taken from RL book) As usual, four action, $\\text{up}, \\text{down}, \\text{right}$ and $\\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state. The task is discounted, episodic with $\\gamma=0.95$.\nFigure 4: Using Dyna-Q with different setting of number of planning steps on the maze.\nThe code can be found here Dyna-Q+ Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.\nFigure 5: The maze before and after change\n(the figure is taken from RL book) With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps. In this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an exploration bonus:\nKeeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. An special bonus reward is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting: \\begin{equation} r+\\kappa\\sqrt{\\tau}, \\end{equation} for a small (time weight) $\\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\\tau$ time steps. The agent actually plans how to visit long unvisited state-action pairs. The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.\nFigure 6: Average performance of Dyna-Q and Dyna-Q+ on blocking maze.\nThe code can be found here We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.\nFigure 7: The maze before and after change\n(the figure is taken from RL book) Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.\nFigure 8: Average performance of Dyna-Q and Dyna-Q+ on shortcut maze.\nThe code can be found here It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).\nThe reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.\nPrioritized Sweeping Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.\nPseudocode of prioritized sweeping is shown below.\nFigure 9: Using prioritized sweeping on mazes.\nThe code can be found here Trajectory Sampling Heuristic Search Preferences [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Richard S. Sutton. Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.\n[3] Harm van Seijen \u0026amp; Richard S. Sutton. Efficient planning in MDPs by small backups. Proceedings of the 30th International Conference on Machine Learning (ICML 2013), 2013.\n[4] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes","permalink":"https://trunghng.github.io/posts/reinforcement-learning/planning-learning/","summary":"\u003cblockquote\u003e\n\u003cp\u003eRecall that when using \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/\"\u003edynamic programming (DP) method\u003c/a\u003e in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/\"\u003eMonte Carlo methods\u003c/a\u003e and \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/td-learning/\"\u003etemporal-difference learning\u003c/a\u003e, the models are unnecessary. Such methods with requirement of a model like the case of DP is called \u003cstrong\u003emodel-based\u003c/strong\u003e, while methods without using a model is called \u003cstrong\u003emodel-free\u003c/strong\u003e. Model-based methods primarily rely on \u003cstrong\u003eplanning\u003c/strong\u003e; and model-free methods, on the other hand, primarily rely on \u003cstrong\u003elearning\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Planning \u0026 Learning"},{"content":" So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\\boldsymbol{\\theta}$, that can select actions without consulting a value function by updating $\\boldsymbol{\\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\\boldsymbol{\\theta}$. Such methods are called policy gradient methods.\nPolicy approximation In policy gradient methods, the policy $\\pi$ can be parameterized in any way, as long as $\\pi(a\\vert s,\\boldsymbol{\\theta})$ is differentiable w.r.t $\\boldsymbol{\\theta}$.\nFor discrete action space $\\mathcal{A}$, a common choice of parameterization is to use parameterized numerical preferences $h(s,a,\\boldsymbol{\\theta})\\in\\mathbb{R}$ for each state-action pair. Then, the actions with the highest preferences in each state are given the highest probabilities of being selected, for instance, according to an exponential softmax distribution \\begin{equation} \\pi(a\\vert s,\\boldsymbol{\\theta})\\doteq\\frac{e^{h(s,a,\\boldsymbol{\\theta})}}{\\sum_b e^{h(s,b,\\boldsymbol{\\theta})}} \\end{equation} We refer this policy approximation as softmax in action preferences.\nThe action preferences $h$ can be linear: \\begin{equation} h(s,a,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a), \\end{equation} where $\\mathbf{x}(s,a)\\in\\mathbb{R}^{d\u0026rsquo;}$ is the feature vector corresponding to state-action pair $(s,a)$. Or $h$ could also be calculated by a neural network.\nPolicy Gradient for Episodic Problems We begin by considering episodic case, for which we define the performance measure $J(\\boldsymbol{\\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have: \\begin{equation} J(\\boldsymbol{\\theta})\\doteq v_{\\pi_\\boldsymbol{\\theta}}(s_0),\\label{eq:pge.1} \\end{equation} where $v_{\\pi_\\boldsymbol{\\theta}}$ is the true value function for $\\pi_\\boldsymbol{\\theta}$, the policy parameterized by $\\boldsymbol{\\theta}$.\nIn policy gradient methods, our goal is to learn a policy $\\pi_{\\boldsymbol{\\theta}^*}$ with a parameter vector $\\boldsymbol{\\theta}^*$ that maximizes the performance measure $J(\\boldsymbol{\\theta})$. Using gradient ascent, we iteratively update $\\boldsymbol{\\theta}$ by \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}+\\alpha\\nabla J(\\boldsymbol{\\theta}_t), \\end{equation} where $\\alpha\u0026gt;0$ is the learning rate. By \\eqref{eq:pge.1}, it is noticeable that $\\nabla J(\\theta)$ depends on the state distribution, which generates the start state $s_0$, which is unfortunately unknown.\nHowever, the following theorem claims that we can express the gradient $\\nabla J(\\boldsymbol{\\theta})$ in a form not involving the state distribution.\nThe Policy Gradient Theorem Theorem 1\nThe policy gradient theorem for the episodic case establishes that \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}),\\label{eq:pgte.1} \\end{equation} where $\\pi$ represents the policy corresponding to parameter vector $\\boldsymbol{\\theta}$.\nProof\nWe have that the gradient of the state-value function w.r.t $\\boldsymbol{\\theta}$ can be written in terms of the action-value function, for any $s\\in\\mathcal{S}$, as: \\begin{align} \\hspace{-1.2cm}\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\u0026amp;=\\nabla_\\boldsymbol{\\theta}\\Big[\\sum_a\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\Big],\\hspace{1cm}\\forall s\\in\\mathcal{S} \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}q_\\pi(s,a)\\Big] \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(s|a)q_\\pi(a,s)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\big(r+v_\\pi(s\u0026rsquo;)\\big)\\Big] \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)\\Big] \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\sum_{a\u0026rsquo;}\\big(\\nabla_\\boldsymbol{\\theta}\\pi(s\u0026rsquo;|a\u0026rsquo;,\\boldsymbol{\\theta})q_\\pi(s\u0026rsquo;,a\u0026rsquo;) \\\\ \u0026amp;\\hspace{2cm}+\\pi(a\u0026rsquo;|s\u0026rsquo;,\\boldsymbol{\\theta})\\sum_{s''}p(s''\\vert s\u0026rsquo;,a\u0026rsquo;)\\nabla_\\boldsymbol{\\theta}v_\\pi(s'')\\big)\\Big] \\\\ \u0026amp;=\\sum_{x\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}P(s\\to x,k,\\pi)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a), \\end{align} After repeated unrolling as in the fifth step, where $P(s\\to x,k,\\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\\pi$. It is then immediate that: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026amp;=\\nabla_\\boldsymbol{\\theta}v_\\pi(s_0) \\\\ \u0026amp;=\\sum_s\\Big(\\sum_{k=0}^{\\infty}P(s_0\\to s,k,\\pi)\\Big)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026amp;=\\sum_s\\eta(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026amp;=\\sum_{s\u0026rsquo;}\\eta(s\u0026rsquo;)\\sum_s\\frac{\\eta(s)}{\\sum_{s\u0026rsquo;}\\eta(s\u0026rsquo;)}\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026amp;=\\sum_{s\u0026rsquo;}\\eta(s\u0026rsquo;)\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026amp;\\propto\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a), \\end{align} where $\\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode: \\begin{equation} \\eta(s)=h(s)+\\sum_{\\bar{s}}\\eta(\\bar{s})\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s|\\bar{s},a),\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} where $h(s)$ denotes the probability that an episode begins in each state $s$; $\\bar{s}$ denotes a preceding state of $s$. This leads to the result that we have used in the fifth step: \\begin{equation} \\mu(s)=\\frac{\\eta(s)}{\\sum_{s\u0026rsquo;}\\eta(s\u0026rsquo;)},\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation}\nREINFORCE Notice that in Theorem 1, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\\mu(s)$) under the target policy $\\pi$. Therefore, we can rewrite \\eqref{eq:pgte.1} as: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026amp;\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}) \\\\ \u0026amp;=\\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})\\right]\\label{eq:reinforce.1} \\end{align} Using SGD on maximizing $J(\\boldsymbol{\\theta})$ gives us the update rule: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha\\sum_a\\hat{q}(S_t,a,\\mathbf{w})\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta}), \\end{equation} where $\\hat{q}$ is some learned approximation to $q_\\pi$ with $\\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called all-actions method because its update involves all of the actions.\nContinue our derivation in \\eqref{eq:reinforce.1}, we have: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026amp;=\\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})\\right] \\\\ \u0026amp;=\\mathbb{E}_\\pi\\left[\\sum_a\\pi(a|S_t,\\boldsymbol{\\theta})q_\\pi(S_t,a)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})}{\\pi(a|S_t,\\boldsymbol{\\theta})}\\right] \\\\ \u0026amp;=\\mathbb{E}_\\pi\\left[q_\\pi(S_t,A_t)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta}}\\right] \\\\ \u0026amp;=\\mathbb{E}_\\pi\\left[G_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\right], \\end{align} where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\\sim\\pi$; and in the fourth step, we have used the identity \\begin{equation} \\mathbb{E}_\\pi\\left[G_t|S_t,A_t\\right]=q_\\pi(S_t,A_t) \\end{equation} With this gradient, we have the SGD update for time step $t$, called the REINFORCE update, is then: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha G_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:reinforce.2} \\end{equation} Pseudocode of the algorithm is given below.\nThe vector \\begin{equation} \\frac{\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})}{\\pi(a|s,\\boldsymbol{\\theta})}=\\nabla_\\boldsymbol{\\theta}\\log\\pi(a|s,\\boldsymbol{\\theta}) \\end{equation} in \\eqref{eq:reinforce.2} is called the eligibility vector.\nConsider using soft-max in action preferences with linear action preferences, which means that: \\begin{equation} \\pi(a|s,\\boldsymbol{\\theta})\\doteq\\dfrac{\\exp\\Big[h(s,a,\\boldsymbol{\\theta})\\Big]}{\\sum_b\\exp\\Big[h(s,b,\\boldsymbol{\\theta})\\Big]}, \\end{equation} where the preferences $h(s,a,\\boldsymbol{\\theta})$ is defined as: \\begin{equation} h(s,a,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a) \\end{equation} Using the chain rule we can rewrite the eligibility vector as: \\begin{align} \\nabla_\\boldsymbol{\\theta}\\log\\pi(a|s,\\boldsymbol{\\theta})\u0026amp;=\\nabla_\\boldsymbol{\\theta}\\log{\\frac{\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a)\\Big]}{\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big]}} \\\\ \u0026amp;=\\nabla_\\boldsymbol{\\theta}\\Big(\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a)\\Big)-\\nabla_\\boldsymbol{\\theta}\\log\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big] \\\\ \u0026amp;=\\mathbf{x}(s,a)-\\dfrac{\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big]\\mathbf{x}(s,b)}{\\sum_{b\u0026rsquo;}\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b\u0026rsquo;)\\Big]} \\\\ \u0026amp;=\\mathbf{x}(s,a)-\\sum_b\\pi(b|s,\\boldsymbol{\\theta})\\mathbf{x}(s,b) \\end{align}\nA result when using REINFORCE to solve the short-corrdor problem (Sutton\u0026rsquo;s book, example 13.1) is shown below.\nFigure 1: REINFORCE on short-corridor problem. The code can be found here REINFORCE with Baseline The policy gradient theorem \\eqref{eq:pgte.1} can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$: \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\\propto\\sum_s\\mu(s)\\sum_a\\Big(q_\\pi(s,a)-b(s)\\Big)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}) \\end{equation} The baseline can be any function, even a r.v, as long as it is independent with $a$. The equation is valid because: \\begin{align} \\sum_a b(s)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})\u0026amp;=b(s)\\nabla_\\boldsymbol{\\theta}\\sum_a\\pi(a|s,\\boldsymbol{\\theta}) \\\\ \u0026amp;=b(s)\\nabla_\\boldsymbol{\\theta}1=0 \\end{align} Using the derivation steps analogous to REINFORCE, we end up with another version of REINFORCE that includes a general baseline: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha\\Big(G_t-b(s)\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:rb.1} \\end{equation} One natural baseline choice is the estimate of the state value, $\\hat{v}(S_t,\\mathbf{w})$, with $\\mathbf{w}\\in\\mathbb{R}^d$ is the weight vector of its. Using this baseline, we have pseudocode of the generalization with baseline of REINFORCE algorithm \\eqref{eq:rb.1} given below.\nAdding a baseline to REINFORCE lets the agent learn much faster, as illustrated in the following figure.\nFigure 2: REINFORCE versus REINFORCE with baseline on short-corridor problem. The code can be found here Actor-Critic Methods In Reinforcement Learning, methods that learn both policy and value function at the same time are called actor-critic methods, in which actor refers to the learned policy and critic is a reference to the learned value function. Although the REINFORCE with Baseline method in the previous section learns both policy and value function, but it is not an actor-critic method. Because its state-value function is used as a baseline, not as a critic, which is used for bootstrapping.\nWe begin by considering one-step actor-critic methods. One-step actor-critic methods replace the full return, $G_t$, of REINFORCE \\eqref{eq:rb.1} with the one-step return, $G_{t:t+1}$: \\begin{align} \\boldsymbol{\\theta}_{t+1}\u0026amp;\\doteq\\boldsymbol{\\theta}_t+\\alpha\\Big(G_{t:t+1}-\\hat{v}(S_t,\\mathbf{w})\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:acm.1} \\\\ \u0026amp;=\\boldsymbol{\\theta}_t+\\alpha\\Big(R_{t+1}+\\hat{v}(S_{t+1},\\mathbf{w})-\\hat{v}(S_t,\\mathbf{w})\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})} \\\\ \u0026amp;=\\boldsymbol{\\theta}_t+\\alpha\\delta_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})} \\end{align} The natural state-value function learning method to pair with this is semi-gradient TD(0), which produces the pseudocode given below.\nTo generalize the one-step methods to the forward view of $n$-step methods and then to $\\lambda$-return, in \\eqref{eq:acm.1}, we simply replace the one-step return, $G_{t+1}$, by the $n$-step return, $G_{t:t+n}$, and the $\\lambda$-return, $G_t^\\lambda$, respectively.\nIn order to obtain the backward view of the $\\lambda$-return algorithm, we use separately eligible traces for the actor and critic, as in the pseudocode given below.\nPolicy Gradient with Continuing Problems In the continuing tasks, we define the performance measure in terms of average-reward, as: \\begin{align} J(\\boldsymbol{\\theta})\\doteq r(\\pi)\u0026amp;\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=1}^{h}\\mathbb{E}\\Big[R_t\\big|S_0,A_{0:1}\\sim\\pi\\Big] \\\\ \u0026amp;=\\lim_{t\\to\\infty}\\mathbb{E}\\Big[R_t|S_0,A_{0:1}\\sim\\pi\\Big] \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)r, \\end{align} where $\\mu$ is the steady-state distribution under $\\pi$, $\\mu(s)\\doteq\\lim_{t\\to\\infty}P(S_t=s|A_{0:t}\\sim\\pi)$ which is assumed to exist and to be independent of $S_0$; and we also have that: \\begin{equation} \\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s\u0026rsquo;|s,a)=\\mu(s\u0026rsquo;),\\hspace{1cm}\\forall s\u0026rsquo;\\in\\mathcal{S} \\end{equation} Recall that in continuing tasks with average-reward setting, we use the differential return, which is defined in terms of differences between rewards and the average reward: \\begin{equation} G_t\\doteq R_{t+1}-r(\\pi)+R_{t+2}-r(\\pi)+R_{t+3}-r(\\pi)+\\dots\\label{eq:pgc.1} \\end{equation} And thus, we also use the differential version of value functions, which are defined as usual except that they use the differential return \\eqref{eq:pgc.1}: \\begin{align} v_\\pi(s)\u0026amp;\\doteq\\mathbb{E}_\\pi\\left[G_t|S_t=s\\right] \\\\ q_\\pi(s,a)\u0026amp;\\doteq\\mathbb{E}_\\pi\\left[G_t|S_t=s,A_t=s\\right] \\end{align}\nThe Policy Gradient Theorem Theorem 2\nThe policy gradient theorem for continuing case with average-reward states that \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s)q_\\pi(s,a) \\end{equation}\nProof\nWe have that the gradient of the state-value function w.r.t $\\boldsymbol{\\theta}$ can be written, for any $s\\in\\mathcal{S}$, as: \\begin{align} \\hspace{-1cm}\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\u0026amp;=\\boldsymbol{\\theta}\\Big[\\sum_a\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\Big],\\hspace{1cm}\\forall s\\in\\mathcal{S} \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}q_\\pi(s,a)\\Big] \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\big(r-r(\\boldsymbol{\\theta})+v_\\pi(s\u0026rsquo;)\\big)\\Big] \\\\ \u0026amp;=\\sum_a\\Bigg[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\Big[-\\nabla_\\boldsymbol{\\theta}r(\\boldsymbol{\\theta})+\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)\\Big]\\Bigg] \\end{align} Thus, the gradient of the performance measure w.r.t $\\boldsymbol{\\theta}$ is: \\begin{align} \\hspace{-1cm}\\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026amp;=\\nabla_\\boldsymbol{\\theta}r(\\boldsymbol{\\theta}) \\\\ \u0026amp;=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)\\Big]-\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\Bigg(\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)\\Big]-\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\\Bigg) \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\sum_{s\u0026rsquo;}\\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s\u0026rsquo;|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\sum_{s\u0026rsquo;}\\mu(s\u0026rsquo;)\\nabla_\\boldsymbol{\\theta}v_\\pi(s\u0026rsquo;)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\end{align}\nPolicy Parameterization for Continuous Actions For tasks having continuous action space with an infinite number of actions, instead of computing learned probabilities for each action, we can learn statistics of the probability distribution.\nIn particular, to produce a policy parameterization, the policy can be defined as the Normal distribution over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state, as given: \\begin{equation} \\pi(a|s,\\boldsymbol{\\theta})\\doteq\\frac{1}{\\sigma(s,\\boldsymbol{\\theta})\\sqrt{2\\pi}}\\exp\\left(-\\frac{(a-\\mu(s,\\boldsymbol{\\theta}))^2}{2\\sigma(s,\\boldsymbol{\\theta})^2}\\right), \\end{equation} where $\\mu:\\mathcal{S}\\times\\mathbb{R}^{d\u0026rsquo;}\\to\\mathbb{R}$ and $\\sigma:\\mathcal{S}\\times\\mathbb{R}^{d\u0026rsquo;}\\to\\mathbb{R}^+$ are two parameterized function approximators.\nWe continue by dividing the policy\u0026rsquo;s parameter vector, $\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_\\mu, \\boldsymbol{\\theta}_\\sigma]^\\text{T}$, into two parts: one part, $\\boldsymbol{\\theta}_\\mu$, is used for the approximation of the mean and the other, $\\boldsymbol{\\theta}_\\sigma$, is used for the approximation of the standard deviation.\nThe mean, $\\mu$, can be approximated as a linear function, while the standard deviation, $\\sigma$, must always be positive, which should be approximated as the exponential of a linear function, as: \\begin{align} \\mu(s,\\boldsymbol{\\theta})\u0026amp;\\doteq\\boldsymbol{\\theta}_\\mu^\\text{T}\\mathbf{x}_\\mu(s) \\\\ \\sigma(s,\\boldsymbol{\\theta})\u0026amp;\\doteq\\exp\\Big(\\boldsymbol{\\theta}_\\sigma^\\text{T}\\mathbf{x}_\\sigma(s)\\Big), \\end{align} where $\\mathbf{x}_\\mu(s)$ and $\\mathbf{x}_\\sigma(s)$ are state feature vectors corresponding to each approximator.\nReferences [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Deepmind x UCL. Reinforcement Learning Lecture Series 2021. Deepmind, 2021.\n[3] Richard S. Sutton \u0026amp; David McAllester \u0026amp; Satinder Singh \u0026amp; Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. NIPS 1999.\nFootnotes","permalink":"https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/","summary":"\u003cblockquote\u003e\n\u003cp\u003eSo far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a \u003cstrong\u003eparameterized policy\u003c/strong\u003e, $\\boldsymbol{\\theta}$, that can select actions without consulting a value function by updating $\\boldsymbol{\\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\\boldsymbol{\\theta}$. Such methods are called \u003cstrong\u003epolicy gradient methods\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Policy Gradient Theorem"},{"content":" Notes on exponential family.\nThe exponential family The exponential family of distributions is defined as family of distributions of form \\begin{equation} p(x;\\eta)=h(x)\\exp\\Big[\\eta^\\text{T}T(x)-A(\\eta)\\Big],\\label{eq:ef.1} \\end{equation} where\n$\\eta$ is known as the natural parameter, or canonical parameter, $T(X)$ is referred to as a sufficient statistic, $A(\\eta)$ is called the cumulant function, which can be view as the logarithm of a normalization factor since integrating \\eqref{eq:ef.1} w.r.t the measure $\\nu$ gives us \\begin{equation} A(\\eta)=\\log\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx),\\label{eq:ef.2} \\end{equation} This also implies that $A(\\eta)$ will be determined once we have specified $\\nu,T(x)$ and $h(x)$. The set of parameters $\\eta$ for which the integral in \\eqref{eq:ef.2} is finite is known as the natural parameter space \\begin{equation} N=\\left\\{\\eta:\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx)\u0026lt;\\infty\\right\\} \\end{equation} which explains why $\\eta$ is also referred as natural parameter. If $N$ is an non-empty open set, the exponential families are said to be regular.\nAn exponential family is known as minimal if there are no linear constraints among the components of $\\eta$ nor are there linear constraints among the components of $T(x)$.\nExamples Each particular choice of $\\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\\eta$. As we vary $\\eta$, we then get different distributions within this family.\nBernoulli distribution The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\\sim\\text{Bern}(\\pi)$, is given by \\begin{align} p(x;\\pi)\u0026amp;=\\pi^x(1-\\pi)^{1-x} \\\\ \u0026amp;=\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026amp;=\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which can be written in the form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026amp;=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026amp;=x \\\\ A(\\eta)\u0026amp;=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026amp;=1 \\end{align} Notice that the relationship between $\\eta$ and $\\pi$ is invertible since \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}}, \\end{equation} which is the sigmoid function.\nBinomial distribution The probability mass function of a Binomial random variable $X$, denoted as $X\\sim\\text{Bin}(N,\\pi)$, is defined as \\begin{align} p(x;N,\\pi)\u0026amp;={N\\choose x}\\pi^{x}(1-\\pi)^{1-x} \\\\ \u0026amp;={N\\choose x}\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026amp;={N\\choose x}\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which is in form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026amp;=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026amp;=x \\\\ A(\\eta)\u0026amp;=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026amp;={N\\choose x} \\end{align} Similar to the Bernoulli case, we also have the invertible relationship between $\\eta$ and $\\pi$ as \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}} \\end{equation}\nPoisson distribution The probability mass function of a Poisson random variable $X$, denoted as $X\\sim\\text{Pois}(\\lambda)$, is given as \\begin{align} p(x;\\lambda)\u0026amp;=\\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\ \u0026amp;=\\frac{1}{x!}\\exp\\left(x\\log\\lambda-\\lambda\\right), \\end{align} which is also able to be written as an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026amp;=\\log\\lambda \\\\ T(x)\u0026amp;=x \\\\ A(\\eta)\u0026amp;=\\lambda=e^{\\eta} \\\\ h(x)\u0026amp;=\\frac{1}{x!} \\end{align} Analogy to Bernoulli distribution, we also have that \\begin{equation} \\lambda=e^{\\eta} \\end{equation}\nGaussian distribution The (univariate) Gaussian density of a random variable $X$, denoted as $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, is given by \\begin{align} p(x;\\mu,\\sigma^2)\u0026amp;=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] \\\\ \u0026amp;=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left[\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2-\\frac{1}{2\\sigma^2}\\mu^2-\\log\\sigma\\right], \\end{align} which allows us to write it as an instance of the exponential family with \\begin{align} \\eta\u0026amp;=\\left[\\begin{matrix}\\mu/\\sigma^2 \\\\ -1/2\\sigma^2\\end{matrix}\\right] \\\\ T(x)\u0026amp;=\\left[\\begin{matrix}x\\\\ x^2\\end{matrix}\\right] \\\\ A(\\eta)\u0026amp;=\\frac{\\mu^2}{2\\sigma^2}+\\log\\sigma=-\\frac{\\eta_1^2}{4\\eta_2}-\\frac{1}{2}\\log(-2\\eta_2) \\\\ h(x)\u0026amp;=\\frac{1}{\\sqrt{2\\pi}} \\end{align}\nMultinomial distribution Let $\\mathbf{X}=(X_1,\\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\\mathbf{\\pi}=(\\pi_1,\\ldots,\\pi_K)$ with $\\sum_{k=1}^{K}\\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.\nThen $\\mathbf{X}$ is said to have Multinomial distribution, denoted as $\\mathbf{X}\\sim\\text{Mult}_K(N,\\boldsymbol{\\pi})$, if its probability mass function is given as with $\\sum_{k=1}^{K}x_k=1$ \\begin{align} p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\pi_1^{x_1}\\pi_2^{x_2}\\ldots\\pi_n^{x_n} \\\\ \u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right)\\label{eq:m.1} \\end{align} It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\\mathbf{x})$, which is \\begin{equation} \\sum_{k=1}^{K}x_k=1 \\end{equation} In order to remove this constraint, we substitute $1-\\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \\eqref{eq:m.1} be written by \\begin{align} \\hspace{-0.8cm}p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right) \\\\ \u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{k=1}^{K-1}x_k\\log\\pi_k+\\left(1-\\sum_{k=1}^{K-1}x_k\\right)\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right] \\\\ \u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{i=1}^{K-1}\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)x_i+\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right]\\label{eq:m.2} \\end{align} With this representation, and also for convenience, for $i=1,\\ldots,K$ we continue by letting \\begin{equation} \\eta_i=\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)=\\log\\left(\\frac{\\pi_i}{\\pi_K}\\right)\\label{eq:m.3} \\end{equation} Take the exponential of both sides and summing over $K$, we have \\begin{equation} \\sum_{i=1}^{K}e^{\\eta_i}=\\frac{\\sum_{i=1}^{K}\\pi_i}{\\pi_K}=\\frac{1}{\\pi_K}\\label{eq:m.4} \\end{equation} From this result, we have that the Multinomial distribution \\eqref{eq:m.2} is therefore also a member of the exponential family with \\begin{align} \\eta\u0026amp;=\\left[\\begin{matrix}\\log\\left(\\pi_1/\\pi_K\\right) \\\\ \\vdots \\\\ \\log\\left(\\pi_K/\\pi_K\\right)\\end{matrix}\\right] \\\\ T(\\mathbf{x})\u0026amp;=\\left[\\begin{matrix}x_1,\\ldots,x_K\\end{matrix}\\right]^\\text{T} \\\\ A(\\eta)\u0026amp;=-\\log\\left(1-\\sum_{i=1}^{K-1}\\pi_i\\right)=-\\log(\\pi_K)=\\log\\left(\\sum_{k=1}^{K}e^{\\eta_k}\\right) \\\\ h(\\mathbf{x})\u0026amp;=\\frac{N!}{x_1!x_2!\\ldots x_K!} \\end{align} Additionally, substituting the result \\eqref{eq:m.4} into \\eqref{eq:m.3} gives us for $i=1,\\ldots,K$ \\begin{equation} \\eta_i=\\log\\left(\\pi_i\\sum_{k=1}^{K}e^{\\eta_k}\\right), \\end{equation} or we can express $\\boldsymbol{\\pi}$ in terms of $\\eta$ by \\begin{equation} \\pi_i=\\frac{e^{\\eta_i}}{\\sum_{k=1}^{K}e^{\\eta_k}}, \\end{equation} which is the softmax function.\nMultivariate Normal distribution Convexity Theorem\nThe natural space $N$ is a convex set and the cumulant function $A(\\eta)$ is a convex function. If the family is minimal, then $A(\\eta)$ is strictly convex.\nProof\nLet $\\eta_1,\\eta_2\\in N$, thus from \\eqref{eq:ef.2}, we have that \\begin{align} \\exp\\big(A(\\eta_1)\\big)\u0026amp;=A_1, \\\\ \\exp\\big(A(\\eta_2)\\big)\u0026amp;=A_2 \\end{align} where $A_1,A_2$ are finite.\nTo prove that $N$ is convex, we need to show that for any $\\eta=\\lambda\\eta_1+(1-\\lambda)\\eta_2$ for $0\\lt\\lambda\\lt 1$, we also have $\\eta\\in N$. From \\eqref{eq:ef.2}, and by Hölder\u0026rsquo;s inequality1, we have \\begin{align} \\exp\\big(A(\\eta)\\big)\u0026amp;=\\int h(x)\\exp\\big(\\eta^\\text{T}T(x)\\big)\\nu(dx) \\\\ \u0026amp;=\\int h(x)\\exp\\Big[\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big)^\\text{T}T(x)\\Big]\\nu(dx) \\\\ \u0026amp;=\\int \\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda}\\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}\\nu(dx) \\\\ \u0026amp;\\leq\\Bigg[\\int h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^\\lambda\\Bigg[\\int h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^{1-\\lambda} \\\\ \u0026amp;=\\Big[\\exp\\big(A(\\eta_1)\\big)\\Big]^\\lambda\\Big[\\exp\\big(A(\\eta_2)\\big)\\Big]^{1-\\lambda} \\\\ \u0026amp;=A_1^\\lambda A_2^{1-\\lambda},\\label{eq:c.1} \\end{align} which proves that $A(\\eta)$ is finite, or $\\eta\\in N$.\nMoreover, taking logarithm of both sides of \\eqref{eq:c.1} gives us \\begin{equation} \\lambda A(\\eta_1)+(1-\\lambda)A(\\eta_2)\\geq A(\\eta)=A\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big), \\end{equation} which also claims the convexity of $A(\\eta)$.\nBy Hölder\u0026rsquo;s inequality, the equality in \\eqref{eq:c.1} holds when \\begin{equation} \\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}=c\\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda(1/\\lambda-1)} \\end{equation} or \\begin{equation} \\exp\\big(\\eta_2^\\text{T}T(x)\\big)=c\\exp\\big(\\eta_1^\\text{T}T(x)\\big), \\end{equation} and therefore \\begin{equation} (\\eta_2-\\eta_1)^\\text{T}T(x)=\\log c, \\end{equation} which is not minimal since $\\eta_1,\\eta_2$ are taken arbitrarily.\nMoments of sufficient statistic In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second cumulants.\nMeans, variances Let us first consider the first derivative of the cumulant function $A(\\eta)$. By the dominated convergence theorem, we have \\begin{align} \\frac{\\partial A(\\eta)}{\\partial\\eta^\\text{T}}\u0026amp;=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\log\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx) \\\\ \u0026amp;=\\frac{\\int T(x)\\exp\\big(\\eta^\\text{T}(x)\\big)h(x)\\nu(dx)}{\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx)} \\\\ \u0026amp;=\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx)\\label{eq:mv.1} \\\\ \u0026amp;=\\int T(x)p(x;\\eta)\\nu(dx) \\\\ \u0026amp;=\\mathbb{E}[T(X)], \\end{align} which is the mean of the sufficient statistic $T(x)$.\nMoreover, taking the second derivative of cumulant function by continuing with the result \\eqref{eq:mv.1}, we have \\begin{align} \\frac{\\partial^2 A(\\eta)}{\\partial\\eta\\partial\\eta^\\text{T}}\u0026amp;=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026amp;=\\int T(x)\\left(T(x)-\\frac{\\partial}{\\partial\\eta^\\text{T}}A(\\eta)\\right)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026amp;=\\int T(x)\\big(T(x)-E(T(X))\\big)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026amp;=\\mathbb{E}\\left[T(X)T(X)^\\text{T}\\right]-\\mathbb{E}[T(X)]\\mathbb{E}[T(X)]^\\text{T} \\\\ \u0026amp;=\\text{Var}[T(X)], \\end{align} which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$.\nMoment generating functions The moment generating function (or MGF) of a random variable $X$, denoted as $M(t)$, is given by \\begin{equation} M(t)=\\mathbb{E}(e^{t^\\text{T}X}), \\end{equation} for all values of $t$ for which the expectation exists.\nThe MGF of the sufficient statistic $T(X)$ then can be computed as \\begin{align} M_{T(X)}(t)\u0026amp;=\\mathbb{E}(e^{t^\\text{T}T(X)}) \\\\ \u0026amp;=\\int \\exp\\big((\\eta+t)^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026amp;=\\exp\\big(A(\\eta+t)-A(\\eta)\\big)\\label{eq:mgf.1} \\end{align}\nCumulant generating functions The cumulant generating function (or CGF) of a random variable $X$, denoted by $K(t)$, is given as \\begin{equation} K(t)=\\log M(t)=\\log\\mathbb{E}(e^{t^\\text{T}X}), \\end{equation} for all values of $t$ for which the expectation exists.\nFrom the MGF of $T(X)$ in \\eqref{eq:mgf.1}, the CGF of the sufficient statistic $T(X)$ therefore can be calculated by \\begin{equation} K_{T(X)}(t)=\\log M_{T(X)}(t)=A(\\eta+t)-A(\\eta) \\end{equation}\nCumulants The $k$-th cumulant of a random variable $X$ is defined to be the $k$-th derivative of $K_{X}(t)$ at $0$, i.e., \\begin{equation} c_k=K^{(k)}(0) \\end{equation}\nThus, the mean of $T(X)$ is exactly the first cumulant, while the variance is the second cumulant of $T(X)$.\nSufficiency Maximum likelihood estimates Consider an i.i.d data set $\\mathcal{D}=\\{x_1,\\ldots,x_N\\}$, the likelihood function is then given by \\begin{align} L(\\eta)=p(\\mathbf{X}\\vert\\eta)\u0026amp;=\\prod_{n=1}^{N}p(x_n\\vert\\eta) \\\\ \u0026amp;=\\prod_{n=1}^{N}h(x_n)\\exp\\big[\\eta^\\text{T}T(x_n)-A(\\eta)\\big] \\\\ \u0026amp;=\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right]\\label{eq:mle.1} \\end{align} Taking the logarithm of both sides gives us the log likelihood as \\begin{equation} \\ell(\\eta)=\\log L(\\eta)=\\log\\left(\\prod_{n=1}^{N}h(x_n)\\right)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta) \\end{equation} Consider the gradient of the log likelihood w.r.t $\\eta$, we have \\begin{align} \\nabla_\\eta\\ell(\\eta)\u0026amp;=\\nabla_\\eta\\left[\\log\\left(\\prod_{n=1}^{N}h(x_n)\\right)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026amp;=\\sum_{n=1}^{N}T(x_n)-N\\nabla_\\eta A(\\eta) \\end{align} Setting the gradient to zero, we have the value of $\\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\\eta$, denoted as $\\eta_\\text{ML}$ satisfies \\begin{equation} \\nabla_{\\eta}A(\\eta_\\text{ML})=\\frac{1}{N}\\sum_{n=1}^{N}T(x_n) \\end{equation}\nConjugate priors Given a probability distribution $p(x\\vert\\eta)$, its prior $p(\\eta)$ is said to be conjugate to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as conjugate prior.\nFor any member of the exponential family, there exists a conjugate prior that can be written in form \\begin{equation} p(\\eta\\vert\\mathcal{X},\\theta)=f(\\mathcal{X},\\theta)\\exp(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)),\\label{eq:cp.1} \\end{equation} where $\\theta\u0026gt;0$ and $\\mathcal{X}$ are hyperparameters.\nBy Bayes\u0026rsquo; rule, and with the likelihood function as given in \\eqref{eq:mle.1}, the posterior distribution can be computed as \\begin{align} \u0026amp;\\hspace{0.7cm}p(\\eta\\vert\\mathbf{X},\\mathcal{X},\\theta) \\\\ \u0026amp;\\propto p(\\eta\\vert\\mathcal{X},\\theta)p(\\mathbf{X}\\vert\\eta) \\\\ \u0026amp;=f(\\mathcal{X},\\theta)\\exp\\big(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)\\big)\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026amp;\\propto\\exp\\left[\\eta^\\text{T}\\left(\\mathcal{X}+\\sum_{n=1}^{N}T(x_n)\\right)-(\\theta+N)A(\\eta)\\right], \\end{align} which is in the same form as \\eqref{eq:cp.1} and therefore claims the conjugacy.\nReferences [1] M. Jordan. The Exponential Family: Basics. 2009.\n[2] Joseph K. Blitzstein \u0026amp; Jessica Hwang. Introduction to Probability.\n[3] Weisstein, Eric W. Hölder\u0026rsquo;s Inequalities From MathWorld\u0026ndash;A Wolfram Web Resource.\nFootnotes Let $p,q\u0026gt;1$ such that \\begin{equation*} \\frac{1}{p}+\\frac{1}{q}=1 \\end{equation*} The Hölder\u0026rsquo;s inequality for integrals states that \\begin{equation*} \\int_a^b\\vert f(x)g(x)\\vert\\hspace{0.1cm}dx\\leq\\left(\\int_a^b\\vert f(x)\\vert\\hspace{0.1cm}dx\\right)^{1/p}\\left(\\int_a^b\\vert g(x)\\vert\\hspace{0.1cm}dx\\right)^{1/q} \\end{equation*} The equality holds with \\begin{equation*} \\vert g(x)\\vert=c\\vert f(x)\\vert^{p-1} \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/probability-statistics/exponential-family/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on exponential family.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"The exponential family"},{"content":" Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\\lambda$ in TD($\\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\\lambda=0$ to Monte Carlo methods with $\\lambda=1$.\nThe $\\lambda$-return Recall that in TD-Learning note, we have defined the $n$-step return as \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n}) \\end{equation} for all $n,t$ such that $n\\geq 1$ and $0\\leq t\\lt T-n$. After the note of Function Approximation, for any parameterized function approximator, we can generalize that equation into: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+ \\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\leq T-n \\end{equation} where $\\hat{v}(s,\\mathbf{w})$ is the approximate value of state $s$ given weight vector $\\mathbf{w}$.\nWe already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate SGD update, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose \\begin{equation} \\frac{1}{2}G_{t:t+2}+\\frac{1}{2}G_{t:t+4} \\end{equation} as the target for our update.\nThe TD($\\lambda$) is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\\lambda^{n-1}$, for $\\lambda\\in\\left[0,1\\right]$, and is normalized by a factor of $1-\\lambda$ to guarantee that the weights sum to $1$, as: \\begin{equation} G_t^\\lambda\\doteq(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_{t:t+n} \\end{equation} The $G_t^\\lambda$ is called $\\lambda$-return of the update.\nThis figure below illustrates the backup diagram of TD($\\lambda$) algorithm.\nFigure 1: The backup diagram of TD($\\lambda$) Offline $\\lambda$-return With the definition of $\\lambda$-return, we can define the offline $\\lambda$-return algorithm, which use semi-gradient update and using $\\lambda$-return as the target: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\hspace{1cm}t=0,\\dots,T-1 \\end{equation}\nA result when applying offline $\\lambda$-return on the random walk problem is shown below.\nFigure 2: Using offline $\\lambda$-return on 19-state random walk. The code can be found here TD($\\lambda$) TD($\\lambda$) improves over the offline $\\lambda$-return algorithm since:\nIt updates the weight vector $\\mathbf{w}$ on every step of an episode rather than only at the end, which leads to a time improvement. Its computations are equally distributed in time rather than all at the end of the episode. It can be applied to continuing problems rather than just to episodic ones. With function approximation, the eligible trace is a vector $\\mathbf{z}_t\\in\\mathbb{R}^d$ with the same number of components as the weight vector $\\mathbf{w}_t$. Whereas $\\mathbf{w}_t$ is long-term memory, $\\mathbf{z}_t$ on the other hand is a short-term memory, typically lasting less time than the length of an episode.\nIn TD($\\lambda$), starting at the initial value of zero at the beginning of the episode, on each time step, the eligible trace vector $\\mathbf{z}_t$ is incremented by the value gradient, and then fades away by $\\gamma\\lambda$: \\begin{align} \\mathbf{z}_{-1}\u0026amp;\\doteq\\mathbf{0} \\\\ \\mathbf{z}_t\u0026amp;\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\hspace{1cm}0\\leq t\\lt T\\label{eq:tl.1} \\end{align} where $\\gamma$ is the discount factor; $\\lambda$ is also called trace-decay parameter. On the other hand, the weight vector $\\mathbf{w}_t$ is updated on each step proportional to the scalar TD errors and the eligible trace vector $\\mathbf{z}_t$: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t,\\label{eq:tl.2} \\end{equation} where the TD error is defined as \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} Pseudocode of semi-gradient TD($\\lambda$) is given below.\nLinear TD($\\lambda$) has been proved to converge in the on-policy case if the step size parameter, $\\alpha$, is reduced over time according to the usual conditions. And also in the continuing discounted case, for any $\\lambda$, $\\overline{\\text{VE}}$ is proven to be within a bounded expansion of the lowest possible error: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w}_\\infty)\\leq\\dfrac{1-\\gamma\\lambda}{1-\\gamma}\\min_\\mathbf{w}\\overline{\\text{VE}}(\\mathbf{w}) \\end{equation}\nThe figure below illustrates the result for using TD($\\lambda$) on the usual random walk task.\nFigure 3: Using TD($\\lambda$) on 19-state random walk. The code can be found here Truncated TD Methods Since in the offline $\\lambda$-return, the target $\\lambda$-return is not known until the end of episode. And moreover, in the continuing case, since the $n$-step returns depend on arbitrary large $n$, it maybe never known. However, the dependence becomes weaker for longer-delayed rewards, falling by $\\gamma\\lambda$ for each step of delay.\nA natural approximation is to truncate the sequence after some number of steps. In general, we define the truncated $\\lambda$-return for time $t$, given data only up to some later horizon, $h$, as: \\begin{equation} G_{t:h}^\\lambda\\doteq(1-\\lambda)\\sum_{n=1}^{h-t-1}\\lambda^{n-1}G_{t:t+n}+\\lambda^{h-t-1}G_{t:h},\\hspace{1cm}0\\leq t\\lt h\\leq T \\end{equation} With this definition of the return, and based on the function approximation version of the $n$-step TD we have defined before, we have the TTD($\\lambda$) is defined as: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\left[G_{t:t+n}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_{t+n-1})\\right]\\nabla_\\mathbf{w}\\hat{w}(S_t,\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\lt T \\end{equation} We have the $k$-step $\\lambda$-return can be written as: \\begin{align} \\hspace{-0.8cm}G_{t:t+k}^\\lambda\u0026amp;=(1-\\lambda)\\sum_{n=1}^{k-1}\\lambda^{n-1}G_{t:t+n}+\\lambda^{k-1}G_{t:t+k} \\\\ \u0026amp;=(1-\\lambda)\\sum_{n=1}^{k-1}\\lambda^{n-1}\\left[R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1})\\right]\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\lambda^{k-1}\\left[R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{k-1}R_{t+k}+\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1})\\right] \\\\ \u0026amp;=R_{t+1}+\\gamma\\lambda R_{t+2}+\\dots+\\gamma^{k-1}\\lambda^{k-1}R_{t+k}\\nonumber \\\\ \u0026amp;\\hspace{1cm}+(1-\\lambda)\\left[\\sum_{n=1}^{k-1}\\lambda^{n-1}\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1})\\right]+\\lambda^{k-1}\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1}) \\\\ \u0026amp;=\\hat{v}(S_t,\\mathbf{w}_{t-1})+\\left[R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_{t-1})\\right]\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\left[\\lambda\\gamma R_{t+2}+\\lambda\\gamma^2\\hat{v}(S_{t+2},\\mathbf{w}_{t+1})-\\lambda\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)\\right]+\\dots\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\left[\\lambda^{k-1}\\gamma^{k-1}R_{t+k}+\\lambda^{k-1}\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1})-\\lambda^{k-1}\\gamma^{k-1}\\hat{v}(S_{t+k-1},\\mathbf{w}_{t+k-2})\\right] \\\\ \u0026amp;=\\hat{v}(S_t,\\mathbf{w}_{t-1})+\\sum_{i=t}^{t+k-1}(\\gamma\\lambda)^{i-t}\\delta_i\u0026rsquo;,\\label{eq:tt.1} \\end{align} with \\begin{equation} \\delta_t\u0026rsquo;\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_{t-1}), \\end{equation} where in the third step of the derivation, we use the identity \\begin{equation} (1-\\lambda)(1+\\lambda+\\dots+\\lambda^{k-2})=1-\\lambda^{k-1} \\end{equation} From \\eqref{eq:tt.1}, we can see that the $k$-step $\\lambda$-return can be written as sums of TD errors if the value function is held constant, which allows us to implement the TTD($\\lambda$) algorithm efficiently.\nFigure 4: The backup diagram of truncated TD($\\lambda$) Online $\\lambda$-return The idea of online $\\lambda$-return involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors.\nLet $\\mathbf{w}_t^h$ denote the weights used to generate the value at time $t$ in the sequence up to horizon $h$. The first weight vector $\\mathbf{w}_0^h$ in each sequence is the one that inherited from the previous episode (thus they are the same for all $h$), and the last weight vector $\\mathbf{w}_h^h$ in each sequence defines the weight-vector sequence of the algorithm. At the final horizon $h=T$, we obtain the final weight $\\mathbf{w}_T^T$ which will be passed on to form the initial weights of the next episode.\nIn particular, we can define the first three sequences as: \\begin{align} h=1:\\hspace{1cm}\u0026amp;\\mathbf{w}_1^1\\doteq\\mathbf{w}_0^1+\\alpha\\left[G_{0:1}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^1)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^1), \\\\\\nonumber \\\\ h=2:\\hspace{1cm}\u0026amp;\\mathbf{w}_1^2\\doteq\\mathbf{w}_0^2+\\alpha\\left[G_{0:2}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^2)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^2), \\\\ \u0026amp;\\mathbf{w}_2^2\\doteq\\mathbf{w}_1^2+\\alpha\\left[G_{1:2}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_1^2)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_1,\\mathbf{w}_1^2), \\\\\\nonumber \\\\ h=3:\\hspace{1cm}\u0026amp;\\mathbf{w}_1^3\\doteq\\mathbf{w}_0^3+\\alpha\\left[G_{0:3}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^3), \\\\ \u0026amp;\\mathbf{w}_2^3\\doteq\\mathbf{w}_1^3+\\alpha\\left[G_{1:3}^\\lambda-\\hat{v}(S_1,\\mathbf{w}_1^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_1,\\mathbf{w}_1^3), \\\\ \u0026amp;\\mathbf{w}_3^3\\doteq\\mathbf{w}_2^3+\\alpha\\left[G_{2:3}^\\lambda-\\hat{v}(S_2,\\mathbf{w}_2^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_2,\\mathbf{w}_2^3) \\end{align} The general form for the update of the online $\\lambda$-return is \\begin{equation} \\mathbf{w}_{t+1}^h\\doteq\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t^h)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t^h),\\hspace{1cm}0\\leq t\\lt h\\leq T,\\label{eq:olr.1} \\end{equation} with $\\mathbf{w}_t\\doteq\\mathbf{w}_t^t$, and $\\mathbf{w}_0^h$ is the same for all $h$, we denote this vector as $\\mathbf{w}_{init}$.\nThe online $\\lambda$-return algorithm is fully online, determining a new weight vector $\\mathbf{w}_t$ at each time step $t$ during an episode, using only information available at time $t$. Whereas the offline version passes through all the steps at the time of termination but does not make any updates during the episode.\nTrue Online TD($\\lambda$) In the online $\\lambda$-return, at each time step a sequence of updates is performed. The length of this sequence, and hence the computation per time step, increase over time.\nHowever, it is possible to compute the weight vector resulting from time step $t+1$, $\\mathbf{w}_{t+1}$, directly from the weight vector resulting from the sequence at time step $t$, $\\mathbf{w}_t$.\nConsider using linear approximation for our task, which gives us \\begin{align} \\hat{v}(S_t,\\mathbf{w}_t)\u0026amp;=\\mathbf{w}_t^\\text{T}\\mathbf{x}_t; \\\\ \\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\u0026amp;=\\mathbf{x}_t, \\end{align} where $\\mathbf{x}_t=\\mathbf{x}(S_t)$ as usual.\nWe begin by rewriting \\eqref{eq:olr.1}, as \\begin{align} \\mathbf{w}_{t+1}^h\u0026amp;\\doteq\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t^h)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t^h) \\\\ \u0026amp;=\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\left(\\mathbf{w}_t^h\\right)^\\text{T}\\mathbf{x}_t\\right]\\mathbf{x}_t \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t^h+\\alpha\\mathbf{x}_t G_{t:h}^\\lambda, \\end{align} where $\\mathbf{I}$ is the identity matrix. With this equation, consider $\\mathbf{w}_t^h$ in the cases of $t=1$ and $t=2$, we have: \\begin{align} \\mathbf{w}_1^h\u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_0^h+\\alpha\\mathbf{x}_0 G_{0:h}^\\lambda \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_{init}+\\alpha\\mathbf{x}_0 G_{0:h}^\\lambda, \\\\ \\mathbf{w}_2^h\u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\mathbf{w}_1^h+\\alpha\\mathbf{x}_1 G_{1:h}^\\lambda \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_{init}+\\alpha\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\mathbf{x}_0 G_{0:h}^\\lambda+\\alpha\\mathbf{x}_1 G_{1:h}^\\lambda \\end{align} In general, for $t\\leq h$, we can write: \\begin{equation} \\mathbf{w}_t^h=\\mathbf{A}_0^{t-1}\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{i:h}^\\lambda, \\end{equation} where $\\mathbf{A}_i^j$ is defined as: \\begin{equation} \\mathbf{A}_i^j\\doteq\\left(\\mathbf{I}-\\alpha\\mathbf{x}_j\\mathbf{x}_j^\\text{T}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{x}_{j-1}\\mathbf{x}_{j-1}^\\text{T}\\right)\\dots\\left(\\mathbf{I}-\\alpha\\mathbf{x}_i\\mathbf{x}_i^\\text{T}\\right),\\hspace{1cm}j\\geq i, \\end{equation} with $\\mathbf{A}_{j+1}^j\\doteq\\mathbf{I}$. Hence, we can express $\\mathbf{w}_t$ as: \\begin{equation} \\mathbf{w}_t=\\mathbf{w}_t^t=\\mathbf{A}_0^{t-1}\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{i:t}^\\lambda\\label{eq:totl.1} \\end{equation} Using \\eqref{eq:tt.1}, we have: \\begin{align} G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\u0026amp;=\\mathbf{w}_i^\\text{T}\\mathbf{x}_i+\\sum_{j=1}^{t}(\\gamma\\lambda)^{j-i}\\delta_j\u0026rsquo;-\\left(\\mathbf{w}_i^\\text{T}\\mathbf{x}_i+\\sum_{j=1}^{t-1}(\\gamma\\lambda)^{j-i}\\delta_j\u0026rsquo;\\right) \\\\ \u0026amp;=(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;\\label{eq:totl.2} \\end{align} with the TD error, $\\delta_t\u0026rsquo;$ is defined as earlier: \\begin{equation} \\delta_t\u0026rsquo;\\doteq R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\label{eq:totl.3} \\end{equation} Using \\eqref{eq:totl.1}, \\eqref{eq:totl.2} and \\eqref{eq:totl.3}, we have: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;=\\mathbf{A}_0^t\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t+1}^\\lambda \\\\ \u0026amp;=\\mathbf{A}_0^t\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t+1}^\\lambda+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026amp;=\\mathbf{A}_0^t\\mathbf{w}_0+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t}^\\lambda+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\left(\\mathbf{A}_0^{t-1}\\mathbf{w}_0+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{t:t+1}^\\lambda\\right)\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}\\right) \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t\\mathbf{x}_t\\right) \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t+\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right) \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;+\\alpha\\mathbf{x}_t\\delta_t\u0026rsquo;-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t\u0026rsquo;-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\delta_t\u0026rsquo;-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\left(\\delta_t+\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\delta_t+\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\left(\\mathbf{z}_t-\\mathbf{x}_t\\right),\\label{eq:totl.4} \\end{align} where in the eleventh step, we define $\\mathbf{z}_t$ as: \\begin{equation} \\mathbf{z}_t\\doteq\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}, \\end{equation} and in the twelfth step, we also define $\\delta_t$ as: \\begin{align} \\delta_t\u0026amp;\\doteq\\delta_t\u0026rsquo;-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t+\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;=R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\end{align} which is the same as the TD error of TD($\\lambda$) we have defined earlier.\nWe then need to derive an update rule to recursively compute $\\mathbf{z}_t$ from $\\mathbf{z}_{t-1}$, as: \\begin{align} \\mathbf{z}_t\u0026amp;=\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i} \\\\ \u0026amp;=\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}+\\mathbf{x}_t \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\gamma\\lambda\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i(\\gamma\\lambda)^{t-i-1}+\\mathbf{x}_t \\\\ \u0026amp;=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\gamma\\lambda\\mathbf{z}_{t-1}+\\mathbf{x}_t \\\\ \u0026amp;=\\gamma\\lambda\\mathbf{z}_{t-1}+\\left(1-\\alpha\\gamma\\lambda\\left(\\mathbf{z}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\right)\\mathbf{x}_t\\label{eq:totl.5} \\end{align} Equations \\eqref{eq:totl.4} and \\eqref{eq:totl.5} form the update of the true online TD($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t+\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\left(\\mathbf{z}t_t-\\mathbf{x}_t\\right), \\end{equation} where \\begin{align} \\mathbf{z}_t\u0026amp;\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\left(1-\\alpha\\gamma\\lambda\\left(\\mathbf{z}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\right)\\mathbf{x}_t,\\label{eq:totl.6} \\\\ \\delta_t\u0026amp;\\doteq R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t \\end{align} Pseudocode of the algorithm is given below.\nAs other methods above, below is an illustration of using true online TD($\\lambda$) on the random walk problem.\nFigure 5: Using True online TD($\\lambda$) on 19-state random walk. The code can be found here The eligible trace \\eqref{eq:totl.6} is called dutch trace to distinguish it from the trace \\eqref{eq:tl.1} of TD($\\lambda$), which is called accumulating trace.\nThere is another kind of trace called replacing trace, defined for the tabular case or for binary feature vectors \\begin{equation} z_{i,t}\\doteq\\begin{cases}1 \u0026amp;\\text{if }x_{i,t}=1 \\\\ \\gamma\\lambda z_{i,t-1} \u0026amp;\\text{if }x_{i,t}=0\\end{cases} \\end{equation}\nEquivalence between forward and backward views In this section, we will show that there is an interchange between forward and backward view.\nTheorem 1\nConsider any forward view that updates towards some interim targets $Y_k^t$ with \\begin{equation} \\mathbf{w}_{k+1}^t=\\mathbf{w}_k+\\eta_k\\left(Y_k^t-\\mathbf{x}_k^\\text{T}\\mathbf{w}_k^t\\right)\\mathbf{x}_k+\\mathbf{u}_k,\\hspace{1cm}0\\leq k\\lt t, \\end{equation} where $\\mathbf{w}_0^t=\\mathbf{w}_0$ for some initial $\\mathbf{w}_0$; $\\mathbf{u}_k\\in\\mathbb{R}^d$ is any vector that does not depend on $t$. Assume that the temporal differences $Y_k^{t+1}-Y_k^t$ for different $k$ are related through \\begin{equation} Y_k^{t+1}-Y_k^t=c_k(Y_{k+1}^{t+1}-Y_{k+1}^t),\\hspace{1cm}\\forall k\\lt t\\label{eq:ebfb.1} \\end{equation} where $c_k$ is a scalar that does not depend on $t$. Then the final weights $\\mathbf{w}_t^t$ at each time step $t$ are equal to the weight $\\mathbf{w}_t$ as defined by $\\mathbf{z}_0=\\eta_0\\mathbf{x}_0$ and the backward view \\begin{align} \\mathbf{w}_{t+1}\u0026amp;=\\mathbf{w}_t+(Y_t^{t+1}-Y_t^t)\\mathbf{z}_t+\\eta_t(Y_t^t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t, \\\\ \\mathbf{z}_t\u0026amp;=c_{t-1}\\mathbf{z}_{t-1}+\\eta_t\\left(1-c_{t-1}\\mathbf{x}_t^\\text{T}\\mathbf{z}_{t-1}\\right)\\mathbf{x}_t,\\hspace{1cm}t\\gt 0 \\end{align}\nProof\nLet $\\mathbf{F}_t\\doteq\\mathbf{I}-\\eta_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}$ be the fading matrix such that $\\mathbf{w}_{t+1}=\\mathbf{F}_k\\mathbf{w}_k^t+\\eta_k Y_k^t\\mathbf{x}_k$. For each step $t$, we have: \\begin{align} \\mathbf{w}_{t+1}^{t+1}-\\mathbf{w}_t^t\u0026amp;=\\mathbf{F}_t\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t+\\eta_t Y_t^{t+1}\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026amp;=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t Y_t^{t+1}\\mathbf{x}_t+(\\mathbf{F}_t-\\mathbf{I})\\mathbf{w}_t^t+\\mathbf{u}_t \\\\ \u0026amp;=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t Y_t^{t+1}\\mathbf{x}_t-\\eta_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\mathbf{w}_t^t+\\mathbf{u}_t \\\\ \u0026amp;=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t^t)\\mathbf{x}_t+\\mathbf{u}_t\\label{eq:ebfb.2} \\end{align} We also have that: \\begin{align} \\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t\u0026amp;=\\mathbf{F}_{t-1}(\\mathbf{w}_{t-1}^{t+1}-\\mathbf{w}_{t-1}^t)+\\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\\mathbf{x}_{t-1} \\\\ \u0026amp;=\\mathbf{F}_{t-1}\\mathbf{F}_{t-2}(\\mathbf{w}_{t-1}^{t+1}-\\mathbf{w}_{t-1}^t)+\\eta_{n-2}(Y_{t-2}^{t+1}-Y_{t-2}^t)\\mathbf{F}_{t-1}\\mathbf{x}_{t-2}\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\\mathbf{x}_{t-1} \\\\ \u0026amp;\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026amp;=\\mathbf{F}_{t-1}\\dots\\mathbf{F}_0(\\mathbf{w}_0^{t+1}-\\mathbf{w}_0^t)+\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\\mathbf{x}_k \\\\ \u0026amp;=\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\\mathbf{x}_k \\\\ \u0026amp;=\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}c_k(Y_{k+1}^{t+1}-Y_{k+1}^t)\\mathbf{x}_k \\\\ \u0026amp;\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026amp;=c_{t-1}\\underbrace{\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-2}c_j\\right)\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k}_{\\doteq\\mathbf{z}_{t-1}}(Y_t^{t+1}-Y_t^t) \\\\ \u0026amp;=c_{t-1}\\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t),\\label{eq:ebfb.3} \\end{align} where in the fifth step, we use the assumption \\eqref{eq:ebfb.1}; the vector $\\mathbf{z}_t$ defined in the sixth step can be computed recursively in terms of $\\mathbf{z}_{t-1}$: \\begin{align} \\mathbf{z}_t\u0026amp;=\\sum_{k=0}^{t}\\eta_k\\left(\\prod_{j=k}^{t-1}c_j\\right)\\mathbf{F}_1\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k \\\\ \u0026amp;=\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-1}c_j\\right)\\mathbf{F}_1\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k+\\eta_t\\mathbf{x}_t \\\\ \u0026amp;=c_{t-1}\\mathbf{F}_t\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-2}c_j\\right)\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k+\\eta_t\\mathbf{x}_t \\\\ \u0026amp;=c_{t-1}\\mathbf{F}_1\\mathbf{z}_{t-1}+\\eta_t\\mathbf{x}_t \\\\ \u0026amp;=c_{t-1}\\mathbf{z}_{t-1}+\\eta_t(1-c_{t-1}\\mathbf{x}_t^\\text{T}\\mathbf{z}_{t-1})\\mathbf{x}_t \\end{align} Plug \\eqref{eq:ebfb.3} back into \\eqref{eq:ebfb.2} we obtain: \\begin{align} \\mathbf{w}_{t+1}^{t+1}-\\mathbf{w}_t^t\u0026amp;=c_{t-1}\\mathbf{F}_t\\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026amp;=(\\mathbf{z}_t-\\eta_t\\mathbf{x}_t)(Y_t^{t+1}-Y_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026amp;=(Y_t^{t+1}-Y_t^t)\\mathbf{z}_t+\\eta_t(Y_t^t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\end{align} Since $\\mathbf{w}_{0,t}\\doteq\\mathbf{w}_0$, the desired result follows through induction.\nDutch Traces In Monte Carlo Sarsa($\\lambda$) To apply the use off eligible traces on control problems, we begin by defining the $n$-step return, which is the same as what we have defined before: \\begin{equation} \\hspace{-0.5cm}G_{t:t+n}\\doteq\\ R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}t+n\\lt T\\label{eq:sl.1} \\end{equation} with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$. With this definition of the return, the action-value form of offline $\\lambda$-return can be defined as: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t^\\lambda-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\right]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\hspace{1cm}t=0,\\dots,T-1 \\end{equation} where $G_t^\\lambda\\doteq G_{t:\\infty}^\\lambda$.\nThe TD method for action values, known as Sarsa($\\lambda$), approximates this forward view and has the same update rule as TD($\\lambda$): \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t, \\end{equation} except that the TD error, $\\delta_t$, is defined in terms of action-value function: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t), \\end{equation} and so it is with eligible trace vector: \\begin{align} \\mathbf{z}_{-1}\u0026amp;\\doteq\\mathbf{0}, \\\\ \\mathbf{z}\u0026amp;_t\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\hspace{1cm}0\\leq t\\lt T \\end{align}\nFigure 6: The backup diagram of Sarsa($\\lambda$) Pseudocode of the Sarsa($\\lambda$) is given below. There is also an action-value version of the online $\\lambda$-return algorithm, and its efficient implementation as true online TD($\\lambda$), called True online Sarsa($\\lambda$), which can be achieved by using $n$-step return \\eqref{eq:sl.1} instead (which also leads to the change of $\\mathbf{x}_t=\\mathbf{x}(S_t)$ to $\\mathbf{x}_t=\\mathbf{x}(S_t,A_t)$).\nPseudocode of the true online Sarsa($\\lambda$) is given below.\nVariable $\\lambda$ and $\\gamma$ We can generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action. In other words, each time step $t$, we will have a different $\\lambda$ and $\\gamma$, denoted as $\\lambda_t$ and $\\gamma_t$.\nIn particular, say $\\lambda:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$ such that $\\lambda_t\\doteq\\lambda(S_t,A_t)$ and similarly, $\\gamma:\\mathcal{S}\\to[0,1]$ such that $\\gamma_t\\doteq\\gamma(S_t)$.\nWith this definition of $\\gamma$, the return can be rewritten generally as: \\begin{align} G_t\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}G_{t+1} \\\\ \u0026amp;=R_{t+1}+\\gamma_{t+1}R_{t+2}+\\gamma_{t+1}\\gamma_{t+2}R_{t+3}+\\dots \\\\ \u0026amp;=\\sum_{k=t}^{\\infty}\\left(\\prod_{i=t+1}^{k}\\gamma_i\\right)R_{k+1}, \\end{align} where we require that $\\prod_{k=t}^{\\infty}\\gamma_k=0$ with probability $1$ for all $t$ to assure the sums are finite.\nThe generalization of $\\lambda$ also lets us rewrite the state-based $\\lambda$-return as: \\begin{equation} G_t^{\\lambda s}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\hat{v}(S_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda s}\\Big),\\label{eq:lg.1} \\end{equation} where $G_t^{\\lambda s}$ denotes that this $\\lambda$ -return is bootstrapped from state values, and hence the $G_t^{\\lambda a}$ denotes the $\\lambda$-return that bootstraps from action values. The Sarsa form of action-based $\\lambda$-return is defined as: \\begin{equation} G_t^{\\lambda a}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda a}\\Big), \\end{equation} and the Expected Sarsa form of its can be defined as: \\begin{equation} G_t^{\\lambda a}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\overline{V}_t(S_{t+1})+\\lambda_{t+1}G_{t+1}^{\\lambda a}\\Big),\\label{eq:lg.2} \\end{equation} where the expected approximate value is generalized to function approximation as: \\begin{equation} \\overline{V}_t\\doteq\\sum_a\\pi(a|s)\\hat{q}(s,a,\\mathbf{w}_t)\\label{eq:lg.3} \\end{equation}\nOff-policy Traces with Control Variates We can also apply the use of importance sampling with eligible traces.\nWe begin with the new definition of $\\lambda$-return, which is achieved by generalizing the $\\lambda$-return \\eqref{eq:lg.1} with the idea of control variates on $n$-step off-policy return: \\begin{equation} \\hspace{-0.5cm}G_t^{\\lambda s}\\doteq\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\big((1-\\lambda_{t+1})\\hat{v}(S_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda s}\\big)\\Big)+(1-\\rho_t)\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} where the single-step importance sampling ratio $\\rho_t$ is defined as usual: \\begin{equation} \\rho_t\\doteq\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation} Much like the other returns, the truncated version of this return can be approximated simply in terms of sums of state-based TD errors: \\begin{equation} G_t^{\\lambda s}\\approx\\hat{v}(S_t,\\mathbf{w}_t)+\\rho_t\\sum_{k=t}^{\\infty}\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i, \\end{equation} where the state-based TD error, $\\delta_t^s$, is defined as: \\begin{equation} \\delta_t^s\\doteq R_{t+1}+\\gamma_{t+1}\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t),\\label{eq:optcv.1} \\end{equation} with the approximation becoming exact if the approximate value function does not change. Given this approximation, we have that: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;=\\mathbf{w}_t+\\alpha\\left(G_t^{\\lambda s}-\\hat{v}(S_t,\\mathbf{w}_t)\\right)\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t) \\\\ \u0026amp;\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\sum_{k=t}^{\\infty}\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i\\right)\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t) \\end{align} This is one time step of a forward view. And in fact, the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time. Since the sum of the forward-view update over time is: \\begin{align} \\sum_{t=1}^{\\infty}(\\mathbf{w}_{t+1}-\\mathbf{w}_t)\u0026amp;\\approx\\sum_{t=1}^{\\infty}\\sum_{k=t}^{\\infty}\\alpha\\rho_t\\delta_k^s\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i \\\\ \u0026amp;=\\sum_{k=1}^{\\infty}\\sum_{t=1}^{k}\\alpha\\rho_t\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i \\\\ \u0026amp;=\\sum_{k=1}^{\\infty}\\alpha\\delta_k^s\\sum_{t=1}^{k}\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i,\\label{eq:optcv.2} \\end{align} where in the second step, we use the summation rule: $\\sum_{t=x}^{y}\\sum_{k=t}^{y}=\\sum_{k=x}^{y}\\sum_{t=x}^{k}$.\nLet $\\mathbf{z}_k$ is defined as: \\begin{align} \\mathbf{z}_k \u0026amp;=\\sum_{t=1}^{k}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t, \\mathbf{w}_t\\right)\\prod_{i=t+1}^{k} \\gamma_i\\lambda_i\\rho_i \\\\ \u0026amp;=\\sum_{t=1}^{k-1}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t,\\mathbf{w}_t\\right)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i+\\rho_k\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right) \\\\ \u0026amp;=\\gamma_k\\lambda_k\\rho_k\\underbrace{\\sum_{t=1}^{k-1}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t,\\mathbf{w}_t\\right)\\prod_{i=t+1}^{k-1}\\gamma_i\\lambda_i\\rho_i}_{\\mathbf{z}_{k-1}}+\\rho_k\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right) \\\\ \u0026amp;=\\rho_k\\big(\\gamma_k\\lambda_k\\mathbf{z}_{k-1}+\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right)\\big) \\end{align} Then we can rewrite \\eqref{eq:optcv.2} as: \\begin{equation} \\sum_{t=1}^{\\infty}\\left(\\mathbf{w}_{t+1}-\\mathbf{w}_t\\right)\\approx\\sum_{k=1}^{\\infty}\\alpha\\delta_k^s\\mathbf{z}_k, \\end{equation} which is sum of the backward-view update over time, with the eligible trace vector is defined as: \\begin{equation} \\mathbf{z}_t\\doteq\\rho_t\\big(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\big)\\label{eq:optcv.3} \\end{equation} Using this eligible trace with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$), we obtain a general TD($\\lambda$) algorithm that can be applied to either on-policy or off-policy data.\nIn the on-policy case, the algorithm is exactly TD($\\lambda$) because $\\rho_t=1$ for all $t$ and \\eqref{eq:optcv.3} becomes the accumulating trace \\eqref{eq:tl.1} with extending to variable $\\lambda$ and $\\gamma$. In the off-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable. For action-value function, we generalize the definition of the $\\lambda$-return \\eqref{eq:lg.2} of Expected Sarsa with the idea of control variate: \\begin{align} G_t^{\\lambda a}\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\big[\\rho_{t+1}G_{t+1}^{\\lambda a}+\\bar{V}_t(S_{t+1})\\nonumber \\\\ \u0026amp;\\hspace{2cm}-\\rho_{t+1}\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\big]\\Big) \\\\ \u0026amp;=R_{t+1}+\\gamma_{t+1}\\Big(\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\rho_{t+1}\\left[G_{t+1}^{\\lambda a}-\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\right]\\Big), \\end{align} where the expected approximate value $\\bar{V}_t(S_{t+1})$ is as given by \\eqref{eq:lg.3}.\nSimilar to the others, this $\\lambda$-return can also be written approximately as the sum of TD errors \\begin{equation} G_t^{\\lambda a}\\approx\\hat{q}(S_t,A_t,\\mathbf{w}_t)+\\sum_{k=t}^{\\infty}\\delta_k^a\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i, \\end{equation} with the action-based TD error is defined in terms of the expected approximate value: \\begin{equation} \\delta_t^a=R_{t+1}+\\gamma_{t+1}\\bar{V}_t(S_{t+1})-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:optcv.4} \\end{equation} Analogy to the state value function case, this approximation also becomes exact if the approximate value function does not change.\nSimilar to the state case \\eqref{eq:optcv.3}, we can also define the eligible trace for action values: \\begin{equation} \\mathbf{z}_t\\doteq\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Using this eligible trace with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$) and the expectation-based TD error \\eqref{eq:optcv.4}, we end up with an Expected Sarsa($\\lambda$) algorithm that can applied to either on-policy or off-policy data.\nIn the on-policy case with constant $\\lambda$ and $\\gamma$, this becomes the Sarsa($\\lambda$) algorithm. Tree-Backup($\\lambda$) Recall that in the note of TD-Learning, we have mentioned that there is an off-policy method without importance sampling called tree-backup. Can we extend the idea of tree-backup to an eligible trace version? Yes, we can.\nAs usual, we begin with establishing the $\\lambda$-return by generalizing the $\\lambda$-return of Expected Sarsa \\eqref{eq:lg.2} with the $n$-step Tree-backup return: \\begin{align} G_t^{\\lambda a}\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}\\Bigg((1-\\lambda_{t+1})\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\Big[\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\pi(A_{t+1}|S_{t+1})G_{t+1}^{\\lambda a}\\Big]\\Bigg) \\\\ \u0026amp;=R_{t+1}+\\gamma_{t+1}\\Big(\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\pi(A_{t+1}|S_{t+1})\\left(G_{t+1}^{\\lambda a}-\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\right)\\Big) \\end{align} This return, as usual, can also be written approximately (ignoring changes in the approximate value function) as sum of TD errors: \\begin{equation} G_t^{\\lambda a}\\approx\\hat{q}(S_t,A_t,\\mathbf{w}_t)+\\sum_{k=t}^{\\infty}\\delta_k^a\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\pi(A_i|S_i), \\end{equation} with the TD error is defined as given by \\eqref{eq:optcv.4}.\nSimilar to how we derive the eligible trace \\eqref{eq:optcv.3}, we can define a new eligible trace in terms of target-policy probabilities of the selected actions: \\begin{equation} \\mathbf{z}_t\\doteq\\gamma_t\\lambda_t\\pi(A_t|S_t)\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Using this eligible trace vector with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$), we end up with the Tree-Backup($\\lambda$) or TB($\\lambda$).\nFigure 7: The backup diagram of Tree Backup($\\lambda$) Other Off-policy Methods with Traces GTD($\\lambda$) GTD($\\lambda$) is the extended version of TDC, a state-value Gradient-TD method, with eligible traces.\nIn this algorithm, we will define a new off-policy, $\\lambda$-return, not like usual but as a function: \\begin{equation} G_t^{\\lambda}(v)\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})v(S_{t+1})+\\lambda_{t+1}G_{t+1}^{\\lambda}(v)\\Big]\\label{eq:gl.1} \\end{equation} where $v(s)$ denotes the value at state $s$, and $\\lambda\\in[0,1]$ is the trace-decay parameter.\nLet $T_\\pi^\\lambda$ denote the $\\lambda$-weighted Bellman operator for policy $\\pi$ such that: \\begin{align} v_\\pi(s)\u0026amp;=\\mathbb{E}\\Big[G_t^\\lambda(v_\\pi)\\big|S_t=s,\\pi\\Big] \\\\ \u0026amp;\\doteq (T_\\pi^\\lambda v_\\pi)(s) \\end{align}\nConsider using linear function approximation, or in particular, we are trying to approximate $v(s)$ by $v_\\mathbf{w}(s)=\\mathbf{w}^\\text{T}\\mathbf{x}(s)$. Our objective is to find the fixed point which satisfies: \\begin{equation} v_\\mathbf{w}=\\Pi T_\\pi^\\lambda v_\\mathbf{w},\\label{eq:gl.2} \\end{equation} where $\\Pi v$ is a projection of $v$ into the space of representable functions $\\{v_\\mathbf{w}|\\mathbf{w}\\in\\mathbb{R}^d\\}$. Let $\\mu$ be the steady-state distribution of states under the behavior policy $b$. Then, the projection can be defined as: \\begin{equation} \\Pi v\\doteq v_{\\mathbf{w}}, \\end{equation} where \\begin{equation} \\mathbf{w}=\\underset{\\mathbf{w}\\in\\mathbb{R}^d}{\\text{argmin}}\\left\\Vert v-v_\\mathbf{w}\\right\\Vert_\\mu^2, \\end{equation} In a linear case, in which $v_\\mathbf{w}=\\mathbf{X}\\mathbf{w}$, the projection operator is linear and independent of $\\mathbf{w}$: \\begin{equation} \\Pi=\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}, \\end{equation} where $\\mathbf{D}$ denotes $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix whose diagonal elements are $\\mu(s)$, and $\\mathbf{X}$ denotes the $\\vert\\mathcal{S}\\vert\\times d$ matrix whose rows are the feature vectors $\\mathbf{x}(s)^\\text{T}$, one for each state $s$.\nWith linear function approximation, we can rewrite the $\\lambda$-return \\eqref{eq:gl.1} as: \\begin{equation} G_t^{\\lambda}(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda}(\\mathbf{w})\\Big]\\label{eq:gl.3} \\end{equation} Let \\begin{equation} \\delta_t^\\lambda(\\mathbf{w})\\doteq G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t, \\end{equation} and \\begin{equation} \\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\doteq\\sum_s\\mu(s)\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})\\big|S_t=s,\\pi\\Big]\\mathbf{x}(s), \\end{equation} where $\\mathcal{P}_\\mu^\\pi$ is an operator.\nThe fixed point in \\eqref{eq:gl.2} can be found by minimizing the Mean Square Projected Bellman Error (MSPBE): \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=\\Big\\Vert v_\\mathbf{w}-\\Pi T_\\pi^\\lambda v_\\mathbf{w}\\Big\\Vert_\\mu^2 \\\\ \u0026amp;=\\Big\\Vert\\Pi(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w})\\Big\\Vert_\\mu^2 \\\\ \u0026amp;=\\Big(\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)\\Big)^\\text{T}\\mathbf{D}\\Big(\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)\\Big) \\\\ \u0026amp;=\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)^\\text{T}\\Pi^\\text{T}\\mathbf{D}\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right) \\\\ \u0026amp;=\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)^\\text{T}\\mathbf{D}^\\text{T}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{D}\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right) \\\\ \u0026amp;=\\Big(\\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-\\mathbf{w}\\right)\\Big)^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)\\label{eq:gl.4} \\end{align}\nFrom the definition of $T_\\pi^\\lambda$ and $\\delta_t^\\lambda$, we have: \\begin{align} (T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{v})(s)\u0026amp;=\\mathbb{E}\\Big[G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\big|S_t=s,\\pi\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})\\big|S_t=s,\\pi\\Big]\\label{eq:gl.5} \\end{align} Therefore, \\begin{align} \\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)\u0026amp;=\\sum_s\\mu(s)\\Big[\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)(s)\\Big]\\mathbf{x}(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})|S_t=s,\\pi\\Big]\\mathbf{x}(s) \\\\ \u0026amp;=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\label{eq:gl.6} \\end{align} Moreover, we also have: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}=\\sum_s\\mu(s)\\mathbf{x}(s)\\mathbf{x}(s)^\\text{T}=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]\\label{eq:gl.7} \\end{equation} Substitute \\eqref{eq:gl.5}, \\eqref{eq:gl.6} and \\eqref{eq:gl.7} back to the \\eqref{eq:gl.4}, we have: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)\\label{eq:gl.8} \\end{equation} In the objective function \\eqref{eq:gl.8}, the expectation terms are w.r.t the policy $\\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy problem, as usual, we use importance sampling.\nWe then instead use an importance-sampling version of $\\lambda$-return \\eqref{eq:gl.3}: \\begin{equation} G_t^{\\lambda\\rho}(\\mathbf{w})=\\rho_t\\left(R_{t+1}+\\gamma_{t+1}\\left[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\right]\\right), \\end{equation} where the single-step importance sampling ratio $\\rho_t$ is defined as usual: \\begin{equation} \\rho_t\\doteq\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation} This also leads to an another version of $\\delta_t^\\lambda$, defined as: \\begin{equation} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\\doteq G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} With this definition of the $\\lambda$-return, we have: \\begin{align} \u0026amp;\\hspace{-1cm}\\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big]\\nonumber \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[\\rho_t\\big(R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big)+\\rho_t\\gamma_{t+1}\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[\\rho_t\\big(R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big)\\big|S_t=s\\Big]+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big|S_t=s,\\pi\\Big]\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\sum_{a,s\u0026rsquo;}p(s\u0026rsquo;|s,a)b(a|s)\\frac{\\pi(a|s)}{b(a|s)}\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big|S_t=s,\\pi\\Big]\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\sum_{a,s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\pi(a|s)\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;\\Big]\\big|S_t=s,\\pi\\Big], \\end{align} which, as it continues to roll out, gives us: \\begin{equation} \\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big]=\\mathbb{E}\\Big[G_t^{\\lambda}(\\mathbf{w})\\big|S_t=s,\\pi\\Big] \\end{equation} And eventually, we get: \\begin{equation} \\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t \\end{equation} because the state distribution is based on behavior state-distribution $\\mu$.\nWith this result, our objective function \\eqref{eq:gl.8} can be written as: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big) \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\\label{eq:gl.9} \\end{align} From the definition of $\\delta_t^{\\lambda\\rho}$, we have: \\begin{align} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\u0026amp;=G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big]\\Big)-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t+\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)\\nonumber \\\\ \u0026amp;\\hspace{2cm}-\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)+\\rho_t\\mathbf{w}^\\text{T}\\mathbf{x}_t-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\Big(G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big) \\\\ \u0026amp;=\\rho_t\\delta_t(\\mathbf{w})+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w}), \\end{align} where the TD error, $\\delta_t(\\mathbf{w})$, is defined as usual: \\begin{equation} \\delta_t(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} Also, we have that: \\begin{align} \\mathbb{E}\\Big[(1-\\rho_t)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\mathbf{x}_t\\Big]\u0026amp;=\\sum_{s,a}\\mu(s)b(a|s)\\left(1-\\frac{\\pi(a|s)}{b(a|s)}\\right)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026amp;=\\sum_s\\mu(s)\\left(\\sum_a b(a|s)-\\sum_a\\pi(a|s)\\right)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026amp;=\\sum_s\\mu(s)(1-1)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026amp;=0 \\end{align} Given these results, we have: \\begin{align} \\hspace{-1cm}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\mathbf{x}_t+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+0+\\mathbb{E}_{\\pi b}\\Big[\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\big(\\rho_t\\delta_t(\\mathbf{w})+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\big(\\rho_t\\delta_t(\\mathbf{w})+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}\\big)+\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}\\big)+\\rho_{t-2}\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}\\Big] \\\\ \u0026amp;\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\rho_t\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}+\\rho_{t-2}\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-2}+\\dots\\big)\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big], \\end{align} where \\begin{equation} \\mathbf{z}_t=\\rho_t(\\mathbf{x}_t+\\gamma_t\\lambda_t\\mathbf{z}_{t-1}) \\end{equation} Plugging this result back to \\eqref{eq:gl.9} lets our objective function become: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\label{eq:gl.10} \\end{equation} Similar to TDC, we also use gradient descent in order to find the minimum value of $\\overline{\\text{PBE}}(\\mathbf{w})$. The gradient of our objective function w.r.t the weight vector $\\mathbf{w}$ is: \\begin{align} \\hspace{-1.2cm}\\frac{1}{2}\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=-\\frac{1}{2}\\nabla_\\mathbf{w}\\Bigg(\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\Bigg) \\\\ \u0026amp;=\\nabla_\\mathbf{w}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\big(\\gamma_{t+1}\\mathbf{x}_{t+1}-\\mathbf{x}_t\\big)\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\rho_t\\big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\mathbf{z}_{t-1}\\big)^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\big(\\mathbf{x}_t\\rho_t\\mathbf{x}_t^\\text{T}+\\mathbf{x}_t\\rho_t\\gamma_t\\lambda_t\\mathbf{z}_{t-1}^\\text{T}\\big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\mathbf{x}_{t+1}\\gamma_{t+1}\\lambda_{t+1}\\mathbf{z}_t^\\text{T}\\big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}-\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbf{v}(\\mathbf{w}),\\label{eq:gl.11} \\end{align} where in the seventh step, we have used shifting indices trick and the identities: \\begin{align} \\mathbb{E}\\Big[\\mathbf{x}_t\\rho_t\\mathbf{x}_t^\\text{T}\\Big]\u0026amp;=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big], \\\\ \\mathbb{E}\\Big[\\mathbf{x}_{t+1}\\rho_t\\gamma_t\\lambda_t\\mathbf{z}_t^\\text{T}\\Big]\u0026amp;=\\mathbb{E}\\Big[\\mathbf{x}_{t+1}\\gamma_t\\lambda_t\\mathbf{z}_t^\\text{T}\\Big] \\end{align} and where in the final step, we define: \\begin{equation} \\mathbf{v}(\\mathbf{w})\\doteq\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} By direct sampling from \\eqref{eq:gl.11} and following TDC derivation steps we obtain the GTD($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^s\\mathbf{z}_t-\\alpha\\gamma_{t+1}(1-\\lambda_{t+1})(\\mathbf{z}_t^\\text{T}\\mathbf{v}_t)\\mathbf{x}_{t+1}, \\end{equation} where\nthe TD error $\\delta_t^s$ is defined, as usual, as state-based TD error \\eqref{eq:optcv.1}; the eligible trace vector $\\mathbf{z}_t$ is defined as given in \\eqref{eq:optcv.3} for state value; and $\\mathbf{v}_t$ is a vector of the same dimension as $\\mathbf{w}$, initialized to $\\mathbf{v}_0=\\mathbf{0}$ with $\\beta\u0026gt;0$ is a step-size parameter: \\begin{align} \\delta_t^s\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026amp;\\doteq\\rho_t(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\mathbf{x}_t), \\\\ \\mathbf{v}_{t+1}\u0026amp;\\doteq\\mathbf{v}_t+\\beta\\delta_t^s\\mathbf{z}_t-\\beta(\\mathbf{v}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t \\end{align} GQ($\\lambda$) GQ($\\lambda$) is another eligible trace version of a Gradient-TD method but with action values. Its goal is to learn a parameter $\\mathbf{w}_t$ such that $\\hat{q}(s,a,\\mathbf{w}_t)\\doteq\\mathbf{w}_t^\\text{T}\\mathbf{x}(s,a)\\approx q_\\pi(s,a)$ from data given by following a behavior policy $b$.\nSimilar to the state-values case of GTD($\\lambda$), we begin with the definition of $\\lambda$-return (function): \\begin{equation} G_t^\\lambda(q)\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})q(S_{t+1},A_{t+1})+\\lambda_{t+1}G_{t+1}^\\lambda(q)\\Big],\\label{eq:gql.1} \\end{equation} where $q(s,a)$ denotes the value of taking action $a$ at state $s$ and $\\lambda\\in[0,1]$ is the trace decay parameter.\nLet $T_\\pi^\\lambda$ denote the $\\lambda$-weighted state-action version of the affine $\\vert\\mathcal{S}\\times\\mathcal{A}\\vert\\times\\vert\\mathcal{S}\\times\\mathcal{A}\\vert$ Bellman operator for the target policy $\\pi$ such that: \\begin{align} q_\\pi(s,a)\u0026amp;=\\mathbb{E}\\Big[G_t^\\lambda(q_\\pi)\\big|S_t=s,A_t=a,\\pi\\Big] \\\\ \u0026amp;\\doteq(T_\\pi^\\lambda q_\\pi)(s,a) \\end{align} Analogous to the state value functions, with linear function approximation (i.e., we are trying to estimate $q(s,a)$ by $q_\\mathbf{w}(s,a)=\\mathbf{w}^\\text{T}\\mathbf{x}(s,a)$), our objective is to find the fixed point $q_\\mathbf{w}$ such that: \\begin{equation} q_\\mathbf{w}=\\Pi T_\\pi^\\lambda q_\\mathbf{w}, \\end{equation} where $\\Pi$ is the projection operator defined as above. This point also can be found by minimizing the MSPBE objective function: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=\\left\\Vert q_\\mathbf{w}-\\Pi T_\\pi^\\lambda q_\\mathbf{w}\\right\\Vert_\\mu^2 \\\\ \u0026amp;=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big),\\label{eq:gql.2} \\end{align} where the second step is acquired from the result \\eqref{eq:gl.8}, and where the TD error $\\delta_t^\\lambda$ is defined as the above section: \\begin{equation} \\delta_t^\\lambda(\\mathbf{w})\\doteq G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} where $G_t^\\lambda$ as given in \\eqref{eq:gl.3}.\nIn the objective function \\eqref{eq:gql.2}, the expectation terms are w.r.t the policy $\\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy issue, as usual, we use importance sampling.\nWe start with the definition of the $\\lambda$-return \\eqref{eq:gql.1}, which is a noisy estimate of the future return by following policy $\\pi$. In order to have a noisy estimate for the return of target policy $\\pi$ while following behavior policy $b$, we define another $\\lambda$-return (function), based on importance sampling: \\begin{equation} G_t^{\\lambda\\rho}(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big],\\label{eq:gql.3} \\end{equation} where $\\bar{\\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\\pi$: \\begin{equation} \\bar{\\mathbf{x}}_t\\doteq\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a), \\end{equation} where $\\rho_t$ is the single-step importance sampling ratio, and $G_t^{\\lambda\\rho}(\\mathbf{w})$ is a noisy guess of future rewards of target policy $\\pi$, if the agent follows policy $\\pi$ from time $t$.\nLet \\begin{equation} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\\doteq G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\label{eq:gql.4} \\end{equation} With the definition of the $\\lambda$-return \\eqref{eq:gql.3}, we have that: \\begin{align} \u0026amp;\\hspace{-0.9cm}\\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big]\\nonumber \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big)\\big|S_t=s,A_t=a\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026amp;+\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026amp;+\\sum_{s\u0026rsquo;}p(s\u0026rsquo;|s,a)\\sum_{a\u0026rsquo;}b(a\u0026rsquo;|s\u0026rsquo;)\\frac{\\pi(a\u0026rsquo;|s\u0026rsquo;)}{b(a\u0026rsquo;|s\u0026rsquo;)}\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;,A_{t+1}=a\u0026rsquo;\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026amp;+\\sum_{s\u0026rsquo;,a\u0026rsquo;}p(s\u0026rsquo;|s,a)\\pi(a\u0026rsquo;|s\u0026rsquo;)\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;,A_{t+1}=a\u0026rsquo;\\Big] \\\\ \u0026amp;\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\nonumber \\\\ \u0026amp;+\\gamma_{t+1}\\lambda_{t_1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s\u0026rsquo;,A_{t+1}=a\u0026rsquo;\\Big]\\big|S_t=s,A_t=a,\\pi\\Big], \\end{align} which, as continues to roll out, gives us: \\begin{equation} \\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big]=\\mathbb{E}\\Big[G_t^\\lambda(\\mathbf{w})\\big|S_t=s,A_t=a,\\pi\\Big] \\end{equation} And eventually, it yields: \\begin{equation} \\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t, \\end{equation} because the state-action distribution is based on the behavior state-action pair distribution, $\\mu$.\nHence, the objective function \\eqref{eq:gql.2} can be written as: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big) \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\\label{eq:gql.5} \\end{align} From the definition of the importance-sampling based TD error\\eqref{eq:gql.4}, we have: \\begin{align} \u0026amp;\\hspace{-0.8cm}\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026amp;\\hspace{-1cm}=G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;\\hspace{-1cm}=R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big]-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;\\hspace{-1cm}=\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\Big]+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026amp;\\hspace{-1cm}=\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)-\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w}) \\\\ \u0026amp;\\hspace{-1cm}=\\delta_t(\\mathbf{w})-\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026amp;\\hspace{1cm}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\Big(\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big) \\\\ \u0026amp;\\hspace{-1cm}=\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\Big(G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big)+\\gamma_{t+1}\\lambda_{t+1}\\Big(\\rho_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\Big) \\\\ \u0026amp;\\hspace{-1cm}=\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big), \\end{align} where in the fifth step, we define: \\begin{equation} \\delta_t(\\mathbf{w})\\doteq R_{t+1}+\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\label{eq:gql.6} \\end{equation} Note that the last part of the above equation has expected value of vector zero under the behavior policy $b$ because: \\begin{align} \\mathbb{E}\\Big[\\rho_t\\mathbf{x}_t\\big|S_t\\Big]\u0026amp;=\\sum_a b(a|S_t)\\frac{\\pi(a|S_t)}{b(a|S_t)}\\mathbf{x}(S_t,a) \\\\ \u0026amp;=\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a) \\\\ \u0026amp;=\\bar{\\mathbf{x}}_t \\end{align} With the result obtained above, we have: \\begin{align} \\hspace{-1cm}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\u0026amp;=\\mathbb{E}\\Big[\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\Big)\\mathbf{x}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big)\\mathbf{x}_t\\Big]\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\mathbb{E}\\Big[\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\mathbf{x}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]+0 \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\rho_t\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\Big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_{t-1}\\Big]\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big]+0 \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_{t-1}\\big)\\Big]+\\mathbb{E}\\Big[\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}\\Big] \\\\ \u0026amp;\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026amp;=\\mathbb{E}_b\\Big[\\delta_t(\\mathbf{w})\\Big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_{t-1}+\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}+\\dots\\Big)\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big], \\end{align} where \\begin{equation} \\mathbf{z}_t\\doteq\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}\\label{eq:gql.7} \\end{equation} Plugging this result back to our objective function \\eqref{eq:gql.5} gives us: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} Following the derivation of GTD($\\lambda$), we have: \\begin{align} \u0026amp;-\\frac{1}{2}\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})\\nonumber \\\\ \u0026amp;=-\\frac{1}{2}\\nabla_\\mathbf{w}\\Bigg(\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\Bigg) \\\\ \u0026amp;=\\nabla_\\mathbf{w}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\big(\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{x}_t\\big)\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\Big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}\\Big)^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_t\\mathbf{z}_{t-1}^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_{t+1}\\lambda_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbf{v}(\\mathbf{w}), \\end{align} where in the eighth step, we have used the identity: \\begin{equation} \\mathbb{E}\\Big[\\rho_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]=\\mathbb{E}\\Big[\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big], \\end{equation} and where in the final step, we define: \\begin{equation} \\mathbf{v}(\\mathbf{w})\\doteq\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} By direct sampling from the above gradient-descent direction and weight-duplication trick, we obtain the GQ($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^a\\mathbf{z}_t-\\alpha\\gamma_{t+1}(1-\\lambda_{t+1})(\\mathbf{z}_t^\\text{T}\\mathbf{v}_t)\\bar{\\mathbf{x}}_{t+1}, \\end{equation} where\n$\\bar{\\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\\pi$; $\\delta_t^a$ is the expectation form of the TD error, defined as \\eqref{eq:gql.6}; the eligible trace vector $\\mathbf{z}_t$ is defined as \\eqref{eq:gql.7} for action value; and $\\mathbf{v}_t$ is defined as in GTD($\\lambda$): \\begin{align} \\bar{\\mathbf{x}}_t\u0026amp;\\doteq\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a), \\\\ \\delta_t^a\u0026amp;\\doteq R_{t+1}+\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026amp;\\doteq\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}+\\mathbf{x}_t, \\\\ \\mathbf{v}_{t+1}\u0026amp;\\doteq\\mathbf{v}_t+\\beta\\delta_t^a\\mathbf{z}_t-\\beta(\\mathbf{v}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t \\end{align} Greedy-GQ($\\lambda$) If the target policy is $\\varepsilon$-greedy, or otherwise biased towards the greedy policy for $\\hat{q}$, then GQ($\\lambda$) can be used as a control algorithm, called Greedy-GQ($\\lambda$).\nIn the case of $\\lambda=0$, called GQ(0), Greedy-GQ($\\lambda$) is defined by: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^a\\mathbf{x}_t+\\alpha\\gamma_{t+1}(\\mathbf{z}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}(S_{t+1},a_{t+1}^{*}), \\end{equation} where the eligible trace $\\mathbf{z}_t$, TD error $\\delta_t^a$ and $a_{t+1}^{*}$ are defined as: \\begin{align} \\mathbf{z}_t\u0026amp;\\doteq\\mathbf{z}_t+\\beta\\delta_t^a\\mathbf{x}_t-\\beta(\\mathbf{z}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t, \\\\ \\delta_t^a\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}\\max_a\\Big(\\mathbf{w}_t^\\text{T}\\mathbf{x}(S_{t+1},a)\\Big)-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ a_{t+1}^{*}\u0026amp;\\doteq\\underset{a}{\\text{argmax}}\\Big(\\mathbf{w}_t^\\text{T}\\mathbf{x}(S_{t+1},a)\\Big), \\end{align} where $\\beta\u0026gt;0$ is a step-size parameter.\nHTD($\\lambda$) HTD($\\lambda$) is a hybrid state-value algorithm combining aspects of GTD($\\lambda$) and TD($\\lambda$), and has the following update: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;\\doteq\\mathbf{w}_t+\\alpha\\delta_t^s\\mathbf{z}_t+\\alpha\\left(\\left(\\mathbf{z}_t-\\mathbf{z}_t^b\\right)^\\text{T}\\mathbf{v}_t\\right)\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right), \\\\ \\mathbf{v}_{t+1}\u0026amp;\\doteq\\mathbf{v}_t+\\beta\\delta_t^s\\mathbf{z}_t-\\beta\\left({\\mathbf{z}_t^b}^\\text{T}\\mathbf{v}_t\\right)\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right), \\\\ \\mathbf{z}_t\u0026amp;\\doteq\\rho_t\\left(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\mathbf{x}_t\\right), \\\\ \\mathbf{z}_t^b\u0026amp;\\doteq\\gamma_t\\lambda_t\\mathbf{z}_{t-1}^b+\\mathbf{x}_t, \\end{align}\nEmphatic TD($\\lambda$) Emphatic TD($\\lambda$) (ETD($\\lambda$)) is the extension of the one-step Emphatic-TD algorithm to eligible traces. It is defined by: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t, \\\\ \\delta_t\u0026amp;\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026amp;\\doteq\\rho_t\\left(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\right), \\\\ M_t\u0026amp;\\doteq\\gamma_t i(S_t)+(1-\\lambda_t)F_t, \\\\ F_t\u0026amp;\\doteq\\rho_{t-1}\\gamma_t F_{t-1}+i(S_t), \\end{align} where\n$M_t\\geq 0$ is the general form of emphasis; $i:\\mathcal{S}\\to[0,\\infty)$ is the interest function $F_t\\geq 0$ is the followon trace, with $F_0\\doteq i(S_0)$. Stability Consider any stochastic algorithm of the form, \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha(\\mathbf{b}_t-\\mathbf{A}_t\\mathbf{w}_t), \\end{equation} where $\\mathbf{A}_t\\in\\mathbb{R}^d\\times\\mathbb{R}^d$ be a matrix and $\\mathbf{b}_t\\in\\mathbb{R}^d$ be a vector that varies over time. Let \\begin{align} \\mathbf{A}\u0026amp;\\doteq\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{A}_t\\right], \\\\ \\mathbf{b}\u0026amp;\\doteq\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{b}_t\\right] \\end{align} We define the stochastic update to be stable if and only if the corresponding deterministic algorithm, \\begin{equation} \\bar{\\mathbf{w}}_{t+1}\\doteq\\bar{\\mathbf{w}}_t+\\alpha\\left(\\mathbf{b}-\\mathbf{A}\\bar{\\mathbf{w}}_t\\right), \\end{equation} is convergent to a unique fixed point independent of the initial $\\bar{\\mathbf{w}}_0$. This will occur iff $\\mathbf{A}$ has a full set of eigenvalues having positive real parts, which can be proved if $\\mathbf{A}$ is positive definite.\nWith this definition of stability, in order to exam the stability of ETD($\\lambda$), we begin by considering the SGD update for the weight vector $\\mathbf{w}$ at time step $t$. \\begin{align} \\mathbf{w}_{t+1}\u0026amp;\\doteq\\mathbf{w}_t+\\alpha\\left(R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right)\\mathbf{z}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\left(\\mathbf{z}_t R_{t+1}-\\mathbf{z}_t\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right)^\\text{T}\\mathbf{w}_t\\right)\\label{eq:es.1} \\end{align} Let $\\mathbf{A}_t\\in\\mathbb{R}^d\\times\\mathbb{R}^d$ be a matrix and $\\mathbf{b}_t\\in\\mathbb{R}^d$ be a vector such that: \\begin{align} \\mathbf{A}_t\u0026amp;\\doteq\\mathbf{z}_t\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right)^\\text{T}, \\\\ \\mathbf{b}_t\u0026amp;\\doteq\\mathbf{z}_t R_{t+1} \\end{align} The stochastic update \\eqref{eq:es.1} is then can be written as: \\begin{align} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left(\\mathbf{b}_t-\\mathbf{A}_t\\mathbf{w}_t\\right) \\end{align} From the definition of $\\mathbf{A}$, we have: \\begin{align} \\mathbf{A}\u0026amp;=\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{A}_t\\right] \\\\ \u0026amp;=\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\Big] \\\\ \u0026amp;=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026amp;=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\rho_t\\big(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big)\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026amp;=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big]\\mathbb{E}_b\\Big[\\rho_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026amp;=\\sum_s\\underbrace{\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big]}_{\\mathbf{z}(s)}\\mathbb{E}_b\\Big[\\rho_k\\big(\\mathbf{x}_k-\\gamma_{k+1}\\mathbf{x}_{k+1}\\big)^\\text{T}\\big|S_k=s\\Big] \\\\ \u0026amp;=\\sum_s\\mathbf{z}(s)\\mathbb{E}_\\pi\\Big[\\mathbf{x}_k-\\gamma_{k+1}\\mathbf{x}_{k+1}\\big|S_k=s\\Big] \\\\ \u0026amp;=\\sum_s\\mathbf{z}(s)\\Big(\\mathbf{x}_t-\\sum_{s\u0026rsquo;}\\left[\\mathbf{P}_\\pi\\right]_{ss\u0026rsquo;}\\gamma(s\u0026rsquo;)\\mathbf{x}(s\u0026rsquo;)\\Big)^\\text{T} \\\\ \u0026amp;=\\mathbf{Z}\\left(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\right)\\mathbf{X},\\label{eq:es.2} \\end{align} where\nin the fifth step, given $S_t=s$, $\\mathbf{z}_{t-1}$ and $M_t$ are independent of $\\rho_t(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1})^\\text{T}$; $\\mathbf{P}_\\pi$ represents the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix of transition probabilities: \\begin{equation} \\left[\\mathbf{P}_\\pi\\right]_{ij}\\doteq\\sum_a\\pi(a|i)p(j|i,a), \\end{equation} where $p(j|i,a)\\doteq P(S_{t+1}=j|S_i=s,A_i=a)$. $\\mathbf{Z}$ is a $\\vert\\mathcal{S}\\vert\\times d$ matrix, whose rows are $\\mathbf{z}(s)$\u0026rsquo;s, i.e. $\\mathbf{Z}^\\text{T}\\doteq\\left[\\mathbf{z}(s_1),\\dots,\\mathbf{z}(s_{\\vert\\mathcal{S}\\vert})\\right]$, with $\\mathbf{z}(s)\\in\\mathbb{R}^d$ is a vector defined by1: \\begin{align} \\mathbf{z}(s)\u0026amp;\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big] \\\\ \u0026amp;=\\underbrace{\\mu_(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big|S_t=s\\Big]}_{m(s)}\\mathbf{x}_t+\\gamma(s)\\lambda(s)\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_t=s\\Big] \\\\ \u0026amp;=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\mu(s)\\lim_{t\\to\\infty}\\sum_{\\bar{s},\\bar{a}}p(S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}|S_t=s)\\nonumber \\\\ \u0026amp;\\hspace{2cm}\\times\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}\\Big] \\\\ \u0026amp;=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\mu(s)\\sum_{\\bar{s},\\bar{a}}\\frac{\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})}{\\mu(s)}\\nonumber \\\\ \u0026amp;\\hspace{2cm}\\times\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}\\Big] \\\\ \u0026amp;=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s},\\bar{a}}\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}\\nonumber \\\\ \u0026amp;\\hspace{2cm}\\times\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_{t-1}\\lambda_{t-1}\\mathbf{z}_{t-2}+M_{t-1}\\mathbf{x}_{t-1}\\big|S_t=s\\Big] \\\\ \u0026amp;=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s}}\\Big(\\sum_{\\bar{a}}\\pi(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\Big)\\mathbf{z}(\\bar{s}) \\\\ \u0026amp;=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s}}\\left[\\mathbf{P}_\\pi\\right]_{\\bar{s}s}\\mathbf{z}(\\bar{s})\\label{eq:es.3} \\end{align} We now introduce three $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrices:\n$\\mathbf{M}$, which has the $m(s)\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big\\vert S_t=s\\Big]$ on its diagonal; $\\mathbf{\\Gamma}$, which has the $\\gamma(s)$ on its diagonal; $\\mathbf{\\Lambda}$, which has the $\\lambda(s)$ on its diagonal. With these matrices, we can rewrite \\eqref{eq:es.3} in matrix form, as: \\begin{align} \\mathbf{Z}^\\text{T}\u0026amp;=\\mathbf{X}^\\text{T}\\mathbf{M}+\\mathbf{Z}^\\text{T}\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda} \\\\ \\Rightarrow\\mathbf{Z}^\\text{T}\u0026amp;=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1} \\end{align} Substitute this equation back to \\eqref{eq:es.2}, we obtain: \\begin{equation} \\mathbf{A}=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{X}\\label{eq:es.4} \\end{equation} Doing similar steps, we can also obtain the ETD($\\lambda$)\u0026rsquo;s $\\mathbf{b}$ vector: \\begin{equation} \\mathbf{b}=\\mathbf{Z}\\mathbf{r}_\\pi=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}\\mathbf{r}_\\pi, \\end{equation} where $\\mathbf{r}_\\pi\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ is the vector of expected immediate rewards from each state under $\\pi$.\nSince the positive definiteness of $\\mathbf{A}$ implies the stability of the algorithm, from \\eqref{eq:es.4}, it is sufficient to prove the positive definiteness of the key matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})$ because this matrix can be written in the form of: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{X}=\\sum_{i=1}^{\\vert\\mathcal{S}\\vert}\\mathbf{x}_i^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{x}_i \\end{equation} To prove this definiteness, we begin by writing the last part of the key matrix in form of the identity matrix minus a probability matrix.\nLet $\\mathbf{P}_\\pi^\\lambda$ be the matrix with this probability as its $\\{ij\\}$-component. This matrix can be written as: \\begin{align} \\mathbf{P}_\\pi^\\lambda\u0026amp;=\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda})+\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda})+\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{\\Lambda}\\mathbf{P}_\\pi\\mathbf{\\Gamma})^2(\\mathbf{I}-\\mathbf{\\Gamma}) \\\\ \u0026amp;=\\left(\\sum_{k=0}^{\\infty}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^k\\right)\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda}) \\\\ \u0026amp;=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda}) \\\\ \u0026amp;=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}) \\\\ \u0026amp;=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}-\\mathbf{I}+\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}) \\\\ \u0026amp;=\\mathbf{I}-(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}), \\end{align} or \\begin{equation} \\mathbf{I}-\\mathbf{P}_\\pi^\\lambda=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}) \\end{equation} Then our key matrix now can be written as: \\begin{equation} \\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})=\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\end{equation} In order to prove the positive definiteness of $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$, analogous to the proof of the convergence to TD fixed point of semi-gradient TD, we use two lemmas:\nLemma 1: Any matrix $\\mathbf{A}$ is positive definite iff the symmetric matrix $\\mathbf{S}=\\mathbf{A}+\\mathbf{A}^\\text{T}$ is positive definite. Lemma 2: Any symmetric real matrix $\\mathbf{S}$ is positive definite if all of its diagonal entries are positive and greater than the sum of the corresponding off-diagonal entries. Since $\\mathbf{M}$ is a diagonal matrix whose diagonal is a distribution and $\\mathbf{P}_\\pi^\\lambda$ is a probability matrix, we have that the matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$ has a diagonal of non-negative entries, and non-positive off-diagonal entries, and its row sums also are non-negative. Hence, our problem remains to show that the column sums of the key matrix are positive.\nTo show this we need to analyze the matrix $\\mathbf{M}$, and to do that we first analyze the vector $\\mathbf{f}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$, which having $f(s)\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\left[F_t|S_t=s\\right]$ as its components. We have: \\begin{align} \\hspace{-0.7cm}f(s)\u0026amp;=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_t\\big|S_t=s\\Big] \\\\ \u0026amp;=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[i(S_t)+\\rho_{t-1}\\gamma_t F_{t-1}\\big|S_t=s\\Big] \\\\ \u0026amp;=\\mu(s)i(s)\\nonumber \\\\ \u0026amp;+\\mu(s)\\gamma(s)\\lim_{t\\to\\infty}\\sum_{\\bar{s},\\bar{a}}P(S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}|S_t=s)\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}]\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026amp;=\\mu(s)i(s)+\\mu(s)\\gamma(s)\\sum_{\\bar{s},\\bar{a}}\\frac{\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})}{\\mu(s)}\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026amp;=\\mu(s)i(s)+\\gamma(s)\\sum_{\\bar{s},\\bar{a}}\\pi(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\mu(\\bar{s})\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026amp;=\\mu(s)i(s)+\\gamma(s)\\sum_s\\left[\\mathbf{P}_\\pi\\right]_{\\bar{s}s}f(\\bar{s})\\label{eq:es.5} \\end{align} Let $\\mathbf{i}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ be the vector having components $[\\mathbf{i}]_s\\doteq\\mu(s)i(s)$. Equation \\eqref{eq:es.5} allows us to write $\\mathbf{f}$ in matrix-vector form, as: \\begin{align} \\mathbf{f}\u0026amp;=\\mathbf{i}+\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\mathbf{f} \\\\ \u0026amp;=\\mathbf{i}+\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\mathbf{i}+(\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})^2\\mathbf{i}+\\dots \\\\ \u0026amp;=\\left(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\right)^{-1} \\end{align} Back to the definition of $m(s)$, we have: \\begin{align} m(s)\u0026amp;=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big|S_t=s\\Big] \\\\ \u0026amp;=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\lambda_t i(S_t)+(1-\\lambda_t)F_t\\big|S_t=s\\Big] \\\\ \u0026amp;=\\mu(s)\\lambda(s)i(s)+(1-\\lambda(s))f(s) \\end{align} Continuing as usual, we rewrite this equation in matrix-vector form by letting $\\mathbf{m}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ be a vector having $m(s)$ as its components: \\begin{align} \\mathbf{m}\u0026amp;=\\mathbf{\\Lambda}\\mathbf{i}+(\\mathbf{I}-\\mathbf{\\Lambda})\\mathbf{f} \\\\ \u0026amp;=\\mathbf{\\Lambda}\\mathbf{i}+(\\mathbf{I}-\\mathbf{\\Lambda})(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})^{-1}\\mathbf{i} \\\\ \u0026amp;=\\Big[\\mathbf{\\Lambda}(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})+(\\mathbf{I}-\\mathbf{\\Lambda})\\Big]\\left(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\right)\\mathbf{i} \\\\ \u0026amp;=\\Big(\\mathbf{I}-\\mathbf{\\Lambda}\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\Big)\\Big(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\Big)^{-1}\\mathbf{i} \\\\ \u0026amp;=\\Big(\\mathbf{I}-{\\mathbf{P}_\\pi^\\lambda}^\\text{T}\\Big)^{-1}\\mathbf{i} \\end{align} Let $\\mathbf{1}$ denote the column vector with all components equal to $1$. And using the result above, we have the vector of column sums of the key matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$ is: \\begin{align} \\mathbf{1}^\\text{T}{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)\u0026amp;=\\mathbf{m}^\\text{T}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\\\ \u0026amp;=\\mathbf{i}^\\text{T}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\\\ \u0026amp;=\\mathbf{i}^\\text{T} \\end{align} Instead of having domain of $[0,\\infty)$, if we further assume that $i(s)\u0026gt;0,\\hspace{0.1cm}\\forall s\\in\\mathcal{S}$, then it implies immediately that the column sums are all positive, the key matrix is positive definite, so is the matrix $\\mathbf{A}$, and the ETD($\\lambda$) and its expected update are stable.\nReferences [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Doina Precup \u0026amp; Richard S. Sutton \u0026amp; Satinder Singh. Eligibility Traces for Off-Policy Policy Evaluation. ICML \u0026lsquo;00 Proceedings of the Seventeenth International Conference on Machine Learning. 80, 2000.\n[3] Deepmind x UCL. Reinforcement Learning Lecture Series 2021. Deepmind, 2021.\n[4] Harm van Seijen \u0026amp; A. Rupam Mahmood \u0026amp; Patrick M. Pilarski \u0026amp; Marlos C. Machado \u0026amp; Richard S. Sutton. True Online Temporal-Difference Learning. Journal of Machine Learning Research. 17(145):1−40, 2016.\n[5] Hado Van Hasselt \u0026amp; A. Rupam Mahmood \u0026amp; Richard S. Sutton. Off-policy TD(λ) with a true online equivalence. Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014.\n[6] Hamid Reza Maei. Gradient Temporal-Difference Learning Algorithms. PhD Thesis, University of Alberta, 2011.\n[7] Hamid Reza Maei \u0026amp; Richard S. Sutton GQ($\\lambda$): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. AGI-09, 2009.\n[8] Richard S. Sutton \u0026amp; A. Rupam Mahmood \u0026amp; Martha White. An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning. arXiv:1503.04269, 2015.\n[9] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes $\\mathbf{z}_t$ is a vector random variable, one per time step, while $\\mathbf{z}(s)$ is a vector expectation, one per state.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/","summary":"\u003cblockquote\u003e\n\u003cp\u003eBeside \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td\"\u003e$n$-step TD\u003c/a\u003e methods, there is another mechanism called \u003cstrong\u003eeligible traces\u003c/strong\u003e that unify TD and Monte Carlo. Setting $\\lambda$ in TD($\\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\\lambda=0$ to Monte Carlo methods with $\\lambda=1$.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Eligible Traces"},{"content":" All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.\nOn-policy Methods So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.\nValue-function Approximation All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a \u0026ldquo;backed-up value\u0026rdquo; (or update target) for that state \\begin{equation} s\\mapsto u, \\end{equation} where $s$ is the state updated and $u$ is the update target that $s$\u0026rsquo;s estimated value is shifted toward.\nFor example,\nthe MC update for value prediction is: $S_t\\mapsto G_t$. the TD(0) update for value prediction is: $S_t\\mapsto R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)$. the $n$-step TD update is: $S_t\\mapsto G_{t:t+n}$. and in the DP, policy-evaluation update, $s\\mapsto\\mathbb{E}\\big[R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)\\vert S_t=s\\big]$, an arbitrary $s$ is updated. Each update $s\\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process function approximation.\nThe Prediction Objective In contrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is impossible to find the exact value function of all states. And moreover, an update at one state also affects many others.\nHence, it is necessary to specify a state distribution $\\mu(s)\\geq0,\\sum_s\\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\\hat{v}(s,\\mathbf{w})$ and the true value $v_\\pi(s)$) in each state $s$. Weighting this over the state space $\\mathcal{S}$ by $\\mu$, we obtain a natural objective function, called the Mean Squared Value Error, denoted as $\\overline{\\text{VE}}$: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})\\doteq\\sum_{s\\in\\mathcal{S}}\\mu(s)\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} The distribution $\\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divided by total amount of visits). Under on-policy training this is called the on-policy distribution.\nIn continuing tasks, the on-policy distribution is the stationary distribution under $\\pi$. In episodic tasks, the on-policy distribution depends on how the initial states are chosen. Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode \\begin{equation} \\eta(s)=h(s)+\\sum_\\bar{s}\\eta(\\bar{s})\\sum_a\\pi(a\\vert\\bar{s})p(s\\vert\\bar{s},a),\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} This system of equation can be solved for the expected number of visits $\\eta(s)$. The on-policy distribution is then \\begin{equation} \\mu(s)=\\frac{\\eta(s)}{\\sum_{s\u0026rsquo;}\\eta(s\u0026rsquo;)},\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} Gradient-based algorithms To solve the least squares problem, we are going to use a popular method, named Gradient descent.\nSay, consider a differentiable function $J(\\mathbf{w})$ of parameter vector $\\mathbf{w}$.\nThe gradient of $J(\\mathbf{w})$ w.r.t $\\mathbf{w}$ is defined to be \\begin{equation} \\nabla_{\\mathbf{w}}J(\\mathbf{w})=\\left(\\begin{smallmatrix}\\dfrac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}_1} \\\\ \\vdots \\\\ \\dfrac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}_d}\\end{smallmatrix}\\right) \\end{equation} The idea of Gradient descent is to minimize the objective function $J(\\mathbf{w})$, we repeatedly move $\\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\\nabla_\\mathbf{w}J(\\mathbf{w})$.\nThus, we have the update rule of Gradient descent: \\begin{equation} \\mathbf{w}:=\\mathbf{w}-\\dfrac{1}{2}\\alpha\\nabla_\\mathbf{w}J(\\mathbf{w}), \\end{equation} where $\\alpha$ is a positive step-size parameter.\nStochastic-gradient Apply gradient descent to our problem, which is we have to find the minimization of \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})=\\sum_{s\\in\\mathcal{S}}\\mu(s)\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} Since $\\mu(s)$ is the state distribution over state space $\\mathcal{S}$, we can rewrite $\\overline{\\text{VE}}$ as \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})=\\mathbb{E}_{s\\sim\\mu}\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} By the update we have defined earlier, in each step, we need to decrease $\\mathbf{w}$ by an amount of \\begin{equation} \\Delta\\mathbf{w}=-\\dfrac{1}{2}\\alpha\\nabla_\\mathbf{w}\\overline{\\text{VE}}(\\mathbf{w})=\\alpha\\mathbb{E}\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]\\nabla_\\mathbf{w}\\hat{v}(s,\\mathbf{w}) \\end{equation} Assume that, on each step, we observe a new example $S_t\\mapsto v_\\pi(S_t)$ consisting of a state $S_t$ and its true value under the policy $\\pi$.\nUsing Stochastic Gradient descent (SGD), we adjust the weight vector after each example by a small amount in the direction that would most reduce the error on that example: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;\\doteq\\mathbf{w}_t-\\frac{1}{2}\\alpha\\nabla_\\mathbf{w}\\big[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbf{w}_t)\\big]^2 \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\big[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\label{eq:sg.1} \\end{align} When the target output, here denoted as $U_t\\in\\mathbb{R}$, of the $t$-th training example, $S_t\\mapsto U_t$, is not the true value, $v_\\pi(S_t)$, but some approximation to it, we cannot perform the exact update \\eqref{eq:sg.1} since $v_\\pi(S_t)$ is unknown, but we can approximate it by substituting $U_t$ in place of $v_\\pi(S_t)$. This yield the following general SGD method for state-value prediction: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[U_t-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\label{eq:sg.2} \\end{equation} If $U_t$ is an unbiased estimate of $v_\\pi(S_t)$, i.e., $\\mathbb{E}\\left[U_t\\vert S_t=s\\right]=v_\\pi(S_t)$, for each $t$, then $\\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic conditions for decreasing $\\alpha$.\nIn particular, since the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t\\doteq G_t$, we have that the SGD version of Monte Carlo state-value prediction, \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[G_t-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} is guaranteed to converge to a local optimal point.\nWe have the pseudocode of the algorithm\nSemi-gradient If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\\sum_{a,s\u0026rsquo;,r}\\pi(a\\vert S_t)p(s\u0026rsquo;,r\\vert S_t,a)\\left[r+\\gamma\\hat{v}(s\u0026rsquo;,\\mathbf{w}_t)\\right]$, which all depend on the current value of the weight vector $\\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.\nSuch methods are called semi-gradient since they include only a part of the gradient.\nLinear Function Approximation One of the most crucial special cases of function approximation is that in which the approximate function, $\\hat{v}(\\cdot,\\mathbf{w})$, is a linear function of the weight vector, $\\mathbf{w}$.\nCorresponding to every state $s$, there is a real-valued vector $\\mathbf{x}(s)\\doteq\\left(x_1(s),x_2(s),\\dots,x_d(s)\\right)$, with the same number of components with $\\mathbf{w}$.\nLinear Methods Linear methods approximate value function by the inner product between $\\mathbf{w}$ and $\\mathbf{x}(s)$: \\begin{equation} \\hat{v}(s,\\mathbf{w})\\doteq\\mathbf{w}^\\text{T}\\mathbf{x}(s)=\\sum_{i=1}^{d}w_ix_i(s)\\label{eq:lm.1} \\end{equation} The vector $\\mathbf{x}(s)$ is called a feature vector representing state $s$, i.e., $x_i:\\mathcal{S}\\to\\mathbb{R}$.\nFor linear methods, features are basis functions because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.\nFrom \\eqref{eq:lm.1}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\\mathbf{w}$ is \\begin{equation} \\nabla_\\mathbf{w}\\hat{v}(s,\\mathbf{w})=\\mathbf{x}(s) \\end{equation} Thus, with linear approximation, the SGD update can be rewrite as \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t-\\hat{v}(S_t,\\mathbf{w}_t)\\right]\\mathbf{x}(S_t) \\end{equation}\nIn the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.\nThe gradient MC algorithm in the previous section converges to the global optimum of the $\\overline{\\text{VE}}$ under linear function approximation if $\\alpha$ is reduced over time according to the usual conditions. In particular, it converges to the fixed point, called $\\mathbf{w}_{\\text{MC}}$, with: \\begin{align} \\nabla_{\\mathbf{w}_{\\text{MC}}}\\mathbb{E}\\left[\\big(G_t-v_{\\mathbf{w}_{\\text{MC}}}(S_t)\\big)^2\\right]\u0026amp;=0 \\\\ \\mathbb{E}\\Big[\\big(G_t-v_{\\mathbf{w}_{\\text{MC}}}(S_t)\\big)\\mathbf{x}_t\\Big]\u0026amp;=0 \\\\ \\mathbb{E}\\Big[(G_t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_{\\text{MC}})\\mathbf{x}_t\\Big]\u0026amp;=0 \\\\ \\mathbb{E}\\left[G_t\\mathbf{x}_t-\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\mathbf{w}_{\\text{MC}}\\right]\u0026amp;=0 \\\\ \\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]\\mathbf{w}_\\text{MC}\u0026amp;=\\mathbb{E}\\left[G_t\\mathbf{x}_t\\right] \\\\ \\mathbf{w}_\\text{MC}\u0026amp;=\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[G_t\\mathbf{x}_t\\right] \\end{align} The semi-gradient TD algorithm also converges under linear approximation. Recall that, at each time $t$, the semi-gradient TD update is \\begin{align} \\mathbf{w}_{t+1}\u0026amp;\\doteq\\mathbf{w}_t+\\alpha\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\left(R_{t+1}\\mathbf{x}_t-\\mathbf{x}_t(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1})^\\text{T}\\mathbf{w}_t\\right), \\end{align} where $\\mathbf{x}_t=\\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\\mathbf{w}_t$, the expected next weight vector can be written as \\begin{equation} \\mathbb{E}\\left[\\mathbf{w}_{t+1}\\vert\\mathbf{w}_t\\right]=\\mathbf{w}_t+\\alpha\\left(\\mathbf{b}-\\mathbf{A}\\mathbf{w}_t\\right),\\label{eq:lm.2} \\end{equation} where \\begin{align} \\mathbf{b}\u0026amp;\\doteq\\mathbb{E}\\left[R_{t+1}\\mathbf{x}_t\\right]\\in\\mathbb{R}^d, \\\\ \\mathbf{A}\u0026amp;\\doteq\\mathbb{E}\\left[\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right]\\in\\mathbb{R}^d\\times\\mathbb{R}^d\\label{eq:lm.3} \\end{align} From \\eqref{eq:lm.2}, it is easily seen that if the system converges, it must converges to the weight vector $\\mathbf{w}_{\\text{TD}}$ at which \\begin{align} \\mathbf{b}-\\mathbf{A}\\mathbf{w}_{\\text{TD}}\u0026amp;=\\mathbf{0} \\\\ \\mathbf{w}_{\\text{TD}}\u0026amp;=\\mathbf{A}^{-1}\\mathbf{b} \\end{align} This quantity, $\\mathbf{w}_{\\text{TD}}$, is called the TD fixed point. And in fact, linear semi-gradient TD(0) converges to this point1. At the TD fixed point, it has also been proven (in the continuing case) that $\\overline{\\text{VE}}$ is within a bounded expansion of the lowest possible error, while the Monte Carlo solutions minimize the value error $\\overline{\\text{VE}}$: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w}_{\\text{TD}})\\leq\\dfrac{1}{1-\\gamma}\\overline{\\text{VE}}(\\mathbf{w}_{\\text{MC}})=\\dfrac{1}{1-\\gamma}\\min_{\\mathbf{w}}\\overline{\\text{VE}}(\\mathbf{w}) \\end{equation} Based on the tabular $n$-step TD we have defined before, applying the semi-gradient method, we have the function approximation version of its, called semi-gradient $\\boldsymbol{n}$-step TD, can be defined as: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\left[G_{t:t+n}-\\hat{v}(S_t,\\mathbf{w}_{t+n-1})\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\lt T \\end{equation} where the $n$-step return is generalized from the tabular version: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\geq t\\geq T-n \\end{equation} We therefore have the pseudocode of the semi-gradient $n$-step TD algorithm.\nFeature Construction There are various ways to define features. The simplest way is to use each variable directly as a basis function along with a constant function, i.e., setting: \\begin{equation} x_0(s)=1;\\hspace{1cm}x_i(s)=s_i,0\\leq i\\leq d \\end{equation} However, most interesting value functions are too complex to be represented in this way. This scheme therefore was generalized into the polynomial basis.\nPolynomial Basis Suppose each state $s$ corresponds to $d$ numbers, $s_1,s_2\\dots,s_d$, with each $s_i\\in\\mathbb{R}$. For this $d$-dimensional state space, each order-$n$ polynomial basis feature $x_i$ can be written as \\begin{equation} x_i(s)=\\prod_{j=1}^{d}s_j^{c_{i,j}}, \\end{equation} where each $c_{i,j}\\in\\{0,1,\\dots,n\\}$ for an integer $n\\geq 0$. These features make up the order-$n$ polynomial basis for dimension $d$, which contains $(n+1)^d$ different features.\nFourier Basis The Univariate Fourier Series Fourier series is applied widely in Mathematics to approximate a periodic function2. For example:\nFigure 1: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\\theta)=\\frac{4\\sin\\theta}{\\pi},f_2(\\theta)=\\frac{4\\sin 3\\theta}{3\\pi},f_3(\\theta)=\\frac{4\\sin 5\\theta}{5\\pi}$ and $f_4(\\theta)=\\frac{4\\sin 7\\theta}{7\\pi}$. The code can be found here In particular, the $n$-degree Fourier expansion of $f$ with period $\\tau$ is \\begin{equation} \\bar{f}(x)=\\dfrac{a_0}{2}+\\sum_{k=1}^{n}\\left[a_k\\cos\\left(k\\frac{2\\pi}{\\tau}x\\right)+b_k\\left(k\\frac{2\\pi}{\\tau}x\\right)\\right], \\end{equation} where \\begin{align} a_k\u0026amp;=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\cos\\left(\\frac{2\\pi kx}{\\tau}\\right)\\hspace{0.1cm}dx, \\\\ b_k\u0026amp;=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi kx}{\\tau}\\right)\\hspace{0.1cm}dx \\end{align} In the RL setting, $f$ is unknown so we cannot compute $a_0,\\dots,a_n$ and $b_1,\\dots,b_n$, but we can instead treat them as parameters in a linear function approximation scheme, with \\begin{equation} \\phi_i(x)=\\begin{cases}1 \u0026amp;\\text{if }i=0 \\\\ \\cos\\left(\\frac{(i+1)\\pi x}{\\tau}\\right) \u0026amp;\\text{if }i\u0026gt;0,i\\text{ odd} \\\\ \\sin\\left(\\frac{i\\pi x}{\\tau}\\right) \u0026amp;\\text{if }i\u0026gt;0,i\\text{ even}\\end{cases} \\end{equation} Thus, a full $n$-th order Fourier approximation to a one-dimensional value function results in a linear function approximation with $2n+1$ terms.\nEven, Odd and Non-Periodic Functions If $f$ is known to be even (i.e., $f(x)=f(-x)$), then $\\forall i\u0026gt;0$, we have: \\begin{align} b_i\u0026amp;=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026amp;=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x-\\tau)\\sin\\left(\\frac{2\\pi ix}{\\tau}-2\\pi i\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026amp;=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x-\\tau)\\sin\\left(\\frac{2\\pi i(x-\\tau)}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026amp;=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{-\\tau/2}^{0}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026amp;=0, \\end{align} so the $\\sin$ terms can be dropped, which reduces the terms required for an $n$-th order Fourier approximation to $n+1$.\nSimilarly, if $f$ is known to be odd (i.e., $f(x)=-f(-x)$), then $\\forall i\u0026gt;0, a_i=0$, so we can omit the $\\cos$ terms.\nHowever, in general, value functions are not even, odd, or periodic (or known to be in advance). In such cases, if $f$ is defined over a bounded interval with length, let us assume, $\\tau$, or without loss of generality, $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, but only project the input variable to $\\left[0,\\frac{\\tau}{2}\\right]$. This results in a function periodic on $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, but unconstrained on $\\left(0,\\frac{\\tau}{2}\\right]$. We are now free to choose whether or not the function is even or odd over $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, and can drop half of the terms in the approximation.\nIn general, we expect it will be better to use the \u0026ldquo;half-even\u0026rdquo; approximation and drop the $\\sin$ terms because this causes only a slight discontinuity at the origin. Thus, we can define the univariate $n$-th order Fourier basis as: \\begin{equation} x_i(s)=\\cos(i\\pi s), \\end{equation} for $i=0,\\dots,n$.\nThe Multivariate Fourier Series The $n$-order Fourier expansion of the multivariate function $F$ with period $\\tau$ in $d$ dimensions is \\begin{equation} \\overline{F}(\\mathbf{x})=\\sum_\\mathbf{c}\\left[a_\\mathbf{c}\\cos\\left(\\frac{2\\pi}{\\tau}\\mathbf{c}\\cdot\\mathbf{x}\\right)+b_\\mathbf{c}\\sin\\left(\\frac{2\\pi}{\\tau}\\mathbf{c}\\cdot\\mathbf{x}\\right)\\right], \\end{equation} where $\\mathbf{c}=(c_1,\\dots,c_d)^\\text{T},c_i\\in\\left[0,\\dots,n\\right],1\\leq i\\leq d$.\nThis results in $2(n+1)^d$ basis functions for an $n$-th order full Fourier approximation to a value function in $d$ dimensions, which can be reduced to $(n+1)^d$ if we drop either the $sin$ or $cos$ terms for each variable as described above. Thus, we can define the $n$-th order Fourier basis in the multi-dimensional case as:\nSuppose each state $s$ corresponds to a vector of $d$ numbers, $\\mathbf{s}=(s_1,\\dots,s_d)^\\text{T}$, with each $s_i\\in[0,1]$. The $i$-th feature in the order-$n$ Fourier cosine basis can then be written as: \\begin{equation} x_i(s)=\\cos\\left(\\pi\\mathbf{s}^\\text{T}\\mathbf{c}^i\\right), \\end{equation} where $\\mathbf{c}=(c_1^i,\\dots,c_d^i)^\\text{T}$, with $c_j^i\\in\\{0,\\dots,n\\}$ for $j=1,\\dots,d$ and $i=0,\\dots,(n+1)^d$.\nThis defines a feature for each of the $(n+1)^d$ possible integer vector $\\mathbf{c}^i$. The inner product $\\mathbf{s}^\\text{T}\\mathbf{c}^i$ has the effect of assigning an integer in $\\{0,\\dots,n\\}$ to each dimension of $\\mathbf{s}$. As in the one-dimensional case, this integer determines the feature\u0026rsquo;s frequency along that dimension. The feature thus can be shifted and scaled to suit the bounded state space of a particular application.\nFigure 2: Fourier basis vs Polynomial basis on the 1000-state random walk\n(RL book - Example 9.2).\nThe code can be found here Coarse Coding Figure 3: Using linear function approximation based on coarse coding to learn a one-dimensional square-wave function (RL book - Example 9.3).\nThe code can be found here Tile Coding Figure 4: Gradient Monte Carlo with single tiling and with multiple tilings on the 1000-state random walk\n(RL book - Example 9.2).\nThe code can be found here Radial Basis Functions Another common scheme is Radial Basis Functions (RBFs). RBFs are the natural generalization of coarse coding to continuous valued features. Rather than each feature taking either $0$ or $1$, it can be anything within $[0,1]$, reflecting various degrees to which the feature is present.\nA typical RBF feature, $x_i$, has a Gaussian response $x_i(s)$ dependent only on the distance between the state, $s$, and the feature\u0026rsquo;s prototypical or center state, $c_i$, and relative to the feature\u0026rsquo;s width, $\\sigma_i$: \\begin{equation} x_i(s)\\doteq\\exp\\left(\\frac{\\Vert s-c_i\\Vert^2}{2\\sigma_i^2}\\right) \\end{equation} The figures below shows a one-dimensional example with a Euclidean distance metric.\nFigure 5: One-dimensional RBFs Least-Squares TD Recall when using TD(0) with linear function approximation, $v_\\mathbf{w}(s)=\\mathbf{w}^\\text{T}\\mathbf{x}(s)$, we need to find a point $\\mathbf{w}$ such that \\begin{equation} \\mathbb{E}\\Big[\\big(R_{t+1}+\\gamma v_\\mathbf{w}(S_{t+1})-v_{\\mathbf{w}}(S_t)\\big)\\mathbf{x}_t\\Big]=\\mathbf{0}\\label{eq:lstd.1} \\end{equation} or \\begin{equation} \\mathbb{E}\\Big[R_{t+1}\\mathbf{x}_t-\\mathbf{x}_t(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1})^\\text{T}\\mathbf{w}_t\\Big]=\\mathbf{0} \\end{equation} We found out that the solution is: \\begin{equation} \\mathbf{w}_{\\text{TD}}=\\mathbf{A}^{-1}\\mathbf{b}, \\end{equation} where \\begin{align} \\mathbf{A}\u0026amp;\\doteq\\mathbb{E}\\left[\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right], \\\\ \\mathbf{b}\u0026amp;\\doteq\\mathbb{E}\\left[R_{t+1}\\mathbf{x}_t\\right] \\end{align} Instead of computing these expectations over all possible states and all possible transitions that could happen, we now only care about the things that did happen. In particular, we now consider the empirical loss of \\eqref{eq:lstd.1}, as: \\begin{equation} \\frac{1}{t}\\sum_{k=0}^{t-1}\\big(R_{k+1}+\\gamma v_\\mathbf{w}(S_{k+1})-v_{\\mathbf{w}}(S_k)\\big)\\mathbf{x}_i=\\mathbf{0}\\label{eq:lstd.2} \\end{equation} By the law of large numbers3, when $t\\to\\infty$, \\eqref{eq:lstd.2} converges to its expectation, which is \\eqref{eq:lstd.1}. Hence, we now just have to compute the estimate of $\\mathbf{w}_{\\text{TD}}$, called $\\mathbf{w}_{\\text{LSTD}}$ (as LSTD stands for Least-Squares TD), which is defined as: \\begin{equation} \\mathbf{w}_{\\text{LSTD}}\\doteq\\left(\\sum_{k=0}^{t-1}\\mathbf{x}_i\\left(\\mathbf{x}_k-\\gamma\\mathbf{x}_{k+1}\\right)^\\text{T}\\right)^{-1}\\left(\\sum_{k=1}^{t-1}R_{k+1}\\mathbf{x}_k\\right)\\label{eq:lstd.3} \\end{equation} In other words, our work is to compute estimates $\\widehat{\\mathbf{A}}_t$ and $\\widehat{\\mathbf{b}}_t$ of $\\mathbf{A}$ and $\\mathbf{b}$: \\begin{align} \\widehat{\\mathbf{A}}_t\u0026amp;\\doteq\\sum_{k=0}^{t-1}\\mathbf{x}_k\\left(\\mathbf{x}_k-\\gamma\\mathbf{x}_{k+1}\\right)^\\text{T}+\\varepsilon\\mathbf{I};\\label{eq:lstd.4} \\\\ \\widehat{\\mathbf{b}}_t\u0026amp;\\doteq\\sum_{k=0}^{t-1}R_{k+1}\\mathbf{x}_k,\\label{eq:lstd.5} \\end{align} where $\\mathbf{I}$ is the identity matrix, and $\\varepsilon\\mathbf{I}$, for some small $\\varepsilon\u0026gt;0$, ensures that $\\widehat{\\mathbf{A}}_t$ is always invertible. Thus, \\eqref{eq:lstd.3} can be rewritten as: \\begin{equation} \\mathbf{w}_{\\text{LSTD}}\\doteq\\widehat{\\mathbf{A}}_t^{-1}\\widehat{\\mathbf{b}}_t \\end{equation} The two approximations in \\eqref{eq:lstd.4} and \\eqref{eq:lstd.5} could be implemented incrementally using the same technique we used to apply earlier so that they can be done in constant time per step. Even so, the update for $\\widehat{\\mathbf{A}}_t$ would have the computational complexity of $O(d^2)$, and so is its memory required to hold the $\\widehat{\\mathbf{A}}_t$ matrix.\nThis leads to a problem that our next step, which is the computation of the inverse $\\widehat{\\mathbf{A}}_t^{-1}$ of $\\widehat{\\mathbf{A}}_t$, is going to be $O(d^3)$. Fortunately, with the so-called Sherman-Morrison formula, an inverse of our special form matrix - a sum of outer products - can also be updated incrementally with only $O(d^2)$ computations, as \\begin{align} \\widehat{\\mathbf{A}}_t^{-1}\u0026amp;=\\left(\\widehat{\\mathbf{A}}_t+\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right)^{-1} \\\\ \u0026amp;=\\widehat{\\mathbf{A}}_{t-1}^{-1}-\\frac{\\widehat{\\mathbf{A}}_{t-1}^{-1}\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\widehat{\\mathbf{A}}_{t-1}^{-1}}{1+\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\widehat{\\mathbf{A}}_{t-1}^{-1}\\mathbf{x}_t}, \\end{align} for $t\u0026gt;0$, with $\\mathbf{\\widehat{A}}_0\\doteq\\varepsilon\\mathbf{I}$.\nFor the estimate $\\widehat{\\mathbf{b}}_t$ of $\\mathbf{b}$, it can be updated using naive approach: \\begin{equation} \\widehat{\\mathbf{b}}_{t+1}=\\widehat{\\mathbf{b}}_t+R_{t+1}\\mathbf{x}_t \\end{equation} The pseudocode for LSTD is given below\nEpisodic Semi-gradient Sarsa We now consider the control problem, with parametric approximation of the action-value function $\\hat{q}(s,a,\\mathbf{w})\\approx q_*(s,a)$, where $\\mathbf{w}\\in\\mathbb{R}^d$ is a finite-dimensional weight vector.\nSimilar to the prediction problem, we can apply semi-gradient methods in solving the control problem. The difference is rather than considering training examples of the form $S_t\\mapsto U_t$, we now consider examples of the form $S_t,A_t\\mapsto U_t$.\nFrom \\eqref{eq:sg.2}, we can derive the general SGD update for action-value prediction as \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[U_t-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:esgs.1} \\end{equation} The update for the one-step Sarsa method therefore would be \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:esgs.2} \\end{equation} We call this method episodic semi-gradient one-step Sarsa.\nTo form the control method, we need to couple the action-value\nThe following figure illustrates the cost-to-go function $\\max_a\\hat{q}(s,a,\\mathbf{w})$ learned during one run of the semi-gradient Sarsa on Mountain Car task.\nFigure 6: Cost-to-go learned during one run of Semi-gradient Sarsa on Mountain Car problem\n(RL book - Example 10.1).\nThe code can be found here Episodic Semi-gradient $\\boldsymbol{n}$-step Sarsa Similar to how we defined the one-step Sarsa version of semi-gradient, we can replace the update target in \\eqref{eq:esgs.1} by an $n$-step return, \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}),\\label{eq:esgnss.1} \\end{equation} for $t+n\\lt T$, with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$, as usual, to obtain the semi-gradient $n$-step Sarsa update: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} for $0\\leq t\\lt T$. The pseudocode is given below.\nThe figure below shows how the $n$-step ($8$-step in particular) tends to learn faster than the one-step algorithm.\nFigure 7: Performance of one-step vs 8-step Semi-gradient Sarsa on Mountain Car task\n(RL book).\nThe code can be found here Average Reward We now consider a new setting for continuing tasks - alongside the episodic and discounted settings - average reward.\nIn the average-reward setting, the quality of a policy $\\pi$ is defined as the average rate of reward, or simply average reward, while following that policy, which we denote as $r(\\pi)$: \\begin{align} r(\\pi)\u0026amp;\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=1}^{h}\\mathbb{E}\\Big[R_t\\vert S_0,A_{0:t-1}\\sim\\pi\\Big] \\\\ \u0026amp;=\\lim_{t\\to\\infty}\\mathbb{E}\\Big[R_t\\vert S_0,A_{0:t-1}\\sim\\pi\\Big] \\\\ \u0026amp;=\\sum_s\\mu_\\pi(s)\\sum_a\\pi(a\\vert s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r\\vert s,a)r, \\end{align} where:\nthe expectations are conditioned on the initial state $S_0$, and on the subsequent action $A_0,A_1,\\dots,A_{t-1}$, being taken according to $\\pi$; $\\mu_\\pi$ is the steady-state distribution, \\begin{equation} \\mu_\\pi\\doteq\\lim_{t\\to\\infty}P\\left(S_t=s\\vert A_{0:t-1}\\sim\\pi\\right), \\end{equation} which is assumed to exist for any $\\pi$ and to be independent of $S_0$. The steady state distribution is the special distribution under which, if we select actions according to $\\pi$, we remain in the same distribution. That is, for which \\begin{equation} \\sum_s\\mu_\\pi(x)\\sum_a\\pi(a\\vert s)p(s\u0026rsquo;\\vert s,a)=\\mu_\\pi(s\u0026rsquo;) \\end{equation} In the average-reward setting, returns are defined in terms of differences between rewards and the average reward: \\begin{equation} G_t\\doteq R_{t+1}-r(\\pi)+R_{t+2}-r(\\pi)+R_{t+3}-r(\\pi)+\\dots\\label{eq:ar.1} \\end{equation} This is known as the differential return, and the corresponding value functions are known as differential value functions, $v_\\pi(s)$ and $q_\\pi(s,a)$, which are defined in the same way as we have done before: \\begin{align} v_\\pi(s)\u0026amp;\\doteq\\mathbb{E}\\big[G_t\\vert S_t=s\\big]; \\\\ q_\\pi(s,a)\u0026amp;\\doteq\\mathbb{E}\\big[G_t\\vert S_t=s,A_t=a\\big], \\end{align} and similarly for $v_{*}$ and $q_{*}$. Likewise, differential value functions also have Bellman equations, with some modifications by replacing all discounted factor $\\gamma$ and replacing all rewards, $r$, by the difference between the reward and the true average reward, $r-r(\\pi)$, as: \\begin{align} \u0026amp;v_\\pi(s)=\\sum_a\\pi(a|s)\\sum_{r,s\u0026rsquo;}p(r,s\u0026rsquo;|s,a)\\left[r-r(\\pi)+v_\\pi(s\u0026rsquo;)\\right], \\\\ \u0026amp;q_\\pi(s,a)=\\sum_{r,s\u0026rsquo;}p(s\u0026rsquo;,r|s,a)\\left[r-r(\\pi)+\\sum_{a\u0026rsquo;}\\pi(a\u0026rsquo;|s\u0026rsquo;)q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\right], \\\\ \u0026amp;v_{*}(s)=\\max_a\\sum_{r,s\u0026rsquo;}p(s\u0026rsquo;,r|s,a)\\left[r-\\max_\\pi r(\\pi)+v_{*}(s\u0026rsquo;)\\right], \\\\ \u0026amp;q_{*}(s,a)=\\sum_{r,s\u0026rsquo;}p(s\u0026rsquo;,r|s,a)\\left[r-\\max_\\pi r(\\pi)+\\max_{a\u0026rsquo;}q_{*}(s\u0026rsquo;,a\u0026rsquo;)\\right] \\end{align}\nDifferential Semi-gradient Sarsa There is also a differential form of the two TD errors: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}_{t+1}+\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} and \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}_{t+1}+\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\label{eq:dsgs.1} \\end{equation} where $\\bar{R}_t$ is an estimate at time $t$ of the average reward $r(\\pi)$.\nWith these alternative definitions, most of our algorithms and many theoretical results carry through to the average-reward setting without change.\nFor example, the average reward version of semi-gradient Sarsa is defined just as in \\eqref{eq:esgs.2} except with the differential version of the TD error \\eqref{eq:dsgs.1}: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:dsgs.2} \\end{equation} The pseudocode of the algorithm is then given below.\nDifferential Semi-gradient $\\boldsymbol{n}$-step Sarsa To derive the $n$-step version of \\eqref{eq:dsgs.2}, we use the same update rule, except with an $n$-step version of the TD error.\nFirst, we need to define the $n$-step differential return, with function approximation, by combining the idea of \\eqref{eq:esgnss.1} and \\eqref{eq:ar.1} together, as: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}-\\bar{R}_{t+1}+R_{t+2}-\\bar{R}_{t+2}+\\dots+R_{t+n}-\\bar{R}_{t+n}+\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}), \\end{equation} where $\\bar{R}$ is an estimate of $r(\\pi),n\\geq 1$, $t+n\\lt T$; $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$ as usual. The $n$-step TD error is then \\begin{equation} \\delta_t\\doteq G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}) \\end{equation} The pseudocode of the algorithm is then given below.\nOff-policy Methods We now consider off-policy methods with function approximation.\nSemi-gradient To derive the semi-gradient form of off-policy tabular methods we have known, we simply replace the update to an array ($V$ or $Q$) to an update to a weight vector $\\mathbf{w}$, using the approximate value function $\\hat{v}$ or $\\hat{q}$ and its gradient.\nRecall that in off-policy learning we seek to learn a value function for a target policy $\\pi$, given data due to a different behavior policy $b$.\nMany of these algorithms use the per-step importance sampling ratio: \\begin{equation} \\rho_t\\doteq\\rho_{t:t}=\\dfrac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation}\nIn particular, for state-value functions, the one-step algorithm is semi-gradient off-policy TD(0) has the update rule: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\rho_t\\delta_t\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\label{eq:opsg.1} \\end{equation} where\nIf the problem is episodic and discounted, we have: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} If the problem is continuing and undiscounted using average reward, we have: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}+\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} For action values, the one-step algorithm is semi-gradient Expected Sarsa, which has the update rule: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}), \\end{equation} with\nEpisodic tasks: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\sum_a\\pi(a\\vert S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Continuing tasks: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}+\\sum_a\\pi(a\\vert S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} With multi-step algorithms, we begin with semi-gradient $\\boldsymbol{n}$-step Expected Sarsa, which has the update rule: \\begin{equation} \\hspace{-0.8cm}\\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\rho_{t+1}\\dots\\rho_{t+n-1}\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} where $\\rho_k=1$ for $k\\geq T$ and $G_{t:n}\\doteq G_t$ if $t+n\\geq T$, and with\nEpisodic tasks: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}) \\end{equation} Continuing tasks: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}-\\bar{R}_t+\\dots+R_{t+n}-\\bar{R}_{t+n-1}+\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}), \\end{equation} For the semi-gradient version of $n$-step tree-backup, called semi-gradient $\\boldsymbol{n}$-step tree-backup, the update rule is: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} where \\begin{equation} G_{t:t+n}\\doteq\\hat{q}(S_t,A_t,\\mathbf{w}_{t-1})+\\sum_{k=t}^{t+n-1}\\delta_k\\prod_{i=t+1}^{k}\\gamma\\pi(A_i|S_i), \\end{equation} with $\\delta_t$ is defined similar to the case of semi-gradient Expected Sarsa.\nResidual Bellman Update Gradient-TD In this section, we will be considering SGD methods for minimizing the $\\overline{\\text{PBE}}$.\nRewrite the objective $\\overline{\\text{PBE}}$ in matrix terms, we have: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026amp;=\\left\\Vert\\Pi\\bar{\\delta}_\\mathbf{w}\\right\\Vert_{\\mu}^{2} \\\\ \u0026amp;=\\left(\\Pi\\bar{\\delta}_\\mathbf{w}\\right)^\\text{T}\\mathbf{D}\\Pi\\bar{\\delta}_\\mathbf{w} \\\\ \u0026amp;=\\bar{\\delta}_\\mathbf{w}^\\text{T}\\Pi^\\text{T}\\mathbf{D}\\Pi\\bar{\\delta}_\\mathbf{w} \\\\ \u0026amp;=\\bar{\\delta}_\\mathbf{w}^\\text{T}\\mathbf{D}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w} \\\\ \u0026amp;=\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right)^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right), \\end{align} where in the fourth step, we use the property of projection operation4 and the identity \\begin{equation} \\Pi^\\text{T}\\mathbf{D}\\Pi=\\mathbf{D}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D} \\end{equation} Thus, the gradient w.r.t weight vector $\\mathbf{w}$ is \\begin{equation} \\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})=2\\nabla_\\mathbf{w}\\left[\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right]^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right)\\label{eq:gt.1} \\end{equation}\nTo turn this into an SGD method, we have to sample something on every time step that has this gradient as its expected value. Let $\\mu$ be the distribution of states visited under the behavior policy. The last factor of \\eqref{eq:gt.1} can be written as: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}=\\sum_s\\mu(s)\\mathbf{x}(s)\\bar{\\delta}_\\mathbf{w}=\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right], \\end{equation} which is the expectation of the semi-gradient TD(0) update \\eqref{eq:opsg.1}. The first factor of \\eqref{eq:gt.1}, which is the transpose of the gradient of this update, then can also be written as: \\begin{align} \\nabla_\\mathbf{w}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]^\\text{T}\u0026amp;=\\mathbb{E}\\left[\\rho_t\\nabla_\\mathbf{w}\\delta_t^\\text{T}\\mathbf{x}_t^\\text{T}\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[\\rho_t\\nabla_\\mathbf{w}\\left(R_{t+1}+\\gamma\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\right)^\\text{T}\\mathbf{x}_t^\\text{T}\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right] \\end{align} And the middle factor, without the inverse operation, can also be written as: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}=\\sum_a\\mu(s)\\mathbf{x}_s\\mathbf{x}_s^\\text{T}=\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right] \\end{equation} Substituting these expectations back to \\eqref{eq:gt.1}, we obtain: \\begin{equation} \\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})=2\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\label{eq:gt.2} \\end{equation}\nHere, we use the Gradient-TD to estimate and store the product of the second two factors in \\eqref{eq:gt.2}, denoted as $\\mathbf{v}$: \\begin{equation} \\mathbf{v}\\approx\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right],\\label{eq:gt.3} \\end{equation} which is the solution of the linear least-squares problem that tries to approximate $\\rho_t\\delta_t$ from the features. The SGD for incrementally finding the vector $\\mathbf{v}$ that minimizes the expected squared error $\\left(\\mathbf{v}^\\text{T}\\mathbf{x}_t\\right)^2$ is known as the Least Mean Square (LMS) rule (here augmented with an IS ratio): \\begin{equation} \\mathbf{v}_{t+1}\\doteq\\mathbf{v}_t+\\beta\\rho_t\\left(\\delta_t-\\mathbf{v}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t, \\end{equation} where $\\beta\u0026gt;0$ is a step-size parameter.\nWith a given stored estimate $\\mathbf{v}_t$ approximating \\eqref{eq:gt.3}, we can apply SGD update to the parameter $\\mathbf{w}_t$: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;=\\mathbf{w}_t-\\frac{1}{2}\\alpha\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w}_t) \\\\ \u0026amp;=\\mathbf{w}_t-\\frac{1}{2}\\alpha 2\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\label{eq:gt.4} \\\\ \u0026amp;\\approx\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbf{v}_t \\\\ \u0026amp;\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t\\mathbf{v}_t \\end{align} This algorithm is called GTD2. From \\eqref{eq:gt.4}, we can also continue to derive as: \\begin{align} \\mathbf{w}_{t+1}\u0026amp;=\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\rho_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026amp;=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\rho_t\\delta_t\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\right) \\\\ \u0026amp;\\approx\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\rho_t\\delta_t\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbf{v}_t \\\\ \u0026amp;\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\delta_t\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\mathbf{v}_t\\right) \\end{align} This algorithm is known as TD(0) with gradient correction (TDC), or as GTD(0).\nEmphatic-TD References [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Deepmind x UCL. Reinforcement Learning Lecture Series 2021.\n[3] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3, 9–44, 1988.\n[4] Konidaris, G. \u0026amp; Osentoski, S. \u0026amp; Thomas, P.. Value Function Approximation in Reinforcement Learning Using the Fourier Basis. AAAI Conference on Artificial Intelligence, North America, aug. 2011.\n[5] Joseph K. Blitzstein \u0026amp; Jessica Hwang. Introduction to Probability.\n[6] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes Proof We have \\eqref{eq:lm.2} can be written as \\begin{equation*} \\mathbb{E}\\left[\\mathbf{w}_{t+1}\\vert\\mathbf{w}_t\\right]=\\left(\\mathbf{I}-\\alpha\\mathbf{A}\\right)\\mathbf{w}_t+\\alpha\\mathbf{b} \\end{equation*} The idea of the proof is prove that the matrix $\\mathbf{A}$ in \\eqref{eq:lm.3} is a positive definite matrix5, since $\\mathbf{w}_t$ will be reduced toward zero whenever $\\mathbf{A}$ is positive definite.\nFor linear TD(0), in the continuing case with $\\gamma\u0026lt;1$, the matrix $\\mathbf{A}$ can be written as \\begin{align} \\mathbf{A}\u0026amp;=\\sum_s\\mu(s)\\sum_a\\pi(a\\vert s)\\sum_{r,s\u0026rsquo;}p(r,s\u0026rsquo;\\vert s,a)\\mathbf{x}(s)\\big(\\mathbf{x}(s)-\\gamma\\mathbf{x}(s\u0026rsquo;)\\big)^\\text{T}\\nonumber \\\\ \u0026amp;=\\sum_s\\mu(s)\\sum_{s\u0026rsquo;}p(s\u0026rsquo;\\vert s)\\mathbf{x}(s)\\big(\\mathbf{x}(s)-\\gamma\\mathbf{x}(s\u0026rsquo;)\\big)^\\text{T}\\nonumber \\\\ \u0026amp;=\\sum_s\\mu(s)\\mathbf{x}(s)\\Big(\\mathbf{x}(s)-\\gamma\\sum_{s\u0026rsquo;}p(s\u0026rsquo;\\vert s)\\mathbf{x}(s\u0026rsquo;)\\Big)^\\text{T}\\nonumber \\\\ \u0026amp;=\\mathbf{X}^\\text{T}\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})\\mathbf{X},\\label{eq:lm.4} \\end{align} where\n$\\mu(s)$ is the stationary distribution under $\\pi$; $p(s\u0026rsquo;\\vert s)$ is the probability transition from $s$ to $s\u0026rsquo;$ under policy $\\pi$; $\\mathbf{P}$ is the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix of these probabilities; $\\mathbf{D}$ is the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix with the $\\mu(s)$ on its diagonal; $\\mathbf{X}$ is the $\\vert\\mathcal{S}\\vert\\times d$ matrix with $\\mathbf{x}(s)$ as its row. Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ in \\eqref{eq:lm.4}.\nTo continue proving the positive definiteness of $\\mathbf{A}$, we use two lemmas:\nLemma 1: A square matrix $\\mathbf{A}$ is positive definite if the symmetric matrix $\\mathbf{S}=\\mathbf{A}+\\mathbf{A}^\\text{T}$ is positive definite. Lemma 2: If $\\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\\mathbf{A}$ is positive definite. With these lemmas, plus since $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\\mathbf{P}$ is a stochastic matrix and $\\gamma\u003c1$. Thus the problem remains to show that the column sums are nonnegative. Let $\\mathbf{1}$ denote the column vector with all components equal to $1$ and $\\boldsymbol{\\mu}(s)$ denote the vectorized version of $\\mu(s)$: i.e., $\\boldsymbol{\\mu}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$. Thus, $\\boldsymbol{\\mu}=\\mathbf{P}^\\text{T}\\boldsymbol{\\mu}$ since $\\mu(s)$ is the stationary distribution. We have: \\begin{align*} \\mathbf{1}^\\text{T}\\mathbf{D}\\left(\\mathbf{I}-\\gamma\\mathbf{P}\\right)\u0026amp;=\\boldsymbol{\\mu}^\\text{T}\\left(\\mathbf{I}-\\gamma\\mathbf{P}\\right) \\\\ \u0026amp;=\\boldsymbol{\\mu}^\\text{T}-\\gamma\\boldsymbol{\\mu}^\\text{T}\\mathbf{P} \\\\ \u0026amp;=\\boldsymbol{\\mu}^\\text{T}-\\gamma\\boldsymbol{\\mu}^\\text{T} \\\\ \u0026amp;=\\left(1-\\gamma\\right)\\boldsymbol{\\mu}^\\text{T}, \\end{align*} which implies that the column sums of $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ are positive.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA function $f$ is periodic with period $\\tau$ if \\begin{equation*} f(x+\\tau)=f(x), \\end{equation*} for all $x$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nConsider i.i.d r.v.s $X_1,X_2,\\dots$ with finite mean $\\mu$ and finite variance $\\sigma^2$. For all positive integer $n$, let: \\begin{equation*} \\overline{X}_n\\doteq\\frac{X_1+\\dots+X_n}{n} \\end{equation*} be the sample mean of $X_1$ through $X_n$. As $n\\to\\infty$, the sample mean $\\overline{X}_n$ converges to the true mean $\\mu$, with probability $1$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor a linear function approximator, the projection is linear, which implies that it can be represented as an $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix: \\begin{equation*} \\Pi\\doteq\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}, \\end{equation*} where $\\mathbf{D}$ denotes the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix with the $\\mu(s)$ on the diagonal, and $\\mathbf{X}$ denotes the $\\vert\\mathcal{S}\\vert\\times d$ matrix whose rows are the feature vectors $\\mathbf{x}(s)^\\text{T}$, one for each state $s$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA $n\\times n$ matrix $A$ is called positive definite if and only if for any non-zero vector $\\mathbf{x}\\in\\mathbb{R}^n$, we always have \\begin{equation*} \\mathbf{x}^\\text{T}\\mathbf{A}\\mathbf{x}\u0026gt;0 \\end{equation*}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/func-approx/","summary":"\u003cblockquote\u003e\n\u003cp\u003eAll of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Function Approximation"},{"content":" So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.\nTD(0) As usual, we approach this new method by considering the prediction problem.\nTD Prediction Borrowing the idea of Monte Carlo, TD methods learn from episodes of experience to solve the prediction problem. The simplest TD method is TD(0) (or one-step TD)1, which has the update form: \\begin{equation} V(S_t)\\leftarrow V(S_t)+\\alpha\\left[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\right]\\label{eq:tp.1}, \\end{equation} where $\\alpha\\gt 0$ is step size of the update. Here is pseudocode of the TD(0) method\nRecall that in Monte Carlo method, or even in its trivial form, constant-$\\alpha$ MC, which has the update form: \\begin{equation} V(S_t)\\leftarrow V(S_t)+\\alpha\\left[G_t-V(S_t)\\right]\\label{eq:tp.2}, \\end{equation} we have to wait until the end of the episode, when the return $G_t$ is determined. However, with TD(0), we can do the update immediately in the next time step $t+1$.\nAs we can see in \\eqref{eq:tp.1} and \\eqref{eq:tp.2}, both TD and MC updates look ahead to a sample successor state (or state-action pair), use the value of the successor and the corresponding reward in order to update the value of the current state (or state-action pair). This kind of updates is called sample update, which differs from expected update used by DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.\nOther than the sampling of Monte Carlo, TD methods also use the bootstrapping of DP. Because similar to DP, TD(0) is also a bootstrapping method, since the target in its update is $R_{t+1}+\\gamma V(S_{t+1})$.\nThe quantity inside bracket in \\eqref{eq:tp.1} is called TD error, denoted as $\\delta$: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma V(S_{t+1})-V(S_t) \\end{equation} If the array $V$ does not change during the episode (as in MC), then the MC error can be written as a sum of TD errors \\begin{align} G_t-V(S_t)\u0026amp;=R_{t+1}+\\gamma G_{t+1}-V(S_t)+\\gamma V(S_{t+1})-\\gamma V(S_{t+1}) \\\\ \u0026amp;=\\delta_t+\\gamma\\left(G_{t+1}-V(S_{t+1})\\right) \\\\ \u0026amp;=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\left(G_{t+2}-V(S_{t+2})\\right) \\\\ \u0026amp;=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}\\left(G_T-V(S_T)\\right) \\\\ \u0026amp;=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}(0-0) \\\\ \u0026amp;=\\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k \\end{align}\nAdvantages over MC \u0026amp; DP With how TD is established, these are some advantages of its over MC and DP:\nOnly experience is required. Can be fully incremental: Can make update before knowing the final outcome. Requires less memory. Requires less peak computation. TD(0) does converge to $v_\\pi$, in the mean for a sufficient small $\\alpha$, and with probability of $1$ if $\\alpha$ decreases according to the stochastic approximation condition \\begin{equation} \\sum_{n=1}^{\\infty}\\alpha_n(a)=\\infty\\hspace{1cm}\\text{and}\\hspace{1cm}\\sum_{n=1}^{\\infty}\\alpha_n^2(a)\u0026lt;\\infty, \\end{equation} where $\\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$-th selection of action $a$.\nOptimality of TD(0) Under batch training, TD(0) converges to the optimal maximum likelihood estimate. The convergence and optimality proofs can be found in this paper.\nFigure 1: Performance of TD(0) and constant-$\\alpha$ MC under batch training on the random walk task. The code can be found here TD Control We begin solving the control problem with an on-policy TD method. Recall that in on-policy methods, we evaluate or improve the policy $\\pi$ used to make decision.\nSarsa As mentioned in MC methods, when the model is not available, we have to learn an action-value function rather than a state-value function. Or in other words, we need to estimate $q_\\pi(s,a)$ for the current policy $\\pi$ and $\\forall s,a$. Thus, instead of considering transitions from state to state in order to learn the value of states, we now take transitions from state-action pair to state-action pair into account so as to learn the value of state-action pairs.\nSimilarly, we\u0026rsquo;ve got the TD update for the action-value function case: \\begin{equation} Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\right] \\end{equation} This update is done after every transition from a non-terminal state $S_t$ to its successor $S_{t+1}$ \\begin{equation} \\left(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\\right) \\end{equation} And if $S_{t+1}$ is terminal (i.e., $S_{t+1}=S_T$), then $Q(S_{t+1},A_{t+1})=0$. The name Sarsa of the method is taken based on acronym of the quintuple.\nAs usual when working on on-policy control problem, we apply the idea of GPI: \\begin{equation} \\pi_0\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_0}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_1\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_1}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_2\\overset{\\small \\text{E}}{\\rightarrow}\\dots\\overset{\\small \\text{I}}{\\rightarrow}\\pi_*\\overset{\\small \\text{E}}{\\rightarrow}q_* \\end{equation} However this time, instead, we use it with TD methods. Which is, we continually estimate $q_\\pi$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness w.r.t $q_\\pi$. That gives us the following pseudocode of the Sarsa control algorithm\nQ-learning We now turn our move to an off-policy method, which evaluates or improves a policy different from that used to generate the data.\nThe method we are talking about is called Q-learning, which was first introduced by Watkins, in which the update on $Q$-value has the form: \\begin{equation} Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)-Q(S_t,A_t)\\right]\\label{eq:ql.1} \\end{equation} In this case, the learned action-value function, $Q$, directly approximates optimal action-value function $q_*$, independent of the policy being followed. Down below is pseudocode of the Q-learning.\nExample: Cliffwalking - Sarsa vs Q-learning (This example is taken from Example 6.6, Reinforcement Learning: An Introduction book.)\nSay that we have an agent in a gridworld, which is an undiscounted, episodic task described by the above image. Start and goal states are denoted as \"S\" and \"G\" respectively. Agent can take up, down, left or right action. All the actions lead to a reward of $-1$, except for cliff region, into which stepping gives a reward of $-100$. We will be solving this problem with Q-learning and Sarsa with $\\varepsilon$-greedy action selection, for $\\varepsilon=0.1$. Solution\nThe source code can be found here.\nWe begin by importing necessary packages we will be using\nimport numpy as np import matplotlib.pyplot as plt from tqdm import tqdm Our first step is to define the environment, gridworld with a cliff, which is constructed by height, width, cliff region, start state, goal state, actions and rewards.\nclass GridWorld: def __init__(self, height, width, start_state, goal_state, cliff): \u0026#39;\u0026#39;\u0026#39; Initialization function Params ------ height: int gridworld\u0026#39;s height width: int gridworld\u0026#39;s width start_state: [int, int] gridworld\u0026#39;s start state goal_state: [int, int] gridworld\u0026#39;s goal state cliff: list\u0026lt;[int, int]\u0026gt; gridworld\u0026#39;s cliff region \u0026#39;\u0026#39;\u0026#39; self.height = height self.width = width self.start_state = start_state self.goal_state = goal_state self.cliff = cliff self.actions = [(-1, 0), (1, 0), (0, 1), (0, -1)] self.rewards = {\u0026#39;cliff\u0026#39;: -100, \u0026#39;non-cliff\u0026#39;: -1} The gridworld also needs some helper functions. is_terminal() function checks whether the current state is the goal state; take_action() takes an state and action as inputs and returns next state and corresponding reward while get_action_idx() gives us the index of action from action list. Putting all these functions inside GridWorld\u0026rsquo;s body, we have:\ndef is_terminal(self, state): \u0026#39;\u0026#39;\u0026#39; Whether state @state is the goal state Params ------ state: [int, int] current state \u0026#39;\u0026#39;\u0026#39; return state == self.goal_state def take_action(self, state, action): \u0026#39;\u0026#39;\u0026#39; Take action @action at state @state Params ------ state: [int, int] current state action: (int, int) action taken Return ------ (next_state, reward): ([int, int], int) a tuple of next state and reward \u0026#39;\u0026#39;\u0026#39; next_state = [state[0] + action[0], state[1] + action[1]] next_state = [max(0, next_state[0]), max(0, next_state[1])] next_state = [min(self.height - 1, next_state[0]), min(self.width - 1, next_state[1])] if next_state in self.cliff: reward = self.rewards[\u0026#39;cliff\u0026#39;] next_state = self.start_state else: reward = self.rewards[\u0026#39;non-cliff\u0026#39;] return next_state, reward def get_action_idx(self, action): \u0026#39;\u0026#39;\u0026#39; Get index of action in action list Params ------ action: (int, int) action \u0026#39;\u0026#39;\u0026#39; return self.actions.index(action) Next, we define the $\\varepsilon$-greedy function used by our methods in epsilon_greedy() function.\ndef epsilon_greedy(grid_world, epsilon, Q, state): \u0026#39;\u0026#39;\u0026#39; Choose action according to epsilon-greedy policy Params: ------- grid_world: GridWorld epsilon: float Q: np.ndarray action-value function state: [int, int] current state Return ------ action: (int, int) \u0026#39;\u0026#39;\u0026#39; if np.random.binomial(1, epsilon): action_idx = np.random.randint(len(grid_world.actions)) action = grid_world.actions[action_idx] else: values = Q[state[0], state[1], :] action_idx = np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)]) action = grid_world.actions[action_idx] return action It\u0026rsquo;s time for our main course, Q-learning and Sarsa.\ndef q_learning(Q, grid_world, epsilon, alpha, gamma): \u0026#39;\u0026#39;\u0026#39; Q-learning Params ------ Q: np.ndarray action-value function grid_world: GridWorld epsilon: float alpha: float step size gamma: float discount factor \u0026#39;\u0026#39;\u0026#39; state = grid_world.start_state rewards = 0 while not grid_world.is_terminal(state): action = epsilon_greedy(grid_world, epsilon, Q, state) next_state, reward = grid_world.take_action(state, action) rewards += reward action_idx = grid_world.get_action_idx(action) Q[state[0], state[1], action_idx] += alpha * (reward + gamma * \\ np.max(Q[next_state[0], next_state[1], :]) - Q[state[0], state[1], action_idx]) state = next_state return rewards def sarsa(Q, grid_world, epsilon, alpha, gamma): \u0026#39;\u0026#39;\u0026#39; Sarsa Params ------ Q: np.ndarray action-value function grid_world: GridWorld epsilon: float alpha: float step size gamma: float discount factor \u0026#39;\u0026#39;\u0026#39; state = grid_world.start_state action = epsilon_greedy(grid_world, epsilon, Q, state) rewards = 0 while not grid_world.is_terminal(state): next_state, reward = grid_world.take_action(state, action) rewards += reward next_action = epsilon_greedy(grid_world, epsilon, Q, next_state) action_idx = grid_world.get_action_idx(action) next_action_idx = grid_world.get_action_idx(next_action) Q[state[0], state[1], action_idx] += alpha * (reward + gamma * Q[next_state[0], \\ next_state[1], next_action_idx] - Q[state[0], state[1], action_idx]) state = next_state action = next_action return rewards And lastly, wrapping everything together in the main function, we have\nif __name__ == \u0026#39;__main__\u0026#39;: height = 4 width = 13 start_state = [3, 0] goal_state = [3, 12] cliff = [[3, x] for x in range(1, 12)] grid_world = GridWorld(height, width, start_state, goal_state, cliff) n_runs = 50 n_eps = 500 epsilon = 0.1 alpha = 0.5 gamma = 1 Q = np.zeros((height, width, len(grid_world.actions))) rewards_q_learning = np.zeros(n_eps) rewards_sarsa = np.zeros(n_eps) for _ in tqdm(range(n_runs)): Q_q_learning = Q.copy() Q_sarsa = Q.copy() for ep in range(n_eps): rewards_q_learning[ep] += q_learning(Q_q_learning, grid_world, epsilon, alpha, gamma) rewards_sarsa[ep] += sarsa(Q_sarsa, grid_world, epsilon, alpha, gamma) rewards_q_learning /= n_runs rewards_sarsa /= n_runs plt.plot(rewards_q_learning, label=\u0026#39;Q-Learning\u0026#39;) plt.plot(rewards_sarsa, label=\u0026#39;Sarsa\u0026#39;) plt.xlabel(\u0026#39;Episodes\u0026#39;) plt.ylabel(\u0026#39;Sum of rewards during episode\u0026#39;) plt.ylim([-100, 0]) plt.legend() plt.savefig(\u0026#39;./cliff_walking.png\u0026#39;) plt.close() This is our result after completing running the code.\nExpected Sarsa In the update \\eqref{eq:ql.1} of Q-learning, rather than taking the maximum over next state-action pairs, we use the expected value to consider how likely each action is under the current policy. That means, we instead have the following update rule for $Q$-value: \\begin{align} Q(S_t,A_t)\u0026amp;\\leftarrow Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma\\mathbb{E}_\\pi\\big[Q(S_{t+1},A_{t+1}\\vert S_{t+1})\\big]-Q(S_t,A_t)\\Big] \\\\ \u0026amp;\\leftarrow Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma\\sum_a\\pi(a|S_{t+1})Q(S_{t+1}|a)-Q(S_t,A_t)\\Big] \\end{align} However, given the next state, $S_{t+1}$, this algorithms move deterministically in the same direction as Sarsa moves in expectation. Thus, this method is also called Expected Sarsa.\nExpected Sarsa performs better than Sarsa since it eliminates the variance due to the randomization in selecting $A_{t+1}$. Which also means that it takes expected Sarsa more resource than Sarsa.\nDouble Q-learning Maximization Bias Consider a set of $M$ random variables $X=\\{X_1,\\dots,X_M\\}$. Say that we are interested in maximizing expected value of the r.v.s in $X$: \\begin{equation} \\max_{i=1,\\dots,M}\\mathbb{E}(X_i)\\label{eq:mb.1} \\end{equation} This value can be approximated by constructing approximations for $\\mathbb{E}(X_i)$ for all $i$. Let \\begin{equation} S=\\bigcup_{i=1}^{M}S_i \\end{equation} denote a set of samples, where $S_i$ is the subset containing samples for the variables $X_i$, and assume that the samples in $S_i$ are i.i.d. Unbiased estimates for the expected values can be obtained by computing the sample average for each variable: \\begin{equation} \\mathbb{E}(X_i)=\\mathbb{E}(\\mu_i)\\approx\\mu_i(S)\\doteq\\frac{1}{\\vert S_i\\vert}\\sum_{s\\in S_i}s, \\end{equation} where $\\mu_i$ is an estimator for variable $X_i$. This approximation is unbiased since every sample $s\\in S_i$ is an unbiased estimate for the value of $\\mathbb{E}(X_i)$. Thus, \\eqref{eq:mb.1} can be approximated by: \\begin{equation} \\max_{i=1,\\dots,M}\\mathbb{E}(X_i)=\\max_{i=1,\\dots,M}\\mathbb{E}(\\mu_i)\\approx\\max_{i=1,\\dots,M}\\mu_i(S) \\end{equation} Let $f_i$, $F_i$ denote the PDF and CDF of $X_i$ and $f_i^\\mu, F_i^\\mu$ denote the PDF and CDF of $\\mu_i$ respectively. Hence we have that \\begin{align} \\mathbb{E}(X_i)\u0026amp;=\\int_{-\\infty}^{\\infty}x f_i(x)\\hspace{0.1cm}dx;\\hspace{0.5cm}F_i(x)=P(X_i\\leq x)=\\int_{-\\infty}^{\\infty}f_i(x)\\hspace{0.1cm}dx \\\\ \\mathbb{E}(\\mu_i)\u0026amp;=\\int_{-\\infty}^{\\infty}x f_i^\\mu(x)\\hspace{0.1cm}dx;\\hspace{0.5cm}F_i^\\mu(x)=P(\\mu_i\\leq x)=\\int_{-\\infty}^{\\infty}f_i^\\mu(x)\\hspace{0.1cm}dx \\end{align} With these notations, considering the maximal estimator $\\mu_i$, which is distributed by some PDF $f_{\\max}^{\\mu}$, we have: \\begin{align} F_{\\max}^{\\mu}\u0026amp;\\doteq P(\\max_i \\mu_i\\leq x) \\\\ \u0026amp;=P(\\mu_1\\leq x;\\dots;\\mu_M\\leq x) \\\\ \u0026amp;=\\prod_{i=1}^{M}P(\\mu_i\\leq x) \\\\ \u0026amp;=\\prod_{i=1}^{M}F_i^\\mu(x) \\end{align} The value $\\max_i\\mu_i(S)$ is an unbiased estimate of $\\mathbb{E}(\\max_i\\mu_i)$, which is given by \\begin{align} \\mathbb{E}\\left(\\max_i\\mu_i\\right) \u0026amp;=\\int_{-\\infty}^{\\infty}x f_{\\max}^{\\mu}(x)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\int_{-\\infty}^{\\infty}x\\frac{d}{dx}\\left(\\prod_{i=1}^{M}F_i^\\mu(x)\\right)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\sum_{i=1}^M\\int_{-\\infty}^{\\infty}f_i^\\mu(x)\\prod_{j\\neq i}^{M}F_i^\\mu(x)\\hspace{0.1cm}dx \\end{align} However, as can be seen in \\eqref{eq:mb.1}, the order of expectation and maximization is the other way around. This leads to the result that $\\max_i\\mu_i(S)$ is a biased estimate of $\\max_i\\mathbb{E}(X_i)$\nA Solution The reason why maximization bias happens is we are using the same samples to decide which action is the best (highest reward one) and also to estimate its action-value. To overcome this situation, Hasselt (2010) proposed an alternative method that uses two set of estimators instead, $\\mu^A=\\{\\mu_1^A,\\dots,\\mu_M^A\\}$ and $\\mu^B=\\{\\mu_1^B,\\dots,\\mu_M^B\\}$. The method is thus also called double estimators.\nSpecifically, we use these two sets to learn two independent estimates, called $Q^A$ and $Q^B$, each is an estimate of the true value $q(a)$, for all $a\\in\\mathcal{A}$.\n$\\boldsymbol{n}$-step TD From the definition of one-step TD, we can formalize the idea into a more general, n-step TD. Once again, first off, we will be considering the prediction problem.\n$\\boldsymbol{n}$-step TD Prediction Recall that in one-step TD, the update is based on the next reward, bootstrapping2 from the value of the state at one step later. In particular, the target of the update is $R_{t+1}+\\gamma V_t(S_{t+1})$, which we are going to denote as $G_{t:t+1}$, or one-step return: \\begin{equation} G_{t:t+1}\\doteq R_{t+1}+\\gamma V_t(S_{t+1}) \\end{equation} where $V_t:\\mathcal{S}\\to\\mathbb{R}$ is the estimate at time step $t$ of $v_\\pi$. Thus, rather than taking into account one step later, in two-step TD, it makes sense to consider the rewards in two steps further, combined with the value function of the state at two step later. In other words, the target of two-step update is the two-step return: \\begin{equation} G_{t:t+2}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 V_{t+1}(S_{t+2}) \\end{equation} Similarly, the target of $n$-step update is the $\\boldsymbol{n}$-step return: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n V_{t+n-1}(S_{t+n}) \\end{equation} for all $n,t$ such that $n\\geq 1$ and $0\\leq t\u0026lt;T-n$. If $t+n\\geq T$, then all the missing terms are taken as zero, and the n-step return defined to be equal to the full return: \\begin{equation} G_{t:t+n}=G_t\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dots+\\gamma^{T-t-1}R_T,\\label{eq:nstp.1} \\end{equation} which is the target of the Monte Carlo update.\nHence, the $\\boldsymbol{n}$-step TD method can be defined as: \\begin{equation} V_{t+n}(S_t)\\doteq V_{t+n-1}(S_t)+\\alpha\\left[G_{t:t+n}-V_{t+n-1}(S_t)\\right], \\end{equation} for $0\\leq t\u0026lt;T$, while the values for all other states remain unchanged: $V_{t+n}(s)=V_{t+n-1}(s),\\forall s\\neq S_t$. Pseudocode of the algorithm is given right below.\nFrom \\eqref{eq:nstp.1} combined with this definition of $n$-step TD method, it is easily seen that by changing the value of $n$ from $1$ to $\\infty$, we obtain a corresponding spectrum ranging from one-step TD method to Monte Carlo method.\nFigure 2: The backup diagram of $n$-step TD methods Example: Random Walk (This example is taken from Example 7.1, Reinforcement Learning: An Introduction book; the random process image is created based on the figure from Singd \u0026amp; Sutton).\nSuppose we have a random process as following\nSpecifically, the reward is zero everywhere except the transitions into terminal states: the transition from State 2 to State 1 (with reward of $-1$) and the transition from State 20 to State 21 (with reward of $1$). The discount factor $\\gamma$ is $1$. The initial value estimates are $0$ for all states. We will implement $n$-step TD method for $n\\in\\{1,2,4,\\dots,512\\}$ and step size $\\alpha\\in\\{0,0.2,0.4,\\dots,1\\}$. The walk starts at State 10.\nSolution\nThe source code can be found here.\nAs usual, we need these packages for our implementation.\nimport numpy as np import matplotlib.pyplot as plt from tqdm import tqdm First off, we need to define our environment, the random walk process. The is_terminal() function is used to check whether the state considered is a terminal state, while the take_action() function itself returns the next state and corresponding reward given the current state and the action taken.\nclass RandomWalk: \u0026#39;\u0026#39;\u0026#39; Random walk environment \u0026#39;\u0026#39;\u0026#39; def __init__(self, n_states, start_state): self.n_states = n_states self.states = np.arange(1, n_states + 1) self.start_state = start_state self.end_states = [0, n_states + 1] self.actions = [-1, 1] self.action_prob = 0.5 self.rewards = [-1, 0, 1] def is_terminal(self, state): \u0026#39;\u0026#39;\u0026#39; Whether state @state is an end state Params ------ state: int current state \u0026#39;\u0026#39;\u0026#39; return state in self.end_states def take_action(self, state, action): \u0026#39;\u0026#39;\u0026#39; Take action @action at state @state Params ------ state: int current state action: int action taken Return ------ (next_state, reward): (int, int) a tuple of next state and reward \u0026#39;\u0026#39;\u0026#39; next_state = state + action if next_state == 0: reward = self.rewards[0] elif next_state == self.n_states + 1: reward = self.rewards[2] else: reward = self.rewards[1] return next_state, reward To calculate the RMSE, we need to compute the true value of states, which can be achieved with the help of get_true_value() function. Here we apply Bellman equations to calculate the true value of states.\ndef get_true_value(random_walk, gamma): \u0026#39;\u0026#39;\u0026#39; Calculate true value of @random_walk by Bellman equations Params ------ random_walk: RandomWalk gamma: float discount factor \u0026#39;\u0026#39;\u0026#39; P = np.zeros((random_walk.n_states, random_walk.n_states)) r = np.zeros((random_walk.n_states + 2, )) true_value = np.zeros((random_walk.n_states + 2, )) for state in random_walk.states: next_states = [] rewards = [] for action in random_walk.actions: next_state = state + action next_states.append(next_state) if next_state == 0: reward = random_walk.rewards[0] elif next_state == random_walk.n_states + 1: reward = random_walk.rewards[2] else: reward = random_walk.rewards[1] rewards.append(reward) for state_, reward_ in zip(next_states, rewards): if not random_walk.is_terminal(state_): P[state - 1, state_ - 1] = random_walk.action_prob * 1 r[state_] = reward_ u = np.zeros((random_walk.n_states, )) u[0] = random_walk.action_prob * 1 * (-1 + gamma * random_walk.rewards[0]) u[-1] = random_walk.action_prob * 1 * (1 + gamma * random_walk.rewards[2]) r = r[1:-1] true_value[1:-1] = np.linalg.inv(np.identity(random_walk.n_states) - gamma * P).dot(0.5 * (P.dot(r) + u)) true_value[0] = true_value[-1] = 0 return true_value In this random walk experiment, we simply use random policy as our action selection.\ndef random_policy(random_walk): \u0026#39;\u0026#39;\u0026#39; Choose an action randomly Params ------ random_walk: RandomWalk \u0026#39;\u0026#39;\u0026#39; return np.random.choice(random_walk.actions) Now it is time to implement our algorithm.\ndef n_step_temporal_difference(V, n, alpha, gamma, random_walk): \u0026#39;\u0026#39;\u0026#39; n-step TD Params ------ V: np.ndarray value function n: int number of steps alpha: float step size random_walk: RandomWalk \u0026#39;\u0026#39;\u0026#39; state = random_walk.start_state states = [state] T = float(\u0026#39;inf\u0026#39;) t = 0 rewards = [0] # dummy reward to save the next reward as R_{t+1} while True: if t \u0026lt; T: action = random_policy(random_walk) next_state, reward = random_walk.take_action(state, action) states.append(next_state) rewards.append(reward) if random_walk.is_terminal(next_state): T = t + 1 tau = t - n + 1 # updated state\u0026#39;s time if tau \u0026gt;= 0: G = 0 # return for i in range(tau + 1, min(tau + n, T) + 1): G += np.power(gamma, i - tau - 1) * rewards[i] if tau + n \u0026lt; T: G += np.power(gamma, n) * V[states[tau + n]] if not random_walk.is_terminal(states[tau]): V[states[tau]] += alpha * (G - V[states[tau]]) t += 1 if tau == T - 1: break state = next_state As usual, we are going illustrate our result in the main function.\nif __name__ == \u0026#39;__main__\u0026#39;: n_states = 19 start_state = 10 gamma = 1 random_walk = RandomWalk(n_states, start_state) true_value = get_true_value(random_walk, gamma) episodes = 10 runs = 100 ns = np.power(2, np.arange(0, 10)) alphas = np.arange(0, 1.1, 0.1) errors = np.zeros((len(ns), len(alphas))) for n_i, n in enumerate(ns): for alpha_i, alpha in enumerate(alphas): for _ in tqdm(range(runs)): V = np.zeros(random_walk.n_states + 2) for _ in range(episodes): n_step_temporal_difference(V, n, alpha, gamma, random_walk) rmse = np.sqrt(np.sum(np.power(V - true_value, 2) / random_walk.n_states)) errors[n_i, alpha_i] += rmse errors /= episodes * runs for i in range(0, len(ns)): plt.plot(alphas, errors[i, :], label=\u0026#39;n = %d\u0026#39; % (ns[i])) plt.xlabel(r\u0026#39;$\\alpha$\u0026#39;) plt.ylabel(\u0026#39;Average RMS error\u0026#39;) plt.ylim([0.25, 0.55]) plt.legend() plt.savefig(\u0026#39;./random_walk.png\u0026#39;) plt.close() This is our result after completing running the code.\n$\\boldsymbol{n}$-step TD Control Similarly, we can apply $n$-step TD methods to control task. In particular, we will combine the idea of $n$-step update with Sarsa, a control method we previously have defined above.\n$\\boldsymbol{n}$-step Sarsa As usual, to apply our method to control problem, rather than taking into account states, we instead consider state-action pairs $s,a$ in order to learn the value functions, $q_\\pi(s,a)$, of them.\nRecall that the target in one-step Sarsa update is \\begin{equation} G_{t:t+1}\\doteq R_{t+1}+\\gamma Q_t(S_{t+1},A_{t+1}) \\end{equation} Similar to what we have done in the previous part of $n$-step TD Prediction, we can redefine the new target of our $n$-step update \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1} R_{t+n}+\\gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}), \\end{equation} for $n\\geq 0,0\\leq t\u0026lt;T-n$, with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$. The $\\boldsymbol{n}$-step Sarsa is then can be defined as: \\begin{equation} Q_{t+n}(S_t,A_t)\\doteq Q_{t+n-1}(S_t,A_t)+\\alpha\\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\right],\\hspace{1cm}0\\leq t\u0026lt;T,\\label{eq:nss.1} \\end{equation} while the values of all other state-action pairs remain unchanged: $Q_{t+n}(s,a)=Q_{t+n-1}(s,a)$, for all $s,a$ such that $s\\neq S_t$ or $a\\neq A_t$.\nFrom this definition of $n$-step Sarsa, we can easily derive the multiple step version of Expected Sarsa, called $\\boldsymbol{n}$-step Expected Sarsa. \\begin{equation} Q_{t+n}(S_t,A_t)\\doteq Q_{t+n-1}(S_t,A_t)+\\alpha\\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\right],\\hspace{1cm}0\\leq t\u0026lt;T, \\end{equation} which has the same rule as \\eqref{eq:nss.1}, except that the target of the update in this case is defined as: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\overline{V}_{t+n-1}(S_{t+n}),\\hspace{1cm}t+n\u0026lt;T,\\label{eq:nss.2} \\end{equation} with $G_{t:t+n}=G_t$ for $t+n\\geq T$, where $\\overline{V}_t(s)$ is the expected approximate value of state $s$, using the estimated action value at time $t$, under the target policy $\\pi$: \\begin{equation} \\overline{V}_t(s)\\doteq\\sum_a\\pi(a|s)Q_t(s,a),\\hspace{1cm}\\forall s\\in\\mathcal{S}\\label{eq:nss.3} \\end{equation} If $s$ is terminal, then its expected approximate value is defined to be zero.\nPseudocode of the $n$-step Sarsa algorithm is given right below.\nWhen taking the value of $n$ from $1$ to $\\infty$, similarly, we also obtain a corresponding spectrum ranging from one-step Sarsa to Monte Carlo.\nFigure 3: The backup diagram of $n$-step methods for state-action values Off-policy $\\boldsymbol{n}$-step TD Recall that off-policy methods are ones that learn the value function of a target policy, $\\pi$, while follows a behavior policy, $b$. In this section, we will be considering an off-policy $n$-step TD, or in specifically, $n$-step TD using Importance Sampling3.\n$\\boldsymbol{n}$-step TD with Importance Sampling In $n$-step methods, returns are constructed over $n$ steps, so we are interested in the relative probability of just those $n$ actions. Thus, by weighting updates by importance sampling ratio, $\\rho_{t:t+n-1}$, which is the relative probability under the two policies $\\pi$ and $b$ of taking $n$ actions from $A_t$ to $A_{t+n-1}$: \\begin{equation} \\rho_{t:h}\\doteq\\prod_{k=t}^{\\min(h,T-1)}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}, \\end{equation} we can get the off-policy $\\boldsymbol{n}$-step TD method. \\begin{equation} V_{t+n}(S_t)\\doteq V_{t+n-1}(S_t)+\\alpha\\rho_{t:t+n-1}\\left[G_{t:t+n}-V_{t+n-1}(S_t)\\right],\\hspace{1cm}0\\leq t\u0026lt;T \\end{equation} Similarly, we have the off-policy $\\boldsymbol{n}$-step Sarsa method. \\begin{equation} Q_{t+n}(S_t,A_t)\\doteq Q_{t+n-1}(S_t,A_t)+\\alpha\\rho_{t:t+n-1}\\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\right],\\hspace{0.5cm}0\\leq t \u0026lt;T\\label{eq:nsti.1} \\end{equation} The off-policy $\\boldsymbol{n}$-step Expected Sarsa uses the same update as \\eqref{eq:nsti.1} except that it uses $\\rho_{t+1:t+n-1}$ as its importance sampling ratio instead of $\\rho_{t+1:t+n}$ and also has \\eqref{eq:nss.2} as its target.\nFollowing is pseudocode of the off-policy $n$-step Sarsa.\nPer-decision Methods with Control Variates Recall that in the note of Monte Carlo Methods, to reduce the variance even in the absence of discounting (i.e., $\\gamma=1$), we used a method called Per-decision Importance Sampling. So how about we use it with multi-step off-policy TD methods?\nWe begin rewriting the $n$-step return ending at horizon $h$ as: \\begin{equation} G_{t:h}=R_{t+1}+\\gamma G_{t+1:h},\\hspace{1cm}1\\lt h\\lt T, \\end{equation} where $G_{h:h}\\doteq V_{h-1}(S_h)$.\nSince we are following a policy $b$ that is not the same as the target policy $\\pi$, all of the resulting experience, including the first reward $R_{t+1}$ and the next state $S_{t+1}$ must be weighted by the importance sampling ratio for time $t$, $\\rho_t=\\frac{\\pi(A_t\\vert S_t)}{b(A_t\\vert S_t)}$. And moreover, to avoid the high variance when the $n$-step return is zero (resulting when the action at time $t$ would never be select by $\\pi$, which leads to $\\rho_t=0$), we define the $n$-step return ending at horizon $h$ for the off-policy state-value prediction as: \\begin{equation} G_{t:h}\\doteq\\rho_t\\left(R_{t+1}+\\gamma G_{t+1:h}\\right)+(1-\\rho_t)V_{h-1}(S_t),\\hspace{1cm}1\\lt h\\lt T\\label{eq:pdcv.1} \\end{equation} where $G_{h:h}\\doteq V_{h-1}(S_h)$. The second term of \\eqref{eq:pdcv.1}, $(1-\\rho_t)V_{h-1}(S_t)$, is called control variate, which has the expected value of $0$, and then does not change the expected update.\nFor state-action values, the off-policy definition of the $n$-step return ending at horizon $h$ can be defined as: \\begin{align} G_{t:h}\u0026amp;\\doteq R_{t+1}+\\gamma\\left(\\rho_{t+1}G_{t+1:h}+\\overline{V}_{h-1}(S_{t+1})-\\rho_{t+1}Q_{h-1}(S_{t+1},A_{t+1})\\right) \\\\ \u0026amp;=R_{t+1}+\\gamma\\rho_{t+1}\\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\big)+\\gamma\\overline{V}_{h-1}(S_{t+1}),\\hspace{1cm}t\\lt h\\leq T\\label{eq:pdcv.2} \\end{align} If $h\\lt T$, the recursion ends with $G_{h:h}\\doteq Q_{h-1}(S_h,A_h)$, whereas, if $h\\geq T$, the recursion ends with $G_{T-1:h}\\doteq R_T$.\n$\\boldsymbol{n}$-step Tree Backup So, is there possibly an off-policy method without the use of importance sampling? Yes, and of them is called Tree-backup.\nThe idea of tree-backup update is to start with the target of the one-step update, which is defined as the first reward plus the discounted estimated value of the next state. This estimated value is computed as the weighted sum of estimated action values. Each weight corresponding to an action is proportional to its probability of occurrence. In particular, the target of one-step tree-backup update is: \\begin{equation} G_{t:t+1}\\doteq R_{t+1}+\\gamma\\sum_a\\pi(a|S_{t+1})Q_t(S_{t+1},a),\\hspace{1cm}t\u0026lt;T-1 \\end{equation} which is the same as that of Expected Sarsa. With two-step update, for a certain action $A_{t+1}$ taken according to the behavior policy, $b$ (i.e. $b(A_{t+1}|S_{t+1})=1$), one step later, the estimated value of the next state similarly now, can be computed as: \\begin{equation} \\pi(A_{t+1}|S_{t+1})\\Big(R_{t+2}+\\gamma\\pi(a|S_{t+2})Q_{t+1}(S_{t+2},a)\\Big) \\end{equation} The target of two-step update, which also is defined as sum of the first reward received plus the discounted estimated value of the next state therefore, can be computed as \\begin{align} G_{t:t+2}\u0026amp;\\doteq R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})Q_{t+1}(S_{t+1},a) \\\\ \u0026amp;\\hspace{1cm}+\\gamma\\pi(A_{t+1}|S_{t+1})\\Big(R_{t+2}+\\gamma\\pi(a|S_{t+2})Q_{t+1}(S_{t+2},a)\\Big) \\\\\u0026amp;=R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})Q_{t+1}(S_{t+1},a)+\\gamma\\pi(A_{t+1}|S_{t+1})G_{t+1:t+2}, \\end{align} for $t\u0026lt;T-2$. Hence, the target of the $n$-step tree-backup update recursively can be defined as: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})Q_{t+n-1}(S_{t+1},a)+\\gamma\\pi(A_{t+1}|S_{t+1})G_{t+1:t+n}\\label{eq:nstb.1} \\end{equation} for $t\u0026lt;T-1,n\\geq 2$. The $n$-step tree-backup update can be illustrated through the following diagram\nFigure 4: The backup diagram of 3-step tree-backup With this definition of the target, we now can define our $\\boldsymbol{n}$-step tree-backup method as: \\begin{equation} Q_{t+n}(S_t,A_t)\\doteq Q_{t+n-1}(S_t,A_t)+\\alpha\\Big[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\Big],\\hspace{1cm}0\\leq t\u0026lt;T \\end{equation} while the values of all other state-action pairs remain unchanged: $Q_{t+n}(s,a)=Q_{t+n-1}(s,a)$, for all $s,a$ such that $s\\neq S_t$ or $a\\neq A_t$. Pseudocode of the n-step tree-backup algorithm is given below.\n$\\boldsymbol{n}$-step $Q(\\sigma)$ In updating the action-value functions, if we choose always to sample, we would obtain Sarsa, whereas if we choose never to sample, we would get the tree-backup algorithm. Expected Sarsa would be the case where we choose to sample for all steps except for the last one. An possible unifying method is choosing on a state-by-state basis whether to sample or not.\nWe begin by rewriting the tree-backup $n$-step return \\eqref{eq:nstb.1} in terms of the horizon $h=t+n$ and the expected approximate value $\\overline{V}$ \\eqref{eq:nss.3}: \\begin{align} \\hspace{-0.6cm}G_{t:h}\u0026amp;=R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})Q_{h-1}(S_{t+1},a)+\\gamma\\pi(A_{t+1}|S_{t+1})G_{t+1:h} \\\\ \u0026amp;=R_{t+1}+\\gamma\\overline{V}_{h-1}(S_{t+1})-\\gamma\\pi(A_{t+1}|S_{t+1})Q_{h-1}(S_{t+1},A_{t+1})+\\gamma\\pi(A_{t+1}|S_{t+1})G_{t+1:h} \\\\ \u0026amp;=R_{t+1}+\\gamma\\pi(A_{t+1}|S_{t+1})\\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\big)+\\gamma\\overline{V}_{h-1}(S_{t+1}), \\end{align} which is exactly the same as the $n$-step return for Sarsa with control variates \\eqref{eq:pdcv.2} except that the importance-sampling ratio $\\rho_{t+1}$ has been replaced with the action probability $\\pi(A_{t+1}|S_{t+1})$.\nLet $\\sigma_t\\in[0,1]$ denote the degree of sampling on step $t$, with $\\sigma=1$ denoting full sampling and $\\sigma=0$ denoting a pure expectation with no sampling. The r.v $\\sigma_t$ might be set as a function of the state, action or state-action pair at time $t$.\nFigure 5: The backup diagrams of $n$-step methods for state-action values: Sarsa, Tree-backup, Expected Sarsa, $Q(\\sigma)$ With the definition of $\\sigma_t$, we can define the $n$-step return ending at horizon $h$ of the $Q(\\sigma)$ as: \\begin{align} G_{t:h}\u0026amp;\\doteq R_{t+1}+\\gamma\\Big(\\sigma_{t+1}\\rho_{t+1}+(1-\\rho_{t+1})\\pi(A_{t+1}|S_{t+1})\\Big)\\Big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\Big)\\nonumber \\\\ \u0026amp;\\hspace{2cm}+\\gamma\\overline{V}_{h-1}(S_{t+1}), \\end{align} for $t\\lt h\\leq T$. The recursion ends with $G_{h:h}\\doteq Q_{h-1}(S_h,A_h)$ if $h\\lt T$, or with $G_{T-1:T}\\doteq R_T$ if $h=T$. Then we use the off-policy $n$-step Sarsa update \\eqref{eq:nsti.1}, which produces the pseudocode below.\nReferences [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Richard S. Sutton. Learning to predict by the methods of temporal differences. Mach Learn 3, 9–44, 1988.\n[3] Chris Watkins. Learning from Delayed Rewards. PhD Thesis, 1989.\n[4] Hado Hasselt. Double Q-learning. NIPS 2010.\n[5] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\n[6] Singh, S.P., Sutton, R.S. Reinforcement learning with replacing eligibility traces. Mach Learn 22, 123–158, 1996.\nFootnotes It is a special case of n-step TD and TD($\\lambda$).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBootstrapping is to update estimates of the value functions of states based on estimates of value functions of other states.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFor the definition of Importance Sampling method, you can read more in this section.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/td-learning/","summary":"\u003cblockquote\u003e\n\u003cp\u003eSo far in this \u003ca href=\"https://trunghng.github.io/tags/my-rl/\"\u003eseries\u003c/a\u003e, we have gone through the ideas of \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/\"\u003e\u003cstrong\u003edynamic programming\u003c/strong\u003e (DP)\u003c/a\u003e and \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/\"\u003e\u003cstrong\u003eMonte Carlo\u003c/strong\u003e\u003c/a\u003e. What will happen if we combine these ideas together? \u003cstrong\u003eTemporal-difference (TD) learning\u003c/strong\u003e is our answer.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Temporal-Difference Learning"},{"content":" The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.\n$\\newcommand{\\Var}{\\mathrm{Var}}$ $\\newcommand{\\Cov}{\\mathrm{Cov}}$\nGaussian (Normal) Distribution A random variable $X$ is said to be Gaussian or to have the Normal distribution with mean $\\mu$ and variance $\\sigma^2$ if its probability density function (PDF) is \\begin{equation} f_X(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right) \\end{equation} which we denote as $X\\sim\\mathcal{N}(\\mu,\\sigma)$.\nStandard Normal When $X$ is normally distributed with mean $\\mu=0$ and variance $\\sigma^2=1$, we call its distribution Standard Normal. \\begin{equation} X\\sim\\mathcal{N}(0,1) \\end{equation} In this case, $X$ has special notations to denote its PDF and CDF, which are \\begin{equation} \\varphi(x)=\\dfrac{1}{\\sqrt{2\\pi}}e^{-z^2/2}, \\end{equation} and \\begin{equation} \\Phi(x)=\\int_{-\\infty}^{x}\\varphi(t)\\hspace{0.1cm}dt=\\int_{-\\infty}^{x}\\dfrac{1}{\\sqrt{2\\pi}}e^{-t^2/2}\\hspace{0.1cm}dt \\end{equation} Below are some illustrations of Normal distribution.\nFigure 1: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found here Multivariate Normal Distribution A $k$-dimensional random vector $\\mathbf{X}=\\left(X_1,\\dots,X_D\\right)^\\text{T}$ is said to have a Multivariate Normal (MVN) distribution if every linear combination of the $X_i$ has a Normal distribution. Which means \\begin{equation} t_1X_1+\\ldots+t_DX_D \\end{equation} is normally distributed for any choice of constants $t_1,\\dots,t_D$. Distribution of $\\mathbf{X}$ then can be written in the following notation \\begin{equation} \\mathbf{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}), \\end{equation} where \\begin{equation} \\boldsymbol{\\mu}=\\mathbb{E}\\mathbf{X}=\\mathbb{E}\\left(\\mu_1,\\ldots,\\mu_k\\right)^\\text{T}=\\left(\\mathbb{E}X_1,\\ldots,\\mathbb{E}X_k\\right)^\\text{T} \\end{equation} is the $D$-dimensional mean vector, and covariance matrix $\\mathbf{\\Sigma}\\in\\mathbb{R}^{D\\times D}$ with \\begin{equation} \\boldsymbol{\\Sigma}_{ij}=\\mathbb{E}\\left(X_i-\\mu_i\\right)\\left(X_j-\\mu_j\\right)=\\Cov(X_i,X_j)\\label{eq:mvn.1} \\end{equation} We also have that $\\boldsymbol{\\Sigma}\\geq 0$ (positive semi-definite matrix)1.\nThus, the PDF of an MVN is defined as \\begin{equation} f_\\mathbf{X}(x_1,\\ldots,x_D)=\\dfrac{1}{(2\\pi)^{D/2}\\vert\\mathbf{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right]\\label{eq:mvn.2} \\end{equation} With this idea, Standard Normal distribution in multi-dimensional case can be defined as a Gaussian with mean $\\boldsymbol{\\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\\boldsymbol{\\Sigma}=\\mathbf{I}_{D\\times D}$.\nBivariate Normal When the number of dimensions in $\\mathbf{X}$, $D=2$, this special case of MVN is referred as Bivariate Normal (BVN).\nAn example of an BVN, $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u0026amp;0.5\\\\0.8\u0026amp;1\\end{smallmatrix}\\right]\\right)$, is shown as following.\nFigure 2: The PDF of $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u00260.5\\\\0.8\u00261\\end{smallmatrix}\\right]\\right)$. The code can be found here Properties of the covariance matrix Symmetric With the definition \\eqref{eq:mvn.1} of the covariance matrix $\\boldsymbol{\\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\\boldsymbol{\\Sigma}$ is symmetric.\nTo prove this property, first off consider a square matrix $\\mathbf{S}$, we have it can be written by \\begin{equation} \\mathbf{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2}+\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2}=\\mathbf{S}_\\text{S}+\\mathbf{S}_\\text{A}, \\end{equation} where \\begin{equation} \\mathbf{S}_\\text{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2},\\hspace{2cm}\\mathbf{S}_\\text{A}=\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2} \\end{equation} It is easily seen that $\\mathbf{S}_\\text{S}$ is symmetric because the $\\{i,j\\}$ element of its equal to the $\\{j,i\\}$ element due to \\begin{equation} (\\mathbf{S}_\\text{S})_{ij}=\\frac{(\\mathbf{S})_{ij}+(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}+(\\mathbf{S})_{ji}}{2}=(\\mathbf{S}_\\text{S})_{ji} \\end{equation} On the other hand, the matrix $\\mathbf{S}_\\text{A}$ is anti-symmetric since \\begin{equation} (\\mathbf{S}_\\text{A})_{ij}=\\frac{(\\mathbf{S})_{ij}-(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}-(\\mathbf{S})_{ji}}{2}=-(\\mathbf{S}_\\text{A})_{ji} \\end{equation} Consider the density of a distribution $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, we have that $\\boldsymbol{\\Sigma}$ is square and so is its inverse $\\boldsymbol{\\Sigma}^{-1}$. Therefore we can express $\\boldsymbol{\\Sigma}^{-1}$ as a sum of a symmetric matrix $\\boldsymbol{\\Sigma}_\\text{S}$ with an anti-symmetric matrix $\\boldsymbol{\\Sigma}_\\text{A}$ \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A} \\end{equation} We have that the density of the distribution is given by \\begin{align} f(\\mathbf{x})\u0026amp;=\\frac{1}{(2\\pi)^{D/2}\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026amp;\\propto\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026amp;=\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}(\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A})(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026amp;\\propto\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}+\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\\right] \\\\ \u0026amp;=\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}\\right] \\end{align} where in the forth step, we have defined $\\mathbf{v}\\doteq\\mathbf{x}-\\boldsymbol{\\mu}$, and where in the fifth-step, the result obtained was due to \\begin{align} \\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\u0026amp;=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i(\\boldsymbol{\\Sigma}_\\text{A})_{ij}\\mathbf{v}_j \\\\ \u0026amp;=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i-(\\boldsymbol{\\Sigma}_\\text{A})_{ji}\\mathbf{v}_j \\\\ \u0026amp;=-\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v} \\end{align} which implies that $\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}=0$.\nThus, when computing the density, the symmetric part of $\\boldsymbol{\\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\\boldsymbol{\\Sigma}^{-1}$ is symmetric, which means that $\\boldsymbol{\\Sigma}$ is also symmetric.\nWith this assumption of symmetry, the covariance matrix $\\boldsymbol{\\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.\nReal eigenvalues Consider an eigenvector, eigenvalue pair $(\\mathbf{v},\\lambda)$ of covariance matrix $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\mathbf{v}\\label{eq:rc.1} \\end{equation} Since $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{D\\times D}$, we have $\\boldsymbol{\\Sigma}=\\overline{\\boldsymbol{\\Sigma}}$. Conjugate both sides of the equation above we have \\begin{equation} \\boldsymbol{\\Sigma}\\overline{\\mathbf{v}}=\\overline{\\lambda}\\overline{\\mathbf{v}},\\label{eq:rc.2} \\end{equation} Since $\\boldsymbol{\\Sigma}$ is symmetric, we have $\\boldsymbol{\\Sigma}=\\boldsymbol{\\Sigma}^\\text{T}$. Taking the transpose of both sides of \\eqref{eq:rc.2} gives us \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\label{eq:rc.3} \\end{equation} Continuing by taking dot product of both sides of \\eqref{eq:rc.3} with $\\mathbf{v}$ lets us obtain \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}\\label{eq:rc.4} \\end{equation} On the other hand, take dot product of $\\overline{\\mathbf{v}}^\\text{T}$ with both sides of \\eqref{eq:rc.1}, we have \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v} \\end{equation} which by \\eqref{eq:rc.4} implies that \\begin{equation} \\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}, \\end{equation} or \\begin{equation} (\\lambda-\\overline{\\lambda})\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=0\\label{eq:rc.5} \\end{equation} Moreover, we have that \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\\sum_{k=1}^{D}a^2+b^2\u0026gt;0 \\end{equation} where we have denoted the complex eigenvector $\\mathbf{v}\\neq\\mathbf{0}$ as \\begin{equation} \\mathbf{v}=(a_1+i b_1,\\ldots,a_D+i b_D)^\\text{T}, \\end{equation} which implies that its complex conjugate $\\overline{\\mathbf{v}}$ can be written by \\begin{equation} \\overline{\\mathbf{v}}=(a_1-i b_1,\\ldots,a_D-i b_D)^\\text{T} \\end{equation} Therefore, by \\eqref{eq:rc.5}, we can claim that \\begin{equation} \\lambda=\\overline{\\lambda} \\end{equation} or in other words, the eigenvalue $\\lambda$ of $\\boldsymbol{\\Sigma}$ is real.\nProjection onto eigenvectors First, we have that eigenvectors $\\mathbf{v}_i$ and $\\mathbf{v}_j$ corresponding to different eigenvalues $\\lambda_i$ and $\\lambda_j$ of $\\boldsymbol{\\Sigma}$ are perpendicular, because \\begin{align} \\lambda_i\\mathbf{v}_i^\\text{T}\\mathbf{v}_j\u0026amp;=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}^\\text{T}\\mathbf{v}_j \\\\ \u0026amp;=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}_j=\\mathbf{v}_i^\\text{T}\\lambda_j\\mathbf{v}_j, \\end{align} which implies that \\begin{equation} (\\lambda_i-\\lambda_j)\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0 \\end{equation} Therefore, $\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0$ since $\\lambda_i\\neq\\lambda_j$.\nHence, for any unit eigenvectors $\\mathbf{q}_i,\\mathbf{q}_j$ of $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\mathbf{q}_i^\\text{T}\\mathbf{q}_j\\begin{cases}1,\u0026amp;\\hspace{0.5cm}\\text{if }i=j \\\\ 0,\u0026amp;\\hspace{0.5cm}\\text{if }i\\neq j\\end{cases} \\end{equation} This allows us to write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\mathbf{Q}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{Q}, \\end{equation} where $\\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\\mathbf{q}_i^\\text{T}$ and $\\boldsymbol{\\Lambda}$ is the diagonal matrix whose $\\{i,i\\}$ element is $\\lambda_i$, as \\begin{equation} \\mathbf{Q}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{q}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{q}_D^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right],\\hspace{2cm}\\boldsymbol{\\Lambda}=\\left[\\begin{matrix}\\lambda_1\u0026amp;\u0026amp; \\\\ \u0026amp;\\ddots\u0026amp; \\\\ \u0026amp;\u0026amp;\\lambda_D\\end{matrix}\\right] \\end{equation} Therefore, we can also write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\sum_{i=1}^{D}\\lambda_i\\mathbf{q}_i\\mathbf{q}_i^\\text{T} \\end{equation} Each matrix $\\mathbf{q}_i\\mathbf{q}_i^\\text{T}$ is the projection matrix onto $\\mathbf{q}_i$, then $\\boldsymbol{\\Sigma}$ can be express as a combination of perpendicular projection matrices.\nOther than that, for any eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of the matrix $\\boldsymbol{\\Sigma}$, we have \\begin{align} \\lambda_i\\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\mathbf{q}_i=\\mathbf{q}_i \\end{align} or \\begin{equation} \\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\frac{1}{\\lambda_i}\\mathbf{q}_i, \\end{equation} which implies that each eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of $\\boldsymbol{\\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\\mathbf{q}_i,1/\\lambda_i)$ of $\\boldsymbol{\\Sigma}^{-1}$. Therefore, $\\boldsymbol{\\Sigma}^{-1}$ can also be written by \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\sum_{i=1}^{D}\\frac{1}{\\lambda_i}\\mathbf{q}_i\\mathbf{q}_i^\\text{T}\\label{eq:pec.1} \\end{equation}\nGeometrical interpretation Consider the probability density function of the Gaussian \\eqref{eq:mvn.2}, by the result \\eqref{eq:pec.1}, we have that the functional dependence of the Gaussian on $\\mathbf{x}$ is through the quadratic form \\begin{align} \\Delta^2\u0026amp;=(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) \\\\ \u0026amp;=\\sum_{i=1}^{D}\\frac{y_i^2}{\\lambda_i}, \\end{align} where we have defined \\begin{equation} y_i=\\mathbf{q}_i^\\text{T}(\\mathbf{x}-\\boldsymbol{\\mu}) \\end{equation} Let $\\mathbf{y}=(y_1,\\ldots,y_D)^\\text{T}$ be the vector comprising $y_i$\u0026rsquo;s together, then we have \\begin{equation} \\mathbf{y}=\\mathbf{Q}(\\mathbf{x}-\\boldsymbol{\\mu}) \\end{equation} Consider the form of the Gaussian distribution in the new coordinate system defined by $y_i$. When changing variable from $\\mathbf{x}$ to $\\mathbf{y}$, firstly we define the Jacobian matrix $\\mathbf{J}$, whose elements are given by \\begin{equation} \\mathbf{J}_{ij}=\\frac{\\partial x_i}{\\partial y_j}=\\mathbf{Q}_{ji}, \\end{equation} which implies that \\begin{equation} \\mathbf{J}=\\mathbf{Q}^\\text{T} \\end{equation} Thus, $\\vert\\mathbf{J}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert=1$ since \\begin{equation} 1=\\vert\\mathbf{I}\\vert=\\vert\\mathbf{Q}^\\text{T}\\mathbf{Q}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert\\vert\\mathbf{Q}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert \\end{equation} Additionally, by \\eqref{eq:pec.1}, we also have \\begin{equation} \\vert\\boldsymbol{\\Sigma}\\vert^{1/2}=\\left\\vert\\mathbf{Q}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{Q}\\right\\vert^{1/2}=\\left(\\vert\\mathbf{Q}^\\text{T}\\vert\\vert\\boldsymbol{\\Lambda}\\vert\\vert\\mathbf{Q}\\vert\\right)^{1/2}=\\prod_{i=1}^{D}\\lambda_i^{1/2} \\end{equation} Therefore, in the $y_j$ coordinate system, the Gaussian distribution takes the form \\begin{equation} p(\\mathbf{y})=\\mathbf{x}\\vert\\mathbf{J}\\vert=\\prod_{j=1}^{D}\\frac{1}{(2\\pi\\lambda_j)^{1/2}}\\exp\\left(-\\frac{y_j^2}{2\\lambda_j}\\right), \\end{equation} which is the product of $D$ independent univariate Gaussian distributions.\nConditional Gaussian distribution Let $\\mathbf{x}$ be a $D$-dimensional random vector such that $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, and that we partition $\\mathbf{x}$ into two disjoint subsets $\\mathbf{x}_a$ and $\\mathbf{x}_b$ with $\\mathbf{x}_a$ is an $M$-dimensional vector and $\\mathbf{x}_b$ is a $(D-M)$-dimensional vector. \\begin{equation} \\mathbf{x}=\\left[\\begin{matrix}\\mathbf{x}_a \\\\ \\mathbf{x}_b\\end{matrix}\\right] \\end{equation} Along with them, we also define their corresponding means, as a partition of $\\boldsymbol{\\mu}$ \\begin{equation} \\boldsymbol{\\mu}=\\left[\\begin{matrix}\\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b\\end{matrix}\\right] \\end{equation} and their corresponding covariance matrices \\begin{equation} \\boldsymbol{\\Sigma}=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026amp;\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026amp;\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right], \\end{equation} which implies that $\\boldsymbol{\\Sigma}_{ab}=\\boldsymbol{\\Sigma}_{b a}^\\text{T}$.\nAnalogously, we also define the partitioned form of the precision matrix $\\boldsymbol{\\Sigma}^{-1}$ \\begin{equation} \\boldsymbol{\\Lambda}\\doteq\\boldsymbol{\\Sigma}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026amp;\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026amp;\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right], \\end{equation} Thus, we also have that $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$ since $\\boldsymbol{\\Sigma}^{-1}$ or in other words, $\\boldsymbol{\\Lambda}$ is symmetric due to the symmetry of $\\boldsymbol{\\Sigma}$. With these partitions, we can rewrite the functional dependence of the Gaussian \\eqref{eq:mvn.2} on $\\mathbf{x}$ as \\begin{align} \\hspace{-1.2cm}-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\u0026amp;=-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{aa}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b) \\\\ \u0026amp;\\hspace{0.5cm}-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{ba}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{bb}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.1} \\end{align} Consider the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$, which is the distribution of $\\mathbf{x}_a$ given $\\mathbf{x}_b$. Viewing $\\mathbf{x}_b$ as a constant, \\eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ on $\\mathbf{x}_a$, which can be continued to derive as \\begin{align} \u0026amp;-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a+\\boldsymbol{\\Lambda}_{aa}^\\text{T}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_b-\\boldsymbol{\\Lambda}_{ba}^\\text{T}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ba}\\boldsymbol{\\mu}_b\\big)+c \\\\ \u0026amp;\\hspace{3cm}=-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big)+c,\\label{eq:cgd.2} \\end{align} where $c$ is a constant, and we have used the $\\boldsymbol{\\Lambda}_{aa}=\\boldsymbol{\\Lambda}_{aa}^\\text{T}$ and $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$.\nMoreover, we have that the variation part which depends on $\\mathbf{x}$ for any Gaussian $\\mathbf{X}\\sim\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ can be written as a quadratic function of $\\mathbf{x}$ \\begin{equation} -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})=-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}+c,\\label{eq:cgd.3} \\end{equation} where $c$ is a constant. With this observation, and by \\eqref{eq:cgd.2} we have that the conditional distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\\boldsymbol{\\Sigma}_{a\\vert b}$, given by \\begin{equation} \\boldsymbol{\\Sigma}_{a\\vert b}=\\boldsymbol{\\Lambda}_{aa}^{-1},\\label{eq:cgd.4} \\end{equation} and with the corresponding mean vector, denoted as $\\boldsymbol{\\mu}_{a\\vert b}$, given by \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026amp;=\\boldsymbol{\\Sigma}_{a\\vert b}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big) \\\\ \u0026amp;=\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.5} \\end{align} To express the mean $\\boldsymbol{\\mu}_{a\\vert b}$ and the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ of $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ in terms of partition of the covariance matrix $\\boldsymbol{\\Sigma}$ instead of the precision matrix $\\boldsymbol{\\Lambda}$\u0026rsquo;s, we will be using the identity for the inverse of a partitioned matrix \\begin{align} \\left[\\begin{matrix}\\mathbf{A}\u0026amp;\\mathbf{B} \\\\ \\mathbf{C}\u0026amp;\\mathbf{D}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}\u0026amp;-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026amp;\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right],\\label{eq:cgd.6} \\end{align} where we have defined \\begin{equation} \\mathbf{M}\\doteq(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})^{-1}, \\end{equation} whose inverse $\\mathbf{M}^{-1}$ is called the Schur complement of the matrix $\\left[\\begin{matrix}\\mathbf{A}\u0026amp;\\mathbf{B} \\\\ \\mathbf{C}\u0026amp;\\mathbf{D}\\end{matrix}\\right]^{-1}$. This identity can be proved by multiplying both sides of \\eqref{eq:cgd.6} with $\\left[\\begin{matrix}\\mathbf{A}\u0026amp;\\mathbf{B} \\\\ \\mathbf{C}\u0026amp;\\mathbf{D}\\end{matrix}\\right]$ to give \\begin{align} \\mathbf{I}\u0026amp;=\\left[\\begin{matrix}\\mathbf{M}\u0026amp;-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026amp;\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{A}\u0026amp;\\mathbf{B} \\\\ \\mathbf{C}\u0026amp;\\mathbf{D}\\end{matrix}\\right] \\\\ \u0026amp;=\\left[\\begin{matrix}\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\u0026amp;\\mathbf{M}\\mathbf{B}-\\mathbf{M}\\mathbf{B} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{A}+\\mathbf{D}^{-1}\\mathbf{C}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C}\u0026amp;-\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}+\\mathbf{I}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\end{matrix}\\right] \\\\ \u0026amp;=\\left[\\begin{matrix}\\mathbf{I}\u0026amp;\\mathbf{0} \\\\ \\mathbf{D}^{-1}\\mathbf{C}\\big(\\mathbf{I}-\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\\big)\u0026amp;\\mathbf{I}\\end{matrix}\\right] \\\\ \u0026amp;=\\left[\\begin{matrix}\\mathbf{I}\u0026amp;\\mathbf{0} \\\\ \\mathbf{0}\u0026amp;\\mathbf{I}\\end{matrix}\\right]=\\mathbf{I}, \\end{align} which claims our argument.\nApplying the identity \\eqref{eq:cgd.6} into the precision matrix $\\boldsymbol{\\Lambda}=\\boldsymbol{\\Sigma}^{-1}$ gives us \\begin{equation} \\hspace{-0.5cm}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026amp;\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026amp;\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026amp;\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026amp;\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026amp;-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\\\ -\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026amp;\\boldsymbol{\\Sigma}_{bb}^{-1}+\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\end{matrix}\\right], \\end{equation} where the Schur complement of $\\mathbf{\\Sigma}^{-1}$ is given by \\begin{equation} \\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1} \\end{equation} Hence, we obtain \\begin{align} \\boldsymbol{\\Lambda}_{aa}\u0026amp;=\\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}, \\\\ \\boldsymbol{\\Lambda}_{ab}\u0026amp;=-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\end{align} Substitute these results into \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ can be rewritten as \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026amp;=\\boldsymbol{\\mu}_a+\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b), \\\\ \\boldsymbol{\\Sigma}_{a\\vert b}\u0026amp;=\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba} \\end{align} It is worth noticing that the mean $\\boldsymbol{\\mu}_{a\\vert b}$ given above is a linear function of $\\mathbf{x}_b$, while the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ is independent of $\\mathbf{x}_b$. This is an example of a linear-Gaussian model.\nMarginal Gaussian distribution Bayes\u0026rsquo; theorem for Gaussian variables In this section, we will apply the Bayes\u0026rsquo; theorem to find the marginal distribution of $p(\\mathbf{y})$ and conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\\mathbf{x})$ and a conditional Gaussian distribution $p(\\mathbf{y}\\vert\\mathbf{x})$ in which $p(\\mathbf{y}\\vert\\mathbf{x})$ has a mean that is a linear function of $\\mathbf{x}$, and a covariance matrix which is independent of $\\mathbf{x}$, as \\begin{align} p(\\mathbf{x})\u0026amp;=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026amp;=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} where $\\mathbf{A},\\mathbf{b}$ are two parameters controlling the means, and $\\boldsymbol{\\Lambda},\\boldsymbol{L}$ are precision matrices.\nIn order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\\mathbf{x},\\mathbf{y})$ by considering the augmented vector \\begin{equation} \\mathbf{z}=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\end{equation} Therefore, we have \\begin{equation} p(\\mathbf{z})=p(\\mathbf{x},\\mathbf{y})=p(\\mathbf{x})p(\\mathbf{y}\\vert\\mathbf{x}) \\end{equation} Taking the natural logarithm of both sides gives us \\begin{align} \\log p(\\mathbf{z})\u0026amp;=\\log p(\\mathbf{x})+\\log p(\\mathbf{y}\\vert\\mathbf{x}) \\\\ \u0026amp;=\\log\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1})+\\log\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}) \\\\ \u0026amp;=-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Lambda}(\\mathbf{x}-\\boldsymbol{\\mu})-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})+c\\label{eq:btg.1} \\end{align} where $c$ is a constant in terms of $\\mathbf{x}$ and $\\mathbf{y}$, i.e., $c$ is independent of $\\mathbf{x},\\mathbf{y}$.\nIt is easily to notice that \\eqref{eq:btg.1} is a quadratic function of the components of $\\mathbf{z}$, which implies that $p(\\mathbf{z})$ is a Gaussian. By \\eqref{eq:cgd.3}, in order to find the covariance matrix of $\\mathbf{z}$, we consider the quadratic terms in \\eqref{eq:btg.1}, which are given by \\begin{align} \u0026amp;-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\\\ \u0026amp;=-\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\big(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\\big)\\mathbf{x}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{y}-\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{A}\\mathbf{x}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{y}\\Big] \\\\ \u0026amp;=-\\frac{1}{2}\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026amp;-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026amp;\\mathbf{L}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\\\ \u0026amp;=-\\frac{1}{2}\\mathbf{z}^\\text{T}\\mathbf{R}\\mathbf{z}, \\end{align} which implies that the precision matrix of $\\mathbf{z}$ is $\\mathbf{R}$, defined as \\begin{equation} \\mathbf{R}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026amp;-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026amp;\\mathbf{L}\\end{matrix}\\right] \\end{equation} Thus, using the identity \\eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution \\begin{equation} \\boldsymbol{\\Sigma}_\\mathbf{z}=\\mathbf{R}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}^{-1}\u0026amp;\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T} \\\\ \\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\u0026amp;\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}\\end{matrix}\\right] \\end{equation} Analogously, by \\eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \\eqref{eq:btg.1}, which are \\begin{align} \\hspace{-0.7cm}\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}+\\boldsymbol{\\mu}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}+(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{b}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\Big]\u0026amp;=\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \u0026amp;=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right] \\end{align} Thus, by \\eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by \\begin{equation} \\boldsymbol{\\mu}_\\mathbf{z}=\\boldsymbol{\\Sigma}_\\mathbf{z}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\mu} \\\\ \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}\\end{matrix}\\right] \\end{equation} Given the mean $\\boldsymbol{\\mu}_\\mathbf{z}$ and the covariance matrix $\\boldsymbol{\\Sigma}_\\mathbf{z}$ of the joint distribution of $\\mathbf{x},\\mathbf{y}$, by \\eqref{22} and \\eqref{23}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\\mathbf{y})$, which are \\begin{align} \\boldsymbol{\\mu}_\\mathbf{y}\u0026amp;=\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}, \\\\ \\boldsymbol{\\Sigma}_\\mathbf{y}\u0026amp;=\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}, \\end{align} and also, by \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$, which are given by \\begin{align} \\boldsymbol{\\mu}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026amp;=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1}\\big(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}\\big) \\\\ \\boldsymbol{\\Sigma}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026amp;=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{align} In Bayesian approach, we can consider $p(\\mathbf{x})$ as a prior distribution over $\\mathbf{x}$, and if $\\mathbf{y}$ is observed, the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ will represents the corresponding posterior distribution over $\\mathbf{x}$.\nRemark\nGiven a marginal Gaussian distribution for $\\mathbf{x}$ and a conditional Gaussian distribution for $\\mathbf{y}$ given $\\mathbf{x}$ in the form \\begin{align} p(\\mathbf{x})\u0026amp;=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026amp;=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} the marginal distribution of $\\mathbf{y}$ and the conditional distribution of $\\mathbf{x}$ given $\\mathbf{y}$ are then given by \\begin{align} p(\\mathbf{y})\u0026amp;=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b},\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}), \\\\ p(\\mathbf{x}\\vert\\mathbf{y})\u0026amp;=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\Sigma}(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}),\\boldsymbol{\\Sigma}) \\end{align} where \\begin{equation} \\boldsymbol{\\Sigma}=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{equation}\nReferences [1] Joseph K. Blitzstein \u0026amp; Jessica Hwang. Introduction to Probability.\n[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer New York, NY, 2006.\n[3] Gilbert Strang. Introduction to Linear Algebra, 5th edition, 2016.\nFootnotes The definition of covariance matrix $\\boldsymbol{\\Sigma}$ can be rewritten as \\begin{equation*} \\boldsymbol{\\Sigma}=\\Cov(\\mathbf{X},\\mathbf{X})=\\Var(\\mathbf{X}) \\end{equation*} Let $\\mathbf{z}\\in\\mathbb{R}^D$, we have \\begin{equation*} \\Var(\\mathbf{z}^\\text{T}\\mathbf{X})=\\mathbf{z}^\\text{T}\\Var(\\mathbf{X})\\mathbf{z}=\\mathbf{z}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{z} \\end{equation*} And since $\\Var(\\mathbf{z}^\\text{T}\\mathbf{X})\\geq0$, we also have that $\\mathbf{z}^\\text{T}\\mathbf{\\Sigma}\\mathbf{z}\\geq0$, which proves that $\\boldsymbol{\\Sigma}$ is a positive semi-definite matrix.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/probability-statistics/normal-dist/","summary":"\u003cblockquote\u003e\n\u003cp\u003eThe \u003cstrong\u003eGaussian (Normal) distribution\u003c/strong\u003e is a continuous distribution with a bell-shaped PDF used widely in statistics due to the \u003cstrong\u003eCentral Limit theorem\u003c/strong\u003e. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Gaussian Distribution"},{"content":" Recall that in the previous note, Infinite Series of Constants, we mentioned a type of series called power series a lot. In the content of this note, we will be diving deeper into details of its.\nPower Series A power series is a series of the form \\begin{equation} \\sum_{n=0}^{\\infty}a_nx^n=a_0+a_1x+a_2x^2+\\ldots+a_nx^n+\\ldots, \\end{equation} where the coefficient $a_n$ are constants and $x$ is a variable.\nThe Interval of Convergence Similar to what we have done in the note of infinite series of constants, we begin studying properties of power series by considering their convergence behavior.\nLemma 1\nIf a power series $\\sum a_nx^n$ converges at $x_1$, with $x_1\\neq 0$, then it converges absolutely at all $x$ with $\\vert x\\vert\u0026lt;\\vert x_1\\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\\vert x\\vert\u0026gt;\\vert x_1\\vert$.\nProof\nBy the $n$-th term test, we have that if $\\sum a_nx^n$ converges, then $a_nx^n\\to 0$. In particular, if $n$ is sufficiently large, then $\\vert a_n{x_1}^n\\vert\u0026lt;1$, and therefore \\begin{equation} \\vert a_nx^n\\vert=\\vert a_n{x_1}^n\\vert\\left\\vert\\dfrac{x}{x_1}\\right\\vert^n\u0026lt;r^n,\\tag{1}\\label{1} \\end{equation} where $r=\\vert\\frac{x}{x_1}\\vert$. Suppose that $\\vert x\\vert\u0026lt;\\vert x_1\\vert$, we have \\begin{equation} r=\\left\\vert\\dfrac{x}{x_1}\\right\\vert\u0026lt;1, \\end{equation} which leads to the result that geometric series $\\sum r^n$ converges (with the sum $\\frac{1}{1-r}$). And hence, from \\eqref{1} and by the comparison test, the series $\\sum\\vert a_nx^n\\vert$ also converges.\nMoreover, if $\\sum a_n{x_1}^n$ diverges, then $\\sum\\vert a_n{x_1}^n\\vert$ also diverges. By the comparison test, for any $x$ such that $\\vert x\\vert\u0026gt;\\vert x_1\\vert$, we also have that $\\sum\\vert a_nx^n\\vert$ diverges. This leads to the divergence of $\\sum a_nx^n$, because if the series $\\sum a_nx^n$ converges, so does $\\sum\\vert a_nx^n\\vert$, which contradicts to our result.\nThese are some main facts about the convergence behavior of an arbitrary power series and some properties of its:\nGiven a power series $\\sum a_nx^n$, precisely one of the following is true: The series converges only for $x=0$. The series is absolutely convergent for all $x$. There exists a positive real number $R$ such that the series is absolutely convergent for $\\vert x\\vert\\lt R$ and divergent for $\\vert x\\vert\u003eR$. The positive real number $R$ is called radius of convergence of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$. The set of all $x$'s for which a power series converges is called its interval of convergence. When the series converges only for $x=0$, we define $R=0$; and we define $R=\\infty$ when the series converges for all $x$. Every power series $\\sum a_nx^n$ has a radius of convergence $R$, where $0\\leq R\\leq\\infty$, with the property that the series converges absolutely if $\\vert x\\vert\\lt R$ and diverges if $\\vert x\\vert\u003eR$. Example Find the interval of convergence of the series \\begin{equation} \\sum_{n=0}^{\\infty}\\dfrac{x^n}{n+1}=1+\\dfrac{x}{2}+\\dfrac{x^2}{3}+\\ldots \\end{equation}\nSolution\nIn order to find the interval of convergence of a series, we begin by identifying its radius of convergence.\nConsider a power series $\\sum a_nx^n$. Suppose that this limit exists, and has $\\infty$ as an allowed value, we have \\begin{equation} \\lim_{n\\to\\infty}\\dfrac{\\vert a_{n+1}x^{n+1}\\vert}{a_nx^n}=\\lim_{n\\to\\infty}\\left\\vert\\dfrac{a_{n+1}}{a_n}\\right\\vert\\cdot\\vert x\\vert=\\dfrac{\\vert x\\vert}{\\lim_{n\\to\\infty}\\left\\vert\\frac{a_n}{a_{n+1}}\\right\\vert}=L \\end{equation} By the ratio test, we have $\\sum a_nx^n$ converges absolutely if $L\u0026lt;1$ and diverges in case of $L\u0026gt;1$. Or in other words, the series converges absolutely if \\begin{equation} \\vert x\\vert\u0026lt;\\lim_{n\\to\\infty}\\left\\vert\\dfrac{a_n}{a_{n+1}}\\right\\vert, \\end{equation} or diverges if \\begin{equation} \\vert x\\vert\u0026gt;\\lim_{n\\to\\infty}\\left\\vert\\dfrac{a_n}{a_{n+1}}\\right\\vert \\end{equation} From the definition of radius of convergence, we can choose the radius of converge of $\\sum a_nx^n$ as \\begin{equation} R=\\lim_{n\\to\\infty}\\left\\vert\\dfrac{a_n}{a_{n+1}}\\right\\vert \\end{equation}\nBack to our problem, for the series $\\sum\\frac{x^n}{n+1}$, we have its radius of convergence is \\begin{equation} R=\\lim_{n\\to\\infty}\\left\\vert\\dfrac{a_n}{a_{n+1}}\\right\\vert=\\lim_{n\\to\\infty}\\dfrac{\\frac{1}{n+1}}{\\frac{1}{n+2}}=\\lim_{n\\to\\infty}\\dfrac{n+2}{n+1}=1 \\end{equation} At $x=1$, the series becomes the harmonic series, i.e. \\begin{equation} 1+\\frac{1}{2}+\\frac{1}{3}+\\ldots, \\end{equation} which diverges; and at $x=-1$, it is the alternating harmonic series, i.e. \\begin{equation} 1-\\frac{1}{2}+\\frac{1}{3}-\\ldots, \\end{equation} which converges. Hence, the interval of convergence of the series is $[-1,1)$.\nDifferentiation and Integration of Power Series It is easily seen that the sum of the series $\\sum_{n=0}^{\\infty}a_nx^n$ is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as \\begin{equation} f(x)=\\sum_{n=0}^{\\infty}a_nx^n=a_0+a_1x+a_2x^2+\\ldots+a_nx^n+\\ldots\\tag{2}\\label{2} \\end{equation} This relation between the series and the function is also expressed by saying that $\\sum a_nx^n$ is a power series expansion of $f(x)$.\nFollowing are some crucial facts about that relation.\nThe function $f(x)$ defined by \\eqref{2} is continuous on the open interval $(-R,R)$. The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula \\begin{equation} f'(x)=a_1+2a_2x+3a_3x^2+\\ldots+na_nx^{n-1}+\\ldots\\tag{3}\\label{3} \\end{equation} If $x$ is any point in $(-R,R)$, then \\begin{equation} \\int_{0}^{x}f(t)\\,dt=a_0x+\\dfrac{1}{2}a_1x^2+\\dfrac{1}{3}a_2x^3+\\ldots+\\dfrac{1}{n+1}a_nx^{n+1}+\\ldots\\tag{4}\\label{4} \\end{equation} Remark\nWe have that series \\eqref{3} and \\eqref{4} converge on the interval $(-R,R)$.\nProof\nWe begin by proving the convergence on $(-R,R)$ of \\eqref{3}.\nLet $x$ be a point in the interval $(-R,R)$ and choose $\\epsilon\u0026gt;0$ so that $\\vert x\\vert+\\epsilon\u0026lt;R$. Since $\\vert x\\vert+\\epsilon$ is in the interval, $\\sum\\vert a_n\\left(\\vert x\\vert+\\epsilon\\right)^n\\vert$ converges.\nWe continue by proving the inequality \\begin{equation} \\vert nx^{n-1}\\vert\\leq\\left(\\vert x\\vert+\\epsilon\\right)^n\\hspace{1cm}\\forall n\\geq n_0, \\end{equation} where $\\epsilon\u0026gt;0$, $n_0$ is a positive integer. We have \\begin{align} \\lim_{n\\to\\infty}n^{1/n}\u0026amp;=\\lim_{n\\to\\infty} \\\\ \u0026amp;=\\lim_{n\\to\\infty}\\exp\\left(\\frac{\\ln n}{n}\\right) \\\\ \u0026amp;=\\exp\\left(\\lim_{n\\to\\infty}\\frac{\\ln n}{n}\\right) \\\\ \u0026amp;={\\rm e}^0=1, \\end{align} where in the fourth step, we use the L’Hospital theorem1. Therefore, as $n\\to\\infty$ \\begin{equation} n^{1/n}\\vert x\\vert^{1-1/n}\\to\\vert x\\vert \\end{equation} Then for all sufficiently large $n$\u0026rsquo;s \\begin{align} n^{1/n}\\vert x\\vert^{1-1/n}\u0026amp;\\leq\\vert x\\vert+\\epsilon \\\\ \\vert nx^{n-1}\\vert\u0026amp;\\leq\\left(\\vert x\\vert+\\epsilon\\right)^n \\end{align} This implies that \\begin{equation} \\vert na_nx^{n-1}\\vert\\leq\\vert a_n\\left(\\vert x\\vert+\\epsilon\\right)^n\\vert \\end{equation} By the comparison test, we have that the series $\\sum\\vert na_nx^{n-1}\\vert$ converges, and so does $\\sum na_nx^{n-1}$. Since $\\sum\\vert a_nx^n\\vert$ converges and \\begin{equation} \\left\\vert\\dfrac{a_nx^n}{n+1}\\right\\vert\\leq\\vert a_nx^n\\vert, \\end{equation} the comparison test implies that $\\sum\\left\\vert\\frac{a_nx^n}{n+1}\\right\\vert$ converges, and therefore \\begin{equation} x\\sum\\frac{a_nx^n}{n+1}=\\sum\\frac{1}{n+1}a_nx^{n+1} \\end{equation} also converges. Differentiation of Power Series If we instead apply (ii) to the function $f\u0026rsquo;(x)$ in \\eqref{3}, then it follows that $f\u0026rsquo;(x)$ is also differentiable. Doing the exact same process to $f\u0026rsquo;'(x)$, we also have that $f\u0026rsquo;'(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:\nIn the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term. \\begin{equation} \\dfrac{d}{dx}\\left(\\sum a_nx^n\\right)=\\sum\\dfrac{d}{dx}(a_nx^n) \\end{equation}\nIntegration of Power Series Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \\eqref{4} as \\begin{equation} \\int\\left(\\sum a_nx^n\\right),dx=\\sum\\left(\\int a_nx^n,dx\\right) \\end{equation}\nExample Find a power series expansion of ${\\rm e}^x$.\nSolution\nSince ${\\rm e}^x$ is the only function that equals its own derivatives2 and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We thus want each term to be the derivative of the one that follows it.\nStarting with $1$ as the constant term, the next should be $x$, then $\\frac{1}{2}x^2$, then $\\frac{1}{2.3}x^3$, and so on. This produces the series \\begin{equation} 1+x+\\dfrac{x^2}{2!}+\\dfrac{x^3}{3!}+\\ldots+\\dfrac{x^n}{n!}+\\ldots,\\tag{5}\\label{5} \\end{equation} which converges for all $x$ because \\begin{equation} R=\\lim_{n\\to\\infty}\\dfrac{\\frac{1}{n!}}{\\frac{1}{(n+1)!}}=\\lim_{n\\to\\infty}(n+1)=\\infty \\end{equation} We have constructed the series \\eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$, \\begin{equation} {\\rm e}^x=1+x+\\dfrac{x^2}{2!}+\\dfrac{x^3}{3!}+\\ldots+\\dfrac{x^n}{n!}+\\ldots \\end{equation}\nTaylor Series, Taylor\u0026rsquo;s Formula Taylor Series Assume that $f(x)$ is the sum of a power series with positive radius of convergence \\begin{equation} f(x)=\\sum_{n=0}^{\\infty}a_nx^n=a_0+a_1x+a_2x^2+\\ldots,\\hspace{1cm}R\u0026gt;0\\tag{6}\\label{6} \\end{equation} By the results obtained from previous section, differentiating \\eqref{6} term by term we have \\begin{align} f^{(1)}(x)\u0026amp;=a_1+2a_2x+3a_3x^2+\\ldots \\\\ f^{(2)}(x)\u0026amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\\ldots \\\\ f^{(3)}(x)\u0026amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\\ldots \\end{align} and in general, \\begin{equation} f^{(n)}(x)=n!a_n+A(x),\\tag{7}\\label{7} \\end{equation} where $A(x)$ contains $x$ as a factor.\nSince these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \\eqref{7} we obtain \\begin{equation} f^{(n)}(0)=n!a_n, \\end{equation} so \\begin{equation} a_n=\\dfrac{f^{(n)}(0)}{n!} \\end{equation} Putting this result in \\eqref{6}, our series becomes \\begin{equation} f(x)=f(0)+f^{(1)}(0)x+\\dfrac{f^{(2)}(0)}{2!}x^2+\\ldots+\\dfrac{f^{(n)}(0)}{n!}x^n+\\ldots\\tag{8}\\label{8} \\end{equation} This power series is called Taylor series of $f(x)$ (at $x=0$), which is named after the person who introduced it, Brook Taylor.\nIf we use the convention that $0!=1$, then \\eqref{8} can be written as \\begin{equation} f(x)=\\sum_{n=0}^{\\infty}\\dfrac{f^{(n)}(0)}{n!}x^n \\end{equation} The numbers $a_n=\\frac{f^{(n)}(0)}{n!}$ are called the Taylor coefficients of $f(x)$.\nRemark\nGiven a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?\n\\begin{equation} f(x)=\\sum_{n=0}^{\\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\\ldots \\end{equation} Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is \\begin{align} f(x)\u0026amp;=\\sum_{n=0}^{\\infty}\\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\\\ \u0026amp;=f(a)+f^{(1)}(a)(x-a)+\\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\\ldots+\\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\\ldots\\tag{9}\\label{9} \\end{align}\nTaylor\u0026rsquo;s Formula If we break off the Taylor series on the right side of \\eqref{8} at the term containing $x^n$ and define the remainder $R_n(x)$ by the equation \\begin{equation} f(x)=f(0)+f^{(1)}(0)x+\\dfrac{f^{(2)}(0)}{2!}x^2+\\ldots+\\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\\tag{10}\\label{10} \\end{equation} then the Taylor series on the right side of \\eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when \\begin{equation} \\lim_{n\\to\\infty}R_n(x)=0 \\end{equation} Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing \\begin{equation} R_n(x)=S_n(x)x^{n+1} \\end{equation} for $x\\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for $0\\leq t\\leq x$ (or $x\\leq t\\leq 0$) by writing \\begin{multline} F(t)=f(x)-f(t)-f^{(1)}(t)(x-t)-\\dfrac{f^{(2)}(t)}{2!}(x-t)^2-\\ldots \\\\ -\\dfrac{f^{(n)}(t)}{n!}(x-t)^n-S_n(x)(x-t)^{n+1} \\end{multline} It is easily seen that $F(x)=0$. Also, from equation \\eqref{10}, we have that $F(0)=0$. Then by the Mean Value Theorem3, $F\u0026rsquo;(c)=0$ for some constant $c$ between $0$ and $x$.\nDifferentiating $F(t)$ w.r.t $t$, and evaluateing it at $t=c$ give us \\begin{equation} F\u0026rsquo;(c)=-\\dfrac{f^{(n+1)}(c)}{n!}(x-c)^n+S_n(x)(n+1)(x-c)^n=0, \\end{equation} which implies that \\begin{equation} S_n(x)=\\dfrac{f^{(n+1)}(c)}{(n+1)!}, \\end{equation} and \\begin{equation} R_n(x)=S_n(x)x^{n+1}=\\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}, \\end{equation} which makes \\eqref{10} become \\begin{equation} f(x)=f(0)+f^{(1)}(0)x+\\dfrac{f^{(2)}(0)}{2!}x^2+\\ldots+\\dfrac{f^{(n)}(0)}{n!}x^n+\\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}, \\end{equation} where $c$ is some number between $0$ and $x$. This equation is called Taylor\u0026rsquo;s formula with derivative remainder.\nMoreover, with this formula we can rewrite \\eqref{9} as \\begin{multline} f(x)=f(a)+f^{(1)}(a)(x-a)+\\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\\ldots \\\\ +\\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1},\\tag{11}\\label{11} \\end{multline} where $c$ is some number between $a$ and $x$.\nThe polynomial part of \\eqref{11} \\begin{multline} \\sum_{j=0}^{n}\\dfrac{f^{(j)}(a)}{j!}(x-a)^j=f(a)+f^{(1)}(a)(x-a) \\\\ +\\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\\ldots+\\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\end{multline} is called the nth-degree Taylor polynomial at $x=a$.\nOn the other hand, the remainder part of \\eqref{11} \\begin{equation} R_n(x)=\\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1} \\end{equation} is often called Lagrange\u0026rsquo;s remainder formula.\nRemark\nIt is worth remarking that power series expansions are unique. This means that if a function $f(x)$ can be expressed as a sum of a power series by any method, then this series must be the Taylor series of $f(x)$.\nOperations on Power Series Multiplication Suppose we are given two power series expansions \\begin{align} f(x)\u0026amp;=\\sum a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+\\ldots\\tag{12}\\label{12} \\\\ g(x)\u0026amp;=\\sum b_nx^n=b_0+b_1x+b_2x^2+b_3x^3+\\ldots\\tag{13}\\label{13} \\end{align} both valid on $(-R,R)$. If we multiply these two series term by term, we obtain the power series \\begin{multline} a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2 \\\\ +(a_0b_3+a_1b_2+a_2b_1+a_3b_0)x^3+\\ldots \\end{multline} Briefly, we have multiplied \\eqref{12} and \\eqref{13} to obtain \\begin{equation} f(x)g(x)=\\sum_{n=0}^{\\infty}\\left(\\sum_{k=0}^{n}a_kb_{n-k}\\right)x^n\\tag{14}\\label{14} \\end{equation} By the Theorem 10, we have that this product of the series \\eqref{12} and \\eqref{13} actually converges on the interval $(-R,R)$ to the product of the functions $f(x)$ and $g(x)$, as indicated by \\eqref{14}.\nDivision With the two series \\eqref{12} and \\eqref{13}, we have \\begin{equation} \\dfrac{\\sum a_nx^n}{\\sum b_nx^n}=\\left(\\sum a_nx^n\\right).\\left(\\dfrac{1}{\\sum b_nx^n}\\right) \\end{equation} This suggests us that if we can expand $\\frac{1}{\\sum b_nx^n}$ in a power series with positive radius of convergence $\\sum c_nx^n$, and multiply this series by $\\sum a_nx^n$, we can compute the division of our two series $\\sum a_nx^n$ and $\\sum b_nx^n$.\nTo do the division properly, it is necessary to assume that $b_0\\neq0$ (for the case $x=0$). Moreover, without any loss of generality, we may assume that $b_0=1$, because with the assumption that $b_0\\neq0$, we simply factor it out \\begin{equation} \\dfrac{1}{b_0+b_1x+b_2x^2+\\ldots}=\\dfrac{1}{b_0}.\\dfrac{1}{1+\\frac{b_1}{b_0}x+\\frac{b_2}{b_0}x^2+\\ldots} \\end{equation}\nWe begin by determining the $c_n$\u0026rsquo;s. Since $\\frac{1}{\\sum b_nx^n}=\\sum c_nx^n$, then $(\\sum b_nx^n)(\\sum c_nx^n)=1$, so \\begin{multline} b_0c_0+(b_0c_1+b_1c_0)x+(b_0c_2+b_1c_1+b_2c_0)x^2+\\ldots \\\\ +(b_0c_n+b_1c_{n-1}+\\ldots+b_nc_0)x^n+\\ldots=1, \\end{multline} and since $b_0=1$, we can determine the $c_n$\u0026rsquo;s recursively \\begin{align} c_0\u0026amp;=1 \\\\ c_1\u0026amp;=-b_1c_0 \\\\ c_2\u0026amp;=-b_1c_1-b_2c_0 \\\\ \u0026amp;\\vdots \\\\ c_n\u0026amp;=-b_1c_{n-1}-b_2c_{n-2}-\\ldots-b_nc_0 \\\\ \u0026amp;\\vdots \\end{align} Our work remains to proving that the power series $\\sum c_nx^n$ with these coefficients has positive radius of convergence, and for this it suffices to show that the series converges for at least one nonzero $x$.\nLet $r$ be any number such that $0\u0026lt;r\u0026lt;R$, so that $\\sum b_nr^n$ converges. Then there exists a constant $K\\geq 1$ with the property that $\\vert b_nr^n\\vert\\leq K$ or $\\vert b_n\\vert\\leq\\frac{K}{r^n}$ for all $n$. Therefore, \\begin{align} \\vert c_0\\vert\u0026amp;=1\\leq K, \\\\ \\vert c_1\\vert\u0026amp;=\\vert b_1c_0\\vert=\\vert b_1\\vert\\leq \\dfrac{K}{r}, \\\\ \\vert c_2\\vert\u0026amp;\\leq\\vert b_1c_1\\vert+\\vert b_2c_0\\vert\\leq\\dfrac{K}{r}.\\dfrac{K}{r}+\\dfrac{K}{r^2}.K=\\dfrac{2K^2}{r^2}, \\\\ \\vert c_3\\vert\u0026amp;\\leq\\vert b_1c_2\\vert+\\vert b_2c_1\\vert+\\vert b_3c_0\\vert\\leq\\dfrac{K}{r}.\\dfrac{2K^2}{r^2}+\\dfrac{K}{r^2}.\\dfrac{K}{r}+\\dfrac{K}{r^3}.K \\\\ \u0026amp;\\hspace{5.3cm}\\leq(2+1+1)\\dfrac{K^3}{r^3}=\\dfrac{4K^3}{r^3}=\\dfrac{2^2K^3}{r^3}, \\end{align} since $K^2\\leq K^3$ since $K\\geq1$. In general, \\begin{align} \\vert c_n\\vert\u0026amp;\\leq\\vert c_1b_{n-1}\\vert+\\vert c_2b_{n-2}\\vert+\\ldots+\\vert b_nc_0\\vert \\\\ \u0026amp;\\leq\\dfrac{K}{r}.\\dfrac{2^{n-2}K^{n-1}}{r^{n-1}}+\\dfrac{K}{r^2}.\\dfrac{2^{n-3}K^{n-2}}{r^{n-2}}+\\ldots+\\dfrac{K}{r^n}.K \\\\ \u0026amp;\\leq(2^{n-2}+2^{n-3}+\\ldots+1+1)\\dfrac{K^n}{r^n}=\\dfrac{2^{n-1}K^n}{r^n}\\leq\\dfrac{2^nK^n}{r^n} \\end{align} Hence, for any $x$ such that $\\vert x\\vert\u0026lt;\\frac{r}{2K}$, we have that the series $\\sum c_nx^n$ converges absolutely, and therefore converges. Or in other words, $\\sum c_nx^n$ has nonzero radius of convergence.\nSubstitution If a power series \\begin{equation} f(X)=a_0+a_1x+a_2x^2+\\ldots\\tag{15}\\label{15} \\end{equation} converges for $\\vert x\\vert\u0026lt;R$ and if $\\vert g(x)\\vert\u0026lt;R$, then we can find $f(g(x))$ by substituting $g(x)$ for $x$ in \\eqref{15}. Suppose $g(x)$ is given by a power series, \\begin{equation} g(x)=b_0+b_1x+b_2x^2+\\ldots,\\tag{16}\\label{16} \\end{equation} therefore, \\begin{align} f(g(x))\u0026amp;=a_0+a_1g(x)+a_2g(x)^2+\\ldots \\\\ \u0026amp;=a_0+a_1(b+0+b_1x+\\ldots)+a_2(b_0+b_1x+\\ldots)^2+\\ldots \\end{align} The power series formed in this way converges to $f(g(x))$ whenever \\eqref{16} is absolutely convergent and $\\vert g(x)\\vert\u0026lt;R$.\nEven and Odd Functions A function $f(x)$ defined on $(-R,R)$ is said to be even if \\begin{equation} f(-x)=f(x), \\end{equation} and odd if \\begin{equation} f(-x)=-f(x) \\end{equation} Then if $f(x)$ is an even function, then its Taylor series has the form \\begin{equation} \\sum_{n=0}^{\\infty}a_{2n}x^{2n}=a_0+a_2x^2+a_4x^4+\\ldots \\end{equation} and if $f(x)$ is an odd function, then its Taylor series has the form \\begin{equation} \\sum_{n=0}^{\\infty}a_{2n+1}x^{2n+1}=a_1x+a_3x^3+a_5x^5+\\ldots \\end{equation} since if $f(x)=\\sum_{n=0}^{\\infty}a_nx^n$ is even, then $\\sum_{n=0}^{\\infty}a_nx^n=\\sum_{n=0}^{\\infty}(-1)^na_nx^n$, so by the uniqueness of the Taylor series expansion, we have that $a_n=(-1)^na_n$.\nAnalogously, $a_n=(-1)^{n+1}a_n$ if $f(x)$ is an odd function.\nUniform Convergence for Power Series Consider a power series $\\sum a_nx^n$ with positive radius of convergence $R$, and let $f(x)$ be its sum.\nIn the section above, we stated that $f(x)$ is continuous and differentiable on $(-R,R)$, and we can differentiate and integrate it term by term. So let\u0026rsquo;s prove these statements!\nLet $S_n(x)$ be the $n$-th partial sum of the series, so that \\begin{equation} S_n(x)=\\sum_{i=0}^{n}a_ix^i=a_0+a_1x+a_2x^2+\\ldots+a_nx^n \\end{equation} Similar to what we did in Taylor\u0026rsquo;s formula, we write \\begin{equation} f(x)=S_n(x)+R_n(x), \\end{equation} which implies that the remainder is given by \\begin{equation} R_n(x)=a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\\ldots \\end{equation} For each $x$ in the interval of convergence, we know that $R_n(x)\\to0$ as $n\\to\\infty$; that is, for any given $\\epsilon\u0026gt;0$, and for an integer $n_0$ large enough, we have \\begin{equation} \\vert R_n(x)\\vert\u0026lt;\\epsilon,\\hspace{1cm}n\\geq n_0\\tag{17}\\label{17} \\end{equation} This is true for each $x$ individually, and is an equivalent way of expressing the fact that $\\sum a_nx^n$ converges to $f(x)$.\nMoreover, for every $x$ in the given a closed interval $\\vert x\\vert\\leq\\vert x_1\\vert\u0026lt;R$, we have \\begin{align} \\vert R_n(x)\\vert\u0026amp;=\\left\\vert a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\\ldots\\right\\vert \\\\ \u0026amp;\\leq\\left\\vert a_{n+1}x^{n+1}\\right\\vert+\\left\\vert a_{n+2}x^{n+2}\\right\\vert+\\ldots \\\\ \u0026amp;\\leq\\left\\vert a_{n+1}{x_1}^{n+1}\\right\\vert+\\left\\vert a_{n+2}{x_1}^{n+2}\\right\\vert+\\ldots \\end{align} Because of the absolute convergence of $\\sum a_n{x_1}^n$, the last sum can be made $\u0026lt;\\epsilon$ by taking $n$ large enough, $n\\geq n_0$. Therefore, we have that \\eqref{17} holds for all $x$ inside the closed interval $\\vert x\\vert\\leq\\vert x_1\\vert$ inside the interval of convergence $(-R,R)$.\nOr in other words, $R_n(x)$ can be made small independently of $x$ in the given closed interval $\\vert x\\vert\\leq\\vert x_1\\vert$, which is equivalent to saying that the series $\\sum a_nx^n$ is uniformly convergent in this interval.\nContinuity of the Sum In order to prove that $f(x)$ is continuous on $(-R,R)$, it suffices to prove that $f(x)$ is continuous at each point $x_0$ in the interval of convergence.\nConsider a closed subinterval $\\vert x\\vert\\leq\\vert x_1\\vert\u0026lt;R$ containing $x_0$ in its interior. If $\\epsilon\u0026gt;0$ is given, then by uniform convergence we can find an $n$ such that $\\vert R_n(x)\\vert\u0026lt;\\epsilon$ for all $x$\u0026rsquo;s in the subinterval.\nSince the polynomial $S_n(x)$ is continuous at $x_0$, we can find $\\delta\u0026gt;0$ small that $\\vert x-x_0\\vert\u0026lt;\\delta$ implies $x$ lies in the subinterval and $\\vert S_n(x)-S_n(x_0)\\vert\u0026lt;\\epsilon$. Putting these conditions together we find that $\\vert x-x_0\\vert\u0026lt;\\delta$ implies \\begin{align} \\vert f(x)-f(x_0)\\vert\u0026amp;=\\left\\vert S_n(x)+R_n(x)-\\left(S_n(x_0)+R_n(x_0)\\right)\\right\\vert \\\\ \u0026amp;=\\left\\vert\\left(S_n(x)-S_n(x_0)\\right)+R_n(x)-R_n(x_0)\\right\\vert \\\\ \u0026amp;\\leq\\left\\vert S_n(x)-S_n(x_0)\\right\\vert+\\left\\vert R_n(x)\\right\\vert+\\left\\vert R_n(x_0)\\right\\vert \\\\ \u0026amp;\u0026lt;\\epsilon+\\epsilon+\\epsilon=3\\epsilon \\end{align} which proves the continuity of $f(x)$ at $x_0$.\nIntegrating term by term With what we have just proved that $f(x)=\\sum a_nx^n$ is continuous on $(-R,R)$, we can therefore integrate this function between $a$ and $b$ that lie inside the interval \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\int_{a}^{b}\\left(\\sum a_nx^n\\right)\\hspace{0.1cm}dx \\end{equation} We need to prove that the right side of this equation can be integrated term by term, which is \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\int_{a}^{b}\\left(\\sum a_nx^n\\right)\\hspace{0.1cm}dx=\\sum\\int_{a}^{b}a_nx^n\\hspace{0.1cm}dx\\tag{18}\\label{18} \\end{equation} In order to prove this, we begin by observing that $S_n(x)$ is a polynomial, and for that reason it is continuous. Thus, all there of the functions in \\begin{equation} f(x)=S_n(x)+R_n(x) \\end{equation} are continuous on $(-R,R)$. This allows us to write \\begin{equation} \\int_{a}^{b}f(x)\\hspace{0.1cm}dx=\\int_{a}^{b}S_n(x)\\hspace{0.1cm}dx+\\int_{a}^{b}R_n(x)\\hspace{0.1cm}dx \\end{equation} Moreover, we can integrate $S_n(x)$ term by term \\begin{align} \\int_{a}^{b}S_n(x)\\hspace{0.1cm}dx\u0026amp;=\\int_{a}^{b}\\left(a_0+a_1x+a_2x^2+\\ldots+a_nx^n\\right)\\hspace{0.1cm}dx \\\\ \u0026amp;=\\int_{a}^{b}a_0\\hspace{0.1cm}dx+\\int_{a}^{b}a_1x\\hspace{0.1cm}dx+\\int_{a}^{b}a_2x^2\\hspace{0.1cm}dx+\\ldots+\\int_{a}^{b}a_nx^n\\hspace{0.1cm}dx \\end{align} To prove \\eqref{18}, it therefore suffices to show that as $n\\to\\infty$ \\begin{equation} \\int_{a}^{b}R_n(x)\\hspace{0.1cm}dx\\to 0 \\end{equation} By uniform convergence, if $\\epsilon\u0026gt;0$ is given and $\\vert x\\vert\\leq\\vert x_1\\vert\u0026lt;R$ is a closed subinterval of $(-R,R)$ that contains both $a,b$, then $\\vert R_n(x)\\vert\u0026lt;\\epsilon$ for all $x$ in the subinterval and $n$ large enough. Hence, \\begin{equation} \\left\\vert\\int_{a}^{b}R_n(x)\\hspace{0.1cm}dx\\right\\vert\\leq\\int_{a}^{b}\\big\\vert R_n(x)\\big\\vert\\hspace{0.1cm}dx\u0026lt;\\epsilon\\vert b-a\\vert \\end{equation} for any $n$ large enough, which proves our statement.\nAs a special case of \\eqref{18}, we take the limits $0$ and $x$ instead of $a$ and $b$, and obtain \\begin{align} \\int_{a}^{b}f(t)\\hspace{0.1cm}dt\u0026amp;=\\sum\\dfrac{1}{n+1}a_nx^{n+1} \\\\ \u0026amp;=a_0x+\\dfrac{1}{2}a_1x^2+\\dfrac{1}{3}a_2x^3+\\ldots+\\dfrac{1}{n+1}a_nx^{n+1}+\\ldots\\tag{19}\\label{19} \\end{align}\nDifferentiating term by term We now prove that the function $f(x)$ is not only continuous but also differentiable on $(-R,R)$, and that its derivative can be calculated by differentiating term by term \\begin{equation} f\u0026rsquo;(x)=\\sum na_nx^{n-1} \\end{equation} It is easily seen that the series on right side of this equation is exact the series on the right side of \\eqref{3}, which is convergent on $(-R,R)$ as we proved. If we denote its sum by $g(x)$ \\begin{equation} g(x)=\\sum na_nx^{n-1}=a_1+2a_2x+3a_3x^2+\\ldots+na_nx^{n-1}+\\ldots, \\end{equation} then \\eqref{19} tells us that \\begin{align} \\int_{0}^{x}g(t)\\hspace{0.1cm}dt\u0026amp;=a_1x+a_2x^2+a_3x^3+\\ldots \\\\ \u0026amp;=f(x)-a_0 \\end{align} Since the left side of this has a derivative, so does the right side, and by differentiating we obtain \\begin{equation} f\u0026rsquo;(x)=g(x)=\\sum na_nx^{n-1} \\end{equation}\nReferences [1] George F.Simmons. Calculus With Analytic Geometry - 2nd Edition.\n[2] Marian M. A Concrete Approach to Classical Analysis.\n[3] MIT 18.01. Single Variable Calculus.\nFootnotes Theorem (L’Hospital)\nAssume $f$ and $g$ are real and differentiable on $]a,b[$ and $g\u0026rsquo;(x)\\neq 0$ for all $x\\in]a,b[$, where $-\\infty\\leq a\u0026lt;b\\leq\\infty$. Suppose as $x\\to a$, \\begin{equation} \\dfrac{f\u0026rsquo;(x)}{g\u0026rsquo;(x)}\\to A,(\\in[-\\infty,\\infty]) \\end{equation} If as $x\\to a$, $f(x)\\to 0$ and $g(x)\\to 0$ or if $g(x)\\to+\\infty$ as $x\\to a$, then \\begin{equation} \\dfrac{f(x)}{g(x)}\\to A \\end{equation} as $x\\to a$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nProof\nConsider the function $f(x)=a^x$.\nUsing the definition of the derivative, we have \\begin{align} \\dfrac{d}{dx}f(x)\u0026amp;=\\lim_{h\\to 0}\\dfrac{f(x+h)-f(x)}{h} \\\\ \u0026amp;=\\lim_{h\\to 0}\\dfrac{a^{x+h}-a^x}{h} \\\\ \u0026amp;=a^x\\lim_{h\\to 0}\\dfrac{a^h-1}{h} \\end{align} Therefore, \\begin{equation} \\lim_{h\\to 0}\\dfrac{a^h-1}{h}=1 \\end{equation} then, let $n=\\frac{1}{h}$, we have \\begin{equation} a=\\lim_{h\\to 0}\\left(1+\\dfrac{1}{h}\\right)^{1/h}=\\lim_{n\\to\\infty}\\left(1+\\dfrac{1}{n}\\right)^n={\\rm e} \\end{equation} Thus, $f(x)=a^x={\\rm e}^x$. Every function $y=c{\\rm e}^x$ also satisfies the differential equation $\\frac{dy}{dx}=y$, because \\begin{equation} \\dfrac{dy}{dx}=\\dfrac{d}{dx}c{\\rm e}^x=c\\dfrac{d}{dx}{\\rm e}^x=c{\\rm e}^x=y \\end{equation}\nThe rest of our proof is to prove that these are only functions that are unchanged by differentiation.\nTo prove this, suppose $f(x)$ is any function with that property. By the quotient rule, \\begin{equation} \\dfrac{d}{dx}\\dfrac{f(x)}{e^x}=\\dfrac{f\u0026rsquo;(x)e^x-e^x f(x)}{e^{2x}}=\\dfrac{e^x f(x)-e^x f(x)}{e^{2x}}=0 \\end{equation} which implies that \\begin{equation} \\dfrac{f(x)}{e^x}=c, \\end{equation} for some constant $c$, and so $f(x)=ce^x$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTheorem (Mean Value Theorem)\nIf a function $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable in the open interval $(a,b)$, then there exists at least one number $c$ between $a$ and $b$ with the property that \\begin{equation} f\u0026rsquo;(c)=\\frac{f(b)-f(a)}{b-a} \\end{equation}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/calculus/power-series/","summary":"\u003cblockquote\u003e\n\u003cp\u003eRecall that in the previous note, \u003ca href=\"https://trunghng.github.io/posts/calculus/infinite-series-of-constants/\"\u003eInfinite Series of Constants\u003c/a\u003e, we mentioned a type of series called \u003cstrong\u003epower series\u003c/strong\u003e a lot. In the content of this note, we will be diving deeper into details of its.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Power Series"},{"content":" Notes on infinite series of constants.\nInfinite Series An infinite series, or simply a series, is an expression of the form \\begin{equation} a_1+a_2+\\dots+a_n+\\dots=\\sum_{n=1}^{\\infty}a_n \\end{equation}\nExamples Infinite decimal. \\begin{equation} .a_1a_2\\ldots a_n\\ldots=\\dfrac{a_1}{10}+\\dfrac{a_2}{10^2}+\\ldots+\\dfrac{a_n}{10^n}+\\ldots, \\end{equation} where $a_i\\in\\\\{0,1,\\dots,9\\\\}$. Power series expansion Geometric series. \\begin{equation} \\dfrac{1}{1-x}=\\sum_{n=0}^{\\infty}x^n=1+x+x^2+x^3+\\dots,\\hspace{1cm}\\vert x\\vert\u003c1 \\end{equation} Exponential function. \\begin{equation} {\\rm e}^x=\\sum_{n=0}^{\\infty}\\dfrac{x^n}{n!}=1+x+\\dfrac{x^2}{2!}+\\dfrac{x^3}{3!}+\\ldots \\end{equation} Sine and cosine formulas. \\begin{align} \\sin x\u0026=\\sum_{n=0}^{\\infty}\\dfrac{(-1)^n x^{2n+1}}{(2n+1)!}=x-\\dfrac{x^3}{3!}+\\dfrac{x^5}{5!}-\\dfrac{x^7}{7!}+\\ldots \\\\ \\cos x\u0026=\\sum_{n=0}^{\\infty}\\dfrac{(-1)^n x^{2n}}{(2n)!}=1-\\dfrac{x^2}{2!}+\\dfrac{x^4}{4!}-\\dfrac{x^6}{6!}+\\ldots \\end{align} Convergent Sequences Sequences If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$\u0026rsquo;s are said to form a sequence (denoted as $\\{x_n\\}$) \\begin{equation} x_1,x_2,\\dots,x_n,\\dots \\end{equation} We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.\nA sequence $\\{x_n\\}$ is said to be bounded if there exists $A, B$ such that $A\\leq x_n\\leq B, \\forall n$. $A, B$ respectively are called lower bound, upper bound of the sequence. A sequence that is not bounded is said to be unbounded.\nLimits of Sequences A sequence $\\{x_n\\}$ is said to have a number $L$ as limit if for each $\\epsilon\u0026gt;0$, there exists a positive integer $n_0$ that \\begin{equation} \\vert x_n-L\\vert\u0026lt;\\epsilon\\hspace{1cm}n\\geq n_0 \\end{equation} We say that $x_n$ converges to $L$ as $n$ approaches infinite ($x_n\\to L$ as $n\\to\\infty$) and denote this as \\begin{equation} \\lim_{n\\to\\infty}x_n=L \\end{equation}\nA sequence is said to converge or to be convergent if it has a limit. A convergent sequence is bounded, but not all bounded sequences are convergent. If $x_n\\to L,y_n\\to M$, then \\begin{align} \u0026amp;\\lim(x_n+y_n)=L+M \\\\ \u0026amp;\\lim(x_n-y_n)=L-M \\\\ \u0026amp;\\lim x_n y_n=LM \\\\ \u0026amp;\\lim\\dfrac{x_n}{y_n}=\\dfrac{L}{M}\\hspace{1cm}M\\neq0 \\end{align} An increasing (or decreasing) sequence converges if and only if it is bounded. Convergent and Divergent Series Recall from the previous sections that if $a_1,a_2,\\dots,a_n,\\dots$ is a sequence of numbers, then \\begin{equation} \\sum_{n=1}^{\\infty}a_n=a_1+a_2+\\ldots+a_n+\\ldots\\tag{1}\\label{1} \\end{equation} is called an infinite series. We begin by establishing the sequence of partial sums \\begin{align} s_1\u0026amp;=a_1 \\\\ s_2\u0026amp;=a_1+a_2 \\\\ \u0026amp;\\hspace{0.1cm}\\vdots \\\\ s_n\u0026amp;=a_1+a_2+\\dots+a_n \\\\ \u0026amp;\\hspace{0.1cm}\\vdots \\end{align} The series \\eqref{1} is said to be convergent if the sequences $\\{s_n\\}$ converges. And if $\\lim s_n=s$, then we say that \\eqref{1} converges to $s$, or that $s$ is the sum of the series. \\begin{equation} \\sum_{n=1}^{\\infty}a_n=s \\end{equation} If the series does not converge, we say that it diverges or is divergent, and no sum is assigned to it.\nExamples (harmonic series)\nLet\u0026rsquo;s consider the convergence of harmonic series \\begin{equation} \\sum_{n=1}^{\\infty}\\frac{1}{n}=1+\\frac{1}{2}+\\frac{1}{3}+\\ldots\\tag{2}\\label{2} \\end{equation} Let $m$ be a positive integer and choose $n\u0026gt;2^{m+1}$. We have \\begin{align} s_n\u0026amp;\u0026gt;1+\\frac{1}{2}+\\frac{1}{3}+\\frac{1}{4}+\\dots+\\frac{1}{2^{m+1}} \\\\ \u0026amp;=\\left(1+\\frac{1}{2}\\right)+\\left(\\frac{1}{3}+\\frac{1}{4}\\right)+\\left(\\frac{1}{5}+\\ldots+\\frac{1}{8}\\right)+\\ldots+\\left(\\frac{1}{2^m+1}+\\ldots+\\frac{1}{2^{m+1}}\\right) \\\\ \u0026amp;\u0026gt;\\frac{1}{2}+2.\\frac{1}{4}+4.\\frac{1}{8}+\\ldots+2^m.\\frac{1}{2^{m+1}} \\\\ \u0026amp;=(m+1)\\frac{1}{2} \\end{align} This proves that $s_n$ can be made larger than the sum of any number of $\\frac{1}{2}$\u0026rsquo;s and therefore as large as we please, by taking $n$ large enough, so the $\\{s_n\\}$ are unbounded, which leads to that \\eqref{2} is a divergent series. \\begin{equation} \\sum_{n=1}^{\\infty}\\frac{1}{n}=1+\\frac{1}{2}+\\frac{1}{3}+\\ldots=\\infty \\end{equation}\nThe simplest general principle that is useful to study the convergence of a series is the $\\mathbf{n}$-th term test.\n$\\mathbf{n}$-th term test If the series $\\{a_n\\}$ converges, then $a_n\\to 0$ as $n\\to\\infty$; or equivalently, if $\\neg(a_n\\to0)$ as $n\\to\\infty$, then the series must necessarily diverge.\nProof\nWhen $\\{a_n\\}$ converges, as $n\\to\\infty$ we have \\begin{equation} a_n=s_n-s_{n-1}\\to s-s=0 \\end{equation} This result shows that $a_n\\to 0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e. it does not imply the convergence of the series when $a_n\\to 0$ as $n\\to\\infty$.\nGeneral Properties of Convergent Series Any finite number of $0$'s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges). When two convergent series are added term by term, the resulting series converges to the expected sum; i.e. if $\\sum_{n=1}^{\\infty}a_n=s$ and $\\sum_{n=1}^{\\infty}b_n=t$, then \\begin{equation} \\sum_{n=1}^{\\infty}(a_n+b_n)=s+t \\end{equation} Proof\nLet $\\{s_n\\}$ and $\\{t_n\\}$ respectively be the sequences of partial sums of $\\sum_{n=1}^{\\infty}a_n$ and $\\sum_{n=1}^{\\infty}b_n$. As $n\\to\\infty$ we have \\begin{align} (a_1+b_1)+(a_2+b_2)+\\dots+(a_n+b_n)\u0026=\\sum_{i=1}^{n}a_i+\\sum_{i=1}^{n}b_i \\\\ \u0026=s_n+t_n\\to s+t \\end{align} Similarly, $\\sum_{n=1}^{\\infty}(a_n-b_n)=s-t$ and $\\sum_{n=1}^{\\infty}ca_n=cs$ for any constant $c$. Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way. Proof\nIf $\\sum_{n=1}^{\\infty}a_n=s$, then \\begin{equation} \\lim_{n\\to\\infty}(a_0+a_1+a_2+\\dots+a_n)=\\lim_{n\\to\\infty} a_0+\\lim_{n\\to\\infty}(a_1+a_2+\\dots+a_n)=a_0+s \\end{equation} Series of Nonnegative terms. Comparison Tests The easiest infinite series to work with are those whose terms are all nonnegative numbers. The reason, as we saw in the above section, is that if $a_n\\geq0$, then the series $\\sum a_n$ converges if and only if its sequence $\\{s_n\\}$ of partial sums is bounded (since $s_{n+1}=s_n+a_{n+1}$).\nThus, in order to establish the convergence of a series of nonnegative terms, it suffices to show that its terms approach zero fast enough, or at least as fast as the terms of a known convergent series of nonnegative terms to keep the partial sums bounded.\nComparison test If $0\\leq a_n\\leq b_n$, then\n$\\sum a_n$ converges if $\\sum b_n$ converges. $\\sum b_n$ diverges if $\\sum a_n$ diverges. Proof\nIf $s_n, t_n$ respectively are the partial sums of $\\sum a_n,\\sum b_n$, then \\begin{equation} 0\\leq s_n=\\sum_{i=1}^{n}a_i\\leq\\sum_{i=1}^{n}b_i=t_n \\end{equation} Then if $\\{t_n\\}$ is bounded, then so is $\\{s_n\\}$; and if $\\{s_n\\}$ is unbounded, then so is $\\{t_n\\}$.\nExample\nConsider convergence behavior of two series \\begin{equation} \\sum_{n=1}^{\\infty}\\frac{1}{2^n+1};\\hspace{2cm}\\sum_{n=1}^{\\infty}\\frac{1}{\\ln n} \\end{equation} The first series converges, because \\begin{equation} \\frac{1}{2^n+1}\u0026lt;\\frac{1}{2^n} \\end{equation} and $\\sum_{n=1}^{\\infty}\\frac{1}{2^n}=1$, which is a convergent series. At the same time, the second series diverges, since \\begin{equation} \\frac{1}{n}\\leq\\frac{1}{\\ln n} \\end{equation} and $\\sum_{n=1}^{\\infty}\\frac{1}{n}$ diverges.\nOne thing worth remarking is that the condition $0\\leq a_n\\leq b_n$ for the comparison test need not hold for all $n$, but only for all $n$ from some point on.\nThe comparison test is simple, but in some cases where it is difficult to establish the necessary inequality between the $n$-th terms of the two series. And since limits are often easier to work with than inequalities, we have the following test.\nLimit comparison test If $\\sum a_n,\\sum b_n$ are series with positive terms such that \\begin{equation} \\lim_{n\\to\\infty}\\frac{a_n}{b_n}=1\\tag{3}\\label{3} \\end{equation} then either both series converge or both series diverge.\nProof\nwe observe that \\eqref{3} implies that for all sufficient large $n$, we have \\begin{align} \\frac{1}{2}\u0026amp;\\leq\\frac{a_n}{b_n}\\leq 2 \\\\ \\text{or}\\hspace{1cm}\\frac{1}{2}b_n\u0026amp;\\leq a_n\\leq 2b_n \\end{align} which leads to the fact that $\\sum a_n$ and $\\sum b_n$ have the same convergence behavior.\nThe condition \\eqref{3} can be generalized by \\begin{equation} \\lim_{n\\to\\infty}\\frac{a_n}{b_n}=L, \\end{equation} where $0\u0026lt;L\u0026lt;\\infty$.\nExample ($p$-series)\nConsider the convergence behavior of the series \\begin{equation} \\sum_{n=1}^{\\infty}\\dfrac{1}{n^p}=1+\\dfrac{1}{2^p}+\\dfrac{1}{3^p}+\\dfrac{1}{4^p}+\\ldots,\\tag{4}\\label{4} \\end{equation} where $p$ is a positive constant.\nIf $p\\leq 1$, then $n^p\\leq n$ or $\\frac{1}{n}\\leq\\frac{1}{n^p}$. Thus, by comparison with the harmonic series $\\sum\\frac{1}{n}$, we have that \\eqref{4} diverges.\nIf $p\u0026gt;1$, let $n$ be given and choose $m$ so that $n\u0026lt;2^m$. Then \\begin{align} s_n\u0026amp;\\leq s_{2^m-1} \\\\ \u0026amp;=1+\\left(\\dfrac{1}{2^p}+\\dfrac{1}{3^p}\\right)+\\left(\\dfrac{1}{4^p}+\\ldots+\\dfrac{1}{7^p}\\right)+\\ldots+\\left[\\dfrac{1}{(2^{m-1})^p}+\\ldots+\\dfrac{1}{(2^m-1)^p}\\right] \\\\ \u0026amp;\\leq 1+\\dfrac{2}{2^p}+\\dfrac{4}{4^p}+\\ldots+\\dfrac{2^{m-1}}{(2^{m-1})^p} \\end{align} Let $a=\\frac{1}{2^{p-1}}$, then $a\u0026lt;1$ since $p\u0026gt;1$, and \\begin{equation} s_n\\leq 1+a+a^2+\\ldots+a^{m-1}=\\dfrac{1-a^m}{1-a}\u0026lt;\\dfrac{1}{1-a} \\end{equation} which proves that $\\{s_n\\}$ has an upper bound. Thus \\eqref{4} converges.\nTheorem 1\nIf a convergent series of nonnegative terms is rearranged in any manner, then the resulting series also converges and has the same sum.\nProof\nConsider two series $\\sum a_n$ and $\\sum b_n$, where $\\sum a_n$ is a convergent series of nonnegative terms and $\\sum b_n$ is formed form $\\sum a_n$ by rearranging its terms.\nLet $p$ be a positive integer and consider the $p$-partial sum of $\\sum b_n$, i.e. \\begin{equation} t_p=b_1+\\ldots+b_p \\end{equation} Since each $b$ is some $a$, then there exists an $m$ such that each term in $t_p$ is one of the terms in $s_m=a_1+\\ldots+a_m$. This shows us that $t_p\\leq s_m\\leq s$. Thus, $\\sum b_n$ converges to a sum $t\\leq s$.\nOn the other hand, $\\sum a_n$ is also a rearrangement of $\\sum b_n$, so by the same procedure, similarly we have that $s\\leq t$, and therefore $t=s$.\nThe Integral test. Euler\u0026rsquo;s constant In this section, we will be going through a more detailed class of infinite series with nonnegative terms which is those whose terms form a decreasing sequence of positive numbers.\nWe begin by considering a series \\begin{equation} \\sum_{n=1}^{\\infty}a_n=a_1+a_2+\\ldots+a_n+\\ldots \\end{equation} whose terms are positive and decreasing. Suppose $a_n=f(n)$, as shown is Figure 1.\nFigure 1 On the left of this figure we see that the rectangles of areas $a_1,a_2,\\dots,a_n$ have a greater combined area than the area under the curve from $x=1$ to $x=n+1$, so \\begin{equation} a_1+a_2+\\dots+a_n\\geq\\int_{1}^{n+1}f(x)\\hspace{0.1cm}dx\\geq\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\tag{5}\\label{5} \\end{equation} On the right side of the figure, the rectangles lie under the curve, which makes \\begin{align} a_2+a_3+\\dots+a_n\u0026amp;\\leq\\int_{1}^{n}f(x)\\hspace{0.1cm}dx \\\\ a_1+a_2+\\dots+a_n\u0026amp;\\leq a_1+\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\tag{6}\\label{6} \\end{align} Putting \\eqref{5} and \\eqref{6} together we have \\begin{equation} \\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\leq a_1+a_2+\\dots+a_n\\leq a_1+\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\tag{7}\\label{7} \\end{equation} The result we obtained in \\eqref{7} allows us to establish the integral test.\nIntegral test If $f(x)$ is a positive decreasing function for $x\\geq1$ such that $f(n)=a_n$ for each positive integer $n$, then the series and integral \\begin{equation} \\sum_{n=1}^{\\infty}a_n;\\hspace{2cm}\\int_{1}^{\\infty}f(x)\\hspace{0.1cm}dx \\end{equation} converge or diverge together.\nThe integral test holds for any interval of the form $x\\geq k$, not just for $x\\geq 1$.\nExample (Abel\u0026rsquo;s series)\nLet\u0026rsquo;s consider the convergence behavior of the series \\begin{equation} \\sum_{n=2}^{\\infty}\\frac{1}{n\\ln n}\\tag{8}\\label{8} \\end{equation} By the integral test, we have that \\eqref{8} diverges, because \\begin{equation} \\sum_{2}^{\\infty}\\frac{dx}{x\\ln x}=\\lim_{b\\to\\infty}\\int_{2}^{b}\\frac{dx}{x\\ln x}=\\lim_{b\\to\\infty}\\left(\\ln\\ln x\\Big|_{2}^{b}\\right)=\\lim_{b\\to\\infty}\\left(\\ln\\ln b-\\ln\\ln 2\\right)=\\infty \\end{equation} More generally, if $p\u0026gt;0$, then \\begin{equation} \\sum_{n=2}^{\\infty}\\frac{1}{n(\\ln n)^p} \\end{equation} converges if $p\u0026gt;1$ and diverges if $0\u0026lt;p\\leq 1$. For if $p\\neq 1$, we have \\begin{align} \\int_{2}^{\\infty}\\frac{dx}{x(\\ln x)^p}\u0026amp;=\\lim_{b\\to\\infty}\\int_{2}^{b}\\frac{dx}{x(\\ln x)^p} \\\\ \u0026amp;=\\lim_{b\\to\\infty}\\left[\\dfrac{(\\ln x)^{1-p}}{1-p}\\Bigg|_2^b\\right] \\\\ \u0026amp;=\\lim_{b\\to\\infty}\\left[\\dfrac{(\\ln b)^{1-p}-(\\ln 2)^{1-p}}{1-p}\\right] \\end{align} exists if and only if $p\u0026gt;1$.\nEuler\u0026rsquo;s constant From \\eqref{7} we have that \\begin{equation} 0\\leq a_1+a_2+\\ldots+a_n-\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\leq a_1 \\end{equation} Denoting $F(n)=a_1+a_2+\\ldots+a_n-\\int_{1}^{n}f(x)\\hspace{0.1cm}dx$, the above expression becomes \\begin{equation} 0\\leq F(n)\\leq a_1 \\end{equation} Moreover, $\\{F(n)\\}$ is a decreasing sequence, because \\begin{align} \u0026amp;F(n)-F(n+1) \\\\ \u0026amp;=\\left[a_1+a_2+\\ldots+a_n-\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\right]-\\left[a_1+a_2+\\ldots+a_{n+1}-\\int_{1}^{n+1}f(x)\\hspace{0.1cm}dx\\right] \\\\ \u0026amp;=\\int_{n}^{n+1}f(x)\\hspace{0.1cm}dx-a_{n+1}\\geq 0, \\end{align} where the last step can be seen by observing the right side of Figure 1.\nSince any decreasing sequence of nonnegative numbers converges, we have that \\begin{equation} L=\\lim_{n\\to\\infty}F(n)=\\lim_{n\\to\\infty}\\left[a_1+a_2+\\ldots+a_n-\\int_{1}^{n}f(x)\\hspace{0.1cm}dx\\right]\\tag{9}\\label{9} \\end{equation} exists and satisfies the inequalities $0\\leq L\\leq a_1$.\nLet $a_n=\\frac{1}{n}$ and $f(x)=\\frac{1}{x}$, the last quantity in \\eqref{9} becomes \\begin{equation} \\lim_{n\\to\\infty}\\left(1+\\dfrac{1}{2}+\\ldots+\\dfrac{1}{n}-\\ln n\\right)\\tag{10}\\label{10} \\end{equation} since \\begin{equation} \\int_{1}^{n}\\dfrac{dx}{x}=\\ln x\\Big|_1^n=\\ln n \\end{equation} The value of the limit \\eqref{10} is called Euler\u0026rsquo;s constant (denoted as $\\gamma$). \\begin{equation} \\gamma=\\lim_{n\\to\\infty}\\left(1+\\dfrac{1}{2}+\\ldots+\\dfrac{1}{n}-\\ln n\\right) \\end{equation}\nThe Ratio test. Root test Ratio test If $\\sum a_n$ is a series of positive terms such that \\begin{equation} \\lim_{n\\to\\infty}\\dfrac{a_{n+1}}{a_n}=L,\\tag{11}\\label{11} \\end{equation} then\nIf $L\u003c1$, the series converges. If $L\u003e1$, the series diverges. If $L=1$, the test is inconclusive. Proof\nLet $L\u003c1$ and choose any number $r$ such that $L\\lt r\\lt 1$. From \\eqref{11}, we have that there exists an $n_0$ such that \\begin{align} \\dfrac{a_{n+1}}{a_n}\u0026\\leq r=\\dfrac{r^{n+1}}{r_n},\\hspace{1cm}\\forall n\\geq n_0 \\\\ \\dfrac{a_{n+1}}{r^{n+1}}\u0026\\leq\\dfrac{a_n}{r^n},\\hspace{2cm}\\forall n\\geq n_0 \\end{align} which means that $\\\\{\\frac{a_n}{r^n}\\\\}$ is a decreasing sequence for $n\\geq n_0$; in particular, $\\frac{a_n}{r^n}\\leq\\frac{a_{n_0}}{r^{n_0}}$ for $n\\geq n_0$. Thus, if we let $K=\\frac{a_{n_0}}{r^{n_0}}$, then we get \\begin{equation} a_n\\leq Kr^n,\\hspace{1cm}\\forall n\\geq n_0\\tag{12}\\label{12} \\end{equation} However, $\\sum Kr^n$ converges since $r\u003c1$. Hence, by the comparison test, \\eqref{12} implies that $\\sum a_n$ converges. When $L\u003e1$, we have that $\\frac{a_{n+1}}{a_n}\\geq 1$, or equivalently $a_{n+1}\\geq a_n$, for all $n\\geq n_0$, for some constant $n_0$. That means $\\neg(a_n\\to 0)$ as $n\\to\\infty$ (since $\\sum a_n$ is a series of positive terms). By the $n$-th term test, it follows that the series diverges. Consider the $p$-series $\\sum\\frac{1}{n^p}$. For all values of $p$, as $n\\to\\infty$ we have \\begin{equation} \\dfrac{a_{n+1}}{a_n}=\\dfrac{n^p}{(n+1)^p}=\\left(\\dfrac{n}{n+1}\\right)^p\\to 1 \\end{equation} As in the above example, we have that this series converges if $p\u003e1$ and diverges if $p\\leq 1$. Root test If $\\sum a_n$ is a series of nonnegative terms such that \\begin{equation} \\lim_{n\\to\\infty}\\sqrt[n]{a_n}=L,\\tag{13}\\label{13} \\end{equation} then\nIf $L\u003c1$, the series converges. If $L\u003e1$, the series diverges. If $L=1$, the test is inconclusive. Proof\nLet $L\u003c1$ and $r$ is any number such that $L\\lt r\\lt 1$. From \\eqref{13}, we have that there exist $n_0$ such that \\begin{align} \\sqrt[n]{a_n}\u0026\\leq r\u003c1,\\hspace{1cm}\\forall n\\geq n_0 \\\\ a_n\u0026\\leq r^n\u003e1,\\hspace{1cm}\\forall n\\geq n_0 \\end{align} And since the geometric series $\\sum r^n$ converges, we clearly have that $\\sum a_n$ also converges. If $L\u003e1$, then $\\sqrt[n]{a_n}\\geq 1$ for all $n\\geq n_0$, for some $n_0$, so $a_n\\geq 1$ for all $n\\geq n_0$. That means as $n\\to\\infty$, $\\neg(a_n\\to 0)$. Therefore, by the $n$-th term test, we have that the series diverges. For $L=1$, we provide 2 examples. One is the divergent series $\\sum\\frac{1}{n}$ and the other is the convergent series $\\sum\\frac{1}{n^2}$ (since $\\sqrt[n]{n}\\to 1$ as $n\\to\\infty$). The Extended Ratio tests of Raabe and Gauss Kummer\u0026rsquo;s theorem Theorem 2 (Kummer\u0026rsquo;s)\nAssume that $a_n\u0026gt;0,b_n\u0026gt;0$ and $\\sum\\frac{1}{b_n}$ diverges. If \\begin{equation} \\lim\\left(b_n-\\dfrac{a_{n+1}}{a_n}.b_{n+1}\\right)=L,\\tag{14}\\label{14} \\end{equation} then $\\sum a_n$ converges if $L\u0026gt;0$ and diverges if $L\u0026lt;0$.\nProof\nIf $L\u003e0$, then there exists $h$ such that $L\u003eh\u003e0$. From \\eqref{14}, for some positive integer $n_0$ we have \\begin{align} b_n-\\dfrac{a_{n+1}}{a_n}.b_{n+1}\u0026\\geq h\u003e0,\\hspace{1cm}\\forall n\\geq n_0 \\\\ a_n b_n-a_{n+1}b_{n+1}\u0026\\geq ha_n\u003e0,\\hspace{1cm}\\forall n\\geq n_0\\tag{15}\\label{15} \\end{align} Hence, $\\{a_n b_n\\}$ is a decreasing sequence of positive numbers for $n\\geq n_0$, so $K=\\lim a_n b_n$ exists.\nMoreover, we have that \\begin{equation} \\sum_{n=n_0}^{\\infty}a_nb_n-a_{n+1}b_{n+1}=a_{n_0}b_{n_0}-\\lim_{n\\to\\infty}a_nb_n=a_{n_0}b_{n_0}-K \\end{equation} Therefore, by \\eqref{15} and comparison test, we can conclude that $\\sum ha_n$ converges, which means that $\\sum a_n$ also converges. If $L\u003c0$, for some positive integer $n_0$ we have \\begin{equation} a_nb_n-a_{n+1}b_{n+1}\\leq 0,\\hspace{1cm}\\forall n\\geq n_0 \\end{equation} Hence, $\\{a_nb_n\\}$ is a increasing sequence of positive number for all $n\\geq n_0$, for some positive integer $n_0$. This also means for all $n\\geq n_0$, \\begin{align} a_nb_n\u0026\\geq a_{n_0}b_{n_0} \\\\ a_n\u0026\\geq (a_{n_0}b_{n_0}).\\dfrac{1}{b_n} \\end{align} Therefore, $\\sum a_n$ diverges (since $\\sum\\frac{1}{b_n}$ diverges). Raabe\u0026rsquo;s test Theorem 3 (Raabe\u0026rsquo;s test)\nIf $a_n\u0026gt;0$ and \\begin{equation} \\dfrac{a_{n+1}}{a_n}=1-\\dfrac{A}{n}+\\dfrac{A_n}{n}, \\end{equation} where $A_n\\to 0$, then $\\sum a_n$ converges if $A\u0026gt;1$ and diverges if $A\u0026lt;1$.\nProof\nTake $n=b_n$ in Kummber\u0026rsquo;s theorem. Then \\begin{align} \\lim\\left(b_n-\\dfrac{a_{n+1}}{a_n}.b_{n+1}\\right)\u0026amp;=\\lim\\left[n-\\left(1-\\dfrac{A}{n}+\\dfrac{A_n}{n}\\right)(n+1)\\right] \\\\ \u0026amp;=\\lim\\left[-1+\\dfrac{A(n+1)}{n}-\\dfrac{A_n(n+1)}{n}\\right] \\\\ \u0026amp;=A-1 \\end{align} and by Kummer\u0026rsquo;s theorem we have that $\\sum a_n$ converges if $A\u0026gt;1$ and diverges if $A\u0026lt;1$.\nRaabe\u0026rsquo;s test can be formulated as followed: If $a_n\u0026gt;0$ and \\begin{equation} \\lim n\\left(1-\\dfrac{a_{n+1}}{a_n}\\right)=A, \\end{equation} then $\\sum a_n$ converges if $A\u0026gt;1$ and diverges if $A\u0026lt;1$.\nWhen $A=1$ in Raabe\u0026rsquo;s test, we turn to Gauss\u0026rsquo;s test\nGauss\u0026rsquo;s test Theorem 4\nIf $a_n\u0026gt;0$ and \\begin{equation} \\dfrac{a_{n+1}}{a_n}=1-\\dfrac{A}{n}+\\dfrac{A_n}{n^{1+c}}, \\end{equation} where $c\u0026gt;0$ and $A_n$ is bounded as $n\\to\\infty$, then $\\sum a_n$ converges if $A\u0026gt;1$ and diverges if $A\\leq 1$.\nProof\nIf $A\\neq 1$, the statement follows exactly from Raabe's test, since $\\frac{A_n}{n^c}\\to 0$ as $n\\to\\infty$. If $A=1$, we begin by taking $b_n=n\\ln n$ in *Kummer's theorem*. Then \\begin{align} \\lim\\left(b_n-\\dfrac{a_{n+1}}{a_n}.b_{n+1}\\right)\u0026=\\lim\\left[n\\ln n-\\left(1-\\dfrac{1}{n}+\\dfrac{A_n}{n^{1+c}}\\right)(n+1)\\ln(n+1)\\right] \\\\\\\\ \u0026=\\lim\\left[n\\ln n-\\dfrac{n^2-1}{n}\\ln(n+1)-\\dfrac{n+1}{n}.\\dfrac{A_n\\ln(n+1)}{n^c}\\right] \\\\\\\\ \u0026=\\lim\\left[n\\ln\\left(\\dfrac{n}{n+1}\\right)+\\dfrac{\\ln(n+1)}{n}-\\dfrac{n+1}{n}.\\dfrac{A_n\\ln(n+1)}{n^c}\\right] \\\\\\\\ \u0026=-1+0-0=-1\u003c0, \\end{align} which, by Kummer's theorem, we have that the series is divergent. In the fourth step we have used the Stolz–Cesàro theorem1.\nTheorem 5 (Gauss\u0026rsquo;s test)\nIf $a_n\u0026gt;0$ and \\begin{equation} \\dfrac{a_{n+1}}{a_n}=\\dfrac{n^k+\\alpha n^{k-1}+\\ldots}{n^k+\\beta n^{k-1}+\\ldots},\\tag{16}\\label{16} \\end{equation} then $\\sum a_n$ converges if $\\beta-\\alpha\u0026gt;1$ and diverges if $\\beta-\\alpha\\leq 1$.\nProof\nIf the quotient on the right of \\eqref{16} is worked out by long division, we get \\begin{equation} \\dfrac{a_{n+1}}{a_n}=1-\\dfrac{\\beta-\\alpha}{n}+\\dfrac{A_n}{n^2}, \\end{equation} where $A_n$ is a quotient of the form \\begin{equation} \\dfrac{\\gamma n^{k-2}+\\ldots}{n^{k-2}+\\ldots} \\end{equation} and is therefore clearly bounded as $n\\to\\infty$. The statement now follows from Theorem 4 with $c=1$.\nThe Alternating Series test. Absolute Convergence Previously, we have been working with series of positive terms and nonnegative terms. It\u0026rsquo;s time to consider series with both positive and negative terms. The simplest are those whose terms are alternatively positive and negative.\nAlternating Series Alternating series is series with the form \\begin{equation} \\sum_{n=1}^{\\infty}(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+\\ldots,\\tag{17}\\label{17} \\end{equation} where $a_n$\u0026rsquo;s are all positive numbers.\nFrom the definition of alternating series, we establish alternating series test.\nAlternating Series test If the alternating series \\eqref{17} has the property that\n$a_1\\geq a_2\\geq a_3\\geq\\ldots$ $a_n\\to 0$ as $n\\to\\infty$ then $\\sum a_n$ converges. Proof\nOn the one hand, we have that a typical even partial sum $s_{2n}$ can be written as \\begin{equation} s_{2n}=(a_1-a_2)+(a_3-a_4)+\\ldots+(a_{2n-1}-a_{2n}), \\end{equation} where each expression in parentheses is nonnegative since $\\{a_n\\}$ is a decreasing sequence. Hence, we also have that $s_{2n}\\leq s_{2n+2}$, which leads to the result that the even partial sums form an increasing sequence.\nMoreover, we can also display $s_{2n}$ as \\begin{equation} s_{2n}=a_1-(a_2-a_3)-(a_4-a_5)-\\ldots-(a_{2n-2}-a_{2n-1})-a_{2n}, \\end{equation} where each expression in parentheses once again is nonnegative. Thus, we have that $s_{2n}\\leq a_1$, so ${s_{2n}}$ has an upper bound. Since every bounded increasing sequence converges, there exists a number $s$ such that \\begin{equation} \\lim_{n\\to\\infty}s_{2n}=s \\end{equation}\nOn the other hand, the odd partial sums approach the same limit, because \\begin{align} s_{2n+1}\u0026amp;=a_1-a_2+a_3-a_4+\\ldots-a_{2n}+a_{2n+1} \\\\ \u0026amp;=s_{2n}+a_{2n+1} \\end{align} and therefore \\begin{equation} \\lim_{n\\to\\infty}s_{2n+1}=\\lim_{n\\to\\infty}s_{2n}+\\lim_{n\\to\\infty}a_{2n+1}=s+0=s \\end{equation} Since both sequence of even sums and sequence of odd partial sums converges to $s$ as $n$ tends to infinity, this shows us that $\\{s_n\\}$ also converges to $s$, and therefore the alternating series \\eqref{17} converges to the sum $s$.\nAbsolute Convergence A series $\\sum a_n$ is said to be absolutely convergent if $\\sum\\vert a_n\\vert$ converges.\nThese are some properties of absolute convergence.\nAbsolute convergence implies convergence.\nProof\nSuppose that $\\sum a_n$ is an absolutely convergent series, or $\\sum\\vert a_n\\vert$ converges. We have that \\begin{equation} 0\\leq a_n+\\vert a_n\\vert\\leq 2\\vert a_n\\vert \\end{equation} And since $\\sum 2\\vert a_n\\vert$ converges, by comparison test, we also have that $\\sum(a_n+\\vert a_n\\vert)$ converges. Since both $\\sum\\vert a_n\\vert$ and $\\sum(a_n+\\vert a_n\\vert)$ converge, so does their difference, which is $\\sum a_n$. A convergent series that is not absolutely convergent is said to be conditionally convergent.\n- Any conditionally convergent series can be made to converge to any given number as its sum, or even to diverge, by suitably changing the order of its terms without changing the terms themselves (check out Theorem 8 to see the proof).\n- On the other hand, any absolutely convergent series can be rearranged in any manner without changing its convergence behavior or its sum (check out Theorem 7 to see the proof). Absolute vs Conditionally Convergence Theorem 6\nConsider a series $\\sum a_n$ and define $p_n$ and $q_n$ by \\begin{equation} p_n=\\dfrac{\\vert a_n\\vert+a_n}{2},\\hspace{2cm}q_n=\\dfrac{\\vert a_n\\vert-a_n}{2} \\end{equation} Then\nIf $\\sum a_n$ converges conditionally, then both $\\sum p_n$ and $\\sum q_n$ diverges. If $\\sum a_n$ converges absolutely, then $\\sum p_n$ and $\\sum q_n$ both converge and the sums of these series are related by the equation \\begin{equation} \\sum a_n=\\sum p_n-\\sum q_n \\end{equation} Proof\nFrom the formulas of $p_n$ and $q_n$, we have \\begin{align} a_n\u0026amp;=p_n-q_n\\tag{18}\\label{18} \\\\ \\vert a_n\\vert\u0026amp;=p_n+q_n\\tag{19}\\label{19} \\end{align} We begin by proving the first statement.\nWhen $\\sum a_n$ converges, from \\eqref{18}, we have $\\sum p_n$ and $\\sum q_n$ both must have the same convergence behavior (i.e. converge or diverge at the same time).\nIf they both converge, then from \\eqref{19}, we have that $\\sum\\vert a_n\\vert$ converges, contrary to the hypothesis, so $\\sum p_n$ and $\\sum q_n$ are both divergent. To prove the second statement, we assume that $\\sum\\vert a_n\\vert$ converges. We have \\begin{equation} p_n=\\dfrac{\\vert a_n\\vert+a_n}{2}\\leq\\dfrac{2\\vert a_n\\vert}{2}=\\vert a_n\\vert \\end{equation} which shows us that $\\sum p_n$ converges. Similarly, for $q_n$, we have \\begin{equation} q_n=\\dfrac{\\vert a_n\\vert-a_n}{2}\\leq\\dfrac{2\\vert a_n\\vert}{2}=\\vert a_n\\vert \\end{equation} which also lets us obtain that $\\sum q_n$ converges. Hence \\begin{equation} \\sum p_n-\\sum q_n=\\sum(p_n-q_n)=\\sum a_n \\end{equation} Theorem 7\nIf $\\sum a_n$ is an absolutely convergent series with sum $s$, and if $a_n$\u0026rsquo;s are rearranged in any way to from a new series $\\sum b_n$, then this new series is also absolutely convergent with sum $s$.\nProof\nSince $\\sum\\vert a_n\\vert$ is a convergent series of nonnegative terms with sum $s$ and since the $b_n$\u0026rsquo;s are just the $a_n$\u0026rsquo;s in a different order, it follows from Theorem 1 that $\\sum\\vert b_n\\vert$ also converges to $s$. Therefore $\\sum b_n$ is absolutely convergent with sum $t$, for some positive $t$.\nTheorem 6 allows us to write \\begin{equation} s=\\sum a_n=\\sum p_n-\\sum q_n, \\end{equation} and \\begin{equation} t=\\sum b_n=\\sum P_n-\\sum Q_n, \\end{equation} where each of the series on the right is convergent and consists of nonnegative. But the $P_n$\u0026rsquo;s and $Q_n$\u0026rsquo;s are simply the $p_n$\u0026rsquo;s and $q_n$\u0026rsquo;s in a different order. Hence, by Theorem 1, we have $\\sum P_n=\\sum p_n$ and $\\sum Q_n=\\sum q_n$. And therefore, $t=s$.\nTheorem 8 (Riemann\u0026rsquo;s rearrangement theorem)\nLet $\\sum a_n$ be a conditionally convergent series. Then its terms can be rearranged to yield a convergent series whose sum is an arbitrary preassigned number, or a series that diverges to $\\infty$, or a series that diverges to $-\\infty$.\nProof\nSince $\\sum a_n$ converges conditionally, we begin by using Theorem 6 to form the two divergent series of nonnegative terms $\\sum p_n$ and $\\sum q_n$.\nTo prove the first statement, let $s$ be any number and construct a rearrangement of the given series as follows. Start by writing down $p$\u0026rsquo;s in order until the partial sum \\begin{equation} p_1+p_2+\\ldots+p_{n_1} \\end{equation} is first $\\geq s$; next we continue with $-q$\u0026rsquo;s until the total partial sum \\begin{equation} p_1+p_2+\\ldots+p_{n_1}-q_1-q_2-\\ldots-q_{m_1} \\end{equation} is first $\\leq s$; then we continue with $p$\u0026rsquo;s until the total partial sum \\begin{equation} p_1+\\ldots+p_{n_1}-q_1-\\ldots-q_{m_1}+p_{n_1+1}+\\ldots+p_{n_2} \\end{equation} is first $\\geq s$; and so on.\nThe possibility of each of these steps is guaranteed by the divergence of $\\sum p_n$ and $\\sum q_n$; and the resulting rearrangement of $\\sum a_n$ converges to $s$ because $p_n\\to 0$ and $q_n\\to 0$.\nIn order to make the rearrangement diverge to $\\infty$, it suffices to write down enough $p$\u0026rsquo;s to yield \\begin{equation} p_1+p_2+\\ldots+p_{n_1}\\geq 1, \\end{equation} then to insert $-q_1$, and then to continue with $p$\u0026rsquo;s until \\begin{equation} p_1+\\ldots+p_{n_1}-q_1+p_{n_1+1}+\\ldots+p_{n_2}\\geq 2, \\end{equation} then to insert $-q_2$, and so on.\nWe can produce divergence to $-\\infty$ by a similar construction.\nOne of the principal application of Theorem 7 relates to the multiplication of series.\nIf we multiply two series \\begin{align} \\sum_{n=0}^{\\infty}a_n\u0026amp;=a_0+a_1+\\ldots+a_n+\\ldots\\tag{20}\\label{20} \\\\ \\sum_{n=0}^{\\infty}b_n\u0026amp;=b_0+b_1+\\ldots+b_n+\\ldots\\tag{21}\\label{21} \\end{align} by forming all possible product $a_i b_j$ (as in the case of finite sums), then we obtain the following doubly infinite array\nFigure 2 There are various ways of arranging these products into a single infinite series, of which two are important. The first one is to group them by diagonals, as indicated in the arrows in Figure 2: \\begin{equation} a_0b_0+(a_0b_1+a_1b_1)+(a_0b_2+a_1b_1+a_2b_0)+\\ldots\\tag{22}\\label{22} \\end{equation} This series can be defined as $\\sum_{n=0}^{\\infty}c_n$, where \\begin{equation} c_n=a_0b_n+a_1b_{n-1}+\\ldots+a_nb_0 \\end{equation}\nIt is called the product (or Cauchy product) of the two series $\\sum a_n$ and $\\sum b_n$.\nThe second crucial method of arranging these products into a series is by squares, as shown in Figure 2: \\begin{equation} a_0b_0+(a_0b_1+a_1b_1+a_1b_0)+(a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0)+\\ldots\\tag{23}\\label{23} \\end{equation} The advantage of this arrangement is that the $n$-th partial sum $s_n$ of \\eqref{23} is given by \\begin{equation} s_n=(a_0+a_1+\\ldots+a_n)(b_0+b_1+\\ldots+b_n)\\tag{24}\\label{24} \\end{equation} Theorem 9\nIf the two series \\eqref{20} and \\eqref{21} have nonnegative terms and converges to $s$ and $t$, then their product \\eqref{22} converges to $st$.\nProof\nIt is clear from \\eqref{24} that \\eqref{23} converges to $st$. Let\u0026rsquo;s denote the series \\eqref{22} and \\eqref{23} without parenthesis by $(22\u0026rsquo;)$ and $(23\u0026rsquo;)$.\nWe have the series $(23\u0026rsquo;)$ of nonnegative terms still converges to $st$ because, for if $m$ is an integer such that $n^2\\leq m\\leq (n+1)^2$, then the $m$-th partial sum of $(23\u0026rsquo;)$ lies between $s_{n-1}$ and $s_n$, and both of these converge to $st$.\nBy Theorem 7, the terms of $(23\u0026rsquo;)$ can be rearranged to yield $(22\u0026rsquo;)$ without changing the sum $st$; and when parentheses are suitably inserted, we see that \\eqref{8} converges to $st$.\nWe now extend Theorem 9 to the case of absolute convergence.\nTheorem 10\nIf the series $\\sum_{n=0}^{\\infty}a_n$ and $\\sum_{n=0}^{\\infty}b_n$ are absolutely convergent, with sum $s$ and $t$, then their product \\begin{multline} \\sum_{n=0}^{\\infty}(a_0b_n+a_1b_{n-1}+\\ldots+a_nb_0)=a_0b_0+(a_0b_1+a_1b_0)\\hspace{0.1cm}+ \\\\ (a_0b_2+a_1b_1+a_2b_0)+\\ldots+(a_0b_n+a_1b_{n-1}+\\ldots+a_nb_0)+\\ldots\\tag{25}\\label{25} \\end{multline} is absolutely convergent, with sum $st$.\nProof\nThe series $\\sum_{n=0}^{\\infty}\\vert a_n\\vert$ and $\\sum_{n=0}^{\\infty}\\vert b_n\\vert$ are convergent and have nonnegative terms. So by Theorem 9 above, their product \\begin{multline} \\vert a_0\\vert\\vert b_0\\vert+\\vert a_0\\vert\\vert b_1\\vert+\\vert a_1\\vert\\vert b_0\\vert+\\ldots+\\vert a_0\\vert\\vert b_n\\vert+\\vert a_1\\vert\\vert b_{n-1}\\vert+\\ldots+\\vert a_n\\vert\\vert b_0\\vert+\\ldots \\\\ =\\vert a_0b_0\\vert+\\vert a_0b_1\\vert+\\vert a_1b_0\\vert+\\ldots+\\vert a_0b_n\\vert+\\vert a_1b_{n-1}\\vert+\\ldots+\\vert a_nb_0\\vert+\\ldots\\tag{26}\\label{26} \\end{multline} converges, and therefore the series \\begin{equation} a_0b_0+a_0b_1+a_1b_0+\\ldots+a_0b_n+\\ldots+a_nb_0+\\ldots\\tag{27}\\label{27} \\end{equation} is absolutely convergent. It follows from Theorem 7 that the sum of \\eqref{27} will not change if we rearrange its terms and write it as \\begin{equation} a_0b_0+a_0b_1+a_1b_1+a_1b_0+a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0+\\ldots\\tag{28}\\label{28} \\end{equation} We now observe that the sum of the first $(n+1)^2$ terms of \\eqref{28} is \\begin{equation} (a_0+a_1+\\ldots+a_n)(b_0+b_1+\\ldots+b_n), \\end{equation} so it is clear that \\eqref{28}, and with it \\eqref{27}, converges to $st$. Thus, \\eqref{25} also converges to $st$, since \\eqref{25} is retrieved by suitably inserted parentheses in \\eqref{27}.\nMoreover, we also have \\begin{equation} \\vert a_0b_n+a_1b_{n-1}+\\ldots+a_nb_0\\vert\\leq\\vert a_0b_n\\vert+\\vert a_1b_{n-1}\\vert+\\ldots+\\vert a_nb_0\\vert \\end{equation} and the series \\begin{equation} \\vert a_0b_0\\vert+(\\vert a_0b_1\\vert+\\vert a_1b_0\\vert)+\\ldots+(\\vert a_0b_n\\vert+\\ldots+\\vert a_nb_0\\vert)+\\ldots \\end{equation} obtained from \\eqref{26} by inserting parentheses. By the comparison test, \\eqref{25} converges absolutely. Hence, we can conclude that \\eqref{25} is absolutely convergent, with sum $st$.\nWe have already gone through convergence tests applied only to series of positive (or nonnegative) terms. Let\u0026rsquo;s end this lengthy note with the alternating series test. ^^!\nDirichlet\u0026rsquo;s test Abel\u0026rsquo;s partial summation formula Consider series $\\sum_{n=1}^{\\infty}a_n$, sequence $\\{b_n\\}$. If $s_n=a_1+a_2+\\ldots+a_n$, then \\begin{equation} a_1b_1+a_2b_2+\\ldots+a_nb_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\\ldots+s_{n-1}(b_{n-1}-b_n)+s_nb_n\\tag{29}\\label{29} \\end{equation}\nProof\nSince $a_1=s_1$ and $a_n=s_n-s_{n-1}$ for $n\u0026gt;1$, we have \\begin{align} a_1b_1\u0026amp;=s_1b_1 \\\\ a_2b_2\u0026amp;=s_2b_2-s_1b_2 \\\\ a_3b_3\u0026amp;=s_3b_3-s_2b_3 \\\\ \u0026amp;\\vdots \\\\ a_nb_n\u0026amp;=s_nb_n-s_{n-1}b_n \\end{align} On adding these equations, and grouping suitably, we obtain \\eqref{29}.\nDirichlet\u0026rsquo;s test If the series $\\sum_{n=1}^{\\infty}a_n$ has bounded partial sums, and if $\\{b_n\\}$ is a decreasing sequence of positive numbers such that $b_n\\to 0$, then the series \\begin{equation} \\sum_{n=1}^{\\infty}a_nb_n=a_1b_1+a_2b_2+\\ldots+a_nb_n+\\ldots\\tag{30}\\label{30} \\end{equation} converges.\nProof\nLet $S_n=a_1b_1+a_2b_2+\\ldots+a_nb_n$ denote the $n$-th partial sum of \\eqref{30}, then \\eqref{29} tells us that \\begin{equation} S_n=T_n+s_nb_n, \\end{equation} where \\begin{equation} T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\\ldots \\end{equation} Since ${s_n}$ is bounded there exists a positive constant $m$ such that $\\vert s_n\\vert\\leq m,\\forall n$, so $\\vert s_nb_n\\vert\\leq mb_n$. And since $b_n\\to 0$, we have that $s_nb_n\\to 0$ as $n\\to\\infty$.\nMoreover, since $\\{b_n\\}$ is a decreasing sequence of positive numbers, we have that \\begin{equation} \\begin{aligned} \\vert s_1(b_1-b_2)\\vert+\\vert s_2(b_3-b_3)\\vert+\\ldots\u0026amp;\\hspace{0.1cm}+\\vert s_{n-1}(b_{n-1}-b_n)\\vert \\\\ \u0026amp;\\leq m(b_1-b_2)+m(b_2-b_3)+\\ldots+m(b_{n-1}-b_n) \\\\ \u0026amp;=m(b_1-b_n)\\leq mb_1 \\end{aligned} \\end{equation} which implies that $T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\\ldots$ converges absolutely, and thus, it converges to a sum $t$. Therefore \\begin{equation} \\lim_{n\\to\\infty}S_n=\\lim_{n\\to\\infty}T_n+s_nb_n=\\lim_{n\\to\\infty}T_n+\\lim_{n\\to\\infty}s_nb_n=t+0=t \\end{equation} which lets us conclude that the series \\eqref{30} converges.\nReferences [1] George F.Simmons. Calculus With Analytic Geometry - 2nd Edition.\n[2] Marian M. A Concrete Approach to Classical Analysis.\n[3] MIT 18.01. Single Variable Calculus.\nFootnotes Theorem (Stolz–Cesaro)\nLet $\\{a_n\\}$ be a sequence of real numbers and $\\{b_n\\}$ be a strictly monotone and divergent sequence. Then \\begin{equation} \\lim_{n\\to\\infty}\\dfrac{a_{n+1}-a_n}{b_{n+1}-b_n}=L\\hspace{1cm}(\\in\\left[-\\infty,+\\infty\\right]) \\end{equation} implies \\begin{equation} \\lim_{n\\to\\infty}\\dfrac{a_n}{b_n}=L \\end{equation}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/calculus/infinite-series-of-constants/","summary":"\u003cblockquote\u003e\n\u003cp\u003eNotes on infinite series of constants.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Infinite Series of Constants"},{"content":" Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.\nMonte Carlo Methods Monte Carlo, named after a casino in Monaco, simulates complex probabilistic events using simple random events, such as tossing a pair of dice to simulate the casino\u0026rsquo;s overall business model.\nFigure 1: Using Monte Carlo method to approximate the value of $\\pi$. The code can be found here Monte Carlo methods have been used in several different tasks:\nSimulating a system and its probability distribution \\begin{equation} x\\sim\\pi(x) \\end{equation} Estimating a quantity through Monte Carlo integration \\begin{equation} c=\\mathbb{E}_\\pi\\left[f(x)\\right]=\\int\\pi(x)f(x)\\,dx \\end{equation} Optimizing a target function to find its modes (maxima or minima) \\begin{equation} x^*=\\text{argmax}\\,\\pi(x) \\end{equation} Learning a parameters from a training set to optimize some loss functions, such as the maximum likelihood estimation from a set of examples $\\{x_i,i=1,2,\\dots,M\\}$ \\begin{equation} \\Theta^*=\\text{argmax}\\sum_{i=1}^{M}\\log p(x_i;\\Theta) \\end{equation} Visualizing the energy landscape of a target function. Monte Carlo Methods in Reinforcement Learning Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns. Here, we define Monte Carlo methods only for episodic tasks. Or in other words, they learn from complete episodes of experience.\nMonte Carlo Prediction1 Since the value of a state $v_\\pi(s)=\\mathbb{E}_\\pi\\left[G_t|S_t=s\\right]$ is defined as the expectation of the return when the process is started from the given state $s$, an obvious way of estimating this value from experience is to compute observed mean returns after visits to that state. As more returns are observed, the average should converge to the expected value. This is an instance of the so-called Monte Carlo method.\nIn particular, suppose we wish to estimate $v_\\pi(s)$ given a set of episodes obtained by following $\\pi$ and passing through $s$. Each time state $s$ appears in an episode, we call it a visit to $s$. There are two types of Monte Carlo methods:\nFirst-visit MC. The first time $s$ is visited in an episode is referred as the first visit to $s$. The method estimates $v_\\pi(s)$ as the average of the returns that have followed the first visit to $s$. Every-visit MC. The method estimates $v_\\pi(s)$ as the average of the returns that have followed all visits to to $s$. The sample mean return for state $s$ is computed as \\begin{equation} v_\\pi(s)=\\dfrac{\\sum_{t=1}^{T}𝟙\\left(S_t=s\\right)G_t}{\\sum_{t=1}^{T}𝟙\\left(S_t=s\\right)}, \\end{equation} where $𝟙(\\cdot)$ is an indicator function. In the case of first-visit MC, $𝟙\\left(S_t=s\\right)$ returns $1$ only in the first time $s$ is encountered in an episode. And for every-visit MC, $𝟙\\left(S_t=s\\right)$ gives value of $1$ every time $s$ is visited.\nFollowing is pseudocode of first-visit MC prediction, for estimating $V\\approx v_\\pi$\nFirst-visit MC vs. every-visit MC Both methods converge to $v_\\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $\\frac{1}{\\sqrt{n}}$, where $n$ is the number of returns averaged.\nFigure 2: Summary of Statistical Results comparing first-visit and every-visit MC method Monte Carlo Control2 Monte Carlo Estimation of Action Values When model is not available, it is particular useful to estimate action values rather than state values (which alone are insufficient to determine a policy). We must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$. To achieve this, we first consider the policy evaluation problem for action values.\nSimilar to when using MC method to estimate $v_\\pi(s)$, we can use both first-visit MC and every-visit MC to approximate the value of $q_\\pi(s,a)$. The only thing we need to keep in mind is, in this case, we work with visits to a state-action pair rather than to a state. Likewise, we define two types of MC methods for estimating $q_\\pi(s,a)$:\nFirst-visit MC: estimates $q_\\pi(s,a)$ as the average of the returns following the first time in each episode that the state $s$ was visited and the action $a$ was selected. Every-visit MC: estimates $q_\\pi(s,a)$ as the average of the returns that have followed all the visits to state-action pair $(s,a)$. Exploring Starts However, here we must exercise exploration. Because many state-action pairs may never be visited, and if $\\pi$ is a deterministic policy, then returns of only single one action for each state will be observed. That leads to the consequence that the other actions will not be evaluated since there are no returns to average.\nThere is one way to achieve this, which is called exploring starts - an assumption that assumes the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This assumption assures that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.\nMonte Carlo Policy Iteration To learn the optimal policy by MC, we apply the idea of GPI: \\begin{equation} \\pi_0\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_0}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_1\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_1}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_2\\overset{\\small \\text{E}}{\\rightarrow}\\dots\\overset{\\small \\text{I}}{\\rightarrow}\\pi_*\\overset{\\small \\text{E}}{\\rightarrow}q_* \\end{equation} Specifically,\nPolicy evaluation (denoted $\\overset{\\small\\text{E}}{\\rightarrow}$): estimates action value function $q_\\pi(s,a)$ using the episode generated from $s, a$, following by current policy $\\pi$ \\begin{equation} q_\\pi(s,a)=\\dfrac{\\sum_{t=1}^{T}𝟙\\left(S_t=s,A_t=a\\right)G_t}{\\sum_{t=1}^{T}𝟙\\left(S_t=s,A_t=a\\right)} \\end{equation} Policy improvement (denoted $\\overset{\\small\\text{I}}{\\rightarrow}$): makes the policy greedy with the current value function (action value function in this case) \\begin{equation} \\pi(s)\\doteq\\underset{a\\in\\mathcal{A(s)}}{\\text{argmax}},q(s,a) \\end{equation} The policy improvement can be done by constructing each $\\pi_{k+1}$ as the greedy policy w.r.t $q_{\\pi_k}$ because \\begin{align} q_{\\pi_k}\\left(s,\\pi_{k+1}(s)\\right)\u0026amp;=q_{\\pi_k}\\left(s,\\underset{a}{\\text{argmax}},q_{\\pi_k}(s,a)\\right) \\\\ \u0026amp;=\\max_a q_{\\pi_k}(s,a) \\\\ \u0026amp;\\geq q_{\\pi_k}\\left(s,\\pi_k(s)\\right) \\\\ \u0026amp;\\geq v_{\\pi_k}(s) \\end{align} Therefore, by the policy improvement theorem, we have that $\\pi_{k+1}\\geq\\pi_k$. Figure 3: MC policy iteration To solve this problem with Monte Carlo policy iteration, in the 1998 version of \u0026ldquo;Reinforcement Learning: An Introduction\u0026rdquo;, authors of the book introduced Monte Carlo ES (MCES), for Monte Carlo with exploring starts.\nIn MCES, value function is approximated by simulated returns and a greedy policy is selected at each iteration. Although MCES does not converge to any sub-optimal policy, the convergence to optimal fixed point is still an open question. For solutions in particular settings, you can check out some results like Tsitsiklis (2002), Chen (2018), Liu (2020).\nDown below is pseudocode of the Monte Carlo ES.\nOn-policy Monte Carlo Control3 In the previous section, we used the assumption of exploring starts (ES) to design a Monte Carlo control method called MCES. In this part, without making that impractical assumption, we will be talking about another Monte Carlo control method.\nIn on-policy control methods, the policy is generally soft (i.e. $\\pi(a|s)\u0026gt;0,\\forall s\\in\\mathcal{S},a\\in\\mathcal{A(s)}$, but gradually shifted closer and closer to a deterministic optimal policy). We can not simply improve the policy by following a greedy policy, since no exploration will take place. Then to get rid of ES, we use the on-policy MC method with $\\varepsilon$-greedy policies, e.g, most of the time they choose an action that maximal estimated action value, but with probability of $\\varepsilon$ they instead select an action at random. Specifically,\n$Pr(\\small\\textit{non-greedy action})=\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ $Pr(\\small\\textit{greedy action})=1-\\varepsilon+\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ The $\\varepsilon$-greedy policies are examples of $\\varepsilon$-soft policies, defined as ones for which $\\pi(a\\vert s)\\geq\\frac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ for all states and actions, for some $\\varepsilon\u0026gt;0$. Among $\\varepsilon$-soft policies, $\\varepsilon$-greedy policies are in some sense those that closest to greedy.\nWe have that any $\\varepsilon$-greedy policy w.r.t $q_\\pi$ is an improvement over any $\\varepsilon$-soft policy is assured by the policy improvement theorem.\nProof\nLet $\\pi\u0026rsquo;$ be the $\\varepsilon$-greedy. The conditions of the policy improvement theorem apply because for any $s\\in\\mathcal{S}$, we have: \\begin{align} q_\\pi\\left(s,\\pi\u0026rsquo;(s)\\right)\u0026amp;=\\sum_a\\pi\u0026rsquo;(a|s)q_\\pi(s,a) \\\\ \u0026amp;=\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a)+(1-\\varepsilon)\\max_a q_\\pi(s,a) \\\\ \u0026amp;\\geq\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}\\sum_a q_\\pi(s,a)+(1-\\varepsilon)\\sum_a\\dfrac{\\pi(a|s)-\\frac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}}{1-\\varepsilon}q_\\pi(s,a) \\\\ \u0026amp;=\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a)+\\sum_a\\pi(a|s)q_\\pi(s,a)-\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a) \\\\ \u0026amp;=v_\\pi(s) \\end{align} where in the third step, we have used the fact that the latter $\\sum$ is a weighted average over $q_\\pi(s,a)$. Thus, by the theorem, $\\pi\u0026rsquo;\\geq\\pi$. The equality holds when both $\\pi\u0026rsquo;$ and $\\pi$ are optimal policies among the $\\varepsilon$-soft ones.\nPseudocode of the complete algorithm is given below.\nOff-policy Monte Carlo Prediction4 When working with control methods, we have to solve a dilemma about exploitation and exploration. In other words, we have to evaluate a policy from episodes generated by following an exploratory policy.\nA straightforward way to solve this problem is to use two different policies, one that is learned about and becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy is being learned about is called the target policy, whereas behavior policy is the one which is used to generate behavior.\nIn this section, we will be considering the off-policy method on prediction task, on which both target (denoted as $\\pi$) and behavior (denoted as $b$) policies are fixed and given. Particularly, we wish to estimate $v_\\pi$ or $q_\\pi$ from episodes retrieved from following another policy $b$, where $\\pi\\neq b$.\nAssumption of Coverage In order to use episodes from $b$ to estimate values for $\\pi$, we require that every action taken under $\\pi$ is also taken, at least occasionally, under $b$. That means, we assume that $\\pi(a|s)\u0026gt;0$ implies $b(s|a)\u0026gt;0$, which leads to a result that $b$ must be stochastic, while $\\pi$ may be deterministic since $\\pi\\neq b$. This is the assumption of coverage.\nImportance Sampling Let $X$ be a variable (or set of variables) that takes on values in some space $\\textit{Val}(X)$. Importance sampling (IS) is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$, typically called the target distribution. We can estimate this expectation by generating samples $x[1],\\dots,x[M]$ from $P$, and then estimating \\begin{equation} \\mathbb{E}_P\\left[f\\right]\\approx\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m]) \\end{equation} In some cases, it might be impossible or computationally very expensive to generate samples from $P$, we instead prefer to generate samples from a different distribution, $Q$, known as the proposal distribution (or sampling distribution).\nUnnormalized Importance Sampling.\nIf we generate samples from $Q$ instead of $P$, we cannot simply average the $f$-value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that \\begin{align} \\mathbb{E}_{P(X)}\\left[f(X)\\right]\u0026=\\sum_x f(x)P(x) \\\\ \u0026=\\sum_x Q(x)f(x)\\dfrac{P(x)}{Q(x)} \\\\ \u0026=\\mathbb{E}_{Q(X)}\\left[f(X)\\dfrac{P(X)}{Q(X)}\\right]\\tag{1}\\label{1} \\end{align} Based on this observation \\eqref{1}, we can use the standard estimator for expectations relative to $Q$. We generate a set of sample $\\mathcal{D}=\\{x[1],\\dots,x[M]\\}$ from $Q$, and then estimate: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m])\\dfrac{P(x[m])}{Q(x[m])}\\tag{2}\\label{2}, \\end{equation} where $\\hat{\\mathbb{E}}$ denotes empirical expectation. We call this estimator the unnormalized importance sampling estimator, this method is also often called unweighted importance sampling. The factor $\\frac{P(x[m])}{Q(x[m])}$ (denoted as $w(x[m])$) can be viewed as a correction weight to the term $f(x[m])$, which we would have used had $Q$ been our target distribution. Normalized Importance Sampling.\nIn many situations, we have that $P$ is known only up to a normalizing constant $Z$. Particularly, what we have access to is a distribution $\\tilde{P}(X)=ZP(X)$. Thus, rather than to define the weights relative to $P$ as above, we define: \\begin{equation} w(X)\\doteq\\dfrac{\\tilde{P}(X)}{Q(X)} \\end{equation} We have that the weight $w(X)$ is a random variable, and has expected value equal to $Z$: \\begin{equation} \\mathbb{E}_{Q(X)}\\left[w(X)\\right]=\\sum_x Q(x)\\dfrac{\\tilde{P}(x)}{Q(x)}=\\sum_x\\tilde{P}(x)=Z \\end{equation} Hence, this quantity is the normalizing constant of the distribution $\\tilde{P}$. We can now rewrite \\eqref{1} as: \\begin{align} \\mathbb{E}_{P(X)}\\left[f(X)\\right]\u0026=\\sum_x P(x)f(x) \\\\ \u0026=\\sum_x Q(x)f(x)\\dfrac{P(x)}{Q(x)} \\\\ \u0026=\\dfrac{1}{Z}\\sum_x Q(x)f(x)\\dfrac{\\tilde{P}(x)}{Q(x)} \\\\ \u0026=\\dfrac{1}{Z}\\mathbb{E}_{Q(X)}\\left[f(X)w(X)\\right] \\\\ \u0026=\\dfrac{\\mathbb{E}_{Q(X)}\\left[f(X)w(X)\\right]}{\\mathbb{E}_{Q(X)}\\left[w(X)\\right]}\\tag{3}\\label{3} \\end{align} We can use an empirical estimator for both the numerator and denominator. Given $M$ samples $\\mathcal{D}=\\{x[1],\\dots,x[M]\\}$ from $Q$, we can estimate: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{\\sum_{m=1}^{M}f(x[m])w(x[m])}{\\sum_{m=1}^{M}w(x[m])}\\tag{4}\\label{4} \\end{equation} We call this estimator the normalized importance sampling estimator (or weighted importance sampling estimator). Off-policy Monte Carlo Prediction via Importance Sampling We apply IS to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance sampling ratio (which we denoted as $w$ as above, but now we change the notation to $\\rho$ in order to follows the book).\nThe probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\\dots,S_T$, occurring under any policy $\\pi$ given starting state $s$ is: \\begin{align} Pr(A_t,S_{t+1},\\dots,S_T|S_t,A_{t:T-1}\\sim\\pi)\u0026amp;=\\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\\dots p(S_T|S_{T-1},A_{T-1}) \\\\ \u0026amp;=\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k,A_k) \\end{align} Thus, the importance sampling ratio as we defined is: \\begin{equation} \\rho_{t:T-1}\\doteq\\dfrac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_t,A_t)}{\\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_t,A_t)}=\\prod_{k=1}^{T-1}\\dfrac{\\pi(A_k|S_k)}{b(A_k|S_k)} \\end{equation} which depends only on the two policies and the sequence, not on the MDP.\nSince $v_b(s)=\\mathbb{E}\\left[G_t|S_t=s\\right]$, then we have \\begin{equation} \\mathbb{E}\\left[\\rho_{t:T-1}G_t|S_t=s\\right]=v_\\pi(s) \\end{equation} To estimate $v_\\pi(s)$, we simply scale the returns by the ratios and average the results: \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{\\vert\\mathcal{T}(s)\\vert},\\tag{5}\\label{5} \\end{equation} where $\\mathcal{T}(s)$ is the set of all states in which $s$ is visited (only for every-visit). For a first-visit,$\\mathcal{T}(s)$ would only include time steps that were first visits to $s$ within their episodes. $T(t)$ denotes the first time of termination following time $t$, and $G_t$ denotes the return after $t$ up through $T(t)$.\nWhen importance sampling is done as simple average in this way, we call it ordinary importance sampling (OIS) (which corresponds to unweighted importance sampling in the previous section).\nAnd the one corresponding to weighted importance sampling (WIS), which uses a weighted average, is defined as: \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}},\\tag{6}\\label{6} \\end{equation} or zero if the denominator is zero.\nIncremental Implementation for Off-policy MC Prediction using IS Incremental Method Incremental method is a way of updating averages with small, constant computation required to process each new reward instead of maintaining a record of all the rewards and then performing this computation whenever the estimated value was needed. It follows the general rule: \\begin{equation} NewEstimate\\leftarrow OldEstimate+StepSize\\left[Target-OldEstimate\\right] \\end{equation}\nApplying to Off-policy MC Prediction using IS In ordinary IS, the returns are scaled by the IS ratio $\\rho_{t:T(t)-1}$, then simply averaged, as in \\eqref{5}. Thus, it\u0026rsquo;s easy to apply incremental method to OIS.\nFor WIS, as in the equation \\eqref{6}, we have to form a weighted average of the returns, and a slightly different incremental incremental algorithm is required. Suppose we have a sequence of returns $G_1,G_2,\\dots,G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$, e.g. $W_i=\\rho_{t_i:T(t_i)}$. We wish to form the estimate \\begin{equation} V_n\\doteq\\dfrac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k},\\hspace{1cm}n\\geq2 \\end{equation} and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first $n$ returns. The update rule for $V_n$ is \\begin{equation} V_{n+1}\\doteq V_n+\\dfrac{W_n}{C_n}\\big[G_n-V_n\\big],\\hspace{1cm}n\\geq1, \\end{equation} and \\begin{equation} C_{n+1}\\doteq C_n+W_{n+1}, \\end{equation} where $C_0=0$. And here is pseudocode of our algorithm.\nOff-policy Monte Carlo Control Similarly, we develop the algorithm for off-policy MC control, based on GPI and WIS, for estimating $\\pi_*$ and $q_*$, which is shown below.\nThe target policy $\\pi\\approx\\pi_*$ is the greedy policy w.r.t $Q$, which is an estimate of $q_\\pi$. The behavior policy, $b$, can be anything, but in order to assure convergence of $\\pi$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be guaranteed by choosing $b$ to be $\\varepsilon$-soft.\nThe policy $\\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$, which may change between or even within episodes.\nExample - Racetrack (This example is taken from Exercise 5.12, Reinforcement Learning: An Introduction book.)\nProblem\nConsider driving a race car around a turn like that shown in Figure 4. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car\u0026rsquo;s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\nFigure 4: A turn for the racetrack task Solution code\nThe source code can be found here.\nWe begin by importing some useful packages.\nimport numpy as np import matplotlib.pyplot as plt from tqdm import tqdm Next, we define our environment\nclass RaceTrack: def __init__(self, grid): self.NOISE = 0 self.MAX_VELOCITY = 4 self.MIN_VELOCITY = 0 self.starting_line = [] self.track = None self.car_position = None self.actions = [[-1,-1],[-1,0],[-1,1],[0,-1],[0,0],[0,1],[1,-1],[1,0],[1,1]] self._load_track(grid) self._generate_start_state() self.velocity = np.array([0, 0], dtype=np.int16) def reset(self): self._generate_start_state() self.velocity = np.array([0, 0], dtype=np.int16) def get_state(self): return self.car_position.copy(), self.velocity.copy() def _generate_start_state(self): index = np.random.choice(len(self.starting_line)) self.car_position = np.array(self.starting_line[index]) def take_action(self, action): if self.is_terminal(): return 0 self._update_state(action) return -1 def _update_state(self, action): # update velocity # with probability of 0.1, keep the velocity unchanged if not np.random.binomial(1, 0.1): self.velocity += np.array(action, dtype=np.int16) self.velocity = np.minimum(self.velocity, self.MAX_VELOCITY) self.velocity = np.maximum(self.velocity, self.MIN_VELOCITY) # update car position for tstep in range(0, self.MAX_VELOCITY + 1): t = tstep / self.MAX_VELOCITY position = self.car_position + np.round(self.velocity * t).astype(np.int16) if self.track[position[0], position[1]] == -1: self.reset() return if self.track[position[0], position[1]] == 2: self.car_position = position self.velocity = np.array([0, 0], dtype=np.int16) return self.car_position = position def _load_track(self, grid): y_len, x_len = len(grid), len(grid[0]) self.track = np.zeros((x_len, y_len), dtype=np.int16) for y in range(y_len): for x in range(x_len): pt = grid[y][x] if pt == \u0026#39;W\u0026#39;: self.track[x, y] = -1 elif pt == \u0026#39;o\u0026#39;: self.track[x, y] = 1 elif pt == \u0026#39;-\u0026#39;: self.track[x, y] = 0 else: self.track[x, y] = 2 # rotate the track in order to sync the track with actions self.track = np.fliplr(self.track) for y in range(y_len): for x in range(x_len): if self.track[x, y] == 0: self.starting_line.append((x, y)) def is_terminal(self): return self.track[self.car_position[0], self.car_position[1]] == 2 We continue by defining our behavior policy and algorithm.\ndef behavior_policy(track, state): index = np.random.choice(len(track.actions)) return np.array(track.actions[index]) def off_policy_MC_control(episodes, gamma, grid): x_len, y_len = len(grid[0]), len(grid) Q = np.zeros((x_len, y_len, 5, 5, 3, 3)) - 40 C = np.zeros((x_len, y_len, 5, 5, 3, 3)) pi = np.zeros((x_len, y_len, 5, 5, 1, 2), dtype=np.int16) track = RaceTrack(grid) # for epsilon-soft greedy policy epsilon = 0.1 for ep in tqdm(range(episodes)): track.reset() trajectory = [] while not track.is_terminal(): state = track.get_state() s_x, s_y = state[0][0], state[0][1] s_vx, s_vy = state[1][0], state[1][1] if not np.random.binomial(1, epsilon): action = pi[s_x, s_y, s_vx, s_vy, 0] else: action = behavior_policy(track, state) reward = track.take_action(action) trajectory.append([state, action, reward]) G = 0 W = 1 while len(trajectory) \u0026gt; 0: state, action, reward = trajectory.pop() G = gamma * G + reward sp_x, sp_y, sv_x, sv_y = state[0][0], state[0][1], state[1][0], state[1][1] a_x, a_y = action s_a = (sp_x, sp_y, sv_x, sv_y, a_x, a_y) C[s_a] += W Q[s_a] += W/C[s_a]*(G-Q[s_a]) q_max = -1e5 a_max = None for act in track.actions: sa_max = sp_x, sp_y, sv_x, sv_y, act[0], act[1] if Q[sa_max] \u0026gt; q_max: q_max = Q[sa_max] a_max = act pi[sp_x, sp_y, sv_x, sv_y, 0] = a_max if not np.array_equal(pi[sp_x, sp_y, sv_x, sv_y, 0], action): break W *= 1/(1-epsilon+epsilon/9) return pi And wrapping everything up with the main function.\nif __name__ == \u0026#39;__main__\u0026#39;: gamma = 0.9 episodes = 10000 grid = [\u0026#39;WWWWWWWWWWWWWWWWWW\u0026#39;, \u0026#39;WWWWooooooooooooo+\u0026#39;, \u0026#39;WWWoooooooooooooo+\u0026#39;, \u0026#39;WWWoooooooooooooo+\u0026#39;, \u0026#39;WWooooooooooooooo+\u0026#39;, \u0026#39;Woooooooooooooooo+\u0026#39;, \u0026#39;Woooooooooooooooo+\u0026#39;, \u0026#39;WooooooooooWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WoooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWooooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWoooooooWWWWWWWW\u0026#39;, \u0026#39;WWWWooooooWWWWWWWW\u0026#39;, \u0026#39;WWWWooooooWWWWWWWW\u0026#39;, \u0026#39;WWWW------WWWWWWWW\u0026#39;] policy = off_policy_MC_control(episodes, gamma, grid) track_ = RaceTrack(grid) x_len, y_len = len(grid[0]), len(grid) trace = np.zeros((x_len, y_len)) for _ in range(1000): state = track_.get_state() sp_x, sp_y, sv_x, sv_y = state[0][0], state[0][1], state[1][0], state[1][1] trace[sp_x, sp_y] += 1 action = policy[sp_x, sp_y, sv_x, sv_y, 0] reward = track_.take_action(action) if track_.is_terminal(): break trace = (trace \u0026gt; 0).astype(np.float32) trace += track_.track plt.imshow(np.flipud(trace.T)) plt.savefig(\u0026#39;./racetrack_off_policy_control.png\u0026#39;) plt.close() We end up with this result after running the code.\nFigure 5: Example - Racetrack's result Discounting-aware Importance Sampling Recall that in the above section, we defined the estimator for $\\mathbb{E}_P[f]$ as: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m])\\dfrac{P(x[m])}{Q(x[m])} \\end{equation} This estimator is unbiased because each of the samples it averages is unbiased: \\begin{equation} \\mathbb{E}_{Q}\\left[\\dfrac{P(x[m])}{Q(x[m])}f(x[m])\\right]=\\int_x Q(x)\\dfrac{P(x)}{Q(x)}f(x)\\hspace{0.1cm}dx=\\int_x P(x)f(x)\\hspace{0.1cm}dx=\\mathbb{E}_{P}\\left[f(x[m])\\right] \\end{equation} This IS estimate is unfortunately often of unnecessarily high variance. To be more specific, for example, the episodes last 100 steps and $\\gamma=0$. Then $G_0=R_1$ will be weighted by \\begin{equation} \\rho_{0:99}=\\dfrac{\\pi(A_0|S_0)}{b(A_0|S_0)}\\dots\\dfrac{\\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})} \\end{equation} but actually, it really needs to be weighted by $\\rho_{0:1}=\\frac{\\pi(A_0|S_0)}{b(A_0|S_0)}$. The other 99 factors $\\frac{\\pi(A_1|S_1)}{b(A_1|S_1)}\\dots\\frac{\\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}$ are irrelevant because after the first reward, the return has already been determined. These later factors are all independent of the return and of expected value $1$; they do not change the expected update, but they add enormously to its variance. They could even make the variance infinite in some cases.\nFigure 6: Infinite variance when using OIS (Eg5.5 - RL: An Introduction book). The code can be found here One of the methods used to avoid this large extraneous variance is discounting-aware IS. The idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination.\nWe begin by defining flat partial returns: \\begin{equation} \\bar{G}_{t:h}\\doteq R_{t+1}+R_{t+2}+\\dots+R_h,\\hspace{1cm}0\\leq t\u0026lt;h\\leq T, \\end{equation} where flat denotes the absence of discounting, and partial denotes that these returns do not extend all the way to termination but instead stop at $h$, called the horizon. The conventional full return $G_t$ can be viewed as a sum of flat partial returns: \\begin{align} G_t\u0026amp;\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dots+\\gamma^{T-t-1}R_T \\\\ \u0026amp;=(1-\\gamma)R_{t+1} \\\\ \u0026amp;\\hspace{0.5cm}+(1-\\gamma)\\gamma(R_{t+1}+R_{t+2}) \\\\ \u0026amp;\\hspace{0.5cm}+(1-\\gamma)\\gamma^2(R_{t+1}+R_{t+2}+R_{t+3}) \\\\ \u0026amp;\\hspace{0.7cm}\\vdots \\\\ \u0026amp;\\hspace{0.5cm}+(1-\\gamma)\\gamma^{T-t-2}(R_{t+1}+R_{t+2}+\\dots+R_{T-1}) \\\\ \u0026amp;\\hspace{0.5cm}+\\gamma^{T-t-1}(R_{t+1}+R_{t+2}+\\dots+R_T) \\\\ \u0026amp;=(1-\\gamma)\\sum_{h=t+1}^{T-1}\\left(\\gamma^{h-t-1}\\bar{G}_{t:h}\\right)+\\gamma^{T-t-1}\\bar{G}_{t:T} \\end{align} Now we need to scale the flat partial returns by an IS ratio that is similarly truncated. As $\\bar{G}_{t:h}$ only involves rewards up to a horizon $h$, we only need the ratio of the probabilities up to $h$. We define:\nDiscounting-aware OIS estimator \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\left[(1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\left(\\gamma^{h-t-1}\\rho_{t:h-1}\\bar{G}_{t:h}\\right)+\\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\bar{G}_{t:T(t)}\\right]}{\\vert\\mathcal{T}(s)\\vert} \\end{equation} Discounting-aware WIS estimator \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\left[(1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\left(\\gamma^{h-t-1}\\rho_{t:h-1}\\bar{G}_{t:h}\\right)+\\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\bar{G}_{t:T(t)}\\right]}{\\sum_{t\\in\\mathcal{T}(s)}\\left[(1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\left(\\gamma^{h-t-1}\\rho_{t:h-1}\\right)+\\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\right]} \\end{equation} These two estimators take into account the discount rate $\\gamma$ but have no effect if $\\gamma=1$. Per-decision Importance Sampling There is another way beside discounting-aware that may be able to reduce variance, even if $\\gamma=1$.\nRecall that in the off-policy estimator \\eqref{5} and \\eqref{6}, each term of the sum in the numerator is itself a sum: \\begin{align} \\rho_{t:T-1}G_t\u0026amp;=\\rho_{t:T-1}\\left(R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{T-t-1}R_T\\right) \\\\ \u0026amp;=\\rho_{t:T-1}R_{t+1}+\\gamma\\rho_{t:T-1}R_{t+2}+\\dots+\\gamma^{T-t-1}\\rho_{t:T-1}R_T\\tag{7}\\label{7} \\end{align} We have that \\begin{equation} \\rho_{t:T-1}R_{t+k}=\\dfrac{\\pi(A_t|S_t)}{b(A_t|S_t)}\\dots\\dfrac{\\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\\dots\\dfrac{\\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+k} \\end{equation} Of all these factors, only the first $k$ factors, $\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)}\\dots\\frac{\\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}$, and the last (the reward $R_{t+k}$) are related. All the others are for event that occurred after the reward. Moreover, we have that \\begin{equation} \\mathbb{E}\\left[\\dfrac{\\pi(A_i|S_i)}{b(A_i|S_i)}\\right]\\doteq\\sum_a b(a|S_i)\\dfrac{\\pi(a|S_i)}{b(a|S_i)}=1 \\end{equation} Therefore, we obtain \\begin{align} \\mathbb{E}\\Big[\\rho_{t:T-1}R_{t+k}\\Big]\u0026amp;=\\mathbb{E}\\left[\\dfrac{\\pi(A_t|S_t)}{b(A_t|S_t)}\\dots\\dfrac{\\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\\right]\\mathbb{E}\\left[\\dfrac{\\pi(A_k|S_k)}{b(A_k|S_k)}\\right]\\dots\\mathbb{E}\\left[\\dfrac{\\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}\\right] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_{t:t+k-1}R_{t+k}\\Big].1\\dots 1 \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_{t:t+k-1}R_{t+k}\\Big] \\end{align} Plug the result we just got into the expectation of \\eqref{7}, we have \\begin{align} \\mathbb{E}\\Big[\\rho_{t:T-1}G_t\\Big]\u0026amp;=\\mathbb{E}\\Big[\\rho_{t:T-1}R_{t+1}+\\gamma\\rho_{t:T-1}R_{t+2}+\\dots+\\gamma^{T-t-1}\\rho_{t:T-1}R_T\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\rho_{t:t}R_{t+1}+\\gamma\\rho_{t:t+1}R_{t+2}+\\dots+\\gamma^{T-t-1}\\rho_{t:T-1}R_T\\Big] \\\\ \u0026amp;=\\mathbb{E}\\Big[\\tilde{G}_t\\Big], \\end{align} where $\\tilde{G}_t=\\rho_{t:T-1}R_{t+1}+\\gamma\\rho_{t:T-1}R_{t+2}+\\dots+\\gamma^{T-t-1}\\rho_{t:T-1}R_T$.\nWe call this idea per-decision IS. Hence, we develop per-decision OIS estimator, using $\\tilde{G}_t$: \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\tilde{G}_t}{\\vert\\mathcal{T}(s)\\vert} \\end{equation}\nReferences [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Adrian Barbu \u0026amp; Song-Chun Zhu. Monte Carlo Methods.\n[3] David Silver. UCL course on RL.\n[4] Csaba Szepesvári. Algorithms for Reinforcement Learning.\n[5] Singh, S.P., Sutton, R.S. Reinforcement learning with replacing eligibility traces. Mach Learn 22, 123–158, 1996.\n[6] John N. Tsitsiklis. On the Convergence of Optimistic Policy Iteration. Journal of Machine Learning Research 3 (2002) 59–72.\n[7] Yuanlong Chen. On the convergence of optimistic policy iteration for stochastic shortest path problem, arXiv:1808.08763, 2018.\n[8] Jun Liu. On the Convergence of Reinforcement Learning with Monte Carlo Exploring Starts. arXiv:2007.10916, 2020.\n[9] Daphne Koller \u0026amp; Nir Friedman. Probabilistic Graphical Models: Principles and Techniques.\n[10] A. Rupam Mahmood, Hado P. van Hasselt, Richard S. Sutton. Weighted importance sampling for off-policy learning with linear function approximation. Advances in Neural Information Processing Systems 27 (NIPS 2014).\nFootnotes A prediction task in RL is where we are given a policy and our goal is to measure how well it performs.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlong with prediction, a control task in RL is where the policy is not fixed, and our goal is to find the optimal policy.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOn-policy is a category of RL algorithms that attempts to evaluate or improve the policy that is used to make decisions.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIn contrast to on-policy, off-policy methods evaluate or improve a policy different from that used to generate the data.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/","summary":"\u003cblockquote\u003e\n\u003cp\u003eRecall that when using \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/\"\u003e\u003cstrong\u003eDynamic Programming\u003c/strong\u003e\u003c/a\u003e algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With \u003cstrong\u003eMonte Carlo\u003c/strong\u003e methods, we only require \u003cstrong\u003eexperience\u003c/strong\u003e - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Monte Carlo Methods in Reinforcement Learning"},{"content":" In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.\nWhat is Dynamic Programming? Dynamic Programming (DP) is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.\nFigure 1: Using Dynamic Programming to find the shortest path in graph Dynamic Programming applied in Markov Decision Processes DP is a very general method for solving problems having two properties: Optimal substructure.\n- Principle of optimality applies.\n- Optimal solution can be decomposed into sub-problems. Overlapping sub-problems.\n- Sub-problems recur many times.\n- Solutions can be cached and reused. MDPs satisfy both properties since: Bellman equation gives recursive decomposition. Value function stores and reuses solutions. DP assumes the model is already known. Policy Evaluation Recall from the definition of Bellman equation that, for all $s\\in\\mathcal{S}$, \\begin{equation} v_\\pi(s)\\doteq\\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_\\pi(s\u0026rsquo;)\\right]\\tag{1}\\label{1} \\end{equation} If the environment\u0026rsquo;s dynamics are completely known, then \\eqref{1} is a system of $\\vert\\mathcal{S}\\vert$ linear equations in $\\vert\\mathcal{S}\\vert$ unknowns. We can use iterative methods to solve this problem.\nConsider a sequence of approximate value functions $v_0,v_1,\\dots$, each mapping $\\mathcal{S}^+\\to\\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\\pi$, we have an update rule: \\begin{align} v_{k+1}(s)\u0026amp;\\doteq\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma v_k(S_{k+1})\\vert S_t=s\\right] \\\\ \u0026amp;=\\sum_a\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_k(s\u0026rsquo;)\\right] \\end{align} for all $s\\in\\mathcal{S}$. Thanks to Banach\u0026rsquo;s fixed points theorem and as we have mentioned in that note, we have that the sequence $\\{v_k\\}\\to v_\\pi$ as $k\\to\\infty$. This algorithm is called iterative policy evaluation.\nWe have the backup diagram for this update.\nFigure 2: Backup diagram for Iterative policy evaluation update When implementing iterative policy evaluation method, for all $s\\in\\mathcal{S}$, we can use:\nOne array to store the value functions, and update them \"in-place\" (asynchronous DP) \\begin{equation} \\color{red}{v(s)}\\leftarrow\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\color{red}{v(s')}\\right] \\end{equation} Two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (synchronous DP) \\begin{align} \\color{red}{v_{new}(s)}\u0026\\leftarrow\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\color{red}{v_{old}(s')}\\right] \\\\ \\color{red}{v_{old}}\u0026\\leftarrow\\color{red}{v_{new}} \\end{align} Here is the pseudocode of the in-place iterative policy evaluation, given a policy $\\pi$, for estimating $V\\approx v_\\pi$\nPolicy Improvement The reason why we compute the value function for a given policy $\\pi$ is to find better policies. Given the computed value function $v_\\pi$ for an deterministic policy $\\pi$, we already know how good it is for a state $s$ to choose action $a=\\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\\neq\\pi$, will it be better?\nIn particular, in state $s$, selecting action $a$ and thereafter following the policy $\\pi$, we have: \\begin{align} q_\\pi(s,a)\u0026amp;\\doteq\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a\\right]\\tag{2}\\label{2} \\\\ \u0026amp;=\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_\\pi(s\u0026rsquo;)\\right] \\end{align} Theorem (Policy improvement theorem)\nLet $\\pi,\\pi\u0026rsquo;$ be any pair of deterministic policies such that, for all $s\\in\\mathcal{S}$, \\begin{equation} q_\\pi(s,\\pi\u0026rsquo;(s))\\geq v_\\pi(s)\\tag{3}\\label{3} \\end{equation} Then $\\pi\u0026rsquo;\\geq\\pi$, which means for all $s\\in\\mathcal{S}$, we have $v_{\\pi\u0026rsquo;}(s)\\geq v_\\pi(s)$.\nProof\nDeriving \\eqref{3} combined with \\eqref{2}, we have1: \\begin{align} v_\\pi(s)\u0026amp;\\leq q_\\pi(s,\\pi\u0026rsquo;(s)) \\\\ \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=\\pi\u0026rsquo;(s)\\right]\\tag{by \\eqref{2}} \\\\ \u0026amp;=\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right] \\\\ \u0026amp;\\leq\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma q_\\pi(S_{t+1},\\pi\u0026rsquo;(S_{t+1}))|S_t=s\\right]\\tag{by \\eqref{3}} \\\\ \u0026amp;=\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+2}+\\gamma v_\\pi(S_{t+2})|S_{t+1},A_{t+1}=\\pi\u0026rsquo;(S_{t+1})\\right]|S_t=s\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 v_\\pi(S_{t+2})|S_t=s\\right] \\\\ \u0026amp;\\leq\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 v_\\pi(S_{t+3})|S_t=s\\right] \\\\ \u0026amp;\\quad\\vdots \\\\ \u0026amp;\\leq\\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+\\dots|S_t=s\\right] \\\\ \u0026amp;=v_{\\pi\u0026rsquo;}(s) \\end{align}\nConsider the new greedy policy, $\\pi\u0026rsquo;$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\\pi$, given by \\begin{align} \\pi\u0026rsquo;(s)\u0026amp;\\doteq\\underset{a}{\\text{argmax}}\\hspace{0.1cm}q_\\pi(s,a) \\\\ \u0026amp;=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a\\right]\\tag{4}\\label{4} \\\\ \u0026amp;=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_\\pi(s\u0026rsquo;)\\right] \\end{align} By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.\nSuppose the new greedy policy, $\\pi\u0026rsquo;$, is as good as, but not better than, $\\pi$. Or in other words, $v_\\pi=v_{\\pi\u0026rsquo;}$. And from \\eqref{4}, we have for all $s\\in\\mathcal{S}$, \\begin{align} v_{\\pi\u0026rsquo;}(s)\u0026amp;=\\max_a\\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi\u0026rsquo;}(S_{t+1})|S_t=s,A_t=a\\right] \\\\ \u0026amp;=\\max_a\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_{\\pi\u0026rsquo;}(s\u0026rsquo;)\\right] \\end{align} which is the Bellman optimality equation for action-value function. And therefore, $v_{\\pi\u0026rsquo;}$ must be $v_*$. Hence, policy improvement must give us a strictly better policy except when the original one is already optimal2.\nPolicy Iteration Once we have obtained a better policy, $\\pi\u0026rsquo;$, by improving a policy $\\pi$ using $v_\\pi$, we can repeat the same process by computing $v_{\\pi\u0026rsquo;}$, and improve it to yield an even better $\\pi\u0026rsquo;\u0026rsquo;$. Repeating it again and again, we get an iterative procedure to improve the policy \\begin{equation} \\pi_0\\xrightarrow[]{\\text{evaluation}}v_{\\pi_0}\\xrightarrow[]{\\text{improvement}}\\pi_1\\xrightarrow[]{\\text{evaluation}}v_{\\pi_1}\\xrightarrow[]{\\text{improvement}}\\pi_2\\xrightarrow[]{\\text{evaluation}}\\dots\\xrightarrow[]{\\text{improvement}}\\pi_*\\xrightarrow[]{\\text{evaluation}}v_* \\end{equation} Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. This algorithm is called policy iteration. And here is the pseudocode of the policy iteration.\nAn example of using policy iteration on the Jack\u0026rsquo;s rental problem (RL book - example 4.2)\nFigure 3: Policy Iteration on Jack's car rental task. The code can be found here Value Iteration When using policy iteration, each of its iterations involves policy evaluation, which requires multiple sweeps through the state set, and thus affects the computation performance.\nPolicy evaluation step of policy iteration, in fact, can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration, which follows this update: \\begin{align} v_{k+1}\u0026amp;\\doteq\\max_a\\mathbb{E}\\left[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s,A_t=a\\right] \\\\ \u0026amp;=\\max_a\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_k(s\u0026rsquo;)\\right], \\end{align} for all $s\\in\\mathcal{S}$. Once again, thanks to Banach\u0026rsquo;s fixed point theorem, for an arbitrary $v_0$, we have that the sequence $\\{v_k\\}\\to v_*$ as $k\\to\\infty$.\nWe have the backup diagram for this update3.\nFigure 4: Backup diagram of Value Iteration update And here is the pseudocode of the value iteration.\nExample - Gambler\u0026rsquo;s Problem (This example is taken from RL book - example 4.3).\nLet\u0026rsquo;s say you are a gambler, who decides to bet on the outcomes of sequence of coin flips. On each flip, you have to decide how many dollars, in integer, you will bet. Each time you win, when the coin comes up head, the amount of money you get is exactly the same as the money that you staked on that flip. Same it goes in the tail case, you will lose that amount of dollars. The game ends when you reach your goal, let\u0026rsquo;s assume, $\\$100$, or when your hands are empty, $\\$0$. This task can be formulated as undiscounted, episodic, finite MDP. The state is your capital, $s\\in\\{1,2,\\dots,99\\}$; the actions are stakes, $a\\in\\{0,1,\\dots,\\min\\left(s,100-s\\right)\\}$. The reward is zero on all trainsitions except those on which you reach your goal, when it is $+1$. And we also assume that the probability of the coin coming up heads, $p_h=0.4$.\nSolution code\nThe code can be found here.\nimport numpy as np import matplotlib.pyplot as plt GOAL = 100 #For convenience, we introduce 2 dummy states: 0 and terminal state states = np.arange(0, GOAL + 1) rewards = {\u0026#39;terminal\u0026#39;: 1, \u0026#39;non-terminal\u0026#39;: 0} HEAD_PROB = 0.4 GAMMA = 1 # discount factor def value_iteration(theta): V = np.zeros(states.shape) V_set = [] policy = np.zeros(V.shape) while True: delta = 0 V_set.append(V.copy()) for state in states[1:GOAL]: old_value = V[state].copy() actions = np.arange(0, min(state, GOAL - state) + 1) new_value = 0 for action in actions: next_head_state = states[state] + action next_tail_state = states[state] - action head_reward = rewards[\u0026#39;terminal\u0026#39;] if next_head_state == GOAL else rewards[\u0026#39;non-terminal\u0026#39;] tail_reward = rewards[\u0026#39;non-terminal\u0026#39;] value = HEAD_PROB * (head_reward + GAMMA * V[next_head_state]) + \\ (1 - HEAD_PROB) * (tail_reward + GAMMA * V[next_tail_state]) if value \u0026gt; new_value: new_value = value V[state] = new_value delta = max(delta, abs(old_value - V[state])) print(\u0026#39;Max value changed: \u0026#39;, delta) if delta \u0026lt; theta: V_set.append(V) break for state in states[1:GOAL]: values = [] actions = np.arange(min(state, 100 - state) + 1) for action in actions: next_head_state = states[state] + action next_tail_state = states[state] - action head_reward = rewards[\u0026#39;terminal\u0026#39;] if next_head_state == GOAL else rewards[\u0026#39;non-terminal\u0026#39;] tail_reward = rewards[\u0026#39;non-terminal\u0026#39;] values.append(HEAD_PROB * (head_reward + GAMMA * V[next_head_state]) + (1 - HEAD_PROB) * (tail_reward + GAMMA * V[next_tail_state])) policy[state] = actions[np.argmax(np.round(values[1:], 4)) + 1] return V_set, policy if __name__ == \u0026#39;__main__\u0026#39;: theta = 1e-13 value_funcs, optimal_policy = value_iteration(theta) optimal_value = value_funcs[-1] print(optimal_value) plt.figure(figsize=(10, 20)) plt.subplot(211) for sweep, value in enumerate(value_funcs): plt.plot(value, label=\u0026#39;sweep {}\u0026#39;.format(sweep)) plt.xlabel(\u0026#39;Capital\u0026#39;) plt.ylabel(\u0026#39;Value estimates\u0026#39;) plt.legend(loc=\u0026#39;best\u0026#39;) plt.subplot(212) plt.scatter(states, optimal_policy) plt.xlabel(\u0026#39;Capital\u0026#39;) plt.ylabel(\u0026#39;Final policy (stake)\u0026#39;) plt.savefig(\u0026#39;./gambler.png\u0026#39;) plt.close() And here is our results after running the code\nFigure 5: Gambler's Problem solved by Value Iteration Result Generalized Policy Iteration The Generalized Policy Iteration (GPI) algorithm refers to the idea of combining policy evaluation and policy improvement together to improve the original policy.\nIn GPI, the value function is repeatedly driven toward the true value of the current policy and at the same time the policy is being improved optimality with respect to its value function, as in the following diagram.\nFigure 6: Generalized Policy Iteration Once it reaches the stationary state (when both evaluation and improvement no long produce any updates), then the current value function and policy must be optimal.\nThe evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They competing in the sense that on the one hand, making policy greedy w.r.t the value function typically makes value function incorrect for the new policy. And on the other hand, approximating the value function closer to the true value of the policy typically forces the policy is no longer to be greedy. But in the long run, they two processes cooperate to find a single joint solution: the optimal value function and an optimal policy.\nFigure 7: Interaction between the evaluation and improvement processes in GPI References [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] David Silver. UCL course on RL.\n[3] Csaba Szepesvári. Algorithms for Reinforcement Learning.\n[4] A. Lazaric. Markov Decision Processes and Dynamic Programming.\n[5] Wikipedia. Dynamic Programming.\n[6] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\n[7] Policy Improvement theorem.\nFootnotes In the third step, the expression \\begin{equation} \\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right] \\end{equation} means \u0026lsquo;\u0026rsquo;the discounted expected value when starting in state $s$, choosing action according to $\\pi\u0026rsquo;$ for the next time step, and following $\\pi$ thereafter\u0026quot;. And so on for the two, or n next steps. Therefore, we have that: \\begin{equation} \\mathbb{E}_{\\pi\u0026rsquo;}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right]=\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=\\pi\u0026rsquo;(s)\\right] \\end{equation}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe idea of policy improvement also extends to stochastic policies.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nValue iteration can be used in conjunction with action-value function, which takes the following update: \\begin{align} q_{k+1}(s,a)\u0026amp;\\doteq\\mathbb{E}\\left[R_{t+1}+\\gamma\\max_{a\u0026rsquo;}q_k(S_{t+1},a\u0026rsquo;)|S_t=s,A_t=a\\right] \\\\ \u0026amp;=\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma\\max_{a\u0026rsquo;}q_k(s\u0026rsquo;,a\u0026rsquo;)\\right] \\end{align} Yep, that\u0026rsquo;s right, the sequence $\\{q_k\\}\\to q_*$ as $k\\to\\infty$ at a geometric rate thanks to Banach\u0026rsquo;s fixed point theorem.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIn two previous notes, \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/\"\u003e\u003cstrong\u003eMDPs and Bellman equations\u003c/strong\u003e\u003c/a\u003e and \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/\"\u003e\u003cstrong\u003eOptimal Policy Existence\u003c/strong\u003e\u003c/a\u003e, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with \u003cstrong\u003eDynamic Programming\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Solving MDPs with Dynamic Programming"},{"content":" In the previous note about Markov Decision Processes, Bellman equations, we mentioned that there exists a policy $\\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.\nPreliminaries Norms Definition\nGiven a vector space $\\mathcal{V}\\subseteq\\mathbb{R}^d$, a function $f:\\mathcal{V}\\to\\mathbb{R}^+_0$ is a norm if and only if\nIf $f(v)=0$ for some $v\\in\\mathcal{V}$, then $v=0$. For any $\\lambda\\in\\mathbb{R},v\\in\\mathcal{V},f(\\lambda v)=\\vert\\lambda\\vert v$. For any $u,v\\in\\mathbb{R}, f(u+v)\\leq f(u)+f(v)$. Examples\n$\\ell^p$ norms: for $p\\geq 1$, \\begin{equation} \\Vert v\\Vert_p=\\left(\\sum_{i=1}^{d}|v_i|^p\\right)^{1/p} \\end{equation} $\\ell^\\infty$ norms: \\begin{equation} \\Vert v\\Vert_\\infty=\\max_{1\\leq i\\leq d}|v_i| \\end{equation} $\\ell^{\\mu,p}$: the weighted variants of these norm are defined as \\begin{equation} \\Vert v\\Vert_p=\\begin{cases}\\left(\\sum_{i=1}^{d}\\frac{|v_i|^p}{w_i}\\right)^{1/p}\u0026amp;\\text{if }1\\leq p\u0026lt;\\infty\\\\ \\max_{1\\leq i\\leq d}\\frac{|v_i|}{w_i}\u0026amp;\\text{if }p=\\infty\\end{cases} \\end{equation} $\\ell^{2,P}$: the matrix-weighted 2-norm is defined as \\begin{equation} \\Vert v\\Vert^2_P=v^\\intercal Pv \\end{equation} Similarly, we can define norms over spaces of functions. For example, if $\\mathcal{V}$ is the vector space of functions over domain $\\mathcal{X}$ which are uniformly bounded1, then \\begin{equation} \\Vert f\\Vert_\\infty=\\sup_{x\\in\\mathcal{X}}\\vert f(x)\\vert \\end{equation} Definition (Convergence in norm)\nLet $\\mathcal{V}=(\\mathcal{V},\\Vert\\cdot\\Vert)$ be a normed vector space2. Let $v_n\\in\\mathcal{V}$ is a sequence of vectors ($n\\in\\mathbb{N}$). The sequence ($v_n,n\\geq 0$) is said to converge to $v\\in\\mathcal{V}$ in the norm $\\Vert\\cdot\\Vert$, denoted as $v_n\\to_{\\Vert\\cdot\\Vert}v$ if \\begin{equation} \\lim_{n\\to\\infty}\\Vert v_n-v\\Vert=0, \\end{equation}\nDefinition (Cauchy sequence3)\nLet ($v_n;n\\geq 0$) be a sequence of vectors of a normed vector space $\\mathcal{V}=(\\mathcal{V},\\Vert\\cdot\\Vert)$. Then $v_n$ is called a Cauchy sequence if \\begin{equation} \\lim_{n\\to\\infty}\\sup_{m\\geq n}\\Vert v_n-v_m\\Vert=0 \\end{equation} Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.\nDefinition (Completeness)\nA normed vector space $\\mathcal{V}=(\\mathcal{V},\\Vert\\cdot\\Vert)$ is called complete if every Cauchy sequence in $\\mathcal{V}$ is convergent in the norm of the vector space.\nContractions Definition (Lipschitz function, Contraction)\nLet $\\mathcal{V}=(\\mathcal{V},\\Vert\\cdot\\Vert)$ be a normed vector space. A mapping $\\mathcal{T}:\\mathcal{V}\\to\\mathcal{V}$ is called $L$-Lipschitz if for any $u,v\\in\\mathcal{V}$, \\begin{equation} \\Vert\\mathcal{T}u-\\mathcal{T}v\\Vert\\leq L\\Vert u-v\\Vert \\end{equation} A mapping $\\mathcal{T}$ is called a non-expansion if it is Lipschitzian with $L\\leq 1$. It is called a contraction if it is Lipschitzian with $L\u0026lt;1$. In this case, $L$ is called the contraction factor of $\\mathcal{T}$ and $\\mathcal{T}$ is called an $L$-contraction.\nRemark\nIf $\\mathcal{T}$ is Lipschitz, it is also continuous in the sense that if $v_n\\to_{\\Vert\\cdot\\Vert}v$, then also $\\mathcal{T}v_n\\to_{\\Vert\\cdot\\Vert}\\mathcal{T}v$. This is because $\\Vert\\mathcal{T}v_n-\\mathcal{T}v\\Vert\\leq L\\Vert v_n-v\\Vert\\to 0$ as $n\\to\\infty$.\nBanach\u0026rsquo;s Fixed-point Theorem Definition (Banach space)\nA complete, normed vector space is called a Banach space.\nDefinition (Fixed point)\nLet $\\mathcal{T}:\\mathcal{V}\\to\\mathcal{V}$ be some mapping. The vector $v\\in\\mathcal{V}$ is called a fixed point of $\\mathcal{T}$ if $\\mathcal{T}v=v$.\nTheorem (Banach\u0026rsquo;s fixed-point)4 Let $\\mathcal{V}$ be a Banach space and $\\mathcal{T}:\\mathcal{V}\\to\\mathcal{V}$ be a $\\gamma$-contraction mapping. Then\n$\\mathcal{T}$ admits a unique fixed point $v$. For any $v_0\\in\\mathcal{V}$, if $v_{n+1}=\\mathcal{T}v_n$, then $v_n\\to_{\\Vert\\cdot\\Vert}v$ with a geometric convergence rate: \\begin{equation} \\Vert v_n-v\\Vert\\leq\\gamma^n\\Vert v_0-v\\Vert \\end{equation} Bellman Operator Previously, we defined Bellman equation for state-value function $v_\\pi(s)$ as: \\begin{align} v_\\pi(s)\u0026amp;=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{s\u0026rsquo;\\in\\mathcal{S},r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_\\pi(s\u0026rsquo;)\\right] \\\\ \\text{or}\\quad v_\\pi(s)\u0026amp;=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left(\\mathcal{R}^a_s+\\gamma\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^a_{ss\u0026rsquo;}v_\\pi(s\u0026rsquo;)\\right)\\tag{1}\\label{1} \\end{align} If we let \\begin{align} \\mathcal{P}^\\pi_{ss\u0026rsquo;}\u0026amp;=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\mathcal{P}^a_{ss\u0026rsquo;}; \\\\\\mathcal{R}^\\pi_s\u0026amp;=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\mathcal{R}^a_s \\end{align} then we can rewrite \\eqref{1} in another form as \\begin{equation} v_\\pi(s)=\\mathcal{R}^\\pi_s+\\gamma\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}v_\\pi(s\u0026rsquo;)\\tag{2}\\label{2} \\end{equation}\nDefinition (Bellman operator)\nWe define the Bellman operator underlying $\\pi,\\mathcal{T}:\\mathbb{R}^\\mathcal{S}\\to\\mathbb{R}^\\mathcal{S}$, by: \\begin{equation} (\\mathcal{T}^\\pi v)(s)=\\mathcal{R}^\\pi_s+\\gamma\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}v(s\u0026rsquo;) \\end{equation} With the help of $\\mathcal{T}^\\pi$, equation \\eqref{2} can be rewrite as: \\begin{equation} \\mathcal{T}^\\pi v_\\pi=v_\\pi\\tag{3}\\label{3} \\end{equation} Similarly, we can rewrite the Bellman optimality equation for $v_*$ \\begin{align} v_*(s)\u0026amp;=\\max_{a\\in\\mathcal{A}}\\sum_{s\u0026rsquo;\\in\\mathcal{S},r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma v_*(s\u0026rsquo;)\\right] \\\\ \u0026amp;=\\max_{a\\in\\mathcal{A}}\\left(\\mathcal{R}^a_s+\\gamma\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^a_{ss\u0026rsquo;}v_*(s\u0026rsquo;)\\right)\\tag{4}\\label{4} \\end{align} and thus, we can define the Bellman optimality operator $\\mathcal{T}^*:\\mathcal{R}^\\mathcal{S}\\to\\mathcal{R}^\\mathcal{S}$, by: \\begin{equation} (\\mathcal{T}^* v)(s)=\\max_{a\\in\\mathcal{A}}\\left(\\mathcal{R}^a_s+\\gamma\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^a_{ss\u0026rsquo;}v(s\u0026rsquo;)\\right) \\end{equation} And thus, with the help of $\\mathcal{T}^*$, we can rewrite the equation \\eqref{4} as: \\begin{equation} \\mathcal{T}^*v_*=v_*\\tag{5}\\label{5} \\end{equation} Now everything is all set, we can move on to the next step.\nProof of the existence Let $B(\\mathcal{S})$ be the space of uniformly bounded functions with domain $\\mathcal{S}$: \\begin{equation} B(\\mathcal{S})=\\{v:\\mathcal{S}\\to\\mathbb{R}:\\Vert v\\Vert_\\infty\u0026lt;+\\infty\\} \\end{equation} We will view $B(\\mathcal{S})$ as a normed vector space with the norm $\\Vert\\cdot\\Vert_\\infty$.\nIt is worth noticing that $(B(\\mathcal{S}),\\Vert\\cdot\\Vert_\\infty)$ is complete: If $(v_n;n\\geq0)$ is a Cauchy sequence in it then for any $s\\in\\mathcal{S}$, $(v_n(s);n\\geq0)$ is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of $(v_n(s))$, we can show that $\\Vert v_n-v\\Vert_\\infty\\to0$. Vaguely speaking, this holds because $(v_n;n\\geq0)$ is a Cauchy sequence in the norm $\\Vert\\cdot\\Vert_\\infty$, so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.\nLet $\\pi$ be some stationary policy. We have that $\\mathcal{T}^\\pi$ is well-defined since: if $u\\in B(\\mathcal{S})$, then also $\\mathcal{T}^\\pi u\\in B(S)$.\nFrom equation \\eqref{3}, we have that $v_\\pi$ is a fixed point to $\\mathcal{T}^\\pi$.\nWe also have that $\\mathcal{T}^\\pi$ is a $\\gamma$-contraction in $\\Vert\\cdot\\Vert_\\infty$ since for any $u, v\\in B(\\mathcal{S})$, \\begin{align} \\Vert\\mathcal{T}^\\pi u-\\mathcal{T}^\\pi v\\Vert_\\infty\u0026amp;=\\gamma\\max_{s\\in\\mathcal{S}}\\left|\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}\\left(u(s\u0026rsquo;)-v(s\u0026rsquo;)\\right)\\right| \\\\ \u0026amp;\\leq\\gamma\\max_{s\\in\\mathcal{S}}\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}\\big|u(s\u0026rsquo;)-v(s\u0026rsquo;)\\big| \\\\ \u0026amp;\\leq\\gamma\\max_{s\\in\\mathcal{S}}\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}\\big\\Vert u-v\\big\\Vert_\\infty \\\\ \u0026amp;=\\gamma\\Vert u-v\\Vert_\\infty, \\end{align} where the last line follows from $\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^\\pi_{ss\u0026rsquo;}=1$.\nIt follows that in order to find $v_\\pi$, we can construct the sequence $v_0,\\mathcal{T}^\\pi v_0,(\\mathcal{T}^\\pi)^2 v_0,\\dots$, which, by Banach\u0026rsquo;s fixed-point theorem will converge to $v_\\pi$ at a geometric rate.\nFrom the definition \\eqref{5} of $\\mathcal{T}^*$, we have that $\\mathcal{T}^*$ is well-defined.\nUsing the fact that $\\left|\\max_{a\\in\\mathcal{A}}f(a)-\\max_{a\\in\\mathcal{A}}g(a)\\right|\\leq\\max_{a\\in\\mathcal{A}}\\left|f(a)-g(a)\\right|$, similarly, we have: \\begin{align} \\Vert\\mathcal{T}^*u-\\mathcal{T}^*v\\Vert_\\infty\u0026amp;\\leq\\gamma\\max_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^a_{ss\u0026rsquo;}\\left|u(s\u0026rsquo;)-v(s\u0026rsquo;)\\right| \\\\ \u0026amp;\\leq\\gamma\\max_{(s,a)\\in\\mathcal{S}\\times\\mathcal{A}}\\sum_{s\u0026rsquo;\\in\\mathcal{S}}\\mathcal{P}^a_{ss\u0026rsquo;}\\Vert u-v\\Vert_\\infty \\\\ \u0026amp;=\\gamma\\Vert u-v\\Vert_\\infty, \\end{align} which tells us that $\\mathcal{T}^*$ is a $\\gamma$-contraction in $\\Vert\\cdot\\Vert_\\infty$. Theorem\nLet $v$ be the fixed point of $\\mathcal{T}^*$ and assume that there is policy $\\pi$ which is greedy w.r.t $v:\\mathcal{T}^\\pi v=\\mathcal{T}^* v$. Then $v=v_*$ and $\\pi$ is an optimal policy.\nProof\nPick any stationary policy $\\pi$. Then $\\mathcal{T}^\\pi\\leq\\mathcal{T}^*$ in the sense that for any function $v\\in B(\\mathcal{S})$, $\\mathcal{T}^\\pi v\\leq\\mathcal{T}^* v$ holds ($u\\leq v$ means that $u(s)\\leq v(s),\\forall s\\in\\mathcal{S}$).\nHence, for all $n\\geq0$, \\begin{equation} v_\\pi=\\mathcal{T}^\\pi v_\\pi\\leq\\mathcal{T}^*v_\\pi\\leq(\\mathcal{T}^*)^2 v_\\pi\\leq\\dots\\leq(\\mathcal{T}^*)^n v_\\pi \\end{equation} or \\begin{equation} v_\\pi\\leq(\\mathcal{T}^*)^n v_\\pi \\end{equation} Since $\\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\\mathcal{T}^*$. Thus, $v_\\pi\\leq v$. And since $\\pi$ was arbitrary, we obtain that $v_*\\leq v$.\nPick a policy $\\pi$ such that $\\mathcal{T}^\\pi v=\\mathcal{T}^*v$, then $v$ is also a fixed point of $\\mathcal{V}^\\pi$. Since $v_\\pi$ is the unique fixed point of $\\mathcal{T}^\\pi$, we have that $v=v_\\pi$, which shows that $v_*=v$ and that $\\pi$ is an optimal policy.\nReferences [1] Csaba Szepesvári. Algorithms for Reinforcement Learning.\n[2] A. Lazaric. Markov Decision Processes and Dynamic Programming.\n[3] What is the Bellman operator in reinforcement learning?. AI.StackExchange.\n[4] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[5] Normed vector space. Wikipedia.\nFootnotes A function is called uniformly bounded exactly when $\\Vert f\\Vert_\\infty\u0026lt;+\\infty$.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nA normed vector space is a vector space over the real or complex number, on which a norm is defined.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe details of sequences are mentioned in another note.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nProof\nPick any $v_0\\in\\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\\mathcal{T}$. c. Finally, we show that $\\mathcal{T}$ has a single fixed point. Assume that $\\mathcal{T}$ is a $\\gamma$-contraction.\na. To show that $(v_n)$ converges, it suffices to show that $(v_n)$ is a Cauchy sequence. We have: \\begin{align} \\Vert v_{n+1}-v_n\\Vert\u0026amp;=\\Vert\\mathcal{T}v_{n}-\\mathcal{T}v_{n-1}\\Vert \\\\ \u0026amp;\\leq\\gamma\\Vert v_{n}-v_{n-1}\\Vert \\\\ \u0026amp;\\quad\\vdots \\\\ \u0026amp;\\leq\\gamma^n\\Vert v_1-v_0\\Vert \\end{align} From the properties of norms, we have: \\begin{align} \\Vert v_{n+k}-v_n\\Vert\u0026amp;\\leq\\Vert v_{n+1}-v_n\\Vert+\\dots+\\Vert v_{n+k}-v_{n+k-1}\\Vert \\\\ \u0026amp;\\leq\\left(\\gamma^n+\\dots+\\gamma^{n+k-1}\\right)\\Vert v_1-v_0\\Vert \\\\ \u0026amp;=\\gamma^n\\dfrac{1-\\gamma^{k}}{1-\\gamma}\\Vert v_1-v_0\\Vert \\end{align} and so \\begin{equation} \\lim_{n\\to\\infty}\\sup_{k\\geq0}\\Vert v_{n+k}-v_n\\Vert=0, \\end{equation} shows us that $(v_n;n\\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.\nb. Recall that the definition of the sequence $(v_n;n\\geq0)$ \\begin{equation} v_{n+1}=\\mathcal{T}v_n \\end{equation} Taking the limes as $n\\to\\infty$ of both sides, one the one hand, we get that $v_{n+1}\\to _{\\Vert\\cdot\\Vert}v$. On the other hand, $\\mathcal{T}v_n\\to _{\\Vert\\cdot\\Vert}\\mathcal{T}v$, since $\\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\\mathcal{T}v$, which tells us that $v$ is a fixed point of $\\mathcal{T}$.\nc. Let us assume that $v,v\u0026rsquo;$ are both fixed points of $\\mathcal{T}$. Then, \\begin{align} \\Vert v-v\u0026rsquo;\\Vert\u0026amp;=\\Vert\\mathcal{T}v-\\mathcal{v\u0026rsquo;}\\Vert \\\\ \u0026amp;\\leq\\gamma\\Vert v-v\u0026rsquo;\\Vert \\\\ \\text{or}\\quad(1-\\gamma)\\Vert v-v\u0026rsquo;\\Vert\u0026amp;\\leq0 \\end{align} Thus, we must have that $\\Vert v-v\u0026rsquo;\\Vert=0$. Therefore, $v-v\u0026rsquo;=0$ or $v=v\u0026rsquo;$.\nAnd finally, \\begin{align} \\Vert v_n-v\\Vert\u0026amp;=\\Vert\\mathcal{T}v_{n-1}-\\mathcal{T}v\\Vert \\\\ \u0026amp;\\leq\\gamma\\Vert v_{n-1}-v\\Vert \\\\ \u0026amp;\\quad\\vdots \\\\ \u0026amp;\\leq\\gamma^n\\Vert v_0-v\\Vert \\end{align}\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIn the previous note about \u003ca href=\"https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/\"\u003e\u003cstrong\u003eMarkov Decision Processes, Bellman equations\u003c/strong\u003e\u003c/a\u003e, we mentioned that there exists a policy $\\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Optimal Policy Existence"},{"content":" When talking about measure, you might associate it with the idea of length, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with area, or even three dimensions with volume.\nDespite of having different number of dimensions, all length, area, and volume share the same properties:\nNon-negative: in principle, length, area, and volume can take any positive value. But negative length has no meaning. Same thing happens with negative area and negative volume. Additivity: to get from Hanoi to Singapore by air, you have to transit at Ho Chi Minh city (HCMC). If we cut that path into two non-overlapping pieces, say Hanoi - HCMC, and HCMC - Singapore, then the total length of the two pieces must be equal to the length of original path. If we divide a rectangular into non-overlapping pieces, the area of pieces combined must be the same as the original one. The same is true for volume as well. Empty set: an empty cup of water has volume zero. Other null sets: the length of a point is $0$. The area of a line, or a curve is $0$. The volume of a plane or a surface is also $0$. Translation invariance: length, area and volume are unchanged (invariant) under shifts (translation) in space. Hyper-rectangles: an interval of form $[a, b]\\subset\\mathbb{R}^3$ has length $b-a$. The area of a rectangle $[a_1,b_1]\\times[a_2,b_2]$ is $(b_1-a_1)(b_2-a_2)$. And the volume of a rectangular $[a_1,b_1]\\times[a_2,b_2]\\times[a_3,b_3]$ is $(b_1-a_1)(b_2-a_2)(b_3-a_3)$. Lebesgue Measure Is an extension of the classical notion of length in $\\mathbb{R}$, area in $\\mathbb{R}^2$ to any $\\mathbb{R}^k$ using k-dimensional hyper-rectangles.\nDefinition\nGiven an open set $S\\equiv\\sum_k(a_k,b_k)$ containing disjoint intervals, the Lebesgue measure is defined by: \\begin{equation} \\mu_L(S)\\equiv\\sum_{k}(b_k-a_k) \\end{equation} Given a closed set $S\u0026rsquo;\\equiv[a,b]-\\sum_k(a_k,b_k)$, \\begin{equation} \\mu_L(S\u0026rsquo;)\\equiv(b-a)-\\sum_k(b_k-a_k) \\end{equation}\nMeasures Definition\nLet $\\mathcal{X}$ be any set. A measure on $\\mathcal{X}$ is a function $\\mu$ that maps the set of subsets on $\\mathcal{X}$ to $[0,\\infty]$ ($\\mu:2^\\mathcal{X}\\rightarrow[0,\\infty]$) that satisfies:\n$\\mu(\\emptyset)=0$ Countable additivity property: for any countable and pairwise disjoint collection of subsets of $\\mathcal{X},\\mathcal{A_1},\\mathcal{A_2},\\dots$, we have \\begin{equation} \\mu\\left(\\bigcup_i\\mathcal{A_i}\\right)=\\sum_i\\mu(\\mathcal{A_i}) \\end{equation} $\\mu(\\mathcal{A})$ is called measure of the set $\\mathcal{A}$, or measure of $\\mathcal{A}$. Properties\nMonotonicity. If $\\mathcal{A}\\subset\\mathcal{B}$, then $\\mu(\\mathcal{A})\\leq\\mu(\\mathcal{B})$ Subadditivity. If $\\mathcal{A_1},\\mathcal{A_2},\\dots$ is a countable collection of sets, not necessarily disjoint, then \\begin{equation} \\mu\\left(\\bigcup_i\\mathcal{A_i}\\right)\\leq\\sum_i\\mu(\\mathcal{A_i}) \\end{equation} Examples\nCardinality of a set. $\\#\\mathcal{A}$ A point mass at $0$. Consider a measure $\\delta_{\\{0\\}}$ on $\\mathbb{R}$ defined to give measure $1$ to any set that contains $0$ and measure $0$ to any set that does not \\begin{equation} \\delta_{\\{0\\}}(\\mathcal{A})=\\#\\left(A\\cap\\{0\\}\\right)=\\begin{cases} 1\\quad\\textsf{if }0\\in\\mathcal{A} \\\\ 0\\quad\\textsf{otherwise} \\end{cases} \\end{equation} where $\\mathcal{A}\\subset\\mathbb{R}$. Counting measure on the integers. Consider a measure $\\mu_\\mathbb{Z}$ that assigns to each set $\\mathcal{A}$ the number of integers contained in $\\mathcal{A}$ \\begin{equation} \\delta_\\mathbb{Z}(\\mathcal{A})=\\#\\left(\\mathcal{A}\\cap\\mathbb{Z}\\right) \\end{equation} Geometric measure. Suppose that $0\\lt r\\lt 1$. We define a measure on $\\mathbb{R}$ that assigns to a set $\\mathcal{A}$ a geometrically weighted sum over non-negative integers in $\\mathcal{A}$ \\begin{equation} \\mu(\\mathcal{A})=\\sum_{i\\in\\mathcal{A}\\cap\\mathbb{Z}^+}r^i \\end{equation} Binomial measure. Let $n\\in\\mathbb{N}^+$ and let $0\\lt p\\lt 1$. We define $\\mu$ as: \\begin{equation} \\mu(\\mathcal{A})=\\sum_{k\\in\\mathcal{A}\\cap\\{0,1,\\dots,n\\}}{n\\choose k}p^k(1-p)^{n-k} \\end{equation} Bivariate Gaussian. We define a measure on $\\mathbb{R}^2$ by: \\begin{equation} \\mu({\\mathcal{A}})=\\int_\\mathcal{A}\\dfrac{1}{2\\pi}\\exp\\left({\\dfrac{-1}{2}(x^2+y^2)}\\right)\\,dx\\,dy \\end{equation} Uniform on a Ball in $\\mathbb{R}^3$. Let $\\mathcal{B}$ be the set of points in $\\mathbb{R}^3$ that are within a distance $1$ from the origin (unit ball in $\\mathbb{R}^3$). We define a measure on $\\mathbb{R}^3$ as: \\begin{equation} \\mu(\\mathcal{A})=\\dfrac{3}{4\\pi}\\mu_L(\\mathcal{A}\\cap\\mathcal{B}) \\end{equation} Integration with respect to a Measure: The Idea Consider $f:\\mathcal{X}\\rightarrow\\mathbb{R}$, where $\\mathcal{X}$ is any set and a measure $\\mu$ on $\\mathcal{X}$ and compute the integral of $f$ w.r.t $\\mu$: $\\int f(x)\\mu(dx)$. We have:\nFor any function $f$, \\begin{equation} \\int g(x)\\hspace{0.1cm}\\mu_L(dx)=\\int g(x)\\,dx, \\end{equation} since $\\mu_L(dx)\\equiv\\mu_L([x,x+dx[)=dx$ For any function $f$, \\begin{equation} \\int g(x)\\delta_{\\{\\alpha\\}}(dx)=g(\\alpha) \\end{equation} Consider the infinitesimal $\\delta_{\\{\\alpha\\}}(dx)$ as $x$ ranges over $\\mathbb{R}$. If $x\\neq\\alpha$, then the infinitesimal interval $[x,x+dx[$ does not contain $\\alpha$, so \\begin{equation} \\delta_{\\{\\alpha\\}}(dx)\\equiv\\delta_{\\{\\alpha\\}}([x,x+dx[)=0 \\end{equation} If $x=\\alpha,\\delta_{\\{\\alpha\\}}(dx)\\equiv\\delta_{\\{\\alpha\\}}([x,x+dx[)=1$. Thus, when we add up all of the infinitesimals, we get $g(\\alpha)\\cdot1=g(\\alpha)$ For any function $f$, \\begin{equation} \\int g(x)\\hspace{0.1cm}\\delta_\\mathbb{Z}(dx)=\\sum_{i\\in\\mathbb{Z}}g(i) \\end{equation} Similarly, consider the infinitesimal $\\delta_\\mathbb{Z}(dx)$ as $x$ ranges over $\\mathbb{R}$. If $x\\notin\\mathbb{Z}$, then $\\delta_\\mathbb{Z}(dx)\\equiv\\delta_\\mathbb{Z}([x,x+dx[)=0$. And otherwise if $x\\in\\mathbb{Z}$, $\\delta_\\mathbb{Z}(dx)\\equiv\\delta_\\mathbb{Z}([x,x+dx[)=1$ since an infinitesimal interval can contain at most one integer.\nHence, $g(x)\\hspace{0.1cm}\\delta_\\mathbb{Z}=g(x)$ if $x\\in\\mathbb{Z}$ and $=0$ otherwise. When we add up all of the infinitesimals over $x$, we get the sum above. Suppose $\\mathcal{C}$ is a countable set. We can define counting measure on $\\mathcal{C}$ to map $\\mathcal{A}\\rightarrow\\#(\\mathcal{A}\\cap\\mathcal{C})$ (recall that $\\delta_\\mathcal{C}(\\mathcal{A})=\\#(\\mathcal{A}\\cap\\mathcal{C})$). For any function $f$, \\begin{equation} \\int g(x)\\hspace{0.1cm}\\delta_\\mathcal{C}(dx)=\\sum_{v\\in\\mathcal{C}}g(v), \\end{equation} using the same basic argument as in the above example. From the above examples, we have that integrals w.r.t to Lebesgue measure are just ordinary integrals, and that integrals w.r.t Counting measure are just ordinary summation.\nConsider measures built from Lebesgue and Counting measure, we have:\nSuppose $\\mu$ is a measure that satisfies $\\mu(dx)=f(x)\\mu_L(dx)$, then for any function $g$, \\begin{equation} \\int g(x)\\mu(dx)=\\int g(x)f(x)\\mu_L(dx)=\\int g(x)f(x)\\,dx \\end{equation} We say that $f$ is the density of $\\mu$ w.r.t Lebesgue measure in this case. Suppose $\\mu$ is a measure that satisfies $\\mu(dx)=p(x)\\delta_\\mathcal{C}(dx)$ for a countable set $\\mathcal{C}$, then for any function g, \\begin{equation} \\int g(x)\\mu(dx)=\\int g(x)p(x)\\delta_\\mathcal{C}(dx)=\\sum_{v\\in\\mathcal{C}}g(v)f(v) \\end{equation} We say that $p$ is the density of $\\mu$ w.r.t Counting measure on $\\mathcal{C}$. Properties of the Integral A function is said to be integrable w.r.t $\\mu$ if \\begin{equation} \\int\\vert f(x)\\vert\\mu(dx)\u0026lt;\\infty \\end{equation} An integrable function has a well-defined and finite integral. If $f(x)\\geq0$, the integral is always well-defined but may be $\\infty$.\nSuppose $\\mu$ is a measure on $\\mathcal{X},\\mathcal{A}\\subset\\mathcal{X}$, and $g$ is a real-valued function on $\\mathcal{X}$. We define the integral of $g$ over the set $\\mathcal{A}$, denoted by $\\int_\\mathcal{A}g(x)\\hspace{0.1cm}\\mu(dx)$, as \\begin{equation} \\int_\\mathcal{A}g(x)\\mu(dx)=\\int g(x)𝟙_\\mathcal{A}(x)\\hspace{0.1cm}\\mu(dx), \\end{equation} where $𝟙_\\mathcal{A}$ is an indicator function ($𝟙_\\mathcal{A}(x)=1$ if $x\\in\\mathcal{A}$, and $=0$ otherwise).\nLet $\\mu$ is a measure on $\\mathcal{X},\\mathcal{A},\\mathcal{B}\\subset\\mathcal{X},c\\in\\mathbb{R}$ and $f,g$ are integrable functions. The following properties hold for every $\\mu$\nConstant functions. \\begin{equation} \\int_\\mathcal{A}c\\,\\mu(dx)=c\\cdot\\mu(\\mathcal{A}) \\end{equation} Linearity. \\begin{align} \\int_\\mathcal{A}cf(x)\\mu(dx)\u0026=c\\int_\\mathcal{A}f(x)\\mu(dx) \\\\ \\int_\\mathcal{A}\\big(f(x)+g(x)\\big)\\mu(dx)\u0026=\\int_\\mathcal{A}f(x)\\mu(dx)+\\int_\\mathcal{A}g(x)\\mu(dx) \\end{align} Monotonicity. If $f\\leq g$, then \\begin{equation} \\int_\\mathcal{A}f(x)\\mu(dx)\\leq\\int_\\mathcal{A}g(x)\\mu(dx),\\forall\\mathcal{A}, \\end{equation} which implies: If $f\\geq0$, then $\\int f(x)\\mu(dx)\\geq0$. If $f\\geq0$ and $\\mathcal{A}\\subset\\mathcal{B}$, then $\\int_\\mathcal{A}f(x)\\mu(dx)\\leq\\int_\\mathcal{B}f(x)\\mu(dx)$. Null sets. If $\\mu(\\mathcal{A})=0$, then $\\int_\\mathcal{A}f(x)\\mu(dx)=0$. Absolute values. \\begin{equation} \\left\\vert\\int f(x)\\mu(dx)\\right\\vert\\leq\\int\\left\\vert f(x)\\right\\vert\\mu(dx) \\end{equation} Monotone convergence. If $0\\leq f_1\\leq f_2\\leq\\dots$ is an increasing sequence of integrable functions that converge to $f$, then \\begin{equation} \\lim_{k\\to\\infty}\\int f_k(x)\\mu(dx)=\\int f(x)\\mu(dx) \\end{equation} Linearity in region of integration. If $\\mathcal{A}\\cap\\mathcal{B}=\\emptyset$, \\begin{equation} \\int_{\\mathcal{A}\\cup\\mathcal{B}}f(x)\\mu(dx)=\\int_\\mathcal{A}f(x)\\mu(dx)+\\int_\\mathcal{B}f(x)\\mu(dx) \\end{equation} Integration with respect to a Measure: The Details Step 1.\n- Define the integral for simple functions, i.e. functions that take only a finite number of different values and have following properties: All constant functions are simple functions. The indicator function ($𝟙_\\mathcal{A}$) of a set $\\mathcal{A}\\subset\\mathcal{X}$ is a simple function (taking values in $\\{0,1\\}$). Any constant times an indicator ($c𝟙_\\mathcal{A}$) is also a simple function (taking values in $\\{0,c\\}$). Similarly, given disjoint sets $\\mathcal{A_1},\\mathcal{A_2}$, the linear combination $c_1𝟙_\\mathcal{A_1}+c_2𝟙_\\mathcal{A_2}$ is a simple function (taking values in $\\{0,c_1,c_2\\}$)[^1]. In fact, any simple function can be expressed as a linear combination of a finite number of indicator functions. That is, if $f$ is *any* simple function on $\\mathcal{X}$, then there exists some finite integer $n$, non-zero constants $c_1,\\dots,c_n$ and *disjoint* sets $\\mathcal{A_1},\\dots\\mathcal{A_n}\\subset\\mathcal{X}$ such that \\begin{equation} f=c_1𝟙\\_\\mathcal{A_1}+\\dots+c_n𝟙\\_\\mathcal{A_n} \\end{equation} - So, if $f:\\mathcal{X}\\to\\mathbb{R}$ is a simple function as just defined, we have that \\begin{equation} \\int \\mu(dx)=c_1\\mu(\\mathcal{A_1})+\\dots+c_n\\mu(\\mathcal{A_n})\\ \\end{equation} Step 2.\n- Define the integral for general non-negative functions, approximating the general function by simple functions.\n- The idea is that we can approximate any general non-negative function $f:\\mathcal{X}\\to[0,\\infty[$ well by some non-negative simple functions that $\\leq f$[^2].\n- If $f:\\mathcal{X}\\to[0,\\infty[$ is a general function and $0\\leq s\\leq f$ is a simple function (then $\\int s(x)\\mu(dx)\\leq\\int f(x)\\mu(dx)$). The closer that $s$ approximates $f$, the closer we expect $\\int s(x)\\mu(dx)$ and $\\int f(x)\\mu(x)$ to be.\n- To be more precise, we define the integral $\\int f(x)\\mu(dx)$ to be the smallest value $I$ such that $\\int s(x)\\mu(x)\\leq I$, for all simple functions $0\\leq s\\leq f$. \\begin{equation} \\int f(x)\\mu(dx)\\approx\\sup\\left\\{\\int s(x)\\mu(dx)\\right\\} \\end{equation} Step 3.\n- Define the integral for general real-valued functions by separately integrating the positive and negative parts of the function.\nIf $f:\\mathcal{X}\\to\\mathbb{R}$ is a general function, we can define its positive part $f^+$ and its negative part $f^-$ by \\begin{align} f^+(x)\u0026=\\max\\left(f(x),0\\right) \\\\ f^-(x)\u0026=\\max\\left(-f(x),0\\right) \\end{align} - Since both $f^+$ and $f^-$ are non-negative functions and $f=f^+-f^-$, we have \\begin{equation} \\int f(x)\\mu(dx)=\\int f^+(x)\\mu(dx)-\\int f^-(x)\\mu(dx) \\end{equation} - This is a well-defined number (possibly infinite) if and only if at least one of $f^+$ and $f^-$ has a finite integral. Constructing Measures from old ones Sums and multiples.\n- Consider the point mass measures at $0$ and $1$, $\\delta_{\\{0\\}},\\delta_{\\{1\\}}$, and construct a two new measures on $\\mathbb{R}$, $\\mu=\\delta_{\\{0\\}}+\\delta_{\\{1\\}}$ and $v=4\\delta_{\\{0\\}}$, defined by \\begin{align} \\mu(\\mathcal{A})\u0026=\\delta_{\\{0\\}}(\\mathcal{A})+\\delta_{\\{0\\}}(\\mathcal{A}) \\\\ v(\\mathcal{A})\u0026=4\\delta_{\\{0\\}}(\\mathcal{A}) \\end{align} - The measure $\\mu$ counts how many elements of $\\{0,1\\}$ are in its argument. Thus, the counting measure of the integers can be re-expressed as \\begin{equation} \\delta_\\mathbb{Z}=\\sum_{i=-\\infty}^{\\infty}\\delta_{\\{i\\}} \\end{equation} - By combining the operations of summation and multiplication, we can write the Geometric measure in the above example \\begin{equation} \\sum_{i=0}^{\\infty}r^i\\delta_{\\{i\\}} \\end{equation} Restriction to a subset.\nSuppose $\\mu$ is a measure on $\\mathcal{X}$ and $\\mathcal{B}\\subset\\mathcal{X}$. We can define a new measure on $\\mathcal{B}$ which maps $\\mathcal{A}\\subset\\mathcal{B}\\to\\mu(\\mathcal{A})$. This is called the restriction of $\\mu$ to the set $\\mathcal{B}$. Measure induced by a function. - Suppose $\\mu$ is a measure on $\\mathcal{X}$ and $g:\\mathcal{X}\\to\\mathcal{Y}$. We can use $\\mu$ and $g$ to define a new measure $v$ on $\\mathcal{Y}$ by \\begin{equation} v(\\mathcal{A})=\\mu(g^{-1}(\\mathcal{A})), \\end{equation} for $\\mathcal{A}\\subset\\mathcal{Y}$. This is called the *measure induced from $\\mu$ by $g$*. - Therefore, for any $f:\\mathcal{Y}\\to\\mathbb{R}$, \\begin{equation} \\int f(y)\\hspace{0.1cm}v(dy)=\\int f(g(x))\\hspace{0.1cm}\\mu(dx) \\end{equation} Integrating a density.\n- Suppose $\\mu$ is a measure on $\\mathcal{X}$ and $f:\\mathcal{X}\\to\\mathbb{R}$. We can define a new measure $v$ on $\\mathcal{X}$ as \\begin{equation} v(\\mathcal{A})=\\int_\\mathcal{A}f(x)\\hspace{0.1cm}\\mu(dx)\\label{eq:1} \\end{equation} - We say that $f$ is the density of the measure $v$ w.r.t $\\mu$.\n- If $v,\\mu$ are measures for which the equation \\eqref{eq:1} holds for every $\\mathcal{A}\\subset\\mathcal{X}$, we say that $v$ has a density $f$ w.r.t $\\mu$. This implies two useful results:\n$\\mu(\\mathcal{A})=0$ implies $v(\\mathcal{A})=0$. $v(dx)=f(x)\\hspace{0.1cm}\\mu(dx)$. Other types of Measures Suppose that $\\mu$ is a measure on $\\mathcal{X}$\nIf $\\mu(\\mathcal{X})=\\infty$, we say that $\\mu$ is an infinite measure. If $\\mu(\\mathcal{X}\u003c\\infty)$, we say that $\\mu$ is a finite measure. If $\\mu(\\mathcal{X}\u003c1)$, we say that $\\mu$ is a probability measure. If there exists a countable set $\\mathcal{S}$ such that $\\mu(\\mathcal{X}-\\mathcal{S})=0$, we say that $\\mu$ is a discrete measure. Equivalently, $\\mu$ has a density w.r.t counting measure on $\\mathcal{S}$. If $\\mu$ has a density w.r.t Lebesgue measure, we say that $\\mu$ is a continuous measure. If $\\mu$ is neither continuous nor discrete, we say that $\\mu$ is a mixed measure. References [1] Literally, this note is mainly written from a source that I\u0026rsquo;ve lost the reference :(. Hope that I can update this line soon.\n[2] Lebesgue Measure.\n[3] Measure Theory for Probability: A Very Brief Introduction.\nOther Resources Music and Measure Theory - 3Blue1Brown - this is my favourite Youtube channel. Footnotes","permalink":"https://trunghng.github.io/posts/measure-theory/measure/","summary":"\u003cblockquote\u003e\n\u003cp\u003eWhen talking about \u003cstrong\u003emeasure\u003c/strong\u003e, you might associate it with the idea of \u003cstrong\u003elength\u003c/strong\u003e, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with \u003cstrong\u003earea\u003c/strong\u003e, or even three dimensions with \u003cstrong\u003evolume\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Measures"},{"content":" You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.\nWhat is Reinforcement Learning? Say, there is an unknown environment that we\u0026rsquo;re trying to put an agent on. By interacting with the agent through taking actions that gives rise to rewards continually, the agent learns a policy that maximize the cumulative rewards.\nReinforcement Learning (RL), roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called policy) for the agent from experimental trials and relative simple feedback received. With the optimal policy, the agent is capable to actively adapt to the environment to maximize future rewards. Markov Decision Processes (MDPs) Markov decision processes (MDPs) formally describe an environment for RL. And almost all RL problems can be formalized as MDPs.\nDefinition (MDP)\nA Markov Decision Process is a tuple $⟨\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma⟩$\n$\\mathcal{S}$ is a set of states called state space $\\mathcal{A}$ is a set of actions called action space $\\mathcal{P}$ is a state transition probability matrix\n$$\\mathcal{P}^a_{ss\u0026rsquo;}=P(S_{t+1}=s\u0026rsquo;|S_t=s,A_t=a)$$ $\\mathcal{R}$ is a reward function\n$$\\mathcal{R}^a_s=\\mathbb{E}\\left[R_{t+1}|S_t=s,A_t=a\\right]$$ $\\gamma\\in[0, 1]$ is a discount factor for future reward MDP is an extension of a Markov chain. If only one action exists for each state, and all rewards are the same, an MDP reduces to a Markov chain. All states in MDP has Markov property, referring to the fact that the current state captures all relevant information from the history. \\begin{equation} P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\\dots,S_t) \\end{equation}\nReturn In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the expected return.\nDefinition (Return)\nThe return $G_t$ is the total discounted reward from $t$ \\begin{equation} G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}, \\end{equation} where $\\gamma\\in[0,1]$ is called discount rate (or discount factor).\nThe discount rate $\\gamma$ determines the present value of future rewards: a reward received k time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\\rightarrow\\infty$ then $\\gamma^k\\rightarrow 0$.\nPolicy Policy, which is denoted as $\\pi$, is the behavior function of the agent. $\\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either deterministic or stochastic.\nDeterministic policy:\t$\\quad\\pi(s)=a$ Stochastic policy: $\\quad\\pi(a|s)=P(A_t=a|S_t=s)$ Value Function Value function measures how good a particular state is (or how good it is to perform a given action in a given state).\nDefinition (state-value function)\nThe state-value function of a state $s$ under a policy $\\pi$, denoted as $v_\\pi(s)$, is the expected return starting from state $s$ and following $\\pi$ thereafter: \\begin{equation} v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s] \\end{equation}\nDefinition (action-value function)\nSimilarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted as $q_\\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$: \\begin{equation} q_\\pi(s,a)=\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] \\end{equation}\nSince we follow the policy $\\pi$, we have that \\begin{equation} v_\\pi(s)=\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a)\\pi(a|s) \\end{equation}\nOptimal Policy and Optimal Value Function For finite MDPs (finite state and action space), we can precisely define an optimal policy. Value functions define a partial ordering over policies. A policy $\\pi$ is defined to be better than or equal to a policy $\\pi\u0026rsquo;$ if its expected return is greater than or equal to that of $\\pi\u0026rsquo;$ for all states. In other words, \\begin{equation} \\pi\\geq\\pi\u0026rsquo;\\iff v_\\pi(s)\\geq v_{\\pi\u0026rsquo;} \\forall s\\in\\mathcal{S} \\end{equation}\nTheorem (Optimal policy)\nFor any MDP, there exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, \\begin{equation} \\pi_*\\geq\\pi,\\forall\\pi \\end{equation}\nThe proof of the above theorem is provided in another note since we need some additional tools to do that.\nThere may be more than one optimal policy, they share the same state-value function, called optimal state-value function though. \\begin{equation} v_*(s)=\\max_{\\pi}v_\\pi(s) \\end{equation} Optimal policies also share the same action-value function, called optimal action-value function \\begin{equation} q_*(s,a)=\\max_{\\pi}q_\\pi(s,a) \\end{equation}\nBellman Equations A fundamental property of value functions used throughout RL is that they satisfy recursive relationships \\begin{align} v_\\pi(s)\u0026amp;\\doteq \\mathbb{E}_\\pi[G_t|S_t=s] \\\\\u0026amp;=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s] \\\\\u0026amp;=\\sum_{s\u0026rsquo;,r,g\u0026rsquo;,a}p(s\u0026rsquo;,r,g\u0026rsquo;,a|s)(r+\\gamma g\u0026rsquo;) \\\\\u0026amp;=\\sum_{a}p(a|s)\\sum_{s\u0026rsquo;,r,g\u0026rsquo;}p(s\u0026rsquo;,r,g\u0026rsquo;|a,s)(r+\\gamma g\u0026rsquo;) \\\\\u0026amp;=\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r,g\u0026rsquo;}p(s\u0026rsquo;,r|a,s)p(g\u0026rsquo;|s\u0026rsquo;,r,a,s)(r+\\gamma g\u0026rsquo;) \\\\\u0026amp;=\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|a,s)\\sum_{g\u0026rsquo;}p(g\u0026rsquo;|s\u0026rsquo;)(r+\\gamma g\u0026rsquo;) \\\\\u0026amp;=\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|a,s)\\left[r+\\gamma\\sum_{g\u0026rsquo;}p(g\u0026rsquo;|s\u0026rsquo;)g\u0026rsquo;\\right] \\\\\u0026amp;=\\sum_{a}\\pi(a|s)\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|a,s)\\left[r+\\gamma v_\\pi(s\u0026rsquo;)\\right], \\end{align} where $p(s\u0026rsquo;,r|s,a)=P(S_{t+1}=s\u0026rsquo;,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the Bellman equation for $v_\\pi(s)$. It expresses a relationship between the value state $s$, $v_\\pi(s)$ and the values of its successor states $s\u0026rsquo;$, $v_\\pi(s\u0026rsquo;)$.\nSimilarly, we define the Bellman equation for $q_\\pi(s,a)$ \\begin{align} q_\\pi(s,a)\u0026amp;\\doteq\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] \\\\\u0026amp;=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\u0026amp;=\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma\\sum_{a\u0026rsquo;}\\pi(a\u0026rsquo;|s\u0026rsquo;)q_\\pi(s\u0026rsquo;,a\u0026rsquo;)\\right] \\end{align}\nBellman Backup Diagram Backup diagram of state-value function and action-value function respectively Figure 1: Backup diagram of state-value function Figure 2: Backup diagram of action-value function Bellman Optimality Equations Since $v_*$ is the value function for a policy, it must satisfy the Bellman equation for state-values. Moreover, it is also the optimal value function, then we have \\begin{align} v_*(s)\u0026amp;=\\max_{a\\in\\mathcal{A(s)}}q_{\\pi_*}(s,a) \\\\\u0026amp;=\\max_{a}\\mathbb{E}_{\\pi_*}[G_t|S_t=s,A_t=a] \\\\\u0026amp;=\\max_{a}\\mathbb{E}_{\\pi_*}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\u0026amp;=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\\\\u0026amp;=\\max_{a}\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)[r+\\gamma v_*(s\u0026rsquo;)] \\end{align} The last two equations are two forms of the Bellman optimality equation for $v_*$. Similarly, we have the Bellman optimality equation for $q_*$ \\begin{align} q_*(s,a)\u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma\\max_{a\u0026rsquo;}q_*(S_{t+1},a\u0026rsquo;)|S_t=s,A_t=a\\right] \\\\\u0026amp;=\\sum_{s\u0026rsquo;,r}p(s\u0026rsquo;,r|s,a)\\left[r+\\gamma\\max_{a\u0026rsquo;}q_*(s\u0026rsquo;,a\u0026rsquo;)\\right] \\end{align}\nBackup diagram for $v_*$ and $q_*$ Figure 3: Backup diagram of optimal value functions References [1] Richard S. Sutton \u0026amp; Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] David Silver. UCL course on RL.\n[3] Lilian Weng A (Long) Peek into Reinforcement Learning. Lil\u0026rsquo;Log, 2018.\n[4] AlphaGo. Deepmind.\n","permalink":"https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/","summary":"\u003cblockquote\u003e\n\u003cp\u003eYou may have known or heard vaguely about a computer program called \u003cstrong\u003eAlphaGo\u003c/strong\u003e - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called \u003cstrong\u003eself-play\u003c/strong\u003e against its other instances, with \u003cstrong\u003eReinforcement Learning\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Markov Decision Processes, Bellman equations"},{"content":" If we have to describe the defintition of Markov chain in one statement, it will be: \u0026ldquo;It only matters where you are, not where you\u0026rsquo;ve been\u0026rdquo;.\nMarkov Property Markov chain1 is a stochastic process in which the random variables follow a special property called Markov.\nA sequence of random variables $X_0, X_1, X_2, \\dots$ taking values in the state space $\\mathcal{S}=\\{1, 2,\\dots,M\\}$ such that for all $n\\geq0$, \\begin{equation} P(X_{n+1}=j|X_n=i)=P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},X_{n-2}=i_{n-2},\\dots,X_0=i_0) \\end{equation} In other words, knowledge of the preceding state is all we need to determine the probability distribution of the current state.\nTransition Matrix The quantity $P(X_{n+1}=j|X_n=i)$ is transition probability from state $i$ to $j$.\nIf we denote that $q_{ij}=P(X_{n+1}=j|X_n=i)$ and let $Q$ $M\\times M$ matrix, defined as \\begin{equation} Q=\\left[\\begin{matrix}q_{11}\u0026amp;\\ldots\u0026amp;q_{1M} \\\\ \\vdots\u0026amp;\\ddots\u0026amp;\\vdots \\\\ q_{M1}\u0026amp;\\ldots\u0026amp;q_{MM}\\end{matrix}\\right] \\end{equation} The matrix $Q$ then is referred as the transition matrix of the chain.\nIt is noticeable that each row of $Q$ is a conditional probability mass function (PMF) of $X_{n+1}$ given $X_n$. And hence, sum of its entries is 1.\n$n$-step Transition Probability The $n$-step transition probability from $i$ to $j$ is the probability of being at $i$ and $n$ steps later being at $j$, and be denoted as $q_{ij}^{(n)}$, \\begin{equation} q_{ij}^{(n)}=P(X_n=j|X_0=i) \\end{equation} We have that \\begin{equation} q_{ij}^{(2)}=\\sum_{k}^{}q_{ik}q_{kj} \\end{equation} since it has to go through an intermediary step $k$ to reach $j$ in 2 steps from $i$. It is easily seen that the right hand side is $Q_{ij}^2$. And by induction, we have that: \\begin{equation} q_{ij}^{(n)}=Q_{ij}^{n} \\end{equation} $Q^n$ is also called the $n$-step transition matrix.\nMarginal Distribution of $X_n$ Let $t=(t_1,\\dots,t_M)^\\text{T}$, where $t_i=P(X_0=i)$. By the law of total probability (LOTP), we have that: \\begin{equation} P(X_n=j)=\\sum_{i=1}^{M}P(X_0=i)P(X_n=j|X_0=i)=\\sum_{i=1}^{M}t_iq_{ij}^{(n)}, \\end{equation} which implies that the marginal distribution of $X_n$ is given by $tQ^n$.\nProperties State $i$ of a Markov chain is defined as recurrent or transient depending upon whether or not the Markov chain will eventually return to it. Starting with recurrent state $i$, the chain will return to it with the probability of $1$. Otherwise, it is transient.\nProposition: Number of returns to transient state is distributed by $\\text{Geom}(p)$, with $p\u003e0$ is the probability of never returning to $i$. A Markov chain is defined as irreducible if there exists a chain of steps between any $i,j$ that has positive probability. That is for any $i,j$, there is some $n\u003e0,\\in\\mathbb{N}$ such that $Q^n_{ij}\u003e0$. If not irreducible, the chain is instead referred as reducible.\nProposition: irreducible implies all states recurrent. A state $i$ has period $k\u003e0$ if \\begin{equation} k=\\text{gcd}(n), \\end{equation} where $n$ is possible number of steps it can take to return to $i$ when starting at $i$, or $Q^n_{ii}\u003e0$.\nState $i$ is known as aperiodic if $k_i=1$, and periodic otherwise. The chain itself is called aperiodic if all its states are aperiodic, and periodic otherwise. Stationary Distribution A vector $s=(s_1,\\dots,s_M)^\\text{T}$ such that $s_i\\geq0$ and $\\sum_{i}s_i=1$ is a stationary distribution for a Markov chain if \\begin{equation} \\sum_{i}s_iq_{ij}=s_j \\end{equation} for all $j$, or equivalently $sQ=s$.\nTheorem (Existence and uniqueness of stationary distribution)\nAny irreducible Markov chain has a unique stationary distribution. In this distribution, every state has positive probability.\nThe theorem is a consequence of a result from Perron-Frobenius theorem.\nTheorem (Convergence to stationary distribution)\nLet $X_0,X_1,\\dots$ be a Markov chain with stationary distribution $s$ and transition matrix $Q$, such that some power $Q^m$ has all entries positive (or in the other words, the chain is irreducible and aperiodic). Then \\begin{equation} P(X_n=i)\\to s_i \\end{equation} as $n\\rightarrow\\infty$ (or $Q^n$ converges to a matrix in which each row is $s$).\nTheorem (Expected time to run)\nLet $X_0,X_1,\\dots$ be an irreducible Markov chain with stationary distribution $s$. Let $r_i$ be the expected time it takes the chain to return to $i$, given that it starts at $i$. Then \\begin{equation} s_i=\\frac{1}{r_i} \\end{equation}\nReversibility Let $Q=(q_{ij})$ denote transition matrix of a Markov chain. Suppose there is an $s=(s_1,\\dots,s_M)^\\text{T}$ with $s_i\\geq0,\\sum_{i}s_i=1$, such that \\begin{equation} s_iq_{ij}=s_jq_{ji} \\end{equation} for all states $i,j$. This equation is called reversibility or detailed balance condition. And if the condition holds, we say that the chain is reversible w.r.t $s$.\nProposition (Reversible implies stationary)\nSuppose that $Q=(q_{ij})$ be the transition matrix of a Markov chain that is reversible w.r.t to an $s=(s_1,\\dots,s_M)^\\text{T}$ with $s_i\\geq0,\\sum_{i}s_i=1$. Then $s$ is a stationary distribution of the chain.\nProof\nWe have that \\begin{equation} \\sum_{j}s_jq_{ji}=\\sum_{j}s_iq_{ij}=s_i\\sum_{j}q_{ij}=s_i \\end{equation}\nProposition\nIf each column of $Q$ sum to $1$, then the Uniform distribution over all states $(1/M,\\dots,1/M)$, is a stationary distribution (this kind of matrix is called doubly stochastic matrix).\nExamples and Applications Finite-state machines, random walks Diced board games such as Ludo, Monopoly,\u0026hellip; Google PageRank - the heart of Google search Markov Decision Process (MDP). And various other applications. References [1] Joseph K. Blitzstein \u0026amp; Jessica Hwang. Introduction to Probability.\n[2] Brillant\u0026rsquo;s Markov chain.\n[3] Perron-Frobenius theorem.\nFootnotes The Markov chain here is time-homogeneous Markov chain, in which the probability of any state transition is independent of time.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://trunghng.github.io/posts/probability-statistics/markov-chain/","summary":"\u003cblockquote\u003e\n\u003cp\u003eIf we have to describe the defintition of \u003cstrong\u003eMarkov chain\u003c/strong\u003e in one statement, it will be: \u0026ldquo;It only matters where you are, not where you\u0026rsquo;ve been\u0026rdquo;.\u003c/p\u003e\n\u003c/blockquote\u003e","title":"Markov Chain"},{"content":" Enjoy my index-zero-ed note while staying tuned for next ones!\nfrom math import sqrt def fibonacci(i) \u0026#34;\u0026#34;\u0026#34; generate i-th number of the Fibonacci sequence, python code obvs :p \u0026#34;\u0026#34;\u0026#34; return 1 / sqrt(5) * (pow((1 + sqrt(5)) / 2, i) - pow((1 - sqrt(5)) / 2, i)) Why did numbers $\\frac{1+\\sqrt{5}}{2}$ and $\\frac{1-\\sqrt{5}}{2}$ come out of nowhere?\nIn fact, these two numbers are eigenvalues of matrix $A=\\left(\\begin{smallmatrix}1 \u0026amp; 1\\\\1 \u0026amp; 0\\end{smallmatrix}\\right)$, which is retrieved from \\begin{equation} u_{k+1}=Au_k, \\end{equation} where $u_k=\\left(\\begin{smallmatrix}F_{k+1} \\\\ F_k\\end{smallmatrix}\\right)$. And thus, $u_k=A^k u_0$.\nThen, the thing is, how can we compute $A^k$ quickly? This is where diagonalizing plays its role. Diagonalizing produces a factorization: \\begin{equation} A=S\\Lambda S^{-1}, \\end{equation} where $S=\\left(\\begin{smallmatrix}x_1 \u0026amp; \\dots \u0026amp; x_n\\end{smallmatrix}\\right)$ is referred as the eigenvector matrix, $\\Lambda=\\left(\\begin{smallmatrix}\\lambda_1\u0026amp;\u0026amp;\\\\\u0026amp;\\ddots\u0026amp;\\\\\u0026amp;\u0026amp;\\lambda_n\\end{smallmatrix}\\right)$ is a diagonal matrix established from eigenvalues of $A$.\nWhen taking the power of $A$, i.e. \\begin{equation} A^k u_0=(S\\Lambda S^{-1})\\dots(S\\Lambda S^{-1})u_0=S\\Lambda^k S^{-1} u_0, \\end{equation} writing $u_0$ as a combination $c_1x_1+\\dots+c_nx_n$ of the eigenvectors, we have that $c=S^{-1}u_0$. Hence: \\begin{equation} u_k=A^ku_0=c_1{\\lambda_1}^kx_1+\\dots+c_n{\\lambda_n}^kx_n \\end{equation} Fact: The $\\frac{1+\\sqrt{5}}{2}\\approx 1.618$ is so-called \u0026ldquo;golden ratio\u0026rdquo;. And for some reason a rectangle with sides 1.618 and 1 looks especially graceful.\nReferences [1] Gilbert Strang. Introduction to Linear Algebra, 5th edition. 2016.\n[2] MIT 18.06. Linear Algebra.\n","permalink":"https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/","summary":"\u003cblockquote\u003e\n\u003cp\u003eEnjoy my index-zero-ed note while staying tuned for next ones!\u003c/p\u003e\n\u003c/blockquote\u003e","title":"My very first post"}]