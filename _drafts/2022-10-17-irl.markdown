---
layout: post
title:  "Inverse Reinforcement Learning"
date:   2022-10-17 13:00:00 +0700
tags: reinforcement-learning inverse-reinforcement-learning
description: Inverse Reinforcement Learning
comments: true
eqn-number: true
---
> A note on Inverse Reinforcement Learning

<!-- excerpt-end -->

- [Preliminaries](#preliminaries)
	- [Markov decision processes](#mdp)
	- [Inverse reinforcement learning](#irl)
- [Max margin methods](#max-margin-methods)
	- [Max-margin & Projection](#max-margin-proj)
		- [Max-margin](#max-margin)
		- [Projection](#proj)
	- [Max margin planning](#max-margin-pln)
	- [LEARCH](#learch)
- [Max entropy methods](#max-ent)
- [Bayesian methods](#bayes)
	- [BIRL](#birl)
- [References](#references)
- [Footnotes](#footnotes)

## Preliminaries
{: #preliminaries}

	
### Markov decision processes
{: #mdp}
We begin by recalling the definition of [**Markov decision processes (MDP)**]({% post_url 2021-06-27-mdp-bellman-eqn %}#mdp).

A **Markov decision process**, or **MDP** is defined to be a tuple $(\mathcal{S},\mathcal{A},T,r,\gamma)$, where
- $\mathcal{S}$ is a set of states, represents the **state space**.
- $\mathcal{A}$ is a set of actions, known as the **action space**.
- $T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the **state transition probabilities**, i.e., $T(s',s,a)=p(s'\vert s,a)$ denotes the probability of transitioning to state $s'$ when taking action $a$ from state $s$.
- $r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ denotes the **reward function**.
- $\gamma\in[0,1]$ is referred as **discount factor**.

A **policy**, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\\{0,1\\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$.

For a policy $\pi$, the **state value function**, denoted as $v_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$. Specifically, $v_\pi(s)$ is defined as the expected sum of discounted rewards when starting in $s$ and following $\pi$. Specifically, for all $s\in\mathcal{S}$
\begin{equation}
v_\pi(s)=\mathbb{E}\_\pi\left[\sum_{k=0}^{\infty}\gamma^k r(S_t,A_t)\Big\vert S_t=s\right]
\end{equation}
The expectation of value of start state $S_0$ presents the value of the policy it is following. In particular, the value of a policy $\pi$ can be given by
\begin{equation}
\mathbb{E}\big[v_\pi(S_0)\big]=\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t r(S_t,A_t)\Big\vert S_0\right]\label{eq:mdp.1}
\end{equation}
Analogously, the **state-action value function**, denoted by $q_\pi$, of a state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$ specifies how good it is to take action $a$ from state $s$. Similar to $v_\pi$, $q_\pi$ is defined as the expected sum of discounted rewards when starting from $s$, taking action $a$ and thereby following policy $\pi$.

### Inverse reinforcement learning
{: #irl}
In **reverse reinforcment learning**, we are instead working on an interface of tuple $(\mathcal{S},\mathcal{A},T,\gamma)$, which represents an $MDP$ without a predefined reward function $r$, or MDP\R for short. Rather than an explicit reward function given, we are provided samples $\\{\tau_i\\}$ sampled from an optimal policy $\pi^\*(\tau)$.

In this problem, our goal is to learn a reward function $r_\boldsymbol{\psi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ parameterized by the weight vector $\boldsymbol{\psi}$, and then use it to learn the optimal policy $\pi^\*$. Specifically, for a linear reward function, we have
\begin{equation}
r_\boldsymbol{\psi}(s,a)=\sum_{i}\psi_i f_i(s,a)=\boldsymbol{\psi}^\text{T}\mathbf{f}(s,a),
\end{equation}
where the weight vector is defined as 
\begin{equation}
\boldsymbol{\psi}=(\psi_1,\ldots,\psi_k)^\text{T},
\end{equation}
and where $\mathbf{f}(s,a)$ is called the **feature vector** of the state-action pair $(s,a)$, defined by
\begin{equation}
\mathbf{f}(s,a)=\big(f_1(s,a),\ldots,f_k(s,a)\big)^\text{T}
\end{equation}
By \eqref{eq:mdp.1}, the value of a policy $\pi_{r_\boldsymbol{\psi}}$ corresponding to the reward function $r_\boldsymbol{\psi}$ thus can be written as
\begin{align}
\mathbb{E}\big[v_{\pi_{r_\boldsymbol{\psi}}}(S_0)\big]&=\mathbb{E}\_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t r(S_t,A_t)\Big\vert S_0\right] \\\\ &=\mathbb{E}\_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\boldsymbol{\psi}^\text{T}\mathbf{f}(S_t,A_t)\Big\vert S_0\right] \\\\ &=\boldsymbol{\psi}^\text{T}\mathbb{E}\_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\mathbf{f}(S_t,A_t)\Big\vert S_0\right]
\end{align}
The **feature expectation** or also called the expected discounted accumulated feature value, denoted as $\boldsymbol{\mu}(\pi)$, of the policy $\pi_{r_\boldsymbol{\psi}}$ is defined as
\begin{equation}
\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})=\mathbb{E}\_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\mathbf{f}(S_t,A_t)\Big\vert S_0\right]
\end{equation}
Hence, we have that
\begin{equation}
\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})=\boldsymbol{\psi}^\text{T}\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})
\end{equation}

## Max margin methods
{: #max-margin-methods}

### Max-margin & Projection
{: #max-margin-proj}


#### Max-margin
{: #max-margin}

#### Projection
{: #proj}

### Max margin planning
{: #max-margin-pln}

### LEARCH
{: #learch}

## Max entropy methods
{: #max-ent}

## References
{: #references}
[1] Pieter Abbeel & Andrew Y. Ng. [Apprenticeship Learning via Inverse Reinforcement Learning](https://doi.org/10.1145/1015330.1015430). ICML '04: Proceedings of the twenty-first international conference on Machine learning. July 2004.

[2] Nathan D. Ratliff, J. Andrew Bagnell & Martin A. Zinkevich. [Maximum margin planning](https://doi.org/10.1145/1143844.1143936). ICML '06: Proceedings of the 23rd international conference on Machine learning. June 2006

[3] Nathan Ratliff, David Silver & J. Andrew (Drew) Bagnell. [Learning to search: Functional gradient techniques for imitation learning](https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/). Autonomous Robots. July 2009.

## Footnotes
{: #footnotes}