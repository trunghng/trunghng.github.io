<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Littleroot</title>
    <link>https://trunghng.github.io/posts/</link>
    <description>Recent content in Posts on Littleroot</description>
    <image>
      <title>Littleroot</title>
      <url>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 19 May 2024 17:37:18 +0700</lastBuildDate><atom:link href="https://trunghng.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Graph generation with predefined chromatic number</title>
      <link>https://trunghng.github.io/posts/graph-theory/leighton-graph-gen/</link>
      <pubDate>Sun, 19 May 2024 17:37:18 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/graph-theory/leighton-graph-gen/</guid>
      <description>Leighton&amp;rsquo;s algorithm The algorithm is used to generate an $n$-vertex graph $G$ with $e$ edges and predefined chromatic number $k$ where $k\vert n$. This leads to the condition: \begin{equation} \frac{k(k-1)}{2}\leq e\leq\frac{n^2(k-1)}{2k} \end{equation} The algorithm begins by choosing positive integer $a,c,m$ such that
$m\gg n$ $\gcd(n,m)=k$ $\gcd(c,m)=1$ If $p\vert m$ then $p\vert(a-1)$ for all primes $p$. If $4\vert m$ then $4\vert(a-1)$. We then use the linear congruential generator method to generate a uniform sequence of random number $\{x_i\}$ on $[0,m-1]$.</description>
    </item>
    
    <item>
      <title>Temporal consistency loss &amp; Ape-X DQfD</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/apex-dqfd/</link>
      <pubDate>Tue, 12 Mar 2024 18:39:34 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/apex-dqfd/</guid>
      <description>&lt;p&gt;An algorithm consists of three components: the transformed Bellman operator, the temporal consistency (TC) loss and the combination of Ape-X DQN and DQfD to learn a more consistent human-level policy.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>PILCO</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/pilco/</link>
      <pubDate>Fri, 08 Mar 2024 10:03:08 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/pilco/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;A model-based RL method that learns a Bayesian nonparametric model (Gaussian process) and reduces model bias to improve sample efficiency.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>MuZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/muzero/</link>
      <pubDate>Tue, 02 Jan 2024 11:52:40 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/muzero/</guid>
      <description>MuZero Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k&amp;gt;0$.
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:
the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$; the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$; the immediate reward $r_t^k\approx u_{t+k}$, where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment.</description>
    </item>
    
    <item>
      <title>AlphaZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</link>
      <pubDate>Tue, 17 Oct 2023 10:23:22 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</guid>
      <description>AlphaGo The training pipeline used in AlphaGo can be divided into following stages:
Using a dataset of human experts positions, a supervised learning (SL) policy network $p_\sigma$ and, a rollout policy $p_\pi$, which can sample actions rapidly, are trained by classification to predict player moves. Initializing with the SL policy network $p_\sigma$, it uses policy gradient to train a reinforcement learning (RL) policy network $p_\rho$ with the goal to maximize the winning outcome against previous versions of the policy network.</description>
    </item>
    
    <item>
      <title>Multi-agent Deep Deterministic Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</link>
      <pubDate>Thu, 25 May 2023 15:25:54 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on MADDPG.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GAN</title>
      <link>https://trunghng.github.io/posts/machine-learning/gan/</link>
      <pubDate>Mon, 01 May 2023 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/gan/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Generative Adversarial Networks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Learning</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-learning/</link>
      <pubDate>Sun, 19 Feb 2023 17:23:56 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Learning in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Inference</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-inference/</link>
      <pubDate>Thu, 02 Feb 2023 15:51:13 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-inference/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Inference in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Categorical Reparameterization with Gumbel-Softmax &amp; Concrete Distribution</title>
      <link>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</link>
      <pubDate>Mon, 02 Jan 2023 13:49:15 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on using Gumbel-Softmax &amp;amp; Concrete Distribution in Categorical sampling&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Maximum Entropy Reinforcement Learning via Soft Q-learning &amp; Soft Actor-Critic</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</link>
      <pubDate>Tue, 27 Dec 2022 13:46:09 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Entropy-Regularized Reinforcement Learning via SQL &amp;amp; SAC&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Representation</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-representation/</link>
      <pubDate>Sat, 10 Dec 2022 17:55:57 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-representation/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Representation in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deterministic Policy Gradients</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</link>
      <pubDate>Fri, 02 Dec 2022 19:26:44 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Deterministic Policy Gradient algorithms&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Trust Region Policy Optimization</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/trpo/</link>
      <pubDate>Wed, 23 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/trpo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on policy optimization using trust region method.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deep Q-learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</link>
      <pubDate>Fri, 18 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on DQN and its variants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Natural Evolution Strategies</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/nes/</link>
      <pubDate>Fri, 07 Oct 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/nes/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Natural Evolution Strategies&lt;/strong&gt;, or &lt;strong&gt;NES&lt;/strong&gt;, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</link>
      <pubDate>Thu, 06 Oct 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Policy gradient methods.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>CMA Evolution Strategy</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/cma-es/</link>
      <pubDate>Wed, 14 Sep 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/cma-es/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on CMA - Evolution Strategy.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - the Lebesgue integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</link>
      <pubDate>Sun, 21 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;(Temporary being stopped) Note III of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>https://trunghng.github.io/posts/machine-learning/glm/</link>
      <pubDate>Sat, 13 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/glm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on using linear models in regression and classification.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - Lebesgue measure</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</link>
      <pubDate>Sun, 03 Jul 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note II of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - Elementary measure, Jordan measure &amp; the Riemann integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</link>
      <pubDate>Thu, 16 Jun 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note I of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Likelihood Ratio Policy Gradient via Importance Sampling</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</link>
      <pubDate>Wed, 25 May 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Connection between Likelihood ratio policy gradient method and Importance sampling method.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Planning &amp; Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</link>
      <pubDate>Thu, 19 May 2022 14:09:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;dynamic programming (DP) method&lt;/a&gt; in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;Monte Carlo methods&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/&#34;&gt;temporal-difference learning&lt;/a&gt;, the models are unnecessary. Such methods with requirement of a model like the case of DP is called &lt;strong&gt;model-based&lt;/strong&gt;, while methods without using a model is called &lt;strong&gt;model-free&lt;/strong&gt;. Model-based methods primarily rely on &lt;strong&gt;planning&lt;/strong&gt;; and model-free methods, on the other hand, primarily rely on &lt;strong&gt;learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient Theorem</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</link>
      <pubDate>Wed, 04 May 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a &lt;strong&gt;parameterized policy&lt;/strong&gt;, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called &lt;strong&gt;policy gradient methods&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>The Exponential Family, Generalized Linear Models</title>
      <link>https://trunghng.github.io/posts/machine-learning/exponential-family-glim/</link>
      <pubDate>Mon, 04 Apr 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/exponential-family-glim/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Exponential Family &amp;amp; Generalized Linear Models.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Eligible Traces</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</link>
      <pubDate>Sun, 13 Mar 2022 14:11:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Beside &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td&#34;&gt;$n$-step TD&lt;/a&gt; methods, there is another mechanism called &lt;strong&gt;eligible traces&lt;/strong&gt; that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Function Approximation</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</link>
      <pubDate>Fri, 11 Feb 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Temporal-Difference Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</link>
      <pubDate>Mon, 31 Jan 2022 16:55:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in this &lt;a href=&#34;https://trunghng.github.io/tags/my-rl/&#34;&gt;series&lt;/a&gt;, we have gone through the ideas of &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;dynamic programming&lt;/strong&gt; (DP)&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;&lt;strong&gt;Monte Carlo&lt;/strong&gt;&lt;/a&gt;. What will happen if we combine these ideas together? &lt;strong&gt;Temporal-difference (TD) learning&lt;/strong&gt; is our answer.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Gaussian Distribution &amp; Gaussian Network Models</title>
      <link>https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/</link>
      <pubDate>Mon, 22 Nov 2021 14:46:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Gaussian distribution &amp;amp; Gaussian network models.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Power Series</title>
      <link>https://trunghng.github.io/posts/calculus/power-series/</link>
      <pubDate>Tue, 21 Sep 2021 15:40:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/power-series/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that in the previous note, &lt;a href=&#34;https://trunghng.github.io/posts/calculus/infinite-series-of-constants/&#34;&gt;Infinite Series of Constants&lt;/a&gt;, we mentioned a type of series called &lt;strong&gt;power series&lt;/strong&gt; a lot. In the content of this note, we will be diving deeper into details of its.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Infinite Series of Constants</title>
      <link>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</link>
      <pubDate>Mon, 06 Sep 2021 11:20:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on infinite series of constants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Monte Carlo Methods in Reinforcement Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</link>
      <pubDate>Sat, 21 Aug 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;Dynamic Programming&lt;/strong&gt;&lt;/a&gt; algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods, we only require &lt;strong&gt;experience&lt;/strong&gt; - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Solving MDPs with Dynamic Programming</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</link>
      <pubDate>Sun, 25 Jul 2021 15:30:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In two previous notes, &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;MDPs and Bellman equations&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/&#34;&gt;&lt;strong&gt;Optimal Policy Existence&lt;/strong&gt;&lt;/a&gt;, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with &lt;strong&gt;Dynamic Programming&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Optimal Policy Existence</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</link>
      <pubDate>Sat, 10 Jul 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In the previous note about &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;Markov Decision Processes, Bellman equations&lt;/strong&gt;&lt;/a&gt;, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. In this note, we will be proving that.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measures</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure/</link>
      <pubDate>Sat, 03 Jul 2021 07:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;When talking about &lt;strong&gt;measure&lt;/strong&gt;, you might associate it with the idea of &lt;strong&gt;length&lt;/strong&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;strong&gt;area&lt;/strong&gt;, or even three dimensions with &lt;strong&gt;volume&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Decision Processes, Bellman equations</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</link>
      <pubDate>Sun, 27 Jun 2021 08:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;You may have known or heard vaguely about a computer program called &lt;strong&gt;AlphaGo&lt;/strong&gt; - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called &lt;strong&gt;self-play&lt;/strong&gt; against its other instances, with &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Chain</title>
      <link>https://trunghng.github.io/posts/probability-statistics/markov-chain/</link>
      <pubDate>Sat, 19 Jun 2021 22:27:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/markov-chain/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If we have to describe the definition of &lt;strong&gt;Markov chain&lt;/strong&gt; in one statement, it will be: &amp;ldquo;It only matters where you are, not where you&amp;rsquo;ve been&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>My very first post</title>
      <link>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</link>
      <pubDate>Sat, 05 Jun 2021 17:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Enjoy my index-zero-ed note while staying tuned for next ones!&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>
