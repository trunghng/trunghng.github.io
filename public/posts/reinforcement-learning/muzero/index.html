<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>MuZero | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="deep-reinforcement-learning,model-based,mcts,my-rl"><meta name=description content="MuZero Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k>0$.
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:
the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$; the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$; the immediate reward $r_t^k\approx u_{t+k}$, where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment."><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/muzero/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="MuZero"><meta property="og:description" content="MuZero Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k>0$.
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:
the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$; the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$; the immediate reward $r_t^k\approx u_{t+k}$, where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment."><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/muzero/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-01-02T11:52:40+07:00"><meta property="article:modified_time" content="2024-01-02T11:52:40+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="MuZero"><meta name=twitter:description content="MuZero Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k>0$.
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:
the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$; the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$; the immediate reward $r_t^k\approx u_{t+k}$, where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"MuZero","item":"https://trunghng.github.io/posts/reinforcement-learning/muzero/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"MuZero","name":"MuZero","description":"MuZero Predictions are made at each time step $t$, for each of $k=0,\\ldots,K$ steps, by a model $\\mu_\\theta$, parameterized by $\\theta$, conditioned on past observations $o_1,\\ldots,o_t$ and on future actions $a_{t+1},\\ldots,a_{t+k}$ for $k\u0026gt;0$.\nThe model $\\mu_\\theta$ predicts three future quantities that are directly relevant for planning:\nthe policy $p_t^k\\approx\\pi(a_{t+k+1}\\vert o_1,\\ldots,o_t,a_{t+1},\\ldots,a_{t+k})$; the value function $v_t^k\\approx\\mathbb{E}\\big[u_{t+k+1}+\\gamma u_{t+k+2}+\\ldots\\vert o_1,\\ldots,o_t,a_{t+1},\\ldots,a_{t+k}\\big]$; the immediate reward $r_t^k\\approx u_{t+k}$, where $u$ is the true, observed reward, $\\pi$ is the policy used to select real actions and $\\gamma$ is the discount function of the environment.","keywords":["deep-reinforcement-learning","model-based","mcts","my-rl"],"articleBody":"MuZero Predictions are made at each time step $t$, for each of $k=0,\\ldots,K$ steps, by a model $\\mu_\\theta$, parameterized by $\\theta$, conditioned on past observations $o_1,\\ldots,o_t$ and on future actions $a_{t+1},\\ldots,a_{t+k}$ for $k\u003e0$.\nThe model $\\mu_\\theta$ predicts three future quantities that are directly relevant for planning:\nthe policy $p_t^k\\approx\\pi(a_{t+k+1}\\vert o_1,\\ldots,o_t,a_{t+1},\\ldots,a_{t+k})$; the value function $v_t^k\\approx\\mathbb{E}\\big[u_{t+k+1}+\\gamma u_{t+k+2}+\\ldots\\vert o_1,\\ldots,o_t,a_{t+1},\\ldots,a_{t+k}\\big]$; the immediate reward $r_t^k\\approx u_{t+k}$, where $u$ is the true, observed reward, $\\pi$ is the policy used to select real actions and $\\gamma$ is the discount function of the environment.\nInternally, at each time step $t$, the model is represented by the combination of a representation function, a dynamics function and a prediction function.\nThe (deterministic) dynamics function $g_\\theta$, is a recurrent process, $(r_t^k,s_t^k)=g_\\theta(s_t^{k-1},a_t^k)$, that computes, at each hypothetical step $k$, an immediate reward $r_t^k$ and an internal state $s_t^k$. The prediction function $f_\\theta$ computes the policy and value functions from the internal state $s^k$, $(p_t^k,v_t^k)=f_\\theta(s_t^k)$, similar to the joint policy and value network of AlphaZero. The representation function $h_\\theta$ initializes the root state $s_t^0$ by encoding past observations, $s_t^0=h_\\theta(o_1,\\ldots,o_t)$. Planning Given such a model, we can apply any MDP planning algorithms, such as dynamic programming or MCTS, to compute the optimal value function or optimal policy for the MDP.\nMuZero uses an MCTS algorithm similar to AlphaZero’s search (with generalized to single-agent and nonzero intermediate reward).\nFigure 1: (taken from MuZero paper) Planning with MCTS Search algorithm Every node in the search tree is associated with an internal state $s$. Corresponding to each action $a$ from $s$ is an edge $(s,a)$ storing a set of statistics \\begin{equation} \\{N(s,a),P(s,a),Q(s,a),R(s,a),S(s,a)\\} \\end{equation} respectively representing visit count $N$, policy $P$, mean value $Q$, reward $R$ and state transition $S$. Analogous to AlphaZero, the algorithm iterates over three stages for a number of simulations.1\nSelection. The stage begins at the internal root state $s^0$ and finishes when reaching a leaf node $s^l$. For each hypothetical time step $k=1,\\ldots,l$ of the simulation, an action $a^k$ that maximizes over a PUCT bound is selected according to the statistics stored at $s^{k-1}$. \\begin{equation} a^k=\\underset{a}{\\text{argmax}}\\big(\\bar{Q}(s^{k-1},a)+U(s^{k-1},a)\\big), \\end{equation} where \\begin{align} \\bar{Q}(s^{k-1},a)\u0026=\\frac{R(s^{k-1},a)+\\gamma Q(s^{k-1},a)-\\underset{s',a'\\in\\text{Tree}}{\\min}Q(s',a')}{\\underset{s',a'\\in\\text{Tree}}{\\max}Q(s',a')-\\underset{s',a'\\in\\text{Tree}}{\\min}Q(s',a')}\\label{eq:mu.1} \\\\ U(s^{k-1},a)\u0026=c_\\text{puct}P(s^{k-1},a)\\frac{\\sqrt{\\sum_b N(s^{k-1},b)}}{1+N(s^{k-1},a)} \\\\ c_\\text{puct}\u0026=c_1+\\log\\frac{\\sum_b N(s^{k-1},b)+c_2+1}{c_2}, \\end{align} where $c_1$ and $c_2$ are constants controlling the influence of the policy $P(s^{k-1},a)$ relative to the value $Q(s^{k-1},a)$ as nodes are visited more often.\nFor $k\\lt l$, the next state and reward are looked up in the state transition and reward table \\begin{align} s^k\u0026=S(s^{k-1},a^k) \\\\ r^k\u0026=R(s^{k-1},a^k) \\end{align} Expansion. At the final step $l$ of the simulation, the reward and state are computed by the dynamics function $g_\\theta$. \\begin{equation} (r^l,s^l)=g_\\theta(s^{l-1},a^l) \\end{equation} and stored in the corresponding tables \\begin{align} R(s^{l-1},a^l)\u0026=r^l \\\\ S(s^{l-1},a^l)\u0026=s^l \\end{align} The policy and value function are computed by the prediction function $f_\\theta$. \\begin{equation} (p^l,v^l)=f_\\theta(s^l) \\end{equation} A new node, corresponding to the state $s^l$ is added to the search tree and each edge $(s^l,a)$ is initialized to \\begin{equation} \\{N(s^l,a)=0,Q(s^l,a)=0,P(s^l,a)=p^l\\} \\end{equation} Backup. For $k=l,\\ldots,0$, we form an $(l-k)$-step estimate of the cumulative discounted reward, bootstrapping from the value function $v^l$. \\begin{equation} G^k=\\sum_{\\tau=0}^{l-1-k}\\gamma^\\tau r_{k+1+\\tau}+\\gamma^{l-k}v^l \\end{equation} The edge statistics are then updated in a backward pass through each step $k\\leq l$. Specifically, for $k=l,\\ldots,1$, the statistics corresponding to each edge $(s^{k-1},a^k)$ in the simulation path are updated as \\begin{align} Q(s^{k-1},a^k)\u0026\\leftarrow\\frac{N(s^{k-1},a^k)Q(s^{k-1},a^k)+G^k}{N(s^{k-1},a^k)+1} \\\\ N(s^{k-1},a^k)\u0026\\leftarrow N(s^{k-1},a^k)+1 \\end{align} The MCTS can be viewed as a pair of a search policy and a search value function $(\\pi_t,\\nu_t)$ \\begin{align} \\pi_t\u0026=Pr(a_{t+1}\\vert o_1,\\ldots,o_t) \\\\ \\nu_t\u0026\\approx\\mathbb{E}\\big[u_{t+1}+\\gamma u_{t+2}+\\ldots\\vert o_1,\\ldots,o_t\\big] \\end{align} that both selects an action and predicts cumulative discounted reward given past observations $o_1,\\ldots,o_t$. At each internal node, it makes use of the policy $p$, value function $v$ and reward estimate $G$ produced by the current model parameter $\\theta$, and combines these values together using lookahead search to produce an improved policy $\\pi_t$ and improved value function $\\nu_t$ at the root of the search tree.\nActing After an MCTS is performed at time step $t$, the next action $a_{t+1}$ is then selected according to the search policy $\\pi_t$. With the action received, the environment generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of episode, the trajectory data are stored into a replay buffer.\nFigure 2: (taken from MuZero paper) Acting in MuZero Training The model training in MuZero proceeds as:\nA trajectory is sampled from the replay buffer. At the initial step, the representation function $h_\\theta$ produces a hidden state $s^0$ from past observations $o_1,\\ldots,o_t$ from the selected trajectory. The model is then unrolled recurrently for $K$ steps. At each step $k=1,\\ldots,K$, the dynamics function $g_\\theta$ receives as input the hidden state $s^{k-1}$ from the previous step and the real action $a_{t+k}$. All parameters of the representation, dynamics and prediction functions are trained jointly, end to end, by backpropagation through time, as a single $\\theta$ to match the policy $p_t^k$, value function $v_t^k$ and reward prediction $r_t^k$, for every hypothetical step $k$ to three corresponding targets observed after $k$ actual time steps have elapsed. Specifically, the overall loss used by MuZero model is \\begin{equation} l_t(\\theta)=\\sum_{k=0}^{K}l^p(\\pi_{t+k},p_t^k)+\\sum_{k=0}^{K}l^v(z^{t+k},v_t^k)+\\sum_{k=1}^{K}l^r(u_{t+k},r_t^k)+c\\Vert\\theta\\Vert^2, \\end{equation} where $l^p,l^v$ and $l^r$ are loss functions (e.g., cross entropy, MSE, etc) for policy, value and reward respectively; and where $\\pi_t$ (recalling) is the search policy, $u_t\\in\\{-1,0,1\\}$ is the final outcomes corresponding to {lose, draw, win} and $z_{t+k}$ is the $n$-step return that bootstraps $n$ steps into the future from the search value, i.e. $z_t=u_{t+1}+\\gamma u_{t+2}+\\ldots+\\gamma^{n-1}u_{t+n}+\\gamma^n \\nu_{t+n}$. Figure 3: (taken from MuZero paper) Model training in MuZero MuZero Reanalyze References [1] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver et al. Mastering Atari, Go, chess and shogi by planning with a learned model. Nature 588, 604–609, 2020.\n[2] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao. Mastering Atari Games with Limited Data. arXiv preprint, arXiv:2111.00210, 2021.\n[3] Richard S. Sutton, Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[4] Julian Schrittwieser. MuZero Intuition.\nFootnotes While in the MuZero paper, the normalized $\\bar{Q}$ values given in \\eqref{eq:mu.1} is calculated as \\begin{equation} \\bar{Q}(s^{k-1},a)=\\frac{Q(s^{k-1},a)-\\underset{s’,a’\\in\\text{Tree}}{\\min}Q(s’,a’)}{\\underset{s’,a’\\in\\text{Tree}}{\\max}Q(s’,a’)-\\underset{s’,a’\\in\\text{Tree}}{\\min}Q(s’,a’)}\\nonumber \\end{equation} ↩︎\n","wordCount":"977","inLanguage":"en","datePublished":"2024-01-02T11:52:40+07:00","dateModified":"2024-01-02T11:52:40+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/muzero/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>MuZero</h1><div class=post-meta><span title='2024-01-02 11:52:40 +0700 +07'>January 2, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#muzero>MuZero</a><ul><li><a href=#planning>Planning</a><ul><li><a href=#search-algorithm>Search algorithm</a></li></ul></li><li><a href=#acting>Acting</a></li><li><a href=#training>Training</a></li><li><a href=#muzero-reanalyze>MuZero Reanalyze</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><h2 id=muzero>MuZero<a hidden class=anchor aria-hidden=true href=#muzero>#</a></h2><p>Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k>0$.<br>The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:</p><ul id=number-list><li>the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$;</li><li>the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$;</li><li>the immediate reward $r_t^k\approx u_{t+k}$,</li></ul><p>where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment.</p><p>Internally, at each time step $t$, the model is represented by the combination of a representation function, a dynamics function and a prediction function.</p><ul id=number-list><li>The (deterministic) <b>dynamics function</b> $g_\theta$, is a recurrent process, $(r_t^k,s_t^k)=g_\theta(s_t^{k-1},a_t^k)$, that computes, at each hypothetical step $k$, an immediate reward $r_t^k$ and an internal state $s_t^k$.</li><li>The <b>prediction function</b> $f_\theta$ computes the policy and value functions from the internal state $s^k$, $(p_t^k,v_t^k)=f_\theta(s_t^k)$, similar to the joint policy and value network of <a href=https://trunghng.github.io/posts/reinforcement-learning/alphazero/#neural-network-training>AlphaZero</a>.</li><li>The <b>representation function</b> $h_\theta$ initializes the root state $s_t^0$ by encoding past observations, $s_t^0=h_\theta(o_1,\ldots,o_t)$.</li></ul><h3 id=planning>Planning<a hidden class=anchor aria-hidden=true href=#planning>#</a></h3><p>Given such a model, we can apply any MDP planning algorithms, such as dynamic programming or MCTS, to compute the optimal value function or optimal policy for the MDP.</p><p>MuZero uses an MCTS algorithm similar to <a href=https://trunghng.github.io/posts/reinforcement-learning/alphazero/#search-algorithm>AlphaZero</a>&rsquo;s search (with generalized to single-agent and nonzero intermediate reward).</p><figure><img src=/images/muzero/planning.png alt="planning with MCTS" width=40% height=40%><figcaption style=text-align:center><b>Figure 1</b>: (taken from <a href=#muzero-paper>MuZero paper</a>) <b>Planning with MCTS</b></figcaption></figure><h4 id=search-algorithm>Search algorithm<a hidden class=anchor aria-hidden=true href=#search-algorithm>#</a></h4><p>Every node in the search tree is associated with an internal state $s$. Corresponding to each action $a$ from $s$ is an edge $(s,a)$ storing a set of statistics
\begin{equation}
\{N(s,a),P(s,a),Q(s,a),R(s,a),S(s,a)\}
\end{equation}
respectively representing visit count $N$, policy $P$, mean value $Q$, reward $R$ and state transition $S$. Analogous to AlphaZero, the algorithm iterates over three stages for a number of simulations.<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></p><ul id=number-list><li><b>Selection</b>. The stage begins at the internal root state $s^0$ and finishes when reaching a leaf node $s^l$. For each hypothetical time step $k=1,\ldots,l$ of the simulation, an action $a^k$ that maximizes over a PUCT bound is selected according to the statistics stored at $s^{k-1}$.
\begin{equation}
a^k=\underset{a}{\text{argmax}}\big(\bar{Q}(s^{k-1},a)+U(s^{k-1},a)\big),
\end{equation}
where
\begin{align}
\bar{Q}(s^{k-1},a)&=\frac{R(s^{k-1},a)+\gamma Q(s^{k-1},a)-\underset{s',a'\in\text{Tree}}{\min}Q(s',a')}{\underset{s',a'\in\text{Tree}}{\max}Q(s',a')-\underset{s',a'\in\text{Tree}}{\min}Q(s',a')}\label{eq:mu.1} \\ U(s^{k-1},a)&=c_\text{puct}P(s^{k-1},a)\frac{\sqrt{\sum_b N(s^{k-1},b)}}{1+N(s^{k-1},a)} \\ c_\text{puct}&=c_1+\log\frac{\sum_b N(s^{k-1},b)+c_2+1}{c_2},
\end{align}
where $c_1$ and $c_2$ are constants controlling the influence of the policy $P(s^{k-1},a)$ relative to the value $Q(s^{k-1},a)$ as nodes are visited more often.<br>For $k\lt l$, the next state and reward are looked up in the state transition and reward table
\begin{align}
s^k&=S(s^{k-1},a^k) \\ r^k&=R(s^{k-1},a^k)
\end{align}</li><li><b>Expansion</b>. At the final step $l$ of the simulation, the reward and state are computed by the dynamics function $g_\theta$.
\begin{equation}
(r^l,s^l)=g_\theta(s^{l-1},a^l)
\end{equation}
and stored in the corresponding tables
\begin{align}
R(s^{l-1},a^l)&=r^l \\ S(s^{l-1},a^l)&=s^l
		\end{align}
The policy and value function are computed by the prediction function $f_\theta$.
\begin{equation}
(p^l,v^l)=f_\theta(s^l)
\end{equation}
A new node, corresponding to the state $s^l$ is added to the search tree and each edge $(s^l,a)$ is initialized to
\begin{equation}
\{N(s^l,a)=0,Q(s^l,a)=0,P(s^l,a)=p^l\}
\end{equation}</li><li><b>Backup</b>. For $k=l,\ldots,0$, we form an $(l-k)$-step estimate of the cumulative discounted reward, bootstrapping from the value function $v^l$.
\begin{equation}
G^k=\sum_{\tau=0}^{l-1-k}\gamma^\tau r_{k+1+\tau}+\gamma^{l-k}v^l
\end{equation}
The edge statistics are then updated in a backward pass through each step $k\leq l$. Specifically, for $k=l,\ldots,1$, the statistics corresponding to each edge $(s^{k-1},a^k)$ in the simulation path are updated as
\begin{align}
Q(s^{k-1},a^k)&\leftarrow\frac{N(s^{k-1},a^k)Q(s^{k-1},a^k)+G^k}{N(s^{k-1},a^k)+1} \\ N(s^{k-1},a^k)&\leftarrow N(s^{k-1},a^k)+1
		\end{align}</li></ul><p>The MCTS can be viewed as a pair of a search policy and a search value function $(\pi_t,\nu_t)$
\begin{align}
\pi_t&=Pr(a_{t+1}\vert o_1,\ldots,o_t) \\ \nu_t&\approx\mathbb{E}\big[u_{t+1}+\gamma u_{t+2}+\ldots\vert o_1,\ldots,o_t\big]
\end{align}
that both selects an action and predicts cumulative discounted reward given past observations $o_1,\ldots,o_t$. At each internal node, it makes use of the policy $p$, value function $v$ and reward estimate $G$ produced by the current model parameter $\theta$, and combines these values together using lookahead search to produce an improved policy $\pi_t$ and improved value function $\nu_t$ at the root of the search tree.</p><h3 id=acting>Acting<a hidden class=anchor aria-hidden=true href=#acting>#</a></h3><p>After an MCTS is performed at time step $t$, the next action $a_{t+1}$ is then selected according to the search policy $\pi_t$. With the action received, the environment generates a new observation $o_{t+1}$ and reward $u_{t+1}$. At the end of episode, the trajectory data are stored into a replay buffer.</p><figure><img src=/images/muzero/acting.png alt="acting in MuZero" width=60% height=60%><figcaption style=text-align:center><b>Figure 2</b>: (taken from <a href=#muzero-paper>MuZero paper</a>) <b>Acting in MuZero</b></figcaption></figure><h3 id=training>Training<a hidden class=anchor aria-hidden=true href=#training>#</a></h3><p>The model training in MuZero proceeds as:</p><ul id=number-list><li>A trajectory is sampled from the replay buffer.</li><li>At the initial step, the representation function $h_\theta$ produces a hidden state $s^0$ from past observations $o_1,\ldots,o_t$ from the selected trajectory.</li><li>The model is then unrolled recurrently for $K$ steps.</li><li>At each step $k=1,\ldots,K$, the dynamics function $g_\theta$ receives as input the hidden state $s^{k-1}$ from the previous step and the real action $a_{t+k}$.</li><li>All parameters of the representation, dynamics and prediction functions are trained jointly, end to end, by backpropagation through time, as a single $\theta$ to match the policy $p_t^k$, value function $v_t^k$ and reward prediction $r_t^k$, for every hypothetical step $k$ to three corresponding targets observed after $k$ actual time steps have elapsed. Specifically, the overall loss used by MuZero model is
\begin{equation}
l_t(\theta)=\sum_{k=0}^{K}l^p(\pi_{t+k},p_t^k)+\sum_{k=0}^{K}l^v(z^{t+k},v_t^k)+\sum_{k=1}^{K}l^r(u_{t+k},r_t^k)+c\Vert\theta\Vert^2,
\end{equation}
where $l^p,l^v$ and $l^r$ are loss functions (e.g., cross entropy, MSE, etc) for policy, value and reward respectively; and where $\pi_t$ (recalling) is the search policy, $u_t\in\{-1,0,1\}$ is the final outcomes corresponding to {lose, draw, win} and $z_{t+k}$ is the $n$-step return that bootstraps $n$ steps into the future from the search value, i.e. $z_t=u_{t+1}+\gamma u_{t+2}+\ldots+\gamma^{n-1}u_{t+n}+\gamma^n \nu_{t+n}$.</li></ul><figure><img src=/images/muzero/training.png alt="model training in MuZero" width=60% height=60%><figcaption style=text-align:center><b>Figure 3</b>: (taken from <a href=#muzero-paper>MuZero paper</a>) <b>Model training in MuZero</b></figcaption></figure><h3 id=muzero-reanalyze>MuZero Reanalyze<a hidden class=anchor aria-hidden=true href=#muzero-reanalyze>#</a></h3><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=muzero-paper>Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, David Silver et al. <a href=https://doi.org/10.1038/s41586-020-03051-4>Mastering Atari, Go, chess and shogi by planning with a learned model</a>. Nature 588, 604–609, 2020.</span></p><p>[2] Weirui Ye, Shaohuai Liu, Thanard Kurutach, Pieter Abbeel, Yang Gao. <a href=https://arxiv.org/abs/2111.00210>Mastering Atari Games with Limited Data</a>. arXiv preprint, arXiv:2111.00210, 2021.</p><p>[3] Richard S. Sutton, Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[4] Julian Schrittwieser. <a href=https://www.furidamu.org/blog/2020/12/22/muzero-intuition>MuZero Intuition</a>.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>While in the <a href=#muzero-paper>MuZero paper</a>, the normalized $\bar{Q}$ values given in \eqref{eq:mu.1} is calculated as
\begin{equation}
\bar{Q}(s^{k-1},a)=\frac{Q(s^{k-1},a)-\underset{s&rsquo;,a&rsquo;\in\text{Tree}}{\min}Q(s&rsquo;,a&rsquo;)}{\underset{s&rsquo;,a&rsquo;\in\text{Tree}}{\max}Q(s&rsquo;,a&rsquo;)-\underset{s&rsquo;,a&rsquo;\in\text{Tree}}{\min}Q(s&rsquo;,a&rsquo;)}\nonumber
\end{equation}&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/deep-reinforcement-learning/>deep-reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/model-based/>model-based</a></li><li><a href=https://trunghng.github.io/tags/mcts/>mcts</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=next href=https://trunghng.github.io/posts/reinforcement-learning/alphazero/><span class=title>Next »</span><br><span>AlphaZero</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on x" href="https://x.com/intent/tweet/?text=MuZero&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f&amp;hashtags=deep-reinforcement-learning%2cmodel-based%2cmcts%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f&amp;title=MuZero&amp;summary=MuZero&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f&title=MuZero"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on whatsapp" href="https://api.whatsapp.com/send?text=MuZero%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on telegram" href="https://telegram.me/share/url?text=MuZero&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share MuZero on ycombinator" href="https://news.ycombinator.com/submitlink?t=MuZero&u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmuzero%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>