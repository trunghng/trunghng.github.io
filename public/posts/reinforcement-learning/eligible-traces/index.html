<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Eligible Traces | Trung's Place</title><meta name=keywords content="reinforcement-learning,td-learning,eligible-traces,function-approximation,importance-sampling,my-rl"><meta name=description content="
Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:5px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Eligible Traces"><meta property="og:description" content="
Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-03-13T14:11:00+07:00"><meta property="article:modified_time" content="2022-03-13T14:11:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Eligible Traces"><meta name=twitter:description content="
Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Eligible Traces","item":"https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Eligible Traces","name":"Eligible Traces","description":" Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\\lambda$ in TD($\\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\\lambda=0$ to Monte Carlo methods with $\\lambda=1$.\n","keywords":["reinforcement-learning","td-learning","eligible-traces","function-approximation","importance-sampling","my-rl"],"articleBody":" Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\\lambda$ in TD($\\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\\lambda=0$ to Monte Carlo methods with $\\lambda=1$.\nThe $\\lambda$-return Recall that in TD-Learning note, we have defined the $n$-step return as \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n}) \\end{equation} for all $n,t$ such that $n\\geq 1$ and $0\\leq t\\lt T-n$. After the note of Function Approximation, for any parameterized function approximator, we can generalize that equation into: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+ \\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\leq T-n \\end{equation} where $\\hat{v}(s,\\mathbf{w})$ is the approximate value of state $s$ given weight vector $\\mathbf{w}$.\nWe already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate SGD update, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose \\begin{equation} \\frac{1}{2}G_{t:t+2}+\\frac{1}{2}G_{t:t+4} \\end{equation} as the target for our update.\nThe TD($\\lambda$) is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\\lambda^{n-1}$, for $\\lambda\\in\\left[0,1\\right]$, and is normalized by a factor of $1-\\lambda$ to guarantee that the weights sum to $1$, as: \\begin{equation} G_t^\\lambda\\doteq(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda^{n-1}G_{t:t+n} \\end{equation} The $G_t^\\lambda$ is called $\\lambda$-return of the update.\nThis figure below illustrates the backup diagram of TD($\\lambda$) algorithm.\nFigure 1: (taken from the RL book) The backup diagram of TD($\\lambda$) Offline $\\lambda$-return With the definition of $\\lambda$-return, we can define the offline $\\lambda$-return algorithm, which use semi-gradient update and using $\\lambda$-return as the target: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\hspace{1cm}t=0,\\dots,T-1 \\end{equation}\nA result when applying offline $\\lambda$-return on the random walk problem is shown below.\nFigure 2: Using offline $\\lambda$-return on 19-state random walk. The code can be found here TD($\\lambda$) TD($\\lambda$) improves over the offline $\\lambda$-return algorithm since:\nIt updates the weight vector $\\mathbf{w}$ on every step of an episode rather than only at the end, which leads to a time improvement. Its computations are equally distributed in time rather than all at the end of the episode. It can be applied to continuing problems rather than just to episodic ones. With function approximation, the eligible trace is a vector $\\mathbf{z}_t\\in\\mathbb{R}^d$ with the same number of components as the weight vector $\\mathbf{w}_t$. Whereas $\\mathbf{w}_t$ is long-term memory, $\\mathbf{z}_t$ on the other hand is a short-term memory, typically lasting less time than the length of an episode.\nIn TD($\\lambda$), starting at the initial value of zero at the beginning of the episode, on each time step, the eligible trace vector $\\mathbf{z}_t$ is incremented by the value gradient, and then fades away by $\\gamma\\lambda$: \\begin{align} \\mathbf{z}_{-1}\u0026\\doteq\\mathbf{0} \\\\ \\mathbf{z}_t\u0026\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\hspace{1cm}0\\leq t\\lt T\\label{eq:tl.1} \\end{align} where $\\gamma$ is the discount factor; $\\lambda$ is also called trace-decay parameter. On the other hand, the weight vector $\\mathbf{w}_t$ is updated on each step proportional to the scalar TD errors and the eligible trace vector $\\mathbf{z}_t$: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t,\\label{eq:tl.2} \\end{equation} where the TD error is defined as \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} Pseudocode of semi-gradient TD($\\lambda$) is given below.\nLinear TD($\\lambda$) has been proved to converge in the on-policy case if the step size parameter, $\\alpha$, is reduced over time according to the usual conditions. And also in the continuing discounted case, for any $\\lambda$, $\\overline{\\text{VE}}$ is proven to be within a bounded expansion of the lowest possible error: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w}_\\infty)\\leq\\dfrac{1-\\gamma\\lambda}{1-\\gamma}\\min_\\mathbf{w}\\overline{\\text{VE}}(\\mathbf{w}) \\end{equation}\nThe figure below illustrates the result for using TD($\\lambda$) on the usual random walk task.\nFigure 3: Using TD($\\lambda$) on 19-state random walk. The code can be found here Truncated TD Methods Since in the offline $\\lambda$-return, the target $\\lambda$-return is not known until the end of episode. And moreover, in the continuing case, since the $n$-step returns depend on arbitrary large $n$, it maybe never known. However, the dependence becomes weaker for longer-delayed rewards, falling by $\\gamma\\lambda$ for each step of delay.\nA natural approximation is to truncate the sequence after some number of steps. In general, we define the truncated $\\lambda$-return for time $t$, given data only up to some later horizon, $h$, as: \\begin{equation} G_{t:h}^\\lambda\\doteq(1-\\lambda)\\sum_{n=1}^{h-t-1}\\lambda^{n-1}G_{t:t+n}+\\lambda^{h-t-1}G_{t:h},\\hspace{1cm}0\\leq t\\lt h\\leq T \\end{equation} With this definition of the return, and based on the function approximation version of the $n$-step TD we have defined before, we have the TTD($\\lambda$) is defined as: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\left[G_{t:t+n}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_{t+n-1})\\right]\\nabla_\\mathbf{w}\\hat{w}(S_t,\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\lt T \\end{equation} We have the $k$-step $\\lambda$-return can be written as: \\begin{align} \\hspace{-0.8cm}G_{t:t+k}^\\lambda\u0026=(1-\\lambda)\\sum_{n=1}^{k-1}\\lambda^{n-1}G_{t:t+n}+\\lambda^{k-1}G_{t:t+k} \\\\ \u0026=(1-\\lambda)\\sum_{n=1}^{k-1}\\lambda^{n-1}\\left[R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1})\\right]\\nonumber \\\\ \u0026\\hspace{1cm}+\\lambda^{k-1}\\left[R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{k-1}R_{t+k}+\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1})\\right] \\\\ \u0026=R_{t+1}+\\gamma\\lambda R_{t+2}+\\dots+\\gamma^{k-1}\\lambda^{k-1}R_{t+k}\\nonumber \\\\ \u0026\\hspace{1cm}+(1-\\lambda)\\left[\\sum_{n=1}^{k-1}\\lambda^{n-1}\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1})\\right]+\\lambda^{k-1}\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1}) \\\\ \u0026=\\hat{v}(S_t,\\mathbf{w}_{t-1})+\\left[R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_{t-1})\\right]\\nonumber \\\\ \u0026\\hspace{1cm}+\\left[\\lambda\\gamma R_{t+2}+\\lambda\\gamma^2\\hat{v}(S_{t+2},\\mathbf{w}_{t+1})-\\lambda\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)\\right]+\\dots\\nonumber \\\\ \u0026\\hspace{1cm}+\\left[\\lambda^{k-1}\\gamma^{k-1}R_{t+k}+\\lambda^{k-1}\\gamma^k\\hat{v}(S_{t+k},\\mathbf{w}_{t+k-1})-\\lambda^{k-1}\\gamma^{k-1}\\hat{v}(S_{t+k-1},\\mathbf{w}_{t+k-2})\\right] \\\\ \u0026=\\hat{v}(S_t,\\mathbf{w}_{t-1})+\\sum_{i=t}^{t+k-1}(\\gamma\\lambda)^{i-t}\\delta_i’,\\label{eq:tt.1} \\end{align} with \\begin{equation} \\delta_t’\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_{t-1}), \\end{equation} where in the third step of the derivation, we use the identity \\begin{equation} (1-\\lambda)(1+\\lambda+\\dots+\\lambda^{k-2})=1-\\lambda^{k-1} \\end{equation} From \\eqref{eq:tt.1}, we can see that the $k$-step $\\lambda$-return can be written as sums of TD errors if the value function is held constant, which allows us to implement the TTD($\\lambda$) algorithm efficiently.\nFigure 4: (taken from the RL book) The backup diagram of truncated TD($\\lambda$) Online $\\lambda$-return The idea of online $\\lambda$-return involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors.\nLet $\\mathbf{w}_t^h$ denote the weights used to generate the value at time $t$ in the sequence up to horizon $h$. The first weight vector $\\mathbf{w}_0^h$ in each sequence is the one that inherited from the previous episode (thus they are the same for all $h$), and the last weight vector $\\mathbf{w}_h^h$ in each sequence defines the weight-vector sequence of the algorithm. At the final horizon $h=T$, we obtain the final weight $\\mathbf{w}_T^T$ which will be passed on to form the initial weights of the next episode.\nIn particular, we can define the first three sequences as: \\begin{align} h=1:\\hspace{1cm}\u0026\\mathbf{w}_1^1\\doteq\\mathbf{w}_0^1+\\alpha\\left[G_{0:1}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^1)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^1), \\\\\\nonumber \\\\ h=2:\\hspace{1cm}\u0026\\mathbf{w}_1^2\\doteq\\mathbf{w}_0^2+\\alpha\\left[G_{0:2}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^2)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^2), \\\\ \u0026\\mathbf{w}_2^2\\doteq\\mathbf{w}_1^2+\\alpha\\left[G_{1:2}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_1^2)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_1,\\mathbf{w}_1^2), \\\\\\nonumber \\\\ h=3:\\hspace{1cm}\u0026\\mathbf{w}_1^3\\doteq\\mathbf{w}_0^3+\\alpha\\left[G_{0:3}^\\lambda-\\hat{v}(S_0,\\mathbf{w}_0^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_0,\\mathbf{w}_0^3), \\\\ \u0026\\mathbf{w}_2^3\\doteq\\mathbf{w}_1^3+\\alpha\\left[G_{1:3}^\\lambda-\\hat{v}(S_1,\\mathbf{w}_1^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_1,\\mathbf{w}_1^3), \\\\ \u0026\\mathbf{w}_3^3\\doteq\\mathbf{w}_2^3+\\alpha\\left[G_{2:3}^\\lambda-\\hat{v}(S_2,\\mathbf{w}_2^3)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_2,\\mathbf{w}_2^3) \\end{align} The general form for the update of the online $\\lambda$-return is \\begin{equation} \\mathbf{w}_{t+1}^h\\doteq\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t^h)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t^h),\\hspace{1cm}0\\leq t\\lt h\\leq T,\\label{eq:olr.1} \\end{equation} with $\\mathbf{w}_t\\doteq\\mathbf{w}_t^t$, and $\\mathbf{w}_0^h$ is the same for all $h$, we denote this vector as $\\mathbf{w}_{init}$.\nThe online $\\lambda$-return algorithm is fully online, determining a new weight vector $\\mathbf{w}_t$ at each time step $t$ during an episode, using only information available at time $t$. Whereas the offline version passes through all the steps at the time of termination but does not make any updates during the episode.\nTrue Online TD($\\lambda$) In the online $\\lambda$-return, at each time step a sequence of updates is performed. The length of this sequence, and hence the computation per time step, increase over time.\nHowever, it is possible to compute the weight vector resulting from time step $t+1$, $\\mathbf{w}_{t+1}$, directly from the weight vector resulting from the sequence at time step $t$, $\\mathbf{w}_t$.\nConsider using linear approximation for our task, which gives us \\begin{align} \\hat{v}(S_t,\\mathbf{w}_t)\u0026=\\mathbf{w}_t^\\text{T}\\mathbf{x}_t; \\\\ \\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\u0026=\\mathbf{x}_t, \\end{align} where $\\mathbf{x}_t=\\mathbf{x}(S_t)$ as usual.\nWe begin by rewriting \\eqref{eq:olr.1}, as \\begin{align} \\mathbf{w}_{t+1}^h\u0026\\doteq\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\hat{v}(S_t,\\mathbf{w}_t^h)\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t^h) \\\\ \u0026=\\mathbf{w}_t^h+\\alpha\\left[G_{t:h}^\\lambda-\\left(\\mathbf{w}_t^h\\right)^\\text{T}\\mathbf{x}_t\\right]\\mathbf{x}_t \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t^h+\\alpha\\mathbf{x}_t G_{t:h}^\\lambda, \\end{align} where $\\mathbf{I}$ is the identity matrix. With this equation, consider $\\mathbf{w}_t^h$ in the cases of $t=1$ and $t=2$, we have: \\begin{align} \\mathbf{w}_1^h\u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_0^h+\\alpha\\mathbf{x}_0 G_{0:h}^\\lambda \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_{init}+\\alpha\\mathbf{x}_0 G_{0:h}^\\lambda, \\\\ \\mathbf{w}_2^h\u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\mathbf{w}_1^h+\\alpha\\mathbf{x}_1 G_{1:h}^\\lambda \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{x}_0\\mathbf{x}_0^\\text{T}\\right)\\mathbf{w}_{init}+\\alpha\\left(\\mathbf{I}-\\alpha\\mathbf{x}_1\\mathbf{x}_1^\\text{T}\\right)\\mathbf{x}_0 G_{0:h}^\\lambda+\\alpha\\mathbf{x}_1 G_{1:h}^\\lambda \\end{align} In general, for $t\\leq h$, we can write: \\begin{equation} \\mathbf{w}_t^h=\\mathbf{A}_0^{t-1}\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{i:h}^\\lambda, \\end{equation} where $\\mathbf{A}_i^j$ is defined as: \\begin{equation} \\mathbf{A}_i^j\\doteq\\left(\\mathbf{I}-\\alpha\\mathbf{x}_j\\mathbf{x}_j^\\text{T}\\right)\\left(\\mathbf{I}-\\alpha\\mathbf{x}_{j-1}\\mathbf{x}_{j-1}^\\text{T}\\right)\\dots\\left(\\mathbf{I}-\\alpha\\mathbf{x}_i\\mathbf{x}_i^\\text{T}\\right),\\hspace{1cm}j\\geq i, \\end{equation} with $\\mathbf{A}_{j+1}^j\\doteq\\mathbf{I}$. Hence, we can express $\\mathbf{w}_t$ as: \\begin{equation} \\mathbf{w}_t=\\mathbf{w}_t^t=\\mathbf{A}_0^{t-1}\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{i:t}^\\lambda\\label{eq:totl.1} \\end{equation} Using \\eqref{eq:tt.1}, we have: \\begin{align} G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\u0026=\\mathbf{w}_i^\\text{T}\\mathbf{x}_i+\\sum_{j=1}^{t}(\\gamma\\lambda)^{j-i}\\delta_j’-\\left(\\mathbf{w}_i^\\text{T}\\mathbf{x}_i+\\sum_{j=1}^{t-1}(\\gamma\\lambda)^{j-i}\\delta_j’\\right) \\\\ \u0026=(\\gamma\\lambda)^{t-i}\\delta_t’\\label{eq:totl.2} \\end{align} with the TD error, $\\delta_t’$ is defined as earlier: \\begin{equation} \\delta_t’\\doteq R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\label{eq:totl.3} \\end{equation} Using \\eqref{eq:totl.1}, \\eqref{eq:totl.2} and \\eqref{eq:totl.3}, we have: \\begin{align} \\mathbf{w}_{t+1}\u0026=\\mathbf{A}_0^t\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t+1}^\\lambda \\\\ \u0026=\\mathbf{A}_0^t\\mathbf{w}_{init}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t+1}^\\lambda+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026=\\mathbf{A}_0^t\\mathbf{w}_0+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i G_{i:t}^\\lambda+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\left(\\mathbf{A}_0^{t-1}\\mathbf{w}_0+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i G_{t:t+1}^\\lambda\\right)\\nonumber \\\\ \u0026\\hspace{1cm}+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i\\left(G_{i:t+1}^\\lambda-G_{i:t}^\\lambda\\right)+\\alpha\\mathbf{x}_t G_{t:t+1}^\\lambda \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}\\delta_t’+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}\\right) \\\\ \u0026=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t’+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t\\mathbf{x}_t\\right) \\\\ \u0026=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t’\\nonumber \\\\ \u0026\\hspace{1cm}+\\alpha\\mathbf{x}_t\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t+\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right) \\\\ \u0026=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t’+\\alpha\\mathbf{x}_t\\delta_t’-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_t(\\gamma\\lambda)^{t-i}\\delta_t’-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\delta_t’-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\left(\\delta_t+\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)-\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\mathbf{z}_t\\delta_t+\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\left(\\mathbf{z}_t-\\mathbf{x}_t\\right),\\label{eq:totl.4} \\end{align} where in the eleventh step, we define $\\mathbf{z}_t$ as: \\begin{equation} \\mathbf{z}_t\\doteq\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}, \\end{equation} and in the twelfth step, we also define $\\delta_t$ as: \\begin{align} \\delta_t\u0026\\doteq\\delta_t’-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t+\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t \\\\ \u0026=R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\end{align} which is the same as the TD error of TD($\\lambda$) we have defined earlier.\nWe then need to derive an update rule to recursively compute $\\mathbf{z}_t$ from $\\mathbf{z}_{t-1}$, as: \\begin{align} \\mathbf{z}_t\u0026=\\sum_{i=0}^{t}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i} \\\\ \u0026=\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^t\\mathbf{x}_i(\\gamma\\lambda)^{t-i}+\\mathbf{x}_t \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\gamma\\lambda\\sum_{i=0}^{t-1}\\mathbf{A}_{i+1}^{t-1}\\mathbf{x}_i(\\gamma\\lambda)^{t-i-1}+\\mathbf{x}_t \\\\ \u0026=\\left(\\mathbf{I}-\\alpha\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right)\\gamma\\lambda\\mathbf{z}_{t-1}+\\mathbf{x}_t \\\\ \u0026=\\gamma\\lambda\\mathbf{z}_{t-1}+\\left(1-\\alpha\\gamma\\lambda\\left(\\mathbf{z}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\right)\\mathbf{x}_t\\label{eq:totl.5} \\end{align} Equations \\eqref{eq:totl.4} and \\eqref{eq:totl.5} form the update of the true online TD($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t+\\alpha\\left(\\mathbf{w}_t^\\text{T}\\mathbf{x}_t-\\mathbf{w}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\left(\\mathbf{z}t_t-\\mathbf{x}_t\\right), \\end{equation} where \\begin{align} \\mathbf{z}_t\u0026\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\left(1-\\alpha\\gamma\\lambda\\left(\\mathbf{z}_{t-1}^\\text{T}\\mathbf{x}_t\\right)\\right)\\mathbf{x}_t,\\label{eq:totl.6} \\\\ \\delta_t\u0026\\doteq R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t \\end{align} Pseudocode of the algorithm is given below.\nAs other methods above, below is an illustration of using true online TD($\\lambda$) on the random walk problem.\nFigure 5: Using True online TD($\\lambda$) on 19-state random walk. The code can be found here The eligible trace \\eqref{eq:totl.6} is called dutch trace to distinguish it from the trace \\eqref{eq:tl.1} of TD($\\lambda$), which is called accumulating trace.\nThere is another kind of trace called replacing trace, defined for the tabular case or for binary feature vectors \\begin{equation} z_{i,t}\\doteq\\begin{cases}1 \u0026\\text{if }x_{i,t}=1 \\\\ \\gamma\\lambda z_{i,t-1} \u0026\\text{if }x_{i,t}=0\\end{cases} \\end{equation}\nEquivalence between forward and backward views In this section, we will show that there is an interchange between forward and backward view.\nTheorem 1\nConsider any forward view that updates towards some interim targets $Y_k^t$ with \\begin{equation} \\mathbf{w}_{k+1}^t=\\mathbf{w}_k+\\eta_k\\left(Y_k^t-\\mathbf{x}_k^\\text{T}\\mathbf{w}_k^t\\right)\\mathbf{x}_k+\\mathbf{u}_k,\\hspace{1cm}0\\leq k\\lt t, \\end{equation} where $\\mathbf{w}_0^t=\\mathbf{w}_0$ for some initial $\\mathbf{w}_0$; $\\mathbf{u}_k\\in\\mathbb{R}^d$ is any vector that does not depend on $t$. Assume that the temporal differences $Y_k^{t+1}-Y_k^t$ for different $k$ are related through \\begin{equation} Y_k^{t+1}-Y_k^t=c_k(Y_{k+1}^{t+1}-Y_{k+1}^t),\\hspace{1cm}\\forall k\\lt t\\label{eq:ebfb.1} \\end{equation} where $c_k$ is a scalar that does not depend on $t$. Then the final weights $\\mathbf{w}_t^t$ at each time step $t$ are equal to the weight $\\mathbf{w}_t$ as defined by $\\mathbf{z}_0=\\eta_0\\mathbf{x}_0$ and the backward view \\begin{align} \\mathbf{w}_{t+1}\u0026=\\mathbf{w}_t+(Y_t^{t+1}-Y_t^t)\\mathbf{z}_t+\\eta_t(Y_t^t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t, \\\\ \\mathbf{z}_t\u0026=c_{t-1}\\mathbf{z}_{t-1}+\\eta_t\\left(1-c_{t-1}\\mathbf{x}_t^\\text{T}\\mathbf{z}_{t-1}\\right)\\mathbf{x}_t,\\hspace{1cm}t\\gt 0 \\end{align}\nProof\nLet $\\mathbf{F}_t\\doteq\\mathbf{I}-\\eta_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}$ be the fading matrix such that $\\mathbf{w}_{t+1}=\\mathbf{F}_k\\mathbf{w}_k^t+\\eta_k Y_k^t\\mathbf{x}_k$. For each step $t$, we have: \\begin{align} \\mathbf{w}_{t+1}^{t+1}-\\mathbf{w}_t^t\u0026=\\mathbf{F}_t\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t+\\eta_t Y_t^{t+1}\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t Y_t^{t+1}\\mathbf{x}_t+(\\mathbf{F}_t-\\mathbf{I})\\mathbf{w}_t^t+\\mathbf{u}_t \\\\ \u0026=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t Y_t^{t+1}\\mathbf{x}_t-\\eta_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\mathbf{w}_t^t+\\mathbf{u}_t \\\\ \u0026=\\mathbf{F}_t(\\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t^t)\\mathbf{x}_t+\\mathbf{u}_t\\label{eq:ebfb.2} \\end{align} We also have that: \\begin{align} \\mathbf{w}_t^{t+1}-\\mathbf{w}_t^t\u0026=\\mathbf{F}_{t-1}(\\mathbf{w}_{t-1}^{t+1}-\\mathbf{w}_{t-1}^t)+\\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\\mathbf{x}_{t-1} \\\\ \u0026=\\mathbf{F}_{t-1}\\mathbf{F}_{t-2}(\\mathbf{w}_{t-1}^{t+1}-\\mathbf{w}_{t-1}^t)+\\eta_{n-2}(Y_{t-2}^{t+1}-Y_{t-2}^t)\\mathbf{F}_{t-1}\\mathbf{x}_{t-2}\\nonumber \\\\ \u0026\\hspace{1cm}+\\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\\mathbf{x}_{t-1} \\\\ \u0026\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026=\\mathbf{F}_{t-1}\\dots\\mathbf{F}_0(\\mathbf{w}_0^{t+1}-\\mathbf{w}_0^t)+\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\\mathbf{x}_k \\\\ \u0026=\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\\mathbf{x}_k \\\\ \u0026=\\sum_{k=0}^{t-1}\\eta_k\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}c_k(Y_{k+1}^{t+1}-Y_{k+1}^t)\\mathbf{x}_k \\\\ \u0026\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026=c_{t-1}\\underbrace{\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-2}c_j\\right)\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k}_{\\doteq\\mathbf{z}_{t-1}}(Y_t^{t+1}-Y_t^t) \\\\ \u0026=c_{t-1}\\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t),\\label{eq:ebfb.3} \\end{align} where in the fifth step, we use the assumption \\eqref{eq:ebfb.1}; the vector $\\mathbf{z}_t$ defined in the sixth step can be computed recursively in terms of $\\mathbf{z}_{t-1}$: \\begin{align} \\mathbf{z}_t\u0026=\\sum_{k=0}^{t}\\eta_k\\left(\\prod_{j=k}^{t-1}c_j\\right)\\mathbf{F}_1\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k \\\\ \u0026=\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-1}c_j\\right)\\mathbf{F}_1\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k+\\eta_t\\mathbf{x}_t \\\\ \u0026=c_{t-1}\\mathbf{F}_t\\sum_{k=0}^{t-1}\\eta_k\\left(\\prod_{j=k}^{t-2}c_j\\right)\\mathbf{F}_{t-1}\\dots\\mathbf{F}_{k+1}\\mathbf{x}_k+\\eta_t\\mathbf{x}_t \\\\ \u0026=c_{t-1}\\mathbf{F}_1\\mathbf{z}_{t-1}+\\eta_t\\mathbf{x}_t \\\\ \u0026=c_{t-1}\\mathbf{z}_{t-1}+\\eta_t(1-c_{t-1}\\mathbf{x}_t^\\text{T}\\mathbf{z}_{t-1})\\mathbf{x}_t \\end{align} Plug \\eqref{eq:ebfb.3} back into \\eqref{eq:ebfb.2} we obtain: \\begin{align} \\mathbf{w}_{t+1}^{t+1}-\\mathbf{w}_t^t\u0026=c_{t-1}\\mathbf{F}_t\\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026=(\\mathbf{z}_t-\\eta_t\\mathbf{x}_t)(Y_t^{t+1}-Y_t^t)+\\eta_t(Y_t^{t+1}-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\\\ \u0026=(Y_t^{t+1}-Y_t^t)\\mathbf{z}_t+\\eta_t(Y_t^t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_t)\\mathbf{x}_t+\\mathbf{u}_t \\end{align} Since $\\mathbf{w}_{0,t}\\doteq\\mathbf{w}_0$, the desired result follows through induction.\nDutch Traces In Monte Carlo Sarsa($\\lambda$) To apply the use off eligible traces on control problems, we begin by defining the $n$-step return, which is the same as what we have defined before: \\begin{equation} \\hspace{-0.5cm}G_{t:t+n}\\doteq\\ R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}t+n\\lt T\\label{eq:sl.1} \\end{equation} with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$. With this definition of the return, the action-value form of offline $\\lambda$-return can be defined as: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t^\\lambda-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\right]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\hspace{1cm}t=0,\\dots,T-1 \\end{equation} where $G_t^\\lambda\\doteq G_{t:\\infty}^\\lambda$.\nThe TD method for action values, known as Sarsa($\\lambda$), approximates this forward view and has the same update rule as TD($\\lambda$): \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t, \\end{equation} except that the TD error, $\\delta_t$, is defined in terms of action-value function: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t), \\end{equation} and so it is with eligible trace vector: \\begin{align} \\mathbf{z}_{-1}\u0026\\doteq\\mathbf{0}, \\\\ \\mathbf{z}\u0026_t\\doteq\\gamma\\lambda\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\hspace{1cm}0\\leq t\\lt T \\end{align}\nFigure 6: (taken from the RL book) The backup diagram of Sarsa($\\lambda$) Pseudocode of the Sarsa($\\lambda$) is given below. There is also an action-value version of the online $\\lambda$-return algorithm, and its efficient implementation as true online TD($\\lambda$), called True online Sarsa($\\lambda$), which can be achieved by using $n$-step return \\eqref{eq:sl.1} instead (which also leads to the change of $\\mathbf{x}_t=\\mathbf{x}(S_t)$ to $\\mathbf{x}_t=\\mathbf{x}(S_t,A_t)$).\nPseudocode of the true online Sarsa($\\lambda$) is given below.\nVariable $\\lambda$ and $\\gamma$ We can generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action. In other words, each time step $t$, we will have a different $\\lambda$ and $\\gamma$, denoted as $\\lambda_t$ and $\\gamma_t$.\nIn particular, say $\\lambda:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$ such that $\\lambda_t\\doteq\\lambda(S_t,A_t)$ and similarly, $\\gamma:\\mathcal{S}\\to[0,1]$ such that $\\gamma_t\\doteq\\gamma(S_t)$.\nWith this definition of $\\gamma$, the return can be rewritten generally as: \\begin{align} G_t\u0026\\doteq R_{t+1}+\\gamma_{t+1}G_{t+1} \\\\ \u0026=R_{t+1}+\\gamma_{t+1}R_{t+2}+\\gamma_{t+1}\\gamma_{t+2}R_{t+3}+\\dots \\\\ \u0026=\\sum_{k=t}^{\\infty}\\left(\\prod_{i=t+1}^{k}\\gamma_i\\right)R_{k+1}, \\end{align} where we require that $\\prod_{k=t}^{\\infty}\\gamma_k=0$ with probability $1$ for all $t$ to assure the sums are finite.\nThe generalization of $\\lambda$ also lets us rewrite the state-based $\\lambda$-return as: \\begin{equation} G_t^{\\lambda s}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\hat{v}(S_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda s}\\Big),\\label{eq:lg.1} \\end{equation} where $G_t^{\\lambda s}$ denotes that this $\\lambda$ -return is bootstrapped from state values, and hence the $G_t^{\\lambda a}$ denotes the $\\lambda$-return that bootstraps from action values. The Sarsa form of action-based $\\lambda$-return is defined as: \\begin{equation} G_t^{\\lambda a}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda a}\\Big), \\end{equation} and the Expected Sarsa form of its can be defined as: \\begin{equation} G_t^{\\lambda a}\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\overline{V}_t(S_{t+1})+\\lambda_{t+1}G_{t+1}^{\\lambda a}\\Big),\\label{eq:lg.2} \\end{equation} where the expected approximate value is generalized to function approximation as: \\begin{equation} \\overline{V}_t\\doteq\\sum_a\\pi(a|s)\\hat{q}(s,a,\\mathbf{w}_t)\\label{eq:lg.3} \\end{equation}\nOff-policy Traces with Control Variates We can also apply the use of importance sampling with eligible traces.\nWe begin with the new definition of $\\lambda$-return, which is achieved by generalizing the $\\lambda$-return \\eqref{eq:lg.1} with the idea of control variates on $n$-step off-policy return: \\begin{equation} \\hspace{-0.5cm}G_t^{\\lambda s}\\doteq\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\big((1-\\lambda_{t+1})\\hat{v}(S_{t+1},\\mathbf{w}_t)+\\lambda_{t+1}G_{t+1}^{\\lambda s}\\big)\\Big)+(1-\\rho_t)\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} where the single-step importance sampling ratio $\\rho_t$ is defined as usual: \\begin{equation} \\rho_t\\doteq\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation} Much like the other returns, the truncated version of this return can be approximated simply in terms of sums of state-based TD errors: \\begin{equation} G_t^{\\lambda s}\\approx\\hat{v}(S_t,\\mathbf{w}_t)+\\rho_t\\sum_{k=t}^{\\infty}\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i, \\end{equation} where the state-based TD error, $\\delta_t^s$, is defined as: \\begin{equation} \\delta_t^s\\doteq R_{t+1}+\\gamma_{t+1}\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t),\\label{eq:optcv.1} \\end{equation} with the approximation becoming exact if the approximate value function does not change. Given this approximation, we have that: \\begin{align} \\mathbf{w}_{t+1}\u0026=\\mathbf{w}_t+\\alpha\\left(G_t^{\\lambda s}-\\hat{v}(S_t,\\mathbf{w}_t)\\right)\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t) \\\\ \u0026\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\sum_{k=t}^{\\infty}\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i\\right)\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t) \\end{align} This is one time step of a forward view. And in fact, the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time. Since the sum of the forward-view update over time is: \\begin{align} \\sum_{t=1}^{\\infty}(\\mathbf{w}_{t+1}-\\mathbf{w}_t)\u0026\\approx\\sum_{t=1}^{\\infty}\\sum_{k=t}^{\\infty}\\alpha\\rho_t\\delta_k^s\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i \\\\ \u0026=\\sum_{k=1}^{\\infty}\\sum_{t=1}^{k}\\alpha\\rho_t\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\delta_k^s\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i \\\\ \u0026=\\sum_{k=1}^{\\infty}\\alpha\\delta_k^s\\sum_{t=1}^{k}\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i,\\label{eq:optcv.2} \\end{align} where in the second step, we use the summation rule: $\\sum_{t=x}^{y}\\sum_{k=t}^{y}=\\sum_{k=x}^{y}\\sum_{t=x}^{k}$.\nLet $\\mathbf{z}_k$ is defined as: \\begin{align} \\mathbf{z}_k \u0026=\\sum_{t=1}^{k}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t, \\mathbf{w}_t\\right)\\prod_{i=t+1}^{k} \\gamma_i\\lambda_i\\rho_i \\\\ \u0026=\\sum_{t=1}^{k-1}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t,\\mathbf{w}_t\\right)\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i+\\rho_k\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right) \\\\ \u0026=\\gamma_k\\lambda_k\\rho_k\\underbrace{\\sum_{t=1}^{k-1}\\rho_t\\nabla_\\mathbf{w}\\hat{v}\\left(S_t,\\mathbf{w}_t\\right)\\prod_{i=t+1}^{k-1}\\gamma_i\\lambda_i\\rho_i}_{\\mathbf{z}_{k-1}}+\\rho_k\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right) \\\\ \u0026=\\rho_k\\big(\\gamma_k\\lambda_k\\mathbf{z}_{k-1}+\\nabla_\\mathbf{w}\\hat{v}\\left(S_k,\\mathbf{w}_k\\right)\\big) \\end{align} Then we can rewrite \\eqref{eq:optcv.2} as: \\begin{equation} \\sum_{t=1}^{\\infty}\\left(\\mathbf{w}_{t+1}-\\mathbf{w}_t\\right)\\approx\\sum_{k=1}^{\\infty}\\alpha\\delta_k^s\\mathbf{z}_k, \\end{equation} which is sum of the backward-view update over time, with the eligible trace vector is defined as: \\begin{equation} \\mathbf{z}_t\\doteq\\rho_t\\big(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\big)\\label{eq:optcv.3} \\end{equation} Using this eligible trace with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$), we obtain a general TD($\\lambda$) algorithm that can be applied to either on-policy or off-policy data.\nIn the on-policy case, the algorithm is exactly TD($\\lambda$) because $\\rho_t=1$ for all $t$ and \\eqref{eq:optcv.3} becomes the accumulating trace \\eqref{eq:tl.1} with extending to variable $\\lambda$ and $\\gamma$. In the off-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable. For action-value function, we generalize the definition of the $\\lambda$-return \\eqref{eq:lg.2} of Expected Sarsa with the idea of control variate: \\begin{align} G_t^{\\lambda a}\u0026\\doteq R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\big[\\rho_{t+1}G_{t+1}^{\\lambda a}+\\bar{V}_t(S_{t+1})\\nonumber \\\\ \u0026\\hspace{2cm}-\\rho_{t+1}\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\big]\\Big) \\\\ \u0026=R_{t+1}+\\gamma_{t+1}\\Big(\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\rho_{t+1}\\left[G_{t+1}^{\\lambda a}-\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\right]\\Big), \\end{align} where the expected approximate value $\\bar{V}_t(S_{t+1})$ is as given by \\eqref{eq:lg.3}.\nSimilar to the others, this $\\lambda$-return can also be written approximately as the sum of TD errors \\begin{equation} G_t^{\\lambda a}\\approx\\hat{q}(S_t,A_t,\\mathbf{w}_t)+\\sum_{k=t}^{\\infty}\\delta_k^a\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\rho_i, \\end{equation} with the action-based TD error is defined in terms of the expected approximate value: \\begin{equation} \\delta_t^a=R_{t+1}+\\gamma_{t+1}\\bar{V}_t(S_{t+1})-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:optcv.4} \\end{equation} Analogy to the state value function case, this approximation also becomes exact if the approximate value function does not change.\nSimilar to the state case \\eqref{eq:optcv.3}, we can also define the eligible trace for action values: \\begin{equation} \\mathbf{z}_t\\doteq\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Using this eligible trace with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$) and the expectation-based TD error \\eqref{eq:optcv.4}, we end up with an Expected Sarsa($\\lambda$) algorithm that can applied to either on-policy or off-policy data.\nIn the on-policy case with constant $\\lambda$ and $\\gamma$, this becomes the Sarsa($\\lambda$) algorithm. Tree-Backup($\\lambda$) Recall that in the note of TD-Learning, we have mentioned that there is an off-policy method without importance sampling called tree-backup. Can we extend the idea of tree-backup to an eligible trace version? Yes, we can.\nAs usual, we begin with establishing the $\\lambda$-return by generalizing the $\\lambda$-return of Expected Sarsa \\eqref{eq:lg.2} with the $n$-step Tree-backup return: \\begin{align} G_t^{\\lambda a}\u0026\\doteq R_{t+1}+\\gamma_{t+1}\\Bigg((1-\\lambda_{t+1})\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\Big[\\sum_{a\\neq A_{t+1}}\\pi(a|S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)\\nonumber \\\\ \u0026\\hspace{2cm}+\\pi(A_{t+1}|S_{t+1})G_{t+1}^{\\lambda a}\\Big]\\Bigg) \\\\ \u0026=R_{t+1}+\\gamma_{t+1}\\Big(\\bar{V}_t(S_{t+1})+\\lambda_{t+1}\\pi(A_{t+1}|S_{t+1})\\left(G_{t+1}^{\\lambda a}-\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)\\right)\\Big) \\end{align} This return, as usual, can also be written approximately (ignoring changes in the approximate value function) as sum of TD errors: \\begin{equation} G_t^{\\lambda a}\\approx\\hat{q}(S_t,A_t,\\mathbf{w}_t)+\\sum_{k=t}^{\\infty}\\delta_k^a\\prod_{i=t+1}^{k}\\gamma_i\\lambda_i\\pi(A_i|S_i), \\end{equation} with the TD error is defined as given by \\eqref{eq:optcv.4}.\nSimilar to how we derive the eligible trace \\eqref{eq:optcv.3}, we can define a new eligible trace in terms of target-policy probabilities of the selected actions: \\begin{equation} \\mathbf{z}_t\\doteq\\gamma_t\\lambda_t\\pi(A_t|S_t)\\mathbf{z}_{t-1}+\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Using this eligible trace vector with the parameter update rule \\eqref{eq:tl.2} of TD($\\lambda$), we end up with the Tree-Backup($\\lambda$) or TB($\\lambda$).\nFigure 7: (taken from the RL book) The backup diagram of Tree Backup($\\lambda$) Other Off-policy Methods with Traces GTD($\\lambda$) GTD($\\lambda$) is the extended version of TDC, a state-value Gradient-TD method, with eligible traces.\nIn this algorithm, we will define a new off-policy, $\\lambda$-return, not like usual but as a function: \\begin{equation} G_t^{\\lambda}(v)\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})v(S_{t+1})+\\lambda_{t+1}G_{t+1}^{\\lambda}(v)\\Big]\\label{eq:gl.1} \\end{equation} where $v(s)$ denotes the value at state $s$, and $\\lambda\\in[0,1]$ is the trace-decay parameter.\nLet $T_\\pi^\\lambda$ denote the $\\lambda$-weighted Bellman operator for policy $\\pi$ such that: \\begin{align} v_\\pi(s)\u0026=\\mathbb{E}\\Big[G_t^\\lambda(v_\\pi)\\big|S_t=s,\\pi\\Big] \\\\ \u0026\\doteq (T_\\pi^\\lambda v_\\pi)(s) \\end{align}\nConsider using linear function approximation, or in particular, we are trying to approximate $v(s)$ by $v_\\mathbf{w}(s)=\\mathbf{w}^\\text{T}\\mathbf{x}(s)$. Our objective is to find the fixed point which satisfies: \\begin{equation} v_\\mathbf{w}=\\Pi T_\\pi^\\lambda v_\\mathbf{w},\\label{eq:gl.2} \\end{equation} where $\\Pi v$ is a projection of $v$ into the space of representable functions $\\{v_\\mathbf{w}|\\mathbf{w}\\in\\mathbb{R}^d\\}$. Let $\\mu$ be the steady-state distribution of states under the behavior policy $b$. Then, the projection can be defined as: \\begin{equation} \\Pi v\\doteq v_{\\mathbf{w}}, \\end{equation} where \\begin{equation} \\mathbf{w}=\\underset{\\mathbf{w}\\in\\mathbb{R}^d}{\\text{argmin}}\\left\\Vert v-v_\\mathbf{w}\\right\\Vert_\\mu^2, \\end{equation} In a linear case, in which $v_\\mathbf{w}=\\mathbf{X}\\mathbf{w}$, the projection operator is linear and independent of $\\mathbf{w}$: \\begin{equation} \\Pi=\\mathbf{X}(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X})^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}, \\end{equation} where $\\mathbf{D}$ denotes $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix whose diagonal elements are $\\mu(s)$, and $\\mathbf{X}$ denotes the $\\vert\\mathcal{S}\\vert\\times d$ matrix whose rows are the feature vectors $\\mathbf{x}(s)^\\text{T}$, one for each state $s$.\nWith linear function approximation, we can rewrite the $\\lambda$-return \\eqref{eq:gl.1} as: \\begin{equation} G_t^{\\lambda}(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda}(\\mathbf{w})\\Big]\\label{eq:gl.3} \\end{equation} Let \\begin{equation} \\delta_t^\\lambda(\\mathbf{w})\\doteq G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t, \\end{equation} and \\begin{equation} \\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\doteq\\sum_s\\mu(s)\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})\\big|S_t=s,\\pi\\Big]\\mathbf{x}(s), \\end{equation} where $\\mathcal{P}_\\mu^\\pi$ is an operator.\nThe fixed point in \\eqref{eq:gl.2} can be found by minimizing the Mean Square Projected Bellman Error (MSPBE): \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026=\\Big\\Vert v_\\mathbf{w}-\\Pi T_\\pi^\\lambda v_\\mathbf{w}\\Big\\Vert_\\mu^2 \\\\ \u0026=\\Big\\Vert\\Pi(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w})\\Big\\Vert_\\mu^2 \\\\ \u0026=\\Big(\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)\\Big)^\\text{T}\\mathbf{D}\\Big(\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)\\Big) \\\\ \u0026=\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)^\\text{T}\\Pi^\\text{T}\\mathbf{D}\\Pi\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right) \\\\ \u0026=\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right)^\\text{T}\\mathbf{D}^\\text{T}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{D}\\left(v_\\mathbf{w}-T_\\pi^\\lambda v_\\mathbf{w}\\right) \\\\ \u0026=\\Big(\\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-\\mathbf{w}\\right)\\Big)^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)\\label{eq:gl.4} \\end{align}\nFrom the definition of $T_\\pi^\\lambda$ and $\\delta_t^\\lambda$, we have: \\begin{align} (T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{v})(s)\u0026=\\mathbb{E}\\Big[G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\big|S_t=s,\\pi\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})\\big|S_t=s,\\pi\\Big]\\label{eq:gl.5} \\end{align} Therefore, \\begin{align} \\mathbf{X}^\\text{T}\\mathbf{D}\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)\u0026=\\sum_s\\mu(s)\\Big[\\left(T_\\pi^\\lambda v_\\mathbf{w}-v_\\mathbf{w}\\right)(s)\\Big]\\mathbf{x}(s) \\\\ \u0026=\\sum_s\\mu(s)\\mathbb{E}\\Big[\\delta_t^\\lambda(\\mathbf{w})|S_t=s,\\pi\\Big]\\mathbf{x}(s) \\\\ \u0026=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\label{eq:gl.6} \\end{align} Moreover, we also have: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}=\\sum_s\\mu(s)\\mathbf{x}(s)\\mathbf{x}(s)^\\text{T}=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]\\label{eq:gl.7} \\end{equation} Substitute \\eqref{eq:gl.5}, \\eqref{eq:gl.6} and \\eqref{eq:gl.7} back to the \\eqref{eq:gl.4}, we have: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)\\label{eq:gl.8} \\end{equation} In the objective function \\eqref{eq:gl.8}, the expectation terms are w.r.t the policy $\\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy problem, as usual, we use importance sampling.\nWe then instead use an importance-sampling version of $\\lambda$-return \\eqref{eq:gl.3}: \\begin{equation} G_t^{\\lambda\\rho}(\\mathbf{w})=\\rho_t\\left(R_{t+1}+\\gamma_{t+1}\\left[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\right]\\right), \\end{equation} where the single-step importance sampling ratio $\\rho_t$ is defined as usual: \\begin{equation} \\rho_t\\doteq\\frac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation} This also leads to an another version of $\\delta_t^\\lambda$, defined as: \\begin{equation} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\\doteq G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} With this definition of the $\\lambda$-return, we have: \\begin{align} \u0026\\hspace{-1cm}\\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big]\\nonumber \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[\\rho_t\\big(R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big)+\\rho_t\\gamma_{t+1}\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[\\rho_t\\big(R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big)\\big|S_t=s\\Big]+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big|S_t=s,\\pi\\Big]\\nonumber \\\\ \u0026\\hspace{1cm}+\\sum_{a,s’}p(s’|s,a)b(a|s)\\frac{\\pi(a|s)}{b(a|s)}\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\big|S_t=s,\\pi\\Big]\\nonumber \\\\ \u0026\\hspace{1cm}+\\sum_{a,s’}p(s’|s,a)\\pi(a|s)\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’\\Big]\\big|S_t=s,\\pi\\Big], \\end{align} which, as it continues to roll out, gives us: \\begin{equation} \\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s\\Big]=\\mathbb{E}\\Big[G_t^{\\lambda}(\\mathbf{w})\\big|S_t=s,\\pi\\Big] \\end{equation} And eventually, we get: \\begin{equation} \\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t \\end{equation} because the state distribution is based on behavior state-distribution $\\mu$.\nWith this result, our objective function \\eqref{eq:gl.8} can be written as: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big) \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\\label{eq:gl.9} \\end{align} From the definition of $\\delta_t^{\\lambda\\rho}$, we have: \\begin{align} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\u0026=G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big]\\Big)-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t+\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)\\nonumber \\\\ \u0026\\hspace{2cm}-\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026=\\rho_t\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)+\\rho_t\\mathbf{w}^\\text{T}\\mathbf{x}_t-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\nonumber \\\\ \u0026\\hspace{2cm}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\Big(G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big) \\\\ \u0026=\\rho_t\\delta_t(\\mathbf{w})+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w}), \\end{align} where the TD error, $\\delta_t(\\mathbf{w})$, is defined as usual: \\begin{equation} \\delta_t(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} Also, we have that: \\begin{align} \\mathbb{E}\\Big[(1-\\rho_t)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\mathbf{x}_t\\Big]\u0026=\\sum_{s,a}\\mu(s)b(a|s)\\left(1-\\frac{\\pi(a|s)}{b(a|s)}\\right)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026=\\sum_s\\mu(s)\\left(\\sum_a b(a|s)-\\sum_a\\pi(a|s)\\right)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026=\\sum_s\\mu(s)(1-1)\\mathbf{w}^\\text{T}\\mathbf{x}(s)\\mathbf{x}(s) \\\\ \u0026=0 \\end{align} Given these results, we have: \\begin{align} \\hspace{-1cm}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\mathbf{x}_t+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+0+\\mathbb{E}_{\\pi b}\\Big[\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\big(\\rho_t\\delta_t(\\mathbf{w})+(\\rho_t-1)\\mathbf{w}^\\text{T}\\mathbf{x}_t\\nonumber \\\\ \u0026\\hspace{2cm}+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\big(\\rho_t\\delta_t(\\mathbf{w})+\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}\\big)+\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\rho_t\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}\\big)+\\rho_{t-2}\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}\\Big] \\\\ \u0026\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\rho_t\\big(\\mathbf{x}_t+\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-1}+\\rho_{t-2}\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\mathbf{x}_{t-2}+\\dots\\big)\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big], \\end{align} where \\begin{equation} \\mathbf{z}_t=\\rho_t(\\mathbf{x}_t+\\gamma_t\\lambda_t\\mathbf{z}_{t-1}) \\end{equation} Plugging this result back to \\eqref{eq:gl.9} lets our objective function become: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\label{eq:gl.10} \\end{equation} Similar to TDC, we also use gradient descent in order to find the minimum value of $\\overline{\\text{PBE}}(\\mathbf{w})$. The gradient of our objective function w.r.t the weight vector $\\mathbf{w}$ is: \\begin{align} \\hspace{-1.2cm}\\frac{1}{2}\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})\u0026=-\\frac{1}{2}\\nabla_\\mathbf{w}\\Bigg(\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\Bigg) \\\\ \u0026=\\nabla_\\mathbf{w}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\big(\\gamma_{t+1}\\mathbf{x}_{t+1}-\\mathbf{x}_t\\big)\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\rho_t\\big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\mathbf{z}_{t-1}\\big)^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\big(\\mathbf{x}_t\\rho_t\\mathbf{x}_t^\\text{T}+\\mathbf{x}_t\\rho_t\\gamma_t\\lambda_t\\mathbf{z}_{t-1}^\\text{T}\\big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}-\\big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\mathbf{x}_{t+1}\\gamma_{t+1}\\lambda_{t+1}\\mathbf{z}_t^\\text{T}\\big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}-\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbf{v}(\\mathbf{w}),\\label{eq:gl.11} \\end{align} where in the seventh step, we have used shifting indices trick and the identities: \\begin{align} \\mathbb{E}\\Big[\\mathbf{x}_t\\rho_t\\mathbf{x}_t^\\text{T}\\Big]\u0026=\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big], \\\\ \\mathbb{E}\\Big[\\mathbf{x}_{t+1}\\rho_t\\gamma_t\\lambda_t\\mathbf{z}_t^\\text{T}\\Big]\u0026=\\mathbb{E}\\Big[\\mathbf{x}_{t+1}\\gamma_t\\lambda_t\\mathbf{z}_t^\\text{T}\\Big] \\end{align} and where in the final step, we define: \\begin{equation} \\mathbf{v}(\\mathbf{w})\\doteq\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} By direct sampling from \\eqref{eq:gl.11} and following TDC derivation steps we obtain the GTD($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^s\\mathbf{z}_t-\\alpha\\gamma_{t+1}(1-\\lambda_{t+1})(\\mathbf{z}_t^\\text{T}\\mathbf{v}_t)\\mathbf{x}_{t+1}, \\end{equation} where\nthe TD error $\\delta_t^s$ is defined, as usual, as state-based TD error \\eqref{eq:optcv.1}; the eligible trace vector $\\mathbf{z}_t$ is defined as given in \\eqref{eq:optcv.3} for state value; and $\\mathbf{v}_t$ is a vector of the same dimension as $\\mathbf{w}$, initialized to $\\mathbf{v}_0=\\mathbf{0}$ with $\\beta\u003e0$ is a step-size parameter: \\begin{align} \\delta_t^s\u0026\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026\\doteq\\rho_t(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\mathbf{x}_t), \\\\ \\mathbf{v}_{t+1}\u0026\\doteq\\mathbf{v}_t+\\beta\\delta_t^s\\mathbf{z}_t-\\beta(\\mathbf{v}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t \\end{align} GQ($\\lambda$) GQ($\\lambda$) is another eligible trace version of a Gradient-TD method but with action values. Its goal is to learn a parameter $\\mathbf{w}_t$ such that $\\hat{q}(s,a,\\mathbf{w}_t)\\doteq\\mathbf{w}_t^\\text{T}\\mathbf{x}(s,a)\\approx q_\\pi(s,a)$ from data given by following a behavior policy $b$.\nSimilar to the state-values case of GTD($\\lambda$), we begin with the definition of $\\lambda$-return (function): \\begin{equation} G_t^\\lambda(q)\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})q(S_{t+1},A_{t+1})+\\lambda_{t+1}G_{t+1}^\\lambda(q)\\Big],\\label{eq:gql.1} \\end{equation} where $q(s,a)$ denotes the value of taking action $a$ at state $s$ and $\\lambda\\in[0,1]$ is the trace decay parameter.\nLet $T_\\pi^\\lambda$ denote the $\\lambda$-weighted state-action version of the affine $\\vert\\mathcal{S}\\times\\mathcal{A}\\vert\\times\\vert\\mathcal{S}\\times\\mathcal{A}\\vert$ Bellman operator for the target policy $\\pi$ such that: \\begin{align} q_\\pi(s,a)\u0026=\\mathbb{E}\\Big[G_t^\\lambda(q_\\pi)\\big|S_t=s,A_t=a,\\pi\\Big] \\\\ \u0026\\doteq(T_\\pi^\\lambda q_\\pi)(s,a) \\end{align} Analogous to the state value functions, with linear function approximation (i.e., we are trying to estimate $q(s,a)$ by $q_\\mathbf{w}(s,a)=\\mathbf{w}^\\text{T}\\mathbf{x}(s,a)$), our objective is to find the fixed point $q_\\mathbf{w}$ such that: \\begin{equation} q_\\mathbf{w}=\\Pi T_\\pi^\\lambda q_\\mathbf{w}, \\end{equation} where $\\Pi$ is the projection operator defined as above. This point also can be found by minimizing the MSPBE objective function: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026=\\left\\Vert q_\\mathbf{w}-\\Pi T_\\pi^\\lambda q_\\mathbf{w}\\right\\Vert_\\mu^2 \\\\ \u0026=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big),\\label{eq:gql.2} \\end{align} where the second step is acquired from the result \\eqref{eq:gl.8}, and where the TD error $\\delta_t^\\lambda$ is defined as the above section: \\begin{equation} \\delta_t^\\lambda(\\mathbf{w})\\doteq G_t^\\lambda(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\end{equation} where $G_t^\\lambda$ as given in \\eqref{eq:gl.3}.\nIn the objective function \\eqref{eq:gql.2}, the expectation terms are w.r.t the policy $\\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy issue, as usual, we use importance sampling.\nWe start with the definition of the $\\lambda$-return \\eqref{eq:gql.1}, which is a noisy estimate of the future return by following policy $\\pi$. In order to have a noisy estimate for the return of target policy $\\pi$ while following behavior policy $b$, we define another $\\lambda$-return (function), based on importance sampling: \\begin{equation} G_t^{\\lambda\\rho}(\\mathbf{w})\\doteq R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big],\\label{eq:gql.3} \\end{equation} where $\\bar{\\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\\pi$: \\begin{equation} \\bar{\\mathbf{x}}_t\\doteq\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a), \\end{equation} where $\\rho_t$ is the single-step importance sampling ratio, and $G_t^{\\lambda\\rho}(\\mathbf{w})$ is a noisy guess of future rewards of target policy $\\pi$, if the agent follows policy $\\pi$ from time $t$.\nLet \\begin{equation} \\delta_t^{\\lambda\\rho}(\\mathbf{w})\\doteq G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\label{eq:gql.4} \\end{equation} With the definition of the $\\lambda$-return \\eqref{eq:gql.3}, we have that: \\begin{align} \u0026\\hspace{-0.9cm}\\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big]\\nonumber \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}\\Big((1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big)\\big|S_t=s,A_t=a\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026+\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026+\\sum_{s’}p(s’|s,a)\\sum_{a’}b(a’|s’)\\frac{\\pi(a’|s’)}{b(a’|s’)}\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’,A_{t+1}=a’\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\big|S_t=s,A_t=a,\\pi\\Big]\\nonumber \\\\ \u0026+\\sum_{s’,a’}p(s’|s,a)\\pi(a’|s’)\\gamma_{t+1}\\lambda_{t+1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’,A_{t+1}=a’\\Big] \\\\ \u0026\\hspace{-1cm}=\\mathbb{E}\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\nonumber \\\\ \u0026+\\gamma_{t+1}\\lambda_{t_1}\\mathbb{E}\\Big[G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\big|S_{t+1}=s’,A_{t+1}=a’\\Big]\\big|S_t=s,A_t=a,\\pi\\Big], \\end{align} which, as continues to roll out, gives us: \\begin{equation} \\mathbb{E}\\Big[G_t^{\\lambda\\rho}(\\mathbf{w})\\big|S_t=s,A_t=a\\Big]=\\mathbb{E}\\Big[G_t^\\lambda(\\mathbf{w})\\big|S_t=s,A_t=a,\\pi\\Big] \\end{equation} And eventually, it yields: \\begin{equation} \\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]=\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t, \\end{equation} because the state-action distribution is based on the behavior state-action pair distribution, $\\mu$.\nHence, the objective function \\eqref{eq:gql.2} can be written as: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026=\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big)^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\Big(\\mathcal{P}_\\mu^\\pi\\delta_t^\\lambda(\\mathbf{w})\\mathbf{x}_t\\Big) \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\\label{eq:gql.5} \\end{align} From the definition of the importance-sampling based TD error\\eqref{eq:gql.4}, we have: \\begin{align} \u0026\\hspace{-0.8cm}\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026\\hspace{-1cm}=G_t^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026\\hspace{-1cm}=R_{t+1}+\\gamma_{t+1}\\Big[(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big]-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026\\hspace{-1cm}=\\Big[R_{t+1}+\\gamma_{t+1}(1-\\lambda_{t+1})\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\Big]+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_t \\\\ \u0026\\hspace{-1cm}=\\Big(R_{t+1}+\\gamma_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\Big)-\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w}) \\\\ \u0026\\hspace{-1cm}=\\delta_t(\\mathbf{w})-\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}G_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026\\hspace{1cm}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\Big(\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big) \\\\ \u0026\\hspace{-1cm}=\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\Big(G_{t+1}^{\\lambda\\rho}(\\mathbf{w})-\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}\\Big)+\\gamma_{t+1}\\lambda_{t+1}\\Big(\\rho_{t+1}\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}\\Big) \\\\ \u0026\\hspace{-1cm}=\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big), \\end{align} where in the fifth step, we define: \\begin{equation} \\delta_t(\\mathbf{w})\\doteq R_{t+1}+\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\label{eq:gql.6} \\end{equation} Note that the last part of the above equation has expected value of vector zero under the behavior policy $b$ because: \\begin{align} \\mathbb{E}\\Big[\\rho_t\\mathbf{x}_t\\big|S_t\\Big]\u0026=\\sum_a b(a|S_t)\\frac{\\pi(a|S_t)}{b(a|S_t)}\\mathbf{x}(S_t,a) \\\\ \u0026=\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a) \\\\ \u0026=\\bar{\\mathbf{x}}_t \\end{align} With the result obtained above, we have: \\begin{align} \\hspace{-1cm}\\mathbb{E}\\Big[\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]\u0026=\\mathbb{E}\\Big[\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026\\hspace{2cm}+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\Big)\\mathbf{x}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\Big)\\mathbf{x}_t\\Big]\\nonumber \\\\ \u0026\\hspace{2cm}+\\mathbb{E}\\Big[\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\mathbf{x}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_t\\Big]+0 \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\rho_t\\Big(\\delta_t(\\mathbf{w})+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\nonumber \\\\ \u0026\\hspace{2cm}+\\gamma_{t+1}\\lambda_{t+1}\\mathbf{w}^\\text{T}\\big(\\rho_{t+1}\\mathbf{x}_{t+1}-\\bar{\\mathbf{x}}_{t+1}\\big)\\Big)\\mathbf{x}_{t-1}\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{x}_t\\Big]+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\delta_t(\\mathbf{w})\\mathbf{x}_{t-1}\\Big]\\nonumber \\\\ \u0026\\hspace{2cm}+\\mathbb{E}\\Big[\\gamma_t\\lambda_t\\rho_t\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\delta_{t+1}^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-1}\\Big]+0 \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_{t-1}\\big)\\Big]+\\mathbb{E}\\Big[\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}\\Big] \\\\ \u0026\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026=\\mathbb{E}_b\\Big[\\delta_t(\\mathbf{w})\\Big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_{t-1}+\\gamma_{t-1}\\lambda_{t-1}\\rho_{t-1}\\gamma_t\\lambda_t\\rho_t\\delta_t^{\\lambda\\rho}(\\mathbf{w})\\mathbf{x}_{t-2}+\\dots\\Big)\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big], \\end{align} where \\begin{equation} \\mathbf{z}_t\\doteq\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}\\label{eq:gql.7} \\end{equation} Plugging this result back to our objective function \\eqref{eq:gql.5} gives us: \\begin{equation} \\overline{\\text{PBE}}(\\mathbf{w})=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} Following the derivation of GTD($\\lambda$), we have: \\begin{align} \u0026-\\frac{1}{2}\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})\\nonumber \\\\ \u0026=-\\frac{1}{2}\\nabla_\\mathbf{w}\\Bigg(\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]^\\text{T}\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]\\Bigg) \\\\ \u0026=\\nabla_\\mathbf{w}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\big(\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{x}_t\\big)\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\Big(\\mathbf{x}_t+\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}\\Big)^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_t\\lambda_t\\rho_t\\mathbf{x}_t\\mathbf{z}_{t-1}^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_{t+1}\\lambda_{t+1}\\rho_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\Big(\\mathbf{x}_t\\mathbf{x}_t^\\text{T}+\\gamma_{t+1}\\lambda_{t+1}\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big)\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}-\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\\\ \u0026=\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big]-\\mathbb{E}\\Big[\\gamma_{t+1}(1-\\lambda_{t+1})\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]\\mathbf{v}(\\mathbf{w}), \\end{align} where in the eighth step, we have used the identity: \\begin{equation} \\mathbb{E}\\Big[\\rho_{t+1}\\mathbf{x}_{t+1}\\mathbf{z}_t^\\text{T}\\Big]=\\mathbb{E}\\Big[\\bar{\\mathbf{x}}_{t+1}\\mathbf{z}_t^\\text{T}\\Big], \\end{equation} and where in the final step, we define: \\begin{equation} \\mathbf{v}(\\mathbf{w})\\doteq\\mathbb{E}\\Big[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\Big]^{-1}\\mathbb{E}\\Big[\\delta_t(\\mathbf{w})\\mathbf{z}_t\\Big] \\end{equation} By direct sampling from the above gradient-descent direction and weight-duplication trick, we obtain the GQ($\\lambda$) algorithm: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^a\\mathbf{z}_t-\\alpha\\gamma_{t+1}(1-\\lambda_{t+1})(\\mathbf{z}_t^\\text{T}\\mathbf{v}_t)\\bar{\\mathbf{x}}_{t+1}, \\end{equation} where\n$\\bar{\\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\\pi$; $\\delta_t^a$ is the expectation form of the TD error, defined as \\eqref{eq:gql.6}; the eligible trace vector $\\mathbf{z}_t$ is defined as \\eqref{eq:gql.7} for action value; and $\\mathbf{v}_t$ is defined as in GTD($\\lambda$): \\begin{align} \\bar{\\mathbf{x}}_t\u0026\\doteq\\sum_a\\pi(a|S_t)\\mathbf{x}(S_t,a), \\\\ \\delta_t^a\u0026\\doteq R_{t+1}+\\lambda_{t+1}\\mathbf{w}^\\text{T}\\bar{\\mathbf{x}}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026\\doteq\\gamma_t\\lambda_t\\rho_t\\mathbf{z}_{t-1}+\\mathbf{x}_t, \\\\ \\mathbf{v}_{t+1}\u0026\\doteq\\mathbf{v}_t+\\beta\\delta_t^a\\mathbf{z}_t-\\beta(\\mathbf{v}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t \\end{align} Greedy-GQ($\\lambda$) If the target policy is $\\varepsilon$-greedy, or otherwise biased towards the greedy policy for $\\hat{q}$, then GQ($\\lambda$) can be used as a control algorithm, called Greedy-GQ($\\lambda$).\nIn the case of $\\lambda=0$, called GQ(0), Greedy-GQ($\\lambda$) is defined by: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t^a\\mathbf{x}_t+\\alpha\\gamma_{t+1}(\\mathbf{z}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}(S_{t+1},a_{t+1}^{*}), \\end{equation} where the eligible trace $\\mathbf{z}_t$, TD error $\\delta_t^a$ and $a_{t+1}^{*}$ are defined as: \\begin{align} \\mathbf{z}_t\u0026\\doteq\\mathbf{z}_t+\\beta\\delta_t^a\\mathbf{x}_t-\\beta(\\mathbf{z}_t^\\text{T}\\mathbf{x}_t)\\mathbf{x}_t, \\\\ \\delta_t^a\u0026\\doteq R_{t+1}+\\gamma_{t+1}\\max_a\\Big(\\mathbf{w}_t^\\text{T}\\mathbf{x}(S_{t+1},a)\\Big)-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ a_{t+1}^{*}\u0026\\doteq\\underset{a}{\\text{argmax}}\\Big(\\mathbf{w}_t^\\text{T}\\mathbf{x}(S_{t+1},a)\\Big), \\end{align} where $\\beta\u003e0$ is a step-size parameter.\nHTD($\\lambda$) HTD($\\lambda$) is a hybrid state-value algorithm combining aspects of GTD($\\lambda$) and TD($\\lambda$), and has the following update: \\begin{align} \\mathbf{w}_{t+1}\u0026\\doteq\\mathbf{w}_t+\\alpha\\delta_t^s\\mathbf{z}_t+\\alpha\\left(\\left(\\mathbf{z}_t-\\mathbf{z}_t^b\\right)^\\text{T}\\mathbf{v}_t\\right)\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right), \\\\ \\mathbf{v}_{t+1}\u0026\\doteq\\mathbf{v}_t+\\beta\\delta_t^s\\mathbf{z}_t-\\beta\\left({\\mathbf{z}_t^b}^\\text{T}\\mathbf{v}_t\\right)\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right), \\\\ \\mathbf{z}_t\u0026\\doteq\\rho_t\\left(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+\\mathbf{x}_t\\right), \\\\ \\mathbf{z}_t^b\u0026\\doteq\\gamma_t\\lambda_t\\mathbf{z}_{t-1}^b+\\mathbf{x}_t, \\end{align}\nEmphatic TD($\\lambda$) Emphatic TD($\\lambda$) (ETD($\\lambda$)) is the extension of the one-step Emphatic-TD algorithm to eligible traces. It is defined by: \\begin{align} \\mathbf{w}_{t+1}\u0026\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\mathbf{z}_t, \\\\ \\delta_t\u0026\\doteq R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t, \\\\ \\mathbf{z}_t\u0026\\doteq\\rho_t\\left(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\right), \\\\ M_t\u0026\\doteq\\gamma_t i(S_t)+(1-\\lambda_t)F_t, \\\\ F_t\u0026\\doteq\\rho_{t-1}\\gamma_t F_{t-1}+i(S_t), \\end{align} where\n$M_t\\geq 0$ is the general form of emphasis; $i:\\mathcal{S}\\to[0,\\infty)$ is the interest function $F_t\\geq 0$ is the followon trace, with $F_0\\doteq i(S_0)$. Stability Consider any stochastic algorithm of the form, \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha(\\mathbf{b}_t-\\mathbf{A}_t\\mathbf{w}_t), \\end{equation} where $\\mathbf{A}_t\\in\\mathbb{R}^d\\times\\mathbb{R}^d$ be a matrix and $\\mathbf{b}_t\\in\\mathbb{R}^d$ be a vector that varies over time. Let \\begin{align} \\mathbf{A}\u0026\\doteq\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{A}_t\\right], \\\\ \\mathbf{b}\u0026\\doteq\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{b}_t\\right] \\end{align} We define the stochastic update to be stable if and only if the corresponding deterministic algorithm, \\begin{equation} \\bar{\\mathbf{w}}_{t+1}\\doteq\\bar{\\mathbf{w}}_t+\\alpha\\left(\\mathbf{b}-\\mathbf{A}\\bar{\\mathbf{w}}_t\\right), \\end{equation} is convergent to a unique fixed point independent of the initial $\\bar{\\mathbf{w}}_0$. This will occur iff $\\mathbf{A}$ has a full set of eigenvalues having positive real parts, which can be proved if $\\mathbf{A}$ is positive definite.\nWith this definition of stability, in order to exam the stability of ETD($\\lambda$), we begin by considering the SGD update for the weight vector $\\mathbf{w}$ at time step $t$. \\begin{align} \\mathbf{w}_{t+1}\u0026\\doteq\\mathbf{w}_t+\\alpha\\left(R_{t+1}+\\gamma_{t+1}\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right)\\mathbf{z}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\left(\\mathbf{z}_t R_{t+1}-\\mathbf{z}_t\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right)^\\text{T}\\mathbf{w}_t\\right)\\label{eq:es.1} \\end{align} Let $\\mathbf{A}_t\\in\\mathbb{R}^d\\times\\mathbb{R}^d$ be a matrix and $\\mathbf{b}_t\\in\\mathbb{R}^d$ be a vector such that: \\begin{align} \\mathbf{A}_t\u0026\\doteq\\mathbf{z}_t\\left(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\right)^\\text{T}, \\\\ \\mathbf{b}_t\u0026\\doteq\\mathbf{z}_t R_{t+1} \\end{align} The stochastic update \\eqref{eq:es.1} is then can be written as: \\begin{align} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left(\\mathbf{b}_t-\\mathbf{A}_t\\mathbf{w}_t\\right) \\end{align} From the definition of $\\mathbf{A}$, we have: \\begin{align} \\mathbf{A}\u0026=\\lim_{t\\to\\infty}\\mathbb{E}\\left[\\mathbf{A}_t\\right] \\\\ \u0026=\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\Big] \\\\ \u0026=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\rho_t\\big(\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big)\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026=\\sum_s\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big]\\mathbb{E}_b\\Big[\\rho_t\\big(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1}\\big)^\\text{T}\\big|S_t=s\\Big] \\\\ \u0026=\\sum_s\\underbrace{\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big]}_{\\mathbf{z}(s)}\\mathbb{E}_b\\Big[\\rho_k\\big(\\mathbf{x}_k-\\gamma_{k+1}\\mathbf{x}_{k+1}\\big)^\\text{T}\\big|S_k=s\\Big] \\\\ \u0026=\\sum_s\\mathbf{z}(s)\\mathbb{E}_\\pi\\Big[\\mathbf{x}_k-\\gamma_{k+1}\\mathbf{x}_{k+1}\\big|S_k=s\\Big] \\\\ \u0026=\\sum_s\\mathbf{z}(s)\\Big(\\mathbf{x}_t-\\sum_{s’}\\left[\\mathbf{P}_\\pi\\right]_{ss’}\\gamma(s’)\\mathbf{x}(s’)\\Big)^\\text{T} \\\\ \u0026=\\mathbf{Z}\\left(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\right)\\mathbf{X},\\label{eq:es.2} \\end{align} where\nin the fifth step, given $S_t=s$, $\\mathbf{z}_{t-1}$ and $M_t$ are independent of $\\rho_t(\\mathbf{x}_t-\\gamma_{t+1}\\mathbf{x}_{t+1})^\\text{T}$; $\\mathbf{P}_\\pi$ represents the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix of transition probabilities: \\begin{equation} \\left[\\mathbf{P}_\\pi\\right]_{ij}\\doteq\\sum_a\\pi(a|i)p(j|i,a), \\end{equation} where $p(j|i,a)\\doteq P(S_{t+1}=j|S_i=s,A_i=a)$. $\\mathbf{Z}$ is a $\\vert\\mathcal{S}\\vert\\times d$ matrix, whose rows are $\\mathbf{z}(s)$’s, i.e. $\\mathbf{Z}^\\text{T}\\doteq\\left[\\mathbf{z}(s_1),\\dots,\\mathbf{z}(s_{\\vert\\mathcal{S}\\vert})\\right]$, with $\\mathbf{z}(s)\\in\\mathbb{R}^d$ is a vector defined by1: \\begin{align} \\mathbf{z}(s)\u0026\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_t\\lambda_t\\mathbf{z}_{t-1}+M_t\\mathbf{x}_t\\big|S_t=s\\Big] \\\\ \u0026=\\underbrace{\\mu_(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big|S_t=s\\Big]}_{m(s)}\\mathbf{x}_t+\\gamma(s)\\lambda(s)\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_t=s\\Big] \\\\ \u0026=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\mu(s)\\lim_{t\\to\\infty}\\sum_{\\bar{s},\\bar{a}}p(S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}|S_t=s)\\nonumber \\\\ \u0026\\hspace{2cm}\\times\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}\\Big] \\\\ \u0026=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\mu(s)\\sum_{\\bar{s},\\bar{a}}\\frac{\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})}{\\mu(s)}\\nonumber \\\\ \u0026\\hspace{2cm}\\times\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\mathbf{z}_{t-1}\\big|S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}\\Big] \\\\ \u0026=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s},\\bar{a}}\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}\\nonumber \\\\ \u0026\\hspace{2cm}\\times\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\gamma_{t-1}\\lambda_{t-1}\\mathbf{z}_{t-2}+M_{t-1}\\mathbf{x}_{t-1}\\big|S_t=s\\Big] \\\\ \u0026=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s}}\\Big(\\sum_{\\bar{a}}\\pi(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\Big)\\mathbf{z}(\\bar{s}) \\\\ \u0026=m(s)\\mathbf{x}(s)+\\gamma(s)\\lambda(s)\\sum_{\\bar{s}}\\left[\\mathbf{P}_\\pi\\right]_{\\bar{s}s}\\mathbf{z}(\\bar{s})\\label{eq:es.3} \\end{align} We now introduce three $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrices:\n$\\mathbf{M}$, which has the $m(s)\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big\\vert S_t=s\\Big]$ on its diagonal; $\\mathbf{\\Gamma}$, which has the $\\gamma(s)$ on its diagonal; $\\mathbf{\\Lambda}$, which has the $\\lambda(s)$ on its diagonal. With these matrices, we can rewrite \\eqref{eq:es.3} in matrix form, as: \\begin{align} \\mathbf{Z}^\\text{T}\u0026=\\mathbf{X}^\\text{T}\\mathbf{M}+\\mathbf{Z}^\\text{T}\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda} \\\\ \\Rightarrow\\mathbf{Z}^\\text{T}\u0026=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1} \\end{align} Substitute this equation back to \\eqref{eq:es.2}, we obtain: \\begin{equation} \\mathbf{A}=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{X}\\label{eq:es.4} \\end{equation} Doing similar steps, we can also obtain the ETD($\\lambda$)’s $\\mathbf{b}$ vector: \\begin{equation} \\mathbf{b}=\\mathbf{Z}\\mathbf{r}_\\pi=\\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}\\mathbf{r}_\\pi, \\end{equation} where $\\mathbf{r}_\\pi\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ is the vector of expected immediate rewards from each state under $\\pi$.\nSince the positive definiteness of $\\mathbf{A}$ implies the stability of the algorithm, from \\eqref{eq:es.4}, it is sufficient to prove the positive definiteness of the key matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})$ because this matrix can be written in the form of: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{X}=\\sum_{i=1}^{\\vert\\mathcal{S}\\vert}\\mathbf{x}_i^\\text{T}\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})\\mathbf{x}_i \\end{equation} To prove this definiteness, we begin by writing the last part of the key matrix in form of the identity matrix minus a probability matrix.\nLet $\\mathbf{P}_\\pi^\\lambda$ be the matrix with this probability as its $\\{ij\\}$-component. This matrix can be written as: \\begin{align} \\mathbf{P}_\\pi^\\lambda\u0026=\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda})+\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda})+\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{\\Lambda}\\mathbf{P}_\\pi\\mathbf{\\Gamma})^2(\\mathbf{I}-\\mathbf{\\Gamma}) \\\\ \u0026=\\left(\\sum_{k=0}^{\\infty}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^k\\right)\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda}) \\\\ \u0026=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}\\mathbf{P}_\\pi\\mathbf{\\Gamma}(\\mathbf{I}-\\mathbf{\\Lambda}) \\\\ \u0026=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}) \\\\ \u0026=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{P}_\\pi\\mathbf{\\Gamma}-\\mathbf{I}+\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda}) \\\\ \u0026=\\mathbf{I}-(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}), \\end{align} or \\begin{equation} \\mathbf{I}-\\mathbf{P}_\\pi^\\lambda=(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}) \\end{equation} Then our key matrix now can be written as: \\begin{equation} \\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma}\\mathbf{\\Lambda})^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi\\mathbf{\\Gamma})=\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\end{equation} In order to prove the positive definiteness of $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$, analogous to the proof of the convergence to TD fixed point of semi-gradient TD, we use two lemmas:\nLemma 1: Any matrix $\\mathbf{A}$ is positive definite iff the symmetric matrix $\\mathbf{S}=\\mathbf{A}+\\mathbf{A}^\\text{T}$ is positive definite. Lemma 2: Any symmetric real matrix $\\mathbf{S}$ is positive definite if all of its diagonal entries are positive and greater than the sum of the corresponding off-diagonal entries. Since $\\mathbf{M}$ is a diagonal matrix whose diagonal is a distribution and $\\mathbf{P}_\\pi^\\lambda$ is a probability matrix, we have that the matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$ has a diagonal of non-negative entries, and non-positive off-diagonal entries, and its row sums also are non-negative. Hence, our problem remains to show that the column sums of the key matrix are positive.\nTo show this we need to analyze the matrix $\\mathbf{M}$, and to do that we first analyze the vector $\\mathbf{f}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$, which having $f(s)\\doteq\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\left[F_t|S_t=s\\right]$ as its components. We have: \\begin{align} \\hspace{-0.7cm}f(s)\u0026=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_t\\big|S_t=s\\Big] \\\\ \u0026=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[i(S_t)+\\rho_{t-1}\\gamma_t F_{t-1}\\big|S_t=s\\Big] \\\\ \u0026=\\mu(s)i(s)\\nonumber \\\\ \u0026+\\mu(s)\\gamma(s)\\lim_{t\\to\\infty}\\sum_{\\bar{s},\\bar{a}}P(S_{t-1}=\\bar{s},A_{t-1}=\\bar{a}|S_t=s)\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}]\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026=\\mu(s)i(s)+\\mu(s)\\gamma(s)\\sum_{\\bar{s},\\bar{a}}\\frac{\\mu(\\bar{s})b(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})}{\\mu(s)}\\frac{\\pi(\\bar{a}|\\bar{s})}{b(\\bar{a}|\\bar{s})}\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026=\\mu(s)i(s)+\\gamma(s)\\sum_{\\bar{s},\\bar{a}}\\pi(\\bar{a}|\\bar{s})p(s|\\bar{s},\\bar{a})\\mu(\\bar{s})\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[F_{t-1}\\big|S_{t-1}=\\bar{s}\\Big] \\\\ \u0026=\\mu(s)i(s)+\\gamma(s)\\sum_s\\left[\\mathbf{P}_\\pi\\right]_{\\bar{s}s}f(\\bar{s})\\label{eq:es.5} \\end{align} Let $\\mathbf{i}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ be the vector having components $[\\mathbf{i}]_s\\doteq\\mu(s)i(s)$. Equation \\eqref{eq:es.5} allows us to write $\\mathbf{f}$ in matrix-vector form, as: \\begin{align} \\mathbf{f}\u0026=\\mathbf{i}+\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\mathbf{f} \\\\ \u0026=\\mathbf{i}+\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\mathbf{i}+(\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})^2\\mathbf{i}+\\dots \\\\ \u0026=\\left(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\right)^{-1} \\end{align} Back to the definition of $m(s)$, we have: \\begin{align} m(s)\u0026=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[M_t\\big|S_t=s\\Big] \\\\ \u0026=\\mu(s)\\lim_{t\\to\\infty}\\mathbb{E}_b\\Big[\\lambda_t i(S_t)+(1-\\lambda_t)F_t\\big|S_t=s\\Big] \\\\ \u0026=\\mu(s)\\lambda(s)i(s)+(1-\\lambda(s))f(s) \\end{align} Continuing as usual, we rewrite this equation in matrix-vector form by letting $\\mathbf{m}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$ be a vector having $m(s)$ as its components: \\begin{align} \\mathbf{m}\u0026=\\mathbf{\\Lambda}\\mathbf{i}+(\\mathbf{I}-\\mathbf{\\Lambda})\\mathbf{f} \\\\ \u0026=\\mathbf{\\Lambda}\\mathbf{i}+(\\mathbf{I}-\\mathbf{\\Lambda})(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})^{-1}\\mathbf{i} \\\\ \u0026=\\Big[\\mathbf{\\Lambda}(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T})+(\\mathbf{I}-\\mathbf{\\Lambda})\\Big]\\left(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\right)\\mathbf{i} \\\\ \u0026=\\Big(\\mathbf{I}-\\mathbf{\\Lambda}\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\Big)\\Big(\\mathbf{I}-\\mathbf{\\Gamma}\\mathbf{P}_\\pi^\\text{T}\\Big)^{-1}\\mathbf{i} \\\\ \u0026=\\Big(\\mathbf{I}-{\\mathbf{P}_\\pi^\\lambda}^\\text{T}\\Big)^{-1}\\mathbf{i} \\end{align} Let $\\mathbf{1}$ denote the column vector with all components equal to $1$. And using the result above, we have the vector of column sums of the key matrix $\\mathbf{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)$ is: \\begin{align} \\mathbf{1}^\\text{T}{M}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)\u0026=\\mathbf{m}^\\text{T}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\\\ \u0026=\\mathbf{i}^\\text{T}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda)^{-1}(\\mathbf{I}-\\mathbf{P}_\\pi^\\lambda) \\\\ \u0026=\\mathbf{i}^\\text{T} \\end{align} Instead of having domain of $[0,\\infty)$, if we further assume that $i(s)\u003e0,\\hspace{0.1cm}\\forall s\\in\\mathcal{S}$, then it implies immediately that the column sums are all positive, the key matrix is positive definite, so is the matrix $\\mathbf{A}$, and the ETD($\\lambda$) and its expected update are stable.\nReferences [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Doina Precup \u0026 Richard S. Sutton \u0026 Satinder Singh. Eligibility Traces for Off-Policy Policy Evaluation. ICML ‘00 Proceedings of the Seventeenth International Conference on Machine Learning. 80, 2000.\n[3] Deepmind x UCL. Reinforcement Learning Lecture Series 2021. Deepmind, 2021.\n[4] Harm van Seijen \u0026 A. Rupam Mahmood \u0026 Patrick M. Pilarski \u0026 Marlos C. Machado \u0026 Richard S. Sutton. True Online Temporal-Difference Learning. Journal of Machine Learning Research. 17(145):1−40, 2016.\n[5] Hado Van Hasselt \u0026 A. Rupam Mahmood \u0026 Richard S. Sutton. Off-policy TD(λ) with a true online equivalence. Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014.\n[6] Hamid Reza Maei. Gradient Temporal-Difference Learning Algorithms. PhD Thesis, University of Alberta, 2011.\n[7] Hamid Reza Maei \u0026 Richard S. Sutton GQ($\\lambda$): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. AGI-09, 2009.\n[8] Richard S. Sutton \u0026 A. Rupam Mahmood \u0026 Martha White. An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning. arXiv:1503.04269, 2015.\n[9] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes $\\mathbf{z}_t$ is a vector random variable, one per time step, while $\\mathbf{z}(s)$ is a vector expectation, one per state. ↩︎\n","wordCount":"5206","inLanguage":"en","datePublished":"2022-03-13T14:11:00+07:00","dateModified":"2022-03-13T14:11:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Eligible Traces</h1><div class=post-meta><span title='2022-03-13 14:11:00 +0700 +07'>March 13, 2022</span>&nbsp;·&nbsp;25 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#lambda-return>The $\lambda$-return</a><ul><li><a href=#off-lambda-return>Offline $\lambda$-return</a></li></ul></li><li><a href=#td-lambda>TD($\lambda$)</a></li><li><a href=#truncated-td>Truncated TD Methods</a></li><li><a href=#truncated-td>Online $\lambda$-return</a></li><li><a href=#true-onl-td-lambda>True Online TD($\lambda$)</a><ul><li><a href=#equivalence-bw-forward-backward>Equivalence between forward and backward views</a></li><li><a href=#dutch-traces-mc>Dutch Traces In Monte Carlo</a></li></ul></li><li><a href=#sarsa-lambda>Sarsa($\lambda$)</a></li><li><a href=#lambda-gamma>Variable $\lambda$ and $\gamma$</a></li><li><a href=#off-policy-traces-control-variates>Off-policy Traces with Control Variates</a></li><li><a href=#tree-backup-lambda>Tree-Backup($\lambda$)</a></li><li><a href=#other-off-policy-methods-traces>Other Off-policy Methods with Traces</a><ul><li><a href=#gtd-lambda>GTD($\lambda$)</a></li><li><a href=#gq-lambda>GQ($\lambda$)</a><ul><li><a href=#greedy-gq-lambda>Greedy-GQ($\lambda$)</a></li></ul></li><li><a href=#htd-lambda>HTD($\lambda$)</a></li><li><a href=#em-td-lambda>Emphatic TD($\lambda$)</a><ul><li><a href=#etd-stability>Stability</a></li></ul></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Beside <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td>$n$-step TD</a> methods, there is another mechanism called <strong>eligible traces</strong> that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.</p></blockquote><h2 id=lambda-return>The $\lambda$-return<a hidden class=anchor aria-hidden=true href=#lambda-return>#</a></h2><p>Recall that in <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td-prediction>TD-Learning</a> note, we have defined the $n$-step return as
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t\lt T-n$. After the note of <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/>Function Approximation</a>, for any parameterized function approximator, we can generalize that equation into:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+
\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\leq T-n
\end{equation}
where $\hat{v}(s,\mathbf{w})$ is the approximate value of state $s$ given weight vector $\mathbf{w}$.</p><p>We already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#stochastic-grad>SGD update</a>, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose
\begin{equation}
\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}
\end{equation}
as the target for our update.</p><p>The <strong>TD($\lambda$)</strong> is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\lambda^{n-1}$, for $\lambda\in\left[0,1\right]$, and is normalized by a factor of $1-\lambda$ to guarantee that the weights sum to $1$, as:
\begin{equation}
G_t^\lambda\doteq(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}
\end{equation}
The $G_t^\lambda$ is called <strong>$\lambda$-return</strong> of the update.</p><p>This figure below illustrates the backup diagram of TD($\lambda$) algorithm.</p><figure><img src=/images/eligible-traces/td-lambda-backup.png alt="Backup diagram of TD(lambda)" style=display:block;margin-left:auto;margin-right:auto;width:70%;height:70%><figcaption><b>Figure 1</b>: (taken from the <a href=#rl-book>RL book</a>) <b>The backup diagram of TD($\lambda$)</b></figcaption></figure><h3 id=off-lambda-return>Offline $\lambda$-return<a hidden class=anchor aria-hidden=true href=#off-lambda-return>#</a></h3><p>With the definition of $\lambda$-return, we can define the <strong>offline $\lambda$-return</strong> algorithm, which use semi-gradient update and using $\lambda$-return as the target:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{v}(S_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}</p><p>A result when applying offline $\lambda$-return on the random walk problem is shown below.</p><figure><img src=/images/eligible-traces/offline-lambda-return.png alt="Offline lambda-return on random walk" style=display:block;margin-left:auto;margin-right:auto><figcaption><b>Figure 2</b>: <b>Using offline $\lambda$-return on 19-state random walk</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-12/random_walk.py target=_blank>here</a></figcaption></figure><h2 id=td-lambda>TD($\lambda$)<a hidden class=anchor aria-hidden=true href=#td-lambda>#</a></h2><p><strong>TD($\lambda$)</strong> improves over the offline $\lambda$-return algorithm since:</p><ul><li>It updates the weight vector $\mathbf{w}$ on every step of an episode rather than only at the end, which leads to a time improvement.</li><li>Its computations are equally distributed in time rather than all at the end of the episode.</li><li>It can be applied to continuing problems rather than just to episodic ones.</li></ul><p>With function approximation, the eligible trace is a vector $\mathbf{z}_t\in\mathbb{R}^d$ with the same number of components as the weight vector $\mathbf{w}_t$. Whereas $\mathbf{w}_t$ is long-term memory, $\mathbf{z}_t$ on the other hand is a short-term memory, typically lasting less time than the length of an episode.</p><p>In TD($\lambda$), starting at the initial value of zero at the beginning of the episode, on each time step, the eligible trace vector $\mathbf{z}_t$ is incremented by the value gradient, and then fades away by $\gamma\lambda$:
\begin{align}
\mathbf{z}_{-1}&\doteq\mathbf{0} \\ \mathbf{z}_t&\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T\label{eq:tl.1}
\end{align}
where $\gamma$ is the discount factor; $\lambda$ is also called <strong>trace-decay parameter</strong>. On the other hand, the weight vector $\mathbf{w}_t$ is updated on each step proportional to the scalar <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#td_error>TD errors</a> and the eligible trace vector $\mathbf{z}_t$:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,\label{eq:tl.2}
\end{equation}
where the TD error is defined as
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}
Pseudocode of <strong>semi-gradient TD($\lambda$)</strong> is given below.</p><figure><img src=/images/eligible-traces/semi-grad-td-lambda.png alt="Semi-gradient TD(lambda)" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><p>Linear TD($\lambda$) has been proved to converge in the on-policy case if the step size parameter, $\alpha$, is reduced over time according to the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#stochastic-approx-condition>usual conditions</a>. And also in the continuing discounted case, for any $\lambda$, $\overline{\text{VE}}$ is proven to be within a bounded expansion of the lowest possible error:
\begin{equation}
\overline{\text{VE}}(\mathbf{w}_\infty)\leq\dfrac{1-\gamma\lambda}{1-\gamma}\min_\mathbf{w}\overline{\text{VE}}(\mathbf{w})
\end{equation}</p><p>The figure below illustrates the result for using TD($\lambda$) on the usual random walk task.</p><figure><img src=/images/eligible-traces/td-lambda.png alt="TD(lambda) on random walk" style=display:block;margin-left:auto;margin-right:auto><figcaption><b>Figure 3</b>: <b>Using TD($\lambda$) on 19-state random walk</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-12/random_walk.py target=_blank>here</a></figcaption></figure><h2 id=truncated-td>Truncated TD Methods<a hidden class=anchor aria-hidden=true href=#truncated-td>#</a></h2><p>Since in the offline $\lambda$-return, the target $\lambda$-return is not known until the end of episode. And moreover, in the continuing case, since the $n$-step returns depend on arbitrary large $n$, it maybe never known.
However, the dependence becomes weaker for longer-delayed rewards, falling by $\gamma\lambda$ for each step of delay.</p><p>A natural approximation is to truncate the sequence after some number of steps. In general, we define the <strong>truncated $\lambda$-return</strong> for time $t$, given data only up to some later horizon, $h$, as:
\begin{equation}
G_{t:h}^\lambda\doteq(1-\lambda)\sum_{n=1}^{h-t-1}\lambda^{n-1}G_{t:t+n}+\lambda^{h-t-1}G_{t:h},\hspace{1cm}0\leq t\lt h\leq T
\end{equation}
With this definition of the return, and based on the function approximation version of the $n$-step TD we have defined <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#semi-grad-n-step-td-update>before</a>, we have the <strong>TTD($\lambda$)</strong> is defined as:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}^\lambda-\hat{v}(S_t,\mathbf{w}_{t+n-1})\right]\nabla_\mathbf{w}\hat{w}(S_t,\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\lt T
\end{equation}
We have the $k$-step $\lambda$-return can be written as:
\begin{align}
\hspace{-0.8cm}G_{t:t+k}^\lambda&=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}G_{t:t+n}+\lambda^{k-1}G_{t:t+k} \\ &=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right]\nonumber \\ &\hspace{1cm}+\lambda^{k-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{k-1}R_{t+k}+\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})\right] \\ &=R_{t+1}+\gamma\lambda R_{t+2}+\dots+\gamma^{k-1}\lambda^{k-1}R_{t+k}\nonumber \\ &\hspace{1cm}+(1-\lambda)\left[\sum_{n=1}^{k-1}\lambda^{n-1}\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right]+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1}) \\ &=\hat{v}(S_t,\mathbf{w}_{t-1})+\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1})\right]\nonumber \\ &\hspace{1cm}+\left[\lambda\gamma R_{t+2}+\lambda\gamma^2\hat{v}(S_{t+2},\mathbf{w}_{t+1})-\lambda\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\right]+\dots\nonumber \\ &\hspace{1cm}+\left[\lambda^{k-1}\gamma^{k-1}R_{t+k}+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})-\lambda^{k-1}\gamma^{k-1}\hat{v}(S_{t+k-1},\mathbf{w}_{t+k-2})\right] \\ &=\hat{v}(S_t,\mathbf{w}_{t-1})+\sum_{i=t}^{t+k-1}(\gamma\lambda)^{i-t}\delta_i&rsquo;,\label{eq:tt.1}
\end{align}
with
\begin{equation}
\delta_t&rsquo;\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1}),
\end{equation}
where in the third step of the derivation, we use the identity
\begin{equation}
(1-\lambda)(1+\lambda+\dots+\lambda^{k-2})=1-\lambda^{k-1}
\end{equation}
From \eqref{eq:tt.1}, we can see that the $k$-step $\lambda$-return can be written as sums of TD errors if the value function is held constant, which allows us to implement the TTD($\lambda$) algorithm efficiently.</p><figure><img src=/images/eligible-traces/ttd-lambda-backup.png alt="Backup diagram of truncated TD(lambda)" style=display:block;margin-left:auto;margin-right:auto;width:500px;height:370px><figcaption><b>Figure 4</b>: (taken from the <a href=#rl-book>RL book</a>) <b>The backup diagram of truncated TD($\lambda$)</b></figcaption></figure><h2 id=truncated-td>Online $\lambda$-return<a hidden class=anchor aria-hidden=true href=#truncated-td>#</a></h2><p>The idea of <strong>online $\lambda$-return</strong> involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors.</p><p>Let $\mathbf{w}_t^h$ denote the weights used to generate the value at time $t$ in the sequence up to horizon $h$. The first weight vector $\mathbf{w}_0^h$ in each sequence is the one that inherited from the previous episode (thus they are the same for all $h$), and the last weight vector $\mathbf{w}_h^h$ in each sequence defines the weight-vector sequence of the algorithm. At the final horizon $h=T$, we obtain the final weight $\mathbf{w}_T^T$ which will be passed on to form the initial weights of the next episode.</p><p>In particular, we can define the first three sequences as:
\begin{align}
h=1:\hspace{1cm}&\mathbf{w}_1^1\doteq\mathbf{w}_0^1+\alpha\left[G_{0:1}^\lambda-\hat{v}(S_0,\mathbf{w}_0^1)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^1), \\\nonumber \\ h=2:\hspace{1cm}&\mathbf{w}_1^2\doteq\mathbf{w}_0^2+\alpha\left[G_{0:2}^\lambda-\hat{v}(S_0,\mathbf{w}_0^2)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^2), \\ &\mathbf{w}_2^2\doteq\mathbf{w}_1^2+\alpha\left[G_{1:2}^\lambda-\hat{v}(S_t,\mathbf{w}_1^2)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^2), \\\nonumber \\ h=3:\hspace{1cm}&\mathbf{w}_1^3\doteq\mathbf{w}_0^3+\alpha\left[G_{0:3}^\lambda-\hat{v}(S_0,\mathbf{w}_0^3)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^3), \\ &\mathbf{w}_2^3\doteq\mathbf{w}_1^3+\alpha\left[G_{1:3}^\lambda-\hat{v}(S_1,\mathbf{w}_1^3)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^3), \\ &\mathbf{w}_3^3\doteq\mathbf{w}_2^3+\alpha\left[G_{2:3}^\lambda-\hat{v}(S_2,\mathbf{w}_2^3)\right]\nabla_\mathbf{w}\hat{v}(S_2,\mathbf{w}_2^3)
\end{align}
The general form for the update of the <strong>online $\lambda$-return</strong> is
\begin{equation}
\mathbf{w}_{t+1}^h\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h),\hspace{1cm}0\leq t\lt h\leq T,\label{eq:olr.1}
\end{equation}
with $\mathbf{w}_t\doteq\mathbf{w}_t^t$, and $\mathbf{w}_0^h$ is the same for all $h$, we denote this vector as $\mathbf{w}_{init}$.</p><p>The online $\lambda$-return algorithm is fully online, determining a new weight vector $\mathbf{w}_t$ at each time step $t$ during an episode, using only information available at time $t$. Whereas the offline version passes through all the steps at the time of termination but does not make any updates during the episode.</p><h2 id=true-onl-td-lambda>True Online TD($\lambda$)<a hidden class=anchor aria-hidden=true href=#true-onl-td-lambda>#</a></h2><p>In the online $\lambda$-return, at each time step a sequence of updates is performed. The length of this sequence, and hence the computation per time step, increase over time.</p><p>However, it is possible to compute the weight vector resulting from time step $t+1$, $\mathbf{w}_{t+1}$, directly from the weight vector resulting from the sequence at time step $t$, $\mathbf{w}_t$.</p><p>Consider using linear approximation for our task, which gives us
\begin{align}
\hat{v}(S_t,\mathbf{w}_t)&=\mathbf{w}_t^\text{T}\mathbf{x}_t; \\ \nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)&=\mathbf{x}_t,
\end{align}
where $\mathbf{x}_t=\mathbf{x}(S_t)$ as usual.</p><p>We begin by rewriting \eqref{eq:olr.1}, as
\begin{align}
\mathbf{w}_{t+1}^h&\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h) \\ &=\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\left(\mathbf{w}_t^h\right)^\text{T}\mathbf{x}_t\right]\mathbf{x}_t \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t^h+\alpha\mathbf{x}_t G_{t:h}^\lambda,
\end{align}
where $\mathbf{I}$ is the identity matrix. With this equation, consider $\mathbf{w}_t^h$ in the cases of $t=1$ and $t=2$, we have:
\begin{align}
\mathbf{w}_1^h&=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_0^h+\alpha\mathbf{x}_0 G_{0:h}^\lambda \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_{init}+\alpha\mathbf{x}_0 G_{0:h}^\lambda, \\ \mathbf{w}_2^h&=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\mathbf{w}_1^h+\alpha\mathbf{x}_1 G_{1:h}^\lambda \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_{init}+\alpha\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\mathbf{x}_0 G_{0:h}^\lambda+\alpha\mathbf{x}_1 G_{1:h}^\lambda
\end{align}
In general, for $t\leq h$, we can write:
\begin{equation}
\mathbf{w}_t^h=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:h}^\lambda,
\end{equation}
where $\mathbf{A}_i^j$ is defined as:
\begin{equation}
\mathbf{A}_i^j\doteq\left(\mathbf{I}-\alpha\mathbf{x}_j\mathbf{x}_j^\text{T}\right)\left(\mathbf{I}-\alpha\mathbf{x}_{j-1}\mathbf{x}_{j-1}^\text{T}\right)\dots\left(\mathbf{I}-\alpha\mathbf{x}_i\mathbf{x}_i^\text{T}\right),\hspace{1cm}j\geq i,
\end{equation}
with $\mathbf{A}_{j+1}^j\doteq\mathbf{I}$. Hence, we can express $\mathbf{w}_t$ as:
\begin{equation}
\mathbf{w}_t=\mathbf{w}_t^t=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:t}^\lambda\label{eq:totl.1}
\end{equation}
Using \eqref{eq:tt.1}, we have:
\begin{align}
G_{i:t+1}^\lambda-G_{i:t}^\lambda&=\mathbf{w}_i^\text{T}\mathbf{x}_i+\sum_{j=1}^{t}(\gamma\lambda)^{j-i}\delta_j&rsquo;-\left(\mathbf{w}_i^\text{T}\mathbf{x}_i+\sum_{j=1}^{t-1}(\gamma\lambda)^{j-i}\delta_j&rsquo;\right) \\ &=(\gamma\lambda)^{t-i}\delta_t&rsquo;\label{eq:totl.2}
\end{align}
with the TD error, $\delta_t&rsquo;$ is defined as earlier:
\begin{equation}
\delta_t&rsquo;\doteq R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\label{eq:totl.3}
\end{equation}
Using \eqref{eq:totl.1}, \eqref{eq:totl.2} and \eqref{eq:totl.3}, we have:
\begin{align}
\mathbf{w}_{t+1}&=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda \\ &=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &=\mathbf{A}_0^t\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t}^\lambda+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\left(\mathbf{A}_0^{t-1}\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{t:t+1}^\lambda\right)\nonumber \\ &\hspace{1cm}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}\delta_t&rsquo;+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}\right) \\ &=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t&rsquo;+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t\mathbf{x}_t\right) \\ &=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t&rsquo;\nonumber \\ &\hspace{1cm}+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t+\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t-\mathbf{w}_t^\text{T}\mathbf{x}_t\right) \\ &=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t&rsquo;+\alpha\mathbf{x}_t\delta_t&rsquo;-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &=\mathbf{w}_t+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t&rsquo;-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t&rsquo;-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &=\mathbf{w}_t+\alpha\mathbf{z}_t\left(\delta_t+\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t+\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\left(\mathbf{z}_t-\mathbf{x}_t\right),\label{eq:totl.4}
\end{align}
where in the eleventh step, we define $\mathbf{z}_t$ as:
\begin{equation}
\mathbf{z}_t\doteq\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i},
\end{equation}
and in the twelfth step, we also define $\delta_t$ as:
\begin{align}
\delta_t&\doteq\delta_t&rsquo;-\mathbf{w}_t^\text{T}\mathbf{x}_t+\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t \\ &=R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t,
\end{align}
which is the same as the TD error of TD($\lambda$) we have defined earlier.</p><p>We then need to derive an update rule to recursively compute $\mathbf{z}_t$ from $\mathbf{z}_{t-1}$, as:
\begin{align}
\mathbf{z}_t&=\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i} \\ &=\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}+\mathbf{x}_t \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\gamma\lambda\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i(\gamma\lambda)^{t-i-1}+\mathbf{x}_t \\ &=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\gamma\lambda\mathbf{z}_{t-1}+\mathbf{x}_t \\ &=\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\text{T}\mathbf{x}_t\right)\right)\mathbf{x}_t\label{eq:totl.5}
\end{align}
Equations \eqref{eq:totl.4} and \eqref{eq:totl.5} form the update of the <strong>true online TD($\lambda$)</strong> algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t+\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\left(\mathbf{z}t_t-\mathbf{x}_t\right),
\end{equation}
where
\begin{align}
\mathbf{z}_t&\doteq\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\text{T}\mathbf{x}_t\right)\right)\mathbf{x}_t,\label{eq:totl.6} \\ \delta_t&\doteq R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t
\end{align}
Pseudocode of the algorithm is given below.</p><figure><img src=/images/eligible-traces/true-onl-td-lambda.png alt="True Online TD(lambda)" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><p>As other methods above, below is an illustration of using true online TD($\lambda$) on the random walk problem.</p><figure><img src=/images/eligible-traces/true-online-td-lambda.png alt="True online TD(lambda) on random walk" style=display:block;margin-left:auto;margin-right:auto><figcaption><b>Figure 5</b>: <b>Using True online TD($\lambda$) on 19-state random walk</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-12/random_walk.py target=_blank>here</a></figcaption></figure><p>The eligible trace \eqref{eq:totl.6} is called <strong>dutch trace</strong> to distinguish it from the trace \eqref{eq:tl.1} of TD($\lambda$), which is called <strong>accumulating trace</strong>.</p><p>There is another kind of trace called <strong>replacing trace</strong>, defined for the tabular case or for binary feature vectors
\begin{equation}
z_{i,t}\doteq\begin{cases}1 &\text{if }x_{i,t}=1 \\ \gamma\lambda z_{i,t-1} &\text{if }x_{i,t}=0\end{cases}
\end{equation}</p><h3 id=equivalence-bw-forward-backward>Equivalence between forward and backward views<a hidden class=anchor aria-hidden=true href=#equivalence-bw-forward-backward>#</a></h3><p>In this section, we will show that there is an interchange between forward and backward view.</p><p><strong>Theorem 1</strong><br><em>Consider any forward view that updates towards some interim targets $Y_k^t$ with
\begin{equation}
\mathbf{w}_{k+1}^t=\mathbf{w}_k+\eta_k\left(Y_k^t-\mathbf{x}_k^\text{T}\mathbf{w}_k^t\right)\mathbf{x}_k+\mathbf{u}_k,\hspace{1cm}0\leq k\lt t,
\end{equation}
where $\mathbf{w}_0^t=\mathbf{w}_0$ for some initial $\mathbf{w}_0$; $\mathbf{u}_k\in\mathbb{R}^d$ is any vector that does not depend on $t$. Assume that the temporal differences $Y_k^{t+1}-Y_k^t$ for different $k$ are related through
\begin{equation}
Y_k^{t+1}-Y_k^t=c_k(Y_{k+1}^{t+1}-Y_{k+1}^t),\hspace{1cm}\forall k\lt t\label{eq:ebfb.1}
\end{equation}
where $c_k$ is a scalar that does not depend on $t$. Then the final weights $\mathbf{w}_t^t$ at each time step $t$ are equal to the weight $\mathbf{w}_t$ as defined by $\mathbf{z}_0=\eta_0\mathbf{x}_0$ and the backward view
\begin{align}
\mathbf{w}_{t+1}&=\mathbf{w}_t+(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t, \\ \mathbf{z}_t&=c_{t-1}\mathbf{z}_{t-1}+\eta_t\left(1-c_{t-1}\mathbf{x}_t^\text{T}\mathbf{z}_{t-1}\right)\mathbf{x}_t,\hspace{1cm}t\gt 0
\end{align}</em></p><p><strong>Proof</strong><br>Let $\mathbf{F}_t\doteq\mathbf{I}-\eta_t\mathbf{x}_t\mathbf{x}_t^\text{T}$ be the <strong>fading matrix</strong> such that $\mathbf{w}_{t+1}=\mathbf{F}_k\mathbf{w}_k^t+\eta_k Y_k^t\mathbf{x}_k$. For each step $t$, we have:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&=\mathbf{F}_t\mathbf{w}_t^{t+1}-\mathbf{w}_t^t+\eta_t Y_t^{t+1}\mathbf{x}_t+\mathbf{u}_t \\ &=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t+(\mathbf{F}_t-\mathbf{I})\mathbf{w}_t^t+\mathbf{u}_t \\ &=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t-\eta_t\mathbf{x}_t\mathbf{x}_t^\text{T}\mathbf{w}_t^t+\mathbf{u}_t \\ &=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t^t)\mathbf{x}_t+\mathbf{u}_t\label{eq:ebfb.2}
\end{align}
We also have that:
\begin{align}
\mathbf{w}_t^{t+1}-\mathbf{w}_t^t&=\mathbf{F}_{t-1}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &=\mathbf{F}_{t-1}\mathbf{F}_{t-2}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{n-2}(Y_{t-2}^{t+1}-Y_{t-2}^t)\mathbf{F}_{t-1}\mathbf{x}_{t-2}\nonumber \\ &\hspace{1cm}+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &\hspace{0.3cm}\vdots\nonumber \\ &=\mathbf{F}_{t-1}\dots\mathbf{F}_0(\mathbf{w}_0^{t+1}-\mathbf{w}_0^t)+\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}c_k(Y_{k+1}^{t+1}-Y_{k+1}^t)\mathbf{x}_k \\ &\hspace{0.3cm}\vdots\nonumber \\ &=c_{t-1}\underbrace{\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k}_{\doteq\mathbf{z}_{t-1}}(Y_t^{t+1}-Y_t^t) \\ &=c_{t-1}\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t),\label{eq:ebfb.3}
\end{align}
where in the fifth step, we use the assumption \eqref{eq:ebfb.1}; the vector $\mathbf{z}_t$ defined in the sixth step can be computed recursively in terms of $\mathbf{z}_{t-1}$:
\begin{align}
\mathbf{z}_t&=\sum_{k=0}^{t}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k \\ &=\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &=c_{t-1}\mathbf{F}_t\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &=c_{t-1}\mathbf{F}_1\mathbf{z}_{t-1}+\eta_t\mathbf{x}_t \\ &=c_{t-1}\mathbf{z}_{t-1}+\eta_t(1-c_{t-1}\mathbf{x}_t^\text{T}\mathbf{z}_{t-1})\mathbf{x}_t
\end{align}
Plug \eqref{eq:ebfb.3} back into \eqref{eq:ebfb.2} we obtain:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&=c_{t-1}\mathbf{F}_t\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &=(\mathbf{z}_t-\eta_t\mathbf{x}_t)(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &=(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t
\end{align}
Since $\mathbf{w}_{0,t}\doteq\mathbf{w}_0$, the desired result follows through induction.</p><h3 id=dutch-traces-mc>Dutch Traces In Monte Carlo<a hidden class=anchor aria-hidden=true href=#dutch-traces-mc>#</a></h3><h2 id=sarsa-lambda>Sarsa($\lambda$)<a hidden class=anchor aria-hidden=true href=#sarsa-lambda>#</a></h2><p>To apply the use off eligible traces on control problems, we begin by defining the $n$-step return, which is the same as what we have defined <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#n-step-return>before</a>:
\begin{equation}
\hspace{-0.5cm}G_{t:t+n}\doteq\ R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}t+n\lt T\label{eq:sl.1}
\end{equation}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$. With this definition of the return, the action-value form of offline $\lambda$-return can be defined as:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{q}(S_t,A_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}
where $G_t^\lambda\doteq G_{t:\infty}^\lambda$.</p><p>The TD method for action values, known as <strong>Sarsa($\lambda$)</strong>, approximates this forward view and has the same update rule as TD($\lambda$):
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,
\end{equation}
except that the TD error, $\delta_t$, is defined in terms of action-value function:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t),
\end{equation}
and so it is with eligible trace vector:
\begin{align}
\mathbf{z}_{-1}&\doteq\mathbf{0}, \\ \mathbf{z}&_t\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T
\end{align}</p><figure><img src=/images/eligible-traces/sarsa-lambda-backup.png alt="Backup diagram of Sarsa(lambda)" style=display:block;margin-left:auto;margin-right:auto;width:450px;height:390px><figcaption><b>Figure 6</b>: (taken from the <a href=#rl-book>RL book</a>) <b>The backup diagram of Sarsa($\lambda$)</b></figcaption></figure>Pseudocode of the Sarsa($\lambda$) is given below.<figure><img src=/images/eligible-traces/sarsa-lambda.png alt=Sarsa(lambda) style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><p>There is also an action-value version of the online $\lambda$-return algorithm, and its efficient implementation as true online TD($\lambda$), called <strong>True online Sarsa($\lambda$)</strong>, which can be achieved by using $n$-step return \eqref{eq:sl.1} instead (which also leads to the change of $\mathbf{x}_t=\mathbf{x}(S_t)$ to $\mathbf{x}_t=\mathbf{x}(S_t,A_t)$).</p><p>Pseudocode of the true online Sarsa($\lambda$) is given below.</p><figure><img src=/images/eligible-traces/true-online-sarsa-lambda.png alt="True online Sarsa(lambda)" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><h2 id=lambda-gamma>Variable $\lambda$ and $\gamma$<a hidden class=anchor aria-hidden=true href=#lambda-gamma>#</a></h2><p>We can generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action. In other words, each time step $t$, we will have a different $\lambda$ and $\gamma$, denoted as $\lambda_t$ and $\gamma_t$.</p><p>In particular, say $\lambda:\mathcal{S}\times\mathcal{A}\to[0,1]$ such that $\lambda_t\doteq\lambda(S_t,A_t)$ and similarly, $\gamma:\mathcal{S}\to[0,1]$ such that $\gamma_t\doteq\gamma(S_t)$.</p><p>With this definition of $\gamma$, the return can be rewritten generally as:
\begin{align}
G_t&\doteq R_{t+1}+\gamma_{t+1}G_{t+1} \\ &=R_{t+1}+\gamma_{t+1}R_{t+2}+\gamma_{t+1}\gamma_{t+2}R_{t+3}+\dots \\ &=\sum_{k=t}^{\infty}\left(\prod_{i=t+1}^{k}\gamma_i\right)R_{k+1},
\end{align}
where we require that $\prod_{k=t}^{\infty}\gamma_k=0$ with probability $1$ for all $t$ to assure the sums are finite.</p><p>The generalization of $\lambda$ also lets us rewrite the state-based $\lambda$-return as:
\begin{equation}
G_t^{\lambda s}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\Big),\label{eq:lg.1}
\end{equation}
where $G_t^{\lambda s}$ denotes that this $\lambda$
-return is bootstrapped from state values, and hence the $G_t^{\lambda a}$ denotes the $\lambda$-return that bootstraps from action values. The Sarsa form of action-based $\lambda$-return is defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),
\end{equation}
and the Expected Sarsa form of its can be defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\overline{V}_t(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),\label{eq:lg.2}
\end{equation}
where the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#expected-approximate-value>expected approximate value</a> is generalized to function approximation as:
\begin{equation}
\overline{V}_t\doteq\sum_a\pi(a|s)\hat{q}(s,a,\mathbf{w}_t)\label{eq:lg.3}
\end{equation}</p><h2 id=off-policy-traces-control-variates>Off-policy Traces with Control Variates<a hidden class=anchor aria-hidden=true href=#off-policy-traces-control-variates>#</a></h2><p>We can also apply the use of importance sampling with eligible traces.</p><p>We begin with the new definition of $\lambda$-return, which is achieved by generalizing the $\lambda$-return \eqref{eq:lg.1} with the idea of <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-return-control-variate-state-value>control variates on $n$-step off-policy return</a>:
\begin{equation}
\hspace{-0.5cm}G_t^{\lambda s}\doteq\rho_t\Big(R_{t+1}+\gamma_{t+1}\big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\big)\Big)+(1-\rho_t)\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
Much like the other returns, the truncated version of this return can be approximated simply in terms of sums of state-based TD errors:
\begin{equation}
G_t^{\lambda s}\approx\hat{v}(S_t,\mathbf{w}_t)+\rho_t\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
where the state-based TD error, $\delta_t^s$, is defined as:
\begin{equation}
\delta_t^s\doteq R_{t+1}+\gamma_{t+1}\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),\label{eq:optcv.1}
\end{equation}
with the approximation becoming exact if the approximate value function does not change. Given this approximation, we have that:
\begin{align}
\mathbf{w}_{t+1}&=\mathbf{w}_t+\alpha\left(G_t^{\lambda s}-\hat{v}(S_t,\mathbf{w}_t)\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t) \\ &\approx\mathbf{w}_t+\alpha\rho_t\left(\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)
\end{align}
This is one time step of a forward view. And in fact, the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time. Since the sum of the forward-view update over time is:
\begin{align}
\sum_{t=1}^{\infty}(\mathbf{w}_{t+1}-\mathbf{w}_t)&\approx\sum_{t=1}^{\infty}\sum_{k=t}^{\infty}\alpha\rho_t\delta_k^s\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &=\sum_{k=1}^{\infty}\sum_{t=1}^{k}\alpha\rho_t\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &=\sum_{k=1}^{\infty}\alpha\delta_k^s\sum_{t=1}^{k}\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,\label{eq:optcv.2}
\end{align}
where in the second step, we use the summation rule: $\sum_{t=x}^{y}\sum_{k=t}^{y}=\sum_{k=x}^{y}\sum_{t=x}^{k}$.</p><p>Let $\mathbf{z}_k$ is defined as:
\begin{align}
\mathbf{z}_k &=\sum_{t=1}^{k}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t, \mathbf{w}_t\right)\prod_{i=t+1}^{k} \gamma_i\lambda_i\rho_i \\ &=\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &=\gamma_k\lambda_k\rho_k\underbrace{\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k-1}\gamma_i\lambda_i\rho_i}_{\mathbf{z}_{k-1}}+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &=\rho_k\big(\gamma_k\lambda_k\mathbf{z}_{k-1}+\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right)\big)
\end{align}
Then we can rewrite \eqref{eq:optcv.2} as:
\begin{equation}
\sum_{t=1}^{\infty}\left(\mathbf{w}_{t+1}-\mathbf{w}_t\right)\approx\sum_{k=1}^{\infty}\alpha\delta_k^s\mathbf{z}_k,
\end{equation}
which is sum of the backward-view update over time, with the eligible trace vector is defined as:
\begin{equation}
\mathbf{z}_t\doteq\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\big)\label{eq:optcv.3}
\end{equation}
Using this eligible trace with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$), we obtain a general TD($\lambda$) algorithm that can be applied to either on-policy or off-policy data.</p><ul id=number-list><li>In the on-policy case, the algorithm is exactly TD($\lambda$) because $\rho_t=1$ for all $t$ and \eqref{eq:optcv.3} becomes the accumulating trace \eqref{eq:tl.1} with extending to variable $\lambda$ and $\gamma$.</li><li>In the off-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable.</li></ul><p>For action-value function, we generalize the definition of the $\lambda$-return \eqref{eq:lg.2} of Expected Sarsa with the idea of <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-return-control-variate-action-value>control variate</a>:
\begin{align}
G_t^{\lambda a}&\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\big[\rho_{t+1}G_{t+1}^{\lambda a}+\bar{V}_t(S_{t+1})\nonumber \\ &\hspace{2cm}-\rho_{t+1}\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\big]\Big) \\ &=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\rho_{t+1}\left[G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right]\Big),
\end{align}
where the expected approximate value $\bar{V}_t(S_{t+1})$ is as given by \eqref{eq:lg.3}.</p><p>Similar to the others, this $\lambda$-return can also be written approximately as the sum of TD errors
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
with the action-based TD error is defined in terms of the expected approximate value:
\begin{equation}
\delta_t^a=R_{t+1}+\gamma_{t+1}\bar{V}_t(S_{t+1})-\hat{q}(S_t,A_t,\mathbf{w}_t)\label{eq:optcv.4}
\end{equation}
Analogy to the state value function case, this approximation also becomes exact if the approximate value function does not change.</p><p>Similar to the state case \eqref{eq:optcv.3}, we can also define the eligible trace for action values:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$) and the expectation-based TD error \eqref{eq:optcv.4}, we end up with an Expected Sarsa($\lambda$) algorithm that can applied to either on-policy or off-policy data.</p><ul><li>In the on-policy case with constant $\lambda$ and $\gamma$, this becomes the Sarsa($\lambda$) algorithm.</li></ul><h2 id=tree-backup-lambda>Tree-Backup($\lambda$)<a hidden class=anchor aria-hidden=true href=#tree-backup-lambda>#</a></h2><p>Recall that in the note of <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/>TD-Learning</a>, we have mentioned that there is an off-policy method without importance sampling called <strong>tree-backup</strong>. Can we extend the idea of tree-backup to an eligible trace version? Yes, we can.</p><p>As usual, we begin with establishing the $\lambda$-return by generalizing the $\lambda$-return of Expected Sarsa \eqref{eq:lg.2} with the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-tree-backup-return>$n$-step Tree-backup return</a>:
\begin{align}
G_t^{\lambda a}&\doteq R_{t+1}+\gamma_{t+1}\Bigg((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\Big[\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)\nonumber \\ &\hspace{2cm}+\pi(A_{t+1}|S_{t+1})G_{t+1}^{\lambda a}\Big]\Bigg) \\ &=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\pi(A_{t+1}|S_{t+1})\left(G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right)\Big)
\end{align}
This return, as usual, can also be written approximately (ignoring changes in the approximate value function) as sum of TD errors:
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\pi(A_i|S_i),
\end{equation}
with the TD error is defined as given by \eqref{eq:optcv.4}.</p><p>Similar to how we derive the eligible trace \eqref{eq:optcv.3}, we can define a new eligible trace in terms of target-policy probabilities of the selected actions:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\pi(A_t|S_t)\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace vector with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$), we end up with the <strong>Tree-Backup($\lambda$)</strong> or <strong>TB($\lambda$)</strong>.</p><figure><img src=/images/eligible-traces/tree-backup-lambda-backup.png alt="Backup diagram of Tree Backup(lambda)" style=display:block;margin-left:auto;margin-right:auto;width:450px;height:390px><figcaption><b>Figure 7</b>: (taken from the <a href=#rl-book>RL book</a>) <b>The backup diagram of Tree Backup($\lambda$)</b></figcaption></figure><h2 id=other-off-policy-methods-traces>Other Off-policy Methods with Traces<a hidden class=anchor aria-hidden=true href=#other-off-policy-methods-traces>#</a></h2><h3 id=gtd-lambda>GTD($\lambda$)<a hidden class=anchor aria-hidden=true href=#gtd-lambda>#</a></h3><p><strong>GTD($\lambda$)</strong> is the extended version of <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#tdc><strong>TDC</strong></a>, a state-value Gradient-TD method, with eligible traces.</p><p>In this algorithm, we will define a new off-policy, $\lambda$-return, not like usual but as a function:
\begin{equation}
G_t^{\lambda}(v)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})v(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda}(v)\Big]\label{eq:gl.1}
\end{equation}
where $v(s)$ denotes the value at state $s$, and $\lambda\in[0,1]$ is the trace-decay parameter.</p><p>Let $T_\pi^\lambda$ denote the $\lambda$-weighted Bellman operator for policy $\pi$ such that:
\begin{align}
v_\pi(s)&=\mathbb{E}\Big[G_t^\lambda(v_\pi)\big|S_t=s,\pi\Big] \\ &\doteq (T_\pi^\lambda v_\pi)(s)
\end{align}</p><p>Consider using linear function approximation, or in particular, we are trying to approximate $v(s)$ by $v_\mathbf{w}(s)=\mathbf{w}^\text{T}\mathbf{x}(s)$. Our objective is to find the fixed point which satisfies:
\begin{equation}
v_\mathbf{w}=\Pi T_\pi^\lambda v_\mathbf{w},\label{eq:gl.2}
\end{equation}
where $\Pi v$ is a projection of $v$ into the space of representable functions $\{v_\mathbf{w}|\mathbf{w}\in\mathbb{R}^d\}$.
Let $\mu$ be the steady-state distribution of states under the behavior policy $b$. Then, the projection can be defined as:
\begin{equation}
\Pi v\doteq v_{\mathbf{w}},
\end{equation}
where
\begin{equation}
\mathbf{w}=\underset{\mathbf{w}\in\mathbb{R}^d}{\text{argmin}}\left\Vert v-v_\mathbf{w}\right\Vert_\mu^2,
\end{equation}
In a linear case, in which $v_\mathbf{w}=\mathbf{X}\mathbf{w}$, the projection operator is linear and independent of $\mathbf{w}$:
\begin{equation}
\Pi=\mathbf{X}(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{D},
\end{equation}
where $\mathbf{D}$ denotes $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix whose diagonal elements are $\mu(s)$, and $\mathbf{X}$ denotes the $\vert\mathcal{S}\vert\times d$ matrix whose rows are the feature vectors $\mathbf{x}(s)^\text{T}$, one for each state $s$.</p><p>With linear function approximation, we can rewrite the $\lambda$-return \eqref{eq:gl.1} as:
\begin{equation}
G_t^{\lambda}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda}(\mathbf{w})\Big]\label{eq:gl.3}
\end{equation}
Let
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t,
\end{equation}
and
\begin{equation}
\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\doteq\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\mathbf{x}(s),
\end{equation}
where $\mathcal{P}_\mu^\pi$ is an operator.</p><p>The fixed point in \eqref{eq:gl.2} can be found by minimizing the <strong>Mean Square Projected Bellman Error</strong> (<strong>MSPBE</strong>):
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&=\Big\Vert v_\mathbf{w}-\Pi T_\pi^\lambda v_\mathbf{w}\Big\Vert_\mu^2 \\ &=\Big\Vert\Pi(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w})\Big\Vert_\mu^2 \\ &=\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big)^\text{T}\mathbf{D}\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big) \\ &=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\text{T}\Pi^\text{T}\mathbf{D}\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\text{T}\mathbf{D}^\text{T}\mathbf{X}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{D}\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &=\Big(\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-\mathbf{w}\right)\Big)^\text{T}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)\label{eq:gl.4}
\end{align}</p><p>From the definition of $T_\pi^\lambda$ and $\delta_t^\lambda$, we have:
\begin{align}
(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{v})(s)&=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t\big|S_t=s,\pi\Big] \\ &=\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\label{eq:gl.5}
\end{align}
Therefore,
\begin{align}
\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)&=\sum_s\mu(s)\Big[\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)(s)\Big]\mathbf{x}(s) \\ &=\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})|S_t=s,\pi\Big]\mathbf{x}(s) \\ &=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\label{eq:gl.6}
\end{align}
Moreover, we also have:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}=\sum_s\mu(s)\mathbf{x}(s)\mathbf{x}(s)^\text{T}=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]\label{eq:gl.7}
\end{equation}
Substitute \eqref{eq:gl.5}, \eqref{eq:gl.6} and \eqref{eq:gl.7} back to the \eqref{eq:gl.4}, we have:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)\label{eq:gl.8}
\end{equation}
In the objective function \eqref{eq:gl.8}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy problem, as usual, we use importance sampling.</p><p>We then instead use an importance-sampling version of $\lambda$-return \eqref{eq:gl.3}:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})=\rho_t\left(R_{t+1}+\gamma_{t+1}\left[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\right]\right),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
This also leads to an another version of $\delta_t^\lambda$, defined as:
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
With this definition of the $\lambda$-return, we have:
\begin{align}
&\hspace{-1cm}\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]\nonumber \\ &\hspace{-1cm}=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big)+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big)\big|S_t=s\Big]+\rho_t\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big|S_t=s,\pi\Big]\nonumber \\ &\hspace{1cm}+\sum_{a,s&rsquo;}p(s&rsquo;|s,a)b(a|s)\frac{\pi(a|s)}{b(a|s)}\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big|S_t=s,\pi\Big]\nonumber \\ &\hspace{1cm}+\sum_{a,s&rsquo;}p(s&rsquo;|s,a)\pi(a|s)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;\Big]\big|S_t=s,\pi\Big],
\end{align}
which, as it continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]=\mathbb{E}\Big[G_t^{\lambda}(\mathbf{w})\big|S_t=s,\pi\Big]
\end{equation}
And eventually, we get:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t
\end{equation}
because the state distribution is based on behavior state-distribution $\mu$.</p><p>With this result, our objective function \eqref{eq:gl.8} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\label{eq:gl.9}
\end{align}
From the definition of $\delta_t^{\lambda\rho}$, we have:
\begin{align}
\delta_t^{\lambda\rho}(\mathbf{w})&=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &=\rho_t\Big(R_{t+1}+\gamma_{t+1}\big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big]\Big)-\mathbf{w}^\text{T}\mathbf{x}_t \\ &=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t+\mathbf{w}^\text{T}\mathbf{x}_t\Big)\nonumber \\ &\hspace{2cm}-\rho_t\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\Big)+\rho_t\mathbf{w}^\text{T}\mathbf{x}_t-\mathbf{w}^\text{T}\mathbf{x}_t\nonumber \\ &\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big) \\ &=\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w}),
\end{align}
where the TD error, $\delta_t(\mathbf{w})$, is defined as usual:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
Also, we have that:
\begin{align}
\mathbb{E}\Big[(1-\rho_t)\mathbf{w}^\text{T}\mathbf{x}_t\mathbf{x}_t\Big]&=\sum_{s,a}\mu(s)b(a|s)\left(1-\frac{\pi(a|s)}{b(a|s)}\right)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &=\sum_s\mu(s)\left(\sum_a b(a|s)-\sum_a\pi(a|s)\right)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &=\sum_s\mu(s)(1-1)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &=0
\end{align}
Given these results, we have:
\begin{align}
\hspace{-1cm}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t\Big]+0+\mathbb{E}_{\pi b}\Big[\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t\nonumber \\ &\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-1}\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &\hspace{0.3cm}\vdots\nonumber \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\rho_t\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-2}+\dots\big)\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t=\rho_t(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1})
\end{equation}
Plugging this result back to \eqref{eq:gl.9} lets our objective function become:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\label{eq:gl.10}
\end{equation}
Similar to TDC, we also use gradient descent in order to find the minimum value of $\overline{\text{PBE}}(\mathbf{w})$. The gradient of our objective function w.r.t the weight vector $\mathbf{w}$ is:
\begin{align}
\hspace{-1.2cm}\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})&=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\big(\gamma_{t+1}\mathbf{x}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\rho_t\big(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1}\big)^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\big(\mathbf{x}_t\rho_t\mathbf{x}_t^\text{T}+\mathbf{x}_t\rho_t\gamma_t\lambda_t\mathbf{z}_{t-1}^\text{T}\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\mathbf{x}_{t+1}\gamma_{t+1}\lambda_{t+1}\mathbf{z}_t^\text{T}\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}-\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbf{v}(\mathbf{w}),\label{eq:gl.11}
\end{align}
where in the seventh step, we have used shifting indices trick and the identities:
\begin{align}
\mathbb{E}\Big[\mathbf{x}_t\rho_t\mathbf{x}_t^\text{T}\Big]&=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big], \\ \mathbb{E}\Big[\mathbf{x}_{t+1}\rho_t\gamma_t\lambda_t\mathbf{z}_t^\text{T}\Big]&=\mathbb{E}\Big[\mathbf{x}_{t+1}\gamma_t\lambda_t\mathbf{z}_t^\text{T}\Big]
\end{align}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from \eqref{eq:gl.11} and following TDC derivation steps we obtain the <strong>GTD($\lambda$)</strong> algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\text{T}\mathbf{v}_t)\mathbf{x}_{t+1},
\end{equation}
where</p><ul><li>the TD error $\delta_t^s$ is defined, as usual, as state-based TD error \eqref{eq:optcv.1};</li><li>the eligible trace vector $\mathbf{z}_t$ is defined as given in \eqref{eq:optcv.3} for state value;</li><li>and $\mathbf{v}_t$ is a vector of the same dimension as $\mathbf{w}$, initialized to $\mathbf{v}_0=\mathbf{0}$ with $\beta>0$ is a step-size parameter:
\begin{align}
\delta_t^s&\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&\doteq\rho_t(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t), \\ \mathbf{v}_{t+1}&\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta(\mathbf{v}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t
\end{align}</li></ul><h3 id=gq-lambda>GQ($\lambda$)<a hidden class=anchor aria-hidden=true href=#gq-lambda>#</a></h3><p><strong>GQ($\lambda$)</strong> is another eligible trace version of a Gradient-TD method but with action values. Its goal is to learn a parameter $\mathbf{w}_t$ such that $\hat{q}(s,a,\mathbf{w}_t)\doteq\mathbf{w}_t^\text{T}\mathbf{x}(s,a)\approx q_\pi(s,a)$ from data given by following a behavior policy $b$.</p><p>Similar to the state-values case of GTD($\lambda$), we begin with the definition of $\lambda$-return (function):
\begin{equation}
G_t^\lambda(q)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})q(S_{t+1},A_{t+1})+\lambda_{t+1}G_{t+1}^\lambda(q)\Big],\label{eq:gql.1}
\end{equation}
where $q(s,a)$ denotes the value of taking action $a$ at state $s$ and $\lambda\in[0,1]$ is the trace decay parameter.</p><p>Let $T_\pi^\lambda$ denote the $\lambda$-weighted state-action version of the affine $\vert\mathcal{S}\times\mathcal{A}\vert\times\vert\mathcal{S}\times\mathcal{A}\vert$ Bellman operator for the target policy $\pi$ such that:
\begin{align}
q_\pi(s,a)&=\mathbb{E}\Big[G_t^\lambda(q_\pi)\big|S_t=s,A_t=a,\pi\Big] \\ &\doteq(T_\pi^\lambda q_\pi)(s,a)
\end{align}
Analogous to the state value functions, with linear function approximation (i.e., we are trying to estimate $q(s,a)$ by $q_\mathbf{w}(s,a)=\mathbf{w}^\text{T}\mathbf{x}(s,a)$), our objective is to find the fixed point $q_\mathbf{w}$ such that:
\begin{equation}
q_\mathbf{w}=\Pi T_\pi^\lambda q_\mathbf{w},
\end{equation}
where $\Pi$ is the projection operator defined as above. This point also can be found by minimizing the MSPBE objective function:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&=\left\Vert q_\mathbf{w}-\Pi T_\pi^\lambda q_\mathbf{w}\right\Vert_\mu^2 \\ &=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big),\label{eq:gql.2}
\end{align}
where the second step is acquired from the result \eqref{eq:gl.8}, and where the TD error $\delta_t^\lambda$ is defined as the above section:
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
where $G_t^\lambda$ as given in \eqref{eq:gl.3}.</p><p>In the objective function \eqref{eq:gql.2}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy issue, as usual, we use importance sampling.</p><p>We start with the definition of the $\lambda$-return \eqref{eq:gql.1}, which is a noisy estimate of the future return by following policy $\pi$. In order to have a noisy estimate for the return of target policy $\pi$ while following behavior policy $b$, we define another $\lambda$-return (function), based on importance sampling:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big],\label{eq:gql.3}
\end{equation}
where $\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$:
\begin{equation}
\bar{\mathbf{x}}_t\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a),
\end{equation}
where $\rho_t$ is the single-step importance sampling ratio, and $G_t^{\lambda\rho}(\mathbf{w})$ is a noisy guess of future rewards of target policy $\pi$, if the agent follows policy $\pi$ from time $t$.<br>Let
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t\label{eq:gql.4}
\end{equation}
With the definition of the $\lambda$-return \eqref{eq:gql.3}, we have that:
\begin{align}
&\hspace{-0.9cm}\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big]\nonumber \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\big|S_t=s,A_t=a\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &+\sum_{s&rsquo;}p(s&rsquo;|s,a)\sum_{a&rsquo;}b(a&rsquo;|s&rsquo;)\frac{\pi(a&rsquo;|s&rsquo;)}{b(a&rsquo;|s&rsquo;)}\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;,A_{t+1}=a&rsquo;\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &+\sum_{s&rsquo;,a&rsquo;}p(s&rsquo;|s,a)\pi(a&rsquo;|s&rsquo;)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;,A_{t+1}=a&rsquo;\Big] \\ &\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\nonumber \\ &+\gamma_{t+1}\lambda_{t_1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s&rsquo;,A_{t+1}=a&rsquo;\Big]\big|S_t=s,A_t=a,\pi\Big],
\end{align}
which, as continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big]=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})\big|S_t=s,A_t=a,\pi\Big]
\end{equation}
And eventually, it yields:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t,
\end{equation}
because the state-action distribution is based on the behavior state-action pair distribution, $\mu$.</p><p>Hence, the objective function \eqref{eq:gql.2} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\label{eq:gql.5}
\end{align}
From the definition of the importance-sampling based TD error\eqref{eq:gql.4}, we have:
\begin{align}
&\hspace{-0.8cm}\delta_t^{\lambda\rho}(\mathbf{w})\nonumber \\ &\hspace{-1cm}=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &\hspace{-1cm}=R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big]-\mathbf{w}^\text{T}\mathbf{x}_t \\ &\hspace{-1cm}=\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\Big]+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &\hspace{-1cm}=\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\Big)-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w}) \\ &\hspace{-1cm}=\delta_t(\mathbf{w})-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &\hspace{1cm}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big) \\ &\hspace{-1cm}=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big)+\gamma_{t+1}\lambda_{t+1}\Big(\rho_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\Big) \\ &\hspace{-1cm}=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big),
\end{align}
where in the fifth step, we define:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\label{eq:gql.6}
\end{equation}
Note that the last part of the above equation has expected value of vector zero under the behavior policy $b$ because:
\begin{align}
\mathbb{E}\Big[\rho_t\mathbf{x}_t\big|S_t\Big]&=\sum_a b(a|S_t)\frac{\pi(a|S_t)}{b(a|S_t)}\mathbf{x}(S_t,a) \\ &=\sum_a\pi(a|S_t)\mathbf{x}(S_t,a) \\ &=\bar{\mathbf{x}}_t
\end{align}
With the result obtained above, we have:
\begin{align}
\hspace{-1cm}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_t\Big] \\ &=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\mathbf{x}_t\Big]\nonumber \\ &\hspace{2cm}+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\mathbf{x}_t\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]+0 \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}_b\Big[\gamma_t\lambda_t\rho_t\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_{t-1}\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t(\mathbf{w})\mathbf{x}_{t-1}\Big]\nonumber \\ &\hspace{2cm}+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big]+0 \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}\big)\Big]+\mathbb{E}\Big[\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &\hspace{0.3cm}\vdots\nonumber \\ &=\mathbb{E}_b\Big[\delta_t(\mathbf{w})\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}+\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}+\dots\Big)\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t\doteq\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\label{eq:gql.7}
\end{equation}
Plugging this result back to our objective function \eqref{eq:gql.5} gives us:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
Following the derivation of GTD($\lambda$), we have:
\begin{align}
&-\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})\nonumber \\ &=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\big(\gamma_{t+1}\bar{\mathbf{x}}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\Big)^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_t\lambda_t\rho_t\mathbf{x}_t\mathbf{z}_{t-1}^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_{t+1}\lambda_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbf{v}(\mathbf{w}),
\end{align}
where in the eighth step, we have used the identity:
\begin{equation}
\mathbb{E}\Big[\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]=\mathbb{E}\Big[\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big],
\end{equation}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from the above gradient-descent direction and weight-duplication trick, we obtain the <strong>GQ($\lambda$)</strong> algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\text{T}\mathbf{v}_t)\bar{\mathbf{x}}_{t+1},
\end{equation}
where</p><ul><li>$\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$;</li><li>$\delta_t^a$ is the expectation form of the TD error, defined as \eqref{eq:gql.6};</li><li>the eligible trace vector $\mathbf{z}_t$ is defined as \eqref{eq:gql.7} for action value;</li><li>and $\mathbf{v}_t$ is defined as in GTD($\lambda$):
\begin{align}
\bar{\mathbf{x}}_t&\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a), \\ \delta_t^a&\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\mathbf{x}_t, \\ \mathbf{v}_{t+1}&\doteq\mathbf{v}_t+\beta\delta_t^a\mathbf{z}_t-\beta(\mathbf{v}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t
\end{align}</li></ul><h4 id=greedy-gq-lambda>Greedy-GQ($\lambda$)<a hidden class=anchor aria-hidden=true href=#greedy-gq-lambda>#</a></h4><p>If the target policy is $\varepsilon$-greedy, or otherwise biased towards the greedy policy for $\hat{q}$, then GQ($\lambda$) can be used as a control algorithm, called <strong>Greedy-GQ($\lambda$)</strong>.</p><p>In the case of $\lambda=0$, called GQ(0), Greedy-GQ($\lambda$) is defined by:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{x}_t+\alpha\gamma_{t+1}(\mathbf{z}_t^\text{T}\mathbf{x}_t)\mathbf{x}(S_{t+1},a_{t+1}^{*}),
\end{equation}
where the eligible trace $\mathbf{z}_t$, TD error $\delta_t^a$ and $a_{t+1}^{*}$ are defined as:
\begin{align}
\mathbf{z}_t&\doteq\mathbf{z}_t+\beta\delta_t^a\mathbf{x}_t-\beta(\mathbf{z}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t, \\ \delta_t^a&\doteq R_{t+1}+\gamma_{t+1}\max_a\Big(\mathbf{w}_t^\text{T}\mathbf{x}(S_{t+1},a)\Big)-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ a_{t+1}^{*}&\doteq\underset{a}{\text{argmax}}\Big(\mathbf{w}_t^\text{T}\mathbf{x}(S_{t+1},a)\Big),
\end{align}
where $\beta>0$ is a step-size parameter.</p><h3 id=htd-lambda>HTD($\lambda$)<a hidden class=anchor aria-hidden=true href=#htd-lambda>#</a></h3><p><strong>HTD($\lambda$)</strong> is a hybrid state-value algorithm combining aspects of GTD($\lambda$) and TD($\lambda$), and has the following update:
\begin{align}
\mathbf{w}_{t+1}&\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t+\alpha\left(\left(\mathbf{z}_t-\mathbf{z}_t^b\right)^\text{T}\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{v}_{t+1}&\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta\left({\mathbf{z}_t^b}^\text{T}\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{z}_t&\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t\right), \\ \mathbf{z}_t^b&\doteq\gamma_t\lambda_t\mathbf{z}_{t-1}^b+\mathbf{x}_t,
\end{align}</p><h3 id=em-td-lambda>Emphatic TD($\lambda$)<a hidden class=anchor aria-hidden=true href=#em-td-lambda>#</a></h3><p><strong>Emphatic TD($\lambda$) (ETD($\lambda$))</strong> is the extension of the <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#em-td>one-step Emphatic-TD algorithm</a> to eligible traces. It is defined by:
\begin{align}
\mathbf{w}_{t+1}&\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t, \\ \delta_t&\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\right), \\ M_t&\doteq\gamma_t i(S_t)+(1-\lambda_t)F_t, \\ F_t&\doteq\rho_{t-1}\gamma_t F_{t-1}+i(S_t),
\end{align}
where</p><ul><li>$M_t\geq 0$ is the general form of <strong>emphasis</strong>;</li><li>$i:\mathcal{S}\to[0,\infty)$ is the <strong>interest function</strong></li><li>$F_t\geq 0$ is the <strong>followon trace</strong>, with $F_0\doteq i(S_0)$.</li></ul><h4 id=etd-stability>Stability<a hidden class=anchor aria-hidden=true href=#etd-stability>#</a></h4><p>Consider any stochastic algorithm of the form,
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t),
\end{equation}
where $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector that varies over time. Let
\begin{align}
\mathbf{A}&\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right], \\ \mathbf{b}&\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{b}_t\right]
\end{align}
We define the stochastic update to be <strong>stable</strong> if and only if the corresponding deterministic algorithm,
\begin{equation}
\bar{\mathbf{w}}_{t+1}\doteq\bar{\mathbf{w}}_t+\alpha\left(\mathbf{b}-\mathbf{A}\bar{\mathbf{w}}_t\right),
\end{equation}
is convergent to a unique fixed point independent of the initial $\bar{\mathbf{w}}_0$. This will occur iff $\mathbf{A}$ has a full set of eigenvalues having positive real parts, which can be proved if $\mathbf{A}$ is positive definite.</p><p>With this definition of stability, in order to exam the stability of ETD($\lambda$), we begin by considering the SGD update for the weight vector $\mathbf{w}$ at time step $t$.
\begin{align}
\mathbf{w}_{t+1}&\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t\right)\mathbf{z}_t \\ &=\mathbf{w}_t+\alpha\left(\mathbf{z}_t R_{t+1}-\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\text{T}\mathbf{w}_t\right)\label{eq:es.1}
\end{align}
Let $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector such that:
\begin{align}
\mathbf{A}_t&\doteq\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\text{T}, \\ \mathbf{b}_t&\doteq\mathbf{z}_t R_{t+1}
\end{align}
The stochastic update \eqref{eq:es.1} is then can be written as:
\begin{align}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t\right)
\end{align}
From the definition of $\mathbf{A}$, we have:
\begin{align}
\mathbf{A}&=\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right] \\ &=\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\Big] \\ &=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big)\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]\mathbb{E}_b\Big[\rho_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &=\sum_s\underbrace{\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]}_{\mathbf{z}(s)}\mathbb{E}_b\Big[\rho_k\big(\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big)^\text{T}\big|S_k=s\Big] \\ &=\sum_s\mathbf{z}(s)\mathbb{E}_\pi\Big[\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big|S_k=s\Big] \\ &=\sum_s\mathbf{z}(s)\Big(\mathbf{x}_t-\sum_{s&rsquo;}\left[\mathbf{P}_\pi\right]_{ss&rsquo;}\gamma(s&rsquo;)\mathbf{x}(s&rsquo;)\Big)^\text{T} \\ &=\mathbf{Z}\left(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\right)\mathbf{X},\label{eq:es.2}
\end{align}
where</p><ul><li>in the fifth step, given $S_t=s$, $\mathbf{z}_{t-1}$ and $M_t$ are independent of $\rho_t(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1})^\text{T}$;</li><li>$\mathbf{P}_\pi$ represents the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of transition probabilities:
\begin{equation}
\left[\mathbf{P}_\pi\right]_{ij}\doteq\sum_a\pi(a|i)p(j|i,a),
\end{equation}
where $p(j|i,a)\doteq P(S_{t+1}=j|S_i=s,A_i=a)$.</li><li>$\mathbf{Z}$ is a $\vert\mathcal{S}\vert\times d$ matrix, whose rows are $\mathbf{z}(s)$&rsquo;s, i.e. $\mathbf{Z}^\text{T}\doteq\left[\mathbf{z}(s_1),\dots,\mathbf{z}(s_{\vert\mathcal{S}\vert})\right]$, with $\mathbf{z}(s)\in\mathbb{R}^d$ is a vector defined by<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:
\begin{align}
\mathbf{z}(s)&\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big] \\ &=\underbrace{\mu_(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big]}_{m(s)}\mathbf{x}_t+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_t=s\Big] \\ &=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}p(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s)\nonumber \\ &\hspace{2cm}\times\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)}\nonumber \\ &\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s},\bar{a}}\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}\nonumber \\ &\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_{t-1}\lambda_{t-1}\mathbf{z}_{t-2}+M_{t-1}\mathbf{x}_{t-1}\big|S_t=s\Big] \\ &=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\Big(\sum_{\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\Big)\mathbf{z}(\bar{s}) \\ &=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\left[\mathbf{P}_\pi\right]_{\bar{s}s}\mathbf{z}(\bar{s})\label{eq:es.3}
\end{align}</li></ul><p>We now introduce three $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrices:</p><ul><li>$\mathbf{M}$, which has the $m(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big\vert S_t=s\Big]$ on its diagonal;</li><li>$\mathbf{\Gamma}$, which has the $\gamma(s)$ on its diagonal;</li><li>$\mathbf{\Lambda}$, which has the $\lambda(s)$ on its diagonal.</li></ul><p>With these matrices, we can rewrite \eqref{eq:es.3} in matrix form, as:
\begin{align}
\mathbf{Z}^\text{T}&=\mathbf{X}^\text{T}\mathbf{M}+\mathbf{Z}^\text{T}\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda} \\ \Rightarrow\mathbf{Z}^\text{T}&=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}
\end{align}
Substitute this equation back to \eqref{eq:es.2}, we obtain:
\begin{equation}
\mathbf{A}=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}\label{eq:es.4}
\end{equation}
Doing similar steps, we can also obtain the ETD($\lambda$)&rsquo;s $\mathbf{b}$ vector:
\begin{equation}
\mathbf{b}=\mathbf{Z}\mathbf{r}_\pi=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{r}_\pi,
\end{equation}
where $\mathbf{r}_\pi\in\mathbb{R}^{\vert\mathcal{S}\vert}$ is the vector of expected immediate rewards from each state under $\pi$.</p><p>Since the positive definiteness of $\mathbf{A}$ implies the stability of the algorithm, from \eqref{eq:es.4}, it is sufficient to prove the positive definiteness of the <strong>key matrix</strong> $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})$ because this matrix can be written in the form of:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}=\sum_{i=1}^{\vert\mathcal{S}\vert}\mathbf{x}_i^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{x}_i
\end{equation}
To prove this definiteness, we begin by writing the last part of the key matrix in form of the identity matrix minus a probability matrix.</p><p>Let $\mathbf{P}_\pi^\lambda$ be the matrix with this probability as its $\{ij\}$-component. This matrix can be written as:
\begin{align}
\mathbf{P}_\pi^\lambda&=\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma})^2(\mathbf{I}-\mathbf{\Gamma}) \\ &=\left(\sum_{k=0}^{\infty}(\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^k\right)\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{I}+\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &=\mathbf{I}-(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}),
\end{align}
or
\begin{equation}
\mathbf{I}-\mathbf{P}_\pi^\lambda=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})
\end{equation}
Then our key matrix now can be written as:
\begin{equation}
\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})=\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)
\end{equation}
In order to prove the positive definiteness of $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$, analogous to the <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#td-fixed-pt-proof>proof</a> of the convergence to TD fixed point of semi-gradient TD, we use two lemmas:</p><ul id=number-list style=font-style:italic><li><b>Lemma 1</b>: Any matrix $\mathbf{A}$ is positive definite iff the symmetric matrix $\mathbf{S}=\mathbf{A}+\mathbf{A}^\text{T}$ is positive definite.</li><li><b>Lemma 2</b>: Any symmetric real matrix $\mathbf{S}$ is positive definite if all of its diagonal entries are positive and greater than the sum of the corresponding off-diagonal entries.</li></ul><p>Since $\mathbf{M}$ is a diagonal matrix whose diagonal is a distribution and $\mathbf{P}_\pi^\lambda$ is a probability matrix, we have that the matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ has a diagonal of non-negative entries, and non-positive off-diagonal entries, and its row sums also are non-negative. Hence, our problem remains to show that the column sums of the key matrix are positive.</p><p>To show this we need to analyze the matrix $\mathbf{M}$, and to do that we first analyze the vector $\mathbf{f}\in\mathbb{R}^{\vert\mathcal{S}\vert}$, which having $f(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\left[F_t|S_t=s\right]$ as its components. We have:
\begin{align}
\hspace{-0.7cm}f(s)&=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[F_t\big|S_t=s\Big] \\ &=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[i(S_t)+\rho_{t-1}\gamma_t F_{t-1}\big|S_t=s\Big] \\ &=\mu(s)i(s)\nonumber \\ &+\mu(s)\gamma(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}P(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s)\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}]\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &=\mu(s)i(s)+\mu(s)\gamma(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)}\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &=\mu(s)i(s)+\gamma(s)\sum_{\bar{s},\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\mu(\bar{s})\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &=\mu(s)i(s)+\gamma(s)\sum_s\left[\mathbf{P}_\pi\right]_{\bar{s}s}f(\bar{s})\label{eq:es.5}
\end{align}
Let $\mathbf{i}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be the vector having components $[\mathbf{i}]_s\doteq\mu(s)i(s)$. Equation \eqref{eq:es.5} allows us to write $\mathbf{f}$ in matrix-vector form, as:
\begin{align}
\mathbf{f}&=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\mathbf{f} \\ &=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\mathbf{i}+(\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})^2\mathbf{i}+\dots \\ &=\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\right)^{-1}
\end{align}
Back to the definition of $m(s)$, we have:
\begin{align}
m(s)&=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big] \\ &=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\lambda_t i(S_t)+(1-\lambda_t)F_t\big|S_t=s\Big] \\ &=\mu(s)\lambda(s)i(s)+(1-\lambda(s))f(s)
\end{align}
Continuing as usual, we rewrite this equation in matrix-vector form by letting $\mathbf{m}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be a vector having $m(s)$ as its components:
\begin{align}
\mathbf{m}&=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})\mathbf{f} \\ &=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})^{-1}\mathbf{i} \\ &=\Big[\mathbf{\Lambda}(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})+(\mathbf{I}-\mathbf{\Lambda})\Big]\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\right)\mathbf{i} \\ &=\Big(\mathbf{I}-\mathbf{\Lambda}\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\Big)\Big(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\Big)^{-1}\mathbf{i} \\ &=\Big(\mathbf{I}-{\mathbf{P}_\pi^\lambda}^\text{T}\Big)^{-1}\mathbf{i}
\end{align}
Let $\mathbf{1}$ denote the column vector with all components equal to $1$. And using the result above, we have the vector of column sums of the key matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ is:
\begin{align}
\mathbf{1}^\text{T}{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)&=\mathbf{m}^\text{T}(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &=\mathbf{i}^\text{T}(\mathbf{I}-\mathbf{P}_\pi^\lambda)^{-1}(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &=\mathbf{i}^\text{T}
\end{align}
Instead of having domain of $[0,\infty)$, if we further assume that $i(s)>0,\hspace{0.1cm}\forall s\in\mathcal{S}$, then it implies immediately that the column sums are all positive, the key matrix is positive definite, so is the matrix $\mathbf{A}$, and the ETD($\lambda$) and its expected update are stable.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</span></p><p>[2] Doina Precup & Richard S. Sutton & Satinder Singh. <a href=https://scholarworks.umass.edu/cs_faculty_pubs/80>Eligibility Traces for Off-Policy Policy Evaluation</a>. ICML &lsquo;00 Proceedings of the Seventeenth International Conference on Machine Learning. 80, 2000.</p><p>[3] Deepmind x UCL. <a href=https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021>Reinforcement Learning Lecture Series 2021</a>. Deepmind, 2021.</p><p>[4] Harm van Seijen & A. Rupam Mahmood & Patrick M. Pilarski & Marlos C. Machado & Richard S. Sutton. <a href=http://jmlr.org/papers/v17/15-599.html>True Online Temporal-Difference Learning</a>. Journal of Machine Learning Research. 17(145):1−40, 2016.</p><p>[5] Hado Van Hasselt & A. Rupam Mahmood & Richard S. Sutton. <a href=https://www.researchgate.net/publication/263653431_Off-policy_TDl_with_a_true_online_equivalence>Off-policy TD(λ) with a true online equivalence</a>. Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014.</p><p>[6] Hamid Reza Maei. <a href=https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a/view/373459a7-72d1-4de2-bcd5-5f51e2f745e9/Hamid_Maei_PhDThesis.pdf>Gradient Temporal-Difference Learning Algorithms</a>. PhD Thesis, University of Alberta, 2011.</p><p>[7] Hamid Reza Maei & Richard S. Sutton <a href=https://agi-conf.org/2010/wp-content/uploads/2009/06/paper_21.pdf>GQ($\lambda$): A general gradient algorithm for temporal-difference prediction learning with eligibility traces</a>. AGI-09, 2009.</p><p>[8] Richard S. Sutton & A. Rupam Mahmood & Martha White. <a href=https://arxiv.org/abs/1503.04269>An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning</a>. arXiv:1503.04269, 2015.</p><p>[9] Shangtong Zhang. <a href=https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>Reinforcement Learning: An Introduction implementation</a>. Github.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>$\mathbf{z}_t$ is a vector random variable, one per time step, while $\mathbf{z}(s)$ is a vector expectation, one per state.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/td-learning/>td-learning</a></li><li><a href=https://trunghng.github.io/tags/eligible-traces/>eligible-traces</a></li><li><a href=https://trunghng.github.io/tags/function-approximation/>function-approximation</a></li><li><a href=https://trunghng.github.io/tags/importance-sampling/>importance-sampling</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/probability-statistics/exponential-family/><span class=title>« Prev</span><br><span>The exponential family</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/><span class=title>Next »</span><br><span>Function Approximation</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on twitter" href="https://twitter.com/intent/tweet/?text=Eligible%20Traces&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f&hashtags=reinforcement-learning%2ctd-learning%2celigible-traces%2cfunction-approximation%2cimportance-sampling%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f&title=Eligible%20Traces&summary=Eligible%20Traces&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f&title=Eligible%20Traces"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on whatsapp" href="https://api.whatsapp.com/send?text=Eligible%20Traces%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Eligible Traces on telegram" href="https://telegram.me/share/url?text=Eligible%20Traces&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2feligible-traces%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>