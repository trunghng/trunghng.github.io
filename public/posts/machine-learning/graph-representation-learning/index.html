<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Graph Representation Learning | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="machine-learning,neural-network,graph-neural-network,graph-theory"><meta name=description content="Traditional feature-based approaches
In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.
Let us consider a network $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges between these nodes. Also let $\mathbf{A}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ be the adjacency matrix of $\mathcal{G}$, i.e. for $u,v\in\mathcal{V}$
\begin{equation}
\mathbf{A}_{u,v}=\begin{cases}1&\text{if }(u,v)\in\mathcal{E} \\ 0&\text{if }(u,v)\notin\mathcal{E}\end{cases}
\end{equation}
In some graph with weighted edges, entries of $\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$."><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/machine-learning/graph-representation-learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://trunghng.github.io/posts/machine-learning/graph-representation-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")</script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")}</script><meta property="og:title" content="Graph Representation Learning"><meta property="og:description" content="Traditional feature-based approaches
In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.
Let us consider a network $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges between these nodes. Also let $\mathbf{A}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ be the adjacency matrix of $\mathcal{G}$, i.e. for $u,v\in\mathcal{V}$
\begin{equation}
\mathbf{A}_{u,v}=\begin{cases}1&\text{if }(u,v)\in\mathcal{E} \\ 0&\text{if }(u,v)\notin\mathcal{E}\end{cases}
\end{equation}
In some graph with weighted edges, entries of $\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$."><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/machine-learning/graph-representation-learning/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-16T14:43:59+07:00"><meta property="article:modified_time" content="2024-04-16T14:43:59+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary"><meta name=twitter:title content="Graph Representation Learning"><meta name=twitter:description content="Traditional feature-based approaches
In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.
Let us consider a network $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges between these nodes. Also let $\mathbf{A}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ be the adjacency matrix of $\mathcal{G}$, i.e. for $u,v\in\mathcal{V}$
\begin{equation}
\mathbf{A}_{u,v}=\begin{cases}1&\text{if }(u,v)\in\mathcal{E} \\ 0&\text{if }(u,v)\notin\mathcal{E}\end{cases}
\end{equation}
In some graph with weighted edges, entries of $\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Graph Representation Learning","item":"https://trunghng.github.io/posts/machine-learning/graph-representation-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Graph Representation Learning","name":"Graph Representation Learning","description":"Traditional feature-based approaches In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.\nLet us consider a network $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, where $\\mathcal{V}$ is the set of nodes and $\\mathcal{E}$ is the set of edges between these nodes. Also let $\\mathbf{A}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert\\times\\vert\\mathcal{V}\\vert}$ be the adjacency matrix of $\\mathcal{G}$, i.e. for $u,v\\in\\mathcal{V}$ \\begin{equation} \\mathbf{A}_{u,v}=\\begin{cases}1\u0026amp;\\text{if }(u,v)\\in\\mathcal{E} \\\\ 0\u0026amp;\\text{if }(u,v)\\notin\\mathcal{E}\\end{cases} \\end{equation} In some graph with weighted edges, entries of $\\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$.\n","keywords":["machine-learning","neural-network","graph-neural-network","graph-theory"],"articleBody":"Traditional feature-based approaches In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.\nLet us consider a network $\\mathcal{G}=(\\mathcal{V},\\mathcal{E})$, where $\\mathcal{V}$ is the set of nodes and $\\mathcal{E}$ is the set of edges between these nodes. Also let $\\mathbf{A}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert\\times\\vert\\mathcal{V}\\vert}$ be the adjacency matrix of $\\mathcal{G}$, i.e. for $u,v\\in\\mathcal{V}$ \\begin{equation} \\mathbf{A}_{u,v}=\\begin{cases}1\u0026\\text{if }(u,v)\\in\\mathcal{E} \\\\ 0\u0026\\text{if }(u,v)\\notin\\mathcal{E}\\end{cases} \\end{equation} In some graph with weighted edges, entries of $\\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$.\nNode-level features The goal of designing node features is to characterize the structures and positions of nodes in the network. For every node $u\\in\\mathcal{V}$, we have\nNode degree. Denoted as $d_u$, it counts the number of neighbors of $u$. \\begin{equation} d_u=\\sum_{v\\in\\mathcal{V}}\\mathbf{A}_{u,v} \\end{equation} Node centrality. Denoted as $c_u$, it takes the node importance into account. Eigenvector centrality. It captures how importance the neighbors of $u$ are, i.e. $u$ is important if it is surrounded by important neighboring nodes. \\begin{equation} c_u=\\frac{1}{\\lambda}\\sum_{v\\in\\mathcal{V}}\\mathbf{A}_{u,v}c_v \\end{equation} where $\\lambda$ is some constant. Rewriting the above equation in vector form with $\\mathbf{c}$ denotes the centrality vector we have \\begin{equation} \\lambda\\mathbf{c}=\\mathbf{A}\\mathbf{c}, \\end{equation} which is the standard eigenvector equation for the adjacency matrix $\\mathbf{A}$. By further assuming that we only use positive centrality, we can replace the constant $\\lambda$ by the largest eigenvector $\\lambda_\\text{max}$, which by Perron-Frobenius theorem is proved to be positive and unique. And the centrality vector $\\mathbf{c}$ is given by the eigenvector corresponding to $\\lambda_\\text{max}$. Betweenness centrality. It measures how often $u$ lies on the shortest path between other nodes. \\begin{equation} c_u=\\sum_{s\\neq t\\neq u}\\frac{\\text{#(shortest paths between }s\\text{ and }t\\text{ containing }u\\text{)}}{\\text{#(shortest paths between }s\\text{ and }t\\text{)}} \\end{equation} Closeness centrality. It measures the average shortest path length between $u$ and all other nodes. \\begin{equation} c_u=\\frac{\\sum_{v\\neq u}\\text{shortest path length between }u\\text{ and }v}{\\vert\\mathcal{V}\\vert-1} \\end{equation} Clustering coefficient. Graphlets. Graph Laplacians and Spectral Methods Spectral methods are used in clustering the nodes in a graph. We first begin with the definition of some important matrices.\nGraph Laplacians In addition to adjacency matrices, Laplacians are another matrix representations that can represent the matrix without loss of information.\nUnnormalized Laplacian The (unnormalized) Laplacian matrix is defined as \\begin{equation} \\mathbf{L}=\\mathbf{D}-\\mathbf{A}, \\end{equation} where $\\mathbf{D}$ is the degree matrix. These are some properties corresponding to the Laplacian matrix $\\mathbf{L}$ of a simple graph\nIt is symmetric and positive semi-definite. This holds for all $\\mathbf{x}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert}$. \\begin{equation} \\mathbf{x}^\\text{T}\\mathbf{L}\\mathbf{x}=\\frac{1}{2}\\sum_{u\\in\\mathcal{V}}\\sum_{v\\in\\mathcal{V}}\\mathbf{A}_{u,v}(\\mathbf{x}_u-\\mathbf{x}_v)^2=\\sum_{(u,v)\\in\\mathcal{E}}(\\mathbf{x}_u-\\mathbf{x}_v)^2 \\end{equation} Theorem 1. The geometric multiplicity of the $0$ eigenvalue of the Laplacian $\\mathbf{L}$ corresponds to the number of connected components in the graph.\nNormalized Laplacian The symmetric normalized Laplacian is given by \\begin{equation} \\mathbf{L}_\\text{sym}=\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{L}\\mathbf{D}^{-\\frac{1}{2}} \\end{equation} while the random walk Laplacian is defined as \\begin{equation} \\mathbf{L}_\\text{RW}=\\mathbf{D}^{-\\frac{1}{2}}\\mathbf{L} \\end{equation} Both of these matrices have similar properties as the (unnormalized) Laplacian, but their algebraic properties differ by small constants due to the normalization.\nGraph Cuts and Clustering We can see that Theorem 1 can be used to assign nodes to clusters based on which connected component they belong to. This method is trivial since it only lets us cluster nodes that are already in disconnected components. Fortunately, we will be showing that Laplacian can be applied to give an optimal clustering of nodes within a fully connected graph.\nIn order to define what an optimal cluster means, we begin with the notion of a cut on a graph.\nGraph cuts Let $\\mathcal{A}\\subset\\mathcal{V}$ be a subset of the nodes in the graph and $\\bar{\\mathcal{A}}$ denote its complement. Given a partitioning of the graph in $K$ non-overlapping subsets $\\mathcal{A}_1,\\ldots,\\mathcal{A}_K$ we define the cut value of this partition as the number of edges crossing the boundary between the partition of nodes \\begin{equation} \\text{cut}(\\mathcal{A}_1,\\ldots,\\mathcal{A}_K)=\\frac{1}{2}\\sum_{k=1}^{K}\\big\\vert(u,v)\\in\\mathcal{E}:u\\in\\mathcal{A}_k,v\\in\\bar{\\mathcal{A}}_k\\big\\vert \\end{equation} Here, it is possible to define an optimal clustering of the nodes into $K$ clusters as selecting a partition that minimizes this cut value. However, this approach also tends to give clusters containing only a single node.\nTo overcome this, beside minimizing the cut we also try to make the partitions are all reasonably large as well. We could do this by minimizing the Ratio Cut \\begin{equation} \\text{RatioCut}(\\mathcal{A}_1,\\ldots,\\mathcal{A}_K)=\\frac{1}{2}\\sum_{k=1}^{K}\\frac{\\big\\vert(u,v)\\in\\mathcal{E}:u\\in\\mathcal{A}_k,v\\in\\bar{\\mathcal{A}}_k\\big\\vert}{\\vert\\mathcal{A}_k\\vert}, \\end{equation} which penalizes the solution for choosing small cluster sizes. Or we can instead minimize the Normalized Cut (NCut) \\begin{equation} \\text{NCut}(\\mathcal{A}_1,\\ldots,\\mathcal{A}_K)=\\frac{1}{2}\\sum_{k=1}^{K}\\frac{\\big\\vert(u,v)\\in\\mathcal{E}:u\\in\\mathcal{A}_k,v\\in\\bar{\\mathcal{A}}_k\\big\\vert}{\\sum_{u\\in\\mathcal{A}_k}d_u}, \\end{equation} which on the other hand enforces that all clusters have the same number of edges incident to their nodes.\nApproximating the RatioCut with the Laplacian spectrum Generalized spectral clustering Find eigenvectors corresponding to $K$ smallest eigenvalues of $\\mathbf{L}$ with the smallest one excluded. Form the matrix $\\mathbf{U}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\times(K-1)\\vert}$ with the eigenvectors obtained from (1) as columns. Choose each row of $\\mathbf{U}$ as the embedding for a corresponding node from $\\mathcal{V}$ \\begin{equation} \\mathbf{z}_u=\\mathbf{U}_u\\hspace{1cm}\\forall u\\in\\mathcal{V} \\end{equation} Run $K$-means clustering on the feature vectors $\\mathbf{z}_u,\\forall u\\in\\mathcal{V}$. Link-level features The goal of designing edge features is to quantify the relationships between nodes. Let $\\mathbf{S}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert\\times\\vert\\mathcal{V}\\vert}$ denote the similarity matrix, where each entry $S_{u,v}$ denotes the value quantifying the relationship between nodes $u,v\\in\\mathcal{V}$.\nDistance-based feature. It measures the shortest path distance between two nodes Local neighborhood overlap. It counts the number of common neighbors shared by two nodes Common neighbors. Which is the naive function \\begin{equation} S_{u,v}=\\vert\\mathcal{N}(u)\\cap\\mathcal{N}(v)\\vert \\end{equation} Sorenson index. It defines a matrix $\\mathbf{S}_\\text{Sorenson}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert\\times\\vert\\mathcal{V}\\vert}$ of node-node neighborhood overlaps with entries given by \\begin{equation} S_{u,v}=\\frac{2\\vert\\mathcal{N}(u)\\cap\\mathcal{N}(v)\\vert}{d_u+d_v} \\end{equation} Salton index. \\begin{equation} S_{u,v}=\\frac{2\\vert\\mathcal{N}(u)\\cap\\mathcal{N}(v)\\vert}{\\sqrt{d_u d_v}} \\end{equation} Jaccard index. \\begin{equation} S_{u,v}=\\frac{\\vert\\mathcal{N}(u)\\cap\\mathcal{N}(v)\\vert}{\\vert\\mathcal{N}(u)\\cup\\mathcal{N}(v)\\vert} \\end{equation} Resource Allocation index. It counts the inverse degrees of the common neighbors. \\begin{equation} S_{v_1,v_2}=\\sum_{u\\in\\mathcal{N}(v_1)\\cap\\mathcal{N}(v_2)}\\frac{1}{d_u} \\end{equation} Adamic-Adar index. It counts the inverse logarithm of the degrees of the common neighbors. \\begin{equation} S_{v_1,v_2}=\\sum_{u\\in\\mathcal{N}(v_1)\\cap\\mathcal{N}(v_2)}\\frac{1}{\\log d_u} \\end{equation} Global neighborhood overlap. The local neighborhood overlap always returns zero when two nodes have no common neighbor even if they are potentially connected in the future. The global metric resolves this limitation by taking the entire graph into account. Katz index. It counts the number of walks of all lengths between two nodes \\begin{equation} S_{u,v}=\\sum_{l=1}^{\\infty}\\beta^l\\mathbf{A}_{u,v}^l, \\end{equation} where $0\u003c\\beta\u003c1$ is the discount factor and each $\\mathbf{A}_{u,v}^l$ is an entry of the power matrix $\\mathbf{A}^l$.\nTo compute the number of walks between two nodes, we consider the number of walks of each length. Let $\\mathbf{P}^{(k)}$ be a matrix where each entry $\\mathbf{P}_{u,v}^{(k)}$ denote the number of walks of length $k$ between a pair of nodes $(u,v)$. We will show that this matrix is also the power of $k$ of $\\mathbf{A}$. \\begin{equation} \\mathbf{P}^{(k)}=\\mathbf{A}^k \\end{equation} It is easily seen that \\begin{equation} \\mathbf{P}_{u,v}^{(1)}=\\mathbf{A}_{u,v} \\end{equation} Let us continue by considering $\\mathbf{P}_{u,v}^{(2)}$. This can be calculated by first computing the number of walks between each of $u$'s neighbors and $v$ then summing them up across $u$'s neighbors \\begin{equation} \\mathbf{P}_{u,v}^{(2)}=\\sum_i\\mathbf{A}_{u,i}\\mathbf{P}_{i,v}^{(1)}=\\sum_i\\mathbf{A}_{u,i}\\mathbf{A}_{i,v}=\\mathbf{A}_{u,v} \\end{equation} We can keep doing this to show that it is true that $\\mathbf{P}_{u,v}^{(k)}=\\mathbf{A}_{u,v}^k$. Or in other words, $\\mathbf{A}_{u,v}^k$ is also the number of walks of length $k$ between $u$ and $v$.\nTheorem 2. Let $\\mathbf{X}$ be a real-valued diagonalizable square matrix and let $\\lambda_\\text{max}$ denote the largest eigenvalue of $\\mathbf{X}$. Then \\begin{equation} (\\mathbf{I}-\\mathbf{X})^{-1}=\\sum_{i=0}^{\\infty}\\mathbf{X}^i \\end{equation} iff $\\vert\\lambda_\\text{max}\\vert\u003c1$ and $(\\mathbf{I}-\\mathbf{X})$ is non-singular.\nProof. Let $s_n=\\sum_{i=0}^{n}\\mathbf{X}^i$, we have \\begin{align} s_n-\\mathbf{X}s_n\u0026=\\sum_{i=0}^{n}\\mathbf{X}^i-\\mathbf{X}\\sum_{i=0}^{n}\\mathbf{X}^i \\\\ s_n(\\mathbf{I}-\\mathbf{X})\u0026=\\mathbf{I}-\\mathbf{X}^{n+1} \\\\ s_n\u0026=(\\mathbf{I}-\\mathbf{X}^{n+1})(\\mathbf{I}-\\mathbf{X})^{-1}\\label{eq:llf.1} \\end{align} Consider the eigendecomposition of $\\mathbf{X}$, we have \\begin{equation} \\mathbf{X}=\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1}, \\end{equation} which leads us to \\begin{equation} \\mathbf{X}^n=(\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^{-1})^n=\\mathbf{Q}\\mathbf{\\Lambda}^n\\mathbf{Q}^{-1}=\\mathbf{Q}\\left[\\begin{matrix}\\lambda_1^n\u0026\u0026 \\\\ \u0026\\ddots\u0026 \\\\ \u0026\u0026\\lambda_D^n\\end{matrix}\\right]\\mathbf{Q}^{-1} \\end{equation} Hence if $\\vert\\lambda_\\text{max}\\vert\u003c1$, as $n\\to\\infty$, we have that $\\mathbf{\\Lambda}^n\\to 0$, and thus $\\mathbf{X}^n\\to 0$. Applying this result into \\eqref{eq:llf.1} gives us \\begin{equation} \\lim_{n\\to\\infty}s_n=\\lim_{n\\to\\infty}(\\mathbf{I}-\\mathbf{X}^{n+1})(\\mathbf{I}-\\mathbf{X})^{-1}=\\mathbf{I}(\\mathbf{I}-\\mathbf{X})^{-1}=(\\mathbf{I}-\\mathbf{X})^{-1} \\end{equation} Based on Theorem 2, we have the closed-form of Katz index is given by \\begin{equation} \\mathbf{S}_\\text{Katz}=(\\mathbf{I}-\\beta\\mathbf{A})^{-1}-\\mathbf{I} \\end{equation} Leicht, Holme and Newman (LHN) similarity. Random walk methods. Graph-level features Node Embeddings The goal of node embedding learning methods is to encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood. Or in other words, nodes are encoded so that the similarity in the latent space approximates the similarity in the original graph.\nEncoder-Decoder framework In this framework, the graph representation learning problem is divided into steps\nAn encoder maps each node in the original graph to a low-dimensional vector in the embedding space. Define a similarity function in the original graph. A decoder takes embedding vectors and use them to reconstruct information about each node's neighborhood in the original graph. Optimize the encoder and the decoder so that the similarity in the embedding space approximates the similarity in the original graph. The Encoder The encoder, denoted $\\text{ENC}$, is a function that maps each node $v\\in\\mathcal{V}$ to an embedding vector $\\mathbf{z}_v\\in\\mathbb{R}^d$. \\begin{equation} \\text{ENC}(v)=\\mathbf{z}_v \\end{equation} In the simplest form, used in the shallow embedding approach, the encoder is simply an embedding-lookup \\begin{equation} \\text{ENC}(v)=\\mathbf{z}_v=\\mathbf{Z}\\mathbf{v}, \\end{equation} where $Z\\in\\mathbb{R}^{d\\times\\vert\\mathcal{V}\\vert}$ is a matrix whose each column is a node embedding and where each $\\mathbf{v}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert}$ is an indicator vector (i.e., all zeroes except a one in column indicating the ID of node $v$).\nThe Decoder The decoder, denoted $\\text{DEC}$, reconstruct certain graph statistics (e.g., set of neighbors $\\mathcal{N}(u)$) from the embedding $\\mathbf{z}_u$ generated by $\\text{ENC}$ of node $u$.\nA pairwise decoder, $\\text{DEC}:\\mathbb{R}^d\\times\\mathbb{R}^d\\mapsto\\mathbb{R}^+$ maps each pair of embeddings $(\\mathbf{z}_u,\\mathbf{z}_v)$ to a similarity score, which describes the relationship between nodes $u$ and $v$.\nGiven this similarity score, our goal is to optimize the encoder and decoder so that the decoded similarity approximates the similarity in the original graph. \\begin{equation} \\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)\\approx\\mathbf{S}(u,v),\\label{eq:td.1} \\end{equation} where $\\mathbf{S}(u,v)$ is a graph-based similarity metric between nodes $u$ and $v$.\nModel optimization The reconstruction objective \\eqref{eq:td.1} can be accomplished by minimizing an empirical reconstruction loss $\\mathcal{L}$ over a set of training node pairs $\\mathcal{D}$ \\begin{equation} \\mathcal{L}=\\sum_{(u,v)\\in\\mathcal{D}}\\ell(\\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v),\\mathbf{S}(u,v)), \\end{equation} where $\\ell:\\mathbb{R}\\times\\mathbb{R}\\mapsto\\mathbb{R}$ is a loss function measures the difference between the decoded similarity values $\\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)$ and the true similarity values $\\mathbf{S}(u,v)$.\nFactorization-based approaches Laplacian eigenmaps In this approach, the decoder is defined as the L2-distance between the embeddings \\begin{equation} \\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)=\\Vert\\mathbf{z}_u-\\mathbf{z}_v\\Vert_2^2 \\end{equation} And the loss function is then given as \\begin{equation} \\mathcal{L}=\\sum_{(u,v)\\in\\mathcal{D}}\\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)\\cdot\\mathbf{S}(u,v) \\end{equation}\nInner-product methods As suggested by their name, the decoder in these approaches is defined as the inner product \\begin{equation} \\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)=\\mathbf{z}_u^\\text{T}\\mathbf{z}_v \\end{equation} These methods have the loss function given as \\begin{equation} \\mathcal{L}=\\sum_{(u,v)\\in\\mathcal{D}}\\Vert\\text{DEC}(\\mathbf{z}_u,\\mathbf{z}_v)-\\mathbf{S}(u,v)\\Vert_2^2 \\end{equation} The above approaches are referred to as matrix-factorization methods, since their loss function can be minimized using factorization algorithm, such as SVD. Stacking the embeddings $\\mathbf{z}_u\\in\\mathbb{R}^d$ into a matrix $\\mathbf{Z}\\in\\mathbb{R}^{\\vert\\mathcal{V}\\vert\\times d}$ the reconstruction objective can be rewritten as \\begin{equation} \\mathcal{L}\\approx\\Vert\\mathbf{Z}\\mathbf{Z}^\\text{T}-\\mathbf{S}\\Vert_2^2, \\end{equation} where $\\mathbf{S}$ is a matrix containing pairwise similarity measures.\nRandom walk embeddings Graph Neural Networks Graph Convolution Networks References [1] William L. Hamilton. Graph Representation Learning. Morgan and Claypool, Synthesis Lectures on Artificial Intelligence and Machine Learning.\n[2] Jure Leskovec. CS224W - Machine Learning with Graphs.\nFootnotes ","wordCount":"1730","inLanguage":"en","datePublished":"2024-04-16T14:43:59+07:00","dateModified":"2024-04-16T14:43:59+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/machine-learning/graph-representation-learning/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io/ accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Graph Representation Learning</h1><div class=post-meta><span title='2024-04-16 14:43:59 +0700 +07'>April 16, 2024</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#traditional-feature-based-approaches>Traditional feature-based approaches</a><ul><li><a href=#node-level-features>Node-level features</a><ul><li><a href=#graph-laplacians-and-spectral-methods>Graph Laplacians and Spectral Methods</a><ul><li><a href=#graph-laplacians>Graph Laplacians</a><ul><li><a href=#unnormalized-laplacian>Unnormalized Laplacian</a></li><li><a href=#normalized-laplacian>Normalized Laplacian</a></li></ul></li><li><a href=#graph-cuts-and-clustering>Graph Cuts and Clustering</a><ul><li><a href=#graph-cuts>Graph cuts</a></li><li><a href=#approximating-the-ratiocut-with-the-laplacian-spectrum>Approximating the RatioCut with the Laplacian spectrum</a></li></ul></li><li><a href=#generalized-spectral-clustering>Generalized spectral clustering</a></li></ul></li></ul></li><li><a href=#link-level-features>Link-level features</a></li><li><a href=#graph-level-features>Graph-level features</a></li></ul></li><li><a href=#node-embeddings>Node Embeddings</a><ul><li><a href=#encoder-decoder-framework>Encoder-Decoder framework</a><ul><li><a href=#the-encoder>The Encoder</a></li><li><a href=#the-decoder>The Decoder</a></li><li><a href=#model-optimization>Model optimization</a></li></ul></li><li><a href=#factorization-based-approaches>Factorization-based approaches</a><ul><li><a href=#laplacian-eigenmaps>Laplacian eigenmaps</a></li><li><a href=#inner-product-methods>Inner-product methods</a></li></ul></li><li><a href=#random-walk-embeddings>Random walk embeddings</a></li></ul></li><li><a href=#graph-neural-networks>Graph Neural Networks</a><ul><li><a href=#graph-convolution-networks>Graph Convolution Networks</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><h2 id=traditional-feature-based-approaches>Traditional feature-based approaches<a hidden class=anchor aria-hidden=true href=#traditional-feature-based-approaches>#</a></h2><p>In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.</p><p>Let us consider a network $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges between these nodes. Also let $\mathbf{A}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ be the adjacency matrix of $\mathcal{G}$, i.e. for $u,v\in\mathcal{V}$
\begin{equation}
\mathbf{A}_{u,v}=\begin{cases}1&\text{if }(u,v)\in\mathcal{E} \\ 0&\text{if }(u,v)\notin\mathcal{E}\end{cases}
\end{equation}
In some graph with weighted edges, entries of $\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$.</p><h3 id=node-level-features>Node-level features<a hidden class=anchor aria-hidden=true href=#node-level-features>#</a></h3><p>The goal of designing node features is to characterize the structures and positions of nodes in the network. For every node $u\in\mathcal{V}$, we have</p><ul class=number-list><li><b>Node degree</b>. Denoted as $d_u$, it counts the number of neighbors of $u$.
\begin{equation}
d_u=\sum_{v\in\mathcal{V}}\mathbf{A}_{u,v}
\end{equation}</li><li><b>Node centrality</b>. Denoted as $c_u$, it takes the node importance into account.<ul><li><b>Eigenvector centrality</b>. It captures how importance the neighbors of $u$ are, i.e. $u$ is important if it is surrounded by important neighboring nodes.
\begin{equation}
c_u=\frac{1}{\lambda}\sum_{v\in\mathcal{V}}\mathbf{A}_{u,v}c_v
\end{equation}
where $\lambda$ is some constant. Rewriting the above equation in vector form with $\mathbf{c}$ denotes the centrality vector we have
\begin{equation}
\lambda\mathbf{c}=\mathbf{A}\mathbf{c},
\end{equation}
which is the standard eigenvector equation for the adjacency matrix $\mathbf{A}$. By further assuming that we only use positive centrality, we can replace the constant $\lambda$ by the largest eigenvector $\lambda_\text{max}$, which by <b>Perron-Frobenius theorem</b> is proved to be positive and unique. And the centrality vector $\mathbf{c}$ is given by the eigenvector corresponding to $\lambda_\text{max}$.</li><li><b>Betweenness centrality</b>. It measures how often $u$ lies on the shortest path between other nodes.
\begin{equation}
c_u=\sum_{s\neq t\neq u}\frac{\text{#(shortest paths between }s\text{ and }t\text{ containing }u\text{)}}{\text{#(shortest paths between }s\text{ and }t\text{)}}
\end{equation}</li><li><b>Closeness centrality</b>. It measures the average shortest path length between $u$ and all other nodes.
\begin{equation}
c_u=\frac{\sum_{v\neq u}\text{shortest path length between }u\text{ and }v}{\vert\mathcal{V}\vert-1}
\end{equation}</li></ul></li><li><b>Clustering coefficient</b>.</li><li><b>Graphlets</b>.</li></ul><h4 id=graph-laplacians-and-spectral-methods>Graph Laplacians and Spectral Methods<a hidden class=anchor aria-hidden=true href=#graph-laplacians-and-spectral-methods>#</a></h4><p>Spectral methods are used in clustering the nodes in a graph. We first begin with the definition of some important matrices.</p><h5 id=graph-laplacians>Graph Laplacians<a hidden class=anchor aria-hidden=true href=#graph-laplacians>#</a></h5><p>In addition to adjacency matrices, <strong>Laplacians</strong> are another matrix representations that can represent the matrix without loss of information.</p><h6 id=unnormalized-laplacian>Unnormalized Laplacian<a hidden class=anchor aria-hidden=true href=#unnormalized-laplacian>#</a></h6><p>The (unnormalized) Laplacian matrix is defined as
\begin{equation}
\mathbf{L}=\mathbf{D}-\mathbf{A},
\end{equation}
where $\mathbf{D}$ is the degree matrix. These are some properties corresponding to the Laplacian matrix $\mathbf{L}$ of a simple graph</p><ul class=roman-list><li>It is symmetric and positive semi-definite.</li><li>This holds for all $\mathbf{x}\in\mathbb{R}^{\vert\mathcal{V}\vert}$.
\begin{equation}
\mathbf{x}^\text{T}\mathbf{L}\mathbf{x}=\frac{1}{2}\sum_{u\in\mathcal{V}}\sum_{v\in\mathcal{V}}\mathbf{A}_{u,v}(\mathbf{x}_u-\mathbf{x}_v)^2=\sum_{(u,v)\in\mathcal{E}}(\mathbf{x}_u-\mathbf{x}_v)^2
\end{equation}</li></ul><p><strong>Theorem 1</strong>. <em>The geometric multiplicity of the $0$ eigenvalue of the Laplacian $\mathbf{L}$ corresponds to the number of connected components in the graph.</em></p><h6 id=normalized-laplacian>Normalized Laplacian<a hidden class=anchor aria-hidden=true href=#normalized-laplacian>#</a></h6><p>The symmetric normalized Laplacian is given by
\begin{equation}
\mathbf{L}_\text{sym}=\mathbf{D}^{-\frac{1}{2}}\mathbf{L}\mathbf{D}^{-\frac{1}{2}}
\end{equation}
while the random walk Laplacian is defined as
\begin{equation}
\mathbf{L}_\text{RW}=\mathbf{D}^{-\frac{1}{2}}\mathbf{L}
\end{equation}
Both of these matrices have similar properties as the (unnormalized) Laplacian, but their algebraic properties differ by small constants due to the normalization.</p><h5 id=graph-cuts-and-clustering>Graph Cuts and Clustering<a hidden class=anchor aria-hidden=true href=#graph-cuts-and-clustering>#</a></h5><p>We can see that Theorem 1 can be used to assign nodes to clusters based on which connected component they belong to. This method is trivial since it only lets us cluster nodes that are already in disconnected components. Fortunately, we will be showing that Laplacian can be applied to give an optimal clustering of nodes within a fully connected graph.</p><p>In order to define what an optimal cluster means, we begin with the notion of a <strong>cut</strong> on a graph.</p><h6 id=graph-cuts>Graph cuts<a hidden class=anchor aria-hidden=true href=#graph-cuts>#</a></h6><p>Let $\mathcal{A}\subset\mathcal{V}$ be a subset of the nodes in the graph and $\bar{\mathcal{A}}$ denote its complement. Given a partitioning of the graph in $K$ non-overlapping subsets $\mathcal{A}_1,\ldots,\mathcal{A}_K$ we define the cut value of this partition as the number of edges crossing the boundary between the partition of nodes
\begin{equation}
\text{cut}(\mathcal{A}_1,\ldots,\mathcal{A}_K)=\frac{1}{2}\sum_{k=1}^{K}\big\vert(u,v)\in\mathcal{E}:u\in\mathcal{A}_k,v\in\bar{\mathcal{A}}_k\big\vert
\end{equation}
Here, it is possible to define an optimal clustering of the nodes into $K$ clusters as selecting a partition that minimizes this cut value. However, this approach also tends to give clusters containing only a single node.</p><p>To overcome this, beside minimizing the cut we also try to make the partitions are all reasonably large as well. We could do this by minimizing the <strong>Ratio Cut</strong>
\begin{equation}
\text{RatioCut}(\mathcal{A}_1,\ldots,\mathcal{A}_K)=\frac{1}{2}\sum_{k=1}^{K}\frac{\big\vert(u,v)\in\mathcal{E}:u\in\mathcal{A}_k,v\in\bar{\mathcal{A}}_k\big\vert}{\vert\mathcal{A}_k\vert},
\end{equation}
which penalizes the solution for choosing small cluster sizes. Or we can instead minimize the <strong>Normalized Cut (NCut)</strong>
\begin{equation}
\text{NCut}(\mathcal{A}_1,\ldots,\mathcal{A}_K)=\frac{1}{2}\sum_{k=1}^{K}\frac{\big\vert(u,v)\in\mathcal{E}:u\in\mathcal{A}_k,v\in\bar{\mathcal{A}}_k\big\vert}{\sum_{u\in\mathcal{A}_k}d_u},
\end{equation}
which on the other hand enforces that all clusters have the same number of edges incident to their nodes.</p><h6 id=approximating-the-ratiocut-with-the-laplacian-spectrum>Approximating the RatioCut with the Laplacian spectrum<a hidden class=anchor aria-hidden=true href=#approximating-the-ratiocut-with-the-laplacian-spectrum>#</a></h6><h5 id=generalized-spectral-clustering>Generalized spectral clustering<a hidden class=anchor aria-hidden=true href=#generalized-spectral-clustering>#</a></h5><ul class=number-list><li>Find eigenvectors corresponding to $K$ smallest eigenvalues of $\mathbf{L}$ with the smallest one excluded.</li><li>Form the matrix $\mathbf{U}\in\mathbb{R}^{\vert\mathcal{V}\times(K-1)\vert}$ with the eigenvectors obtained from (1) as columns.</li><li>Choose each row of $\mathbf{U}$ as the embedding for a corresponding node from $\mathcal{V}$
\begin{equation}
\mathbf{z}_u=\mathbf{U}_u\hspace{1cm}\forall u\in\mathcal{V}
\end{equation}</li><li>Run $K$-means clustering on the feature vectors $\mathbf{z}_u,\forall u\in\mathcal{V}$.</li></ul><h3 id=link-level-features>Link-level features<a hidden class=anchor aria-hidden=true href=#link-level-features>#</a></h3><p>The goal of designing edge features is to quantify the relationships between nodes. Let $\mathbf{S}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ denote the similarity matrix, where each entry $S_{u,v}$ denotes the value quantifying the relationship between nodes $u,v\in\mathcal{V}$.</p><ul class=number-list><li><b>Distance-based feature</b>. It measures the shortest path distance between two nodes</li><li><b>Local neighborhood overlap</b>. It counts the number of common neighbors shared by two nodes<ul><li><b>Common neighbors</b>. Which is the naive function
\begin{equation}
S_{u,v}=\vert\mathcal{N}(u)\cap\mathcal{N}(v)\vert
\end{equation}</li><li><b>Sorenson index</b>. It defines a matrix $\mathbf{S}_\text{Sorenson}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ of node-node neighborhood overlaps with entries given by
\begin{equation}
S_{u,v}=\frac{2\vert\mathcal{N}(u)\cap\mathcal{N}(v)\vert}{d_u+d_v}
\end{equation}</li><li><b>Salton index</b>.
\begin{equation}
S_{u,v}=\frac{2\vert\mathcal{N}(u)\cap\mathcal{N}(v)\vert}{\sqrt{d_u d_v}}
\end{equation}</li><li><b>Jaccard index</b>.
\begin{equation}
S_{u,v}=\frac{\vert\mathcal{N}(u)\cap\mathcal{N}(v)\vert}{\vert\mathcal{N}(u)\cup\mathcal{N}(v)\vert}
\end{equation}</li><li><b>Resource Allocation index</b>. It counts the inverse degrees of the common neighbors.
\begin{equation}
S_{v_1,v_2}=\sum_{u\in\mathcal{N}(v_1)\cap\mathcal{N}(v_2)}\frac{1}{d_u}
\end{equation}</li><li><b>Adamic-Adar index</b>. It counts the inverse logarithm of the degrees of the common neighbors.
\begin{equation}
S_{v_1,v_2}=\sum_{u\in\mathcal{N}(v_1)\cap\mathcal{N}(v_2)}\frac{1}{\log d_u}
\end{equation}</li></ul></li><li><b>Global neighborhood overlap</b>. The local neighborhood overlap always returns zero when two nodes have no common neighbor even if they are potentially connected in the future. The global metric resolves this limitation by taking the entire graph into account.<ul><li><b>Katz index</b>. It counts the number of walks of all lengths between two nodes
\begin{equation}
S_{u,v}=\sum_{l=1}^{\infty}\beta^l\mathbf{A}_{u,v}^l,
\end{equation}
where $0<\beta<1$ is the discount factor and each $\mathbf{A}_{u,v}^l$ is an entry of the power matrix $\mathbf{A}^l$.<br><br>To compute the number of walks between two nodes, we consider the number of walks of each length. Let $\mathbf{P}^{(k)}$ be a matrix where each entry $\mathbf{P}_{u,v}^{(k)}$ denote the number of walks of length $k$ between a pair of nodes $(u,v)$. We will show that this matrix is also the power of $k$ of $\mathbf{A}$.
\begin{equation}
\mathbf{P}^{(k)}=\mathbf{A}^k
\end{equation}
It is easily seen that
\begin{equation}
\mathbf{P}_{u,v}^{(1)}=\mathbf{A}_{u,v}
\end{equation}
Let us continue by considering $\mathbf{P}_{u,v}^{(2)}$. This can be calculated by first computing the number of walks between each of $u$'s neighbors and $v$ then summing them up across $u$'s neighbors
\begin{equation}
\mathbf{P}_{u,v}^{(2)}=\sum_i\mathbf{A}_{u,i}\mathbf{P}_{i,v}^{(1)}=\sum_i\mathbf{A}_{u,i}\mathbf{A}_{i,v}=\mathbf{A}_{u,v}
\end{equation}
We can keep doing this to show that it is true that $\mathbf{P}_{u,v}^{(k)}=\mathbf{A}_{u,v}^k$. Or in other words, $\mathbf{A}_{u,v}^k$ is also the number of walks of length $k$ between $u$ and $v$.<br><br><b>Theorem 2</b>. <i>Let $\mathbf{X}$ be a real-valued diagonalizable square matrix and let $\lambda_\text{max}$ denote the largest eigenvalue of $\mathbf{X}$. Then</i>
\begin{equation}
(\mathbf{I}-\mathbf{X})^{-1}=\sum_{i=0}^{\infty}\mathbf{X}^i
\end{equation}
<i>iff $\vert\lambda_\text{max}\vert<1$ and $(\mathbf{I}-\mathbf{X})$ is non-singular.</i><br><br><b>Proof</b>. Let $s_n=\sum_{i=0}^{n}\mathbf{X}^i$, we have
\begin{align}
s_n-\mathbf{X}s_n&=\sum_{i=0}^{n}\mathbf{X}^i-\mathbf{X}\sum_{i=0}^{n}\mathbf{X}^i \\ s_n(\mathbf{I}-\mathbf{X})&=\mathbf{I}-\mathbf{X}^{n+1} \\ s_n&=(\mathbf{I}-\mathbf{X}^{n+1})(\mathbf{I}-\mathbf{X})^{-1}\label{eq:llf.1}
\end{align}
Consider the eigendecomposition of $\mathbf{X}$, we have
\begin{equation}
\mathbf{X}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1},
\end{equation}
which leads us to
\begin{equation}
\mathbf{X}^n=(\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1})^n=\mathbf{Q}\mathbf{\Lambda}^n\mathbf{Q}^{-1}=\mathbf{Q}\left[\begin{matrix}\lambda_1^n&& \\ &\ddots& \\ &&\lambda_D^n\end{matrix}\right]\mathbf{Q}^{-1}
\end{equation}
Hence if $\vert\lambda_\text{max}\vert<1$, as $n\to\infty$, we have that $\mathbf{\Lambda}^n\to 0$, and thus $\mathbf{X}^n\to 0$. Applying this result into \eqref{eq:llf.1} gives us
\begin{equation}
\lim_{n\to\infty}s_n=\lim_{n\to\infty}(\mathbf{I}-\mathbf{X}^{n+1})(\mathbf{I}-\mathbf{X})^{-1}=\mathbf{I}(\mathbf{I}-\mathbf{X})^{-1}=(\mathbf{I}-\mathbf{X})^{-1}
\end{equation}
Based on Theorem 2, we have the closed-form of Katz index is given by
\begin{equation}
\mathbf{S}_\text{Katz}=(\mathbf{I}-\beta\mathbf{A})^{-1}-\mathbf{I}
\end{equation}</li><li><b>Leicht, Holme and Newman (LHN) similarity</b>.</li><li><b>Random walk methods</b>.</li></ul></li></ul><h3 id=graph-level-features>Graph-level features<a hidden class=anchor aria-hidden=true href=#graph-level-features>#</a></h3><h2 id=node-embeddings>Node Embeddings<a hidden class=anchor aria-hidden=true href=#node-embeddings>#</a></h2><p>The goal of node embedding learning methods is to encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood. Or in other words, nodes are encoded so that the similarity in the latent space approximates the similarity in the original graph.</p><h3 id=encoder-decoder-framework>Encoder-Decoder framework<a hidden class=anchor aria-hidden=true href=#encoder-decoder-framework>#</a></h3><p>In this framework, the graph representation learning problem is divided into steps</p><ul class=number-list><li>An <b>encoder</b> maps each node in the original graph to a low-dimensional vector in the embedding space.</li><li>Define a similarity function in the original graph.</li><li>A <b>decoder</b> takes embedding vectors and use them to reconstruct information about each node's neighborhood in the original graph.</li><li>Optimize the encoder and the decoder so that the similarity in the embedding space approximates the similarity in the original graph.</li></ul><h4 id=the-encoder>The Encoder<a hidden class=anchor aria-hidden=true href=#the-encoder>#</a></h4><p>The encoder, denoted $\text{ENC}$, is a function that maps each node $v\in\mathcal{V}$ to an embedding vector $\mathbf{z}_v\in\mathbb{R}^d$.
\begin{equation}
\text{ENC}(v)=\mathbf{z}_v
\end{equation}
In the simplest form, used in the <strong>shallow embedding</strong> approach, the encoder is simply an embedding-lookup
\begin{equation}
\text{ENC}(v)=\mathbf{z}_v=\mathbf{Z}\mathbf{v},
\end{equation}
where $Z\in\mathbb{R}^{d\times\vert\mathcal{V}\vert}$ is a matrix whose each column is a node embedding and where each $\mathbf{v}\in\mathbb{R}^{\vert\mathcal{V}\vert}$ is an indicator vector (i.e., all zeroes except a one in column indicating the ID of node $v$).</p><h4 id=the-decoder>The Decoder<a hidden class=anchor aria-hidden=true href=#the-decoder>#</a></h4><p>The decoder, denoted $\text{DEC}$, reconstruct certain graph statistics (e.g., set of neighbors $\mathcal{N}(u)$) from the embedding $\mathbf{z}_u$ generated by $\text{ENC}$ of node $u$.</p><p>A pairwise decoder, $\text{DEC}:\mathbb{R}^d\times\mathbb{R}^d\mapsto\mathbb{R}^+$ maps each pair of embeddings $(\mathbf{z}_u,\mathbf{z}_v)$ to a similarity score, which describes the relationship between nodes $u$ and $v$.</p><p>Given this similarity score, our goal is to optimize the encoder and decoder so that the decoded similarity approximates the similarity in the original graph.
\begin{equation}
\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)\approx\mathbf{S}(u,v),\label{eq:td.1}
\end{equation}
where $\mathbf{S}(u,v)$ is a graph-based similarity metric between nodes $u$ and $v$.</p><h4 id=model-optimization>Model optimization<a hidden class=anchor aria-hidden=true href=#model-optimization>#</a></h4><p>The reconstruction objective \eqref{eq:td.1} can be accomplished by minimizing an empirical reconstruction loss $\mathcal{L}$ over a set of training node pairs $\mathcal{D}$
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\ell(\text{DEC}(\mathbf{z}_u,\mathbf{z}_v),\mathbf{S}(u,v)),
\end{equation}
where $\ell:\mathbb{R}\times\mathbb{R}\mapsto\mathbb{R}$ is a loss function measures the difference between the decoded similarity values $\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)$ and the true similarity values $\mathbf{S}(u,v)$.</p><h3 id=factorization-based-approaches>Factorization-based approaches<a hidden class=anchor aria-hidden=true href=#factorization-based-approaches>#</a></h3><h4 id=laplacian-eigenmaps>Laplacian eigenmaps<a hidden class=anchor aria-hidden=true href=#laplacian-eigenmaps>#</a></h4><p>In this approach, the decoder is defined as the L2-distance between the embeddings
\begin{equation}
\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)=\Vert\mathbf{z}_u-\mathbf{z}_v\Vert_2^2
\end{equation}
And the loss function is then given as
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)\cdot\mathbf{S}(u,v)
\end{equation}</p><h4 id=inner-product-methods>Inner-product methods<a hidden class=anchor aria-hidden=true href=#inner-product-methods>#</a></h4><p>As suggested by their name, the decoder in these approaches is defined as the inner product
\begin{equation}
\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)=\mathbf{z}_u^\text{T}\mathbf{z}_v
\end{equation}
These methods have the loss function given as
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\Vert\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)-\mathbf{S}(u,v)\Vert_2^2
\end{equation}
The above approaches are referred to as matrix-factorization methods, since their loss function can be minimized using factorization algorithm, such as SVD. Stacking the embeddings $\mathbf{z}_u\in\mathbb{R}^d$ into a matrix $\mathbf{Z}\in\mathbb{R}^{\vert\mathcal{V}\vert\times d}$ the reconstruction objective can be rewritten as
\begin{equation}
\mathcal{L}\approx\Vert\mathbf{Z}\mathbf{Z}^\text{T}-\mathbf{S}\Vert_2^2,
\end{equation}
where $\mathbf{S}$ is a matrix containing pairwise similarity measures.</p><h3 id=random-walk-embeddings>Random walk embeddings<a hidden class=anchor aria-hidden=true href=#random-walk-embeddings>#</a></h3><h2 id=graph-neural-networks>Graph Neural Networks<a hidden class=anchor aria-hidden=true href=#graph-neural-networks>#</a></h2><h3 id=graph-convolution-networks>Graph Convolution Networks<a hidden class=anchor aria-hidden=true href=#graph-convolution-networks>#</a></h3><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] William L. Hamilton. <a href=https://www.cs.mcgill.ca/~wlh/grl_book/>Graph Representation Learning</a>. Morgan and Claypool, Synthesis Lectures on Artificial Intelligence and Machine Learning.</p><p>[2] Jure Leskovec. <a href=https://web.stanford.edu/class/cs224w/>CS224W - Machine Learning with Graphs</a>.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://trunghng.github.io/tags/neural-network/>neural-network</a></li><li><a href=https://trunghng.github.io/tags/graph-neural-network/>graph-neural-network</a></li><li><a href=https://trunghng.github.io/tags/graph-theory/>graph-theory</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/machine-learning/vae/><span class=title>« Prev</span><br><span>Variational Autoencoder</span>
</a><a class=next href=https://trunghng.github.io/posts/reinforcement-learning/muzero/><span class=title>Next »</span><br><span>MuZero</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on x" href="https://x.com/intent/tweet/?text=Graph%20Representation%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f&amp;hashtags=machine-learning%2cneural-network%2cgraph-neural-network%2cgraph-theory"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f&amp;title=Graph%20Representation%20Learning&amp;summary=Graph%20Representation%20Learning&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f&title=Graph%20Representation%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on whatsapp" href="https://api.whatsapp.com/send?text=Graph%20Representation%20Learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on telegram" href="https://telegram.me/share/url?text=Graph%20Representation%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Graph Representation Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Graph%20Representation%20Learning&u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgraph-representation-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io/>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>