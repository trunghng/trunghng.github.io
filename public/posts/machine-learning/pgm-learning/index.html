<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Read-through: Probabilistic Graphical Models - Learning | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="machine-learning,probabilistic-graphical-model"><meta name=description content="
Notes on Learning in PGMs.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/machine-learning/pgm-learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Read-through: Probabilistic Graphical Models - Learning"><meta property="og:description" content="
Notes on Learning in PGMs.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/machine-learning/pgm-learning/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-02-19T17:23:56+07:00"><meta property="article:modified_time" content="2023-02-19T17:23:56+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Read-through: Probabilistic Graphical Models - Learning"><meta name=twitter:description content="
Notes on Learning in PGMs.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Read-through: Probabilistic Graphical Models - Learning","item":"https://trunghng.github.io/posts/machine-learning/pgm-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Read-through: Probabilistic Graphical Models - Learning","name":"Read-through: Probabilistic Graphical Models - Learning","description":" Notes on Learning in PGMs.\n","keywords":["machine-learning","probabilistic-graphical-model"],"articleBody":" Notes on Learning in PGMs.\nParameter Learning Fully Observed Data MLE for Bayesian Networks Suppose that we have a Bayesian network of two binary nodes $X,Y$ connected by $X\\to Y$.\nThe network is parameterized by a parameter vector $\\boldsymbol{\\theta}$, which defines the set of parameters of all the CPDs in the network, i.e. \\begin{equation} \\boldsymbol{\\theta}_X=\\{\\theta_{x^0},\\theta_{x^1}\\} \\end{equation} and \\begin{equation} \\boldsymbol{\\theta}_{Y\\vert X}=\\boldsymbol{\\theta}_{Y\\vert x_0}\\cup\\boldsymbol{\\theta}_{Y\\vert x^1}=\\{\\theta_{y^0\\vert x^0},\\theta_{y^1\\vert x^0}\\}\\cup\\{\\theta_{y^0\\vert x^1},\\theta_{y^1\\vert x^1}\\} \\end{equation} Assuming that we are given the training set \\begin{equation} \\mathcal{D}=\\{(x[1],y[1]),\\ldots,(x[M],y[M])\\} \\end{equation} which describes $M$ instances of variables $X$ and $Y$. The likelihood function is then given as \\begin{align} L(\\boldsymbol{\\theta}:\\mathcal{D})\u0026=\\prod_{m=1}^{M}P(x[m],y[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\prod_{m=1}^{M}P(x[m];\\boldsymbol{\\theta})P(y[m]\\big\\vert x[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\left(\\prod_{m=1}^{M}P(x[m];\\boldsymbol{\\theta})\\right)\\left(\\prod_{m=1}^{M}P(y[m]\\big\\vert x[m];\\boldsymbol{\\theta})\\right), \\end{align} which decomposes into two terms, on for each variable. Each of these are referred as local likelihood function that measures how well the variable is predicted given its parents.\nGlobal Likelihood Decomposition Generally, suppose that we want to learn a parameters $\\boldsymbol{\\theta}$ for Bayesian network structure $\\mathcal{G}$. Given a dataset $\\mathcal{D}=\\{\\xi[1],\\ldots,\\xi[M]\\}$, analogy to the argument above, we have that the likelihood function is given by \\begin{align} L(\\boldsymbol{\\theta}:\\mathcal{D})\u0026=\\prod_{m=1}^{M}P_\\mathcal{G}(\\xi[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\prod_{m=1}^{M}\\prod_i P\\big(x_i[m]\\big\\vert\\text{pa}_{X_i}[m];\\boldsymbol{\\theta}\\big) \\\\ \u0026=\\prod_i\\left[\\prod_{m=1}^{M}P\\big(x_i[m]\\big\\vert\\text{pa}_{X_i}[m];\\boldsymbol{\\theta}\\big)\\right]\\label{eq:gld.1} \\end{align} Each of the terms in the square brackets refers to the conditional likelihood of a particular variable given its parents in the network. Also, let $\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}$ denote the subset of parameters that determines $P(X_i\\vert\\text{Pa}_{X_i})$. Thus, the local likelihood function for $X_i$ is then given by \\begin{equation} L_i(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}:\\mathcal{D})=\\prod_{m=1}^{M}P\\big(x_i[m]\\big\\vert\\text{pa}_{X_i}[m];\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}\\big), \\end{equation} which allows us to rewrite the likelihood function \\eqref{eq:gld.1} as \\begin{equation} L(\\boldsymbol{\\theta}:\\mathcal{D})=\\prod_i L_i(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}:\\mathcal{D}) \\end{equation} In other words, when $\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}$ are disjoint, the likelihood can be decomposed as a product of independent terms, one for each CPD of the network. This property is known as the global decomposition of the likelihood function.\nAdditionally, we can maximize each local likelihood function $L_i(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}:\\mathcal{D})$ independently of the others, and then combine the solutions together to get an MLE solution.\nTable-CPDs As the MLE solution for a Bayesian network can be computed via parameterization of its CPDs, we now consider the simplest parameterization of the CPD, tabular CPD, or table-CPD.\nSuppose we have a variable $X$ with parents $\\mathbf{U}$. If we represent the CPD $P(X\\vert\\mathbf{U})$ as a table, we then have a parameter $\\theta_{x\\vert\\mathbf{u}}$ for each $x\\in\\text{Val}(X)$ and $\\mathbf{u}\\in\\text{Val}(\\mathbf{U})$. The local likelihood function is then can be decomposed further as \\begin{align} L_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}})\u0026=\\prod_{m=1}^{M}\\theta_{x[m]\\vert\\mathbf{u}[m]} \\\\ \u0026=\\prod_{\\mathbf{u}\\in\\text{Val}(\\mathbf{U})}\\left(\\prod_{x\\in\\text{Val}(X)}\\theta_{x\\vert\\mathbf{u}}^{M[\\mathbf{u},x]}\\right), \\end{align} where $M[\\mathbf{u},x]$ is the number of times $x[m]=x$ and $\\mathbf{u}[m]=\\mathbf{u}$ in $\\mathcal{D}$.\nGaussian Bayesian Networks Consider a variable $X$ with parents $\\mathbf{U}=\\{U_1,\\ldots,U_k\\}$ with a linear Gaussian CPD \\begin{equation} P(X\\vert\\mathbf{u})=\\mathcal{N}(\\beta_0+\\beta_1 u_1+\\ldots+\\beta_k u_k;\\sigma^2) \\end{equation} Thus, we have that \\begin{equation} P(x\\vert\\mathbf{u})=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(\\beta_0+\\beta_1 u_1+\\ldots+\\beta_k u_k-x)^2}{2\\sigma^2}\\right] \\end{equation} Our task is to learn the parameters $\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}=(\\beta_0,\\ldots,\\beta_k,\\sigma)$. We continue by considering the log-likelihood \\begin{align} \\ell_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}:\\mathcal{D})\u0026=\\log L_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}) \\\\ \u0026=\\log\\prod_{m=1}^{M}P\\big(x[m]\\big\\vert\\mathbf{u}[m];\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}\\big) \\\\ \u0026=\\sum_{m=1}^{M}\\log P\\big(x[m]\\big\\vert\\mathbf{u}[m];\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}\\big) \\\\ \u0026=\\sum_{m=1}^{M}\\left[\\frac{1}{2}\\log(2\\pi\\sigma^2)-\\frac{1}{2}\\frac{1}{\\sigma^2}\\big(\\beta_0+\\beta_1 u_1[m]+\\ldots+\\beta_k u_k[m]-x[m]\\big)^2\\right] \\end{align} Taking the derivative of the log-likelihood w.r.t $\\beta_0$ gives us \\begin{align} \\frac{\\partial}{\\partial\\beta_0}\\ell_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}:\\mathcal{D})\u0026=\\sum_{m=1}^{M}-\\frac{1}{\\sigma^2}\\big(\\beta_0+\\beta_1 u_1[m]+\\ldots+\\beta_k u_k[m]-x[m]\\big) \\\\ \u0026=-\\frac{1}{\\sigma^2}\\left(M\\beta_0+\\beta_1\\sum_{m=1}^{M}u_1[m]+\\ldots+\\beta_k\\sum_{m=1}^{M}u_k[m]-\\sum_{m=1}^{M}x[m]\\right) \\end{align} Setting the derivative to zero, we have that \\begin{equation} \\frac{1}{M}\\sum_{m=1}^{M}x[m]=\\beta_0+\\beta_1\\frac{1}{M}\\sum_{m=1}^{M}u_1[m]+\\ldots+\\beta_k\\frac{1}{M}\\sum_{m=1}^{M}u_k[m]\\label{eq:gbn.1} \\end{equation} Then, if we define \\begin{equation} \\mathbb{E}_\\mathcal{D}[X]\\doteq\\frac{1}{M}\\sum_{m=1}^{M}x[m], \\end{equation} which represents the average value a variable $X$. Thus, we can rewrite \\eqref{eq:gbn.1} as \\begin{equation} \\mathbb{E}_\\mathcal{D}[X]=\\beta_0+\\beta_1\\mathbb{E}_\\mathcal{D}[U_1]+\\ldots+\\beta_k\\mathbb{E}_\\mathcal{D}[U_k]\\label{eq:gbn.2} \\end{equation} On the other hand, differentiating the log-likelihood function w.r.t $\\beta_i$ for $i\\neq 0$ gives us \\begin{align} \u0026\\hspace{-0.5cm}\\frac{\\partial}{\\partial\\beta_i}\\ell_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}:\\mathcal{D})\\nonumber \\\\ \u0026\\hspace{-0.5cm}=\\sum_{m=1}^{M}-\\frac{u_i[m]}{\\sigma^2}\\big(\\beta_0+\\beta_1 u_1[m]+\\ldots+\\beta_k u_k[m]-x[m]\\big) \\\\ \u0026\\hspace{-0.5cm}=-\\frac{1}{\\sigma^2}\\left(M\\beta_0+\\beta_1\\sum_{m=1}^{M}u_1[m]u_i[m]+\\ldots+\\beta_k\\sum_{m=1}^{M}u_k[m]u_i[m]-\\sum_{m=1}^{M}x[m]u_i[m]\\right) \\end{align} Similarly, setting this derivative to zero lets us obtain \\begin{equation} \\mathbb{E}_\\mathcal{D}[X U_i]=\\beta_0\\mathbb{E}_\\mathcal{D}[U_i]+\\beta_1\\mathbb{E}_\\mathcal{D}[U_1 U_i]+\\ldots+\\beta_k\\mathbb{E}_\\mathcal{D}[U_k U_i]\\label{eq:gbn.3} \\end{equation} From the results \\eqref{eq:gbn.2} and \\eqref{eq:gbn.3}, we can find the MLE solution $\\hat{\\boldsymbol{\\beta}}$ by solving the system of linear equations \\begin{equation} \\left[\\begin{matrix}1\u0026\\mathbb{E}_\\mathcal{D}[U_1]\u0026\\ldots\u0026\\mathbb{E}_\\mathcal{D}[U_k] \\\\ \\mathbb{E}_\\mathcal{D}[U_1]\u0026\\mathbb{E}_\\mathcal{D}[U_1 U_1]\u0026\\ldots\u0026\\mathbb{E}_\\mathcal{D}[U_1 U_k] \\\\ \\vdots\u0026\\vdots\u0026\\ddots\u0026\\vdots \\\\ \\mathbb{E}_\\mathcal{D}[U_k]\u0026\\mathbb{E}_\\mathcal{D}[U_k U_1]\u0026\\ldots\u0026\\mathbb{E}_\\mathcal{D}[U_k U_k]\\end{matrix}\\right]\\left[\\begin{matrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k\\end{matrix}\\right]=\\left[\\begin{matrix}\\mathbb{E}_\\mathcal{D}[X] \\\\ \\mathbb{E}_\\mathcal{D}[X U_1] \\\\ \\vdots \\\\ \\mathbb{E}_\\mathcal{D}[X U_k]\\end{matrix}\\right] \\end{equation} Additionally, by multiplying both sides of \\eqref{eq:gbn.2} with $\\mathbb{E}_\\mathcal{D}[U_i]$, we have \\begin{equation} \\mathbb{E}_\\mathcal{D}[X]\\mathbb{E}_\\mathcal{D}[U_i]=\\beta_0\\mathbb{E}_\\mathcal{D}[U_i]+\\beta_1\\mathbb{E}_\\mathcal{D}[U_1]\\mathbb{E}_\\mathcal{D}[U_i]+\\ldots+\\beta_k\\mathbb{E}_\\mathcal{D}[U_k]\\mathbb{E}_\\mathcal{D}[U_i] \\end{equation} Then, subtracting this equation from \\eqref{eq:gbn.3} gives us \\begin{align} \\mathbb{E}_\\mathcal{D}[X U_i]-\\mathbb{E}_\\mathcal{D}[X]\\mathbb{E}_\\mathcal{D}[U_i]\u0026=\\beta_1\\big(\\mathbb{E}_\\mathcal{D}[U_1 U_i]-\\mathbb{E}_\\mathcal{D}[U_1]\\mathbb{E}_\\mathcal{D}[U_i]\\big)+\\ldots+\\nonumber \\\\ \u0026\\hspace{0.6cm}\\beta_k\\big(\\mathbb{E}_\\mathcal{D}[U_k U_i]-\\mathbb{E}_\\mathcal{D}[U_k]\\mathbb{E}_\\mathcal{D}[U_i]\\big), \\end{align} or \\begin{equation} \\text{Cov}_\\mathcal{D}[X,U_i]=\\beta_1\\text{Cov}_\\mathcal{D}[U_1,U_i]+\\ldots+\\beta_k\\text{Cov}_\\mathcal{D}[U_k,U_i]\\label{eq:gbn.4} \\end{equation} where we have defined $\\text{Cov}_\\mathcal{D}[X,U_i]$ as the observed covariance of $X$ and $U_i$ in the data.\nFinally, differentiating the log-likelihood w.r.t $\\sigma^2$, we have that \\begin{equation} \\frac{\\partial}{\\partial\\sigma}\\ell_X(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}}:\\mathcal{D})=\\sum_{m=1}^{M}\\left[\\frac{1}{2}\\frac{1}{\\sigma^2}+\\frac{1}{2}\\frac{1}{(\\sigma^2)^2}\\big(\\beta_0+\\beta_1 u_1[m]+\\ldots+\\beta_k u_k[m]-x[m]\\big)^2\\right] \\end{equation} Analogously, setting this derivative to zero, we have that \\begin{equation} \\sigma^2=\\text{Cov}_\\mathcal{D}[X,X]-\\sum_{i=1}^{k}\\sum_{j=1}^{k}\\beta_i\\beta_j\\text{Cov}_\\mathcal{D}[U_i,U_j]\\label{eq:gbn.5} \\end{equation}\nRemark\nExample 1: Let us estimate a joint multivariate Gaussian distribution. Specifically, consider continuous r.v.s $X$ and $Y$ and assume we have a dataset of $M$ samples $\\mathcal{D}=\\{(x[1],y[1]),\\ldots,(x[M],y[M])\\}$. Our job is to find the MLE estimate for a joint Gaussian distribution over $X,Y$.\nLet $\\mathbf{Z}$ be a random vector that encodes the joint distribution of $X$ and $Y$. In particular \\begin{equation} \\mathbf{Z}=\\left[\\begin{matrix}X \\\\ Y\\end{matrix}\\right] \\end{equation} We have that $\\mathbf{Z}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with \\begin{equation} \\boldsymbol{\\mu}=\\left[\\begin{matrix}\\mu_X \\\\ \\mu_Y\\end{matrix}\\right],\\hspace{1cm}\\boldsymbol{\\Sigma}=\\left[\\begin{matrix}\\Sigma_{XX}\u0026\\Sigma_{XY} \\\\ \\Sigma_{YX}\u0026\\Sigma_{YY}\\end{matrix}\\right] \\end{equation} Thus, we have that \\begin{equation} P(\\mathbf{z})=\\frac{1}{2\\pi\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{z}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}-\\boldsymbol{\\mu})\\right] \\end{equation} Our job then is to learn the parameter $\\boldsymbol{\\theta}=(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$. We begin by considering the log-likelihood function \\begin{align} \\ell(\\boldsymbol{\\theta})\u0026=\\log\\prod_{m=1}^{M}P(x[m],y[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\log\\prod_{m=1}^{M}\\frac{1}{2\\pi\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{z}[m]-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}[m]-\\boldsymbol{\\mu})\\right] \\\\ \u0026=\\sum_{m=1}^{M}-\\log(2\\pi)-\\frac{1}{2}\\log\\vert\\boldsymbol{\\Sigma}\\vert-\\frac{1}{2}(\\mathbf{z}[m]-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}[m]-\\boldsymbol{\\mu}) \\end{align} Taking the derivative of the log-likelihood w.r.t $\\boldsymbol{\\mu}$ gives us \\begin{align} \\frac{\\partial}{\\partial\\boldsymbol{\\mu}}\\ell(\\boldsymbol{\\theta})\u0026=\\sum_{m=1}^{M}\\frac{1}{2}\\big[\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}[m]-\\boldsymbol{\\mu})-(\\boldsymbol{\\Sigma}^{-1})^\\text{T}(\\mathbf{z}[m]-\\boldsymbol{\\mu})\\big] \\\\ \u0026=\\sum_{m=1}^{M}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}[m]-\\boldsymbol{\\mu}), \\end{align} where we have used the fact that the covariance matrix $\\boldsymbol{\\Sigma}$ is symmetric. Setting the derivative to zero we obtain the MLE solution for $\\boldsymbol{\\mu}$ \\begin{equation} \\boldsymbol{\\mu}=\\frac{1}{M}\\sum_{m=1}^{M}\\mathbf{z}[m]=\\left[\\begin{matrix}\\mathbb{E}_\\mathcal{D}[X] \\\\ \\mathbb{E}_\\mathcal{D}[Y]\\end{matrix}\\right] \\end{equation} On the other hand, differentiating the log-likelihood w.r.t $\\boldsymbol{\\Sigma}$, we obtain \\begin{align} \\frac{\\partial}{\\partial\\boldsymbol{\\Sigma}}\\ell(\\boldsymbol{\\theta})\u0026=\\sum_{m=1}^{M}-\\frac{1}{2}\\frac{\\vert\\boldsymbol{\\Sigma}\\vert\\boldsymbol{\\Sigma}^{-1}}{\\vert\\boldsymbol{\\Sigma}\\vert}+\\frac{1}{2}\\big[(\\boldsymbol{\\Sigma}^{-1})^\\text{T}(\\mathbf{z}[m]-\\boldsymbol{\\mu})(\\mathbf{z}[m]-\\boldsymbol{\\mu})^\\text{T}(\\boldsymbol{\\Sigma}^{-1})^\\text{T}\\big] \\\\ \u0026=\\frac{1}{2}\\sum_{m=1}^{M}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{z}[m]-\\boldsymbol{\\mu})(\\mathbf{z}[m]-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}-\\boldsymbol{\\Sigma}^{-1} \\end{align} Setting this derivative to zero, we have that \\begin{align} \\boldsymbol{\\Sigma}\u0026=\\sum_{m=1}^{M}(\\mathbf{z}[m]-\\boldsymbol{\\mu})(\\mathbf{z}[m]-\\boldsymbol{\\mu})^\\text{T} \\\\ \u0026=\\sum_{m=1}^{M}\\left[\\begin{matrix}(x[m]-\\mu_X)^2\u0026(x[m]-\\mu_X)(y[m]-\\mu_Y) \\\\ (y[m]-\\mu_Y)(x[m]-\\mu_X)\u0026(y[m]-\\mu_Y)^2\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\text{Cov}_\\mathcal{D}[X,X]\u0026\\text{Cov}_\\mathcal{D}[X,Y] \\\\ \\text{Cov}_\\mathcal{D}[Y,X]\u0026\\text{Cov}_\\mathcal{D}[Y,Y]\\end{matrix}\\right] \\end{align}\nBayesian Parameter Estimation General setting In the Bayesian approach, as before, we assume a general learning problem where we observe a training set $\\mathcal{D}=\\{\\xi[1],\\ldots,\\xi[M]\\}$ and a parametric model $P(\\xi\\vert\\boldsymbol{\\theta})$ where we can choose parameters from a parameter space $\\Theta$, i.e. in this case, samples according to the probabilistic model are conditionally i.i.d given $\\boldsymbol{\\theta}$ instead of, recalling that in MLE, being (unconditionally) i.i.d.\nPriors, Posteriors To perform the task, we need to define a joint distribution $P(\\mathcal{D},\\boldsymbol{\\theta})$ over the data and the parameters, which can be written by \\begin{equation} P(\\mathcal{D},\\boldsymbol{\\theta})=P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}), \\end{equation} where\n$P(\\mathcal{D}\\vert\\boldsymbol{\\theta})$ is the likelihood function, which is the probability of the observations given the parameters, as in the MLE approach. $P(\\boldsymbol{\\theta})$ is referred as the prior distribution, which encodes our prior beliefs, i.e. before data is observed. By Bayes’ rule, from the likelihood and prior, combined with the defined joint distribution, we can derive the so-called posterior distribution over the parameters, which corresponds to our beliefs after observing the data, as \\begin{align} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})\u0026=\\frac{P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(\\mathcal{D})} \\\\ \u0026=\\frac{P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{\\int_\\Theta P(\\mathcal{D}\\vert\\boldsymbol{\\theta}’)P(\\boldsymbol{\\theta}’)d\\boldsymbol{\\theta}’}, \\end{align} where, since the denominator is just a normalizing constant, can be expressed as \\begin{equation} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})\\propto P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}) \\end{equation} Since the posterior is a (normalized) product of the prior and the likelihood, it seems natural to require that the prior also have a form similar to the likelihood, such priors are referred as conjugate priors.\nMore formally, a family of priors $P(\\boldsymbol{\\theta}:\\boldsymbol{\\alpha})$ is conjugate to a particular model $P(\\xi\\vert\\boldsymbol{\\theta})$ if for any possible dataset $\\mathcal{D}$ of i.i.d samples from $P(\\xi\\vert\\boldsymbol{\\theta})$, and any choice of legal hyperparameters $\\boldsymbol{\\alpha}$ for the prior over $\\boldsymbol{\\theta}$, there are hyperparameters $\\boldsymbol{\\alpha}’$ that describe the posterior, i.e. \\begin{equation} P(\\boldsymbol{\\theta}:\\boldsymbol{\\alpha}’)\\propto P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}:\\boldsymbol{\\alpha}) \\end{equation}\nExample 2: If we select Dirichlet as the prior distribution, specifically, let $\\boldsymbol{\\theta}=(\\theta_1,\\ldots,\\theta_K)$ and $\\boldsymbol{\\theta}\\sim\\text{Dirichlet}(\\alpha_1,\\ldots,\\alpha_K)$, we have that \\begin{equation} P(\\boldsymbol{\\theta})\\propto\\prod_{k=1}^{K}\\theta_k^{\\alpha_k-1} \\end{equation} Then we have that the posterior $P(\\boldsymbol{\\theta}\\vert\\mathcal{D})$ is $\\text{Dirichlet}(\\alpha_1+M[1],\\ldots,\\alpha_K+M[K])$, where $M[k]$ is the number of occurrences of $x^k$.\nBayesian Estimator From the posterior, we can predict the probability of future samples. Specifically, suppose that we are about to sample a new instance $\\xi[M+1]$, then the Bayesian estimator, or the predictive distribution, is the posterior distribution over a new example. \\begin{align} P(\\xi[M+1]\\vert\\mathcal{D})\u0026=\\int P(\\xi[M+1]\\vert\\mathcal{D},\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}\\vert\\mathcal{D})d\\boldsymbol{\\theta} \\\\ \u0026=\\int P(\\xi[M+1]\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}\\vert\\mathcal{D})d\\boldsymbol{\\theta} \\\\ \u0026=\\mathbb{E}_{P(\\boldsymbol{\\theta}\\vert\\mathcal{D})}\\big[P(\\xi[M+1]\\vert\\boldsymbol{\\theta})\\big], \\end{align} where in the second step, we use the fact that samples are i.i.d given $\\boldsymbol{\\theta}$.\nBayesian Parameter Estimation in Bayesian Networks Since in Bayesian framework, it is required to specify a joint distribution over the training examples and the unknown parameters. Additionally, this joint distribution can be considered as a Bayesian network.\nParameter Independence and Global Decomposition Suppose that we want to estimate parameters for a networks consisting of two variables $X$ and $Y$ with edge $X\\rightarrow Y$. Also, we are given a training dataset $\\mathcal{D}=\\{(x[1],y[1]),\\ldots,(x[M],y[M])\\}$. Additionally, we have unknown parameters $\\boldsymbol{\\theta}_X,\\boldsymbol{\\theta}_{Y\\vert X}$. The dependencies between variables are described in the following network.\nFigure 1: (taken from PGM book) Meta-network for i.i.d samples from a network $X\\rightarrow Y$ with global parameter independence. (a) Plate model; (b) Ground Bayesian network We have already known that in using Bayesian approach, the samples are independent given the parameters, i.e. \\begin{equation} \\text{d-sep}(\\{X[m],Y[m]\\};\\{X[m’],Y[m’]\\}\\vert\\{\\boldsymbol{\\theta}_X,\\boldsymbol{\\theta}_{Y\\vert X}\\}), \\end{equation} which can be justified by an examination of active trails from the above figure. Moreover, the network structure given in Figure 1 also satisfies the global parameter independence.\nLet $\\mathcal{G}$ be a BN structure with parameters $\\boldsymbol{\\theta}=(\\boldsymbol{\\theta}_{X_1\\vert\\text{Pa}_{X_1}},\\ldots,\\boldsymbol{\\theta}_{X_n\\vert\\text{Pa}_{X_n}})$. Then, a prior $P(\\boldsymbol{\\theta})$ is said to satisfy global parameter independence if it has the form \\begin{equation} P(\\boldsymbol{\\theta})=\\prod_{i=1}^{n}P(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}) \\end{equation} This property allows us to conclude that complete data, $\\mathcal{D}$, d-separates the parameters for different CPDs, i.e. \\begin{equation} \\text{d-sep}\\big(\\boldsymbol{\\theta}_X;\\boldsymbol{\\theta}_{Y\\vert X}\\big\\vert\\mathcal{D}\\big) \\end{equation} This is due to for each $m$, we have that any path between $X[m]$ and $Y[m]$ has the form \\begin{equation} \\boldsymbol{\\theta}_X\\rightarrow X[m]\\rightarrow Y[m]\\leftarrow\\boldsymbol{\\theta}_{Y\\vert X}, \\end{equation} which is inactive given the observation $x[m],y[m]$. Thus, we obtain that \\begin{equation} P(\\boldsymbol{\\theta}_X,\\boldsymbol{\\theta}_{Y\\vert X}\\vert\\mathcal{D})=P(\\boldsymbol{\\theta}_X\\vert\\mathcal{D})P(\\boldsymbol{\\theta}_{Y\\vert X}\\vert\\mathcal{D}) \\end{equation} This decomposition suggests that given $\\mathcal{D}$, we can find the solution for the posterior over $\\boldsymbol{\\theta}$ by independently finding the result corresponding to the posterior over each of $\\boldsymbol{\\theta}_X$ and $\\boldsymbol{\\theta}_{Y\\vert X}$, which is analogous to the global likelihood decomposition in the MLE. In Bayesian setting this property has additional importance.\nGenerally, suppose that we are given a Bayesian network graph $\\mathcal{G}$ with parameters $\\boldsymbol{\\theta}$. As mentioned before that in using Bayesian approach, we need to specify a prior distribution over the parameter space $P(\\boldsymbol{\\theta})$ and a posterior distribution over the parameters given the samples $\\mathcal{D}$ \\begin{equation} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})=\\frac{P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(\\mathcal{D})}\\label{eq:pigd.1} \\end{equation} Moreover, assuming that we have global parameter independence, then combining with the global likelihood decomposition mentioned above, the posterior in \\eqref{eq:pigd.1} can be continued to derive as a product of local terms: \\begin{align} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})\u0026=\\frac{P(\\mathcal{D}\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(\\mathcal{D})} \\\\ \u0026=\\frac{1}{P(\\mathcal{D})}\\left(\\prod_i L_i(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}:\\mathcal{D})\\right)\\left(\\prod_i P(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}})\\right) \\\\ \u0026=\\frac{1}{P(\\mathcal{D})}\\prod_i\\left(L_i(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}:\\mathcal{D})P(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}})\\right) \\end{align} This gives rise to the following result.\nProposition 1: Let $\\mathcal{D}$ be a complete dataset for $\\mathcal{X}$, let $\\mathcal{G}$ be a BN graph over $\\mathcal{X}$. If $P(\\boldsymbol{\\theta})$ satisfies global parameter independence, then \\begin{equation} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})=\\prod_i P(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}\\vert\\mathcal{D}) \\end{equation}\nExample 3: With the network specified in Figure 1, let us compute the predictive distribution. We have that \\begin{align} \u0026\\hspace{-0.5cm}P(x[M+1],y[M+1]\\vert\\mathcal{D})\\nonumber \\\\ \u0026\\hspace{-0.3cm}=\\int P(x[M+1],y[M+1]\\vert\\mathcal{D},\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}\\vert\\mathcal{D})d\\boldsymbol{\\theta} \\\\ \u0026\\hspace{-0.3cm}=\\int P(x[M+1],y[M+1]\\vert\\boldsymbol{\\theta})P(\\boldsymbol{\\theta}\\vert\\mathcal{D})d\\boldsymbol{\\theta} \\\\ \u0026\\hspace{-0.3cm}=\\int P(x[M+1]\\vert\\boldsymbol{\\theta}_X)P(y[M+1]\\vert x[M+1],\\boldsymbol{\\theta}_{Y\\vert X})P(\\boldsymbol{\\theta}_X\\vert\\mathcal{D})P(\\boldsymbol{\\theta}_{Y\\vert X}\\vert\\mathcal{D})d\\boldsymbol{\\theta} \\\\ \u0026\\hspace{-0.3cm}=\\int\\int P(x[M+1]\\vert\\boldsymbol{\\theta}_X)P(y[M+1]\\vert x[M+1],\\boldsymbol{\\theta}_{Y\\vert X})P(\\boldsymbol{\\theta}_X\\vert\\mathcal{D})P(\\boldsymbol{\\theta}_{Y\\vert X}\\vert\\mathcal{D})d\\boldsymbol{\\theta}_X d\\boldsymbol{\\theta}_{Y\\vert X} \\\\ \u0026\\hspace{-0.3cm}=\\left(\\int P(x[M+1]\\vert\\boldsymbol{\\theta}_X)P(\\boldsymbol{\\theta}_X\\vert\\mathcal{D})d\\boldsymbol{\\theta}_X\\right)\\nonumber \\\\ \u0026\\hspace{2cm}\\left(\\int P(y[M+1]\\vert x[M+1],\\boldsymbol{\\theta}_{Y\\vert X})P(\\boldsymbol{\\theta}_{Y\\vert X}\\vert\\mathcal{D})d\\boldsymbol{\\theta}_{Y\\vert X}\\right) \\end{align} Thus, we can solve the prediction problem for the two variables $X$ and $Y$ independently.\nIn general, we can solve the prediction problem for each CPD, $P(X_i\\vert\\text{Pa}_{X_i})$, independently the combine the results together, i.e. \\begin{align} \u0026P(X_1[M+1],\\ldots,X_n[M+1]\\vert\\mathcal{D})\\nonumber \\\\ \u0026=\\prod_{i=1}^{n}\\int P\\big(X_i[M+1]\\big\\vert\\text{Pa}_{X_i}[M+1],\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}\\big)P\\big(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}\\big\\vert\\mathcal{D}\\big)d\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}} \\end{align}\nLocal Decomposition By the global decomposition, our attention is then to solve localized Bayesian estimation problems.\nLet $X$ be a variable with parent $\\mathbf{U}$. We say that the prior $P(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}})$ satisfies local parameter independence if \\begin{equation} P(\\boldsymbol{\\theta}_{X\\vert\\mathbf{U}})=\\prod_\\mathbf{u} P(\\theta_{X\\vert\\mathbf{u}}), \\end{equation} This definition gives rise to the following result.\nProposition 2: Let $\\mathcal{D}$ be a complete dataset for $\\mathcal{X}$, and let $\\mathcal{G}$ be a BN graph over these variables with table-CPDs. If the prior $P(\\boldsymbol{\\theta})$ satisfies global and local parameter independence, then \\begin{align} P(\\boldsymbol{\\theta}\\vert\\mathcal{D})\u0026=\\prod_i P(\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}}\\vert\\mathcal{D}) \\\\ \u0026=\\prod_i\\prod_{\\text{pa}_{X_i}}P(\\boldsymbol{\\theta}_{X_i\\vert\\text{pa}_{X_i}}\\vert\\mathcal{D}) \\end{align}\nMAP Estimation Partially Observed Data Likelihood of Data and Observation Models Let $\\mathbf{X}=\\{X_1,\\ldots,X_n\\}$ be some set of r.v.s, and let $O_\\mathbf{X}=\\{O_{X_1},\\ldots,O_{X_n}\\}$ be their observability variable, which indicates whether the value of $X_i$ is observed. The observability model is a joint distribution \\begin{equation} P_\\text{missing}(\\mathbf{X},O_\\mathbf{X})=P(\\mathbf{X})P_\\text{missing}(O_\\mathbf{X}\\vert\\mathbf{X}), \\end{equation} so that $P(\\mathbf{X})$ is parameterized by parameters $\\boldsymbol{\\theta}$, and $P_\\text{missing}(O_\\mathbf{X}\\vert\\mathbf{X})$ is parameterized by $\\boldsymbol{\\psi}$.\nWe define a new set of r.v.s $\\mathbf{Y}=\\{Y_1,\\ldots,Y_n\\}$, where $\\text{Val}(Y_i)=\\text{Val}(X_i)\\cup\\{?\\}$. The actual observation is $\\mathbf{Y}$, which is a deterministic function of $\\mathbf{X}$ and $O_\\mathbf{X}$ \\begin{equation} Y_i=\\begin{cases}X_i\u0026\\hspace{1cm}O_{X_i}=o^1 \\\\ ?\u0026\\hspace{1cm}O_{X_i}=o^0\\end{cases} \\end{equation} The variables $Y_1,\\ldots,Y_n$ represents the values we actually observe, either an actual value or a ?, which denotes a missing value.\nExample 4: Consider the following observability model with $X\\sim\\text{Bern}(\\theta)$ and $O_X\\sim\\text{Bern}(\\psi)$.\nWe have that \\begin{align} \u0026P(X=1)=\\theta,\u0026\u0026\\hspace{1cm}P(x=0)=1-\\theta \\\\ \u0026P(O_X=o^1)=\\psi,\u0026\u0026\\hspace{1cm}P(O_X=o^0)=1-\\psi \\end{align} and thus \\begin{align} P(Y=1)\u0026=\\theta\\psi \\\\ P(Y=0)\u0026=(1-\\theta)\\psi \\\\ P(Y=?)\u0026=1-\\psi \\end{align} Thus if we see a dataset $\\mathcal{D}$ with $M[1],M[0]$ and $M[?]$ instances, then the likelihood is \\begin{align} L(\\theta,\\psi:\\mathcal{D})\u0026=(\\theta\\psi)^{M[1]}\\big((1-\\theta)\\psi\\big)^{M[0]}(1-\\psi)^{M[?]} \\\\ \u0026=\\theta^{M[1]}(1-\\theta)^{M[0]}\\psi^{M[1]+M[0]}(1-\\psi)^{M[?]} \\end{align} Differentiating the likelihood w.r.t $\\theta$ and $\\psi$ and setting the derivatives to zero we have that \\begin{align} \\hat{\\theta}_\\text{ML}\u0026=\\frac{M[1]}{M[1]+M[0]} \\\\ \\hat{\\psi}_\\text{ML}\u0026=\\frac{M[1]+M[0]}{M[1]+M[0]+M[?]} \\end{align}\nExample 5: Consider the following observability model with $X\\sim\\text{Bern}(\\theta)$, $(O_X\\vert X=1)\\sim\\text{Bern}(\\psi_{O_X\\vert x^1})$ and $(O_X\\vert X=0)\\sim\\text{Bern}(\\psi_{O_X\\vert x^0})$.\nIn this case, we have that \\begin{align} \u0026P(X=1)=\\theta,\u0026\u0026\\hspace{1cm}P(x=0)=1-\\theta \\\\ \u0026P(O_X=o^1\\vert X=1)=\\psi_{O_X\\vert x^1},\u0026\u0026\\hspace{1cm}P(O_X=o^0\\vert X=1)=1-\\psi_{O_X\\vert x^1} \\\\ \u0026P(O_X=o^1\\vert X=0)=\\psi_{O_X\\vert x^0},\u0026\u0026\\hspace{1cm}P(O_X=o^0\\vert X=0)=1-\\psi_{O_X\\vert x^0} \\end{align} and thus \\begin{align} P(Y=1)\u0026=\\theta\\psi_{O_X\\vert x^1} \\\\ P(Y=0)\u0026=(1-\\theta)\\psi_{O_X\\vert x^0} \\\\ P(Y=?)\u0026=\\theta(1-\\psi_{O_X\\vert x^1})+(1-\\theta)(1-\\psi_{O_X\\vert x^0}) \\end{align} Then, given the dataset $\\mathcal{D}$ with $M[1],M[0]$ and $M[?]$ examples, the likelihood function is given as \\begin{align} \u0026L(\\theta,\\psi_{O_X\\vert x^1},\\psi_{O_X\\vert x^0}:\\mathcal{D})\\nonumber \\\\ \u0026=(\\theta\\psi_{O_X\\vert x^1})^{M[1]}\\big[(1-\\theta)\\psi_{O_X\\vert x^0}\\big]^{M[0]}\\big[\\theta(1-\\psi_{O_X\\vert x^1})+(1-\\theta)(1-\\psi_{O_X\\vert x^0})\\big]^{M[?]} \\end{align}\nDecoupling of Observation Mechanism Missing Completely At Random A missing data model $P_\\text{missing}$ is missing completely at random (MCAR) if $P_\\text{missing}\\models(\\mathbf{X}\\perp O_\\mathbf{X})$.\nIn this case, the likelihood function of $X$ and $O_X$ decomposes as a product, and we can maximize each part separately. However, MCAR is sufficient but not necessary for the decomposition of the likelihood function.\nMissing At Random Let $\\mathbf{y}$ be a tuple of observations. These observations partition the variables $\\mathbf{X}$ into two sets\nthe observed variables $\\mathbf{X}_\\text{obs}^\\mathbf{y}=\\{X_i:y_i\\neq ?\\}$; the hidden variables $\\mathbf{X}_\\text{hidden}^\\mathbf{y}=\\{X_i:y_i=?\\}$. The observations $\\mathbf{y}$ determines the values of the observed variables, but not the hidden ones.\nA missing data model $P_\\text{missing}$ is missing at random (MAR) if for all observations $\\mathbf{y}$ with $P_\\text{missing}(\\mathbf{y})\u003e0$, and for all $\\mathbf{x}_\\text{hidden}^\\mathbf{y}\\in\\text{Val}(\\mathbf{X}_\\text{hidden}^\\mathbf{y})$, we have that \\begin{equation} P_\\text{missing}\\models(o_\\mathbf{X}\\perp\\mathbf{x}_\\text{hidden}^\\mathbf{y}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y}), \\end{equation} where $o_\\mathbf{X}$ are specific values of the observation variables given $\\mathbf{Y}$.\nThis implies that given the observed variables, the observation pattern does not give any additional information about the hidden variables \\begin{equation} P_\\text{missing}(\\mathbf{x}_\\text{hidden}^\\mathbf{y}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y},o_\\mathbf{X})=P_\\text{missing}(\\mathbf{x}_\\text{hidden}^\\mathbf{y}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y}) \\end{equation} Thus, with a missing model satisfying MAR, we have \\begin{align} P_\\text{missing}(\\mathbf{y})\u0026=P_\\text{missing}(o_\\mathbf{X},\\mathbf{x}_\\text{obs}^\\mathbf{y}) \\\\ \u0026=\\sum_{\\mathbf{x}_\\text{hidden}^\\mathbf{y}}\\Big[P(\\mathbf{x}_\\text{obs}^\\mathbf{y},\\mathbf{x}_\\text{hidden}^\\mathbf{y})P_\\text{missing}(o_\\mathbf{X}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y},\\mathbf{x}_\\text{hidden}^\\mathbf{y})\\Big] \\\\ \u0026=\\sum_{\\mathbf{x}_\\text{hidden}^\\mathbf{y}}\\Big[P(\\mathbf{x}_\\text{obs}^\\mathbf{y},\\mathbf{x}_\\text{hidden}^\\mathbf{y})P_\\text{missing}(o_\\mathbf{X}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y})\\Big] \\\\ \u0026=P_\\text{missing}(o_\\mathbf{X}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y})\\sum_{\\mathbf{x}_\\text{hidden}^\\mathbf{y}}P(\\mathbf{x}_\\text{obs}^\\mathbf{y},\\mathbf{x}_\\text{hidden}^\\mathbf{y}) \\\\ \u0026=P_\\text{missing}(o_\\mathbf{X}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y})P(\\mathbf{x}_\\text{obs}^\\mathbf{y}),\\label{eq:mar.1} \\end{align} where the first equality is justified due to the fact that $\\mathbf{y}$ are completely determined once we have the knowledge about $\\mathbf{x}_\\text{obs}^\\mathbf{y}$ and $o_\\mathbf{X}$.\nNotice that the first term in \\eqref{eq:mar.1}, $P_\\text{missing}(o_\\mathbf{X}\\vert\\mathbf{x}_\\text{obs}^\\mathbf{y})$, depends only on the parameters $\\boldsymbol{\\psi}$; while the second one, $P(\\mathbf{x}_\\text{obs}^\\mathbf{y})$, depends only the parameters $\\boldsymbol{\\theta}$. And since we have this product for every observed instance, we then have the following result.\nTheorem 3: If $P_\\text{missing}$ satisfies MAR, then $L(\\boldsymbol{\\theta},\\boldsymbol{\\psi}:\\mathcal{D})$ can be written as a product of two likelihood functions $L(\\boldsymbol{\\theta}:\\mathcal{D})$ and $L(\\boldsymbol{\\psi}:\\mathcal{D})$.\nThe Likelihood Function Given a Bayesian network structure $\\mathcal{G}$ over a set of variables $\\mathbf{X}$, and dataset $\\mathcal{D}$ of $M$ training instances, each of which has\na different set of observed variables, denoted $\\{\\mathbf{O}[m]:m=1,\\ldots,M\\}$ and their corresponding values $\\{\\mathbf{o}[m]:m=1,\\ldots,M\\}$; a different set of hidden (or latent) variables, denoted $\\{\\mathbf{H}[m]:m=1,\\ldots,M\\}$ The likelihood function, $L(\\boldsymbol{\\theta}:\\mathcal{D})$, is defined as the probability of the observed variables in the data, marginalizing the hidden variables, and ignoring the observability model: \\begin{equation} L(\\boldsymbol{\\theta}:\\mathcal{D})=\\prod_{m=1}^{M}P(\\mathbf{o}[m];\\boldsymbol{\\theta})\\label{eq:tlf.1} \\end{equation} And thus the log-likelihood is given as \\begin{equation} \\ell(\\boldsymbol{\\theta}:\\mathcal{D})=\\sum_{m=1}^{M}\\log P(\\mathbf{o}[m];\\boldsymbol{\\theta}) \\end{equation} This definition of likelihood function suggests that the problem of learning with partially observed data basically does not differ from the problem of learning with fully observed data. However, the computational complexity is bigger in this case and it requires the use of missing data. \\begin{equation} L(\\boldsymbol{\\theta}:\\mathcal{D})=\\prod_{m=1}^{M}P(\\mathbf{o}[m];\\boldsymbol{\\theta})=\\prod_{m=1}^{M}\\sum_{\\mathbf{h}[m]}P(\\mathbf{o}[m],\\mathbf{h}[m];\\boldsymbol{\\theta}) \\end{equation} And thus the log-likelihood can be computed by \\begin{equation} \\ell(\\boldsymbol{\\theta}:\\mathcal{D})=\\sum_{m=1}^{M}\\log\\sum_{\\mathbf{h}[m]}P(\\mathbf{o}[m],\\mathbf{h}[m];\\boldsymbol{\\theta})\\label{eq:tlf.2} \\end{equation}\nMLE Gradient Ascent Lemma 4: Let $\\mathcal{B}$ be a Bayesian network with graph $\\mathcal{G}$ over $\\mathcal{X}$ that induces a probability distribution $P$, let $\\mathbf{o}$ be a tuple of observations for some variables, and let $X\\in\\mathcal{X}$ be some r.v. If $P(x\\vert\\mathbf{u})\u003e0$, where $x\\in\\text{Val}(X)$, $\\mathbf{u}\\in\\text{Val}(\\text{Pa}_X)$, then we have \\begin{equation} \\frac{\\partial P(\\mathbf{o})}{\\partial P(x\\vert\\mathbf{u})}=\\frac{P(x,\\mathbf{u},\\mathbf{o})}{P(x\\vert\\mathbf{u})} \\end{equation}\nProof\nConsider the case of full assignment $\\xi$. We have \\begin{equation} P(\\xi)=\\prod_{X_i\\in\\mathcal{X}}P(\\xi\\langle X_i\\rangle\\vert\\xi\\langle\\text{Pa}_{X_i}\\rangle) \\end{equation} Thus, we have that \\begin{equation} \\hspace{-1cm}\\frac{\\partial P(\\xi)}{\\partial P(x\\vert\\mathbf{u})}=\\begin{cases}\\displaystyle\\prod_{X_i\\in\\mathcal{X},X_i\\neq X}P\\big(\\xi\\langle X_i\\rangle\\vert\\xi\\langle\\text{Pa}_{X_i}\\rangle\\big)=\\frac{P(\\xi)}{P(x\\vert\\mathbf{u})}\u0026\\hspace{1cm}\\text{if }\\xi\\langle X,\\text{Pa}_X\\rangle=\\langle x,\\mathbf{u}\\rangle \\\\ 0\u0026\\hspace{1cm}\\text{otherwise}\\end{cases}\\label{eq:ga.1} \\end{equation} Now consider the case of partial assignment $\\xi$. We have \\begin{equation} P(\\mathbf{o})=\\sum_{\\xi:\\xi\\langle\\mathbf{O}\\rangle=\\mathbf{o}}P(\\xi) \\end{equation} Using the result \\eqref{eq:ga.1}, we then have that \\begin{align} \\frac{\\partial P(\\mathbf{o})}{\\partial P(x\\vert\\mathbf{u})}\u0026=\\sum_{\\xi:\\xi\\langle\\mathbf{O}\\rangle=\\mathbf{o}}\\frac{\\partial P(\\xi)}{\\partial P(x\\vert\\mathbf{u})} \\\\ \u0026=\\sum_{\\xi:\\xi\\langle\\mathbf{O}\\rangle=\\mathbf{o},\\xi\\langle X,\\text{Pa}_X\\rangle=\\langle x,\\mathbf{u}\\rangle}\\frac{P(\\xi)}{P(x\\vert\\mathbf{u})} \\\\ \u0026=\\frac{P(x,\\mathbf{u},\\mathbf{o})}{P(x\\vert\\mathbf{u})} \\end{align}\nGiven this result, we immediately obtain the form of the gradient of the likelihood function for table-CPDs.\nTheorem 5: Let $\\mathcal{G}$ be a Bayesian network graph over $\\mathcal{X}$, and let $\\mathcal{D}=\\{\\mathbf{o}[1],\\ldots,\\mathbf{o}[M]\\}$ be a partially observable dataset. Let $X$ be a variable with parents $\\mathbf{U}$ in $\\mathcal{G}$. If $P(x\\vert\\mathbf{u})\u003e0$, where $x\\in\\text{Val}(X)$, $\\mathbf{u}\\in\\text{Val}(\\mathbf{U})$, then we have \\begin{equation} \\frac{\\partial\\ell(\\boldsymbol{\\theta}:\\mathcal{D})}{\\partial P(x\\vert\\mathbf{u})}=\\frac{1}{P(x\\vert\\mathbf{u})}\\sum_{m=1}^{M}P(x,\\mathbf{u}\\vert\\mathbf{o}[m];\\boldsymbol{\\theta}) \\end{equation}\nProof\nThe gradient of the log-likelihood can be written as \\begin{align} \\frac{\\partial\\ell(\\boldsymbol{\\theta}:\\mathcal{D})}{\\partial P(x\\vert\\mathbf{u})}\u0026=\\sum_{m=1}^{M}\\frac{\\partial}{\\partial P(x\\vert\\mathbf{u})}\\log P(\\mathbf{o}[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\sum_{m=1}^{M}\\frac{1}{P(\\mathbf{o}[m];\\boldsymbol{\\theta})}\\frac{\\partial P(\\mathbf{o}[m];\\boldsymbol{\\theta})}{\\partial P(x\\vert\\mathbf{u})} \\\\ \u0026=\\sum_{m=1}^{M}\\frac{1}{P(\\mathbf{o}[m];\\boldsymbol{\\theta})}\\frac{P(x,\\mathbf{u},\\mathbf{o}[m];\\boldsymbol{\\theta})}{P(x\\vert\\mathbf{u})} \\\\ \u0026=\\frac{1}{P(x\\vert\\mathbf{u})}\\sum_{m=1}^{M}P(x,\\mathbf{u}\\vert\\mathbf{o}[m];\\boldsymbol{\\theta}) \\end{align} where the third equality is obtained by applying Lemma 4.\nUsing this result, suppose that the CPDs entries of $P(X\\vert\\mathbf{U})$ are written as functions of some set of parameters $\\boldsymbol{\\theta}$. Then for a specific parameter $\\theta\\in\\boldsymbol{\\theta}$, we have \\begin{equation} \\frac{\\partial\\ell(\\boldsymbol{\\theta}:\\mathcal{D})}{\\partial\\theta}=\\sum_{x,\\mathbf{u}}\\frac{\\partial\\ell(\\boldsymbol{\\theta}:\\mathcal{D})}{\\partial P(x\\vert\\mathbf{u})}\\frac{\\partial P(x\\vert\\mathbf{u})}{\\partial\\theta} \\end{equation}\nExpectation Maximization [TODO] Consider the following model, which is described by the following meta-network, where $X$ is fully observed, while $Z$ is partially observed.\nThe log-likelihood, as described in \\eqref{eq:tlf.2}, is then given as \\begin{align} \\ell(\\boldsymbol{\\theta}:\\mathcal{D})\u0026=\\sum_{m=1}^{M}\\log P(x[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\sum_{m=1}^{M}\\log\\sum_{z[m]}P(x[m],z[m];\\boldsymbol{\\theta}) \\end{align} [TODO] For each $m=1,\\ldots,M$, let $Q_m$ be some distribution over $Z$, i.e. $\\sum_z Q_m(z)=1$ and $Q_m(z)\\geq 0$1. We have \\begin{align} \\hspace{-0.5cm}\\sum_{m=1}^{M}\\log P(x[m];\\boldsymbol{\\theta})\u0026=\\sum_{m=1}^{M}\\log\\sum_{z[m]\\in\\text{Val}(Z)}P(x[m],z[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\sum_{m=1}^{M}\\log\\sum_{z[m]\\in\\text{Val}(Z)}Q_m(z[m])\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])} \\\\ \u0026\\geq\\sum_{m=1}^{M}\\sum_{z[m]\\in\\text{Val}(Z)}Q_m(z[m])\\log\\left(\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right),\\label{eq:em.1} \\end{align} where in the third step, we have used Jensen’s inequality. Specifically, since $\\log(\\cdot)$ is a strictly concave function, due to $(\\log(x))’’=-1/x^2\u003c0$, then by Jensen’s inequality, we have \\begin{align} \\log\\left(\\sum_{z[m]}Q_m(z[m])\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right)\u0026=\\log\\left(\\mathbb{E}_{z[m]\\sim Q_m}\\left[\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right]\\right) \\\\ \u0026\\geq\\mathbb{E}_{z[m]\\sim Q_m}\\left[\\log\\left(\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right)\\right] \\\\ \u0026=\\sum_{z[m]}Q_m(z[m])\\log\\left(\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right), \\end{align} where equality holds where \\begin{equation} \\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}=\\mathbb{E}_{z[m]\\sim Q_m}\\left[\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}\\right], \\end{equation} with probability $1$. This implies that \\begin{equation} \\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])}=c, \\end{equation} where $c$ is a constant w.r.t $z[m]$. Moreover, since $\\sum_z Q_m(z)=1$, we then have \\begin{equation} Q_m(z[m])\\propto P(x[m],z[m];\\boldsymbol{\\theta}) \\end{equation} And since $Q_m$ is a distribution, we further have that \\begin{align} Q_m(z[m])\u0026=\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{\\sum_z P(x[m],z;\\boldsymbol{\\theta})} \\\\ \u0026=\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{P(x[m];\\boldsymbol{\\theta})} \\\\ \u0026=P(z[m]\\vert x[m];\\boldsymbol{\\theta})\\label{eq:em.3} \\end{align}\nHence, the RHS of \\eqref{eq:em.1} then gives us a lower bound of the log-likelihood function and that $\\ell(\\boldsymbol{\\theta}:\\mathcal{D})$ reaches this value if our selection of $Q_m$ satisfies the equation \\eqref{eq:em.3}, i.e. $Q_m(Z)=P(Z\\vert x[m];\\boldsymbol{\\theta})$.\nOur goal is to find parameters $\\boldsymbol{\\theta}$ that maximizes the log-likelihood $\\ell(\\boldsymbol{\\theta}:\\mathcal{D})$\nRepeat until convergence:\n$\\hspace{1cm}$(E-step) For each $m=1,\\ldots,M$, set: \\begin{equation*} Q_m(Z):=P(Z\\vert x[m];\\boldsymbol{\\theta}) \\end{equation*} $\\hspace{1cm}$(M-step) Set: \\begin{equation*} \\boldsymbol{\\theta}:=\\underset{\\boldsymbol{\\theta}}{\\text{argmax}}\\sum_{m=1}^{M}\\sum_{z[m]\\in\\text{Val}(Z)}Q_m(z[m])\\log\\frac{P(x[m],z[m];\\boldsymbol{\\theta})}{Q_m(z[m])} \\end{equation*}\nEM for Bayesian networks Let us consider the EM algorithm for a general Bayesian network structure $\\mathcal{G}$ with table-CPDs over random variables $X_1,\\ldots,X_n$. Suppose that we are given a dataset $\\mathcal{D}=\\{\\xi[1],\\ldots,x[M]\\}$, where for each instance $\\xi[m]$, $\\mathbf{O}[m]$ and $\\mathbf{o}[m]$ denote the set of observed variables and their values, while $\\mathbf{H}[m]$ represents the set of hidden variables. The log-likelihood function is given as \\begin{align} \\ell(\\boldsymbol{\\theta}:\\mathcal{D})\u0026=\\sum_{m=1}^{M}\\log P(\\mathbf{o}[m];\\boldsymbol{\\theta}) \\\\ \u0026=\\sum_{m=1}^{M}\\log\\sum_{\\mathbf{h}\\in\\text{Val}(\\mathbf{H}[m])}P(\\mathbf{o}[m],\\mathbf{h};\\boldsymbol{\\theta}) \\\\ \u0026\\geq\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\text{Val}(\\mathbf{H}[m])}Q_m(\\mathbf{h})\\log\\frac{P(\\mathbf{o}[m],\\mathbf{h};\\boldsymbol{\\theta})}{Q_m(\\mathbf{h})} \\\\ \u0026=\\left(\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\text{Val}(\\mathbf{H}[m])}Q_m(\\mathbf{h})\\big[\\log P(\\mathbf{o}[m],\\mathbf{h};\\boldsymbol{\\theta})-\\log Q_m(\\mathbf{h})\\big]\\right) \\\\ \u0026=\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\text{Val}(\\mathbf{H}[m])}Q_m(\\mathbf{h})\\log\\prod_{i=1}^{n}P\\Big(\\xi[m]\\langle X_i\\rangle\\Big\\vert\\xi[m]\\langle\\text{Pa}_{X_i}^\\mathcal{G}\\rangle;\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G}}\\Big)\\nonumber \\\\ \u0026\\hspace{1cm}+\\sum_{m=1}^{M}H_{Q_m}(Q_m(\\mathbf{H}[m])) \\\\ \u0026=\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\text{Val}(\\mathbf{H}[m])}Q_m(\\mathbf{h})\\sum_{i=1}^{n}\\log P\\Big(\\xi[m]\\langle X_i\\rangle\\Big\\vert\\xi[m]\\langle\\text{Pa}_{X_i}^\\mathcal{G}\\rangle;\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G}}\\Big)\\nonumber \\\\ \u0026\\hspace{1cm}+\\sum_{m=1}^{M}H_{Q_m}(Q_m(\\mathbf{H}[m])) \\\\ \u0026=\\sum_{i=1}^{n}\\left(\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\mathbf{H}[m]}Q_m(\\mathbf{h})\\log P\\Big(\\xi[m]\\langle X_i\\rangle\\Big\\vert\\xi[m]\\langle\\text{Pa}_{X_i}^\\mathcal{G}\\rangle;\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G}}\\Big)\\right)\\nonumber \\\\ \u0026\\hspace{1cm}+\\sum_{m=1}^{M}H_{Q_m}(Q_m(\\mathbf{H}[m])) \\\\ \u0026\\triangleq\\tilde{\\ell}(\\boldsymbol{\\theta}) \\end{align} This gives us a lower bound for the log-likelihood function $\\ell(\\boldsymbol{\\theta}:\\mathcal{D})$. And similarly, in the E-step of EM algorithm, for each $m=1,\\ldots,M$, we set \\begin{equation} Q_m(\\mathbf{H})=P(\\mathbf{H}\\vert\\mathbf{o}[m];\\boldsymbol{\\theta}) \\end{equation} In the M-step of EM, we find a parameter $\\boldsymbol{\\theta}$ that maximizes $\\tilde{\\ell}(\\boldsymbol{\\theta})$. Before performing this step, we notice that the function $\\tilde{\\ell}(\\boldsymbol{\\theta})$ can be decomposed into sum of function of local CPDs $P(X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G};\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G}})$, which parameterized by $\\boldsymbol{\\theta}_{X_i\\vert\\text{Pa}_{X_i}^\\mathcal{G}}$. Hence, as usual, we can independently solve for the local problems and then combine the solutions together.\nIn particular, during M-step, for each variable $X$, and for each $U\\in\\text{Pa}_X^\\mathcal{G}$, we try to solve the optimization problem \\begin{align} \\underset{\\boldsymbol{\\theta}_{X\\vert U}}{\\text{maximize}}\u0026\\sum_{m=1}^{M}\\sum_{\\mathbf{h}\\in\\mathbf{H}[m]}Q_m(\\mathbf{h})\\log P\\big(\\xi[m]\\langle X\\rangle\\big\\vert\\xi[m]\\langle U\\rangle;\\boldsymbol{\\theta}_{X\\vert u}) \\\\ \\text{s.t.}\u0026\\sum_{x\\in\\text{Val}(X)}\\theta_{x\\vert u}=1,\\hspace{1cm}\\forall u\\in\\text{Val}(U) \\end{align} The Lagrangian of this problem is \\begin{align} \u0026\\mathcal{L}(\\boldsymbol{\\theta}_{X\\vert U},\\lambda)\\nonumber \\\\ \u0026=-\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\log P\\big(\\xi[m]\\langle X\\rangle\\big\\vert\\xi[m]\\langle U\\rangle;\\boldsymbol{\\theta}_{X\\vert u})+\\sum_{u\\in\\text{Val}(U)}\\lambda_u\\left(\\sum_x\\theta_{x\\vert u}-1\\right) \\\\ \u0026=-\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\log\\prod_{(x,u)\\in\\text{Val}(X,U)}\\theta_{x\\vert u}^{\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}}+\\sum_u\\lambda_u\\left(\\sum_x\\theta_{x\\vert u}-1\\right) \\\\ \u0026=-\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\sum_{x,u}\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}\\log\\theta_{x\\vert u}+\\sum_u\\lambda_u\\left(\\sum_x\\theta_{x\\vert u}-1\\right) \\end{align} Differentiating w.r.t $\\theta_{x\\vert u}$ yields \\begin{equation} \\nabla_{\\theta_{x\\vert u}}\\mathcal{L}(\\boldsymbol{\\theta}_{X\\vert U},\\lambda)=-\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\frac{\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}}{\\theta_{x\\vert u}}+\\lambda_u \\end{equation} Setting this derivative to zero, we obtain \\begin{equation} \\theta_{x\\vert u}=\\frac{1}{\\lambda_u}\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}\\triangleq\\frac{f(x,u)}{\\lambda_u} \\end{equation} The dual function is then given as \\begin{align} g(\\lambda)\u0026=\\sup_{\\boldsymbol{\\theta}_{X\\vert U}}\\mathcal{L}(\\boldsymbol{\\theta}_{X\\vert U},\\lambda) \\\\ \u0026=-\\sum_{x,u}\\log\\left(\\frac{f(x,u)}{\\lambda_u}\\right)f(x,u)+\\sum_u\\lambda_u\\left(\\sum_x\\frac{f(x,u)}{\\lambda_u}-1\\right) \\\\ \u0026=\\sum_{x,u}\\big(\\log\\lambda_u-\\log f(x,u)\\big)f(x,u)+\\sum_{x,u}f(x,u)-\\sum_u\\lambda_u \\end{align} Taking the derivative w.r.t $\\lambda_u$ lets us obtain \\begin{align} \\frac{\\partial g(\\lambda)}{\\partial\\lambda_u}\u0026=\\sum_x\\frac{f(x,u)}{\\lambda_u}-1, \\end{align} which gives us the value of $\\lambda_u$ if being set to zero \\begin{equation} \\lambda_u=\\sum_x f(x,u) \\end{equation} Hence, we have the solution for $\\theta_{x\\vert u}$: \\begin{align} \\theta_{x\\vert u}\u0026=\\frac{f(x,u)}{\\sum_x f(x,u)} \\\\ \u0026=\\frac{\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}}{\\sum_x\\sum_m\\sum_\\mathbf{h}Q_m(\\mathbf{h})\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}} \\\\ \u0026=\\frac{\\sum_m\\sum_\\mathbf{h}P(\\mathbf{h}\\vert\\mathbf{o}[m];\\boldsymbol{\\theta})\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}}{\\sum_x\\sum_m\\sum_\\mathbf{h}P(\\mathbf{h}\\vert\\mathbf{o}[m];\\boldsymbol{\\theta})\\mathbf{1}\\{\\xi[m]\\langle X,U\\rangle=\\langle x,u\\rangle\\}} \\\\ \u0026=\\frac{\\sum_m P(x,u\\vert\\mathbf{o}[m];\\boldsymbol{\\theta})}{\\sum_x\\sum_m P(x,u\\vert\\mathbf{o}[m];\\boldsymbol{\\theta})} \\end{align} We end up with this pseudocode for EM in Bayesian network with table-CPDs.\nBayesian Learning Structure Learning References [1] Daphne Koller, Nir Friedman. Probabilistic Graphical Models. The MIT Press.\n[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer New York, NY, 2006.\n[3] Stanford CS229. Machine Learning.\nFootnotes Without loss of generality, we have implicitly assumed that $Z$ is discrete. The same arguments can be extended to the continuous case by using integrations instead of summations. ↩︎\n","wordCount":"3364","inLanguage":"en","datePublished":"2023-02-19T17:23:56+07:00","dateModified":"2023-02-19T17:23:56+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/machine-learning/pgm-learning/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Read-through: Probabilistic Graphical Models - Learning</h1><div class=post-meta><span title='2023-02-19 17:23:56 +0700 +07'>February 19, 2023</span>&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#parameter-learning>Parameter Learning</a><ul><li><a href=#fully-observed-data>Fully Observed Data</a><ul><li><a href=#mle-bn>MLE for Bayesian Networks</a><ul><li><a href=#global-likelihood-decomposition>Global Likelihood Decomposition</a></li><li><a href=#table-cpds>Table-CPDs</a></li><li><a href=#gaussian-bn>Gaussian Bayesian Networks</a></li></ul></li><li><a href=#bayesian-parameter-estimation>Bayesian Parameter Estimation</a><ul><li><a href=#general-setting>General setting</a><ul><li><a href=#priors-posteriors>Priors, Posteriors</a></li><li><a href=#bayesian-estimator>Bayesian Estimator</a></li></ul></li><li><a href=#bayesian-parameter-estimation-in-bayesian-networks>Bayesian Parameter Estimation in Bayesian Networks</a><ul><li><a href=#parameter-independence-and-global-decomposition>Parameter Independence and Global Decomposition</a></li><li><a href=#local-decomposition>Local Decomposition</a></li><li><a href=#map-estimation>MAP Estimation</a></li></ul></li></ul></li></ul></li><li><a href=#partially-observed-data>Partially Observed Data</a><ul><li><a href=#likelihood-of-data-and-observation-models>Likelihood of Data and Observation Models</a></li><li><a href=#decoupling-of-observation-mechanism>Decoupling of Observation Mechanism</a><ul><li><a href=#mcar>Missing Completely At Random</a></li><li><a href=#mar>Missing At Random</a></li></ul></li><li><a href=#the-likelihood-function>The Likelihood Function</a></li><li><a href=#mle>MLE</a><ul><li><a href=#gradient-ascent>Gradient Ascent</a></li><li><a href=#em>Expectation Maximization</a><ul><li><a href=#em-for-bayesian-networks>EM for Bayesian networks</a></li></ul></li></ul></li><li><a href=#bayesian-learning>Bayesian Learning</a></li></ul></li></ul></li><li><a href=#structure-learning>Structure Learning</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Learning in PGMs.</p></blockquote><h2 id=parameter-learning>Parameter Learning<a hidden class=anchor aria-hidden=true href=#parameter-learning>#</a></h2><h3 id=fully-observed-data>Fully Observed Data<a hidden class=anchor aria-hidden=true href=#fully-observed-data>#</a></h3><h4 id=mle-bn>MLE for Bayesian Networks<a hidden class=anchor aria-hidden=true href=#mle-bn>#</a></h4><p>Suppose that we have a Bayesian network of two binary nodes $X,Y$ connected by $X\to Y$.</p><figure><img width=27% height=27% src=/images/pgm-learning/mle-bn.png alt=BN></figure><p>The network is parameterized by a parameter vector $\boldsymbol{\theta}$, which defines the set of parameters of all the CPDs in the network, i.e.
\begin{equation}
\boldsymbol{\theta}_X=\{\theta_{x^0},\theta_{x^1}\}
\end{equation}
and
\begin{equation}
\boldsymbol{\theta}_{Y\vert X}=\boldsymbol{\theta}_{Y\vert x_0}\cup\boldsymbol{\theta}_{Y\vert x^1}=\{\theta_{y^0\vert x^0},\theta_{y^1\vert x^0}\}\cup\{\theta_{y^0\vert x^1},\theta_{y^1\vert x^1}\}
\end{equation}
Assuming that we are given the training set
\begin{equation}
\mathcal{D}=\{(x[1],y[1]),\ldots,(x[M],y[M])\}
\end{equation}
which describes $M$ instances of variables $X$ and $Y$. The likelihood function is then given as
\begin{align}
L(\boldsymbol{\theta}:\mathcal{D})&=\prod_{m=1}^{M}P(x[m],y[m];\boldsymbol{\theta}) \\ &=\prod_{m=1}^{M}P(x[m];\boldsymbol{\theta})P(y[m]\big\vert x[m];\boldsymbol{\theta}) \\ &=\left(\prod_{m=1}^{M}P(x[m];\boldsymbol{\theta})\right)\left(\prod_{m=1}^{M}P(y[m]\big\vert x[m];\boldsymbol{\theta})\right),
\end{align}
which decomposes into two terms, on for each variable. Each of these are referred as <strong>local likelihood function</strong> that measures how well the variable is predicted given its parents.</p><h5 id=global-likelihood-decomposition>Global Likelihood Decomposition<a hidden class=anchor aria-hidden=true href=#global-likelihood-decomposition>#</a></h5><p>Generally, suppose that we want to learn a parameters $\boldsymbol{\theta}$ for Bayesian network structure $\mathcal{G}$. Given a dataset $\mathcal{D}=\{\xi[1],\ldots,\xi[M]\}$, analogy to the argument above, we have that the likelihood function is given by
\begin{align}
L(\boldsymbol{\theta}:\mathcal{D})&=\prod_{m=1}^{M}P_\mathcal{G}(\xi[m];\boldsymbol{\theta}) \\ &=\prod_{m=1}^{M}\prod_i P\big(x_i[m]\big\vert\text{pa}_{X_i}[m];\boldsymbol{\theta}\big) \\ &=\prod_i\left[\prod_{m=1}^{M}P\big(x_i[m]\big\vert\text{pa}_{X_i}[m];\boldsymbol{\theta}\big)\right]\label{eq:gld.1}
\end{align}
Each of the terms in the square brackets refers to the <strong>conditional likelihood</strong> of a particular variable given its parents in the network. Also, let $\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}$ denote the subset of parameters that determines $P(X_i\vert\text{Pa}_{X_i})$. Thus, the local likelihood function for $X_i$ is then given by
\begin{equation}
L_i(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}:\mathcal{D})=\prod_{m=1}^{M}P\big(x_i[m]\big\vert\text{pa}_{X_i}[m];\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}\big),
\end{equation}
which allows us to rewrite the likelihood function \eqref{eq:gld.1} as
\begin{equation}
L(\boldsymbol{\theta}:\mathcal{D})=\prod_i L_i(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}:\mathcal{D})
\end{equation}
In other words, when $\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}$ are disjoint, the likelihood can be decomposed as a product of independent terms, one for each CPD of the network. This property is known as the <strong>global decomposition</strong> of the likelihood function.</p><p>Additionally, we can maximize each local likelihood function $L_i(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}:\mathcal{D})$ independently of the others, and then combine the solutions together to get an MLE solution.</p><h5 id=table-cpds>Table-CPDs<a hidden class=anchor aria-hidden=true href=#table-cpds>#</a></h5><p>As the MLE solution for a Bayesian network can be computed via parameterization of its CPDs, we now consider the simplest parameterization of the CPD, tabular CPD, or table-CPD.</p><p>Suppose we have a variable $X$ with parents $\mathbf{U}$. If we represent the CPD $P(X\vert\mathbf{U})$ as a table, we then have a parameter $\theta_{x\vert\mathbf{u}}$ for each $x\in\text{Val}(X)$ and $\mathbf{u}\in\text{Val}(\mathbf{U})$. The local likelihood function is then can be decomposed further as
\begin{align}
L_X(\boldsymbol{\theta}_{X\vert\mathbf{U}})&=\prod_{m=1}^{M}\theta_{x[m]\vert\mathbf{u}[m]} \\ &=\prod_{\mathbf{u}\in\text{Val}(\mathbf{U})}\left(\prod_{x\in\text{Val}(X)}\theta_{x\vert\mathbf{u}}^{M[\mathbf{u},x]}\right),
\end{align}
where $M[\mathbf{u},x]$ is the number of times $x[m]=x$ and $\mathbf{u}[m]=\mathbf{u}$ in $\mathcal{D}$.</p><h5 id=gaussian-bn>Gaussian Bayesian Networks<a hidden class=anchor aria-hidden=true href=#gaussian-bn>#</a></h5><p>Consider a variable $X$ with parents $\mathbf{U}=\{U_1,\ldots,U_k\}$ with a <a href=https://trunghng.github.io/posts/machine-learning/pgm-representation/#linear-gaussian-model>linear Gaussian CPD</a>
\begin{equation}
P(X\vert\mathbf{u})=\mathcal{N}(\beta_0+\beta_1 u_1+\ldots+\beta_k u_k;\sigma^2)
\end{equation}
Thus, we have that
\begin{equation}
P(x\vert\mathbf{u})=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(\beta_0+\beta_1 u_1+\ldots+\beta_k u_k-x)^2}{2\sigma^2}\right]
\end{equation}
Our task is to learn the parameters $\boldsymbol{\theta}_{X\vert\mathbf{U}}=(\beta_0,\ldots,\beta_k,\sigma)$. We continue by considering the log-likelihood
\begin{align}
\ell_X(\boldsymbol{\theta}_{X\vert\mathbf{U}}:\mathcal{D})&=\log L_X(\boldsymbol{\theta}_{X\vert\mathbf{U}}) \\ &=\log\prod_{m=1}^{M}P\big(x[m]\big\vert\mathbf{u}[m];\boldsymbol{\theta}_{X\vert\mathbf{U}}\big) \\ &=\sum_{m=1}^{M}\log P\big(x[m]\big\vert\mathbf{u}[m];\boldsymbol{\theta}_{X\vert\mathbf{U}}\big) \\ &=\sum_{m=1}^{M}\left[\frac{1}{2}\log(2\pi\sigma^2)-\frac{1}{2}\frac{1}{\sigma^2}\big(\beta_0+\beta_1 u_1[m]+\ldots+\beta_k u_k[m]-x[m]\big)^2\right]
\end{align}
Taking the derivative of the log-likelihood w.r.t $\beta_0$ gives us
\begin{align}
\frac{\partial}{\partial\beta_0}\ell_X(\boldsymbol{\theta}_{X\vert\mathbf{U}}:\mathcal{D})&=\sum_{m=1}^{M}-\frac{1}{\sigma^2}\big(\beta_0+\beta_1 u_1[m]+\ldots+\beta_k u_k[m]-x[m]\big) \\ &=-\frac{1}{\sigma^2}\left(M\beta_0+\beta_1\sum_{m=1}^{M}u_1[m]+\ldots+\beta_k\sum_{m=1}^{M}u_k[m]-\sum_{m=1}^{M}x[m]\right)
\end{align}
Setting the derivative to zero, we have that
\begin{equation}
\frac{1}{M}\sum_{m=1}^{M}x[m]=\beta_0+\beta_1\frac{1}{M}\sum_{m=1}^{M}u_1[m]+\ldots+\beta_k\frac{1}{M}\sum_{m=1}^{M}u_k[m]\label{eq:gbn.1}
\end{equation}
Then, if we define
\begin{equation}
\mathbb{E}_\mathcal{D}[X]\doteq\frac{1}{M}\sum_{m=1}^{M}x[m],
\end{equation}
which represents the average value a variable $X$. Thus, we can rewrite \eqref{eq:gbn.1} as
\begin{equation}
\mathbb{E}_\mathcal{D}[X]=\beta_0+\beta_1\mathbb{E}_\mathcal{D}[U_1]+\ldots+\beta_k\mathbb{E}_\mathcal{D}[U_k]\label{eq:gbn.2}
\end{equation}
On the other hand, differentiating the log-likelihood function w.r.t $\beta_i$ for $i\neq 0$ gives us
\begin{align}
&\hspace{-0.5cm}\frac{\partial}{\partial\beta_i}\ell_X(\boldsymbol{\theta}_{X\vert\mathbf{U}}:\mathcal{D})\nonumber \\ &\hspace{-0.5cm}=\sum_{m=1}^{M}-\frac{u_i[m]}{\sigma^2}\big(\beta_0+\beta_1 u_1[m]+\ldots+\beta_k u_k[m]-x[m]\big) \\ &\hspace{-0.5cm}=-\frac{1}{\sigma^2}\left(M\beta_0+\beta_1\sum_{m=1}^{M}u_1[m]u_i[m]+\ldots+\beta_k\sum_{m=1}^{M}u_k[m]u_i[m]-\sum_{m=1}^{M}x[m]u_i[m]\right)
\end{align}
Similarly, setting this derivative to zero lets us obtain
\begin{equation}
\mathbb{E}_\mathcal{D}[X U_i]=\beta_0\mathbb{E}_\mathcal{D}[U_i]+\beta_1\mathbb{E}_\mathcal{D}[U_1 U_i]+\ldots+\beta_k\mathbb{E}_\mathcal{D}[U_k U_i]\label{eq:gbn.3}
\end{equation}
From the results \eqref{eq:gbn.2} and \eqref{eq:gbn.3}, we can find the MLE solution $\hat{\boldsymbol{\beta}}$ by solving the system of linear equations
\begin{equation}
\left[\begin{matrix}1&\mathbb{E}_\mathcal{D}[U_1]&\ldots&\mathbb{E}_\mathcal{D}[U_k] \\ \mathbb{E}_\mathcal{D}[U_1]&\mathbb{E}_\mathcal{D}[U_1 U_1]&\ldots&\mathbb{E}_\mathcal{D}[U_1 U_k] \\ \vdots&\vdots&\ddots&\vdots \\ \mathbb{E}_\mathcal{D}[U_k]&\mathbb{E}_\mathcal{D}[U_k U_1]&\ldots&\mathbb{E}_\mathcal{D}[U_k U_k]\end{matrix}\right]\left[\begin{matrix}\beta_0 \\ \beta_1 \\ \vdots \\ \beta_k\end{matrix}\right]=\left[\begin{matrix}\mathbb{E}_\mathcal{D}[X] \\ \mathbb{E}_\mathcal{D}[X U_1] \\ \vdots \\ \mathbb{E}_\mathcal{D}[X U_k]\end{matrix}\right]
\end{equation}
Additionally, by multiplying both sides of \eqref{eq:gbn.2} with $\mathbb{E}_\mathcal{D}[U_i]$, we have
\begin{equation}
\mathbb{E}_\mathcal{D}[X]\mathbb{E}_\mathcal{D}[U_i]=\beta_0\mathbb{E}_\mathcal{D}[U_i]+\beta_1\mathbb{E}_\mathcal{D}[U_1]\mathbb{E}_\mathcal{D}[U_i]+\ldots+\beta_k\mathbb{E}_\mathcal{D}[U_k]\mathbb{E}_\mathcal{D}[U_i]
\end{equation}
Then, subtracting this equation from \eqref{eq:gbn.3} gives us
\begin{align}
\mathbb{E}_\mathcal{D}[X U_i]-\mathbb{E}_\mathcal{D}[X]\mathbb{E}_\mathcal{D}[U_i]&=\beta_1\big(\mathbb{E}_\mathcal{D}[U_1 U_i]-\mathbb{E}_\mathcal{D}[U_1]\mathbb{E}_\mathcal{D}[U_i]\big)+\ldots+\nonumber \\ &\hspace{0.6cm}\beta_k\big(\mathbb{E}_\mathcal{D}[U_k U_i]-\mathbb{E}_\mathcal{D}[U_k]\mathbb{E}_\mathcal{D}[U_i]\big),
\end{align}
or
\begin{equation}
\text{Cov}_\mathcal{D}[X,U_i]=\beta_1\text{Cov}_\mathcal{D}[U_1,U_i]+\ldots+\beta_k\text{Cov}_\mathcal{D}[U_k,U_i]\label{eq:gbn.4}
\end{equation}
where we have defined $\text{Cov}_\mathcal{D}[X,U_i]$ as the observed covariance of $X$ and $U_i$ in the data.</p><p>Finally, differentiating the log-likelihood w.r.t $\sigma^2$, we have that
\begin{equation}
\frac{\partial}{\partial\sigma}\ell_X(\boldsymbol{\theta}_{X\vert\mathbf{U}}:\mathcal{D})=\sum_{m=1}^{M}\left[\frac{1}{2}\frac{1}{\sigma^2}+\frac{1}{2}\frac{1}{(\sigma^2)^2}\big(\beta_0+\beta_1 u_1[m]+\ldots+\beta_k u_k[m]-x[m]\big)^2\right]
\end{equation}
Analogously, setting this derivative to zero, we have that
\begin{equation}
\sigma^2=\text{Cov}_\mathcal{D}[X,X]-\sum_{i=1}^{k}\sum_{j=1}^{k}\beta_i\beta_j\text{Cov}_\mathcal{D}[U_i,U_j]\label{eq:gbn.5}
\end{equation}</p><p><strong>Remark</strong></p><p><strong>Example 1</strong>: Let us estimate a joint multivariate Gaussian distribution. Specifically, consider continuous r.v.s $X$ and $Y$ and assume we have a dataset of $M$ samples $\mathcal{D}=\{(x[1],y[1]),\ldots,(x[M],y[M])\}$. Our job is to find the MLE estimate for a joint Gaussian distribution over $X,Y$.</p><p>Let $\mathbf{Z}$ be a random vector that encodes the joint distribution of $X$ and $Y$. In particular
\begin{equation}
\mathbf{Z}=\left[\begin{matrix}X \\ Y\end{matrix}\right]
\end{equation}
We have that $\mathbf{Z}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$ with
\begin{equation}
\boldsymbol{\mu}=\left[\begin{matrix}\mu_X \\ \mu_Y\end{matrix}\right],\hspace{1cm}\boldsymbol{\Sigma}=\left[\begin{matrix}\Sigma_{XX}&\Sigma_{XY} \\ \Sigma_{YX}&\Sigma_{YY}\end{matrix}\right]
\end{equation}
Thus, we have that
\begin{equation}
P(\mathbf{z})=\frac{1}{2\pi\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{z}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{z}-\boldsymbol{\mu})\right]
\end{equation}
Our job then is to learn the parameter $\boldsymbol{\theta}=(\boldsymbol{\mu},\boldsymbol{\Sigma})$. We begin by considering the log-likelihood function
\begin{align}
\ell(\boldsymbol{\theta})&=\log\prod_{m=1}^{M}P(x[m],y[m];\boldsymbol{\theta}) \\ &=\log\prod_{m=1}^{M}\frac{1}{2\pi\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{z}[m]-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{z}[m]-\boldsymbol{\mu})\right] \\ &=\sum_{m=1}^{M}-\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}(\mathbf{z}[m]-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{z}[m]-\boldsymbol{\mu})
\end{align}
Taking the derivative of the log-likelihood w.r.t $\boldsymbol{\mu}$ gives us
\begin{align}
\frac{\partial}{\partial\boldsymbol{\mu}}\ell(\boldsymbol{\theta})&=\sum_{m=1}^{M}\frac{1}{2}\big[\boldsymbol{\Sigma}^{-1}(\mathbf{z}[m]-\boldsymbol{\mu})-(\boldsymbol{\Sigma}^{-1})^\text{T}(\mathbf{z}[m]-\boldsymbol{\mu})\big] \\ &=\sum_{m=1}^{M}\boldsymbol{\Sigma}^{-1}(\mathbf{z}[m]-\boldsymbol{\mu}),
\end{align}
where we have used the fact that the covariance matrix $\boldsymbol{\Sigma}$ is symmetric. Setting the derivative to zero we obtain the MLE solution for $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}=\frac{1}{M}\sum_{m=1}^{M}\mathbf{z}[m]=\left[\begin{matrix}\mathbb{E}_\mathcal{D}[X] \\ \mathbb{E}_\mathcal{D}[Y]\end{matrix}\right]
\end{equation}
On the other hand, differentiating the log-likelihood w.r.t $\boldsymbol{\Sigma}$, we obtain
\begin{align}
\frac{\partial}{\partial\boldsymbol{\Sigma}}\ell(\boldsymbol{\theta})&=\sum_{m=1}^{M}-\frac{1}{2}\frac{\vert\boldsymbol{\Sigma}\vert\boldsymbol{\Sigma}^{-1}}{\vert\boldsymbol{\Sigma}\vert}+\frac{1}{2}\big[(\boldsymbol{\Sigma}^{-1})^\text{T}(\mathbf{z}[m]-\boldsymbol{\mu})(\mathbf{z}[m]-\boldsymbol{\mu})^\text{T}(\boldsymbol{\Sigma}^{-1})^\text{T}\big] \\ &=\frac{1}{2}\sum_{m=1}^{M}\boldsymbol{\Sigma}^{-1}(\mathbf{z}[m]-\boldsymbol{\mu})(\mathbf{z}[m]-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}
\end{align}
Setting this derivative to zero, we have that
\begin{align}
\boldsymbol{\Sigma}&=\sum_{m=1}^{M}(\mathbf{z}[m]-\boldsymbol{\mu})(\mathbf{z}[m]-\boldsymbol{\mu})^\text{T} \\ &=\sum_{m=1}^{M}\left[\begin{matrix}(x[m]-\mu_X)^2&(x[m]-\mu_X)(y[m]-\mu_Y) \\ (y[m]-\mu_Y)(x[m]-\mu_X)&(y[m]-\mu_Y)^2\end{matrix}\right] \\ &=\left[\begin{matrix}\text{Cov}_\mathcal{D}[X,X]&\text{Cov}_\mathcal{D}[X,Y] \\ \text{Cov}_\mathcal{D}[Y,X]&\text{Cov}_\mathcal{D}[Y,Y]\end{matrix}\right]
\end{align}</p><h4 id=bayesian-parameter-estimation>Bayesian Parameter Estimation<a hidden class=anchor aria-hidden=true href=#bayesian-parameter-estimation>#</a></h4><h5 id=general-setting>General setting<a hidden class=anchor aria-hidden=true href=#general-setting>#</a></h5><p>In the <strong>Bayesian approach</strong>, as before, we assume a general learning problem where we observe a training set $\mathcal{D}=\{\xi[1],\ldots,\xi[M]\}$ and a parametric model $P(\xi\vert\boldsymbol{\theta})$ where we can choose parameters from a parameter space $\Theta$, i.e. in this case, samples according to the probabilistic model are conditionally i.i.d given $\boldsymbol{\theta}$ instead of, recalling that in MLE, being (unconditionally) i.i.d.</p><h6 id=priors-posteriors>Priors, Posteriors<a hidden class=anchor aria-hidden=true href=#priors-posteriors>#</a></h6><p>To perform the task, we need to define a joint distribution $P(\mathcal{D},\boldsymbol{\theta})$ over the data and the parameters, which can be written by
\begin{equation}
P(\mathcal{D},\boldsymbol{\theta})=P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta}),
\end{equation}
where</p><ul id=number-list><li>$P(\mathcal{D}\vert\boldsymbol{\theta})$ is the <b>likelihood function</b>, which is the probability of the observations given the parameters, as in the MLE approach.</li><li>$P(\boldsymbol{\theta})$ is referred as the <b>prior distribution</b>, which encodes our prior beliefs, i.e. before data is observed.</li></ul><p>By Bayes&rsquo; rule, from the likelihood and prior, combined with the defined joint distribution, we can derive the so-called <strong>posterior distribution</strong> over the parameters, which corresponds to our beliefs after observing the data, as
\begin{align}
P(\boldsymbol{\theta}\vert\mathcal{D})&=\frac{P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta})}{P(\mathcal{D})} \\ &=\frac{P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta})}{\int_\Theta P(\mathcal{D}\vert\boldsymbol{\theta}&rsquo;)P(\boldsymbol{\theta}&rsquo;)d\boldsymbol{\theta}&rsquo;},
\end{align}
where, since the denominator is just a normalizing constant, can be expressed as
\begin{equation}
P(\boldsymbol{\theta}\vert\mathcal{D})\propto P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta})
\end{equation}
Since the posterior is a (normalized) product of the prior and the likelihood, it seems natural to require that the prior also have a form similar to the likelihood, such priors are referred as <strong>conjugate priors</strong>.</p><p>More formally, a family of priors $P(\boldsymbol{\theta}:\boldsymbol{\alpha})$ is <strong>conjugate</strong> to a particular model $P(\xi\vert\boldsymbol{\theta})$ if for any possible dataset $\mathcal{D}$ of i.i.d samples from $P(\xi\vert\boldsymbol{\theta})$, and any choice of legal hyperparameters $\boldsymbol{\alpha}$ for the prior over $\boldsymbol{\theta}$, there are hyperparameters $\boldsymbol{\alpha}&rsquo;$ that describe the posterior, i.e.
\begin{equation}
P(\boldsymbol{\theta}:\boldsymbol{\alpha}&rsquo;)\propto P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta}:\boldsymbol{\alpha})
\end{equation}</p><p><strong>Example 2</strong>: If we select Dirichlet as the prior distribution, specifically, let $\boldsymbol{\theta}=(\theta_1,\ldots,\theta_K)$ and $\boldsymbol{\theta}\sim\text{Dirichlet}(\alpha_1,\ldots,\alpha_K)$, we have that
\begin{equation}
P(\boldsymbol{\theta})\propto\prod_{k=1}^{K}\theta_k^{\alpha_k-1}
\end{equation}
Then we have that the posterior $P(\boldsymbol{\theta}\vert\mathcal{D})$ is $\text{Dirichlet}(\alpha_1+M[1],\ldots,\alpha_K+M[K])$, where $M[k]$ is the number of occurrences of $x^k$.</p><h6 id=bayesian-estimator>Bayesian Estimator<a hidden class=anchor aria-hidden=true href=#bayesian-estimator>#</a></h6><p>From the posterior, we can predict the probability of future samples. Specifically, suppose that we are about to sample a new instance $\xi[M+1]$, then the <strong>Bayesian estimator</strong>, or the <strong>predictive distribution</strong>, is the posterior distribution over a new example.
\begin{align}
P(\xi[M+1]\vert\mathcal{D})&=\int P(\xi[M+1]\vert\mathcal{D},\boldsymbol{\theta})P(\boldsymbol{\theta}\vert\mathcal{D})d\boldsymbol{\theta} \\ &=\int P(\xi[M+1]\vert\boldsymbol{\theta})P(\boldsymbol{\theta}\vert\mathcal{D})d\boldsymbol{\theta} \\ &=\mathbb{E}_{P(\boldsymbol{\theta}\vert\mathcal{D})}\big[P(\xi[M+1]\vert\boldsymbol{\theta})\big],
\end{align}
where in the second step, we use the fact that samples are i.i.d given $\boldsymbol{\theta}$.</p><h5 id=bayesian-parameter-estimation-in-bayesian-networks>Bayesian Parameter Estimation in Bayesian Networks<a hidden class=anchor aria-hidden=true href=#bayesian-parameter-estimation-in-bayesian-networks>#</a></h5><p>Since in Bayesian framework, it is required to specify a joint distribution over the training examples and the unknown parameters. Additionally, this joint distribution can be considered as a Bayesian network.</p><h6 id=parameter-independence-and-global-decomposition>Parameter Independence and Global Decomposition<a hidden class=anchor aria-hidden=true href=#parameter-independence-and-global-decomposition>#</a></h6><p>Suppose that we want to estimate parameters for a networks consisting of two variables $X$ and $Y$ with edge $X\rightarrow Y$. Also, we are given a training dataset $\mathcal{D}=\{(x[1],y[1]),\ldots,(x[M],y[M])\}$. Additionally, we have unknown parameters $\boldsymbol{\theta}_X,\boldsymbol{\theta}_{Y\vert X}$. The dependencies between variables are described in the following network.</p><figure id=fig1><img width=70% height=70% src=/images/pgm-learning/meta-network.png alt="Normal distribution"><figcaption><b>Figure 1</b>: (taken from <a href=#pgm-book>PGM book</a>) <b>Meta-network for i.i.d samples from a network $X\rightarrow Y$ with global parameter independence.</b> (a) <a href=https://trunghng.github.io/posts/machine-learning/pgm-representation/#plate-models>Plate model</a>; (b) <a href=https://trunghng.github.io/posts/machine-learning/pgm-representation/#gound-bn-plate-models>Ground Bayesian network</a></figcaption></figure><p>We have already known that in using Bayesian approach, the samples are independent given the parameters, i.e.
\begin{equation}
\text{d-sep}(\{X[m],Y[m]\};\{X[m&rsquo;],Y[m&rsquo;]\}\vert\{\boldsymbol{\theta}_X,\boldsymbol{\theta}_{Y\vert X}\}),
\end{equation}
which can be justified by an examination of active trails from the above figure. Moreover, the network structure given in <a href=#fig1>Figure 1</a> also satisfies the <strong>global parameter independence</strong>.</p><p>Let $\mathcal{G}$ be a BN structure with parameters $\boldsymbol{\theta}=(\boldsymbol{\theta}_{X_1\vert\text{Pa}_{X_1}},\ldots,\boldsymbol{\theta}_{X_n\vert\text{Pa}_{X_n}})$. Then, a prior $P(\boldsymbol{\theta})$ is said to satisfy <strong>global parameter independence</strong> if it has the form
\begin{equation}
P(\boldsymbol{\theta})=\prod_{i=1}^{n}P(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}})
\end{equation}
This property allows us to conclude that complete data, $\mathcal{D}$, d-separates the parameters for different CPDs, i.e.
\begin{equation}
\text{d-sep}\big(\boldsymbol{\theta}_X;\boldsymbol{\theta}_{Y\vert X}\big\vert\mathcal{D}\big)
\end{equation}
This is due to for each $m$, we have that any path between $X[m]$ and $Y[m]$ has the form
\begin{equation}
\boldsymbol{\theta}_X\rightarrow X[m]\rightarrow Y[m]\leftarrow\boldsymbol{\theta}_{Y\vert X},
\end{equation}
which is inactive given the observation $x[m],y[m]$. Thus, we obtain that
\begin{equation}
P(\boldsymbol{\theta}_X,\boldsymbol{\theta}_{Y\vert X}\vert\mathcal{D})=P(\boldsymbol{\theta}_X\vert\mathcal{D})P(\boldsymbol{\theta}_{Y\vert X}\vert\mathcal{D})
\end{equation}
This decomposition suggests that given $\mathcal{D}$, we can find the solution for the posterior over $\boldsymbol{\theta}$ by independently finding the result corresponding to the posterior over each of $\boldsymbol{\theta}_X$ and $\boldsymbol{\theta}_{Y\vert X}$, which is analogous to the <a href=#global-likelihood-decomposition>global likelihood decomposition</a> in the MLE. In Bayesian setting this property has additional importance.</p><p>Generally, suppose that we are given a Bayesian network graph $\mathcal{G}$ with parameters $\boldsymbol{\theta}$. As mentioned before that in using Bayesian approach, we need to specify a prior distribution over the parameter space $P(\boldsymbol{\theta})$ and a posterior distribution over the parameters given the samples $\mathcal{D}$
\begin{equation}
P(\boldsymbol{\theta}\vert\mathcal{D})=\frac{P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta})}{P(\mathcal{D})}\label{eq:pigd.1}
\end{equation}
Moreover, assuming that we have global parameter independence, then combining with the global likelihood decomposition mentioned above, the posterior in \eqref{eq:pigd.1} can be continued to derive as a product of local terms:
\begin{align}
P(\boldsymbol{\theta}\vert\mathcal{D})&=\frac{P(\mathcal{D}\vert\boldsymbol{\theta})P(\boldsymbol{\theta})}{P(\mathcal{D})} \\ &=\frac{1}{P(\mathcal{D})}\left(\prod_i L_i(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}:\mathcal{D})\right)\left(\prod_i P(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}})\right) \\ &=\frac{1}{P(\mathcal{D})}\prod_i\left(L_i(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}:\mathcal{D})P(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}})\right)
\end{align}
This gives rise to the following result.</p><p><strong>Proposition 1</strong>: Let $\mathcal{D}$ be a complete dataset for $\mathcal{X}$, let $\mathcal{G}$ be a BN graph over $\mathcal{X}$. If $P(\boldsymbol{\theta})$ satisfies global parameter independence, then
\begin{equation}
P(\boldsymbol{\theta}\vert\mathcal{D})=\prod_i P(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}\vert\mathcal{D})
\end{equation}</p><p><strong>Example 3</strong>: With the network specified in <a href=#fig1>Figure 1</a>, let us compute the predictive distribution. We have that
\begin{align}
&\hspace{-0.5cm}P(x[M+1],y[M+1]\vert\mathcal{D})\nonumber \\ &\hspace{-0.3cm}=\int P(x[M+1],y[M+1]\vert\mathcal{D},\boldsymbol{\theta})P(\boldsymbol{\theta}\vert\mathcal{D})d\boldsymbol{\theta} \\ &\hspace{-0.3cm}=\int P(x[M+1],y[M+1]\vert\boldsymbol{\theta})P(\boldsymbol{\theta}\vert\mathcal{D})d\boldsymbol{\theta} \\ &\hspace{-0.3cm}=\int P(x[M+1]\vert\boldsymbol{\theta}_X)P(y[M+1]\vert x[M+1],\boldsymbol{\theta}_{Y\vert X})P(\boldsymbol{\theta}_X\vert\mathcal{D})P(\boldsymbol{\theta}_{Y\vert X}\vert\mathcal{D})d\boldsymbol{\theta} \\ &\hspace{-0.3cm}=\int\int P(x[M+1]\vert\boldsymbol{\theta}_X)P(y[M+1]\vert x[M+1],\boldsymbol{\theta}_{Y\vert X})P(\boldsymbol{\theta}_X\vert\mathcal{D})P(\boldsymbol{\theta}_{Y\vert X}\vert\mathcal{D})d\boldsymbol{\theta}_X d\boldsymbol{\theta}_{Y\vert X} \\ &\hspace{-0.3cm}=\left(\int P(x[M+1]\vert\boldsymbol{\theta}_X)P(\boldsymbol{\theta}_X\vert\mathcal{D})d\boldsymbol{\theta}_X\right)\nonumber \\ &\hspace{2cm}\left(\int P(y[M+1]\vert x[M+1],\boldsymbol{\theta}_{Y\vert X})P(\boldsymbol{\theta}_{Y\vert X}\vert\mathcal{D})d\boldsymbol{\theta}_{Y\vert X}\right)
\end{align}
Thus, we can solve the prediction problem for the two variables $X$ and $Y$ independently.</p><p>In general, we can solve the prediction problem for each CPD, $P(X_i\vert\text{Pa}_{X_i})$, independently the combine the results together, i.e.
\begin{align}
&amp;P(X_1[M+1],\ldots,X_n[M+1]\vert\mathcal{D})\nonumber \\ &=\prod_{i=1}^{n}\int P\big(X_i[M+1]\big\vert\text{Pa}_{X_i}[M+1],\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}\big)P\big(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}\big\vert\mathcal{D}\big)d\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}
\end{align}</p><h6 id=local-decomposition>Local Decomposition<a hidden class=anchor aria-hidden=true href=#local-decomposition>#</a></h6><p>By the global decomposition, our attention is then to solve localized Bayesian estimation problems.</p><p>Let $X$ be a variable with parent $\mathbf{U}$. We say that the prior $P(\boldsymbol{\theta}_{X\vert\mathbf{U}})$ satisfies <strong>local parameter independence</strong> if
\begin{equation}
P(\boldsymbol{\theta}_{X\vert\mathbf{U}})=\prod_\mathbf{u} P(\theta_{X\vert\mathbf{u}}),
\end{equation}
This definition gives rise to the following result.</p><p><strong>Proposition 2</strong>: Let $\mathcal{D}$ be a complete dataset for $\mathcal{X}$, and let $\mathcal{G}$ be a BN graph over these variables with table-CPDs. If the prior $P(\boldsymbol{\theta})$ satisfies global and local parameter independence, then
\begin{align}
P(\boldsymbol{\theta}\vert\mathcal{D})&=\prod_i P(\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}}\vert\mathcal{D}) \\ &=\prod_i\prod_{\text{pa}_{X_i}}P(\boldsymbol{\theta}_{X_i\vert\text{pa}_{X_i}}\vert\mathcal{D})
\end{align}</p><h6 id=map-estimation>MAP Estimation<a hidden class=anchor aria-hidden=true href=#map-estimation>#</a></h6><h3 id=partially-observed-data>Partially Observed Data<a hidden class=anchor aria-hidden=true href=#partially-observed-data>#</a></h3><h4 id=likelihood-of-data-and-observation-models>Likelihood of Data and Observation Models<a hidden class=anchor aria-hidden=true href=#likelihood-of-data-and-observation-models>#</a></h4><p>Let $\mathbf{X}=\{X_1,\ldots,X_n\}$ be some set of r.v.s, and let $O_\mathbf{X}=\{O_{X_1},\ldots,O_{X_n}\}$ be their <strong>observability variable</strong>, which indicates whether the value of $X_i$ is observed. The <strong>observability model</strong> is a joint distribution
\begin{equation}
P_\text{missing}(\mathbf{X},O_\mathbf{X})=P(\mathbf{X})P_\text{missing}(O_\mathbf{X}\vert\mathbf{X}),
\end{equation}
so that $P(\mathbf{X})$ is parameterized by parameters $\boldsymbol{\theta}$, and $P_\text{missing}(O_\mathbf{X}\vert\mathbf{X})$ is parameterized by $\boldsymbol{\psi}$.</p><p>We define a new set of r.v.s $\mathbf{Y}=\{Y_1,\ldots,Y_n\}$, where $\text{Val}(Y_i)=\text{Val}(X_i)\cup\{?\}$. The actual observation is $\mathbf{Y}$, which is a deterministic function of $\mathbf{X}$ and $O_\mathbf{X}$
\begin{equation}
Y_i=\begin{cases}X_i&\hspace{1cm}O_{X_i}=o^1 \\ ?&\hspace{1cm}O_{X_i}=o^0\end{cases}
\end{equation}
The variables $Y_1,\ldots,Y_n$ represents the values we actually observe, either an actual value or a ?, which denotes a missing value.</p><p><strong>Example 4</strong>: Consider the following observability model with $X\sim\text{Bern}(\theta)$ and $O_X\sim\text{Bern}(\psi)$.</p><figure><img width=30% height=30% src=/images/pgm-learning/observability-model1.png alt="Observability model"></figure><p>We have that
\begin{align}
&amp;P(X=1)=\theta,&&\hspace{1cm}P(x=0)=1-\theta \\ &amp;P(O_X=o^1)=\psi,&&\hspace{1cm}P(O_X=o^0)=1-\psi
\end{align}
and thus
\begin{align}
P(Y=1)&=\theta\psi \\ P(Y=0)&=(1-\theta)\psi \\ P(Y=?)&=1-\psi
\end{align}
Thus if we see a dataset $\mathcal{D}$ with $M[1],M[0]$ and $M[?]$ instances, then the likelihood is
\begin{align}
L(\theta,\psi:\mathcal{D})&=(\theta\psi)^{M[1]}\big((1-\theta)\psi\big)^{M[0]}(1-\psi)^{M[?]} \\ &=\theta^{M[1]}(1-\theta)^{M[0]}\psi^{M[1]+M[0]}(1-\psi)^{M[?]}
\end{align}
Differentiating the likelihood w.r.t $\theta$ and $\psi$ and setting the derivatives to zero we have that
\begin{align}
\hat{\theta}_\text{ML}&=\frac{M[1]}{M[1]+M[0]} \\ \hat{\psi}_\text{ML}&=\frac{M[1]+M[0]}{M[1]+M[0]+M[?]}
\end{align}</p><p><strong>Example 5</strong>: Consider the following observability model with $X\sim\text{Bern}(\theta)$, $(O_X\vert X=1)\sim\text{Bern}(\psi_{O_X\vert x^1})$ and $(O_X\vert X=0)\sim\text{Bern}(\psi_{O_X\vert x^0})$.</p><figure><img width=30% height=30% src=/images/pgm-learning/observability-model2.png alt="Observability model"></figure><p>In this case, we have that
\begin{align}
&amp;P(X=1)=\theta,&&\hspace{1cm}P(x=0)=1-\theta \\ &amp;P(O_X=o^1\vert X=1)=\psi_{O_X\vert x^1},&&\hspace{1cm}P(O_X=o^0\vert X=1)=1-\psi_{O_X\vert x^1} \\ &amp;P(O_X=o^1\vert X=0)=\psi_{O_X\vert x^0},&&\hspace{1cm}P(O_X=o^0\vert X=0)=1-\psi_{O_X\vert x^0}
\end{align}
and thus
\begin{align}
P(Y=1)&=\theta\psi_{O_X\vert x^1} \\ P(Y=0)&=(1-\theta)\psi_{O_X\vert x^0} \\ P(Y=?)&=\theta(1-\psi_{O_X\vert x^1})+(1-\theta)(1-\psi_{O_X\vert x^0})
\end{align}
Then, given the dataset $\mathcal{D}$ with $M[1],M[0]$ and $M[?]$ examples, the likelihood function is given as
\begin{align}
&amp;L(\theta,\psi_{O_X\vert x^1},\psi_{O_X\vert x^0}:\mathcal{D})\nonumber \\ &=(\theta\psi_{O_X\vert x^1})^{M[1]}\big[(1-\theta)\psi_{O_X\vert x^0}\big]^{M[0]}\big[\theta(1-\psi_{O_X\vert x^1})+(1-\theta)(1-\psi_{O_X\vert x^0})\big]^{M[?]}
\end{align}</p><h4 id=decoupling-of-observation-mechanism>Decoupling of Observation Mechanism<a hidden class=anchor aria-hidden=true href=#decoupling-of-observation-mechanism>#</a></h4><h5 id=mcar>Missing Completely At Random<a hidden class=anchor aria-hidden=true href=#mcar>#</a></h5><p>A missing data model $P_\text{missing}$ is <strong>missing completely at random</strong> (<strong>MCAR</strong>) if $P_\text{missing}\models(\mathbf{X}\perp O_\mathbf{X})$.</p><p>In this case, the likelihood function of $X$ and $O_X$ decomposes as a product, and we can maximize each part separately. However, MCAR is sufficient but not necessary for the decomposition of the likelihood function.</p><h5 id=mar>Missing At Random<a hidden class=anchor aria-hidden=true href=#mar>#</a></h5><p>Let $\mathbf{y}$ be a tuple of observations. These observations partition the variables $\mathbf{X}$ into two sets</p><ul id=number-list><li>the observed variables $\mathbf{X}_\text{obs}^\mathbf{y}=\{X_i:y_i\neq ?\}$;</li><li>the hidden variables $\mathbf{X}_\text{hidden}^\mathbf{y}=\{X_i:y_i=?\}$.</li></ul><p>The observations $\mathbf{y}$ determines the values of the observed variables, but not the hidden ones.</p><p>A missing data model $P_\text{missing}$ is <strong>missing at random</strong> (<strong>MAR</strong>) if for all observations $\mathbf{y}$ with $P_\text{missing}(\mathbf{y})>0$, and for all $\mathbf{x}_\text{hidden}^\mathbf{y}\in\text{Val}(\mathbf{X}_\text{hidden}^\mathbf{y})$, we have that
\begin{equation}
P_\text{missing}\models(o_\mathbf{X}\perp\mathbf{x}_\text{hidden}^\mathbf{y}\vert\mathbf{x}_\text{obs}^\mathbf{y}),
\end{equation}
where $o_\mathbf{X}$ are specific values of the observation variables given $\mathbf{Y}$.</p><p>This implies that given the observed variables, the observation pattern does not give any additional information about the hidden variables
\begin{equation}
P_\text{missing}(\mathbf{x}_\text{hidden}^\mathbf{y}\vert\mathbf{x}_\text{obs}^\mathbf{y},o_\mathbf{X})=P_\text{missing}(\mathbf{x}_\text{hidden}^\mathbf{y}\vert\mathbf{x}_\text{obs}^\mathbf{y})
\end{equation}
Thus, with a missing model satisfying MAR, we have
\begin{align}
P_\text{missing}(\mathbf{y})&=P_\text{missing}(o_\mathbf{X},\mathbf{x}_\text{obs}^\mathbf{y}) \\ &=\sum_{\mathbf{x}_\text{hidden}^\mathbf{y}}\Big[P(\mathbf{x}_\text{obs}^\mathbf{y},\mathbf{x}_\text{hidden}^\mathbf{y})P_\text{missing}(o_\mathbf{X}\vert\mathbf{x}_\text{obs}^\mathbf{y},\mathbf{x}_\text{hidden}^\mathbf{y})\Big] \\ &=\sum_{\mathbf{x}_\text{hidden}^\mathbf{y}}\Big[P(\mathbf{x}_\text{obs}^\mathbf{y},\mathbf{x}_\text{hidden}^\mathbf{y})P_\text{missing}(o_\mathbf{X}\vert\mathbf{x}_\text{obs}^\mathbf{y})\Big] \\ &=P_\text{missing}(o_\mathbf{X}\vert\mathbf{x}_\text{obs}^\mathbf{y})\sum_{\mathbf{x}_\text{hidden}^\mathbf{y}}P(\mathbf{x}_\text{obs}^\mathbf{y},\mathbf{x}_\text{hidden}^\mathbf{y}) \\ &=P_\text{missing}(o_\mathbf{X}\vert\mathbf{x}_\text{obs}^\mathbf{y})P(\mathbf{x}_\text{obs}^\mathbf{y}),\label{eq:mar.1}
\end{align}
where the first equality is justified due to the fact that $\mathbf{y}$ are completely determined once we have the knowledge about $\mathbf{x}_\text{obs}^\mathbf{y}$ and $o_\mathbf{X}$.</p><p>Notice that the first term in \eqref{eq:mar.1}, $P_\text{missing}(o_\mathbf{X}\vert\mathbf{x}_\text{obs}^\mathbf{y})$, depends only on the parameters $\boldsymbol{\psi}$; while the second one, $P(\mathbf{x}_\text{obs}^\mathbf{y})$, depends only the parameters $\boldsymbol{\theta}$. And since we have this product for every observed instance, we then have the following result.</p><p><strong>Theorem 3</strong>: <em>If $P_\text{missing}$ satisfies MAR, then $L(\boldsymbol{\theta},\boldsymbol{\psi}:\mathcal{D})$ can be written as a product of two likelihood functions $L(\boldsymbol{\theta}:\mathcal{D})$ and $L(\boldsymbol{\psi}:\mathcal{D})$.</em></p><h4 id=the-likelihood-function>The Likelihood Function<a hidden class=anchor aria-hidden=true href=#the-likelihood-function>#</a></h4><p>Given a Bayesian network structure $\mathcal{G}$ over a set of variables $\mathbf{X}$, and dataset $\mathcal{D}$ of $M$ training instances, each of which has</p><ul id=number-list><li>a different set of observed variables, denoted $\{\mathbf{O}[m]:m=1,\ldots,M\}$ and their corresponding values $\{\mathbf{o}[m]:m=1,\ldots,M\}$;</li><li>a different set of hidden (or latent) variables, denoted $\{\mathbf{H}[m]:m=1,\ldots,M\}$</li></ul><p>The likelihood function, $L(\boldsymbol{\theta}:\mathcal{D})$, is defined as the probability of the observed variables in the data, marginalizing the hidden variables, and ignoring the observability model:
\begin{equation}
L(\boldsymbol{\theta}:\mathcal{D})=\prod_{m=1}^{M}P(\mathbf{o}[m];\boldsymbol{\theta})\label{eq:tlf.1}
\end{equation}
And thus the log-likelihood is given as
\begin{equation}
\ell(\boldsymbol{\theta}:\mathcal{D})=\sum_{m=1}^{M}\log P(\mathbf{o}[m];\boldsymbol{\theta})
\end{equation}
This definition of likelihood function suggests that the problem of learning with partially observed data basically does not differ from the problem of learning with fully observed data. However, the computational complexity is bigger in this case and it requires the use of missing data.
\begin{equation}
L(\boldsymbol{\theta}:\mathcal{D})=\prod_{m=1}^{M}P(\mathbf{o}[m];\boldsymbol{\theta})=\prod_{m=1}^{M}\sum_{\mathbf{h}[m]}P(\mathbf{o}[m],\mathbf{h}[m];\boldsymbol{\theta})
\end{equation}
And thus the log-likelihood can be computed by
\begin{equation}
\ell(\boldsymbol{\theta}:\mathcal{D})=\sum_{m=1}^{M}\log\sum_{\mathbf{h}[m]}P(\mathbf{o}[m],\mathbf{h}[m];\boldsymbol{\theta})\label{eq:tlf.2}
\end{equation}</p><h4 id=mle>MLE<a hidden class=anchor aria-hidden=true href=#mle>#</a></h4><h5 id=gradient-ascent>Gradient Ascent<a hidden class=anchor aria-hidden=true href=#gradient-ascent>#</a></h5><p><b id=lemma4>Lemma 4</b>: <em>Let $\mathcal{B}$ be a Bayesian network with graph $\mathcal{G}$ over $\mathcal{X}$ that induces a probability distribution $P$, let $\mathbf{o}$ be a tuple of observations for some variables, and let $X\in\mathcal{X}$ be some r.v. If $P(x\vert\mathbf{u})>0$, where $x\in\text{Val}(X)$, $\mathbf{u}\in\text{Val}(\text{Pa}_X)$, then we have</em>
\begin{equation}
\frac{\partial P(\mathbf{o})}{\partial P(x\vert\mathbf{u})}=\frac{P(x,\mathbf{u},\mathbf{o})}{P(x\vert\mathbf{u})}
\end{equation}</p><p><strong>Proof</strong><br>Consider the case of full assignment $\xi$. We have
\begin{equation}
P(\xi)=\prod_{X_i\in\mathcal{X}}P(\xi\langle X_i\rangle\vert\xi\langle\text{Pa}_{X_i}\rangle)
\end{equation}
Thus, we have that
\begin{equation}
\hspace{-1cm}\frac{\partial P(\xi)}{\partial P(x\vert\mathbf{u})}=\begin{cases}\displaystyle\prod_{X_i\in\mathcal{X},X_i\neq X}P\big(\xi\langle X_i\rangle\vert\xi\langle\text{Pa}_{X_i}\rangle\big)=\frac{P(\xi)}{P(x\vert\mathbf{u})}&\hspace{1cm}\text{if }\xi\langle X,\text{Pa}_X\rangle=\langle x,\mathbf{u}\rangle \\ 0&\hspace{1cm}\text{otherwise}\end{cases}\label{eq:ga.1}
\end{equation}
Now consider the case of partial assignment $\xi$. We have
\begin{equation}
P(\mathbf{o})=\sum_{\xi:\xi\langle\mathbf{O}\rangle=\mathbf{o}}P(\xi)
\end{equation}
Using the result \eqref{eq:ga.1}, we then have that
\begin{align}
\frac{\partial P(\mathbf{o})}{\partial P(x\vert\mathbf{u})}&=\sum_{\xi:\xi\langle\mathbf{O}\rangle=\mathbf{o}}\frac{\partial P(\xi)}{\partial P(x\vert\mathbf{u})} \\ &=\sum_{\xi:\xi\langle\mathbf{O}\rangle=\mathbf{o},\xi\langle X,\text{Pa}_X\rangle=\langle x,\mathbf{u}\rangle}\frac{P(\xi)}{P(x\vert\mathbf{u})} \\ &=\frac{P(x,\mathbf{u},\mathbf{o})}{P(x\vert\mathbf{u})}
\end{align}</p><p>Given this result, we immediately obtain the form of the gradient of the likelihood function for table-CPDs.</p><p><strong>Theorem 5</strong>: <em>Let $\mathcal{G}$ be a Bayesian network graph over $\mathcal{X}$, and let $\mathcal{D}=\{\mathbf{o}[1],\ldots,\mathbf{o}[M]\}$ be a partially observable dataset. Let $X$ be a variable with parents $\mathbf{U}$ in $\mathcal{G}$. If $P(x\vert\mathbf{u})>0$, where $x\in\text{Val}(X)$, $\mathbf{u}\in\text{Val}(\mathbf{U})$, then we have</em>
\begin{equation}
\frac{\partial\ell(\boldsymbol{\theta}:\mathcal{D})}{\partial P(x\vert\mathbf{u})}=\frac{1}{P(x\vert\mathbf{u})}\sum_{m=1}^{M}P(x,\mathbf{u}\vert\mathbf{o}[m];\boldsymbol{\theta})
\end{equation}</p><p><strong>Proof</strong><br>The gradient of the log-likelihood can be written as
\begin{align}
\frac{\partial\ell(\boldsymbol{\theta}:\mathcal{D})}{\partial P(x\vert\mathbf{u})}&=\sum_{m=1}^{M}\frac{\partial}{\partial P(x\vert\mathbf{u})}\log P(\mathbf{o}[m];\boldsymbol{\theta}) \\ &=\sum_{m=1}^{M}\frac{1}{P(\mathbf{o}[m];\boldsymbol{\theta})}\frac{\partial P(\mathbf{o}[m];\boldsymbol{\theta})}{\partial P(x\vert\mathbf{u})} \\ &=\sum_{m=1}^{M}\frac{1}{P(\mathbf{o}[m];\boldsymbol{\theta})}\frac{P(x,\mathbf{u},\mathbf{o}[m];\boldsymbol{\theta})}{P(x\vert\mathbf{u})} \\ &=\frac{1}{P(x\vert\mathbf{u})}\sum_{m=1}^{M}P(x,\mathbf{u}\vert\mathbf{o}[m];\boldsymbol{\theta})
\end{align}
where the third equality is obtained by applying <a href=#lemma4>Lemma 4</a>.</p><p>Using this result, suppose that the CPDs entries of $P(X\vert\mathbf{U})$ are written as functions of some set of parameters $\boldsymbol{\theta}$. Then for a specific parameter $\theta\in\boldsymbol{\theta}$, we have
\begin{equation}
\frac{\partial\ell(\boldsymbol{\theta}:\mathcal{D})}{\partial\theta}=\sum_{x,\mathbf{u}}\frac{\partial\ell(\boldsymbol{\theta}:\mathcal{D})}{\partial P(x\vert\mathbf{u})}\frac{\partial P(x\vert\mathbf{u})}{\partial\theta}
\end{equation}</p><figure><img src=/images/pgm-learning/gradient-ascent.png alt="gradient ascent table-CPDs network"></figure><h5 id=em>Expectation Maximization<a hidden class=anchor aria-hidden=true href=#em>#</a></h5><p>[TODO]
Consider the following model, which is described by the following meta-network, where $X$ is fully observed, while $Z$ is partially observed.</p><p>The log-likelihood, as described in \eqref{eq:tlf.2}, is then given as
\begin{align}
\ell(\boldsymbol{\theta}:\mathcal{D})&=\sum_{m=1}^{M}\log P(x[m];\boldsymbol{\theta}) \\ &=\sum_{m=1}^{M}\log\sum_{z[m]}P(x[m],z[m];\boldsymbol{\theta})
\end{align}
[TODO]
For each $m=1,\ldots,M$, let $Q_m$ be some distribution over $Z$, i.e. $\sum_z Q_m(z)=1$ and $Q_m(z)\geq 0$<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. We have
\begin{align}
\hspace{-0.5cm}\sum_{m=1}^{M}\log P(x[m];\boldsymbol{\theta})&=\sum_{m=1}^{M}\log\sum_{z[m]\in\text{Val}(Z)}P(x[m],z[m];\boldsymbol{\theta}) \\ &=\sum_{m=1}^{M}\log\sum_{z[m]\in\text{Val}(Z)}Q_m(z[m])\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])} \\ &\geq\sum_{m=1}^{M}\sum_{z[m]\in\text{Val}(Z)}Q_m(z[m])\log\left(\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right),\label{eq:em.1}
\end{align}
where in the third step, we have used <strong>Jensen&rsquo;s inequality</strong>. Specifically, since $\log(\cdot)$ is a strictly concave function, due to $(\log(x))&rsquo;&rsquo;=-1/x^2&lt;0$, then by Jensen&rsquo;s inequality, we have
\begin{align}
\log\left(\sum_{z[m]}Q_m(z[m])\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right)&=\log\left(\mathbb{E}_{z[m]\sim Q_m}\left[\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right]\right) \\ &\geq\mathbb{E}_{z[m]\sim Q_m}\left[\log\left(\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right)\right] \\ &=\sum_{z[m]}Q_m(z[m])\log\left(\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right),
\end{align}
where equality holds where
\begin{equation}
\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}=\mathbb{E}_{z[m]\sim Q_m}\left[\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}\right],
\end{equation}
with probability $1$. This implies that
\begin{equation}
\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}=c,
\end{equation}
where $c$ is a constant w.r.t $z[m]$. Moreover, since $\sum_z Q_m(z)=1$, we then have
\begin{equation}
Q_m(z[m])\propto P(x[m],z[m];\boldsymbol{\theta})
\end{equation}
And since $Q_m$ is a distribution, we further have that
\begin{align}
Q_m(z[m])&=\frac{P(x[m],z[m];\boldsymbol{\theta})}{\sum_z P(x[m],z;\boldsymbol{\theta})} \\ &=\frac{P(x[m],z[m];\boldsymbol{\theta})}{P(x[m];\boldsymbol{\theta})} \\ &=P(z[m]\vert x[m];\boldsymbol{\theta})\label{eq:em.3}
\end{align}</p><p>Hence, the RHS of \eqref{eq:em.1} then gives us a lower bound of the log-likelihood function and that $\ell(\boldsymbol{\theta}:\mathcal{D})$ reaches this value if our selection of $Q_m$ satisfies the equation \eqref{eq:em.3}, i.e. $Q_m(Z)=P(Z\vert x[m];\boldsymbol{\theta})$.</p><p>Our goal is to find parameters $\boldsymbol{\theta}$ that maximizes the log-likelihood $\ell(\boldsymbol{\theta}:\mathcal{D})$</p><blockquote><p>Repeat until convergence:<br>$\hspace{1cm}$(E-step) For each $m=1,\ldots,M$, set:
\begin{equation*}
Q_m(Z):=P(Z\vert x[m];\boldsymbol{\theta})
\end{equation*}
$\hspace{1cm}$(M-step) Set:
\begin{equation*}
\boldsymbol{\theta}:=\underset{\boldsymbol{\theta}}{\text{argmax}}\sum_{m=1}^{M}\sum_{z[m]\in\text{Val}(Z)}Q_m(z[m])\log\frac{P(x[m],z[m];\boldsymbol{\theta})}{Q_m(z[m])}
\end{equation*}</p></blockquote><h6 id=em-for-bayesian-networks>EM for Bayesian networks<a hidden class=anchor aria-hidden=true href=#em-for-bayesian-networks>#</a></h6><p>Let us consider the EM algorithm for a general Bayesian network structure $\mathcal{G}$ with table-CPDs over random variables $X_1,\ldots,X_n$. Suppose that we are given a dataset $\mathcal{D}=\{\xi[1],\ldots,x[M]\}$, where for each instance $\xi[m]$, $\mathbf{O}[m]$ and $\mathbf{o}[m]$ denote the set of observed variables and their values, while $\mathbf{H}[m]$ represents the set of hidden variables. The log-likelihood function is given as
\begin{align}
\ell(\boldsymbol{\theta}:\mathcal{D})&=\sum_{m=1}^{M}\log P(\mathbf{o}[m];\boldsymbol{\theta}) \\ &=\sum_{m=1}^{M}\log\sum_{\mathbf{h}\in\text{Val}(\mathbf{H}[m])}P(\mathbf{o}[m],\mathbf{h};\boldsymbol{\theta}) \\ &\geq\sum_{m=1}^{M}\sum_{\mathbf{h}\in\text{Val}(\mathbf{H}[m])}Q_m(\mathbf{h})\log\frac{P(\mathbf{o}[m],\mathbf{h};\boldsymbol{\theta})}{Q_m(\mathbf{h})} \\ &=\left(\sum_{m=1}^{M}\sum_{\mathbf{h}\in\text{Val}(\mathbf{H}[m])}Q_m(\mathbf{h})\big[\log P(\mathbf{o}[m],\mathbf{h};\boldsymbol{\theta})-\log Q_m(\mathbf{h})\big]\right) \\ &=\sum_{m=1}^{M}\sum_{\mathbf{h}\in\text{Val}(\mathbf{H}[m])}Q_m(\mathbf{h})\log\prod_{i=1}^{n}P\Big(\xi[m]\langle X_i\rangle\Big\vert\xi[m]\langle\text{Pa}_{X_i}^\mathcal{G}\rangle;\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}^\mathcal{G}}\Big)\nonumber \\ &\hspace{1cm}+\sum_{m=1}^{M}H_{Q_m}(Q_m(\mathbf{H}[m])) \\ &=\sum_{m=1}^{M}\sum_{\mathbf{h}\in\text{Val}(\mathbf{H}[m])}Q_m(\mathbf{h})\sum_{i=1}^{n}\log P\Big(\xi[m]\langle X_i\rangle\Big\vert\xi[m]\langle\text{Pa}_{X_i}^\mathcal{G}\rangle;\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}^\mathcal{G}}\Big)\nonumber \\ &\hspace{1cm}+\sum_{m=1}^{M}H_{Q_m}(Q_m(\mathbf{H}[m])) \\ &=\sum_{i=1}^{n}\left(\sum_{m=1}^{M}\sum_{\mathbf{h}\in\mathbf{H}[m]}Q_m(\mathbf{h})\log P\Big(\xi[m]\langle X_i\rangle\Big\vert\xi[m]\langle\text{Pa}_{X_i}^\mathcal{G}\rangle;\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}^\mathcal{G}}\Big)\right)\nonumber \\ &\hspace{1cm}+\sum_{m=1}^{M}H_{Q_m}(Q_m(\mathbf{H}[m])) \\ &\triangleq\tilde{\ell}(\boldsymbol{\theta})
\end{align}
This gives us a lower bound for the log-likelihood function $\ell(\boldsymbol{\theta}:\mathcal{D})$. And similarly, in the E-step of EM algorithm, for each $m=1,\ldots,M$, we set
\begin{equation}
Q_m(\mathbf{H})=P(\mathbf{H}\vert\mathbf{o}[m];\boldsymbol{\theta})
\end{equation}
In the M-step of EM, we find a parameter $\boldsymbol{\theta}$ that maximizes $\tilde{\ell}(\boldsymbol{\theta})$. Before performing this step, we notice that the function $\tilde{\ell}(\boldsymbol{\theta})$ can be decomposed into sum of function of local CPDs $P(X_i\vert\text{Pa}_{X_i}^\mathcal{G};\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}^\mathcal{G}})$, which parameterized by $\boldsymbol{\theta}_{X_i\vert\text{Pa}_{X_i}^\mathcal{G}}$. Hence, as usual, we can independently solve for the local problems and then combine the solutions together.</p><p>In particular, during M-step, for each variable $X$, and for each $U\in\text{Pa}_X^\mathcal{G}$, we try to solve the optimization problem
\begin{align}
\underset{\boldsymbol{\theta}_{X\vert U}}{\text{maximize}}&\sum_{m=1}^{M}\sum_{\mathbf{h}\in\mathbf{H}[m]}Q_m(\mathbf{h})\log P\big(\xi[m]\langle X\rangle\big\vert\xi[m]\langle U\rangle;\boldsymbol{\theta}_{X\vert u}) \\ \text{s.t.}&\sum_{x\in\text{Val}(X)}\theta_{x\vert u}=1,\hspace{1cm}\forall u\in\text{Val}(U)
\end{align}
The Lagrangian of this problem is
\begin{align}
&\mathcal{L}(\boldsymbol{\theta}_{X\vert U},\lambda)\nonumber \\ &=-\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\log P\big(\xi[m]\langle X\rangle\big\vert\xi[m]\langle U\rangle;\boldsymbol{\theta}_{X\vert u})+\sum_{u\in\text{Val}(U)}\lambda_u\left(\sum_x\theta_{x\vert u}-1\right) \\ &=-\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\log\prod_{(x,u)\in\text{Val}(X,U)}\theta_{x\vert u}^{\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}}+\sum_u\lambda_u\left(\sum_x\theta_{x\vert u}-1\right) \\ &=-\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\sum_{x,u}\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}\log\theta_{x\vert u}+\sum_u\lambda_u\left(\sum_x\theta_{x\vert u}-1\right)
\end{align}
Differentiating w.r.t $\theta_{x\vert u}$ yields
\begin{equation}
\nabla_{\theta_{x\vert u}}\mathcal{L}(\boldsymbol{\theta}_{X\vert U},\lambda)=-\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\frac{\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}}{\theta_{x\vert u}}+\lambda_u
\end{equation}
Setting this derivative to zero, we obtain
\begin{equation}
\theta_{x\vert u}=\frac{1}{\lambda_u}\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}\triangleq\frac{f(x,u)}{\lambda_u}
\end{equation}
The dual function is then given as
\begin{align}
g(\lambda)&=\sup_{\boldsymbol{\theta}_{X\vert U}}\mathcal{L}(\boldsymbol{\theta}_{X\vert U},\lambda) \\ &=-\sum_{x,u}\log\left(\frac{f(x,u)}{\lambda_u}\right)f(x,u)+\sum_u\lambda_u\left(\sum_x\frac{f(x,u)}{\lambda_u}-1\right) \\ &=\sum_{x,u}\big(\log\lambda_u-\log f(x,u)\big)f(x,u)+\sum_{x,u}f(x,u)-\sum_u\lambda_u
\end{align}
Taking the derivative w.r.t $\lambda_u$ lets us obtain
\begin{align}
\frac{\partial g(\lambda)}{\partial\lambda_u}&=\sum_x\frac{f(x,u)}{\lambda_u}-1,
\end{align}
which gives us the value of $\lambda_u$ if being set to zero
\begin{equation}
\lambda_u=\sum_x f(x,u)
\end{equation}
Hence, we have the solution for $\theta_{x\vert u}$:
\begin{align}
\theta_{x\vert u}&=\frac{f(x,u)}{\sum_x f(x,u)} \\ &=\frac{\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}}{\sum_x\sum_m\sum_\mathbf{h}Q_m(\mathbf{h})\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}} \\ &=\frac{\sum_m\sum_\mathbf{h}P(\mathbf{h}\vert\mathbf{o}[m];\boldsymbol{\theta})\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}}{\sum_x\sum_m\sum_\mathbf{h}P(\mathbf{h}\vert\mathbf{o}[m];\boldsymbol{\theta})\mathbf{1}\{\xi[m]\langle X,U\rangle=\langle x,u\rangle\}} \\ &=\frac{\sum_m P(x,u\vert\mathbf{o}[m];\boldsymbol{\theta})}{\sum_x\sum_m P(x,u\vert\mathbf{o}[m];\boldsymbol{\theta})}
\end{align}
We end up with this pseudocode for EM in Bayesian network with table-CPDs.</p><h4 id=bayesian-learning>Bayesian Learning<a hidden class=anchor aria-hidden=true href=#bayesian-learning>#</a></h4><h2 id=structure-learning>Structure Learning<a hidden class=anchor aria-hidden=true href=#structure-learning>#</a></h2><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=pgm-book>Daphne Koller, Nir Friedman. <a href=https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/>Probabilistic Graphical Models</a>. The MIT Press.</span></p><p>[2] Christopher M. Bishop. <a href=https://link.springer.com/book/9780387310732>Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p><p>[3] Stanford CS229. <a href=https://cs229.stanford.edu>Machine Learning</a>.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Without loss of generality, we have implicitly assumed that $Z$ is discrete. The same arguments can be extended to the continuous case by using integrations instead of summations.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://trunghng.github.io/tags/probabilistic-graphical-model/>probabilistic-graphical-model</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/maddpg/><span class=title>« Prev</span><br><span>Multi-agent Deep Deterministic Policy Gradient</span>
</a><a class=next href=https://trunghng.github.io/posts/machine-learning/pgm-inference/><span class=title>Next »</span><br><span>Read-through: Probabilistic Graphical Models - Inference</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on x" href="https://x.com/intent/tweet/?text=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f&amp;hashtags=machine-learning%2cprobabilistic-graphical-model"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f&amp;title=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning&amp;summary=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f&title=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on whatsapp" href="https://api.whatsapp.com/send?text=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on telegram" href="https://telegram.me/share/url?text=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Read-through: Probabilistic Graphical Models - Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Read-through%3a%20Probabilistic%20Graphical%20Models%20-%20Learning&u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fpgm-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>