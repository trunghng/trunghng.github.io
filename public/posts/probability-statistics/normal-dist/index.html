<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Gaussian Distribution | Trung's Place</title><meta name=keywords content="mathematics,probability-statistics,normal-distribution"><meta name=description content="
The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/probability-statistics/normal-dist/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.ffe3ae1550a8515c6bee279f558c3f92f2c8035a66b1abe2187f2be1166b0494.css integrity="sha256-/+OuFVCoUVxr7iefVYw/kvLIA1pmsaviGH8r4RZrBJQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a{text-decoration:none}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list{counter-reset:section}#roman-list,#number-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-.75em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Gaussian Distribution"><meta property="og:description" content="
The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/probability-statistics/normal-dist/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-22T14:46:00+07:00"><meta property="article:modified_time" content="2021-11-22T14:46:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Gaussian Distribution"><meta name=twitter:description content="
The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Gaussian Distribution","item":"https://trunghng.github.io/posts/probability-statistics/normal-dist/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gaussian Distribution","name":"Gaussian Distribution","description":" The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.\n","keywords":["mathematics","probability-statistics","normal-distribution"],"articleBody":" The Gaussian (Normal) distribution is a continuous distribution with a bell-shaped PDF used widely in statistics due to the Central Limit theorem. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.\n$\\newcommand{\\Var}{\\mathrm{Var}}$ $\\newcommand{\\Cov}{\\mathrm{Cov}}$\nGaussian (Normal) Distribution A random variable $X$ is said to be Gaussian or to have the Normal distribution with mean $\\mu$ and variance $\\sigma^2$ if its probability density function (PDF) is \\begin{equation} f_X(x)=\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right) \\end{equation} which we denote as $X\\sim\\mathcal{N}(\\mu,\\sigma)$.\nStandard Normal When $X$ is normally distributed with mean $\\mu=0$ and variance $\\sigma^2=1$, we call its distribution Standard Normal. \\begin{equation} X\\sim\\mathcal{N}(0,1) \\end{equation} In this case, $X$ has special notations to denote its PDF and CDF, which are \\begin{equation} \\varphi(x)=\\dfrac{1}{\\sqrt{2\\pi}}e^{-z^2/2}, \\end{equation} and \\begin{equation} \\Phi(x)=\\int_{-\\infty}^{x}\\varphi(t)\\hspace{0.1cm}dt=\\int_{-\\infty}^{x}\\dfrac{1}{\\sqrt{2\\pi}}e^{-t^2/2}\\hspace{0.1cm}dt \\end{equation} Below are some illustrations of Normal distribution.\nFigure 1: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found here Multivariate Normal Distribution A $k$-dimensional random vector $\\mathbf{X}=\\left(X_1,\\dots,X_D\\right)^\\text{T}$ is said to have a Multivariate Normal (MVN) distribution if every linear combination of the $X_i$ has a Normal distribution. Which means \\begin{equation} t_1X_1+\\ldots+t_DX_D \\end{equation} is normally distributed for any choice of constants $t_1,\\dots,t_D$. Distribution of $\\mathbf{X}$ then can be written in the following notation \\begin{equation} \\mathbf{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}), \\end{equation} where \\begin{equation} \\boldsymbol{\\mu}=\\mathbb{E}\\mathbf{X}=\\mathbb{E}\\left(\\mu_1,\\ldots,\\mu_k\\right)^\\text{T}=\\left(\\mathbb{E}X_1,\\ldots,\\mathbb{E}X_k\\right)^\\text{T} \\end{equation} is the $D$-dimensional mean vector, and covariance matrix $\\mathbf{\\Sigma}\\in\\mathbb{R}^{D\\times D}$ with \\begin{equation} \\boldsymbol{\\Sigma}_{ij}=\\mathbb{E}\\left(X_i-\\mu_i\\right)\\left(X_j-\\mu_j\\right)=\\Cov(X_i,X_j)\\label{eq:mvn.1} \\end{equation} We also have that $\\boldsymbol{\\Sigma}\\geq 0$ (positive semi-definite matrix)1.\nThus, the PDF of an MVN is defined as \\begin{equation} f_\\mathbf{X}(x_1,\\ldots,x_D)=\\dfrac{1}{(2\\pi)^{D/2}\\vert\\mathbf{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right]\\label{eq:mvn.2} \\end{equation} With this idea, Standard Normal distribution in multi-dimensional case can be defined as a Gaussian with mean $\\boldsymbol{\\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\\boldsymbol{\\Sigma}=\\mathbf{I}_{D\\times D}$.\nBivariate Normal When the number of dimensions in $\\mathbf{X}$, $D=2$, this special case of MVN is referred as Bivariate Normal (BVN).\nAn example of an BVN, $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u00260.5\\\\0.8\u00261\\end{smallmatrix}\\right]\\right)$, is shown as following.\nFigure 2: The PDF of $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u00260.5\\\\0.8\u00261\\end{smallmatrix}\\right]\\right)$. The code can be found here Properties of the covariance matrix Symmetric With the definition \\eqref{eq:mvn.1} of the covariance matrix $\\boldsymbol{\\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\\boldsymbol{\\Sigma}$ is symmetric.\nTo prove this property, first off consider a square matrix $\\mathbf{S}$, we have it can be written by \\begin{equation} \\mathbf{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2}+\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2}=\\mathbf{S}_\\text{S}+\\mathbf{S}_\\text{A}, \\end{equation} where \\begin{equation} \\mathbf{S}_\\text{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2},\\hspace{2cm}\\mathbf{S}_\\text{A}=\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2} \\end{equation} It is easily seen that $\\mathbf{S}_\\text{S}$ is symmetric because the $\\{i,j\\}$ element of its equal to the $\\{j,i\\}$ element due to \\begin{equation} (\\mathbf{S}_\\text{S})_{ij}=\\frac{(\\mathbf{S})_{ij}+(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}+(\\mathbf{S})_{ji}}{2}=(\\mathbf{S}_\\text{S})_{ji} \\end{equation} On the other hand, the matrix $\\mathbf{S}_\\text{A}$ is anti-symmetric since \\begin{equation} (\\mathbf{S}_\\text{A})_{ij}=\\frac{(\\mathbf{S})_{ij}-(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}-(\\mathbf{S})_{ji}}{2}=-(\\mathbf{S}_\\text{A})_{ji} \\end{equation} Consider the density of a distribution $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, we have that $\\boldsymbol{\\Sigma}$ is square and so is its inverse $\\boldsymbol{\\Sigma}^{-1}$. Therefore we can express $\\boldsymbol{\\Sigma}^{-1}$ as a sum of a symmetric matrix $\\boldsymbol{\\Sigma}_\\text{S}$ with an anti-symmetric matrix $\\boldsymbol{\\Sigma}_\\text{A}$ \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A} \\end{equation} We have that the density of the distribution is given by \\begin{align} f(\\mathbf{x})\u0026=\\frac{1}{(2\\pi)^{D/2}\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026\\propto\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026=\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}(\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A})(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026\\propto\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}+\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\\right] \\\\ \u0026=\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}\\right] \\end{align} where in the forth step, we have defined $\\mathbf{v}\\doteq\\mathbf{x}-\\boldsymbol{\\mu}$, and where in the fifth-step, the result obtained was due to \\begin{align} \\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\u0026=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i(\\boldsymbol{\\Sigma}_\\text{A})_{ij}\\mathbf{v}_j \\\\ \u0026=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i-(\\boldsymbol{\\Sigma}_\\text{A})_{ji}\\mathbf{v}_j \\\\ \u0026=-\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v} \\end{align} which implies that $\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}=0$.\nThus, when computing the density, the symmetric part of $\\boldsymbol{\\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\\boldsymbol{\\Sigma}^{-1}$ is symmetric, which means that $\\boldsymbol{\\Sigma}$ is also symmetric.\nWith this assumption of symmetry, the covariance matrix $\\boldsymbol{\\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.\nReal eigenvalues Consider an eigenvector, eigenvalue pair $(\\mathbf{v},\\lambda)$ of covariance matrix $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\mathbf{v}\\label{eq:rc.1} \\end{equation} Since $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{D\\times D}$, we have $\\boldsymbol{\\Sigma}=\\overline{\\boldsymbol{\\Sigma}}$. Conjugate both sides of the equation above we have \\begin{equation} \\boldsymbol{\\Sigma}\\overline{\\mathbf{v}}=\\overline{\\lambda}\\overline{\\mathbf{v}},\\label{eq:rc.2} \\end{equation} Since $\\boldsymbol{\\Sigma}$ is symmetric, we have $\\boldsymbol{\\Sigma}=\\boldsymbol{\\Sigma}^\\text{T}$. Taking the transpose of both sides of \\eqref{eq:rc.2} gives us \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\label{eq:rc.3} \\end{equation} Continuing by taking dot product of both sides of \\eqref{eq:rc.3} with $\\mathbf{v}$ lets us obtain \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}\\label{eq:rc.4} \\end{equation} On the other hand, take dot product of $\\overline{\\mathbf{v}}^\\text{T}$ with both sides of \\eqref{eq:rc.1}, we have \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v} \\end{equation} which by \\eqref{eq:rc.4} implies that \\begin{equation} \\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}, \\end{equation} or \\begin{equation} (\\lambda-\\overline{\\lambda})\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=0\\label{eq:rc.5} \\end{equation} Moreover, we have that \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\\sum_{k=1}^{D}a^2+b^2\u003e0 \\end{equation} where we have denoted the complex eigenvector $\\mathbf{v}\\neq\\mathbf{0}$ as \\begin{equation} \\mathbf{v}=(a_1+i b_1,\\ldots,a_D+i b_D)^\\text{T}, \\end{equation} which implies that its complex conjugate $\\overline{\\mathbf{v}}$ can be written by \\begin{equation} \\overline{\\mathbf{v}}=(a_1-i b_1,\\ldots,a_D-i b_D)^\\text{T} \\end{equation} Therefore, by \\eqref{eq:rc.5}, we can claim that \\begin{equation} \\lambda=\\overline{\\lambda} \\end{equation} or in other words, the eigenvalue $\\lambda$ of $\\boldsymbol{\\Sigma}$ is real.\nProjection onto eigenvectors First, we have that eigenvectors $\\mathbf{v}_i$ and $\\mathbf{v}_j$ corresponding to different eigenvalues $\\lambda_i$ and $\\lambda_j$ of $\\boldsymbol{\\Sigma}$ are perpendicular, because \\begin{align} \\lambda_i\\mathbf{v}_i^\\text{T}\\mathbf{v}_j\u0026=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}^\\text{T}\\mathbf{v}_j \\\\ \u0026=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}_j=\\mathbf{v}_i^\\text{T}\\lambda_j\\mathbf{v}_j, \\end{align} which implies that \\begin{equation} (\\lambda_i-\\lambda_j)\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0 \\end{equation} Therefore, $\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0$ since $\\lambda_i\\neq\\lambda_j$.\nHence, for any unit eigenvectors $\\mathbf{q}_i,\\mathbf{q}_j$ of $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\mathbf{q}_i^\\text{T}\\mathbf{q}_j\\begin{cases}1,\u0026\\hspace{0.5cm}\\text{if }i=j \\\\ 0,\u0026\\hspace{0.5cm}\\text{if }i\\neq j\\end{cases} \\end{equation} This allows us to write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\mathbf{Q}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{Q}, \\end{equation} where $\\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\\mathbf{q}_i^\\text{T}$ and $\\boldsymbol{\\Lambda}$ is the diagonal matrix whose $\\{i,i\\}$ element is $\\lambda_i$, as \\begin{equation} \\mathbf{Q}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{q}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{q}_D^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right],\\hspace{2cm}\\boldsymbol{\\Lambda}=\\left[\\begin{matrix}\\lambda_1\u0026\u0026 \\\\ \u0026\\ddots\u0026 \\\\ \u0026\u0026\\lambda_D\\end{matrix}\\right] \\end{equation} Therefore, we can also write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\sum_{i=1}^{D}\\lambda_i\\mathbf{q}_i\\mathbf{q}_i^\\text{T} \\end{equation} Each matrix $\\mathbf{q}_i\\mathbf{q}_i^\\text{T}$ is the projection matrix onto $\\mathbf{q}_i$, then $\\boldsymbol{\\Sigma}$ can be express as a combination of perpendicular projection matrices.\nOther than that, for any eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of the matrix $\\boldsymbol{\\Sigma}$, we have \\begin{align} \\lambda_i\\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\mathbf{q}_i=\\mathbf{q}_i \\end{align} or \\begin{equation} \\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\frac{1}{\\lambda_i}\\mathbf{q}_i, \\end{equation} which implies that each eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of $\\boldsymbol{\\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\\mathbf{q}_i,1/\\lambda_i)$ of $\\boldsymbol{\\Sigma}^{-1}$. Therefore, $\\boldsymbol{\\Sigma}^{-1}$ can also be written by \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\sum_{i=1}^{D}\\frac{1}{\\lambda_i}\\mathbf{q}_i\\mathbf{q}_i^\\text{T}\\label{eq:pec.1} \\end{equation}\nGeometrical interpretation Consider the probability density function of the Gaussian \\eqref{eq:mvn.2}, by the result \\eqref{eq:pec.1}, we have that the functional dependence of the Gaussian on $\\mathbf{x}$ is through the quadratic form \\begin{align} \\Delta^2\u0026=(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) \\\\ \u0026=\\sum_{i=1}^{D}\\frac{y_i^2}{\\lambda_i}, \\end{align} where we have defined \\begin{equation} y_i=\\mathbf{q}_i^\\text{T}(\\mathbf{x}-\\boldsymbol{\\mu}) \\end{equation} Let $\\mathbf{y}=(y_1,\\ldots,y_D)^\\text{T}$ be the vector comprising $y_i$’s together, then we have \\begin{equation} \\mathbf{y}=\\mathbf{Q}(\\mathbf{x}-\\boldsymbol{\\mu}) \\end{equation} Consider the form of the Gaussian distribution in the new coordinate system defined by $y_i$. When changing variable from $\\mathbf{x}$ to $\\mathbf{y}$, firstly we define the Jacobian matrix $\\mathbf{J}$, whose elements are given by \\begin{equation} \\mathbf{J}_{ij}=\\frac{\\partial x_i}{\\partial y_j}=\\mathbf{Q}_{ji}, \\end{equation} which implies that \\begin{equation} \\mathbf{J}=\\mathbf{Q}^\\text{T} \\end{equation} Thus, $\\vert\\mathbf{J}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert=1$ since \\begin{equation} 1=\\vert\\mathbf{I}\\vert=\\vert\\mathbf{Q}^\\text{T}\\mathbf{Q}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert\\vert\\mathbf{Q}\\vert=\\vert\\mathbf{Q}^\\text{T}\\vert \\end{equation} Additionally, by \\eqref{eq:pec.1}, we also have \\begin{equation} \\vert\\boldsymbol{\\Sigma}\\vert^{1/2}=\\left\\vert\\mathbf{Q}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{Q}\\right\\vert^{1/2}=\\left(\\vert\\mathbf{Q}^\\text{T}\\vert\\vert\\boldsymbol{\\Lambda}\\vert\\vert\\mathbf{Q}\\vert\\right)^{1/2}=\\prod_{i=1}^{D}\\lambda_i^{1/2} \\end{equation} Therefore, in the $y_j$ coordinate system, the Gaussian distribution takes the form \\begin{equation} p(\\mathbf{y})=\\mathbf{x}\\vert\\mathbf{J}\\vert=\\prod_{j=1}^{D}\\frac{1}{(2\\pi\\lambda_j)^{1/2}}\\exp\\left(-\\frac{y_j^2}{2\\lambda_j}\\right), \\end{equation} which is the product of $D$ independent univariate Gaussian distributions.\nConditional Gaussian distribution Let $\\mathbf{x}$ be a $D$-dimensional random vector such that $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, and that we partition $\\mathbf{x}$ into two disjoint subsets $\\mathbf{x}_a$ and $\\mathbf{x}_b$ with $\\mathbf{x}_a$ is an $M$-dimensional vector and $\\mathbf{x}_b$ is a $(D-M)$-dimensional vector. \\begin{equation} \\mathbf{x}=\\left[\\begin{matrix}\\mathbf{x}_a \\\\ \\mathbf{x}_b\\end{matrix}\\right] \\end{equation} Along with them, we also define their corresponding means, as a partition of $\\boldsymbol{\\mu}$ \\begin{equation} \\boldsymbol{\\mu}=\\left[\\begin{matrix}\\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b\\end{matrix}\\right] \\end{equation} and their corresponding covariance matrices \\begin{equation} \\boldsymbol{\\Sigma}=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right], \\end{equation} which implies that $\\boldsymbol{\\Sigma}_{ab}=\\boldsymbol{\\Sigma}_{b a}^\\text{T}$.\nAnalogously, we also define the partitioned form of the precision matrix $\\boldsymbol{\\Sigma}^{-1}$ \\begin{equation} \\boldsymbol{\\Lambda}\\doteq\\boldsymbol{\\Sigma}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right], \\end{equation} Thus, we also have that $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$ since $\\boldsymbol{\\Sigma}^{-1}$ or in other words, $\\boldsymbol{\\Lambda}$ is symmetric due to the symmetry of $\\boldsymbol{\\Sigma}$. With these partitions, we can rewrite the functional dependence of the Gaussian \\eqref{eq:mvn.2} on $\\mathbf{x}$ as \\begin{align} \\hspace{-1.2cm}-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\u0026=-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{aa}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b) \\\\ \u0026\\hspace{0.5cm}-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{ba}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{bb}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.1} \\end{align} Consider the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$, which is the distribution of $\\mathbf{x}_a$ given $\\mathbf{x}_b$. Viewing $\\mathbf{x}_b$ as a constant, \\eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ on $\\mathbf{x}_a$, which can be continued to derive as \\begin{align} \u0026-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a+\\boldsymbol{\\Lambda}_{aa}^\\text{T}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_b-\\boldsymbol{\\Lambda}_{ba}^\\text{T}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ba}\\boldsymbol{\\mu}_b\\big)+c \\\\ \u0026\\hspace{3cm}=-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big)+c,\\label{eq:cgd.2} \\end{align} where $c$ is a constant, and we have used the $\\boldsymbol{\\Lambda}_{aa}=\\boldsymbol{\\Lambda}_{aa}^\\text{T}$ and $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$.\nMoreover, we have that the variation part which depends on $\\mathbf{x}$ for any Gaussian $\\mathbf{X}\\sim\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ can be written as a quadratic function of $\\mathbf{x}$ \\begin{equation} -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})=-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}+c,\\label{eq:cgd.3} \\end{equation} where $c$ is a constant. With this observation, and by \\eqref{eq:cgd.2} we have that the conditional distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\\boldsymbol{\\Sigma}_{a\\vert b}$, given by \\begin{equation} \\boldsymbol{\\Sigma}_{a\\vert b}=\\boldsymbol{\\Lambda}_{aa}^{-1},\\label{eq:cgd.4} \\end{equation} and with the corresponding mean vector, denoted as $\\boldsymbol{\\mu}_{a\\vert b}$, given by \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026=\\boldsymbol{\\Sigma}_{a\\vert b}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big) \\\\ \u0026=\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.5} \\end{align} To express the mean $\\boldsymbol{\\mu}_{a\\vert b}$ and the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ of $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ in terms of partition of the covariance matrix $\\boldsymbol{\\Sigma}$ instead of the precision matrix $\\boldsymbol{\\Lambda}$’s, we will be using the identity for the inverse of a partitioned matrix \\begin{align} \\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}\u0026-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right],\\label{eq:cgd.6} \\end{align} where we have defined \\begin{equation} \\mathbf{M}\\doteq(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})^{-1}, \\end{equation} whose inverse $\\mathbf{M}^{-1}$ is called the Schur complement of the matrix $\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]^{-1}$. This identity can be proved by multiplying both sides of \\eqref{eq:cgd.6} with $\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]$ to give \\begin{align} \\mathbf{I}\u0026=\\left[\\begin{matrix}\\mathbf{M}\u0026-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\u0026\\mathbf{M}\\mathbf{B}-\\mathbf{M}\\mathbf{B} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{A}+\\mathbf{D}^{-1}\\mathbf{C}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C}\u0026-\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}+\\mathbf{I}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{I}\u0026\\mathbf{0} \\\\ \\mathbf{D}^{-1}\\mathbf{C}\\big(\\mathbf{I}-\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\\big)\u0026\\mathbf{I}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{I}\u0026\\mathbf{0} \\\\ \\mathbf{0}\u0026\\mathbf{I}\\end{matrix}\\right]=\\mathbf{I}, \\end{align} which claims our argument.\nApplying the identity \\eqref{eq:cgd.6} into the precision matrix $\\boldsymbol{\\Lambda}=\\boldsymbol{\\Sigma}^{-1}$ gives us \\begin{equation} \\hspace{-0.5cm}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\\\ -\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026\\boldsymbol{\\Sigma}_{bb}^{-1}+\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\end{matrix}\\right], \\end{equation} where the Schur complement of $\\mathbf{\\Sigma}^{-1}$ is given by \\begin{equation} \\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1} \\end{equation} Hence, we obtain \\begin{align} \\boldsymbol{\\Lambda}_{aa}\u0026=\\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}, \\\\ \\boldsymbol{\\Lambda}_{ab}\u0026=-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\end{align} Substitute these results into \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ can be rewritten as \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026=\\boldsymbol{\\mu}_a+\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b), \\\\ \\boldsymbol{\\Sigma}_{a\\vert b}\u0026=\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba} \\end{align} It is worth noticing that the mean $\\boldsymbol{\\mu}_{a\\vert b}$ given above is a linear function of $\\mathbf{x}_b$, while the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ is independent of $\\mathbf{x}_b$. This is an example of a linear-Gaussian model.\nMarginal Gaussian distribution Bayes’ theorem for Gaussian variables In this section, we will apply the Bayes’ theorem to find the marginal distribution of $p(\\mathbf{y})$ and conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\\mathbf{x})$ and a conditional Gaussian distribution $p(\\mathbf{y}\\vert\\mathbf{x})$ in which $p(\\mathbf{y}\\vert\\mathbf{x})$ has a mean that is a linear function of $\\mathbf{x}$, and a covariance matrix which is independent of $\\mathbf{x}$, as \\begin{align} p(\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} where $\\mathbf{A},\\mathbf{b}$ are two parameters controlling the means, and $\\boldsymbol{\\Lambda},\\boldsymbol{L}$ are precision matrices.\nIn order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\\mathbf{x},\\mathbf{y})$ by considering the augmented vector \\begin{equation} \\mathbf{z}=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\end{equation} Therefore, we have \\begin{equation} p(\\mathbf{z})=p(\\mathbf{x},\\mathbf{y})=p(\\mathbf{x})p(\\mathbf{y}\\vert\\mathbf{x}) \\end{equation} Taking the natural logarithm of both sides gives us \\begin{align} \\log p(\\mathbf{z})\u0026=\\log p(\\mathbf{x})+\\log p(\\mathbf{y}\\vert\\mathbf{x}) \\\\ \u0026=\\log\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1})+\\log\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}) \\\\ \u0026=-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Lambda}(\\mathbf{x}-\\boldsymbol{\\mu})-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})+c\\label{eq:btg.1} \\end{align} where $c$ is a constant in terms of $\\mathbf{x}$ and $\\mathbf{y}$, i.e., $c$ is independent of $\\mathbf{x},\\mathbf{y}$.\nIt is easily to notice that \\eqref{eq:btg.1} is a quadratic function of the components of $\\mathbf{z}$, which implies that $p(\\mathbf{z})$ is a Gaussian. By \\eqref{eq:cgd.3}, in order to find the covariance matrix of $\\mathbf{z}$, we consider the quadratic terms in \\eqref{eq:btg.1}, which are given by \\begin{align} \u0026-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\\\ \u0026=-\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\big(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\\big)\\mathbf{x}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{y}-\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{A}\\mathbf{x}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{y}\\Big] \\\\ \u0026=-\\frac{1}{2}\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026\\mathbf{L}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\\\ \u0026=-\\frac{1}{2}\\mathbf{z}^\\text{T}\\mathbf{R}\\mathbf{z}, \\end{align} which implies that the precision matrix of $\\mathbf{z}$ is $\\mathbf{R}$, defined as \\begin{equation} \\mathbf{R}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026\\mathbf{L}\\end{matrix}\\right] \\end{equation} Thus, using the identity \\eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution \\begin{equation} \\boldsymbol{\\Sigma}_\\mathbf{z}=\\mathbf{R}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}^{-1}\u0026\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T} \\\\ \\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\u0026\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}\\end{matrix}\\right] \\end{equation} Analogously, by \\eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \\eqref{eq:btg.1}, which are \\begin{align} \\hspace{-0.7cm}\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}+\\boldsymbol{\\mu}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}+(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{b}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\Big]\u0026=\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right] \\end{align} Thus, by \\eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by \\begin{equation} \\boldsymbol{\\mu}_\\mathbf{z}=\\boldsymbol{\\Sigma}_\\mathbf{z}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\mu} \\\\ \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}\\end{matrix}\\right] \\end{equation} Given the mean $\\boldsymbol{\\mu}_\\mathbf{z}$ and the covariance matrix $\\boldsymbol{\\Sigma}_\\mathbf{z}$ of the joint distribution of $\\mathbf{x},\\mathbf{y}$, by \\eqref{22} and \\eqref{23}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\\mathbf{y})$, which are \\begin{align} \\boldsymbol{\\mu}_\\mathbf{y}\u0026=\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}, \\\\ \\boldsymbol{\\Sigma}_\\mathbf{y}\u0026=\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}, \\end{align} and also, by \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$, which are given by \\begin{align} \\boldsymbol{\\mu}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1}\\big(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}\\big) \\\\ \\boldsymbol{\\Sigma}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{align} In Bayesian approach, we can consider $p(\\mathbf{x})$ as a prior distribution over $\\mathbf{x}$, and if $\\mathbf{y}$ is observed, the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ will represents the corresponding posterior distribution over $\\mathbf{x}$.\nRemark\nGiven a marginal Gaussian distribution for $\\mathbf{x}$ and a conditional Gaussian distribution for $\\mathbf{y}$ given $\\mathbf{x}$ in the form \\begin{align} p(\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} the marginal distribution of $\\mathbf{y}$ and the conditional distribution of $\\mathbf{x}$ given $\\mathbf{y}$ are then given by \\begin{align} p(\\mathbf{y})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b},\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}), \\\\ p(\\mathbf{x}\\vert\\mathbf{y})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\Sigma}(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}),\\boldsymbol{\\Sigma}) \\end{align} where \\begin{equation} \\boldsymbol{\\Sigma}=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{equation}\nReferences [1] Joseph K. Blitzstein \u0026 Jessica Hwang. Introduction to Probability.\n[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer New York, NY, 2006.\n[3] Gilbert Strang. Introduction to Linear Algebra, 5th edition, 2016.\nFootnotes The definition of covariance matrix $\\boldsymbol{\\Sigma}$ can be rewritten as \\begin{equation*} \\boldsymbol{\\Sigma}=\\Cov(\\mathbf{X},\\mathbf{X})=\\Var(\\mathbf{X}) \\end{equation*} Let $\\mathbf{z}\\in\\mathbb{R}^D$, we have \\begin{equation*} \\Var(\\mathbf{z}^\\text{T}\\mathbf{X})=\\mathbf{z}^\\text{T}\\Var(\\mathbf{X})\\mathbf{z}=\\mathbf{z}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{z} \\end{equation*} And since $\\Var(\\mathbf{z}^\\text{T}\\mathbf{X})\\geq0$, we also have that $\\mathbf{z}^\\text{T}\\mathbf{\\Sigma}\\mathbf{z}\\geq0$, which proves that $\\boldsymbol{\\Sigma}$ is a positive semi-definite matrix. ↩︎\n","wordCount":"2156","inLanguage":"en","datePublished":"2021-11-22T14:46:00+07:00","dateModified":"2021-11-22T14:46:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/probability-statistics/normal-dist/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/apple-touch-icon.png alt aria-label=logo height=35>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://trunghng.github.io>Home</a>&nbsp;»&nbsp;<a href=https://trunghng.github.io/posts/>Posts</a></div><h1 class=post-title>Gaussian Distribution</h1><div class=post-meta><span title='2021-11-22 14:46:00 +0700 +07'>November 22, 2021</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#gauss-dist>Gaussian (Normal) Distribution</a><ul><li><a href=#std-norm>Standard Normal</a></li></ul></li><li><a href=#mvn>Multivariate Normal Distribution</a><ul><li><a href=#bvn>Bivariate Normal</a></li></ul></li><li><a href=#prop-cov>Properties of the covariance matrix</a><ul><li><a href=#sym-cov>Symmetric</a></li><li><a href=#re-cov>Real eigenvalues</a></li><li><a href=#proj-ev-cov>Projection onto eigenvectors</a></li></ul></li><li><a href=#geo-int>Geometrical interpretation</a></li><li><a href=#cond-gauss-dist>Conditional Gaussian distribution</a></li><li><a href=#marg-gauss-dist>Marginal Gaussian distribution</a></li><li><a href=#bayes-theorem-gauss>Bayes&rsquo; theorem for Gaussian variables</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>The <strong>Gaussian (Normal) distribution</strong> is a continuous distribution with a bell-shaped PDF used widely in statistics due to the <strong>Central Limit theorem</strong>. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.</p></blockquote><p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p><h2 id=gauss-dist>Gaussian (Normal) Distribution<a hidden class=anchor aria-hidden=true href=#gauss-dist>#</a></h2><p>A random variable $X$ is said to be <strong>Gaussian</strong> or to have the <strong>Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$.</p><h3 id=std-norm>Standard Normal<a hidden class=anchor aria-hidden=true href=#std-norm>#</a></h3><p>When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution <strong>Standard Normal</strong>.
\begin{equation}
X\sim\mathcal{N}(0,1)
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2},
\end{equation}
and
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\hspace{0.1cm}dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\hspace{0.1cm}dt
\end{equation}
Below are some illustrations of Normal distribution.</p><figure><img src=/images/normal-dist/normal.png alt="normal distribution" style=display:block;margin-left:auto;margin-right:auto;width:900px;height:380px><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found <a href=https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/gauss-dist.py target=_blank>here</a></figcaption></figure><h2 id=mvn>Multivariate Normal Distribution<a hidden class=anchor aria-hidden=true href=#mvn>#</a></h2><p>A $k$-dimensional random vector $\mathbf{X}=\left(X_1,\dots,X_D\right)^\text{T}$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_DX_D
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_D$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}),
\end{equation}
where
\begin{equation}
\boldsymbol{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^\text{T}=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^\text{T}
\end{equation}
is the $D$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{D\times D}$ with
\begin{equation}
\boldsymbol{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)\label{eq:mvn.1}
\end{equation}
We also have that $\boldsymbol{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>Thus, the PDF of an MVN is defined as
\begin{equation}
f_\mathbf{X}(x_1,\ldots,x_D)=\dfrac{1}{(2\pi)^{D/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]\label{eq:mvn.2}
\end{equation}
With this idea, <em>Standard Normal</em> distribution in multi-dimensional case can be defined as a Gaussian with mean $\boldsymbol{\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\boldsymbol{\Sigma}=\mathbf{I}_{D\times D}$.</p><h3 id=bvn>Bivariate Normal<a hidden class=anchor aria-hidden=true href=#bvn>#</a></h3><p>When the number of dimensions in $\mathbf{X}$, $D=2$, this special case of MVN is referred as <strong>Bivariate Normal (BVN)</strong>.</p><p>An example of an BVN, $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&0.5\\0.8&1\end{smallmatrix}\right]\right)$, is shown as following.</p><figure><img src=/images/normal-dist/bvn.png alt="monte carlo method" style=display:block;margin-left:auto;margin-right:auto;width:750px;height:350px><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&0.5\\0.8&1\end{smallmatrix}\right]\right)$. The code can be found <a href=https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/mvn.py target=_blank>here</a></figcaption></figure><h2 id=prop-cov>Properties of the covariance matrix<a hidden class=anchor aria-hidden=true href=#prop-cov>#</a></h2><h3 id=sym-cov>Symmetric<a hidden class=anchor aria-hidden=true href=#sym-cov>#</a></h3><p>With the definition \eqref{eq:mvn.1} of the covariance matrix $\boldsymbol{\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\boldsymbol{\Sigma}$ is symmetric.</p><p>To prove this property, first off consider a square matrix $\mathbf{S}$, we have it can be written by
\begin{equation}
\mathbf{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2}+\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}=\mathbf{S}_\text{S}+\mathbf{S}_\text{A},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2},\hspace{2cm}\mathbf{S}_\text{A}=\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}
\end{equation}
It is easily seen that $\mathbf{S}_\text{S}$ is symmetric because the $\{i,j\}$ element of its equal to the $\{j,i\}$ element due to
\begin{equation}
(\mathbf{S}_\text{S})_{ij}=\frac{(\mathbf{S})_{ij}+(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}+(\mathbf{S})_{ji}}{2}=(\mathbf{S}_\text{S})_{ji}
\end{equation}
On the other hand, the matrix $\mathbf{S}_\text{A}$ is anti-symmetric since
\begin{equation}
(\mathbf{S}_\text{A})_{ij}=\frac{(\mathbf{S})_{ij}-(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}-(\mathbf{S})_{ji}}{2}=-(\mathbf{S}_\text{A})_{ji}
\end{equation}
Consider the density of a distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, we have that $\boldsymbol{\Sigma}$ is square and so is its inverse $\boldsymbol{\Sigma}^{-1}$. Therefore we can express $\boldsymbol{\Sigma}^{-1}$ as a sum of a symmetric matrix $\boldsymbol{\Sigma}_\text{S}$ with an anti-symmetric matrix $\boldsymbol{\Sigma}_\text{A}$
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A}
\end{equation}
We have that the density of the distribution is given by
\begin{align}
f(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &\propto\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &=\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}(\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A})(\mathbf{x}-\boldsymbol{\mu})\right] \\ &\propto\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}+\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}\right] \\ &=\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}\right]
\end{align}
where in the forth step, we have defined $\mathbf{v}\doteq\mathbf{x}-\boldsymbol{\mu}$, and where in the fifth-step, the result obtained was due to
\begin{align}
\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}&=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i(\boldsymbol{\Sigma}_\text{A})_{ij}\mathbf{v}_j \\ &=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i-(\boldsymbol{\Sigma}_\text{A})_{ji}\mathbf{v}_j \\ &=-\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}
\end{align}
which implies that $\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}=0$.</p><p>Thus, when computing the density, the symmetric part of $\boldsymbol{\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\boldsymbol{\Sigma}^{-1}$ is symmetric, which means that $\boldsymbol{\Sigma}$ is also symmetric.</p><p>With this assumption of symmetry, the covariance matrix $\boldsymbol{\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.</p><h3 id=re-cov>Real eigenvalues<a hidden class=anchor aria-hidden=true href=#re-cov>#</a></h3><p>Consider an eigenvector, eigenvalue pair $(\mathbf{v},\lambda)$ of covariance matrix $\boldsymbol{\Sigma}$, we have
\begin{equation}
\boldsymbol{\Sigma}\mathbf{v}=\lambda\mathbf{v}\label{eq:rc.1}
\end{equation}
Since $\boldsymbol{\Sigma}\in\mathbb{R}^{D\times D}$, we have $\boldsymbol{\Sigma}=\overline{\boldsymbol{\Sigma}}$. Conjugate both sides of the equation above we have
\begin{equation}
\boldsymbol{\Sigma}\overline{\mathbf{v}}=\overline{\lambda}\overline{\mathbf{v}},\label{eq:rc.2}
\end{equation}
Since $\boldsymbol{\Sigma}$ is symmetric, we have $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}^\text{T}$. Taking the transpose of both sides of \eqref{eq:rc.2} gives us
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\label{eq:rc.3}
\end{equation}
Continuing by taking dot product of both sides of \eqref{eq:rc.3} with $\mathbf{v}$ lets us obtain
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}\label{eq:rc.4}
\end{equation}
On the other hand, take dot product of $\overline{\mathbf{v}}^\text{T}$ with both sides of \eqref{eq:rc.1}, we have
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v}
\end{equation}
which by \eqref{eq:rc.4} implies that
\begin{equation}
\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v},
\end{equation}
or
\begin{equation}
(\lambda-\overline{\lambda})\overline{\mathbf{v}}^\text{T}\mathbf{v}=0\label{eq:rc.5}
\end{equation}
Moreover, we have that
\begin{equation}
\overline{\mathbf{v}}^\text{T}\mathbf{v}=\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\sum_{k=1}^{D}a^2+b^2>0
\end{equation}
where we have denoted the complex eigenvector $\mathbf{v}\neq\mathbf{0}$ as
\begin{equation}
\mathbf{v}=(a_1+i b_1,\ldots,a_D+i b_D)^\text{T},
\end{equation}
which implies that its complex conjugate $\overline{\mathbf{v}}$ can be written by
\begin{equation}
\overline{\mathbf{v}}=(a_1-i b_1,\ldots,a_D-i b_D)^\text{T}
\end{equation}
Therefore, by \eqref{eq:rc.5}, we can claim that
\begin{equation}
\lambda=\overline{\lambda}
\end{equation}
or in other words, the eigenvalue $\lambda$ of $\boldsymbol{\Sigma}$ is real.</p><h3 id=proj-ev-cov>Projection onto eigenvectors<a hidden class=anchor aria-hidden=true href=#proj-ev-cov>#</a></h3><p>First, we have that eigenvectors $\mathbf{v}_i$ and $\mathbf{v}_j$ corresponding to different eigenvalues $\lambda_i$ and $\lambda_j$ of $\boldsymbol{\Sigma}$ are perpendicular, because
\begin{align}
\lambda_i\mathbf{v}_i^\text{T}\mathbf{v}_j&=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}^\text{T}\mathbf{v}_j \\ &=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}\mathbf{v}_j=\mathbf{v}_i^\text{T}\lambda_j\mathbf{v}_j,
\end{align}
which implies that
\begin{equation}
(\lambda_i-\lambda_j)\mathbf{v}_i^\text{T}\mathbf{v}_j=0
\end{equation}
Therefore, $\mathbf{v}_i^\text{T}\mathbf{v}_j=0$ since $\lambda_i\neq\lambda_j$.</p><p>Hence, for any unit eigenvectors $\mathbf{q}_i,\mathbf{q}_j$ of $\boldsymbol{\Sigma}$, we have
\begin{equation}
\mathbf{q}_i^\text{T}\mathbf{q}_j\begin{cases}1,&\hspace{0.5cm}\text{if }i=j \\ 0,&\hspace{0.5cm}\text{if }i\neq j\end{cases}
\end{equation}
This allows us to write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{Q}^\text{T}\boldsymbol{\Lambda}\mathbf{Q},
\end{equation}
where $\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\mathbf{q}_i^\text{T}$ and $\boldsymbol{\Lambda}$ is the diagonal matrix whose $\{i,i\}$ element is $\lambda_i$, as
\begin{equation}
\mathbf{Q}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{q}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{q}_D^\text{T}\hspace{0.15cm}-\end{matrix}\right],\hspace{2cm}\boldsymbol{\Lambda}=\left[\begin{matrix}\lambda_1&& \\ &\ddots& \\ &&\lambda_D\end{matrix}\right]
\end{equation}
Therefore, we can also write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\sum_{i=1}^{D}\lambda_i\mathbf{q}_i\mathbf{q}_i^\text{T}
\end{equation}
Each matrix $\mathbf{q}_i\mathbf{q}_i^\text{T}$ is the projection matrix onto $\mathbf{q}_i$, then $\boldsymbol{\Sigma}$ can be express as a combination of perpendicular projection matrices.</p><p>Other than that, for any eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of the matrix $\boldsymbol{\Sigma}$, we have
\begin{align}
\lambda_i\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}\mathbf{q}_i=\mathbf{q}_i
\end{align}
or
\begin{equation}
\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\frac{1}{\lambda_i}\mathbf{q}_i,
\end{equation}
<span id=precision-eigenvalue>which implies that each eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of $\boldsymbol{\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\mathbf{q}_i,1/\lambda_i)$ of $\boldsymbol{\Sigma}^{-1}$. Therefore, $\boldsymbol{\Sigma}^{-1}$ can also be written by</span>
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_i}\mathbf{q}_i\mathbf{q}_i^\text{T}\label{eq:pec.1}
\end{equation}</p><h2 id=geo-int>Geometrical interpretation<a hidden class=anchor aria-hidden=true href=#geo-int>#</a></h2><p>Consider the probability density function of the Gaussian \eqref{eq:mvn.2}, by the result \eqref{eq:pec.1}, we have that the functional dependence of the Gaussian on $\mathbf{x}$ is through the quadratic form
\begin{align}
\Delta^2&=(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \\ &=\sum_{i=1}^{D}\frac{y_i^2}{\lambda_i},
\end{align}
where we have defined
\begin{equation}
y_i=\mathbf{q}_i^\text{T}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}
Let $\mathbf{y}=(y_1,\ldots,y_D)^\text{T}$ be the vector comprising $y_i$&rsquo;s together, then we have
\begin{equation}
\mathbf{y}=\mathbf{Q}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}
Consider the form of the Gaussian distribution in the new coordinate system defined by $y_i$. When changing variable from $\mathbf{x}$ to $\mathbf{y}$, firstly we define the <strong>Jacobian matrix</strong> $\mathbf{J}$, whose elements are given by
\begin{equation}
\mathbf{J}_{ij}=\frac{\partial x_i}{\partial y_j}=\mathbf{Q}_{ji},
\end{equation}
which implies that
\begin{equation}
\mathbf{J}=\mathbf{Q}^\text{T}
\end{equation}
Thus, $\vert\mathbf{J}\vert=\vert\mathbf{Q}^\text{T}\vert=1$ since
\begin{equation}
1=\vert\mathbf{I}\vert=\vert\mathbf{Q}^\text{T}\mathbf{Q}\vert=\vert\mathbf{Q}^\text{T}\vert\vert\mathbf{Q}\vert=\vert\mathbf{Q}^\text{T}\vert
\end{equation}
Additionally, by \eqref{eq:pec.1}, we also have
\begin{equation}
\vert\boldsymbol{\Sigma}\vert^{1/2}=\left\vert\mathbf{Q}^\text{T}\boldsymbol{\Lambda}\mathbf{Q}\right\vert^{1/2}=\left(\vert\mathbf{Q}^\text{T}\vert\vert\boldsymbol{\Lambda}\vert\vert\mathbf{Q}\vert\right)^{1/2}=\prod_{i=1}^{D}\lambda_i^{1/2}
\end{equation}
Therefore, in the $y_j$ coordinate system, the Gaussian distribution takes the form
\begin{equation}
p(\mathbf{y})=\mathbf{x}\vert\mathbf{J}\vert=\prod_{j=1}^{D}\frac{1}{(2\pi\lambda_j)^{1/2}}\exp\left(-\frac{y_j^2}{2\lambda_j}\right),
\end{equation}
which is the product of $D$ independent univariate Gaussian distributions.</p><h2 id=cond-gauss-dist>Conditional Gaussian distribution<a hidden class=anchor aria-hidden=true href=#cond-gauss-dist>#</a></h2><p>Let $\mathbf{x}$ be a $D$-dimensional random vector such that $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, and that we partition $\mathbf{x}$ into two disjoint subsets $\mathbf{x}_a$ and $\mathbf{x}_b$ with $\mathbf{x}_a$ is an $M$-dimensional vector and $\mathbf{x}_b$ is a $(D-M)$-dimensional vector.
\begin{equation}
\mathbf{x}=\left[\begin{matrix}\mathbf{x}_a \\ \mathbf{x}_b\end{matrix}\right]
\end{equation}
Along with them, we also define their corresponding means, as a partition of $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}=\left[\begin{matrix}\boldsymbol{\mu}_a \\ \boldsymbol{\mu}_b\end{matrix}\right]
\end{equation}
and their corresponding covariance matrices
\begin{equation}
\boldsymbol{\Sigma}=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&\boldsymbol{\Sigma}_{bb}\end{matrix}\right],
\end{equation}
which implies that $\boldsymbol{\Sigma}_{ab}=\boldsymbol{\Sigma}_{b a}^\text{T}$.</p><p>Analogously, we also define the partitioned form of the precision matrix $\boldsymbol{\Sigma}^{-1}$
\begin{equation}
\boldsymbol{\Lambda}\doteq\boldsymbol{\Sigma}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&\boldsymbol{\Lambda}_{bb}\end{matrix}\right],
\end{equation}
Thus, we also have that $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$ since $\boldsymbol{\Sigma}^{-1}$ or in other words, $\boldsymbol{\Lambda}$ is symmetric due to the symmetry of $\boldsymbol{\Sigma}$.
With these partitions, we can rewrite the functional dependence of the Gaussian \eqref{eq:mvn.2} on $\mathbf{x}$ as
\begin{align}
\hspace{-1.2cm}-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})&=-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{aa}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b) \\ &\hspace{0.5cm}-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{ba}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{bb}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.1}
\end{align}
Consider the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$, which is the distribution of $\mathbf{x}_a$ given $\mathbf{x}_b$. Viewing $\mathbf{x}_b$ as a constant, \eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$ on $\mathbf{x}_a$, which can be continued to derive as
\begin{align}
&-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\frac{1}{2}\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{aa}^\text{T}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}\mathbf{x}_b+\boldsymbol{\Lambda}_{ab}\boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{ba}^\text{T}\mathbf{x}_b+\boldsymbol{\Lambda}_{ba}\boldsymbol{\mu}_b\big)+c \\ &\hspace{3cm}=-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big)+c,\label{eq:cgd.2}
\end{align}
where $c$ is a constant, and we have used the $\boldsymbol{\Lambda}_{aa}=\boldsymbol{\Lambda}_{aa}^\text{T}$ and $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$.</p><p>Moreover, we have that the variation part which depends on $\mathbf{x}$ for any Gaussian $\mathbf{X}\sim\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Sigma})$ can be written as a quadratic function of $\mathbf{x}$
\begin{equation}
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})=-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}+c,\label{eq:cgd.3}
\end{equation}
where $c$ is a constant. With this observation, and by \eqref{eq:cgd.2} we have that the conditional distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\boldsymbol{\Sigma}_{a\vert b}$, given by
\begin{equation}
\boldsymbol{\Sigma}_{a\vert b}=\boldsymbol{\Lambda}_{aa}^{-1},\label{eq:cgd.4}
\end{equation}
and with the corresponding mean vector, denoted as $\boldsymbol{\mu}_{a\vert b}$, given by
\begin{align}
\boldsymbol{\mu}_{a\vert b}&=\boldsymbol{\Sigma}_{a\vert b}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big) \\ &=\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{aa}^{-1}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.5}
\end{align}
To express the mean $\boldsymbol{\mu}_{a\vert b}$ and the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ of $p(\mathbf{x}_a\vert\mathbf{x}_b)$ in terms of partition of the covariance matrix $\boldsymbol{\Sigma}$ instead of the precision matrix $\boldsymbol{\Lambda}$&rsquo;s, we will be using the identity for the inverse of a partitioned matrix
\begin{align}
\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}&-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right],\label{eq:cgd.6}
\end{align}
where we have defined
\begin{equation}
\mathbf{M}\doteq(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1},
\end{equation}
whose inverse $\mathbf{M}^{-1}$ is called the <strong>Schur complement</strong> of the matrix $\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]^{-1}$. This identity can be proved by multiplying both sides of \eqref{eq:cgd.6} with $\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]$ to give
\begin{align}
\mathbf{I}&=\left[\begin{matrix}\mathbf{M}&-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right]\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})&\mathbf{M}\mathbf{B}-\mathbf{M}\mathbf{B} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{A}+\mathbf{D}^{-1}\mathbf{C}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\mathbf{C}&-\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}+\mathbf{I}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{I}&\mathbf{0} \\ \mathbf{D}^{-1}\mathbf{C}\big(\mathbf{I}-\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})\big)&\mathbf{I}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{I}&\mathbf{0} \\ \mathbf{0}&\mathbf{I}\end{matrix}\right]=\mathbf{I},
\end{align}
which claims our argument.</p><p>Applying the identity \eqref{eq:cgd.6} into the precision matrix $\boldsymbol{\Lambda}=\boldsymbol{\Sigma}^{-1}$ gives us
\begin{equation}
\hspace{-0.5cm}\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&\boldsymbol{\Lambda}_{bb}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&\boldsymbol{\Sigma}_{bb}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}_\boldsymbol{\Sigma}&-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1} \\ -\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}&\boldsymbol{\Sigma}_{bb}^{-1}+\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\end{matrix}\right],
\end{equation}
where the Schur complement of $\mathbf{\Sigma}^{-1}$ is given by
\begin{equation}
\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}
\end{equation}
Hence, we obtain
\begin{align}
\boldsymbol{\Lambda}_{aa}&=\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}, \\ \boldsymbol{\Lambda}_{ab}&=-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}
\end{align}
Substitute these results into \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ can be rewritten as
\begin{align}
\boldsymbol{\mu}_{a\vert b}&=\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}(\mathbf{x}_b-\boldsymbol{\mu}_b), \\ \boldsymbol{\Sigma}_{a\vert b}&=\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}
\end{align}
It is worth noticing that the mean $\boldsymbol{\mu}_{a\vert b}$ given above is a linear function of $\mathbf{x}_b$, while the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ is independent of $\mathbf{x}_b$. This is an example of a <strong>linear-Gaussian model</strong>.</p><h2 id=marg-gauss-dist>Marginal Gaussian distribution<a hidden class=anchor aria-hidden=true href=#marg-gauss-dist>#</a></h2><h2 id=bayes-theorem-gauss>Bayes&rsquo; theorem for Gaussian variables<a hidden class=anchor aria-hidden=true href=#bayes-theorem-gauss>#</a></h2><p>In this section, we will apply the Bayes&rsquo; theorem to find the marginal distribution of $p(\mathbf{y})$ and conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\mathbf{x})$ and a conditional Gaussian distribution $p(\mathbf{y}\vert\mathbf{x})$ in which $p(\mathbf{y}\vert\mathbf{x})$ has a mean that is a linear function of $\mathbf{x}$, and a covariance matrix which is independent of $\mathbf{x}$, as
\begin{align}
p(\mathbf{x})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
where $\mathbf{A},\mathbf{b}$ are two parameters controlling the means, and $\boldsymbol{\Lambda},\boldsymbol{L}$ are precision matrices.</p><p>In order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\mathbf{x},\mathbf{y})$ by considering the augmented vector
\begin{equation}
\mathbf{z}=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]
\end{equation}
Therefore, we have
\begin{equation}
p(\mathbf{z})=p(\mathbf{x},\mathbf{y})=p(\mathbf{x})p(\mathbf{y}\vert\mathbf{x})
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{z})&=\log p(\mathbf{x})+\log p(\mathbf{y}\vert\mathbf{x}) \\ &=\log\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1})+\log\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}) \\ &=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\mu})-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})+c\label{eq:btg.1}
\end{align}
where $c$ is a constant in terms of $\mathbf{x}$ and $\mathbf{y}$, i.e., $c$ is independent of $\mathbf{x},\mathbf{y}$.</p><p>It is easily to notice that \eqref{eq:btg.1} is a quadratic function of the components of $\mathbf{z}$, which implies that $p(\mathbf{z})$ is a Gaussian. By \eqref{eq:cgd.3}, in order to find the covariance matrix of $\mathbf{z}$, we consider the quadratic terms in \eqref{eq:btg.1}, which are given by
\begin{align}
&-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Lambda}\mathbf{x}-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \\ &=-\frac{1}{2}\Big[\mathbf{x}^\text{T}\big(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}\big)\mathbf{x}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{y}-\mathbf{y}^\text{T}\mathbf{L}\mathbf{A}\mathbf{x}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{y}\Big] \\ &=-\frac{1}{2}\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&\mathbf{L}\end{matrix}\right]\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right] \\ &=-\frac{1}{2}\mathbf{z}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
which implies that the precision matrix of $\mathbf{z}$ is $\mathbf{R}$, defined as
\begin{equation}
\mathbf{R}=\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&\mathbf{L}\end{matrix}\right]
\end{equation}
Thus, using the identity \eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution
\begin{equation}
\boldsymbol{\Sigma}_\mathbf{z}=\mathbf{R}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}^{-1}&\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T} \\ \mathbf{A}\boldsymbol{\Lambda}^{-1}&\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}\end{matrix}\right]
\end{equation}
Analogously, by \eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \eqref{eq:btg.1}, which are
\begin{align}
\hspace{-0.7cm}\frac{1}{2}\Big[\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}+\boldsymbol{\mu}^\text{T}\boldsymbol{\Lambda}\mathbf{x}+(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}\mathbf{b}+\mathbf{b}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \Big]&=\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{b}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{b} \\ &=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]
\end{align}
Thus, by \eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by
\begin{equation}
\boldsymbol{\mu}_\mathbf{z}=\boldsymbol{\Sigma}_\mathbf{z}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\mu} \\ \mathbf{A}\boldsymbol{\mu}+\mathbf{b}\end{matrix}\right]
\end{equation}
Given the mean $\boldsymbol{\mu}_\mathbf{z}$ and the covariance matrix $\boldsymbol{\Sigma}_\mathbf{z}$ of the joint distribution of $\mathbf{x},\mathbf{y}$, by \eqref{22} and \eqref{23}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\mathbf{y})$, which are
\begin{align}
\boldsymbol{\mu}_\mathbf{y}&=\mathbf{A}\boldsymbol{\mu}+\mathbf{b}, \\ \boldsymbol{\Sigma}_\mathbf{y}&=\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T},
\end{align}
and also, by \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$, which are given by
\begin{align}
\boldsymbol{\mu}_{\mathbf{x}\vert\mathbf{y}}&=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}\big(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}\big) \\ \boldsymbol{\Sigma}_{\mathbf{x}\vert\mathbf{y}}&=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{align}
In Bayesian approach, we can consider $p(\mathbf{x})$ as a prior distribution over $\mathbf{x}$, and if $\mathbf{y}$ is observed, the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ will represents the corresponding posterior distribution over $\mathbf{x}$.</p><p><span id=marg-cond-gaussian><strong>Remark</strong></span><br>Given a marginal Gaussian distribution for $\mathbf{x}$ and a conditional Gaussian distribution for $\mathbf{y}$ given $\mathbf{x}$ in the form
\begin{align}
p(\mathbf{x})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
the marginal distribution of $\mathbf{y}$ and the conditional distribution of $\mathbf{x}$ given $\mathbf{y}$ are then given by
\begin{align}
p(\mathbf{y})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\boldsymbol{\mu}+\mathbf{b},\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}), \\ p(\mathbf{x}\vert\mathbf{y})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\Sigma}(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}),\boldsymbol{\Sigma})
\end{align}
where
\begin{equation}
\boldsymbol{\Sigma}=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{equation}</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Joseph K. Blitzstein & Jessica Hwang. <a href=https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573>Introduction to Probability</a>.</p><p>[2] Christopher M. Bishop. <a href=https://link.springer.com/book/9780387310732>Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p><p>[3] Gilbert Strang. <a href=http://math.mit.edu/~gs/linearalgebra/>Introduction to Linear Algebra, 5th edition</a>, 2016.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>The definition of covariance matrix $\boldsymbol{\Sigma}$ can be rewritten as
\begin{equation*}
\boldsymbol{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation*}
Let $\mathbf{z}\in\mathbb{R}^D$, we have
\begin{equation*}
\Var(\mathbf{z}^\text{T}\mathbf{X})=\mathbf{z}^\text{T}\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^\text{T}\boldsymbol{\Sigma}\mathbf{z}
\end{equation*}
And since $\Var(\mathbf{z}^\text{T}\mathbf{X})\geq0$, we also have that $\mathbf{z}^\text{T}\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\boldsymbol{\Sigma}$ is a positive semi-definite matrix.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/mathematics/>mathematics</a></li><li><a href=https://trunghng.github.io/tags/probability-statistics/>probability-statistics</a></li><li><a href=https://trunghng.github.io/tags/normal-distribution/>normal-distribution</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/optimization/cvx-sets-funcs/><span class=title>« Prev</span><br><span>Convex sets, convex functions</span></a>
<a class=next href=https://trunghng.github.io/posts/calculus/power-series/><span class=title>Next »</span><br><span>Power Series</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on twitter" href="https://twitter.com/intent/tweet/?text=Gaussian%20Distribution&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f&hashtags=mathematics%2cprobability-statistics%2cnormal-distribution"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f&title=Gaussian%20Distribution&summary=Gaussian%20Distribution&source=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f&title=Gaussian%20Distribution"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on whatsapp" href="https://api.whatsapp.com/send?text=Gaussian%20Distribution%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution on telegram" href="https://telegram.me/share/url?text=Gaussian%20Distribution&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fnormal-dist%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>