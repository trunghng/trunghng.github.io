<!doctype html><html lang=en dir=auto><head><meta name=generator content="Hugo 0.120.3"><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="Blog,Portfolio,hugo-PaperMod"><meta name=description content="ExampleSite description"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://trunghng.github.io/index.xml><link rel=alternate type=application/json href=https://trunghng.github.io/index.json><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Littleroot"><meta property="og:description" content="ExampleSite description"><meta property="og:type" content="website"><meta property="og:url" content="https://trunghng.github.io/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Littleroot"><meta name=twitter:description content="ExampleSite description"><script type=application/ld+json>{"@context":"https://schema.org","@type":"Organization","name":"Littleroot","url":"https://trunghng.github.io","description":"ExampleSite description","thumbnailUrl":"https://trunghng.github.io/images/favicon/favicon.ico","sameAs":["https://github.com/trunghng","mailto:trung.skipper@gmail.com"]}</script></head><body class=list id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-entry><header class=entry-header><h2>Likelihood Ratio Policy Gradient via Importance Sampling</h2></header><div class=entry-content><p>Connection between Likelihood ratio policy gradient method and Importance sampling method.
...</p></div><footer class=entry-footer><span title='2022-05-25 15:26:00 +0700 +07'>May 25, 2022</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Likelihood Ratio Policy Gradient via Importance Sampling" href=https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/></a></article><article class=post-entry><header class=entry-header><h2>Planning & Learning</h2></header><div class=entry-content><p>Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.
...</p></div><footer class=entry-footer><span title='2022-05-19 14:09:00 +0700 +07'>May 19, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Planning & Learning" href=https://trunghng.github.io/posts/reinforcement-learning/planning-learning/></a></article><article class=post-entry><header class=entry-header><h2>Policy Gradient Theorem</h2></header><div class=entry-content><p>So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.
...</p></div><footer class=entry-footer><span title='2022-05-04 14:00:00 +0700 +07'>May 4, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Policy Gradient Theorem" href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/></a></article><article class=post-entry><header class=entry-header><h2>The Exponential Family, Generalized Linear Models</h2></header><div class=entry-content><p>Notes on Exponential Family & Generalized Linear Models.
...</p></div><footer class=entry-footer><span title='2022-04-04 14:00:00 +0700 +07'>April 4, 2022</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to The Exponential Family, Generalized Linear Models" href=https://trunghng.github.io/posts/machine-learning/exponential-family-glim/></a></article><article class=post-entry><header class=entry-header><h2>Eligible Traces</h2></header><div class=entry-content><p>Beside $n$-step TD methods, there is another mechanism called eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
...</p></div><footer class=entry-footer><span title='2022-03-13 14:11:00 +0700 +07'>March 13, 2022</span>&nbsp;·&nbsp;25 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Eligible Traces" href=https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/></a></article><article class=post-entry><header class=entry-header><h2>Function Approximation</h2></header><div class=entry-content><p>All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.
...</p></div><footer class=entry-footer><span title='2022-02-11 15:26:00 +0700 +07'>February 11, 2022</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Function Approximation" href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/></a></article><article class=post-entry><header class=entry-header><h2>Temporal-Difference Learning</h2></header><div class=entry-content><p>So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.
...</p></div><footer class=entry-footer><span title='2022-01-31 16:55:00 +0700 +07'>January 31, 2022</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Trung H. Nguyen</footer><a class=entry-link aria-label="post link to Temporal-Difference Learning" href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://trunghng.github.io/page/3/>«&nbsp;Prev&nbsp;
</a><a class=next href=https://trunghng.github.io/page/5/>Next&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>