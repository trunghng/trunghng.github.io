<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>deep-reinforcement-learning on Littleroot</title>
    <link>https://trunghng.github.io/tags/deep-reinforcement-learning/</link>
    <description>Recent content in deep-reinforcement-learning on Littleroot</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 22 Sep 2024 17:54:43 +0700</lastBuildDate><atom:link href="https://trunghng.github.io/tags/deep-reinforcement-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Model-based RL with latent variable models</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</link>
      <pubDate>Sun, 22 Sep 2024 17:54:43 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Model-based RL methods that learn latent-variable models instead of trying to predict dynamics models in the observed space. The learned world model then can be used in planning effectively rather than being less efficiently, for instance in visual-based tasks, generating images for future time steps and feed them back into the model to predict the next ones, which requires more computation.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>MuZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/muzero/</link>
      <pubDate>Tue, 02 Jan 2024 11:52:40 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/muzero/</guid>
      <description>&lt;h2 id=&#34;muzero&#34;&gt;MuZero&lt;/h2&gt;
&lt;p&gt;Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k&amp;gt;0$.&lt;br&gt;
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:&lt;/p&gt;
&lt;ul class=&#39;number-list&#39;&gt;
	&lt;li&gt;
		the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$;
	&lt;/li&gt;
	&lt;li&gt;
		the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$;
	&lt;/li&gt;
	&lt;li&gt;
		the immediate reward $r_t^k\approx u_{t+k}$,
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AlphaGo, AlphaGo Zero, AlphaZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</link>
      <pubDate>Tue, 17 Oct 2023 10:23:22 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Model-based RL methods that use Monte Carlo Tree Search for planning and ultilize self-play mechanism for training.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- for Milu --&gt;</description>
    </item>
    
    <item>
      <title>Multi-agent Deep Deterministic Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</link>
      <pubDate>Thu, 25 May 2023 15:25:54 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on MADDPG.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Maximum Entropy Reinforcement Learning via Soft Q-learning &amp; Soft Actor-Critic</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</link>
      <pubDate>Tue, 27 Dec 2022 13:46:09 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Entropy-Regularized Reinforcement Learning via SQL &amp;amp; SAC&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Trust Region Policy Optimization</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/trpo/</link>
      <pubDate>Wed, 23 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/trpo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;A model-free RL algorithm that ensures stable and efficient policy updates by optimizing within a trust region, limiting the step size to prevent drastic policy changes and improve convergence.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deep Q-learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</link>
      <pubDate>Fri, 18 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on DQN and its variants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</link>
      <pubDate>Thu, 06 Oct 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Policy gradient methods.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>
