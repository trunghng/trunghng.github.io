<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Littleroot</title>
    <link>https://trunghng.github.io/</link>
    <description>Recent content on Littleroot</description>
    <image>
      <title>Littleroot</title>
      <url>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 13 Oct 2024 16:09:06 +0700</lastBuildDate><atom:link href="https://trunghng.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Model-based RL with latent-variable models</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</link>
      <pubDate>Sun, 22 Sep 2024 17:54:43 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Model-based RL methods that learn latent-variable models instead of trying to predict dynamics models in the observed space.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Graph generation with predefined chromatic number</title>
      <link>https://trunghng.github.io/posts/graph-theory/leighton-graph-gen/</link>
      <pubDate>Sun, 19 May 2024 17:37:18 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/graph-theory/leighton-graph-gen/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Generating predefined-chromatic-number graphs with Leighton&amp;rsquo;s algorithm.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Variational Autoencoder</title>
      <link>https://trunghng.github.io/posts/machine-learning/vae/</link>
      <pubDate>Tue, 30 Apr 2024 16:28:52 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/vae/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;An autoencoder that differs from others (with deterministic encoder) by mapping each input $\mathbf{x}$ to a distribution over the possible values of the latent representation $\mathbf{z}$ from which $\mathbf{x}$ could have been generated.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Graph Representation Learning</title>
      <link>https://trunghng.github.io/posts/machine-learning/graph-representation-learning/</link>
      <pubDate>Tue, 16 Apr 2024 14:43:59 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/graph-representation-learning/</guid>
      <description>&lt;h2 id=&#34;traditional-feature-based-approaches&#34;&gt;Traditional feature-based approaches&lt;/h2&gt;
&lt;p&gt;In traditional Machine Learning methods, we first represent our data points (nodes, links, entire graphs) as vectors of (hand-designed) features and then train a classical ML model (random forest, SVM, neural network) on top of that.&lt;/p&gt;
&lt;p&gt;Let us consider a network $\mathcal{G}=(\mathcal{V},\mathcal{E})$, where $\mathcal{V}$ is the set of nodes and $\mathcal{E}$ is the set of edges between these nodes. Also let $\mathbf{A}\in\mathbb{R}^{\vert\mathcal{V}\vert\times\vert\mathcal{V}\vert}$ be the adjacency matrix of $\mathcal{G}$, i.e. for $u,v\in\mathcal{V}$
\begin{equation}
\mathbf{A}_{u,v}=\begin{cases}1&amp;amp;\text{if }(u,v)\in\mathcal{E} \\ 0&amp;amp;\text{if }(u,v)\notin\mathcal{E}\end{cases}
\end{equation}
In some graph with weighted edges, entries of $\mathbf{A}$ will be arbitrary real-values rather than $0$ or $1$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>MuZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/muzero/</link>
      <pubDate>Tue, 02 Jan 2024 11:52:40 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/muzero/</guid>
      <description>&lt;h2 id=&#34;muzero&#34;&gt;MuZero&lt;/h2&gt;
&lt;p&gt;Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k&amp;gt;0$.&lt;br&gt;
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:&lt;/p&gt;
&lt;ul class=&#39;number-list&#39;&gt;
	&lt;li&gt;
		the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$;
	&lt;/li&gt;
	&lt;li&gt;
		the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$;
	&lt;/li&gt;
	&lt;li&gt;
		the immediate reward $r_t^k\approx u_{t+k}$,
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AlphaZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</link>
      <pubDate>Tue, 17 Oct 2023 10:23:22 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</guid>
      <description>&lt;!-- for Milu --&gt;
&lt;h2 id=&#34;alphago&#34;&gt;AlphaGo&lt;/h2&gt;
&lt;p&gt;The training pipeline used in &lt;strong&gt;AlphaGo&lt;/strong&gt; can be divided into following stages:&lt;/p&gt;
&lt;ul class=&#39;number-list&#39;&gt;
	&lt;li&gt;
		Using a dataset of human experts positions, a &lt;b&gt;supervised learning (SL) policy network&lt;/b&gt; $p_\sigma$ and, a &lt;b&gt;rollout policy&lt;/b&gt; $p_\pi$, which can sample actions rapidly, are trained by classification to predict player moves.
	&lt;/li&gt;
	&lt;li&gt;
		Initializing with the &lt;b&gt;SL policy network&lt;/b&gt; $p_\sigma$, it uses policy gradient to train a &lt;b&gt;reinforcement learning (RL) policy network&lt;/b&gt; $p_\rho$ with the goal to maximize the winning outcome against previous versions of the policy network. This process generates a dataset of self-play games.
	&lt;/li&gt;
	&lt;li&gt;
		Via the dataset of self-play moves, a &lt;b&gt;value network&lt;/b&gt; $v_\theta$ is trained by regression to predict the expected outcome (win or lose).
	&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
	&lt;img src=&#34;https://trunghng.github.io/images/alphazero/alphago-training-pipeline.png&#34; alt=&#34;AlphaGo training pipeline&#34; width=&#34;80%&#34; height=&#34;80%&#34;/&gt;
	&lt;figcaption style=&#34;text-align: center&#34;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: (taken from &lt;a href=&#39;#alphago-paper&#39;&gt;AlphaGo paper&lt;/a&gt;) &lt;b&gt;AlphaGo neural network training pipeline&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;sl-policy-network-p_sigma-rollout-network-p_pi&#34;&gt;SL policy network $p_\sigma$, rollout network $p_\pi$&lt;/h3&gt;
&lt;p&gt;The policy network $p_\sigma(a\vert s)$ takes as its input a simple representation of the board state $s$ and outputs a probability distribution over all legal moves $a$. The network is trained to maximize the likelihood of the human move $a$ selected in state $s$ by using SGA
\begin{equation}
\Delta\sigma\propto\frac{\partial\log p_\sigma(a\vert s)}{\partial\sigma}
\end{equation}
The rollout network $p_\pi(a\vert s)$ is trained using a linear softmax of small pattern features. This network is less accurate but faster selecting action than $p_\sigma$.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Multi-agent Deep Deterministic Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</link>
      <pubDate>Thu, 25 May 2023 15:25:54 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maddpg/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on MADDPG.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>GAN</title>
      <link>https://trunghng.github.io/posts/machine-learning/gan/</link>
      <pubDate>Mon, 01 May 2023 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/gan/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Generative Adversarial Networks.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Learning</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-learning/</link>
      <pubDate>Sun, 19 Feb 2023 17:23:56 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Learning in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Inference</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-inference/</link>
      <pubDate>Thu, 02 Feb 2023 15:51:13 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-inference/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Inference in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Categorical Reparameterization with Gumbel-Softmax &amp; Concrete Distribution</title>
      <link>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</link>
      <pubDate>Mon, 02 Jan 2023 13:49:15 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on using Gumbel-Softmax &amp;amp; Concrete Distribution in Categorical sampling&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Maximum Entropy Reinforcement Learning via Soft Q-learning &amp; Soft Actor-Critic</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</link>
      <pubDate>Tue, 27 Dec 2022 13:46:09 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Entropy-Regularized Reinforcement Learning via SQL &amp;amp; SAC&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Probabilistic Graphical Models - Representation</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-representation/</link>
      <pubDate>Sat, 10 Dec 2022 17:55:57 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-representation/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Representation in PGMs.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deterministic Policy Gradients</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</link>
      <pubDate>Fri, 02 Dec 2022 19:26:44 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Deterministic Policy Gradient algorithms&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Trust Region Policy Optimization</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/trpo/</link>
      <pubDate>Wed, 23 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/trpo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on policy optimization using trust region method.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deep Q-learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</link>
      <pubDate>Fri, 18 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on DQN and its variants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Natural Evolution Strategies</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/nes/</link>
      <pubDate>Fri, 07 Oct 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/nes/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Natural Evolution Strategies&lt;/strong&gt;, or &lt;strong&gt;NES&lt;/strong&gt;, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</link>
      <pubDate>Thu, 06 Oct 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Policy gradient methods.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>CMA Evolution Strategy</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/cma-es/</link>
      <pubDate>Wed, 14 Sep 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/cma-es/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Covariance Matrix Adaptation Evolution Strategy&lt;/strong&gt; (&lt;strong&gt;CMA-ES&lt;/strong&gt;) is an evolutionary algorithm for complex non-linear non-convex blackbox optimization problems in continuous domain.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - the Lebesgue integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</link>
      <pubDate>Sun, 21 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;(Temporary being stopped) Note III of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Linear Models</title>
      <link>https://trunghng.github.io/posts/machine-learning/glm/</link>
      <pubDate>Sat, 13 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/glm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on using linear models in regression and classification.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - Lebesgue measure</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</link>
      <pubDate>Sun, 03 Jul 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note II of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Read-through: Measure theory - Elementary measure, Jordan measure &amp; the Riemann integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</link>
      <pubDate>Thu, 16 Jun 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note I of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Likelihood Ratio Policy Gradient via Importance Sampling</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</link>
      <pubDate>Wed, 25 May 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Connection between Likelihood ratio policy gradient method and Importance sampling method.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Planning &amp; Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</link>
      <pubDate>Thu, 19 May 2022 14:09:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;dynamic programming (DP) method&lt;/a&gt; in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;Monte Carlo methods&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/&#34;&gt;temporal-difference learning&lt;/a&gt;, the models are unnecessary. Such methods with requirement of a model like the case of DP is called &lt;strong&gt;model-based&lt;/strong&gt;, while methods without using a model is called &lt;strong&gt;model-free&lt;/strong&gt;. Model-based methods primarily rely on &lt;strong&gt;planning&lt;/strong&gt;; and model-free methods, on the other hand, primarily rely on &lt;strong&gt;learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient Theorem</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</link>
      <pubDate>Wed, 04 May 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a &lt;strong&gt;parameterized policy&lt;/strong&gt;, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called &lt;strong&gt;policy gradient methods&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>The Exponential Family, Generalized Linear Models</title>
      <link>https://trunghng.github.io/posts/machine-learning/exponential-family-glim/</link>
      <pubDate>Mon, 04 Apr 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/exponential-family-glim/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Exponential Family &amp;amp; Generalized Linear Models.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Eligible Traces</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</link>
      <pubDate>Sun, 13 Mar 2022 14:11:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Beside &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td&#34;&gt;$n$-step TD&lt;/a&gt; methods, there is another mechanism called &lt;strong&gt;eligible traces&lt;/strong&gt; that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Function Approximation</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</link>
      <pubDate>Fri, 11 Feb 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Temporal-Difference Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</link>
      <pubDate>Mon, 31 Jan 2022 16:55:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in this &lt;a href=&#34;https://trunghng.github.io/tags/my-rl/&#34;&gt;series&lt;/a&gt;, we have gone through the ideas of &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;dynamic programming&lt;/strong&gt; (DP)&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;&lt;strong&gt;Monte Carlo&lt;/strong&gt;&lt;/a&gt;. What will happen if we combine these ideas together? &lt;strong&gt;Temporal-difference (TD) learning&lt;/strong&gt; is our answer.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Gaussian Distribution &amp; Gaussian Network Models</title>
      <link>https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/</link>
      <pubDate>Mon, 22 Nov 2021 14:46:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Gaussian distribution &amp;amp; Gaussian network models.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Power Series</title>
      <link>https://trunghng.github.io/posts/calculus/power-series/</link>
      <pubDate>Tue, 21 Sep 2021 15:40:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/power-series/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that in the previous note, &lt;a href=&#34;https://trunghng.github.io/posts/calculus/infinite-series-of-constants/&#34;&gt;Infinite Series of Constants&lt;/a&gt;, we mentioned a type of series called &lt;strong&gt;power series&lt;/strong&gt; a lot. In the content of this note, we will be diving deeper into details of its.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Infinite Series of Constants</title>
      <link>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</link>
      <pubDate>Mon, 06 Sep 2021 11:20:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on infinite series of constants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Monte Carlo Methods in Reinforcement Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</link>
      <pubDate>Sat, 21 Aug 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;Dynamic Programming&lt;/strong&gt;&lt;/a&gt; algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods, we only require &lt;strong&gt;experience&lt;/strong&gt; - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Solving MDPs with Dynamic Programming</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</link>
      <pubDate>Sun, 25 Jul 2021 15:30:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In two previous notes, &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;MDPs and Bellman equations&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/&#34;&gt;&lt;strong&gt;Optimal Policy Existence&lt;/strong&gt;&lt;/a&gt;, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with &lt;strong&gt;Dynamic Programming&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Optimal Policy Existence</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</link>
      <pubDate>Sat, 10 Jul 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In the previous note about &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;Markov Decision Processes, Bellman equations&lt;/strong&gt;&lt;/a&gt;, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. In this note, we will be proving that.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measures</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure/</link>
      <pubDate>Sat, 03 Jul 2021 07:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;When talking about &lt;strong&gt;measure&lt;/strong&gt;, you might associate it with the idea of &lt;strong&gt;length&lt;/strong&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;strong&gt;area&lt;/strong&gt;, or even three dimensions with &lt;strong&gt;volume&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Decision Processes, Bellman equations</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</link>
      <pubDate>Sun, 27 Jun 2021 08:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;You may have known or heard vaguely about a computer program called &lt;strong&gt;AlphaGo&lt;/strong&gt; - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called &lt;strong&gt;self-play&lt;/strong&gt; against its other instances, with &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Chain</title>
      <link>https://trunghng.github.io/posts/probability-statistics/markov-chain/</link>
      <pubDate>Sat, 19 Jun 2021 22:27:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/markov-chain/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If we have to describe the definition of &lt;strong&gt;Markov chain&lt;/strong&gt; in one statement, it will be: &amp;ldquo;It only matters where you are, not where you&amp;rsquo;ve been&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>My very first post</title>
      <link>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</link>
      <pubDate>Sat, 05 Jun 2021 17:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Enjoy my index-zero-ed note while staying tuned for next ones!&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://trunghng.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://trunghng.github.io/about/</guid>
      <description>&lt;style&gt;
    .about-me {
		padding: 20px 0;
		text-align: center;
    }
	#joker {
		border-radius: 50%;
		display: block;
		margin-left: auto;
		margin-right: auto;
		margin-top: 100px;
    }
&lt;/style&gt;
&lt;div class=&#34;about-me&#34;&gt;
	&lt;img id=&#39;joker&#39; src=&#34;https://trunghng.github.io/images/others/shaymin.jpg&#34; alt=&#34;Avatar&#34; style=&#34;width:400px; height: 360px&#34;&gt;
	&lt;p&gt;&lt;i&gt;A learner at day, a dreamer at night, a Pokémon trainer at heart&lt;/i&gt;&lt;p&gt;
&lt;/div&gt;</description>
    </item>
    
    
    
  </channel>
</rss>
