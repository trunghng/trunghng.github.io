---
layout: post
title:  "Monte Carlo Tree Search"
date:   2021-11-06 18:50:00 +0700
categories: [artificial-intelligent, reinforcement-learning]
tags: artificial-intelligent reinforcement-learning monte-carlo mcts
description: monte carlo tree search
comments: true
---
> Since its first appearance in 2016, **AlphaGo** had continued. **Monte Carlo Tree Search (MCTS)** is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.

<!-- excerpt-end -->
- [Background](#background)
	- [Bandit-based Methods](#bandit-based-method)
- [Monte Carlo Tree Search](#mcts)
	- [Algorithm](#algorithm)


- [References](#references)
- [Footnotes](#footnotes)

## Background

### Bandit-based Methods[^1]
{: #bandit-based-method}


## References
[1] C. B. Browne et al., [A Survey of Monte Carlo Tree Search Methods](https://ieeexplore.ieee.org/document/6145622), in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012, doi: 10.1109/TCIAIG.2012.2186810.  

[2] Richard S. Sutton & Andrew G. Barto. [Reinforcement Learning: An Introduction](https://mitpress.mit.edu/books/reinforcement-learning-second-edition)  

[3] 

## Footnotes
[^1]: For more detailed about Multi Armed Bandit problem, we will save it for another post since this one is gonna be a long post. 


