---
layout: post
title:  "Inverse Reinforcement Learning"
date:   2022-10-17 13:00:00 +0700
categories: artificial-intelligent reinforcement-learning
tags: artificial-intelligent reinforcement-learning inverse-reinforcement-learning
description: Inverse Reinforcement Learning
comments: true
eqn-number: true
---
> A note on Inverse Reinforcement Learning

<!-- excerpt-end -->

- [Preliminaries](#preliminaries)
	- [Markov decision processes](#mdp)
	- [Inverse reinforcement learning](#irl)
- [Max margin methods](#max-margin-methods)
	- [Max-margin & Projection](#max-margin-proj)
		- [Max-margin](#max-margin)
		- [Projection](#proj)
	- [Max margin planning](#max-margin-pln)
	- [LEARCH](#learch)
- [Max entropy methods](#max-ent)
- [Bayesian methods](#bayes)
	- [BIRL](#birl)
- [References](#references)
- [Footnotes](#footnotes)

## Preliminaries
{: #preliminaries}

	
### Markov decision processes
{: #mdp}
We begin by recalling the definition of [**Markov decision processes (MDP)**]({% post_url 2021-06-27-mdp-bellman-eqn %}#mdp).

A **Markov decision process**, or **MDP** is defined to be a tuple $(\mathcal{S},\mathcal{A},T,R,\gamma)$, where
- $\mathcal{S}$ is a set of states, represents the **state space**.
- $\mathcal{A}$ is a set of actions, known as the **action space**.
- $T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the **state transition probabilities**, i.e., $T(s'\vert s,a)$ denotes the probability of transitioning to state $s'$ when taking action $a$ from state $s$.
- $R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ denotes the **reward function**.
- $\gamma\in[0,1]$ is referred as **discount factor**.

A **policy**, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\\{0,1\\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$.

Under a particular policy $\pi$, the **state value function**, denoted as $v_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$. Specifically, $v_\pi(s)$ is defined as the expected sum of discounted rewards when starting in $s$ and following $\pi$.

For an initial state $s_0$, its value can be computed as
\begin{equation}
v_\pi(s_0)=\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t R(S_t,\pi(S_t))\big\vert S_0=s_0\right]\label{eq:mdp.1}
\end{equation}
Analogously, the **state-action value function**, denoted by $q_\pi$, of a state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$ specifies how good it is to take action $a$ from state $s$. Similar to $v_\pi$, $q_\pi$ is defined as the expected sum of discounted rewards when starting from $s$, taking action $a$ and thereby following policy $\pi$.

### Inverse reinforcement learning
{: #irl}
The tuple $(\mathcal{S},\mathcal{A},T,\gamma)$ represents an $MDP$ without a predefined reward function $R$, or MDP\R for short. Suppose that we are given either a set of demonstrated trajectories, denoted as $\mathcal{D}$, or an **expert** (policy) $\pi_E$, which is good at our task. Without an explicit reward function, with **inverse reinforcement learning**, or **IRL** methods, we instead try to find a reward function $\tilde{R}\_E$ that is good at explaining $\pi_E$ or $\mathcal{D}$.

For each state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$, we assume there is a corresponding feature vector $\boldsymbol{\phi}(s,a)\in\mathbb{R}^k$ where
\begin{equation}
\boldsymbol{\phi}(s,a)=\big(\phi_1(s,a),\ldots,\phi_k(s,a)\big)^\text{T},
\end{equation}
where $\phi_i:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ are feature functions such that the reward function can be expressed as a linear weighted sum of these features. In particular, for $w_i\in\mathbb{R}$ we can represent the reward function $R$ as
\begin{align}
R(s,a)&=w_1\phi_1(s,a)+\ldots+w_k\phi_k(s,a) \\\\ &=\mathbf{w}^\text{T}\boldsymbol{\phi}(s,a)
\end{align}
Thus, we can rewrite \eqref{eq:mdp.1} as
\begin{align}
v_\pi(s_0)&=\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t R\big(S_t,\pi(S_t)\big)\big\vert S_0=s_0\right] \\\\ &=\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t\mathbf{w}^\text{T}\boldsymbol{\phi}\big(S_t,\pi(S_t)\big)\right] \\\\ &=\mathbf{w}^\text{T}\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t\boldsymbol{\phi}\big(S_t,\pi(S_t)\big)\right]
\end{align}
The **feature expectation** or also called the expected discounted accumulated feature value, denoted as $\boldsymbol{\mu}(\pi)$, of a policy $\pi$ is defined as
\begin{equation}
\boldsymbol{\mu}(\pi)=\mathbb{E}\_\pi\left[\sum_{t=0}^{\infty}\gamma^t\boldsymbol{\phi}\big(S_t,\pi(A_t)\big)\right]
\end{equation}
Hence, we have that
\begin{equation}
v_\pi(s_0)=\mathbf{w}^\text{T}\boldsymbol{\mu}(\pi)
\end{equation}

## Max margin methods
{: #max-margin-methods}
The idea of **max margin methods** is to learn a reward function $\tilde{R}\_E$ that explains the demonstrated policy $\pi_E$ better than alternative policies by a margin.

### Max-margin & Projection
{: #max-margin-proj}
In these algorithms setting, we are given the expert's feature expectation $\boldsymbol{\mu}\_E$ and our work is to find a policy $\tilde{\pi}$ such that
\begin{equation}
\left\Vert\boldsymbol{\tilde{\pi}}-\boldsymbol{\mu}\_E\right\Vert_2\leq\varepsilon,
\end{equation}
where $\varepsilon>0$.

For such $\tilde{\pi}$, we have
\begin{equation}
\left\vert\mathbb{E}\_{\pi_E}\left[\sum_{t=0}^{\infty}\right]\right\vert
\end{equation}

#### Max-margin
{: #max-margin}

#### Projection
{: #proj}

### Max margin planning
{: #max-margin-pln}

### LEARCH
{: #learch}

## Max entropy methods
{: #max-ent}

## References
{: #references}
[1] Pieter Abbeel & Andrew Y. Ng. [Apprenticeship Learning via Inverse Reinforcement Learning](https://doi.org/10.1145/1015330.1015430). ICML '04: Proceedings of the twenty-first international conference on Machine learning. July 2004.

[2] Nathan D. Ratliff, J. Andrew Bagnell & Martin A. Zinkevich. [Maximum margin planning](https://doi.org/10.1145/1143844.1143936). ICML '06: Proceedings of the 23rd international conference on Machine learning. June 2006

[3] Nathan Ratliff, David Silver & J. Andrew (Drew) Bagnell. [Learning to search: Functional gradient techniques for imitation learning](https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/). Autonomous Robots. July 2009.

## Footnotes
{: #footnotes}