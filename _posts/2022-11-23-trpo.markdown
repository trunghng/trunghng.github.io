---
layout: post
title:  "Trust Region Policy Optimization"
date:   2022-11-23 15:26:00 +0700
tags: deep-reinforcement-learning policy-gradient my-rl
description: Trust Region Policy Optimization
comments: true
eqn-number: true
---
> TRPO.

<!-- excerpt-end -->
- [Preliminaries](#preliminaries)
	- [Markov Decision Processes](#mdp)
	- [Coupling & Total variation distance](#coupling-tvd)
	- [Likelihood Ratio Policy Gradient](#likelihood-ratio-pg)
	- [Importance Sampling](#is)
- [Policy Improvement](#policy-imp)
- [Parameterized Policy Optimization by Trust Region](#param-policy-opt)
- [References](#references)
- [Footnotes](#footnotes)

## Preliminaries
{: #preliminaries}
We begin by recalling the familiar definition of MDPs, also combined with mentioning about likelihood ratio policy gradient, coupling and total variation distance.

### Markov Decision Processes
{: #mdp}
An infinite-horizon discounted **Markov Decision Process** (**MDP**) is defined as the tuple $(\mathcal{S},\mathcal{A},P,r,\rho_0,\gamma)$, where
- $\mathcal{S}$ is a finite set of states, or **state space**.
- $\mathcal{A}$ is a finite set of actions, or **action space**.
- $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$ is the **transition probability distribution**, i.e. $P(s,a,s')=P(s'\vert s,a)$ denotes the probability of transitioning to state $s'$ when taking action $a$ from state $s$.
- $r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the **reward function**.
- $\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.
- $\gamma\in(0,1)$ is the **discount factor**.

A **policy**, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\\{0,1\\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$. Here, we consider the stochastic policy only.

We continue by letting $\eta(\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\pi$ thereafter
\begin{equation}
\eta(\pi)=\mathbb{E}\_{s_0,a_0,\ldots}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right],
\end{equation}
where
\begin{equation}
s_0\sim\rho_0(s_0),\hspace{1cm}a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)
\end{equation}
For a policy $\pi$, the **state value function**, denoted as $V_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$, and the **action value function**, referred as $Q_\pi$, of a state-action pair $(s,a)\in\mathcal{S}\times\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as
\begin{align}
V_\pi(s_t)&=\mathbb{E}\_{a_t,s_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right], \\\\ Q_\pi(s_t,a_t)&=\mathbb{E}\_{s_{t+1},a_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right],
\end{align}
where
\begin{equation}
a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)\hspace{1cm}t\geq 0
\end{equation}
Along with these value functions, we will also define the **advantage function** for $\pi$, denoted $A_\pi$, given as
\begin{equation}
A_\pi(s_t,a_t)=Q_\pi(s_t,a_t)-V_\pi(s_t)
\end{equation}

### Coupling & Total variation distance
{: #coupling-tvd}
Consider two probability measures $\mu$ and $\nu$ on a probability space $(\Omega,\mathcal{F},P)$. One refers a **coupling** of $\mu$ and $\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\mu$ and $\nu$.

Specifically, if $p$ is a joint distribution of $X,Y$ on $\Omega$, then it implies that
\begin{align}
\sum_{y\in\Omega}p(x,y)&=\sum_{y\in\Omega}P(X=x,Y=y)=P(X=x)=\mu(x) \\\\ \sum_{x\in\Omega}p(x,y)&=\sum_{x\in\Omega}P(X=x,Y=y)=P(Y=y)=\nu(y)
\end{align}
For probability distributions $\mu$ and $\nu$ on $\Omega$ as above, the **total variation distance** between $\mu$ and $\nu$, denoted $\big\Vert\mu-\nu\big\Vert_\text{TV}$, is defined by
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}\doteq\sup_{A\\subset\Omega}\big\vert\mu(A)-\nu(A)\big\vert
\end{equation}
**Proposition 1**  
Let $\mu$ and $\nu$ be probability distributions on $\Omega$, we then have
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
**Proof**  
Let $B=\\{x:\mu(x)\geq\nu(x)\\}$ and let $A\subset\Omega$. We have
\begin{align}
\mu(A)-\nu(A)&=\mu(A\cap B)+\mu(A\cap B^c)-\nu(A\cap B)-\nu(A\cap B^c) \\\\ &\leq\mu(A\cap B)-\nu(A\cap B) \\\\ &\leq\mu(B)-\nu(B)
\end{align}
Analogously, we also have
\begin{equation}
\nu(A)-\mu(A)\leq\nu(B^c)-\mu(B^c)
\end{equation}
Hence, combining these results gives us
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\left(\mu(B)-\nu(B)+\nu(B^c)-\mu(B^c)\right)=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
This proof also implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sum_{x\in\Omega;\,\mu(x)\geq\nu(x)}\mu(x)-\nu(x)
\end{equation}
**Proposition 2**  
Let $\mu$ and $\nu$ be two probability measures defined in a probability space $\Omega$, we then have that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
**Proof**  
For any $A\subset\Omega$ and for any coupling $(X,Y)$ of $\mu$ and $\nu$ we have
\begin{align}
\mu(A)-\nu(A)&=P(X\in A)-P(Y\in A) \\\\ &=P(X\in A,Y\notin A)+P(X\in A,Y\in A)-P(Y\in A) \\\\ &\leq P(X\in A,Y\notin A) \\\\ &\leq P(X\neq Y),
\end{align}
which implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sup_{A'\subset\Omega}\big\vert\mu(A')-\nu(A')\big\vert\leq P(X\neq Y)\leq\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
Thus, it suffices to construct a coupling $(X,Y)$ of $\mu$ and $\nu$ such that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=P(X\neq Y)
\end{equation}

### Likelihood Ratio Policy Gradient
{: #likelihood-ratio-pg}
Let $H$ denote the **horizon** of an MDP[^1]. Consider **likelihood ratio policy gradient** problem, in which the policy $\pi_\theta$ is parameterized by a vector $\theta\in\mathbb{R}^n$. The expected return of $\pi_\theta$ is then given by
\begin{equation}
\eta(\pi_\theta)=\mathbb{E}\_{P(\tau;\theta)}\left[\left.\sum_{t=0}^{H-1}\gamma^t r(s_t,a_t)\right\vert\pi_\theta\right]=\sum_{\tau}P(\tau;\theta)R(\tau),\label{eq:lrp.1}
\end{equation}
where
- $P(\tau;\theta)$ is the probability distribution induced by the policy $\pi_\theta$, i.e. $s_t$
- $\tau=(s_0,a_0,s_1,a_1,\ldots,s_H,a_H)$ are trajectories generated by rolls out, i.e. $\tau\sim P(\tau;\theta)$.
- $R(\tau)$ is the discounted cumulative rewards along the trajectory $\tau$, given as
\begin{equation}
R(\tau)=\sum_{t=0}^{H-1}\gamma^t r(s_t,a_t)
\end{equation}

The likelihood ratio policy gradient performs a SGA (stochastic gradient ascent) over the policy parameter space $\Theta$ to find a local optimum of $\eta(\pi_\theta)$ by taking into account the policy gradient
\begin{align}
\nabla_\theta\eta(\pi_\theta)&=\nabla_\theta\sum_{\tau}P(\tau;\theta)R(\tau) \\\\ &=\sum_\tau\nabla_\theta P(\tau;\theta)R(\tau) \\\\ &=\sum_\tau\nabla_\theta\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau) \\\\ &=\sum_{\tau}P(\tau;\theta)\nabla_\theta\log P(\tau;\theta)R(\tau) \\\\ &=\mathbb{E}\_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big]\label{eq:lrp.2}
\end{align}
This gradient can be approximated with empirical estimate from $m$ trajectories $\tau^{(1)},\ldots,\tau^{(m)}$ under policy $\pi_\theta$
\begin{align}
\nabla_\theta\eta(\pi_\theta)&=\mathbb{E}\_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big] \\\\ &\approx\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\hat{g},
\end{align}
which is an unbiased estimate of the policy gradient.

Additionally, since
\begin{align}
\nabla_\theta\log P(\tau^{(i)};\theta)&=\nabla_\theta\log\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\\\ &=\nabla_\theta\sum_{t=0}^{H-1}\log P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})+\nabla_\theta\sum_{t=0}^{H-1}\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\\\ &=\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}),
\end{align}
we can rewrite the policy gradient estimate in a form which no longer require the dynamics model
\begin{equation}
\hat{g}=\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})R(\tau^{(i)})
\end{equation}
Moreover, as
\begin{equation}
\mathbb{E}\_{P(\tau\vert\theta)}\Big[\nabla_\theta\log P(\tau;\theta)\Big]=\nabla_\theta\sum_\tau P(\tau;\theta)=\nabla_\theta 1=\mathbf{0},
\end{equation}
a constant baseline in terms of $\theta$ (i.e. independent of $\theta$) $b$ can be inserted into \eqref{eq:lrp.2} to reduce the variance (where $b$ is a vector which can be optimized to minimize the variance). In particular
\begin{align}
\nabla_\theta\eta(\pi_\theta)&=\mathbb{E}\_{P(\tau;\theta)}\Big[\nabla_\theta\log P(\tau;\theta)(R(\tau)-b)\Big] \\\\ &\approx\frac{1}{m}\nabla_\theta\log P(\tau^{(i)};\theta)\left(R(\tau^{(i)})-b\right) \\\\ &=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)})-b\right)=\hat{g}
\end{align}
By separating $R(\tau^{(i)})$ into sum of discounted rewards from the past, which does not depend on the current action $a_t^{(i)}$ (and thus can be removed to reduce the variance), and sum of discounted future rewards, we can continue to decompose the estimator $\hat{g}$ as
\begin{align}
\hat{g}&=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)}-b)\right) \\\\ &=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\Bigg[\sum_{k=0}^{t-1}r(s_k^{(i)},a_k^{(i)})+\left(\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})\right)-b\Bigg] \\\\ &=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})-b\right)
\end{align}
These following are some possible choices of baseline $b$.
<ul id='number-list'>
	<li>
		<b>Average rewards.</b>
		\begin{equation}
		b=\mathbb{E}\big[R(\tau)\big]\approx\frac{1}{m}\sum_{i=1}^{m}R(\tau^{(i)})
		\end{equation}
	</li>
	<li>
		<b>Optimal baseline</b>.
		\begin{equation}
		b=\frac{\sum_{i=1}^{m}\left(\nabla_\theta\log P(\tau^{(i)};\theta)\right)^2 R(\tau^{(i)})}{\sum_{i=1}^{m}\left(\nabla_\theta\log P(\tau^{(i)};\theta)\right)^2}
		\end{equation}
	</li>
	<li>
		<b>Time-dependent baseline</b>.
		\begin{equation}
		b_t=\frac{1}{m}\sum_{i=1}^{m}\sum_{k=t}^{H-1}R(s_k^{(i)},a_k^{(i)})
		\end{equation}
	</li>
	<li>
		<b>State-dependent baseline</b>.
		\begin{equation}
		b(s_t^{(i)})=\mathbb{E}_\pi\left[\sum_{k=t}^{H-1}\gamma^k r(s_k^{(i)},a_k^{(i)})\right]=V_\pi(s_t^{(i)})
		\end{equation}
	</li>
</ul>

### Importance Sampling
{: #is}
Consider a function $f$ and a probability measure $P$. The expectation of $f$, defined by
\begin{equation}
\mathbb{E}\_{P(X)}\big[f(X)\big]=\int_{x}P(x)f(x)\,dx,
\end{equation}
sometimes can be difficult to compute. We can resolve this problem by instead approximating the expectation by sampling method, as
\begin{equation}
\mathbb{E}\_{P(X)}f(X)\approx\frac{1}{m}\sum_{i=1}^{m}f(x^{(i)}),
\end{equation}
where $x^{(i)}\sim P$. However, samples generated from $P$ are not always easily obtained. This is where **importance sampling** plays its role.

The idea of **importance sampling**, or **IS** is an observation that
\begin{align}
\mathbb{E}\_{P(X)}\big[f(X)\big]&=\int_{x}P(x)f(x)\,dx \\\\ &=\int_{x}Q(x)\frac{P(x)}{Q(x)}f(x)\,dx \\\\ &=\mathbb{E}\_{Q(X)}\left[\frac{P(X)}{Q(X)}f(X)\right],
\end{align}
where we have assumed that $Q(x)=0\Rightarrow P(x)=0$. Hence, we can use a sample-based method on $Q$ to get estimate of the expectation. Specifically, given $x^{(i)}\sim Q$, for $i=1,\ldots,m$, we can construct an unbiased estimator
\begin{equation}
\mathbb{E}\_{P(X)}\big[f(X)\big]=\mathbb{E}\_{Q(X)}\left[\frac{P(X)}{Q(X)}f(X)\right]\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(x^{(i)})}{Q(x^{(i)})}f(x^{(i)})
\end{equation}
The IS method suggests us rewrite the expected return of policy $\pi_\theta$ as[^2]
\begin{align}
\eta(\pi_\theta)&=\mathbb{E}\_{\tau\sim P(\tau;\theta)}\big[R(\tau)\big] \\\\ &=\mathbb{E}\_{\tau\sim P(\tau;\theta')}\left[\frac{P(\tau;\theta)}{P(\tau;\theta')}R(\tau)\right]
\end{align}
Taking the gradient w.r.t $\theta$ gives us another representation of the policy gradient 
\begin{align}
\nabla_\theta\eta(\pi_\theta)&=\nabla_\theta\mathbb{E}\_{\tau\sim P(\tau;\theta')}\left[\frac{P(\tau;\theta)}{P(\tau;\theta')}R(\tau)\right] \\\\ &=\mathbb{E}\_{\tau\sim P(\tau;\theta')}\left[\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta')}R(\tau)\right],
\end{align}
which implies that
\begin{align}
\nabla_\theta\eta(\pi_\theta)\big\vert_{\theta=\theta'}&=\mathbb{E}\_{\tau\sim P(\tau;\theta')}\left[\frac{\nabla_\theta P(\tau;\theta)\big\vert_{\theta=\theta'}}{P(\tau;\theta')}R(\tau)\right] \\\\ &=\mathbb{E}\_{\tau\sim P(\tau;\theta')}\big[\nabla_\theta\log P(\tau;\theta)\big\vert_{\theta=\theta'}R(\tau)\big]
\end{align}

## Policy improvement
{: #policy-imp}
We begin by proving an identity that expresses the expected return $\eta(\tilde{\pi})$ of a policy $\tilde{\pi}$ in terms of the advantage over another policy $\pi$, accumulated over time steps.

**Lemma 3**  
Given two policies $\pi,\tilde{\pi}$, we have
\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}\_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]\label{eq:pi.1}
\end{equation}
**Proof**  
By definition of advantage function $A_\pi$ of policy $\pi$, we have
\begin{align}
\mathbb{E}\_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]&=\mathbb{E}\_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\left(Q_\pi(s_t,a_t)-V_\pi(s_t)\right)\right] \\\\  &=\mathbb{E}\_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\big(r(s_t,a_t)+\gamma V_\pi(s_{t+1})-V_\pi(s_t)\big)\right] \\\\ &=\mathbb{E}\_{\tilde{\pi}}\left[-V_\pi(s_0)+\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\\\ &=-\mathbb{E}\_{s_0}\big[V_\pi(s_0)\big]+\mathbb{E}\_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\\\ &=-\eta(\pi)+\eta(\tilde{\pi}),
\end{align}
where in the third step, since $\gamma\in(0,1)$ as $t\to\infty$, we have that $\gamma^t V_\pi(s_{t+1})\to 0$.

Let $\rho_\pi$ be the unnormalized discounted visitation frequencies for state $s$:
\begin{equation}
\rho_\pi(s)\doteq P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots
\end{equation}
where $s_0\sim\rho_0$ and the actions are chosen according to $\pi$. This allows us to rewrite \eqref{eq:pi.1} as
\begin{align}
\eta(\tilde{\pi})&=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)\gamma^t A_\pi(s,a) \\\\ &=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma^t P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a) \\\\ &=\eta(\pi)+\sum_{s}\rho_\tilde{\pi}(s)\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.2}
\end{align}
This result implies that any policy update $\pi\to\tilde{\pi}$ that has a nonnegative expected advantage at every state $s$, i.e. $\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\geq 0$, is guaranteed to make an improvement on $\eta$ (or unchanged in case the expected advantage take the value of zero for every $s$). By letting $\tilde{\pi}$ be the deterministic policy that
\begin{equation}
\tilde{\pi}(s)=\underset{a}{\text{argmax}}\,A_\pi(s,a),
\end{equation}
we obtain the [**policy improvement**]({% post_url 2021-07-25-dp-in-mdp %}#policy-improvement) result used in [**policy iteration**]({% post_url 2021-07-25-dp-in-mdp %}#policy-iteration).

However, there are cases when \eqref{eq:pi.2} is difficult to be optimized, especially when the expected advantage is negative, i.e. $\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)$, due to estimation and approximation error in the approximate setting. We instead consider a local approximation to $\eta$:
\begin{equation}
L_\pi(\tilde{\pi})=\eta(\pi)+\sum_s\rho_\pi(s)\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.5}
\end{equation}

If $\pi$ is a policy parameterized by $\theta$, in which $\pi_\theta(a\vert s)$ s differentiable w.r.t $\theta$, we then have for any parameter value $\theta_0$
\begin{align}
L_{\pi_{\theta_0}}(\pi_{\theta_0})&=\eta(\pi_{\theta_0}) \\\\ \nabla_\theta L_{\pi_{\theta_0}}(\pi_\theta)\big\vert_{\theta=\theta_0}&=\nabla_\theta\eta(\pi_\theta)\big\vert_{\theta=\theta_0},
\end{align}
which suggests that a sufficiently small step $\pi_{\theta_0}\to\tilde{\pi}$ that leads to an improvement on $L_{\pi_{\theta_\text{old}}}$ will also make an improvement on $\eta$.

To measure the improvement on updating $\pi_\text{old}\to\pi_\text{new}$, we choose the total variance distance metric, as defined above with an observation that each policy $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$ can be viewed as a distribution function defined on $\mathcal{S}\times\mathcal{A}$. Thus, those results and definitions mentioned above for probability measures $\mu$ and $\nu$ defined on $\Omega$ can also be applied to policies $\pi$ and $\tilde{\pi}$ specified on $\mathcal{S}\times\mathcal{A}$.

In addition, we need to define some notations:
<ul id='number-list'>
	<li>
		Let
		\begin{equation}
		\big\Vert\pi-\tilde{\pi}\big\Vert_{\text{TV}}^{\text{max}}\doteq\max_s\big\Vert\pi(\cdot\vert s)-\tilde{\pi}(\cdot\vert s)\big\Vert_\text{TV}
		\end{equation}
	</li>
	<li>
		A policy pair $(\pi,\tilde{\pi})$ is referred as <b>$\alpha$-coupled</b> if it defines a joint distribution $(a,\tilde{a})\vert s$ such that
		\begin{equation}
		P(a\neq\tilde{a}\vert s)\leq\alpha,\hspace{1cm}\forall s
		\end{equation}
		$\pi$ and $\tilde{\pi}$ will respectively denote the marginal distributions of $a$ and $\tilde{a}$.<br><br>
		<b>Proposition 4</b><br>
		Let $(\pi,\tilde{\pi})$ be $\alpha$-coupled policy pair, for all $s$, we have
		\begin{equation}
		\big\vert\bar{A}(s)\big\vert\leq 2\alpha\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert,
		\end{equation}
		where $\bar{A}(s)$ is the expected advantage of $\tilde{\pi}$ over $\pi$ at state $s$, given as
		\begin{equation}
		\bar{A}(s)=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big]
		\end{equation}
		<b>Proof</b><br>
		By definition of the advantage function, it is easily noticed that $\mathbb{E}_{a\sim\pi}\big[A_\pi(s,a)\big]=0$, which lets us obtain
		\begin{align}
		\bar{A}(s)&=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big] \\\\ &=\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big] \\\\ &=P(a\neq\tilde{a}\vert s)\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}\vert a\neq\tilde{a}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big],
		\end{align}
		which by definition of $\alpha$-coupling implies that
		\begin{equation}
		\big\vert\bar{A}(s)\big\vert\leq\alpha\cdot 2\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert
		\end{equation}
	</li>
</ul>

**Theorem 5**  
Let $\alpha=\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^\text{max}$. The following holds
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-\frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2,
\end{equation}
where
\begin{equation}
\epsilon=\max_{s,a}\big\vert A_\pi(s,a)\big\vert
\end{equation}
**Proof**  


On the other hand, by **Pinsker's inequality**, which bounds the total variation distance in terms of the **Kullback-Leibler divergence**, denoted $D_\text{KL}$, we have that
\begin{equation}
\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^2\leq\frac{1}{2}D_\text{KL}(\pi\Vert\tilde{\pi})\leq D_\text{KL}(\pi\Vert\tilde{\pi}),\label{eq:pi.3}
\end{equation}
since $D_\text{KL}(\cdot\Vert\cdot)\geq 0$. Thus, let
\begin{equation}
D_\text{KL}^\text{max}(\pi,\tilde{\pi})\doteq\max_s D_\text{KL}\big(\pi(\cdot\vert s)\Vert\tilde{\pi}(\cdot\vert s)\big),
\end{equation}
with the result \eqref{eq:pi.3} and by **Theorem 5**, we have
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-CD_\text{KL}^\text{max}(\pi,\tilde{\pi}),\label{eq:pi.4}
\end{equation}
where
\begin{equation}
C=\frac{4\epsilon\gamma}{(1-\gamma)^2}
\end{equation}
The policy improvement bound \eqref{eq:pi.4} allows us to specify a policy iteration, as given in the following pseudocode
<figure>
	<img src="/assets/images/2022-11-23/policy-iteration-nondec-exp-return.png" alt="Non-decreasing expected return policy iteration" style="display: block; margin-left: auto; margin-right: auto;"/>
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
It is worth noticing that \eqref{eq:pi.4} allows the policy iteration above to guarantee to generating a sequence of non-decreasing expected returns
\begin{equation}
\eta(\pi_0)\leq\eta(\pi_1)\leq\eta(\pi_2)\leq\ldots
\end{equation}
To see this, let
\begin{equation}
M_i(\pi)\doteq L_{\pi_i}(\pi)-CD_\text{KL}^\text{max}(\pi_i,\pi),
\end{equation}
by \eqref{eq:pi.4}, we then have
\begin{equation}
\eta(\pi_{i+1})\geq M_i(\pi_{i+1}),
\end{equation}
which implies that
\begin{equation}
\eta(\pi_{i+1})-\eta(\pi_i)=\eta(\pi_{i+1})-M_i(\pi_i)\geq M_i(\pi_{i+1})-M_i(\pi_i)
\end{equation}

## Parameterized Policy Optimization by Trust Region
{: #param-policy-opt}
We now consider the policy optimization problem in which the policy is parameterized by $\theta$.

We begin by simplifying notations. In particular, let $\eta(\theta)\doteq\eta(\pi_\theta)$, let $L_\theta(\tilde{\theta})\doteq L_{\pi_\theta}(\pi_\tilde{\theta})$ and $D_\text{KL}(\theta\Vert\tilde{\theta})\doteq D_\text{KL}(\pi_\theta\Vert\pi_\tilde{\theta})$, which allows us to represent
\begin{equation}
D_\text{KL}^\text{max}(\theta,\tilde{\theta})\doteq D_\text{KL}^\text{max}(\pi_\theta,\pi_\tilde{\theta})=\max_s D_\text{KL}\big(\pi_\theta(\cdot\vert s)\Vert\pi_\tilde{\theta}(\cdot\vert s)\big)
\end{equation}
Also let $\theta_\text{old}$ denote the previous policy parameters that we want to improve. Hence, by the previous section, we have
\begin{equation}
\eta(\theta)\geq L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta),
\end{equation}
where the equality holds at $\theta=\theta_\text{old}$. This means, we get a guaranteed improvement to the true objective function $\eta$ by solving the following optimization problem
\begin{equation}
\underset{\theta}{\text{maximize}}\,\big[L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta)\big]
\end{equation}
To speed up the algorithm, we make some robust modification. Specifically, we instead solve a **trust region problem**:
\begin{align}
\underset{\theta}{\text{maximize}}&\,L_{\theta_\text{old}}(\theta) \\\\ \text{s.t.}&\,\overline{D}\_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\leq\delta,
\end{align}
where $\overline{D}\_\text{KL}^{\rho_{\theta_\text{old}}}$ is the average KL divergence, given as
\begin{equation}
\overline{D}\_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\doteq\mathbb{E}\_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]
\end{equation}
By the definition of $L$, given in \eqref{eq:pi.5}, we can rewrite our optimization problem as
\begin{align}
\underset{\theta}{\text{maximize}}&\,\sum_s\rho_{\theta_\text{old}}(s)\sum_a\pi_\theta(a\vert s)A_{\theta_\text{old}}(s,a) \\\\ \text{s.t.}&\,\overline{D}\_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\leq\delta,
\end{align}
where $A_{\theta_\text{old}}\doteq A_{\pi_{\theta_\text{old}}}$.



## References
{: #references}
[1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. [Trust Region Policy Optimization](https://dl.acm.org/doi/10.5555/3045118.3045319). ICML'15, pp 1889–1897, 2015.  

[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. [Markov chains and mixing times](https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf). American Mathematical Society, 2009.

[3] Jie Tang, Pieter Abbeel. [On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient](https://proceedings.neurips.cc/paper/2010/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html). NIPS 2010.  

[4] Sham Kakade,  John Langford. [Approximately optimal approximate reinforcement learning](https://dl.acm.org/doi/10.5555/645531.656005). ICML'2, pp. 267–274, 2002.  

## Footnotes
{: #footnotes}

[^1]: Any infinite-horizon discounted MDP, as defined in the preceding subsection, can be $\epsilon$-approximated by a finite horizon MDP, using a horizon
	\begin{equation\*}
	H_\epsilon=\left\lceil\log_\gamma\left(\frac{\epsilon(1-\gamma)}{R_\text{max}}\right)\right\rceil,
	\end{equation\*}
	where
	\begin{equation\*}
	R_\text{max}=\max_s\big\vert R(s)\big\vert
	\end{equation\*}

[^2]: We can also use importance sampling to construct an unbiased estimate of the expected return $\eta(\pi_\theta)$. In particular, the expected return of $\pi_\theta$ given in \eqref{eq:lrp.1} can be approximated by
	\begin{equation\*}
	\eta(\pi_\theta)=\sum_{\tau\sim P}P(\tau;\theta)R(\tau)\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{Q(\tau^{(i)})}R(\tau^{(i)})
	\end{equation\*}
	where $\tau^{(i)}\sim Q$ and we have assumed that $Q(\tau^{(i)})=0\Rightarrow P(\tau^{(i)};\theta)=0$.  
	If we choose $Q(\tau)\doteq P(\tau;\theta')$, this means we are approximating the expected return of $\pi_\theta$ from trajectories given according to another parameterized policy $\pi_{\theta'}$.
	\begin{equation\*}
	\eta(\pi_\theta)\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)};\theta')}R(\tau^{(i)})
	\end{equation\*}
	Taking the gradient of this estimator w.r.t $\theta$, we obtain an unbiased estimator for the policy gradient specified at \eqref{eq:lrp.2}
	\begin{align\*}
	\nabla_\theta\eta(\pi_\theta)&\approx\nabla_\theta\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)};\theta')}R(\tau^{(i)})
	\end{align\*}
	Similar to applying IS to [**off-policy learning**]({% post_url 2021-08-21-monte-carlo-in-rl %}#is-off-policy), evaluating the **importance weights**, or **importance sampling ratio** does not require a dynamics model
	\begin{equation\*}
	\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)});\theta'}=\frac{\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}\vert s_t^{(i)})}{\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_{\theta'}(a_t^{(i)}\vert s_t^{(i)})}=\prod_{t=0}^{H-1}\frac{\pi_\theta(a_t^{(i)}\vert s_t^{(i)})}{\pi_{\theta'}(a_t^{(i)}\vert s_t^{(i)})}
	\end{equation\*}