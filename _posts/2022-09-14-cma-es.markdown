---
layout: post
title:  "CMA Evolution Strategy"
date:   2022-09-14 13:00:00 +0700
categories: artificial-intelligent machine-learning
tags: artificial-intelligent evolution-strategy
description: A note CMA-ES
comments: true
eqn-number: true
---
> A note on CMA - Evolution Strategy
<!-- excerpt-end -->
- [Preliminaries](#preliminaries)
- [Basic equation](#bsc-eqn)
- [Updating the mean](#upd-mean)
- [Adapting the covariance matrix](#adp-cov)
	- [Estimating from scratch](#est-scratch)
	- [Rank-$\gamma$-update](#rank-lambda-mu-update)
- [References](#references)
- [Footnotes](#footnotes)

## Preliminaries
{: #preliminaries}
The **condition number** of a matrix $\mathbf{A}$ is defined by
\begin{equation}
\kappa(\mathbf{A})\doteq\Vert\mathbf{A}\Vert\Vert\mathbf{A}^{-1}\Vert,
\end{equation}
where $\Vert\mathbf{A}\Vert=\sup_{\Vert\mathbf{x}\Vert=1}\Vert\mathbf{Ax}\Vert$.

For $\mathbf{A}$ that is non-singular, $\kappa(\mathbf{A})=\infty$.

For $\mathbf{A}$ which is positive definite, we thus have $\Vert\mathbf{A}\Vert=\lambda_\text{max}$ where $\lambda_\text{max}$ denotes the largest eigenvalue of $\mathbf{A}$, correspondingly $\lambda_\text{min}$ denotes the smallest eigenvalue of $\mathbf{A}$. The condition number of $\mathbf{A}$ therefore can be written as
\begin{equation}
\kappa(\mathbf{A})=\frac{\lambda_\text{max}}{\lambda_\text{min}}\geq 1,
\end{equation}
since corresponding to each eigenvalue $\lambda$ of $\mathbf{A}$, the inverse matrix $\mathbf{A}^{-1}$ takes $1/\lambda$ as its eigenvalue.

## Basic equation
{: #bsc-eqn}
In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\ldots$
\begin{equation}
\mathbf{x}\_k^{(t+1)}\sim\boldsymbol{\mu}^{(t)}+\sigma^{(t)}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})\sim\mathcal{N}\left(\boldsymbol{\mu}^{(t)},{\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}\right),\hspace{1cm}k=1,\ldots,\lambda\label{eq:be.1}
\end{equation}
where
- $\mathbf{x}\_k^{(t+1)}\in\mathbb{R}^n$: the $k$-th sample at generation $t+1$.
- $\boldsymbol{\mu}^{(t)}\in\mathbb{R}^n$: mean of the search distribution at generation $t$.
- $\sigma^{(t)}\in\mathbb{R}$: step-size at generation $t$.
- $\boldsymbol{\Sigma}^{(t)}$: covariance matrix at generation $t$.
- ${\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$.
- $\lambda\geq 2$: sample size.

## Updating the mean
{: #update-mean}
The mean $\boldsymbol{\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\gamma$ selected points from the sample $\mathbf{x}\_1^{(t+1)},\ldots,\mathbf{x}\_\lambda^{(t+1)}$:
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\sum_{i=1}^{\gamma}w_i\mathbf{x}\_{i:\lambda}^{(t+1)},\label{eq:um.1}
\end{equation}
where
- $\sum_{i=1}^{\gamma}w_i=1$ with $w_1\geq w_2\geq\ldots\geq w_{\gamma}>0$.
- $\gamma\leq\lambda$: number of selected points.
- $\mathbf{x}\_{i:\lambda}^{(t+1)}$: $i$-th best sample out of $\mathbf{x}\_1^{(t+1)},\ldots,\mathbf{x}\_\lambda^{(t+1)}$ from \eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have
\begin{equation}
f(\mathbf{x}\_{1:\lambda}^{(t+1)})\geq f(\mathbf{x}\_{2:\lambda}^{(t+1)})\geq\ldots\geq f(\mathbf{x}\_{\lambda:\lambda}^{(t+1)})
\end{equation}
We can rewrite \eqref{eq:um.1} as an update rule for the mean $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\boldsymbol{\mu}^{(t)}+\alpha_\boldsymbol{\mu}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}\_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right),
\end{equation}
where $\alpha_\boldsymbol{\mu}\leq 1$ is the learning rate, which is usually set to $1$.

## Adapting the covariance matrix
{: #adp-cov}

### Estimating from scratch
{: #est-scratch}
Rather than using the empirical covariance matrix as an estimator for $\boldsymbol{\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation
\begin{equation}
\boldsymbol{\Sigma}\_\lambda^{(t+1)}=\frac{1}{\lambda{\sigma^{(t)}}^2}\sum_{i=1}^{\lambda}\left(\mathbf{x}\_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}\_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T}\label{eq:ac.1}
\end{equation}
Notice that in the above estimation \eqref{eq:ac.1}, we have used all of the $\lambda$ samples. We thus can estimate a better covariance matrix by select some of the best individual out of $\lambda$ samples, which is analogous to how we update the mean $\boldsymbol{\mu}$.

In particular, we instead consider the estimation
\begin{equation}
\boldsymbol{\Sigma}\_{\gamma}^{(t+1)}=\frac{1}{\{\sigma^{(t)}\}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}\_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}\_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T},\label{eq:ac.2}
\end{equation}
where $\gamma\leq\lambda$ is the number of selected points; the weights $w_i$ and selected points $\mathbf{x}\_{i:\lambda}^{(t+1)}$ are defined as given in the update for $\boldsymbol{\mu}$.

### Rank-$\gamma$-update
{: #rank-lambda-mu-update}



## References
{: #references}
[1] Nikolaus Hansen. [The CMA Evolution Strategy: A Tutorial](#https://arxiv.org/abs/1604.00772). 

## Footnotes
{: #footnotes}
