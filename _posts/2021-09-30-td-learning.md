---
layout: post
title:  "Temporal-Difference Learning"
date:   2021-09-30 00:50:00 +0700
categories: artificial-intelligent reinforcement-learning
tags: artificial-intelligent reinforcement-learning td-learning q-learning my-rl
description: Temporal-Difference Learning, Q-learning
comments: true
---
> So far in this [series](/tag/my-rl), we have gone through ideas of [**dynamic programming** (DP)]({% post_url 2021-07-25-dp-in-mdp %}) and [**Monte Carlo**]({% post_url 2021-08-21-monte-carlo-in-rl %}). What will happen if we combine these ideas together? **Temporal-deffirence (TD) learning** is our answer.

<!-- excerpt-end -->
- [TD Prediction](#td-prediction)
- [References](#references)
- [Footnotes](#footnotes)

## TD Prediction
Borrowing the ideas of Monte Carlo, TD methods learn from episodes of experience to solve the [prediction problem]({% post_url 2021-08-21-monte-carlo-in-rl %}#fn:2). However, 



## References
[1] Richard S. Sutton & Andrew G. Barto. [Reinforcement Learning: An Introduction](https://mitpress.mit.edu/books/reinforcement-learning-second-edition)  

[2] David Silver. [UCL course on RL](https://www.davidsilver.uk/teaching/)  

[3] 


## Footnotes
