<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Trung&#39;s Place</title>
    <link>https://trunghng.github.io/posts/</link>
    <description>Recent content in Posts on Trung&#39;s Place</description>
    <image>
      <url>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Thu, 02 Feb 2023 15:51:13 +0700</lastBuildDate><atom:link href="https://trunghng.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Probabilistic Graphical Models - Inference</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-inference/</link>
      <pubDate>Thu, 02 Feb 2023 15:51:13 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-inference/</guid>
      <description>&lt;p&gt;Notes on Inference in PGMs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Categorical Reparameterization with Gumbel-Softmax &amp; Concrete Distribution</title>
      <link>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</link>
      <pubDate>Mon, 02 Jan 2023 13:49:15 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/</guid>
      <description>&lt;p&gt;Notes on using Gumbel-Softmax &amp;amp; Concrete Distribution in Categorical sampling.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Maximum Entropy Reinforcement Learning via Soft Q-learning &amp; Soft Actor-Critic</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</link>
      <pubDate>Tue, 27 Dec 2022 13:46:09 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Maximum Entropy Reinforcement Learning via SQL &amp;amp; SAC.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Models - Representation</title>
      <link>https://trunghng.github.io/posts/machine-learning/pgm-representation/</link>
      <pubDate>Sat, 10 Dec 2022 17:55:57 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/pgm-representation/</guid>
      <description>&lt;p&gt;Notes on Representation in PGMs.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Deterministic Policy Gradients</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</link>
      <pubDate>Fri, 02 Dec 2022 19:26:44 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Deterministic Policy Gradient Algorithms&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Trust Region Policy Optimization</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/trpo/</link>
      <pubDate>Wed, 23 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/trpo/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on TRPO.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Deep Q-learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</link>
      <pubDate>Fri, 18 Nov 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on DQN.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Natural Evolution Strategies</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/nes/</link>
      <pubDate>Fri, 07 Oct 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/nes/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Natural Evolution Strategies&lt;/strong&gt;, or &lt;strong&gt;NES&lt;/strong&gt;, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</link>
      <pubDate>Thu, 06 Oct 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on Policy gradient methods.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>CMA Evolution Strategy</title>
      <link>https://trunghng.github.io/posts/evolution-strategy/cma-es/</link>
      <pubDate>Wed, 14 Sep 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/evolution-strategy/cma-es/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on CMA - Evolution Strategy.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measure theory - III: the Lebesgue integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</link>
      <pubDate>Sun, 21 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p3/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;(Temporary being stopped) Note III of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p3/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Generalized Linear Models</title>
      <link>https://trunghng.github.io/posts/machine-learning/glm/</link>
      <pubDate>Sat, 13 Aug 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/machine-learning/glm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measure theory - II: Lebesgue measure</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</link>
      <pubDate>Sun, 03 Jul 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p2/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note II of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p2/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measure theory - I: Elementary measure, Jordan measure &amp; the Riemann integral</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</link>
      <pubDate>Thu, 16 Jun 2022 13:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure-theory-p1/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note I of the measure theory series. Materials are mostly taken from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#taos-book&#34;&gt;Tao&amp;rsquo;s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&#34;https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#steins-book&#34;&gt;Stein&amp;rsquo;s book&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Likelihood Ratio Policy Gradient via Importance Sampling</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</link>
      <pubDate>Wed, 25 May 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Connection between Likelihood ratio policy gradient method and Importance sampling method.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Planning &amp; Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</link>
      <pubDate>Thu, 19 May 2022 14:09:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/planning-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;dynamic programming (DP) method&lt;/a&gt; in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;Monte Carlo methods&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/&#34;&gt;temporal-difference learning&lt;/a&gt;, the models are unnecessary. Such methods with requirement of a model like the case of DP is called &lt;strong&gt;model-based&lt;/strong&gt;, while methods without using a model is called &lt;strong&gt;model-free&lt;/strong&gt;. Model-based methods primarily rely on &lt;strong&gt;planning&lt;/strong&gt;; and model-free methods, on the other hand, primarily rely on &lt;strong&gt;learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Policy Gradient Theorem</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</link>
      <pubDate>Wed, 04 May 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a &lt;strong&gt;parameterized policy&lt;/strong&gt;, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called &lt;strong&gt;policy gradient methods&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>The Exponential Family</title>
      <link>https://trunghng.github.io/posts/probability-statistics/exponential-family/</link>
      <pubDate>Mon, 04 Apr 2022 14:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/exponential-family/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on exponential family.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Eligible Traces</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</link>
      <pubDate>Sun, 13 Mar 2022 14:11:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Beside &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td&#34;&gt;$n$-step TD&lt;/a&gt; methods, there is another mechanism called &lt;strong&gt;eligible traces&lt;/strong&gt; that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Function Approximation</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</link>
      <pubDate>Fri, 11 Feb 2022 15:26:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/func-approx/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Temporal-Difference Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</link>
      <pubDate>Mon, 31 Jan 2022 16:55:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/td-learning/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;So far in this &lt;a href=&#34;https://trunghng.github.io/tags/my-rl/&#34;&gt;series&lt;/a&gt;, we have gone through the ideas of &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;dynamic programming&lt;/strong&gt; (DP)&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/&#34;&gt;&lt;strong&gt;Monte Carlo&lt;/strong&gt;&lt;/a&gt;. What will happen if we combine these ideas together? &lt;strong&gt;Temporal-difference (TD) learning&lt;/strong&gt; is our answer.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Gaussian Distribution</title>
      <link>https://trunghng.github.io/posts/probability-statistics/normal-dist/</link>
      <pubDate>Mon, 22 Nov 2021 14:46:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/normal-dist/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;The &lt;strong&gt;Gaussian (Normal) distribution&lt;/strong&gt; is a continuous distribution with a bell-shaped PDF used widely in statistics due to the &lt;strong&gt;Central Limit theorem&lt;/strong&gt;. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Power Series</title>
      <link>https://trunghng.github.io/posts/calculus/power-series/</link>
      <pubDate>Tue, 21 Sep 2021 15:40:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/power-series/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that in the previous note, &lt;a href=&#34;https://trunghng.github.io/posts/calculus/infinite-series-of-constants/&#34;&gt;Infinite Series of Constants&lt;/a&gt;, we mentioned a type of series called &lt;strong&gt;power series&lt;/strong&gt; a lot. In the content of this note, we will be diving deeper into details of its.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Infinite Series of Constants</title>
      <link>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</link>
      <pubDate>Mon, 06 Sep 2021 11:20:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/calculus/infinite-series-of-constants/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Notes on infinite series of constants.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Monte Carlo Methods in Reinforcement Learning</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</link>
      <pubDate>Sat, 21 Aug 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Recall that when using &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/&#34;&gt;&lt;strong&gt;Dynamic Programming&lt;/strong&gt;&lt;/a&gt; algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods, we only require &lt;strong&gt;experience&lt;/strong&gt; - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Solving MDPs with Dynamic Programming</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</link>
      <pubDate>Sun, 25 Jul 2021 15:30:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In two previous notes, &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;MDPs and Bellman equations&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/&#34;&gt;&lt;strong&gt;Optimal Policy Existence&lt;/strong&gt;&lt;/a&gt;, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with &lt;strong&gt;Dynamic Programming&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Optimal Policy Existence</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</link>
      <pubDate>Sat, 10 Jul 2021 13:03:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;In the previous note about &lt;a href=&#34;https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/&#34;&gt;&lt;strong&gt;Markov Decision Processes, Bellman equations&lt;/strong&gt;&lt;/a&gt;, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Measures</title>
      <link>https://trunghng.github.io/posts/measure-theory/measure/</link>
      <pubDate>Sat, 03 Jul 2021 07:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/measure-theory/measure/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;When talking about &lt;strong&gt;measure&lt;/strong&gt;, you might associate it with the idea of &lt;strong&gt;length&lt;/strong&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;strong&gt;area&lt;/strong&gt;, or even three dimensions with &lt;strong&gt;volume&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Decision Processes, Bellman equations</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</link>
      <pubDate>Sun, 27 Jun 2021 08:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;You may have known or heard vaguely about a computer program called &lt;strong&gt;AlphaGo&lt;/strong&gt; - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called &lt;strong&gt;self-play&lt;/strong&gt; against its other instances, with &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>Markov Chain</title>
      <link>https://trunghng.github.io/posts/probability-statistics/markov-chain/</link>
      <pubDate>Sat, 19 Jun 2021 22:27:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/probability-statistics/markov-chain/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;If we have to describe the defintition of &lt;strong&gt;Markov chain&lt;/strong&gt; in one statement, it will be: &amp;ldquo;It only matters where you are, not where you&amp;rsquo;ve been&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>My very first post</title>
      <link>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</link>
      <pubDate>Sat, 05 Jun 2021 17:00:00 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/linear-algebra/fibonacci-generator/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Enjoy my index-zero-ed note while staying tuned for next ones!&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
  </channel>
</rss>
