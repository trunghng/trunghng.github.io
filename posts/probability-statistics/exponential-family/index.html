<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The exponential family | Trung's Place</title><meta name=keywords content="mathematics,probability-statistics,exponential-family"><meta name=description content="
Notes on exponential family.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/probability-statistics/exponential-family/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:0}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="The exponential family"><meta property="og:description" content="
Notes on exponential family.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/probability-statistics/exponential-family/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-04T14:00:00+07:00"><meta property="article:modified_time" content="2022-04-04T14:00:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="The exponential family"><meta name=twitter:description content="
Notes on exponential family.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"The exponential family","item":"https://trunghng.github.io/posts/probability-statistics/exponential-family/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The exponential family","name":"The exponential family","description":" Notes on exponential family.\n","keywords":["mathematics","probability-statistics","exponential-family"],"articleBody":" Notes on exponential family.\nThe exponential family The exponential family of distributions is defined as family of distributions of form \\begin{equation} p(x;\\eta)=h(x)\\exp\\Big[\\eta^\\text{T}T(x)-A(\\eta)\\Big],\\label{eq:ef.1} \\end{equation} where\n$\\eta$ is known as the natural parameter, or canonical parameter, $T(X)$ is referred to as a sufficient statistic, $A(\\eta)$ is called the cumulant function, which can be view as the logarithm of a normalization factor since integrating \\eqref{eq:ef.1} w.r.t the measure $\\nu$ gives us \\begin{equation} A(\\eta)=\\log\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx),\\label{eq:ef.2} \\end{equation} This also implies that $A(\\eta)$ will be determined once we have specified $\\nu,T(x)$ and $h(x)$. The set of parameters $\\eta$ for which the integral in \\eqref{eq:ef.2} is finite is known as the natural parameter space \\begin{equation} N=\\left\\{\\eta:\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx)\u003c\\infty\\right\\} \\end{equation} which explains why $\\eta$ is also referred as natural parameter. If $N$ is an non-empty open set, the exponential families are said to be regular.\nAn exponential family is known as minimal if there are no linear constraints among the components of $\\eta$ nor are there linear constraints among the components of $T(x)$.\nExamples Each particular choice of $\\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\\eta$. As we vary $\\eta$, we then get different distributions within this family.\nBernoulli distribution The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\\sim\\text{Bern}(\\pi)$, is given by \\begin{align} p(x;\\pi)\u0026=\\pi^x(1-\\pi)^{1-x} \\\\ \u0026=\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026=\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which can be written in the form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026=1 \\end{align} Notice that the relationship between $\\eta$ and $\\pi$ is invertible since \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}}, \\end{equation} which is the sigmoid function.\nBinomial distribution The probability mass function of a Binomial random variable $X$, denoted as $X\\sim\\text{Bin}(N,\\pi)$, is defined as \\begin{align} p(x;N,\\pi)\u0026={N\\choose x}\\pi^{x}(1-\\pi)^{1-x} \\\\ \u0026={N\\choose x}\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026={N\\choose x}\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which is in form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026={N\\choose x} \\end{align} Similar to the Bernoulli case, we also have the invertible relationship between $\\eta$ and $\\pi$ as \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}} \\end{equation}\nPoisson distribution The probability mass function of a Poisson random variable $X$, denoted as $X\\sim\\text{Pois}(\\lambda)$, is given as \\begin{align} p(x;\\lambda)\u0026=\\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\ \u0026=\\frac{1}{x!}\\exp\\left(x\\log\\lambda-\\lambda\\right), \\end{align} which is also able to be written as an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\log\\lambda \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=\\lambda=e^{\\eta} \\\\ h(x)\u0026=\\frac{1}{x!} \\end{align} Analogy to Bernoulli distribution, we also have that \\begin{equation} \\lambda=e^{\\eta} \\end{equation}\nGaussian distribution The (univariate) Gaussian density of a random variable $X$, denoted as $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, is given by \\begin{align} p(x;\\mu,\\sigma^2)\u0026=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] \\\\ \u0026=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left[\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2-\\frac{1}{2\\sigma^2}\\mu^2-\\log\\sigma\\right], \\end{align} which allows us to write it as an instance of the exponential family with \\begin{align} \\eta\u0026=\\left[\\begin{matrix}\\mu/\\sigma^2 \\\\ -1/2\\sigma^2\\end{matrix}\\right] \\\\ T(x)\u0026=\\left[\\begin{matrix}x\\\\ x^2\\end{matrix}\\right] \\\\ A(\\eta)\u0026=\\frac{\\mu^2}{2\\sigma^2}+\\log\\sigma=-\\frac{\\eta_1^2}{4\\eta_2}-\\frac{1}{2}\\log(-2\\eta_2) \\\\ h(x)\u0026=\\frac{1}{\\sqrt{2\\pi}} \\end{align}\nMultinomial distribution Let $\\mathbf{X}=(X_1,\\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\\mathbf{\\pi}=(\\pi_1,\\ldots,\\pi_K)$ with $\\sum_{k=1}^{K}\\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.\nThen $\\mathbf{X}$ is said to have Multinomial distribution, denoted as $\\mathbf{X}\\sim\\text{Mult}_K(N,\\boldsymbol{\\pi})$, if its probability mass function is given as with $\\sum_{k=1}^{K}x_k=1$ \\begin{align} p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\pi_1^{x_1}\\pi_2^{x_2}\\ldots\\pi_n^{x_n} \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right)\\label{eq:m.1} \\end{align} It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\\mathbf{x})$, which is \\begin{equation} \\sum_{k=1}^{K}x_k=1 \\end{equation} In order to remove this constraint, we substitute $1-\\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \\eqref{eq:m.1} be written by \\begin{align} \\hspace{-0.8cm}p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right) \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{k=1}^{K-1}x_k\\log\\pi_k+\\left(1-\\sum_{k=1}^{K-1}x_k\\right)\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right] \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{i=1}^{K-1}\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)x_i+\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right]\\label{eq:m.2} \\end{align} With this representation, and also for convenience, for $i=1,\\ldots,K$ we continue by letting \\begin{equation} \\eta_i=\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)=\\log\\left(\\frac{\\pi_i}{\\pi_K}\\right)\\label{eq:m.3} \\end{equation} Take the exponential of both sides and summing over $K$, we have \\begin{equation} \\sum_{i=1}^{K}e^{\\eta_i}=\\frac{\\sum_{i=1}^{K}\\pi_i}{\\pi_K}=\\frac{1}{\\pi_K}\\label{eq:m.4} \\end{equation} From this result, we have that the Multinomial distribution \\eqref{eq:m.2} is therefore also a member of the exponential family with \\begin{align} \\eta\u0026=\\left[\\begin{matrix}\\log\\left(\\pi_1/\\pi_K\\right) \\\\ \\vdots \\\\ \\log\\left(\\pi_K/\\pi_K\\right)\\end{matrix}\\right] \\\\ T(\\mathbf{x})\u0026=\\left[\\begin{matrix}x_1,\\ldots,x_K\\end{matrix}\\right]^\\text{T} \\\\ A(\\eta)\u0026=-\\log\\left(1-\\sum_{i=1}^{K-1}\\pi_i\\right)=-\\log(\\pi_K)=\\log\\left(\\sum_{k=1}^{K}e^{\\eta_k}\\right) \\\\ h(\\mathbf{x})\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!} \\end{align} Additionally, substituting the result \\eqref{eq:m.4} into \\eqref{eq:m.3} gives us for $i=1,\\ldots,K$ \\begin{equation} \\eta_i=\\log\\left(\\pi_i\\sum_{k=1}^{K}e^{\\eta_k}\\right), \\end{equation} or we can express $\\boldsymbol{\\pi}$ in terms of $\\eta$ by \\begin{equation} \\pi_i=\\frac{e^{\\eta_i}}{\\sum_{k=1}^{K}e^{\\eta_k}}, \\end{equation} which is the softmax function.\nMultivariate Normal distribution Convexity Theorem\nThe natural space $N$ is a convex set and the cumulant function $A(\\eta)$ is a convex function. If the family is minimal, then $A(\\eta)$ is strictly convex.\nProof\nLet $\\eta_1,\\eta_2\\in N$, thus from \\eqref{eq:ef.2}, we have that \\begin{align} \\exp\\big(A(\\eta_1)\\big)\u0026=A_1, \\\\ \\exp\\big(A(\\eta_2)\\big)\u0026=A_2 \\end{align} where $A_1,A_2$ are finite.\nTo prove that $N$ is convex, we need to show that for any $\\eta=\\lambda\\eta_1+(1-\\lambda)\\eta_2$ for $0\\lt\\lambda\\lt 1$, we also have $\\eta\\in N$. From \\eqref{eq:ef.2}, and by Hölder’s inequality1, we have \\begin{align} \\exp\\big(A(\\eta)\\big)\u0026=\\int h(x)\\exp\\big(\\eta^\\text{T}T(x)\\big)\\nu(dx) \\\\ \u0026=\\int h(x)\\exp\\Big[\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big)^\\text{T}T(x)\\Big]\\nu(dx) \\\\ \u0026=\\int \\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda}\\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}\\nu(dx) \\\\ \u0026\\leq\\Bigg[\\int h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^\\lambda\\Bigg[\\int h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^{1-\\lambda} \\\\ \u0026=\\Big[\\exp\\big(A(\\eta_1)\\big)\\Big]^\\lambda\\Big[\\exp\\big(A(\\eta_2)\\big)\\Big]^{1-\\lambda} \\\\ \u0026=A_1^\\lambda A_2^{1-\\lambda},\\label{eq:c.1} \\end{align} which proves that $A(\\eta)$ is finite, or $\\eta\\in N$.\nMoreover, taking logarithm of both sides of \\eqref{eq:c.1} gives us \\begin{equation} \\lambda A(\\eta_1)+(1-\\lambda)A(\\eta_2)\\geq A(\\eta)=A\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big), \\end{equation} which also claims the convexity of $A(\\eta)$.\nBy Hölder’s inequality, the equality in \\eqref{eq:c.1} holds when \\begin{equation} \\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}=c\\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda(1/\\lambda-1)} \\end{equation} or \\begin{equation} \\exp\\big(\\eta_2^\\text{T}T(x)\\big)=c\\exp\\big(\\eta_1^\\text{T}T(x)\\big), \\end{equation} and therefore \\begin{equation} (\\eta_2-\\eta_1)^\\text{T}T(x)=\\log c, \\end{equation} which is not minimal since $\\eta_1,\\eta_2$ are taken arbitrarily.\nMoments of sufficient statistic In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second cumulants.\nMeans, variances Let us first consider the first derivative of the cumulant function $A(\\eta)$. By the dominated convergence theorem, we have \\begin{align} \\frac{\\partial A(\\eta)}{\\partial\\eta^\\text{T}}\u0026=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\log\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx) \\\\ \u0026=\\frac{\\int T(x)\\exp\\big(\\eta^\\text{T}(x)\\big)h(x)\\nu(dx)}{\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx)} \\\\ \u0026=\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx)\\label{eq:mv.1} \\\\ \u0026=\\int T(x)p(x;\\eta)\\nu(dx) \\\\ \u0026=\\mathbb{E}[T(X)], \\end{align} which is the mean of the sufficient statistic $T(x)$.\nMoreover, taking the second derivative of cumulant function by continuing with the result \\eqref{eq:mv.1}, we have \\begin{align} \\frac{\\partial^2 A(\\eta)}{\\partial\\eta\\partial\\eta^\\text{T}}\u0026=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\int T(x)\\left(T(x)-\\frac{\\partial}{\\partial\\eta^\\text{T}}A(\\eta)\\right)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\int T(x)\\big(T(x)-E(T(X))\\big)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\mathbb{E}\\left[T(X)T(X)^\\text{T}\\right]-\\mathbb{E}[T(X)]\\mathbb{E}[T(X)]^\\text{T} \\\\ \u0026=\\text{Var}[T(X)], \\end{align} which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$.\nMoment generating functions The moment generating function (or MGF) of a random variable $X$, denoted as $M(t)$, is given by \\begin{equation} M(t)=\\mathbb{E}(e^{t^\\text{T}X}), \\end{equation} for all values of $t$ for which the expectation exists.\nThe MGF of the sufficient statistic $T(X)$ then can be computed as \\begin{align} M_{T(X)}(t)\u0026=\\mathbb{E}(e^{t^\\text{T}T(X)}) \\\\ \u0026=\\int \\exp\\big((\\eta+t)^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\exp\\big(A(\\eta+t)-A(\\eta)\\big)\\label{eq:mgf.1} \\end{align}\nCumulant generating functions The cumulant generating function (or CGF) of a random variable $X$, denoted by $K(t)$, is given as \\begin{equation} K(t)=\\log M(t)=\\log\\mathbb{E}(e^{t^\\text{T}X}), \\end{equation} for all values of $t$ for which the expectation exists.\nFrom the MGF of $T(X)$ in \\eqref{eq:mgf.1}, the CGF of the sufficient statistic $T(X)$ therefore can be calculated by \\begin{equation} K_{T(X)}(t)=\\log M_{T(X)}(t)=A(\\eta+t)-A(\\eta) \\end{equation}\nCumulants The $k$-th cumulant of a random variable $X$ is defined to be the $k$-th derivative of $K_{X}(t)$ at $0$, i.e., \\begin{equation} c_k=K^{(k)}(0) \\end{equation}\nThus, the mean of $T(X)$ is exactly the first cumulant, while the variance is the second cumulant of $T(X)$.\nSufficiency Maximum likelihood estimates Consider an i.i.d data set $\\mathcal{D}=\\{x_1,\\ldots,x_N\\}$, the likelihood function is then given by \\begin{align} L(\\eta)=p(\\mathbf{X}\\vert\\eta)\u0026=\\prod_{n=1}^{N}p(x_n\\vert\\eta) \\\\ \u0026=\\prod_{n=1}^{N}h(x_n)\\exp\\big[\\eta^\\text{T}T(x_n)-A(\\eta)\\big] \\\\ \u0026=\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right]\\label{eq:mle.1} \\end{align} Taking the logarithm of both sides gives us the log likelihood as \\begin{equation} \\ell(\\eta)=\\log L(\\eta)=\\log\\left(\\prod_{n=1}^{N}h(x_n)\\right)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta) \\end{equation} Consider the gradient of the log likelihood w.r.t $\\eta$, we have \\begin{align} \\nabla_\\eta\\ell(\\eta)\u0026=\\nabla_\\eta\\left[\\log\\left(\\prod_{n=1}^{N}h(x_n)\\right)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026=\\sum_{n=1}^{N}T(x_n)-N\\nabla_\\eta A(\\eta) \\end{align} Setting the gradient to zero, we have the value of $\\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\\eta$, denoted as $\\eta_\\text{ML}$ satisfies \\begin{equation} \\nabla_{\\eta}A(\\eta_\\text{ML})=\\frac{1}{N}\\sum_{n=1}^{N}T(x_n) \\end{equation}\nConjugate priors Given a probability distribution $p(x\\vert\\eta)$, its prior $p(\\eta)$ is said to be conjugate to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as conjugate prior.\nFor any member of the exponential family, there exists a conjugate prior that can be written in form \\begin{equation} p(\\eta\\vert\\mathcal{X},\\theta)=f(\\mathcal{X},\\theta)\\exp(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)),\\label{eq:cp.1} \\end{equation} where $\\theta\u003e0$ and $\\mathcal{X}$ are hyperparameters.\nBy Bayes’ rule, and with the likelihood function as given in \\eqref{eq:mle.1}, the posterior distribution can be computed as \\begin{align} \u0026\\hspace{0.7cm}p(\\eta\\vert\\mathbf{X},\\mathcal{X},\\theta) \\\\ \u0026\\propto p(\\eta\\vert\\mathcal{X},\\theta)p(\\mathbf{X}\\vert\\eta) \\\\ \u0026=f(\\mathcal{X},\\theta)\\exp\\big(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)\\big)\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026\\propto\\exp\\left[\\eta^\\text{T}\\left(\\mathcal{X}+\\sum_{n=1}^{N}T(x_n)\\right)-(\\theta+N)A(\\eta)\\right], \\end{align} which is in the same form as \\eqref{eq:cp.1} and therefore claims the conjugacy.\nReferences [1] M. Jordan. The Exponential Family: Basics. 2009.\n[2] Joseph K. Blitzstein \u0026 Jessica Hwang. Introduction to Probability.\n[3] Weisstein, Eric W. Hölder’s Inequalities From MathWorld–A Wolfram Web Resource.\nFootnotes Let $p,q\u003e1$ such that \\begin{equation*} \\frac{1}{p}+\\frac{1}{q}=1 \\end{equation*} The Hölder’s inequality for integrals states that \\begin{equation*} \\int_a^b\\vert f(x)g(x)\\vert\\hspace{0.1cm}dx\\leq\\left(\\int_a^b\\vert f(x)\\vert\\hspace{0.1cm}dx\\right)^{1/p}\\left(\\int_a^b\\vert g(x)\\vert\\hspace{0.1cm}dx\\right)^{1/q} \\end{equation*} The equality holds with \\begin{equation*} \\vert g(x)\\vert=c\\vert f(x)\\vert^{p-1} \\end{equation*} ↩︎\n","wordCount":"1390","inLanguage":"en","datePublished":"2022-04-04T14:00:00+07:00","dateModified":"2022-04-04T14:00:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/probability-statistics/exponential-family/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>The exponential family</h1><div class=post-meta><span title='2022-04-04 14:00:00 +0700 +0700'>April 4, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#exp-fam>The exponential family</a></li><li><a href=#examples>Examples</a><ul><li><a href=#bern>Bernoulli distribution</a></li><li><a href=#binomial-distribution>Binomial distribution</a></li><li><a href=#poisson-distribution>Poisson distribution</a></li><li><a href=#gaussian-distribution>Gaussian distribution</a></li><li><a href=#multinomial-distribution>Multinomial distribution</a></li><li><a href=#mvn>Multivariate Normal distribution</a></li></ul></li><li><a href=#cvxt>Convexity</a></li><li><a href=#mmt-suff-stat>Moments of sufficient statistic</a><ul><li><a href=#mean-var>Means, variances</a></li><li><a href=#mgf>Moment generating functions</a></li><li><a href=#cgf>Cumulant generating functions</a></li><li><a href=#cumulants>Cumulants</a></li></ul></li><li><a href=#sufficiency>Sufficiency</a></li><li><a href=#mle>Maximum likelihood estimates</a></li><li><a href=#conj-prior>Conjugate priors</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on exponential family.</p></blockquote><h2 id=exp-fam>The exponential family<a hidden class=anchor aria-hidden=true href=#exp-fam>#</a></h2><p>The <strong>exponential family</strong> of distributions is defined as family of distributions of form
\begin{equation}
p(x;\eta)=h(x)\exp\Big[\eta^\text{T}T(x)-A(\eta)\Big],\label{eq:ef.1}
\end{equation}
where</p><ul><li>$\eta$ is known as the <strong>natural parameter</strong>, or <strong>canonical parameter</strong>,</li><li>$T(X)$ is referred to as a <strong>sufficient statistic</strong>,</li><li>$A(\eta)$ is called the <strong>cumulant function</strong>, which can be view as the logarithm of a normalization factor since integrating \eqref{eq:ef.1} w.r.t the measure $\nu$ gives us
\begin{equation}
A(\eta)=\log\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx),\label{eq:ef.2}
\end{equation}
This also implies that $A(\eta)$ will be determined once we have specified $\nu,T(x)$ and $h(x)$.</li></ul><p>The set of parameters $\eta$ for which the integral in \eqref{eq:ef.2} is finite is known as the <strong>natural parameter space</strong>
\begin{equation}
N=\left\{\eta:\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx)&lt;\infty\right\}
\end{equation}
which explains why $\eta$ is also referred as <strong>natural parameter</strong>. If $N$ is an non-empty open set, the exponential families are said to be <strong>regular</strong>.</p><p>An exponential family is known as <strong>minimal</strong> if there are no linear constraints among the components of $\eta$ nor are there linear constraints among the components of $T(x)$.</p><h2 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h2><p>Each particular choice of $\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\eta$. As we vary $\eta$, we then get different distributions within this family.</p><h3 id=bern>Bernoulli distribution<a hidden class=anchor aria-hidden=true href=#bern>#</a></h3><p>The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\sim\text{Bern}(\pi)$, is given by
\begin{align}
p(x;\pi)&=\pi^x(1-\pi)^{1-x} \\ &=\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &=\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which can be written in the form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\frac{\pi}{1-\pi} \\ T(x)&=x \\ A(\eta)&=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&=1
\end{align}
Notice that the relationship between $\eta$ and $\pi$ is invertible since
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}},
\end{equation}
which is the <strong>sigmoid function</strong>.</p><h3 id=binomial-distribution>Binomial distribution<a hidden class=anchor aria-hidden=true href=#binomial-distribution>#</a></h3><p>The probability mass function of a Binomial random variable $X$, denoted as $X\sim\text{Bin}(N,\pi)$, is defined as
\begin{align}
p(x;N,\pi)&={N\choose x}\pi^{x}(1-\pi)^{1-x} \\ &={N\choose x}\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &={N\choose x}\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which is in form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\frac{\pi}{1-\pi} \\ T(x)&=x \\ A(\eta)&=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&={N\choose x}
\end{align}
Similar to the Bernoulli case, we also have the invertible relationship between $\eta$ and $\pi$ as
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}}
\end{equation}</p><h3 id=poisson-distribution>Poisson distribution<a hidden class=anchor aria-hidden=true href=#poisson-distribution>#</a></h3><p>The probability mass function of a Poisson random variable $X$, denoted as $X\sim\text{Pois}(\lambda)$, is given as
\begin{align}
p(x;\lambda)&=\frac{\lambda^x e^{-\lambda}}{x!} \\ &=\frac{1}{x!}\exp\left(x\log\lambda-\lambda\right),
\end{align}
which is also able to be written as an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\log\lambda \\ T(x)&=x \\ A(\eta)&=\lambda=e^{\eta} \\ h(x)&=\frac{1}{x!}
\end{align}
Analogy to Bernoulli distribution, we also have that
\begin{equation}
\lambda=e^{\eta}
\end{equation}</p><h3 id=gaussian-distribution>Gaussian distribution<a hidden class=anchor aria-hidden=true href=#gaussian-distribution>#</a></h3><p>The (univariate) Gaussian density of a random variable $X$, denoted as $X\sim\mathcal{N}(\mu,\sigma^2)$, is given by
\begin{align}
p(x;\mu,\sigma^2)&=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ &=\frac{1}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right],
\end{align}
which allows us to write it as an instance of the exponential family with
\begin{align}
\eta&=\left[\begin{matrix}\mu/\sigma^2 \\ -1/2\sigma^2\end{matrix}\right] \\ T(x)&=\left[\begin{matrix}x\\ x^2\end{matrix}\right] \\ A(\eta)&=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2) \\ h(x)&=\frac{1}{\sqrt{2\pi}}
\end{align}</p><h3 id=multinomial-distribution>Multinomial distribution<a hidden class=anchor aria-hidden=true href=#multinomial-distribution>#</a></h3><p>Let $\mathbf{X}=(X_1,\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\mathbf{\pi}=(\pi_1,\ldots,\pi_K)$ with $\sum_{k=1}^{K}\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.</p><p>Then $\mathbf{X}$ is said to have Multinomial distribution, denoted as $\mathbf{X}\sim\text{Mult}_K(N,\boldsymbol{\pi})$, if its probability mass function is given as with $\sum_{k=1}^{K}x_k=1$
\begin{align}
p(\mathbf{x};\boldsymbol{\pi},N,K)&=\frac{N!}{x_1!x_2!\ldots x_K!}\pi_1^{x_1}\pi_2^{x_2}\ldots\pi_n^{x_n} \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right)\label{eq:m.1}
\end{align}
It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\mathbf{x})$, which is
\begin{equation}
\sum_{k=1}^{K}x_k=1
\end{equation}
In order to remove this constraint, we substitute $1-\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \eqref{eq:m.1} be written by
\begin{align}
\hspace{-0.8cm}p(\mathbf{x};\boldsymbol{\pi},N,K)&=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right) \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{k=1}^{K-1}x_k\log\pi_k+\left(1-\sum_{k=1}^{K-1}x_k\right)\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right] \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{i=1}^{K-1}\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)x_i+\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right]\label{eq:m.2}
\end{align}
With this representation, and also for convenience, for $i=1,\ldots,K$ we continue by letting
\begin{equation}
\eta_i=\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)=\log\left(\frac{\pi_i}{\pi_K}\right)\label{eq:m.3}
\end{equation}
Take the exponential of both sides and summing over $K$, we have
\begin{equation}
\sum_{i=1}^{K}e^{\eta_i}=\frac{\sum_{i=1}^{K}\pi_i}{\pi_K}=\frac{1}{\pi_K}\label{eq:m.4}
\end{equation}
From this result, we have that the Multinomial distribution \eqref{eq:m.2} is therefore also a member of the exponential family with
\begin{align}
\eta&=\left[\begin{matrix}\log\left(\pi_1/\pi_K\right) \\ \vdots \\ \log\left(\pi_K/\pi_K\right)\end{matrix}\right] \\ T(\mathbf{x})&=\left[\begin{matrix}x_1,\ldots,x_K\end{matrix}\right]^\text{T} \\ A(\eta)&=-\log\left(1-\sum_{i=1}^{K-1}\pi_i\right)=-\log(\pi_K)=\log\left(\sum_{k=1}^{K}e^{\eta_k}\right) \\ h(\mathbf{x})&=\frac{N!}{x_1!x_2!\ldots x_K!}
\end{align}
Additionally, substituting the result \eqref{eq:m.4} into \eqref{eq:m.3} gives us for $i=1,\ldots,K$
\begin{equation}
\eta_i=\log\left(\pi_i\sum_{k=1}^{K}e^{\eta_k}\right),
\end{equation}
or we can express $\boldsymbol{\pi}$ in terms of $\eta$ by
\begin{equation}
\pi_i=\frac{e^{\eta_i}}{\sum_{k=1}^{K}e^{\eta_k}},
\end{equation}
which is the <strong>softmax function</strong>.</p><h3 id=mvn>Multivariate Normal distribution<a hidden class=anchor aria-hidden=true href=#mvn>#</a></h3><h2 id=cvxt>Convexity<a hidden class=anchor aria-hidden=true href=#cvxt>#</a></h2><p><strong>Theorem</strong><br>The natural space $N$ is a convex set and the cumulant function $A(\eta)$ is a convex function. If the family is minimal, then $A(\eta)$ is strictly convex.</p><p><strong>Proof</strong><br>Let $\eta_1,\eta_2\in N$, thus from \eqref{eq:ef.2}, we have that
\begin{align}
\exp\big(A(\eta_1)\big)&=A_1, \\ \exp\big(A(\eta_2)\big)&=A_2
\end{align}
where $A_1,A_2$ are finite.</p><p>To prove that $N$ is convex, we need to show that for any $\eta=\lambda\eta_1+(1-\lambda)\eta_2$ for $0\lt\lambda\lt 1$, we also have $\eta\in N$. From \eqref{eq:ef.2}, and by <strong>Hölder&rsquo;s inequality</strong><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, we have
\begin{align}
\exp\big(A(\eta)\big)&=\int h(x)\exp\big(\eta^\text{T}T(x)\big)\nu(dx) \\ &=\int h(x)\exp\Big[\big(\lambda\eta_1+(1-\lambda)\eta_2\big)^\text{T}T(x)\Big]\nu(dx) \\ &=\int \Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda}\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}\nu(dx) \\ &\leq\Bigg[\int h(x)\exp\big(\eta_1^\text{T}T(x)\big)\nu(dx)\Bigg]^\lambda\Bigg[\int h(x)\exp\big(\eta_2^\text{T}T(x)\big)\nu(dx)\Bigg]^{1-\lambda} \\ &=\Big[\exp\big(A(\eta_1)\big)\Big]^\lambda\Big[\exp\big(A(\eta_2)\big)\Big]^{1-\lambda} \\ &=A_1^\lambda A_2^{1-\lambda},\label{eq:c.1}
\end{align}
which proves that $A(\eta)$ is finite, or $\eta\in N$.</p><p>Moreover, taking logarithm of both sides of \eqref{eq:c.1} gives us
\begin{equation}
\lambda A(\eta_1)+(1-\lambda)A(\eta_2)\geq A(\eta)=A\big(\lambda\eta_1+(1-\lambda)\eta_2\big),
\end{equation}
which also claims the convexity of $A(\eta)$.</p><p>By Hölder&rsquo;s inequality, the equality in \eqref{eq:c.1} holds when
\begin{equation}
\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}=c\Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda(1/\lambda-1)}
\end{equation}
or
\begin{equation}
\exp\big(\eta_2^\text{T}T(x)\big)=c\exp\big(\eta_1^\text{T}T(x)\big),
\end{equation}
and therefore
\begin{equation}
(\eta_2-\eta_1)^\text{T}T(x)=\log c,
\end{equation}
which is not minimal since $\eta_1,\eta_2$ are taken arbitrarily.</p><h2 id=mmt-suff-stat>Moments of sufficient statistic<a hidden class=anchor aria-hidden=true href=#mmt-suff-stat>#</a></h2><p>In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second <strong>cumulants</strong>.</p><h3 id=mean-var>Means, variances<a hidden class=anchor aria-hidden=true href=#mean-var>#</a></h3><p>Let us first consider the first derivative of the cumulant function $A(\eta)$. By the <strong>dominated convergence theorem</strong>, we have
\begin{align}
\frac{\partial A(\eta)}{\partial\eta^\text{T}}&=\frac{\partial}{\partial\eta^\text{T}}\log\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx) \\ &=\frac{\int T(x)\exp\big(\eta^\text{T}(x)\big)h(x)\nu(dx)}{\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx)} \\ &=\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx)\label{eq:mv.1} \\ &=\int T(x)p(x;\eta)\nu(dx) \\ &=\mathbb{E}[T(X)],
\end{align}
which is the mean of the sufficient statistic $T(x)$.</p><p>Moreover, taking the second derivative of cumulant function by continuing with the result \eqref{eq:mv.1}, we have
\begin{align}
\frac{\partial^2 A(\eta)}{\partial\eta\partial\eta^\text{T}}&=\frac{\partial}{\partial\eta^\text{T}}\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\int T(x)\left(T(x)-\frac{\partial}{\partial\eta^\text{T}}A(\eta)\right)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\int T(x)\big(T(x)-E(T(X))\big)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\mathbb{E}\left[T(X)T(X)^\text{T}\right]-\mathbb{E}[T(X)]\mathbb{E}[T(X)]^\text{T} \\ &=\text{Var}[T(X)],
\end{align}
which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$.</p><h3 id=mgf>Moment generating functions<a hidden class=anchor aria-hidden=true href=#mgf>#</a></h3><p>The <strong>moment generating function</strong> (or <strong>MGF</strong>) of a random variable $X$, denoted as $M(t)$, is given by
\begin{equation}
M(t)=\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.</p><p>The MGF of the sufficient statistic $T(X)$ then can be computed as
\begin{align}
M_{T(X)}(t)&=\mathbb{E}(e^{t^\text{T}T(X)}) \\ &=\int \exp\big((\eta+t)^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\exp\big(A(\eta+t)-A(\eta)\big)\label{eq:mgf.1}
\end{align}</p><h3 id=cgf>Cumulant generating functions<a hidden class=anchor aria-hidden=true href=#cgf>#</a></h3><p>The <strong>cumulant generating function</strong> (or <strong>CGF</strong>) of a random variable $X$, denoted by $K(t)$, is given as
\begin{equation}
K(t)=\log M(t)=\log\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.</p><p>From the MGF of $T(X)$ in \eqref{eq:mgf.1}, the CGF of the sufficient statistic $T(X)$ therefore can be calculated by
\begin{equation}
K_{T(X)}(t)=\log M_{T(X)}(t)=A(\eta+t)-A(\eta)
\end{equation}</p><h3 id=cumulants>Cumulants<a hidden class=anchor aria-hidden=true href=#cumulants>#</a></h3><p>The $k$-th <strong>cumulant</strong> of a random variable $X$ is defined to be the $k$-th derivative of $K_{X}(t)$ at $0$, i.e.,
\begin{equation}
c_k=K^{(k)}(0)
\end{equation}</p><p>Thus, the mean of $T(X)$ is exactly the first cumulant, while the variance is the second cumulant of $T(X)$.</p><h2 id=sufficiency>Sufficiency<a hidden class=anchor aria-hidden=true href=#sufficiency>#</a></h2><h2 id=mle>Maximum likelihood estimates<a hidden class=anchor aria-hidden=true href=#mle>#</a></h2><p>Consider an i.i.d data set $\mathcal{D}=\{x_1,\ldots,x_N\}$, the likelihood function is then given by
\begin{align}
L(\eta)=p(\mathbf{X}\vert\eta)&=\prod_{n=1}^{N}p(x_n\vert\eta) \\ &=\prod_{n=1}^{N}h(x_n)\exp\big[\eta^\text{T}T(x_n)-A(\eta)\big] \\ &=\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right]\label{eq:mle.1}
\end{align}
Taking the logarithm of both sides gives us the log likelihood as
\begin{equation}
\ell(\eta)=\log L(\eta)=\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)
\end{equation}
Consider the gradient of the log likelihood w.r.t $\eta$, we have
\begin{align}
\nabla_\eta\ell(\eta)&=\nabla_\eta\left[\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &=\sum_{n=1}^{N}T(x_n)-N\nabla_\eta A(\eta)
\end{align}
Setting the gradient to zero, we have the value of $\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\eta$, denoted as $\eta_\text{ML}$ satisfies
\begin{equation}
\nabla_{\eta}A(\eta_\text{ML})=\frac{1}{N}\sum_{n=1}^{N}T(x_n)
\end{equation}</p><h2 id=conj-prior>Conjugate priors<a hidden class=anchor aria-hidden=true href=#conj-prior>#</a></h2><p>Given a probability distribution $p(x\vert\eta)$, its prior $p(\eta)$ is said to be <strong>conjugate</strong> to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as <strong>conjugate prior</strong>.</p><p>For any member of the exponential family, there exists a conjugate prior that can be written in form
\begin{equation}
p(\eta\vert\mathcal{X},\theta)=f(\mathcal{X},\theta)\exp(\eta^\text{T}\mathcal{X}-\theta A(\eta)),\label{eq:cp.1}
\end{equation}
where $\theta>0$ and $\mathcal{X}$ are hyperparameters.</p><p>By Bayes&rsquo; rule, and with the likelihood function as given in \eqref{eq:mle.1}, the posterior distribution can be computed as
\begin{align}
&\hspace{0.7cm}p(\eta\vert\mathbf{X},\mathcal{X},\theta) \\ &\propto p(\eta\vert\mathcal{X},\theta)p(\mathbf{X}\vert\eta) \\ &=f(\mathcal{X},\theta)\exp\big(\eta^\text{T}\mathcal{X}-\theta A(\eta)\big)\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &\propto\exp\left[\eta^\text{T}\left(\mathcal{X}+\sum_{n=1}^{N}T(x_n)\right)-(\theta+N)A(\eta)\right],
\end{align}
which is in the same form as \eqref{eq:cp.1} and therefore claims the conjugacy.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] M. Jordan. <a href=https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf>The Exponential Family: Basics</a>. 2009.</p><p>[2] Joseph K. Blitzstein & Jessica Hwang. <a href=https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573>Introduction to Probability</a>.</p><p>[3] Weisstein, Eric W. <a href=https://mathworld.wolfram.com/HoeldersInequalities.html>Hölder&rsquo;s Inequalities</a> From MathWorld&ndash;A Wolfram Web Resource.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Let $p,q>1$ such that
\begin{equation*}
\frac{1}{p}+\frac{1}{q}=1
\end{equation*}
The <strong>Hölder&rsquo;s inequality</strong> for integrals states that
\begin{equation*}
\int_a^b\vert f(x)g(x)\vert\hspace{0.1cm}dx\leq\left(\int_a^b\vert f(x)\vert\hspace{0.1cm}dx\right)^{1/p}\left(\int_a^b\vert g(x)\vert\hspace{0.1cm}dx\right)^{1/q}
\end{equation*}
The equality holds with
\begin{equation*}
\vert g(x)\vert=c\vert f(x)\vert^{p-1}
\end{equation*}&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/mathematics/>mathematics</a></li><li><a href=https://trunghng.github.io/tags/probability-statistics/>probability-statistics</a></li><li><a href=https://trunghng.github.io/tags/exponential-family/>exponential-family</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/><span class=title>« Prev</span><br><span>Policy Gradient Theorem</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/><span class=title>Next »</span><br><span>Eligible Traces</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on twitter" href="https://twitter.com/intent/tweet/?text=The%20exponential%20family&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f&hashtags=mathematics%2cprobability-statistics%2cexponential-family"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f&title=The%20exponential%20family&summary=The%20exponential%20family&source=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f&title=The%20exponential%20family"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on whatsapp" href="https://api.whatsapp.com/send?text=The%20exponential%20family%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The exponential family on telegram" href="https://telegram.me/share/url?text=The%20exponential%20family&url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fexponential-family%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>