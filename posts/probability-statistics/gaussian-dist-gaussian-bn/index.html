<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Gaussian Distribution & Gaussian Network Models | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="mathematics,probability-statistics,normal-distribution,probabilistic-graphical-model"><meta name=description content="
Notes on Gaussian distribution & Gaussian network models.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><meta property="og:title" content="Gaussian Distribution & Gaussian Network Models"><meta property="og:description" content="
Notes on Gaussian distribution & Gaussian network models.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-11-22T14:46:00+07:00"><meta property="article:modified_time" content="2021-11-22T14:46:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Gaussian Distribution & Gaussian Network Models"><meta name=twitter:description content="
Notes on Gaussian distribution & Gaussian network models.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Gaussian Distribution \u0026 Gaussian Network Models","item":"https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Gaussian Distribution \u0026 Gaussian Network Models","name":"Gaussian Distribution \u0026 Gaussian Network Models","description":" Notes on Gaussian distribution \u0026amp; Gaussian network models.\n","keywords":["mathematics","probability-statistics","normal-distribution","probabilistic-graphical-model"],"articleBody":" Notes on Gaussian distribution \u0026 Gaussian network models.\n$\\newcommand{\\Var}{\\mathrm{Var}}$ $\\newcommand{\\Cov}{\\mathrm{Cov}}$\nNormal Distribution Standard Normal Distribution A continuous random variable $Z$ is said to have the standard Normal distribution if its PDF $\\varphi$ is given by \\begin{equation} \\varphi(z)=\\frac{1}{\\sqrt{2\\pi}e^{-z^2/2}},\\hspace{1cm}z\\in(-\\infty,\\infty) \\end{equation} and denoted $Z\\sim\\mathcal{N}(0,1)$, since, as we will show, $Z$ has mean $0$ and variance $1$. Before that, let us compute the CDF of $Z$, which is given as \\begin{equation} \\Phi(z)=\\int_{-\\infty}^{z}\\varphi(t)dt=\\int_{-\\infty}^{z}\\frac{1}{\\sqrt{2\\pi}}e^{-t^2/2}dt \\end{equation} We continue by verifying that $\\mathcal{N}{(0,1)}$ is indeed a distribution. Since $\\varphi(z)\\geq 0$, our problem remains to show that the PDF of $Z$ integrates to $1$. In particular, we have \\begin{align} \\left(\\int_{-\\infty}^{\\infty}e^{-x^2/2}dx\\right)\\left(\\int_{-\\infty}^{\\infty}e^{-y^2/2}dy\\right)\u0026=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}e^{-x^2/2}e^{-y^2/2}dydx \\\\ \u0026=\\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2+y^2}{2}}dxdy\\label{eq:sn.1} \\end{align} Let us change the variables, specifically, we will be changing from the Cartesian coordinate to the polar coordinate, by letting \\begin{align} x\u0026=r\\cos\\theta, \\\\ y\u0026=r\\sin\\theta, \\end{align} where $r\\geq 0$ is the distance from $(x,y)$ to the origin and $\\theta\\in[0,2\\pi)$ is the angle. The Jacobian matrix of this transformation is \\begin{equation} \\frac{d(x,y)}{d(r,\\theta)}=\\left[\\begin{matrix}\\cos\\theta\u0026-r\\sin\\theta \\\\ \\sin\\theta\u0026 r\\cos\\theta\\end{matrix}\\right], \\end{equation} which implies that \\begin{equation} \\text{det}\\frac{d(x,y)}{d(r,\\theta)}=\\text{det}\\left[\\begin{matrix}\\cos\\theta\u0026-r\\sin\\theta \\\\ \\sin\\theta\u0026 r\\cos\\theta\\end{matrix}\\right]=1 \\end{equation} This makes us continue to derive \\eqref{eq:sn.1} as \\begin{align} \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}e^{-\\frac{x^2+y^2}{2}}dxdy\u0026=\\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-r^2/2}\\left\\vert\\text{det}\\frac{d(x,y)}{d(r,\\theta)}\\right\\vert rdrd\\theta \\\\ \u0026=\\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-r^2/2}rdrd\\theta \\end{align} Let $u=r^2/2$, then $du=rdr$, we have \\begin{equation} \\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-r^2/2}rdrd\\theta=\\int_{0}^{2\\pi}\\int_{0}^{\\infty}e^{-u}dud\\theta=\\int_{0}^{2\\pi}1d\\theta=2\\pi \\end{equation} And hence, we can conclude that \\begin{equation} \\int_{-\\infty}^{\\infty}e^{-z^2/2}dz=\\sqrt{2\\pi} \\end{equation} or \\begin{equation} \\int_{-\\infty}^{\\infty}\\varphi(z)dz=\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}e^{-z^2/2}dz=1 \\end{equation}\nStandard Mean, Standard Variance We have that the mean of a standard Normal r.v $Z$ is given as \\begin{align} \\mathbb{E}(Z)\u0026=\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}z e^{-z^2/2}dz \\\\ \u0026=\\frac{1}{\\sqrt{2\\pi}}\\left(\\int_{0}^{\\infty}z e^{-z^2/2}dz+\\int_{-\\infty}^{0}z e^{-z^2/2}dz\\right) \\\\ \u0026=\\frac{1}{\\sqrt{2\\pi}}\\left(\\int_{0}^{\\infty}z e^{-z^2/2}dz-\\int_{0}^{\\infty}z e^{-z^2/2}dz\\right) \\\\ \u0026=0 \\end{align} where in the third step, we use the fact that $ze^{-z^2/2}$ is an odd function.\nGiven the mean of $Z$, we have its variance is then can be computed by \\begin{align} \\Var(Z)\u0026=\\mathbb{E}(Z^2)-(\\mathbb{E}Z)^2 \\\\ \u0026=\\mathbb{E}(Z^2) \\\\ \u0026=\\int_{-\\infty}^{\\infty}\\frac{1}{\\sqrt{2\\pi}}z^2 e^{-z^2/2}dz \\\\ \u0026=\\frac{\\sqrt{2}}{\\sqrt{\\pi}}\\int_{0}^{\\infty}z^2 e^{-z^2/2}dz \\end{align} where the last step uses the fact that $z^2 e^{-z^2/2}$ is an even function. We continue by using integration by parts with $u=z$ and $dv=z e^{-z^2/2}$, then $du=dz$ and $v=-e^{-z^2/2}$. Thus, \\begin{align} \\Var(Z)\u0026=\\frac{\\sqrt{2}}{\\sqrt{\\pi}}\\left(-z e^{-z^2/2}\\Big\\vert_{0}^{\\infty}+\\int_{0}^{\\infty}e^{-z^2/2}dz\\right) \\\\ \u0026=\\frac{\\sqrt{2}}{\\sqrt{\\pi}}\\left(0+\\frac{\\sqrt{2\\pi}}{2}\\right) \\\\ \u0026=1 \\end{align}\nUnivariate Normal Distribution Let $Z\\sim\\mathcal{N}(0,1)$ be a standard Normal r.v, then a continuous r.v $X$ is said to be a Gaussian or to have the (Univariate) Normal distribution with mean $\\mu$ and variance $\\sigma^2$, denoted $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$ if \\begin{equation} X=\\mu+\\sigma Z \\end{equation} The mean and variance of $X$ can be verified to be $\\mu$ and $\\sigma^2$ respectively easily by using the linearity of expectation and variance.\nTo derive the PDF formula for $X$, let us first start with its CDF, which is given by \\begin{equation} P(X\\leq x)=P\\left(\\frac{X-\\mu}{\\sigma}\\leq\\frac{x-\\mu}{\\sigma}\\right)=P\\left(Z\\leq\\frac{x-\\mu}{\\sigma}\\right)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right) \\end{equation} We then obtain the PDF of $X$ by differentiating its CDF \\begin{align} p_X(x)\u0026=\\frac{d}{dx}\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right)=\\frac{1}{\\sigma}\\varphi\\left(\\frac{x-\\mu}{\\sigma}\\right) \\\\ \u0026=\\dfrac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\dfrac{(x-\\mu)^2}{2\\sigma^2}\\right) \\end{align}\nBelow are some illustrations of the Univariate Normal distribution.\nFigure 1: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found here Multivariate Normal Distribution A $k$-dimensional random vector $\\mathbf{X}=\\left(X_1,\\dots,X_D\\right)^\\text{T}$ is said to have a Multivariate Normal (MVN) distribution if every linear combination of the $X_i$ has a Normal distribution. Which means \\begin{equation} t_1X_1+\\ldots+t_DX_D \\end{equation} is normally distributed for any choice of constants $t_1,\\dots,t_D$. Distribution of $\\mathbf{X}$ then can be written in the following notation \\begin{equation} \\mathbf{X}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma}), \\end{equation} where \\begin{equation} \\boldsymbol{\\mu}=\\mathbb{E}\\mathbf{X}=\\mathbb{E}\\left(\\mu_1,\\ldots,\\mu_k\\right)^\\text{T}=\\left(\\mathbb{E}X_1,\\ldots,\\mathbb{E}X_k\\right)^\\text{T} \\end{equation} is the $D$-dimensional mean vector, and covariance matrix $\\mathbf{\\Sigma}\\in\\mathbb{R}^{D\\times D}$ with \\begin{equation} \\boldsymbol{\\Sigma}_{ij}=\\mathbb{E}\\left(X_i-\\mu_i\\right)\\left(X_j-\\mu_j\\right)=\\Cov(X_i,X_j)\\label{eq:mvn.1} \\end{equation} Thus, the PDF of an MVN is defined as \\begin{equation} p_\\mathbf{X}(\\mathbf{x})=\\dfrac{1}{(2\\pi)^{D/2}\\vert\\mathbf{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right]\\label{eq:mvn.2} \\end{equation} With this idea, standard Normal distribution in multi-dimensional case can be defined as a Gaussian with mean $\\boldsymbol{\\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\\boldsymbol{\\Sigma}=\\mathbf{I}_{D\\times D}$.\nIt is worth remarking that $\\boldsymbol{\\Sigma}\\geq 0$ (positive semi-definite matrix)1. If $\\boldsymbol{\\Sigma}$ is positive definite, then $\\boldsymbol{\\Sigma}$ is invertible, then we can obtain its inverse, which is known as the precision matrix (or information matrix), denoted $\\boldsymbol{\\Lambda}$. \\begin{equation} \\boldsymbol{\\Lambda}\\doteq\\boldsymbol{\\Sigma}^{-1} \\end{equation} Moreover, let us consider the expression in the exponent of \\eqref{eq:mvn.2}: \\begin{align} -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\Sigma})\u0026=-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Lambda}(\\mathbf{x}-\\boldsymbol{\\Sigma}) \\\\ \u0026=-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}+(\\boldsymbol{\\Lambda}\\boldsymbol{\\mu})^\\text{T}\\mathbf{x}+\\boldsymbol{\\mu}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu} \\end{align} which allows us to obtain \\begin{equation} p(\\mathbf{x})\\propto\\exp\\left[-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}+(\\boldsymbol{\\Lambda}\\boldsymbol{\\mu})^\\text{T}\\mathbf{x}\\right] \\end{equation} This formulation is called the information form, and $\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}$ is known as the potential vector. The information form defines a valid Gaussian density iff the information matrix $\\boldsymbol{\\Lambda}$ is symmetric and positive definite.\nBivariate Normal When the number of dimensions in $\\mathbf{X}$, $D=2$, this special case of MVN is referred as Bivariate Normal (BVN).\nAn example of an BVN, $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u00260.5\\\\0.8\u00261\\end{smallmatrix}\\right]\\right)$, is shown as following.\nFigure 2: The PDF of $\\mathcal{N}\\left(\\left[\\begin{smallmatrix}0\\\\0\\end{smallmatrix}\\right],\\left[\\begin{smallmatrix}1\u00260.5\\\\0.8\u00261\\end{smallmatrix}\\right]\\right)$. The code can be found here Properties of the Covariance Matrix Symmetric With the definition \\eqref{eq:mvn.1} of the covariance matrix $\\boldsymbol{\\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\\boldsymbol{\\Sigma}$ is symmetric.\nTo prove this property, first off consider a square matrix $\\mathbf{S}$, we have it can be written by \\begin{equation} \\mathbf{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2}+\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2}=\\mathbf{S}_\\text{S}+\\mathbf{S}_\\text{A}, \\end{equation} where \\begin{equation} \\mathbf{S}_\\text{S}=\\frac{\\mathbf{S}+\\mathbf{S}^\\text{T}}{2},\\hspace{2cm}\\mathbf{S}_\\text{A}=\\frac{\\mathbf{S}-\\mathbf{S}^\\text{T}}{2} \\end{equation} It is easily seen that $\\mathbf{S}_\\text{S}$ is symmetric because the $\\{i,j\\}$ element of its equal to the $\\{j,i\\}$ element due to \\begin{equation} (\\mathbf{S}_\\text{S})_{ij}=\\frac{(\\mathbf{S})_{ij}+(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}+(\\mathbf{S})_{ji}}{2}=(\\mathbf{S}_\\text{S})_{ji} \\end{equation} On the other hand, the matrix $\\mathbf{S}_\\text{A}$ is anti-symmetric since \\begin{equation} (\\mathbf{S}_\\text{A})_{ij}=\\frac{(\\mathbf{S})_{ij}-(\\mathbf{S}^\\text{T})_{ij}}{2}=\\frac{(\\mathbf{S}^\\text{T})_{ji}-(\\mathbf{S})_{ji}}{2}=-(\\mathbf{S}_\\text{A})_{ji} \\end{equation} Consider the density of a distribution $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, we have that $\\boldsymbol{\\Sigma}$ is square and so is its inverse $\\boldsymbol{\\Sigma}^{-1}$. Therefore we can express $\\boldsymbol{\\Sigma}^{-1}$ as a sum of a symmetric matrix $\\boldsymbol{\\Sigma}_\\text{S}$ with an anti-symmetric matrix $\\boldsymbol{\\Sigma}_\\text{A}$ \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A} \\end{equation} We have that the density of the distribution is given by \\begin{align} f(\\mathbf{x})\u0026=\\frac{1}{(2\\pi)^{D/2}\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026\\propto\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026=\\exp\\left[-\\dfrac{1}{2}\\left(\\mathbf{x}-\\boldsymbol{\\mu}\\right)^\\text{T}(\\boldsymbol{\\Sigma}_\\text{S}+\\boldsymbol{\\Sigma}_\\text{A})(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \u0026\\propto\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}+\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\\right] \\\\ \u0026=\\exp\\left[\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{S}\\mathbf{v}\\right] \\end{align} where in the forth step, we have defined $\\mathbf{v}\\doteq\\mathbf{x}-\\boldsymbol{\\mu}$, and where in the fifth-step, the result obtained was due to \\begin{align} \\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}\u0026=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i(\\boldsymbol{\\Sigma}_\\text{A})_{ij}\\mathbf{v}_j \\\\ \u0026=\\sum_{i=1}^{D}\\sum_{j=1}^{D}\\mathbf{v}_i-(\\boldsymbol{\\Sigma}_\\text{A})_{ji}\\mathbf{v}_j \\\\ \u0026=-\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v} \\end{align} which implies that $\\mathbf{v}^\\text{T}\\boldsymbol{\\Sigma}_\\text{A}\\mathbf{v}=0$.\nThus, when computing the density, the symmetric part of $\\boldsymbol{\\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\\boldsymbol{\\Sigma}^{-1}$ is symmetric, which means that $\\boldsymbol{\\Sigma}$ is also symmetric.\nWith this assumption of symmetry, the covariance matrix $\\boldsymbol{\\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.\nReal eigenvalues Consider an eigenvector, eigenvalue pair $(\\mathbf{v},\\lambda)$ of covariance matrix $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\mathbf{v}\\label{eq:rc.1} \\end{equation} Since $\\boldsymbol{\\Sigma}\\in\\mathbb{R}^{D\\times D}$, we have $\\boldsymbol{\\Sigma}=\\overline{\\boldsymbol{\\Sigma}}$. Conjugate both sides of the equation above we have \\begin{equation} \\boldsymbol{\\Sigma}\\overline{\\mathbf{v}}=\\overline{\\lambda}\\overline{\\mathbf{v}},\\label{eq:rc.2} \\end{equation} Since $\\boldsymbol{\\Sigma}$ is symmetric, we have $\\boldsymbol{\\Sigma}=\\boldsymbol{\\Sigma}^\\text{T}$. Taking the transpose of both sides of \\eqref{eq:rc.2} gives us \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\label{eq:rc.3} \\end{equation} Continuing by taking dot product of both sides of \\eqref{eq:rc.3} with $\\mathbf{v}$ lets us obtain \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}\\label{eq:rc.4} \\end{equation} On the other hand, take dot product of $\\overline{\\mathbf{v}}^\\text{T}$ with both sides of \\eqref{eq:rc.1}, we have \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v} \\end{equation} which by \\eqref{eq:rc.4} implies that \\begin{equation} \\overline{\\lambda}\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\lambda\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}, \\end{equation} or \\begin{equation} (\\lambda-\\overline{\\lambda})\\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=0\\label{eq:rc.5} \\end{equation} Moreover, we have that \\begin{equation} \\overline{\\mathbf{v}}^\\text{T}\\mathbf{v}=\\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\\sum_{k=1}^{D}a^2+b^2\u003e0 \\end{equation} where we have denoted the complex eigenvector $\\mathbf{v}\\neq\\mathbf{0}$ as \\begin{equation} \\mathbf{v}=(a_1+i b_1,\\ldots,a_D+i b_D)^\\text{T}, \\end{equation} which implies that its complex conjugate $\\overline{\\mathbf{v}}$ can be written by \\begin{equation} \\overline{\\mathbf{v}}=(a_1-i b_1,\\ldots,a_D-i b_D)^\\text{T} \\end{equation} Therefore, by \\eqref{eq:rc.5}, we can claim that \\begin{equation} \\lambda=\\overline{\\lambda} \\end{equation} or in other words, the eigenvalue $\\lambda$ of $\\boldsymbol{\\Sigma}$ is real.\nProjection onto eigenvectors First, we have that eigenvectors $\\mathbf{v}_i$ and $\\mathbf{v}_j$ corresponding to different eigenvalues $\\lambda_i$ and $\\lambda_j$ of $\\boldsymbol{\\Sigma}$ are perpendicular, because \\begin{align} \\lambda_i\\mathbf{v}_i^\\text{T}\\mathbf{v}_j\u0026=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}^\\text{T}\\mathbf{v}_j \\\\ \u0026=\\mathbf{v}_i^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{v}_j=\\mathbf{v}_i^\\text{T}\\lambda_j\\mathbf{v}_j, \\end{align} which implies that \\begin{equation} (\\lambda_i-\\lambda_j)\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0 \\end{equation} Therefore, $\\mathbf{v}_i^\\text{T}\\mathbf{v}_j=0$ since $\\lambda_i\\neq\\lambda_j$.\nHence, for any unit eigenvectors $\\mathbf{q}_i,\\mathbf{q}_j$ of $\\boldsymbol{\\Sigma}$, we have \\begin{equation} \\mathbf{q}_i^\\text{T}\\mathbf{q}_j\\begin{cases}1,\u0026\\hspace{0.5cm}\\text{if }i=j \\\\ 0,\u0026\\hspace{0.5cm}\\text{if }i\\neq j\\end{cases} \\end{equation} This allows us to write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\mathbf{Q}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{Q}, \\end{equation} where $\\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\\mathbf{q}_i^\\text{T}$ and $\\boldsymbol{\\Lambda}$ is the diagonal matrix whose $\\{i,i\\}$ element is $\\lambda_i$, as \\begin{equation} \\mathbf{Q}=\\left[\\begin{matrix}-\\hspace{0.15cm}\\mathbf{q}_1^\\text{T}\\hspace{0.15cm}- \\\\ \\vdots \\\\ -\\hspace{0.15cm}\\mathbf{q}_D^\\text{T}\\hspace{0.15cm}-\\end{matrix}\\right],\\hspace{2cm}\\boldsymbol{\\Lambda}=\\left[\\begin{matrix}\\lambda_1\u0026\u0026 \\\\ \u0026\\ddots\u0026 \\\\ \u0026\u0026\\lambda_D\\end{matrix}\\right] \\end{equation} Therefore, we can also write $\\boldsymbol{\\Sigma}$ as \\begin{equation} \\boldsymbol{\\Sigma}=\\sum_{i=1}^{D}\\lambda_i\\mathbf{q}_i\\mathbf{q}_i^\\text{T} \\end{equation} Each matrix $\\mathbf{q}_i\\mathbf{q}_i^\\text{T}$ is the projection matrix onto $\\mathbf{q}_i$, then $\\boldsymbol{\\Sigma}$ can be express as a combination of perpendicular projection matrices.\nOther than that, for any eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of the matrix $\\boldsymbol{\\Sigma}$, we have \\begin{align} \\lambda_i\\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\Sigma}\\mathbf{q}_i=\\mathbf{q}_i \\end{align} or \\begin{equation} \\boldsymbol{\\Sigma}^{-1}\\mathbf{q}_i=\\frac{1}{\\lambda_i}\\mathbf{q}_i, \\end{equation} which implies that each eigenvector, eigenvalue pair $(\\mathbf{q_i},\\lambda_i)$ of $\\boldsymbol{\\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\\mathbf{q}_i,1/\\lambda_i)$ of $\\boldsymbol{\\Sigma}^{-1}$. Therefore, $\\boldsymbol{\\Sigma}^{-1}$ can also be written by \\begin{equation} \\boldsymbol{\\Sigma}^{-1}=\\sum_{i=1}^{D}\\frac{1}{\\lambda_i}\\mathbf{q}_i\\mathbf{q}_i^\\text{T}\\label{eq:pec.1} \\end{equation}\nProperties of Normal Distribution An crucial property of the multivariate Normal distribution is that if two sets of variables are jointly Gaussian, then the conditional probability distribution of one set given the other is then also Gaussian. Analogously, the marginal of either set is Gaussian too.\nConditional Gaussian Distribution Let $\\mathbf{x}$ be a $D$-dimensional random vector such that $\\mathbf{x}\\sim\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, and that we partition $\\mathbf{x}$ into two disjoint subsets $\\mathbf{x}_a$ and $\\mathbf{x}_b$ with $\\mathbf{x}_a$ is an $M$-dimensional vector and $\\mathbf{x}_b$ is a $(D-M)$-dimensional vector. \\begin{equation} \\mathbf{x}=\\left[\\begin{matrix}\\mathbf{x}_a \\\\ \\mathbf{x}_b\\end{matrix}\\right] \\end{equation} Along with them, we also define their corresponding means, as a partition of $\\boldsymbol{\\mu}$ \\begin{equation} \\boldsymbol{\\mu}=\\left[\\begin{matrix}\\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b\\end{matrix}\\right] \\end{equation} and their corresponding covariance matrices \\begin{equation} \\boldsymbol{\\Sigma}=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right], \\end{equation} which implies that $\\boldsymbol{\\Sigma}_{ab}=\\boldsymbol{\\Sigma}_{b a}^\\text{T}$.\nAnalogously, we also define the partitioned form of the precision matrix $\\boldsymbol{\\Sigma}^{-1}$ \\begin{equation} \\boldsymbol{\\Lambda}\\doteq\\boldsymbol{\\Sigma}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right], \\end{equation} Thus, we also have that $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$ since $\\boldsymbol{\\Sigma}^{-1}$ or in other words, $\\boldsymbol{\\Lambda}$ is symmetric due to the symmetry of $\\boldsymbol{\\Sigma}$. With these partitions, we can rewrite the functional dependence of the Gaussian \\eqref{eq:mvn.2} on $\\mathbf{x}$ as \\begin{align} \\hspace{-1.2cm}-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\u0026=-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{aa}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)^\\text{T}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b) \\\\ \u0026\\hspace{0.5cm}-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{ba}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a)-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)^\\text{T}\\boldsymbol{\\Lambda}_{bb}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.1} \\end{align} Consider the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$, which is the distribution of $\\mathbf{x}_a$ given $\\mathbf{x}_b$. Viewing $\\mathbf{x}_b$ as a constant, \\eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ on $\\mathbf{x}_a$, which can be continued to derive as \\begin{align} \u0026-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a+\\boldsymbol{\\Lambda}_{aa}^\\text{T}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_b-\\boldsymbol{\\Lambda}_{ba}^\\text{T}\\mathbf{x}_b+\\boldsymbol{\\Lambda}_{ba}\\boldsymbol{\\mu}_b\\big)+c \\\\ \u0026\\hspace{3cm}=-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\mathbf{x}_a^\\text{T}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big)+c,\\label{eq:cgd.2} \\end{align} where $c$ is a constant, and we have used the $\\boldsymbol{\\Lambda}_{aa}=\\boldsymbol{\\Lambda}_{aa}^\\text{T}$ and $\\boldsymbol{\\Lambda}_{ab}=\\boldsymbol{\\Lambda}_{ba}^\\text{T}$.\nMoreover, we have that the variation part which depends on $\\mathbf{x}$ for any Gaussian $\\mathbf{X}\\sim\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ can be written as a quadratic function of $\\mathbf{x}$ \\begin{equation} -\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})=-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\mathbf{x}+\\mathbf{x}^\\text{T}\\boldsymbol{\\Sigma}^{-1}\\boldsymbol{\\mu}+c,\\label{eq:cgd.3} \\end{equation} where $c$ is a constant. With this observation, and by \\eqref{eq:cgd.2} we have that the conditional distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\\boldsymbol{\\Sigma}_{a\\vert b}$, given by \\begin{equation} \\boldsymbol{\\Sigma}_{a\\vert b}=\\boldsymbol{\\Lambda}_{aa}^{-1},\\label{eq:cgd.4} \\end{equation} and with the corresponding mean vector, denoted as $\\boldsymbol{\\mu}_{a\\vert b}$, given by \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026=\\boldsymbol{\\Sigma}_{a\\vert b}\\big(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\big) \\\\ \u0026=\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b)\\label{eq:cgd.5} \\end{align} To express the mean $\\boldsymbol{\\mu}_{a\\vert b}$ and the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ of $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ in terms of partition of the covariance matrix $\\boldsymbol{\\Sigma}$ instead of the precision matrix $\\boldsymbol{\\Lambda}$’s, we will be using the identity for the inverse of a partitioned matrix \\begin{align} \\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}\u0026-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right],\\label{eq:cgd.6} \\end{align} where we have defined \\begin{equation} \\mathbf{M}\\doteq(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})^{-1}, \\end{equation} whose inverse $\\mathbf{M}^{-1}$ is called the Schur complement of the matrix $\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]^{-1}$. This identity can be proved by multiplying both sides of \\eqref{eq:cgd.6} with $\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right]$ to give \\begin{align} \\mathbf{I}\u0026=\\left[\\begin{matrix}\\mathbf{M}\u0026-\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\u0026\\mathbf{D}^{-1}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{A}\u0026\\mathbf{B} \\\\ \\mathbf{C}\u0026\\mathbf{D}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\u0026\\mathbf{M}\\mathbf{B}-\\mathbf{M}\\mathbf{B} \\\\ -\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{A}+\\mathbf{D}^{-1}\\mathbf{C}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C}\u0026-\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}+\\mathbf{I}+\\mathbf{D}^{-1}\\mathbf{C}\\mathbf{M}\\mathbf{B}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{I}\u0026\\mathbf{0} \\\\ \\mathbf{D}^{-1}\\mathbf{C}\\big(\\mathbf{I}-\\mathbf{M}(\\mathbf{A}-\\mathbf{B}\\mathbf{D}^{-1}\\mathbf{C})\\big)\u0026\\mathbf{I}\\end{matrix}\\right] \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{I}\u0026\\mathbf{0} \\\\ \\mathbf{0}\u0026\\mathbf{I}\\end{matrix}\\right]=\\mathbf{I}, \\end{align} which claims our argument.\nApplying the identity \\eqref{eq:cgd.6} into the precision matrix $\\boldsymbol{\\Lambda}=\\boldsymbol{\\Sigma}^{-1}$ gives us \\begin{equation} \\hspace{-0.5cm}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026\\boldsymbol{\\Lambda}_{ab} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{b a}\u0026\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right]^{-1}=\\left[\\begin{matrix}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\\\ -\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\u0026\\boldsymbol{\\Sigma}_{bb}^{-1}+\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\end{matrix}\\right], \\end{equation} where the Schur complement of $\\mathbf{\\Sigma}^{-1}$ is given by \\begin{equation} \\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1} \\end{equation} Hence, we obtain \\begin{align} \\boldsymbol{\\Lambda}_{aa}\u0026=\\mathbf{M}_\\boldsymbol{\\Sigma}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}, \\\\ \\boldsymbol{\\Lambda}_{ab}\u0026=-\\mathbf{M}_\\boldsymbol{\\Sigma}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}=\\big(\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba}\\big)^{-1}\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1} \\end{align} Substitute these results into \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\\mathbf{x}_a\\vert\\mathbf{x}_b)$ can be rewritten as \\begin{align} \\boldsymbol{\\mu}_{a\\vert b}\u0026=\\boldsymbol{\\mu}_a+\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b), \\\\ \\boldsymbol{\\Sigma}_{a\\vert b}\u0026=\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba} \\end{align} It is worth noticing that the mean $\\boldsymbol{\\mu}_{a\\vert b}$ given above is a linear function of $\\mathbf{x}_b$, while the covariance matrix $\\boldsymbol{\\Sigma}_{a\\vert b}$ is independent of $\\mathbf{x}_b$. This is an example of a linear Gaussian model.\nMarginal Gaussian Distribution Given the settings as in previous section, let us consider the marginal distribution of $\\mathbf{x}_a$, which can be computed by marginalizing the joint distribution \\begin{equation} p(\\mathbf{x}_a)=\\int p(\\mathbf{x}_a,\\mathbf{x}_b)d\\mathbf{x}_b\\label{eq:mgd.1} \\end{equation} which is an integration over $\\mathbf{x}_b$, and thus terms that does not depend on $\\mathbf{x}_b$ can be removed out of the integral.\nHence, using the result \\eqref{eq:cgd.1}, the terms that the depends on $\\mathbf{x}_b$ is \\begin{align} \u0026-\\frac{1}{2}\\Big[\\mathbf{x}_b^\\text{T}\\boldsymbol{\\Lambda}_{bb}\\mathbf{x}_b+\\mathbf{x}_b^\\text{T}(\\boldsymbol{\\Lambda}_{bb}\\boldsymbol{\\mu}_b+\\boldsymbol{\\Lambda}_{bb}^\\text{T}\\boldsymbol{\\mu}_b-\\boldsymbol{\\Lambda}_{ba}\\mathbf{x}_a+\\boldsymbol{\\Lambda}_{ba}\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{ab}^\\text{T}\\mathbf{x}_a+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_a)\\Big] \\\\ \u0026=-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m})^\\text{T}\\boldsymbol{\\Lambda}_{bb}(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m})+\\frac{1}{2}\\mathbf{m}^ \\text{T}\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m},\\label{eq:mgd.2} \\end{align} where we have defined \\begin{equation} \\mathbf{m}=\\boldsymbol{\\Lambda}_{bb}\\boldsymbol{\\mu}_b-\\boldsymbol{\\Lambda}_{ba}(\\mathbf{x}_a-\\boldsymbol{\\mu}_a) \\end{equation} The first term in \\eqref{eq:mgd.2} involves $\\mathbf{x}_b$, while the second one, $\\frac{1}{2}\\mathbf{m}^\\text{T}\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}$, does not, but it does depend on $\\mathbf{x}_a$. Thus the integration over $\\mathbf{x}_b$ required in \\eqref{eq:mgd.1} will take the form \\begin{equation} \\int\\exp\\left[-\\frac{1}{2}(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m})^\\text{T}\\boldsymbol{\\Lambda}_{bb}(\\mathbf{x}_b-\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m})\\right]d\\mathbf{x}_b, \\end{equation} which can be seen as an integral over an unnormalized Gaussian, and so the result will be the reciprocal of the normalizing coefficient. Moreover, from \\eqref{eq:mvn.2}, we have that this coefficient is independent of the mean, $\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}$, which involves $\\mathbf{x}_a$, and depends only on the determinant of the covariance matrix, $\\boldsymbol{\\Lambda}_{bb}$, which does not involve $\\mathbf{x}_a$. Therefore, using the result \\eqref{eq:cgd.1} to select out the terms uninvolved $\\mathbf{x}_b$, we have that \\begin{align} \u0026\\frac{1}{2}\\mathbf{m}^\\text{T}\\boldsymbol{\\Lambda}_{bb}^{-1}\\mathbf{m}-\\frac{1}{2}\\mathbf{x}_a^\\text{T}\\boldsymbol{\\Lambda}_{aa}\\mathbf{x}_a+\\frac{1}{2}\\mathbf{x}_a^\\text{T}(\\boldsymbol{\\Lambda}_{aa}\\boldsymbol{\\mu}_a+\\boldsymbol{\\Lambda}_{aa}^\\text{T}\\boldsymbol{\\mu}_a+\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\mu}_b+\\boldsymbol{\\Lambda}_{ba}\\boldsymbol{\\mu}_b)+c\\nonumber \\\\ \u0026=-\\frac{1}{2}\\mathbf{x}_a^\\text{T}(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba})\\mathbf{x}_a+\\mathbf{x}_a^\\text{T}(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba})^{-1}\\boldsymbol{\\mu}_a+c, \\end{align} where $c$ denotes the term that is independent of $\\mathbf{x}_a$. Hence, using \\eqref{eq:cgd.3} once again, we have that the marginal of $\\mathbf{x}_a$, $p(\\mathbf{x}_a)$, is also a Gaussian, with the corresponding covariance matrix, denoted $\\boldsymbol{\\Sigma}_a$, given by \\begin{equation} \\boldsymbol{\\Sigma}=(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba})^{-1}=\\boldsymbol{\\Sigma}_{aa},\\label{eq:mgd.3} \\end{equation} where we have used \\eqref{eq:cgd.6} as in the conditional distribution. The mean of $p(\\mathbf{x}_a)$ is then given as \\begin{equation} \\boldsymbol{\\Sigma}_a(\\boldsymbol{\\Lambda}_{aa}-\\boldsymbol{\\Lambda}_{ab}\\boldsymbol{\\Lambda}_{bb}^{-1}\\boldsymbol{\\Lambda}_{ba}) \\boldsymbol{\\mu}_a=\\boldsymbol{\\mu}_a\\label{eq:mgd.4} \\end{equation}\nRemark:\nGiven a joint Gaussian distribution $\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ with precision matrix $\\boldsymbol{\\Lambda}\\equiv\\boldsymbol{\\Sigma}^{-1}$ and \\begin{align} \\mathbf{x}\u0026=\\left[\\begin{matrix}\\mathbf{x}_a \\\\ \\mathbf{x}_b\\end{matrix}\\right],\u0026\u0026\\boldsymbol{\\mu}=\\left[\\begin{matrix}\\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b\\end{matrix}\\right] \\\\ \\boldsymbol{\\Sigma}\u0026=\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{aa}\u0026\\boldsymbol{\\Sigma}_{ab} \\\\ \\boldsymbol{\\Sigma}_{ba}\u0026\\boldsymbol{\\Sigma}_{bb}\\end{matrix}\\right],\u0026\u0026\\boldsymbol{\\Lambda}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}_{aa}\u0026\\boldsymbol{\\Lambda}_{aa} \\\\ \\boldsymbol{\\Lambda}_{ba}\u0026\\boldsymbol{\\Lambda}_{bb}\\end{matrix}\\right] \\end{align} The resulting conditional distribution is a Gaussian \\begin{align} p(\\mathbf{x}_a\\vert\\mathbf{x}_b)\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu}_{a\\vert b},\\boldsymbol{\\Sigma}_{a\\vert b}) \\\\ \\boldsymbol{\\mu}_{a\\vert b}\u0026=\\boldsymbol{\\mu}_a-\\boldsymbol{\\Lambda}_{aa}^{-1}\\boldsymbol{\\Lambda}_{ab}(\\mathbf{x}_b-\\boldsymbol{\\mu}_b) \\\\ \\boldsymbol{\\Sigma}_{a\\vert b}\u0026=\\boldsymbol{\\Lambda}_{aa}^{-1}=\\boldsymbol{\\Sigma}_{aa}-\\boldsymbol{\\Sigma}_{ab}\\boldsymbol{\\Sigma}_{bb}^{-1}\\boldsymbol{\\Sigma}_{ba} \\end{align} Also, the marginal distribution is a Gaussian \\begin{equation} p(\\mathbf{x}_a)=\\mathcal{N}(\\mathbf{x}_a\\vert\\boldsymbol{\\mu}_a,\\boldsymbol{\\Sigma}_{aa}) \\end{equation}\nBayes’ theorem for Gaussian variables In this section, we will apply the Bayes’ theorem to find the marginal distribution of $p(\\mathbf{y})$ and conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\\mathbf{x})$ and a conditional Gaussian distribution $p(\\mathbf{y}\\vert\\mathbf{x})$ in which $p(\\mathbf{y}\\vert\\mathbf{x})$ has a mean that is a linear function of $\\mathbf{x}$, and a covariance matrix which is independent of $\\mathbf{x}$, as \\begin{align} p(\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} where $\\mathbf{A},\\mathbf{b}$ are two parameters controlling the means, and $\\boldsymbol{\\Lambda},\\boldsymbol{L}$ are precision matrices.\nIn order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\\mathbf{x},\\mathbf{y})$ by considering the augmented vector \\begin{equation} \\mathbf{z}=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\end{equation} Therefore, we have \\begin{equation} p(\\mathbf{z})=p(\\mathbf{x},\\mathbf{y})=p(\\mathbf{x})p(\\mathbf{y}\\vert\\mathbf{x}) \\end{equation} Taking the natural logarithm of both sides gives us \\begin{align} \\log p(\\mathbf{z})\u0026=\\log p(\\mathbf{x})+\\log p(\\mathbf{y}\\vert\\mathbf{x}) \\\\ \u0026=\\log\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1})+\\log\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}) \\\\ \u0026=-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Lambda}(\\mathbf{x}-\\boldsymbol{\\mu})-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}-\\mathbf{b})+c\\label{eq:btg.1} \\end{align} where $c$ is a constant in terms of $\\mathbf{x}$ and $\\mathbf{y}$, i.e., $c$ is independent of $\\mathbf{x},\\mathbf{y}$.\nIt is easily to notice that \\eqref{eq:btg.1} is a quadratic function of the components of $\\mathbf{z}$, which implies that $p(\\mathbf{z})$ is a Gaussian. By \\eqref{eq:cgd.3}, in order to find the covariance matrix of $\\mathbf{z}$, we consider the quadratic terms in \\eqref{eq:btg.1}, which are given by \\begin{align} \u0026-\\frac{1}{2}\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}-\\frac{1}{2}(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\\\ \u0026=-\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\big(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\\big)\\mathbf{x}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{y}-\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{A}\\mathbf{x}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{y}\\Big] \\\\ \u0026=-\\frac{1}{2}\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026\\mathbf{L}\\end{matrix}\\right]\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right] \\\\ \u0026=-\\frac{1}{2}\\mathbf{z}^\\text{T}\\mathbf{R}\\mathbf{z}, \\end{align} which implies that the precision matrix of $\\mathbf{z}$ is $\\mathbf{R}$, defined as \\begin{equation} \\mathbf{R}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A}\u0026-\\mathbf{A}^\\text{T}\\mathbf{L} \\\\ -\\mathbf{L}\\mathbf{A}\u0026\\mathbf{L}\\end{matrix}\\right] \\end{equation} Thus, using the identity \\eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution \\begin{equation} \\boldsymbol{\\Sigma}_\\mathbf{z}=\\mathbf{R}^{-1}=\\left[\\begin{matrix}\\boldsymbol{\\Lambda}^{-1}\u0026\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T} \\\\ \\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\u0026\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}\\end{matrix}\\right] \\end{equation} Analogously, by \\eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \\eqref{eq:btg.1}, which are \\begin{align} \\hspace{-1.1cm}\\frac{1}{2}\\Big[\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}+\\boldsymbol{\\mu}^\\text{T}\\boldsymbol{\\Lambda}\\mathbf{x}+(\\mathbf{y}-\\mathbf{A}\\mathbf{x})^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{b}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{A}\\mathbf{x}) \\Big]\u0026=\\mathbf{x}^\\text{T}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{x}^\\text{T}\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b}+\\mathbf{y}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \u0026=\\left[\\begin{matrix}\\mathbf{x} \\\\ \\mathbf{y}\\end{matrix}\\right]^\\text{T}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right] \\end{align} Thus, by \\eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by \\begin{equation} \\boldsymbol{\\mu}_\\mathbf{z}=\\boldsymbol{\\Sigma}_\\mathbf{z}\\left[\\begin{matrix}\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}-\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{b} \\\\ \\mathbf{L}\\mathbf{b}\\end{matrix}\\right]=\\left[\\begin{matrix}\\boldsymbol{\\mu} \\\\ \\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}\\end{matrix}\\right] \\end{equation} Given the mean $\\boldsymbol{\\mu}_\\mathbf{z}$ and the covariance matrix $\\boldsymbol{\\Sigma}_\\mathbf{z}$ of the joint distribution of $\\mathbf{x},\\mathbf{y}$, by \\eqref{eq:mgd.3} and \\eqref{eq:mgd.4}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\\mathbf{y})$, which are \\begin{align} \\boldsymbol{\\mu}_\\mathbf{y}\u0026=\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b}, \\\\ \\boldsymbol{\\Sigma}_\\mathbf{y}\u0026=\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}, \\end{align} and also, by \\eqref{eq:cgd.4} and \\eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$, which are given by \\begin{align} \\boldsymbol{\\mu}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1}\\big(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}\\big) \\\\ \\boldsymbol{\\Sigma}_{\\mathbf{x}\\vert\\mathbf{y}}\u0026=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{align} In Bayesian approach, we can consider $p(\\mathbf{x})$ as a prior distribution over $\\mathbf{x}$, and if $\\mathbf{y}$ is observed, the conditional distribution $p(\\mathbf{x}\\vert\\mathbf{y})$ will represents the corresponding posterior distribution over $\\mathbf{x}$.\nRemark:\nGiven a marginal Gaussian distribution for $\\mathbf{x}$ and a conditional Gaussian distribution for $\\mathbf{y}$ given $\\mathbf{x}$ in the form \\begin{align} p(\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\mu},\\boldsymbol{\\Lambda}^{-1}), \\\\ p(\\mathbf{y}\\vert\\mathbf{x})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\mathbf{x}+\\mathbf{b},\\mathbf{L}^{-1}), \\end{align} the marginal distribution of $\\mathbf{y}$ and the conditional distribution of $\\mathbf{x}$ given $\\mathbf{y}$ are then given by \\begin{align} p(\\mathbf{y})\u0026=\\mathcal{N}(\\mathbf{y}\\vert\\mathbf{A}\\boldsymbol{\\mu}+\\mathbf{b},\\mathbf{L}^{-1}+\\mathbf{A}\\boldsymbol{\\Lambda}^{-1}\\mathbf{A}^\\text{T}), \\\\ p(\\mathbf{x}\\vert\\mathbf{y})\u0026=\\mathcal{N}(\\mathbf{x}\\vert\\boldsymbol{\\Sigma}(\\mathbf{A}^\\text{T}\\mathbf{L}(\\mathbf{y}-\\mathbf{b})+\\boldsymbol{\\Lambda}\\boldsymbol{\\mu}),\\boldsymbol{\\Sigma}) \\end{align} where \\begin{equation} \\boldsymbol{\\Sigma}=(\\boldsymbol{\\Lambda}+\\mathbf{A}^\\text{T}\\mathbf{L}\\mathbf{A})^{-1} \\end{equation}\nIndependencies in Normal Distributions Theorem 1: Let $\\mathbf{X}=X_1,\\ldots,X_n$ have a joint Normal distribution $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$. Then $X_i$ and $X_j$ are independent iff $\\boldsymbol{\\Sigma}_{ij}=0$.\nProof\nThis can be easily proved by the formulas for conditional and marginal distributions obtained above and using the fact that \\begin{equation} p\\models X_i\\perp X_j\\Leftrightarrow p(X_i)=p(X_i\\vert X_j) \\end{equation}\nTheorem 2: Consider a Gaussian distribution $p(X_1,\\ldots,X_n)=\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$, and let $\\boldsymbol{\\Lambda}=\\boldsymbol{\\Sigma}^{-1}$ denote the precision (information) matrix. Then \\begin{equation} \\boldsymbol{\\Lambda}_{ij}=0\\Leftrightarrow p\\models(X_i\\perp X_j\\vert\\mathcal{X}\\backslash\\{X_i,X_j\\}) \\end{equation}\nProof\nSimilarly, this can be proved by using the formula for conditional Gaussian, combined with the fact that \\begin{equation} p\\models(X_i\\perp X_j\\vert\\mathcal{X}\\backslash\\{X_i,X_j\\})\\Leftrightarrow p(X_i\\vert\\mathcal{X}\\backslash\\{X_j\\})=p(X_j\\vert\\mathcal{X}\\backslash\\{X_i\\}) \\end{equation}\nRemark: It is followed by Theorem 2 that the information matrix $\\boldsymbol{\\Lambda}$ directly defines a minimal I-map Markov network for $p$. Specifically, by Theorem 15, since Gaussian is a positive distribution, we include an edge $X_i-X_j$ whenever $p\\not\\models(X_i\\perp X_j\\vert\\mathcal{X}\\backslash\\{X_i,X_j\\})$, which exactly when $\\boldsymbol{\\Lambda}_{ij}=0$.\nGaussian Bayesian Networks We begin by first recalling the definition of linear Gaussian model: Let $Y$ be a continuous variable with continuous parents $X_1,\\ldots,X_k$. We say that $Y$ has a linear Gaussian model if there are parameters $\\beta_0,\\ldots,\\beta_k$ such that \\begin{equation} p(Y\\vert x_1,\\ldots,x_k)=\\mathcal{N}(\\beta_0+\\beta_1 x_1+\\ldots+\\beta_k x_k;\\sigma^2), \\end{equation} or \\begin{equation} p(Y\\vert\\mathbf{x})=\\mathcal{N}(\\beta_0+\\boldsymbol{\\beta}^\\text{T}\\mathbf{x};\\sigma^2) \\end{equation} Thus, a Gaussian Bayesian network is a Bayesian network all of whose variables are continuous and where all of CPDs are linear Gaussian.\nTheorem 3: Let $Y$ be a linear Gaussian of its parents $X_1,\\ldots,X_k$, i.e. \\begin{equation} p(Y\\vert\\mathbf{x})=\\mathcal{N}(\\beta_0+\\boldsymbol{\\beta}^\\text{T}\\mathbf{x};\\sigma^2) \\end{equation} Assume that $X_1,\\ldots,X_k$ are jointly Gaussian with distribution $\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$. Then\nThe distribution of $Y$ is a Normal distribution $p(Y)=\\mathcal{N}(\\mu_Y;\\sigma_Y^2)$ where \\begin{align} \\mu_Y\u0026=\\beta_0+\\boldsymbol{\\beta}^\\text{T}\\boldsymbol{\\mu} \\\\ \\sigma_Y^2\u0026=\\sigma^2+\\boldsymbol{\\beta}^\\text{T}\\boldsymbol{\\Sigma}\\boldsymbol{\\beta} \\end{align} The distribution over $\\{\\mathbf{X},Y\\}$ is a Normal distribution where \\begin{equation} \\Cov(X_i,Y)=\\sum_{j=1}^{k}\\beta_j\\boldsymbol{\\Sigma}_{ij} \\end{equation} From this theorem, it follows by induction that if $\\mathcal{B}$ is a linear Gaussian Bayesian network, then it defines a joint distribution that is jointly Gaussian. The inverse of this theorem is also true.\nTheorem 4: Let $\\{\\mathbf{X},Y\\}$ have a joint Normal distribution defined as usual \\begin{equation} p(\\mathbf{X},Y)=\\mathcal{N}\\left(\\left[\\begin{matrix}\\boldsymbol{\\mu}_\\mathbf{X} \\\\ \\boldsymbol{\\mu}_Y\\end{matrix}\\right],\\left[\\begin{matrix}\\boldsymbol{\\Sigma}_{\\mathbf{X}\\mathbf{X}}\u0026\\boldsymbol{\\Sigma}_{\\mathbf{X}Y} \\\\ \\boldsymbol{\\Sigma}_{Y\\mathbf{X}}\u0026\\boldsymbol{\\Sigma}_{YY}\\end{matrix}\\right]\\right) \\end{equation} where since $Y\\in\\mathbb{R}$, we have that $\\boldsymbol{\\mu}_Y$ and $\\boldsymbol{\\Sigma}_{YY}$ are real numbers. Then the conditional distribution is a Gaussian \\begin{equation} p(Y\\vert\\mathbf{X})=\\mathcal{N}(\\beta_0+\\boldsymbol{\\beta}^\\text{T}\\mathbf{X};\\sigma^2), \\end{equation} where \\begin{align} \\beta_0\u0026=\\boldsymbol{\\mu}_Y-\\boldsymbol{\\Sigma}_{Y\\mathbf{X}}\\boldsymbol{\\Sigma}_{\\mathbf{X}\\mathbf{X}}^{-1}\\boldsymbol{\\mu}_\\mathbf{X} \\\\ \\boldsymbol{\\beta}\u0026=\\boldsymbol{\\Sigma}_{\\mathbf{X}\\mathbf{X}}^{-1}\\boldsymbol{\\Sigma}_{Y\\mathbf{X}} \\\\ \\sigma^2\u0026=\\boldsymbol{\\Sigma}_{YY}-\\boldsymbol{\\Sigma}_{Y\\mathbf{X}}\\boldsymbol{\\Sigma}_{\\mathbf{X}\\mathbf{X}}^{-1}\\boldsymbol{\\Sigma}_{\\mathbf{X}Y} \\end{align} This theorem allows us to take a joint Gaussian distribution and produce a Bayesian network.\nTheorem 5: Let $\\mathcal{X}=\\{X_1,\\ldots,X_n\\}$ be a set of r.v.s, and let $p$ be a joint Gaussian distribution over $\\mathcal{X}$. Given any ordering $X_1,\\ldots,X_n$ over $\\mathcal{X}$, we can construct a Bayesian network structure $\\mathcal{G}$ and a Bayesian network $\\mathcal{B}$ over $\\mathcal{G}$ such that\n$\\text{Pa}_{X_i}^\\mathcal{G}\\subset\\{X_1,\\ldots,X_{i-1}\\}$; the CPD of $X_i$ in $\\mathcal{B}$ is a linear Gaussian of its parents; $\\mathcal{G}$ is a minimal I-map for $p$. References [1] Joseph K. Blitzstein \u0026 Jessica Hwang. Introduction to Probability.\n[2] Christopher M. Bishop. Pattern Recognition and Machine Learning. Springer New York, NY, 2006.\n[3] Daphne Koller, Nir Friedman. Probabilistic Graphical Models. The MIT Press.\n[4] Gilbert Strang. Introduction to Linear Algebra, 5th edition, 2016.\nFootnotes The definition of covariance matrix $\\boldsymbol{\\Sigma}$ can be rewritten as \\begin{equation*} \\boldsymbol{\\Sigma}=\\Cov(\\mathbf{X},\\mathbf{X})=\\Var(\\mathbf{X}) \\end{equation*} Let $\\mathbf{z}\\in\\mathbb{R}^D$, we have \\begin{equation*} \\Var(\\mathbf{z}^\\text{T}\\mathbf{X})=\\mathbf{z}^\\text{T}\\Var(\\mathbf{X})\\mathbf{z}=\\mathbf{z}^\\text{T}\\boldsymbol{\\Sigma}\\mathbf{z} \\end{equation*} And since $\\Var(\\mathbf{z}^\\text{T}\\mathbf{X})\\geq0$, we also have that $\\mathbf{z}^\\text{T}\\mathbf{\\Sigma}\\mathbf{z}\\geq0$, which proves that $\\boldsymbol{\\Sigma}$ is a positive semi-definite matrix. ↩︎\n","wordCount":"3131","inLanguage":"en","datePublished":"2021-11-22T14:46:00+07:00","dateModified":"2021-11-22T14:46:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io/ accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Gaussian Distribution & Gaussian Network Models</h1><div class=post-meta><span title='2021-11-22 14:46:00 +0700 +0700'>November 22, 2021</span>&nbsp;·&nbsp;15 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#normal-distribution>Normal Distribution</a><ul><li><a href=#std-norm>Standard Normal Distribution</a><ul><li><a href=#standard-mean-standard-variance>Standard Mean, Standard Variance</a></li></ul></li><li><a href=#gauss-dist>Univariate Normal Distribution</a></li><li><a href=#mvn>Multivariate Normal Distribution</a><ul><li><a href=#bvn>Bivariate Normal</a></li></ul></li></ul></li><li><a href=#prop-cov>Properties of the Covariance Matrix</a><ul><li><a href=#sym-cov>Symmetric</a></li><li><a href=#re-cov>Real eigenvalues</a></li><li><a href=#proj-ev-cov>Projection onto eigenvectors</a></li></ul></li><li><a href=#properties-of-normal-distribution>Properties of Normal Distribution</a><ul><li><a href=#cond-gauss-dist>Conditional Gaussian Distribution</a></li><li><a href=#marg-gauss-dist>Marginal Gaussian Distribution</a></li><li><a href=#bayes-theorem-gauss>Bayes&rsquo; theorem for Gaussian variables</a></li><li><a href=#independencies-in-normal-distributions>Independencies in Normal Distributions</a></li></ul></li><li><a href=#gaussian-bayesian-networks>Gaussian Bayesian Networks</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Gaussian distribution & Gaussian network models.</p></blockquote><p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p><h2 id=normal-distribution>Normal Distribution<a hidden class=anchor aria-hidden=true href=#normal-distribution>#</a></h2><h3 id=std-norm>Standard Normal Distribution<a hidden class=anchor aria-hidden=true href=#std-norm>#</a></h3><p>A continuous random variable $Z$ is said to have the <strong>standard Normal distribution</strong> if its PDF $\varphi$ is given by
\begin{equation}
\varphi(z)=\frac{1}{\sqrt{2\pi}e^{-z^2/2}},\hspace{1cm}z\in(-\infty,\infty)
\end{equation}
and denoted $Z\sim\mathcal{N}(0,1)$, since, as we will show, $Z$ has mean $0$ and variance $1$. Before that, let us compute the CDF of $Z$, which is given as
\begin{equation}
\Phi(z)=\int_{-\infty}^{z}\varphi(t)dt=\int_{-\infty}^{z}\frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt
\end{equation}
We continue by verifying that $\mathcal{N}{(0,1)}$ is indeed a distribution. Since $\varphi(z)\geq 0$, our problem remains to show that the PDF of $Z$ integrates to $1$. In particular, we have
\begin{align}
\left(\int_{-\infty}^{\infty}e^{-x^2/2}dx\right)\left(\int_{-\infty}^{\infty}e^{-y^2/2}dy\right)&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-x^2/2}e^{-y^2/2}dydx \\ &=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\frac{x^2+y^2}{2}}dxdy\label{eq:sn.1}
\end{align}
Let us change the variables, specifically, we will be changing from the Cartesian coordinate to the polar coordinate, by letting
\begin{align}
x&=r\cos\theta, \\ y&=r\sin\theta,
\end{align}
where $r\geq 0$ is the distance from $(x,y)$ to the origin and $\theta\in[0,2\pi)$ is the angle. The Jacobian matrix of this transformation is
\begin{equation}
\frac{d(x,y)}{d(r,\theta)}=\left[\begin{matrix}\cos\theta&-r\sin\theta \\ \sin\theta& r\cos\theta\end{matrix}\right],
\end{equation}
which implies that
\begin{equation}
\text{det}\frac{d(x,y)}{d(r,\theta)}=\text{det}\left[\begin{matrix}\cos\theta&-r\sin\theta \\ \sin\theta& r\cos\theta\end{matrix}\right]=1
\end{equation}
This makes us continue to derive \eqref{eq:sn.1} as
\begin{align}
\int_{-\infty}^{\infty}\int_{-\infty}^{\infty}e^{-\frac{x^2+y^2}{2}}dxdy&=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-r^2/2}\left\vert\text{det}\frac{d(x,y)}{d(r,\theta)}\right\vert rdrd\theta \\ &=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-r^2/2}rdrd\theta
\end{align}
Let $u=r^2/2$, then $du=rdr$, we have
\begin{equation}
\int_{0}^{2\pi}\int_{0}^{\infty}e^{-r^2/2}rdrd\theta=\int_{0}^{2\pi}\int_{0}^{\infty}e^{-u}dud\theta=\int_{0}^{2\pi}1d\theta=2\pi
\end{equation}
And hence, we can conclude that
\begin{equation}
\int_{-\infty}^{\infty}e^{-z^2/2}dz=\sqrt{2\pi}
\end{equation}
or
\begin{equation}
\int_{-\infty}^{\infty}\varphi(z)dz=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-z^2/2}dz=1
\end{equation}</p><h4 id=standard-mean-standard-variance>Standard Mean, Standard Variance<a hidden class=anchor aria-hidden=true href=#standard-mean-standard-variance>#</a></h4><p>We have that the mean of a standard Normal r.v $Z$ is given as
\begin{align}
\mathbb{E}(Z)&=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}z e^{-z^2/2}dz \\ &=\frac{1}{\sqrt{2\pi}}\left(\int_{0}^{\infty}z e^{-z^2/2}dz+\int_{-\infty}^{0}z e^{-z^2/2}dz\right) \\ &=\frac{1}{\sqrt{2\pi}}\left(\int_{0}^{\infty}z e^{-z^2/2}dz-\int_{0}^{\infty}z e^{-z^2/2}dz\right) \\ &=0
\end{align}
where in the third step, we use the fact that $ze^{-z^2/2}$ is an odd function.</p><p>Given the mean of $Z$, we have its variance is then can be computed by
\begin{align}
\Var(Z)&=\mathbb{E}(Z^2)-(\mathbb{E}Z)^2 \\ &=\mathbb{E}(Z^2) \\ &=\int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}z^2 e^{-z^2/2}dz \\ &=\frac{\sqrt{2}}{\sqrt{\pi}}\int_{0}^{\infty}z^2 e^{-z^2/2}dz
\end{align}
where the last step uses the fact that $z^2 e^{-z^2/2}$ is an even function. We continue by using integration by parts with $u=z$ and $dv=z e^{-z^2/2}$, then $du=dz$ and $v=-e^{-z^2/2}$. Thus,
\begin{align}
\Var(Z)&=\frac{\sqrt{2}}{\sqrt{\pi}}\left(-z e^{-z^2/2}\Big\vert_{0}^{\infty}+\int_{0}^{\infty}e^{-z^2/2}dz\right) \\ &=\frac{\sqrt{2}}{\sqrt{\pi}}\left(0+\frac{\sqrt{2\pi}}{2}\right) \\ &=1
\end{align}</p><h3 id=gauss-dist>Univariate Normal Distribution<a hidden class=anchor aria-hidden=true href=#gauss-dist>#</a></h3><p>Let $Z\sim\mathcal{N}(0,1)$ be a standard Normal r.v, then a continuous r.v $X$ is said to be a <strong>Gaussian</strong> or to have the <strong>(Univariate) Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$, denoted $X\sim\mathcal{N}(\mu,\sigma^2)$ if
\begin{equation}
X=\mu+\sigma Z
\end{equation}
The mean and variance of $X$ can be verified to be $\mu$ and $\sigma^2$ respectively easily by using the linearity of expectation and variance.</p><p>To derive the PDF formula for $X$, let us first start with its CDF, which is given by
\begin{equation}
P(X\leq x)=P\left(\frac{X-\mu}{\sigma}\leq\frac{x-\mu}{\sigma}\right)=P\left(Z\leq\frac{x-\mu}{\sigma}\right)=\Phi\left(\frac{x-\mu}{\sigma}\right)
\end{equation}
We then obtain the PDF of $X$ by differentiating its CDF
\begin{align}
p_X(x)&=\frac{d}{dx}\Phi\left(\frac{x-\mu}{\sigma}\right)=\frac{1}{\sigma}\varphi\left(\frac{x-\mu}{\sigma}\right) \\ &=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{align}</p><p>Below are some illustrations of the Univariate Normal distribution.</p><figure><img src=/images/gaussian-dist-gaussian-bn/normal.png alt="Normal distribution"><figcaption><b>Figure 1</b>: <b>10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis</b>. The code can be found <a href=https://github.com/trunghng/visualization-collection/blob/main/distributions/gauss-dist.py target=_blank>here</a></figcaption></figure><h3 id=mvn>Multivariate Normal Distribution<a hidden class=anchor aria-hidden=true href=#mvn>#</a></h3><p>A $k$-dimensional random vector $\mathbf{X}=\left(X_1,\dots,X_D\right)^\text{T}$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_DX_D
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_D$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma}),
\end{equation}
where
\begin{equation}
\boldsymbol{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^\text{T}=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^\text{T}
\end{equation}
is the $D$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{D\times D}$ with
\begin{equation}
\boldsymbol{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)\label{eq:mvn.1}
\end{equation}
Thus, the PDF of an MVN is defined as
\begin{equation}
p_\mathbf{X}(\mathbf{x})=\dfrac{1}{(2\pi)^{D/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]\label{eq:mvn.2}
\end{equation}
With this idea, <strong>standard Normal</strong> distribution in multi-dimensional case can be defined as a Gaussian with mean $\boldsymbol{\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\boldsymbol{\Sigma}=\mathbf{I}_{D\times D}$.</p><p>It is worth remarking that $\boldsymbol{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. If $\boldsymbol{\Sigma}$ is positive definite, then $\boldsymbol{\Sigma}$ is invertible, then we can obtain its inverse, which is known as the <strong>precision matrix</strong> (or <strong>information matrix</strong>), denoted $\boldsymbol{\Lambda}$.
\begin{equation}
\boldsymbol{\Lambda}\doteq\boldsymbol{\Sigma}^{-1}
\end{equation}
Moreover, let us consider the expression in the exponent of \eqref{eq:mvn.2}:
\begin{align}
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\Sigma})&=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\Sigma}) \\ &=-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Lambda}\mathbf{x}+(\boldsymbol{\Lambda}\boldsymbol{\mu})^\text{T}\mathbf{x}+\boldsymbol{\mu}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}
\end{align}
which allows us to obtain
\begin{equation}
p(\mathbf{x})\propto\exp\left[-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Lambda}\mathbf{x}+(\boldsymbol{\Lambda}\boldsymbol{\mu})^\text{T}\mathbf{x}\right]
\end{equation}
This formulation is called the <b id=information-form>information form</b>, and $\boldsymbol{\Lambda}\boldsymbol{\mu}$ is known as the <strong>potential vector</strong>. The information form defines a valid Gaussian density iff the information matrix $\boldsymbol{\Lambda}$ is symmetric and positive definite.</p><h4 id=bvn>Bivariate Normal<a hidden class=anchor aria-hidden=true href=#bvn>#</a></h4><p>When the number of dimensions in $\mathbf{X}$, $D=2$, this special case of MVN is referred as <strong>Bivariate Normal (BVN)</strong>.</p><p>An example of an BVN, $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$, is shown as following.</p><figure><img src=/images/gaussian-dist-gaussian-bn/bvn.png alt="Monte Carlo method"><figcaption><b>Figure 2</b>: <b>The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&0.5\\0.8&1\end{smallmatrix}\right]\right)$</b>. The code can be found <a href=https://github.com/trunghng/visualization-collection/blob/main/distributions/mvn.py target=_blank>here</a></figcaption></figure><h2 id=prop-cov>Properties of the Covariance Matrix<a hidden class=anchor aria-hidden=true href=#prop-cov>#</a></h2><h3 id=sym-cov>Symmetric<a hidden class=anchor aria-hidden=true href=#sym-cov>#</a></h3><p>With the definition \eqref{eq:mvn.1} of the covariance matrix $\boldsymbol{\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\boldsymbol{\Sigma}$ is symmetric.</p><p>To prove this property, first off consider a square matrix $\mathbf{S}$, we have it can be written by
\begin{equation}
\mathbf{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2}+\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}=\mathbf{S}_\text{S}+\mathbf{S}_\text{A},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2},\hspace{2cm}\mathbf{S}_\text{A}=\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}
\end{equation}
It is easily seen that $\mathbf{S}_\text{S}$ is symmetric because the $\{i,j\}$ element of its equal to the $\{j,i\}$ element due to
\begin{equation}
(\mathbf{S}_\text{S})_{ij}=\frac{(\mathbf{S})_{ij}+(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}+(\mathbf{S})_{ji}}{2}=(\mathbf{S}_\text{S})_{ji}
\end{equation}
On the other hand, the matrix $\mathbf{S}_\text{A}$ is anti-symmetric since
\begin{equation}
(\mathbf{S}_\text{A})_{ij}=\frac{(\mathbf{S})_{ij}-(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}-(\mathbf{S})_{ji}}{2}=-(\mathbf{S}_\text{A})_{ji}
\end{equation}
Consider the density of a distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, we have that $\boldsymbol{\Sigma}$ is square and so is its inverse $\boldsymbol{\Sigma}^{-1}$. Therefore we can express $\boldsymbol{\Sigma}^{-1}$ as a sum of a symmetric matrix $\boldsymbol{\Sigma}_\text{S}$ with an anti-symmetric matrix $\boldsymbol{\Sigma}_\text{A}$
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A}
\end{equation}
We have that the density of the distribution is given by
\begin{align}
f(\mathbf{x})&=\frac{1}{(2\pi)^{D/2}\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &\propto\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &=\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}(\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A})(\mathbf{x}-\boldsymbol{\mu})\right] \\ &\propto\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}+\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}\right] \\ &=\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}\right]
\end{align}
where in the forth step, we have defined $\mathbf{v}\doteq\mathbf{x}-\boldsymbol{\mu}$, and where in the fifth-step, the result obtained was due to
\begin{align}
\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}&=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i(\boldsymbol{\Sigma}_\text{A})_{ij}\mathbf{v}_j \\ &=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i-(\boldsymbol{\Sigma}_\text{A})_{ji}\mathbf{v}_j \\ &=-\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}
\end{align}
which implies that $\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}=0$.</p><p>Thus, when computing the density, the symmetric part of $\boldsymbol{\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\boldsymbol{\Sigma}^{-1}$ is symmetric, which means that $\boldsymbol{\Sigma}$ is also symmetric.</p><p>With this assumption of symmetry, the covariance matrix $\boldsymbol{\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.</p><h3 id=re-cov>Real eigenvalues<a hidden class=anchor aria-hidden=true href=#re-cov>#</a></h3><p>Consider an eigenvector, eigenvalue pair $(\mathbf{v},\lambda)$ of covariance matrix $\boldsymbol{\Sigma}$, we have
\begin{equation}
\boldsymbol{\Sigma}\mathbf{v}=\lambda\mathbf{v}\label{eq:rc.1}
\end{equation}
Since $\boldsymbol{\Sigma}\in\mathbb{R}^{D\times D}$, we have $\boldsymbol{\Sigma}=\overline{\boldsymbol{\Sigma}}$. Conjugate both sides of the equation above we have
\begin{equation}
\boldsymbol{\Sigma}\overline{\mathbf{v}}=\overline{\lambda}\overline{\mathbf{v}},\label{eq:rc.2}
\end{equation}
Since $\boldsymbol{\Sigma}$ is symmetric, we have $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}^\text{T}$. Taking the transpose of both sides of \eqref{eq:rc.2} gives us
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\label{eq:rc.3}
\end{equation}
Continuing by taking dot product of both sides of \eqref{eq:rc.3} with $\mathbf{v}$ lets us obtain
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}\label{eq:rc.4}
\end{equation}
On the other hand, take dot product of $\overline{\mathbf{v}}^\text{T}$ with both sides of \eqref{eq:rc.1}, we have
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v}
\end{equation}
which by \eqref{eq:rc.4} implies that
\begin{equation}
\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v},
\end{equation}
or
\begin{equation}
(\lambda-\overline{\lambda})\overline{\mathbf{v}}^\text{T}\mathbf{v}=0\label{eq:rc.5}
\end{equation}
Moreover, we have that
\begin{equation}
\overline{\mathbf{v}}^\text{T}\mathbf{v}=\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\sum_{k=1}^{D}a^2+b^2>0
\end{equation}
where we have denoted the complex eigenvector $\mathbf{v}\neq\mathbf{0}$ as
\begin{equation}
\mathbf{v}=(a_1+i b_1,\ldots,a_D+i b_D)^\text{T},
\end{equation}
which implies that its complex conjugate $\overline{\mathbf{v}}$ can be written by
\begin{equation}
\overline{\mathbf{v}}=(a_1-i b_1,\ldots,a_D-i b_D)^\text{T}
\end{equation}
Therefore, by \eqref{eq:rc.5}, we can claim that
\begin{equation}
\lambda=\overline{\lambda}
\end{equation}
or in other words, the eigenvalue $\lambda$ of $\boldsymbol{\Sigma}$ is real.</p><h3 id=proj-ev-cov>Projection onto eigenvectors<a hidden class=anchor aria-hidden=true href=#proj-ev-cov>#</a></h3><p>First, we have that eigenvectors $\mathbf{v}_i$ and $\mathbf{v}_j$ corresponding to different eigenvalues $\lambda_i$ and $\lambda_j$ of $\boldsymbol{\Sigma}$ are perpendicular, because
\begin{align}
\lambda_i\mathbf{v}_i^\text{T}\mathbf{v}_j&=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}^\text{T}\mathbf{v}_j \\ &=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}\mathbf{v}_j=\mathbf{v}_i^\text{T}\lambda_j\mathbf{v}_j,
\end{align}
which implies that
\begin{equation}
(\lambda_i-\lambda_j)\mathbf{v}_i^\text{T}\mathbf{v}_j=0
\end{equation}
Therefore, $\mathbf{v}_i^\text{T}\mathbf{v}_j=0$ since $\lambda_i\neq\lambda_j$.</p><p>Hence, for any unit eigenvectors $\mathbf{q}_i,\mathbf{q}_j$ of $\boldsymbol{\Sigma}$, we have
\begin{equation}
\mathbf{q}_i^\text{T}\mathbf{q}_j\begin{cases}1,&\hspace{0.5cm}\text{if }i=j \\ 0,&\hspace{0.5cm}\text{if }i\neq j\end{cases}
\end{equation}
This allows us to write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{Q}^\text{T}\boldsymbol{\Lambda}\mathbf{Q},
\end{equation}
where $\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\mathbf{q}_i^\text{T}$ and $\boldsymbol{\Lambda}$ is the diagonal matrix whose $\{i,i\}$ element is $\lambda_i$, as
\begin{equation}
\mathbf{Q}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{q}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{q}_D^\text{T}\hspace{0.15cm}-\end{matrix}\right],\hspace{2cm}\boldsymbol{\Lambda}=\left[\begin{matrix}\lambda_1&& \\ &\ddots& \\ &&\lambda_D\end{matrix}\right]
\end{equation}
Therefore, we can also write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\sum_{i=1}^{D}\lambda_i\mathbf{q}_i\mathbf{q}_i^\text{T}
\end{equation}
Each matrix $\mathbf{q}_i\mathbf{q}_i^\text{T}$ is the projection matrix onto $\mathbf{q}_i$, then $\boldsymbol{\Sigma}$ can be express as a combination of perpendicular projection matrices.</p><p>Other than that, for any eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of the matrix $\boldsymbol{\Sigma}$, we have
\begin{align}
\lambda_i\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}\mathbf{q}_i=\mathbf{q}_i
\end{align}
or
\begin{equation}
\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\frac{1}{\lambda_i}\mathbf{q}_i,
\end{equation}
<span id=precision-eigenvalue>which implies that each eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of $\boldsymbol{\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\mathbf{q}_i,1/\lambda_i)$ of $\boldsymbol{\Sigma}^{-1}$. Therefore, $\boldsymbol{\Sigma}^{-1}$ can also be written by</span>
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_i}\mathbf{q}_i\mathbf{q}_i^\text{T}\label{eq:pec.1}
\end{equation}</p><h2 id=properties-of-normal-distribution>Properties of Normal Distribution<a hidden class=anchor aria-hidden=true href=#properties-of-normal-distribution>#</a></h2><p>An crucial property of the multivariate Normal distribution is that if two sets of variables are jointly Gaussian, then the conditional probability distribution of one set given the other is then also Gaussian. Analogously, the marginal of either set is Gaussian too.</p><h3 id=cond-gauss-dist>Conditional Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#cond-gauss-dist>#</a></h3><p>Let $\mathbf{x}$ be a $D$-dimensional random vector such that $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, and that we partition $\mathbf{x}$ into two disjoint subsets $\mathbf{x}_a$ and $\mathbf{x}_b$ with $\mathbf{x}_a$ is an $M$-dimensional vector and $\mathbf{x}_b$ is a $(D-M)$-dimensional vector.
\begin{equation}
\mathbf{x}=\left[\begin{matrix}\mathbf{x}_a \\ \mathbf{x}_b\end{matrix}\right]
\end{equation}
Along with them, we also define their corresponding means, as a partition of $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}=\left[\begin{matrix}\boldsymbol{\mu}_a \\ \boldsymbol{\mu}_b\end{matrix}\right]
\end{equation}
and their corresponding covariance matrices
\begin{equation}
\boldsymbol{\Sigma}=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&\boldsymbol{\Sigma}_{bb}\end{matrix}\right],
\end{equation}
which implies that $\boldsymbol{\Sigma}_{ab}=\boldsymbol{\Sigma}_{b a}^\text{T}$.</p><p>Analogously, we also define the partitioned form of the precision matrix $\boldsymbol{\Sigma}^{-1}$
\begin{equation}
\boldsymbol{\Lambda}\doteq\boldsymbol{\Sigma}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&\boldsymbol{\Lambda}_{bb}\end{matrix}\right],
\end{equation}
Thus, we also have that $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$ since $\boldsymbol{\Sigma}^{-1}$ or in other words, $\boldsymbol{\Lambda}$ is symmetric due to the symmetry of $\boldsymbol{\Sigma}$.
With these partitions, we can rewrite the functional dependence of the Gaussian \eqref{eq:mvn.2} on $\mathbf{x}$ as
\begin{align}
\hspace{-1.2cm}-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})&=-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{aa}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b) \\ &\hspace{0.5cm}-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{ba}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{bb}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.1}
\end{align}
Consider the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$, which is the distribution of $\mathbf{x}_a$ given $\mathbf{x}_b$. Viewing $\mathbf{x}_b$ as a constant, \eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$ on $\mathbf{x}_a$, which can be continued to derive as
\begin{align}
&-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\frac{1}{2}\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{aa}^\text{T}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}\mathbf{x}_b+\boldsymbol{\Lambda}_{ab}\boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{ba}^\text{T}\mathbf{x}_b+\boldsymbol{\Lambda}_{ba}\boldsymbol{\mu}_b\big)+c \\ &\hspace{3cm}=-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big)+c,\label{eq:cgd.2}
\end{align}
where $c$ is a constant, and we have used the $\boldsymbol{\Lambda}_{aa}=\boldsymbol{\Lambda}_{aa}^\text{T}$ and $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$.</p><p>Moreover, we have that the variation part which depends on $\mathbf{x}$ for any Gaussian $\mathbf{X}\sim\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Sigma})$ can be written as a quadratic function of $\mathbf{x}$
\begin{equation}
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})=-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}+c,\label{eq:cgd.3}
\end{equation}
where $c$ is a constant. With this observation, and by \eqref{eq:cgd.2} we have that the conditional distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\boldsymbol{\Sigma}_{a\vert b}$, given by
\begin{equation}
\boldsymbol{\Sigma}_{a\vert b}=\boldsymbol{\Lambda}_{aa}^{-1},\label{eq:cgd.4}
\end{equation}
and with the corresponding mean vector, denoted as $\boldsymbol{\mu}_{a\vert b}$, given by
\begin{align}
\boldsymbol{\mu}_{a\vert b}&=\boldsymbol{\Sigma}_{a\vert b}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big) \\ &=\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{aa}^{-1}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.5}
\end{align}
To express the mean $\boldsymbol{\mu}_{a\vert b}$ and the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ of $p(\mathbf{x}_a\vert\mathbf{x}_b)$ in terms of partition of the covariance matrix $\boldsymbol{\Sigma}$ instead of the precision matrix $\boldsymbol{\Lambda}$&rsquo;s, we will be using the identity for the inverse of a partitioned matrix
\begin{align}
\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}&-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right],\label{eq:cgd.6}
\end{align}
where we have defined
\begin{equation}
\mathbf{M}\doteq(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1},
\end{equation}
whose inverse $\mathbf{M}^{-1}$ is called the <strong>Schur complement</strong> of the matrix $\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]^{-1}$. This identity can be proved by multiplying both sides of \eqref{eq:cgd.6} with $\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right]$ to give
\begin{align}
\mathbf{I}&=\left[\begin{matrix}\mathbf{M}&-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right]\left[\begin{matrix}\mathbf{A}&\mathbf{B} \\ \mathbf{C}&\mathbf{D}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})&\mathbf{M}\mathbf{B}-\mathbf{M}\mathbf{B} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{A}+\mathbf{D}^{-1}\mathbf{C}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\mathbf{C}&-\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}+\mathbf{I}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{I}&\mathbf{0} \\ \mathbf{D}^{-1}\mathbf{C}\big(\mathbf{I}-\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})\big)&\mathbf{I}\end{matrix}\right] \\ &=\left[\begin{matrix}\mathbf{I}&\mathbf{0} \\ \mathbf{0}&\mathbf{I}\end{matrix}\right]=\mathbf{I},
\end{align}
which claims our argument.</p><p>Applying the identity \eqref{eq:cgd.6} into the precision matrix $\boldsymbol{\Lambda}=\boldsymbol{\Sigma}^{-1}$ gives us
\begin{equation}
\hspace{-0.5cm}\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&\boldsymbol{\Lambda}_{bb}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&\boldsymbol{\Sigma}_{bb}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}_\boldsymbol{\Sigma}&-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1} \\ -\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}&\boldsymbol{\Sigma}_{bb}^{-1}+\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\end{matrix}\right],
\end{equation}
where the Schur complement of $\mathbf{\Sigma}^{-1}$ is given by
\begin{equation}
\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}
\end{equation}
Hence, we obtain
\begin{align}
\boldsymbol{\Lambda}_{aa}&=\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}, \\ \boldsymbol{\Lambda}_{ab}&=-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}
\end{align}
Substitute these results into \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ can be rewritten as
\begin{align}
\boldsymbol{\mu}_{a\vert b}&=\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}(\mathbf{x}_b-\boldsymbol{\mu}_b), \\ \boldsymbol{\Sigma}_{a\vert b}&=\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}
\end{align}
It is worth noticing that the mean $\boldsymbol{\mu}_{a\vert b}$ given above is a linear function of $\mathbf{x}_b$, while the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ is independent of $\mathbf{x}_b$. This is an example of a <a href=#linear-gaussian-model>linear Gaussian model</a>.</p><h3 id=marg-gauss-dist>Marginal Gaussian Distribution<a hidden class=anchor aria-hidden=true href=#marg-gauss-dist>#</a></h3><p>Given the settings as in previous section, let us consider the marginal distribution of $\mathbf{x}_a$, which can be computed by marginalizing the joint distribution
\begin{equation}
p(\mathbf{x}_a)=\int p(\mathbf{x}_a,\mathbf{x}_b)d\mathbf{x}_b\label{eq:mgd.1}
\end{equation}
which is an integration over $\mathbf{x}_b$, and thus terms that does not depend on $\mathbf{x}_b$ can be removed out of the integral.</p><p>Hence, using the result \eqref{eq:cgd.1}, the terms that the depends on $\mathbf{x}_b$ is
\begin{align}
&-\frac{1}{2}\Big[\mathbf{x}_b^\text{T}\boldsymbol{\Lambda}_{bb}\mathbf{x}_b+\mathbf{x}_b^\text{T}(\boldsymbol{\Lambda}_{bb}\boldsymbol{\mu}_b+\boldsymbol{\Lambda}_{bb}^\text{T}\boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{ba}\mathbf{x}_a+\boldsymbol{\Lambda}_{ba}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}^\text{T}\mathbf{x}_a+\boldsymbol{\Lambda}_{ab}\boldsymbol{\mu}_a)\Big] \\ &=-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m})^\text{T}\boldsymbol{\Lambda}_{bb}(\mathbf{x}_b-\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m})+\frac{1}{2}\mathbf{m}^
\text{T}\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m},\label{eq:mgd.2}
\end{align}
where we have defined
\begin{equation}
\mathbf{m}=\boldsymbol{\Lambda}_{bb}\boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{ba}(\mathbf{x}_a-\boldsymbol{\mu}_a)
\end{equation}
The first term in \eqref{eq:mgd.2} involves $\mathbf{x}_b$, while the second one, $\frac{1}{2}\mathbf{m}^\text{T}\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m}$, does not, but it does depend on $\mathbf{x}_a$. Thus the integration over $\mathbf{x}_b$ required in \eqref{eq:mgd.1} will take the form
\begin{equation}
\int\exp\left[-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m})^\text{T}\boldsymbol{\Lambda}_{bb}(\mathbf{x}_b-\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m})\right]d\mathbf{x}_b,
\end{equation}
which can be seen as an integral over an unnormalized Gaussian, and so the result will be the reciprocal of the normalizing coefficient. Moreover, from \eqref{eq:mvn.2}, we have that this coefficient is independent of the mean, $\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m}$, which involves $\mathbf{x}_a$, and depends only on the determinant of the covariance matrix, $\boldsymbol{\Lambda}_{bb}$, which does not involve $\mathbf{x}_a$. Therefore, using the result \eqref{eq:cgd.1} to select out the terms uninvolved $\mathbf{x}_b$, we have that
\begin{align}
&\frac{1}{2}\mathbf{m}^\text{T}\boldsymbol{\Lambda}_{bb}^{-1}\mathbf{m}-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\frac{1}{2}\mathbf{x}_a^\text{T}(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{aa}^\text{T}\boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{ab}\boldsymbol{\mu}_b+\boldsymbol{\Lambda}_{ba}\boldsymbol{\mu}_b)+c\nonumber \\ &=-\frac{1}{2}\mathbf{x}_a^\text{T}(\boldsymbol{\Lambda}_{aa}-\boldsymbol{\Lambda}_{ab}\boldsymbol{\Lambda}_{bb}^{-1}\boldsymbol{\Lambda}_{ba})\mathbf{x}_a+\mathbf{x}_a^\text{T}(\boldsymbol{\Lambda}_{aa}-\boldsymbol{\Lambda}_{ab}\boldsymbol{\Lambda}_{bb}^{-1}\boldsymbol{\Lambda}_{ba})^{-1}\boldsymbol{\mu}_a+c,
\end{align}
where $c$ denotes the term that is independent of $\mathbf{x}_a$. Hence, using \eqref{eq:cgd.3} once again, we have that the marginal of $\mathbf{x}_a$, $p(\mathbf{x}_a)$, is also a Gaussian, with the corresponding covariance matrix, denoted $\boldsymbol{\Sigma}_a$, given by
\begin{equation}
\boldsymbol{\Sigma}=(\boldsymbol{\Lambda}_{aa}-\boldsymbol{\Lambda}_{ab}\boldsymbol{\Lambda}_{bb}^{-1}\boldsymbol{\Lambda}_{ba})^{-1}=\boldsymbol{\Sigma}_{aa},\label{eq:mgd.3}
\end{equation}
where we have used \eqref{eq:cgd.6} as in the conditional distribution. The mean of $p(\mathbf{x}_a)$ is then given as
\begin{equation}
\boldsymbol{\Sigma}_a(\boldsymbol{\Lambda}_{aa}-\boldsymbol{\Lambda}_{ab}\boldsymbol{\Lambda}_{bb}^{-1}\boldsymbol{\Lambda}_{ba})
\boldsymbol{\mu}_a=\boldsymbol{\mu}_a\label{eq:mgd.4}
\end{equation}</p><p><strong>Remark</strong>:<br>Given a joint Gaussian distribution $\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Sigma})$ with precision matrix $\boldsymbol{\Lambda}\equiv\boldsymbol{\Sigma}^{-1}$ and
\begin{align}
\mathbf{x}&=\left[\begin{matrix}\mathbf{x}_a \\ \mathbf{x}_b\end{matrix}\right],&&\boldsymbol{\mu}=\left[\begin{matrix}\boldsymbol{\mu}_a \\ \boldsymbol{\mu}_b\end{matrix}\right] \\ \boldsymbol{\Sigma}&=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{ba}&\boldsymbol{\Sigma}_{bb}\end{matrix}\right],&&\boldsymbol{\Lambda}=\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&\boldsymbol{\Lambda}_{aa} \\ \boldsymbol{\Lambda}_{ba}&\boldsymbol{\Lambda}_{bb}\end{matrix}\right]
\end{align}
The resulting conditional distribution is a Gaussian
\begin{align}
p(\mathbf{x}_a\vert\mathbf{x}_b)&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu}_{a\vert b},\boldsymbol{\Sigma}_{a\vert b}) \\ \boldsymbol{\mu}_{a\vert b}&=\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{aa}^{-1}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b) \\ \boldsymbol{\Sigma}_{a\vert b}&=\boldsymbol{\Lambda}_{aa}^{-1}=\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}
\end{align}
Also, the marginal distribution is a Gaussian
\begin{equation}
p(\mathbf{x}_a)=\mathcal{N}(\mathbf{x}_a\vert\boldsymbol{\mu}_a,\boldsymbol{\Sigma}_{aa})
\end{equation}</p><h3 id=bayes-theorem-gauss>Bayes&rsquo; theorem for Gaussian variables<a hidden class=anchor aria-hidden=true href=#bayes-theorem-gauss>#</a></h3><p>In this section, we will apply the Bayes&rsquo; theorem to find the marginal distribution of $p(\mathbf{y})$ and conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\mathbf{x})$ and a conditional Gaussian distribution $p(\mathbf{y}\vert\mathbf{x})$ in which $p(\mathbf{y}\vert\mathbf{x})$ has a mean that is a linear function of $\mathbf{x}$, and a covariance matrix which is independent of $\mathbf{x}$, as
\begin{align}
p(\mathbf{x})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
where $\mathbf{A},\mathbf{b}$ are two parameters controlling the means, and $\boldsymbol{\Lambda},\boldsymbol{L}$ are precision matrices.</p><p>In order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\mathbf{x},\mathbf{y})$ by considering the augmented vector
\begin{equation}
\mathbf{z}=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]
\end{equation}
Therefore, we have
\begin{equation}
p(\mathbf{z})=p(\mathbf{x},\mathbf{y})=p(\mathbf{x})p(\mathbf{y}\vert\mathbf{x})
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{z})&=\log p(\mathbf{x})+\log p(\mathbf{y}\vert\mathbf{x}) \\ &=\log\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1})+\log\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}) \\ &=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\mu})-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})+c\label{eq:btg.1}
\end{align}
where $c$ is a constant in terms of $\mathbf{x}$ and $\mathbf{y}$, i.e., $c$ is independent of $\mathbf{x},\mathbf{y}$.</p><p>It is easily to notice that \eqref{eq:btg.1} is a quadratic function of the components of $\mathbf{z}$, which implies that $p(\mathbf{z})$ is a Gaussian. By \eqref{eq:cgd.3}, in order to find the covariance matrix of $\mathbf{z}$, we consider the quadratic terms in \eqref{eq:btg.1}, which are given by
\begin{align}
&-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Lambda}\mathbf{x}-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \\ &=-\frac{1}{2}\Big[\mathbf{x}^\text{T}\big(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}\big)\mathbf{x}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{y}-\mathbf{y}^\text{T}\mathbf{L}\mathbf{A}\mathbf{x}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{y}\Big] \\ &=-\frac{1}{2}\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&\mathbf{L}\end{matrix}\right]\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right] \\ &=-\frac{1}{2}\mathbf{z}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
which implies that the precision matrix of $\mathbf{z}$ is $\mathbf{R}$, defined as
\begin{equation}
\mathbf{R}=\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&\mathbf{L}\end{matrix}\right]
\end{equation}
Thus, using the identity \eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution
\begin{equation}
\boldsymbol{\Sigma}_\mathbf{z}=\mathbf{R}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}^{-1}&\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T} \\ \mathbf{A}\boldsymbol{\Lambda}^{-1}&\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}\end{matrix}\right]
\end{equation}
Analogously, by \eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \eqref{eq:btg.1}, which are
\begin{align}
\hspace{-1.1cm}\frac{1}{2}\Big[\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}+\boldsymbol{\mu}^\text{T}\boldsymbol{\Lambda}\mathbf{x}+(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}\mathbf{b}+\mathbf{b}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \Big]&=\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{b}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{b} \\ &=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]
\end{align}
Thus, by \eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by
\begin{equation}
\boldsymbol{\mu}_\mathbf{z}=\boldsymbol{\Sigma}_\mathbf{z}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\mu} \\ \mathbf{A}\boldsymbol{\mu}+\mathbf{b}\end{matrix}\right]
\end{equation}
Given the mean $\boldsymbol{\mu}_\mathbf{z}$ and the covariance matrix $\boldsymbol{\Sigma}_\mathbf{z}$ of the joint distribution of $\mathbf{x},\mathbf{y}$, by \eqref{eq:mgd.3} and \eqref{eq:mgd.4}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\mathbf{y})$, which are
\begin{align}
\boldsymbol{\mu}_\mathbf{y}&=\mathbf{A}\boldsymbol{\mu}+\mathbf{b}, \\ \boldsymbol{\Sigma}_\mathbf{y}&=\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T},
\end{align}
and also, by \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$, which are given by
\begin{align}
\boldsymbol{\mu}_{\mathbf{x}\vert\mathbf{y}}&=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}\big(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}\big) \\ \boldsymbol{\Sigma}_{\mathbf{x}\vert\mathbf{y}}&=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{align}
In Bayesian approach, we can consider $p(\mathbf{x})$ as a prior distribution over $\mathbf{x}$, and if $\mathbf{y}$ is observed, the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ will represents the corresponding posterior distribution over $\mathbf{x}$.</p><p><b id=marg-cond-gaussian>Remark</b>:<br>Given a marginal Gaussian distribution for $\mathbf{x}$ and a conditional Gaussian distribution for $\mathbf{y}$ given $\mathbf{x}$ in the form
\begin{align}
p(\mathbf{x})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
the marginal distribution of $\mathbf{y}$ and the conditional distribution of $\mathbf{x}$ given $\mathbf{y}$ are then given by
\begin{align}
p(\mathbf{y})&=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\boldsymbol{\mu}+\mathbf{b},\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}), \\ p(\mathbf{x}\vert\mathbf{y})&=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\Sigma}(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}),\boldsymbol{\Sigma})
\end{align}
where
\begin{equation}
\boldsymbol{\Sigma}=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{equation}</p><h3 id=independencies-in-normal-distributions>Independencies in Normal Distributions<a hidden class=anchor aria-hidden=true href=#independencies-in-normal-distributions>#</a></h3><p><strong>Theorem 1</strong>: <em>Let $\mathbf{X}=X_1,\ldots,X_n$ have a joint Normal distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Then $X_i$ and $X_j$ are independent iff $\boldsymbol{\Sigma}_{ij}=0$.</em></p><p><strong>Proof</strong><br>This can be easily proved by the formulas for conditional and marginal distributions obtained above and using the fact that
\begin{equation}
p\models X_i\perp X_j\Leftrightarrow p(X_i)=p(X_i\vert X_j)
\end{equation}</p><p><b id=theorem2>Theorem 2</b>: <em>Consider a Gaussian distribution $p(X_1,\ldots,X_n)=\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, and let $\boldsymbol{\Lambda}=\boldsymbol{\Sigma}^{-1}$ denote the precision (information) matrix. Then</em>
\begin{equation}
\boldsymbol{\Lambda}_{ij}=0\Leftrightarrow p\models(X_i\perp X_j\vert\mathcal{X}\backslash\{X_i,X_j\})
\end{equation}</p><p><strong>Proof</strong><br>Similarly, this can be proved by using the formula for conditional Gaussian, combined with the fact that
\begin{equation}
p\models(X_i\perp X_j\vert\mathcal{X}\backslash\{X_i,X_j\})\Leftrightarrow p(X_i\vert\mathcal{X}\backslash\{X_j\})=p(X_j\vert\mathcal{X}\backslash\{X_i\})
\end{equation}</p><p><strong>Remark</strong>: It is followed by <a href=#theorem2>Theorem 2</a> that the information matrix $\boldsymbol{\Lambda}$ directly defines a minimal I-map Markov network for $p$. Specifically, by <a href=https://trunghng.github.io/posts/machine-learning/pgm-representation/#theorem15>Theorem 15</a>, since Gaussian is a positive distribution, we include an edge $X_i-X_j$ whenever $p\not\models(X_i\perp X_j\vert\mathcal{X}\backslash\{X_i,X_j\})$, which exactly when $\boldsymbol{\Lambda}_{ij}=0$.</p><h2 id=gaussian-bayesian-networks>Gaussian Bayesian Networks<a hidden class=anchor aria-hidden=true href=#gaussian-bayesian-networks>#</a></h2><p>We begin by first recalling the definition of <span id=linear-gaussian-model><a href=https://trunghng.github.io/posts/machine-learning/pgm-representation/#linear-gaussian-model>linear Gaussian model</a></span>: Let $Y$ be a continuous variable with continuous parents $X_1,\ldots,X_k$. We say that $Y$ has a <strong>linear Gaussian model</strong> if there are parameters $\beta_0,\ldots,\beta_k$ such that
\begin{equation}
p(Y\vert x_1,\ldots,x_k)=\mathcal{N}(\beta_0+\beta_1 x_1+\ldots+\beta_k x_k;\sigma^2),
\end{equation}
or
\begin{equation}
p(Y\vert\mathbf{x})=\mathcal{N}(\beta_0+\boldsymbol{\beta}^\text{T}\mathbf{x};\sigma^2)
\end{equation}
Thus, a <strong>Gaussian Bayesian network</strong> is a Bayesian network all of whose variables are continuous and where all of CPDs are linear Gaussian.</p><p><b id=theorem3>Theorem 3</b>: <em>Let $Y$ be a linear Gaussian of its parents $X_1,\ldots,X_k$, i.e.</em>
\begin{equation}
p(Y\vert\mathbf{x})=\mathcal{N}(\beta_0+\boldsymbol{\beta}^\text{T}\mathbf{x};\sigma^2)
\end{equation}
<em>Assume that $X_1,\ldots,X_k$ are jointly Gaussian with distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$. Then</em></p><ul class=number-list style=font-style:italic><li>The distribution of $Y$ is a Normal distribution $p(Y)=\mathcal{N}(\mu_Y;\sigma_Y^2)$ where
\begin{align}
\mu_Y&=\beta_0+\boldsymbol{\beta}^\text{T}\boldsymbol{\mu} \\ \sigma_Y^2&=\sigma^2+\boldsymbol{\beta}^\text{T}\boldsymbol{\Sigma}\boldsymbol{\beta}
\end{align}</li><li>The distribution over $\{\mathbf{X},Y\}$ is a Normal distribution where
\begin{equation}
\Cov(X_i,Y)=\sum_{j=1}^{k}\beta_j\boldsymbol{\Sigma}_{ij}
\end{equation}</li></ul><p>From this theorem, it follows by induction that if $\mathcal{B}$ is a linear Gaussian Bayesian network, then it defines a joint distribution that is jointly Gaussian. The inverse of this theorem is also true.</p><p><strong>Theorem 4</strong>: <em>Let $\{\mathbf{X},Y\}$ have a joint Normal distribution defined as usual</em>
\begin{equation}
p(\mathbf{X},Y)=\mathcal{N}\left(\left[\begin{matrix}\boldsymbol{\mu}_\mathbf{X} \\ \boldsymbol{\mu}_Y\end{matrix}\right],\left[\begin{matrix}\boldsymbol{\Sigma}_{\mathbf{X}\mathbf{X}}&\boldsymbol{\Sigma}_{\mathbf{X}Y} \\ \boldsymbol{\Sigma}_{Y\mathbf{X}}&\boldsymbol{\Sigma}_{YY}\end{matrix}\right]\right)
\end{equation}
<em>where since $Y\in\mathbb{R}$, we have that $\boldsymbol{\mu}_Y$ and $\boldsymbol{\Sigma}_{YY}$ are real numbers. Then the conditional distribution is a Gaussian</em>
\begin{equation}
p(Y\vert\mathbf{X})=\mathcal{N}(\beta_0+\boldsymbol{\beta}^\text{T}\mathbf{X};\sigma^2),
\end{equation}
<em>where</em>
\begin{align}
\beta_0&=\boldsymbol{\mu}_Y-\boldsymbol{\Sigma}_{Y\mathbf{X}}\boldsymbol{\Sigma}_{\mathbf{X}\mathbf{X}}^{-1}\boldsymbol{\mu}_\mathbf{X} \\ \boldsymbol{\beta}&=\boldsymbol{\Sigma}_{\mathbf{X}\mathbf{X}}^{-1}\boldsymbol{\Sigma}_{Y\mathbf{X}} \\ \sigma^2&=\boldsymbol{\Sigma}_{YY}-\boldsymbol{\Sigma}_{Y\mathbf{X}}\boldsymbol{\Sigma}_{\mathbf{X}\mathbf{X}}^{-1}\boldsymbol{\Sigma}_{\mathbf{X}Y}
\end{align}
This theorem allows us to take a joint Gaussian distribution and produce a Bayesian network.</p><p><strong>Theorem 5</strong>: <em>Let $\mathcal{X}=\{X_1,\ldots,X_n\}$ be a set of r.v.s, and let $p$ be a joint Gaussian distribution over $\mathcal{X}$. Given any ordering $X_1,\ldots,X_n$ over $\mathcal{X}$, we can construct a Bayesian network structure $\mathcal{G}$ and a Bayesian network $\mathcal{B}$ over $\mathcal{G}$ such that</em></p><ul class=number-list style=font-style:italic><li>$\text{Pa}_{X_i}^\mathcal{G}\subset\{X_1,\ldots,X_{i-1}\}$;</li><li>the CPD of $X_i$ in $\mathcal{B}$ is a linear Gaussian of its parents;</li><li>$\mathcal{G}$ is a minimal I-map for $p$.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Joseph K. Blitzstein & Jessica Hwang. <a href=https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573>Introduction to Probability</a>.</p><p>[2] Christopher M. Bishop. <a href=https://link.springer.com/book/9780387310732>Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p><p>[3] Daphne Koller, Nir Friedman. <a href=https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/>Probabilistic Graphical Models</a>. The MIT Press.</p><p>[4] Gilbert Strang. <a href=http://math.mit.edu/~gs/linearalgebra/>Introduction to Linear Algebra, 5th edition</a>, 2016.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>The definition of covariance matrix $\boldsymbol{\Sigma}$ can be rewritten as
\begin{equation*}
\boldsymbol{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation*}
Let $\mathbf{z}\in\mathbb{R}^D$, we have
\begin{equation*}
\Var(\mathbf{z}^\text{T}\mathbf{X})=\mathbf{z}^\text{T}\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^\text{T}\boldsymbol{\Sigma}\mathbf{z}
\end{equation*}
And since $\Var(\mathbf{z}^\text{T}\mathbf{X})\geq0$, we also have that $\mathbf{z}^\text{T}\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\boldsymbol{\Sigma}$ is a positive semi-definite matrix.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/mathematics/>Mathematics</a></li><li><a href=https://trunghng.github.io/tags/probability-statistics/>Probability-Statistics</a></li><li><a href=https://trunghng.github.io/tags/normal-distribution/>Normal-Distribution</a></li><li><a href=https://trunghng.github.io/tags/probabilistic-graphical-model/>Probabilistic-Graphical-Model</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/><span class=title>« Prev</span><br><span>Temporal-Difference Learning</span>
</a><a class=next href=https://trunghng.github.io/posts/calculus/power-series/><span class=title>Next »</span><br><span>Power Series</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on x" href="https://x.com/intent/tweet/?text=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f&amp;hashtags=mathematics%2cprobability-statistics%2cnormal-distribution%2cprobabilistic-graphical-model"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f&amp;title=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models&amp;summary=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f&title=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on whatsapp" href="https://api.whatsapp.com/send?text=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on telegram" href="https://telegram.me/share/url?text=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Gaussian Distribution & Gaussian Network Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=Gaussian%20Distribution%20%26%20Gaussian%20Network%20Models&u=https%3a%2f%2ftrunghng.github.io%2fposts%2fprobability-statistics%2fgaussian-dist-gaussian-bn%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io/>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>