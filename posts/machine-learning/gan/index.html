<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>GAN | Littleroot</title><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="machine-learning,generative-model"><meta name=description content="
Notes on Generative Adversarial Networks.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/machine-learning/gan/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://trunghng.github.io/posts/machine-learning/gan/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><meta property="og:title" content="GAN"><meta property="og:description" content="
Notes on Generative Adversarial Networks.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/machine-learning/gan/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-05-01T13:00:00+07:00"><meta property="article:modified_time" content="2023-05-01T13:00:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="GAN"><meta name=twitter:description content="
Notes on Generative Adversarial Networks.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":2,"name":"GAN","item":"https://trunghng.github.io/posts/machine-learning/gan/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"GAN","name":"GAN","description":" Notes on Generative Adversarial Networks.\n","keywords":["machine-learning","generative-model"],"articleBody":" Notes on Generative Adversarial Networks.\nGenerative Adversarial Network (GAN) A generative adversarial network consists of two independent components, each of which is a neural network1, acting as adversaries:\nGenerator: A generative model, denoted $G$, parameterized by $\\theta_g$. The model is trained to generate fake samples as real as possible. Discriminator: A discriminative model, denoted $D$, parameterized by $\\theta_d$. The model is trained to distinguish between the samples generated by $G$ and the training samples. To be more precise,\nLet $p_r$ denote the distribution over real data (training sample), the discriminator $D$ is trained to maximize $\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\big[\\log D(\\mathbf{x};\\theta_d)\\big]$. $G$ implicitly defines a probability distribution, which we denote as $p_g$. Let $\\mathbf{z}$ denote an input noise variable, $\\mathbf{z}\\sim p_\\mathbf{z}$ , which is the input to $G$. Thus, as assigning the correct label to data generated by $G$ and the real data, $D$ is also trained to maximize $\\mathbb{E}_{\\mathbf{z}\\sim p_\\mathbf{z}}\\big[\\log(1-D(G(\\mathbf{z};\\theta_g);\\theta_d))\\big]$. At the same time, the generator $G$ is trained to minimize $\\mathbb{E}_{\\mathbf{z}\\sim p_\\mathbf{z}}\\big[\\log(1-D(G(\\mathbf{z};\\theta_g);\\theta_d))\\big]$. Therefore, the adversarial framework can be considered as a two-player minimax game in which we are trying to optimize the value function $V(D,G)$ \\begin{equation} \\min_{\\theta_g}\\max_{\\theta_d}V(D,G)=\\mathbb{E}_{\\mathbf{x}\\sim p_r(\\mathbf{x})}\\big[\\log D(\\mathbf{x};\\theta_d)\\big]+\\mathbb{E}_{\\mathbf{z}\\sim p_\\mathbf{z}(\\mathbf{z})}\\big[\\log\\big(1-D(G(\\mathbf{z};\\theta_g);\\theta_d)\\big)\\big]\\label{eq:gan.1} \\end{equation}\nLet us consider the optimal discriminator $D$ for any given generator $G$. For $\\theta_g$ fixed, we need to maximize \\begin{align} V(D,G)\u0026=\\mathbb{E}_{\\mathbf{x}\\sim p_r(\\mathbf{x})}\\big[\\log D(\\mathbf{x};\\theta_d)\\big]+\\mathbb{E}_{\\mathbf{z}\\sim p_\\mathbf{z}(\\mathbf{z})}\\big[\\log\\big(1-D(g(\\mathbf{z});\\theta_d)\\big)\\big] \\\\ \u0026= \\int_\\mathbf{x}p_r(\\mathbf{x})\\log D(\\mathbf{x};\\theta_d)\\hspace{0.1cm}d\\mathbf{x}+\\int_\\mathbf{z}p_\\mathbf{z}(\\mathbf{z})\\log\\big(1-D(g(\\mathbf{z});\\theta_d))\\hspace{0.1cm}d\\mathbf{z} \\\\ \u0026=\\int_\\mathbf{x}p_r(\\mathbf{x})\\log D(\\mathbf{x};\\theta_d)\\hspace{0.1cm}d\\mathbf{x}+\\int_\\mathbf{x}p_g(\\mathbf{x})(1-\\log D(\\mathbf{x};\\theta_d))\\hspace{0.1cm}d\\mathbf{x} \\\\ \u0026=\\int_\\mathbf{x}p_r(\\mathbf{x})\\log D(\\mathbf{x};\\theta_d)+p_g(\\mathbf{x})(1-\\log D(\\mathbf{x};\\theta_d))\\hspace{0.1cm}d\\mathbf{x}, \\end{align} which, with letting $\\bar{\\mathbf{x}}$ denote $D(\\mathbf{x};\\theta_d)$, can be achieved by maximizing \\begin{align} f(\\bar{\\mathbf{x}})\u0026=p_r(\\mathbf{x})\\log\\bar{\\mathbf{x}}+p_g(\\mathbf{x})(1-\\log\\bar{\\mathbf{x}}) \\end{align} Differentiating $f$ w.r.t $\\bar{\\mathbf{x}}$ gives us \\begin{equation} \\frac{\\partial f}{\\partial\\bar{\\mathbf{x}}}=\\frac{p_r(\\mathbf{x})}{\\bar{\\mathbf{x}}}-\\frac{p_g(\\mathbf{x})}{1-\\bar{\\mathbf{x}}} \\end{equation} Setting the derivative to zero, we have that the optimal discriminator $D$ is given as \\begin{equation} D_G^*(\\mathbf{x})=\\frac{p_r(\\mathbf{x})}{p_r(\\mathbf{x})+p_g(\\mathbf{x})}, \\end{equation} at which $f(\\bar{\\mathbf{x}})$ achieves its maximum since \\begin{equation} \\frac{\\partial^2 f}{\\partial\\bar{\\mathbf{x}}^2}=-\\frac{p_r(\\mathbf{x})}{\\bar{\\mathbf{x}}^2}-\\frac{p_g(\\mathbf{x})}{(1-\\bar{\\mathbf{x}})^2}\u003c0 \\end{equation} When $\\theta_d$ is optimal and fixed, if we define \\begin{equation} C(G)\\overset{\\Delta}{=}\\max_{\\theta_d}V(G,D)=V(G,D_G^*), \\end{equation} the minimax game in \\eqref{eq:gan.1} can be rewritten as minimizing \\begin{align} \\hspace{-0.5cm}C(G)\u0026=\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\big[\\log D_G^*(\\mathbf{x})\\big]+\\mathbb{E}_{\\mathbf{z}\\sim p_\\mathbf{z}}\\big[\\log(1-D_G^*(G(\\mathbf{z})))\\big] \\\\ \u0026=\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\big[\\log D_G^*(\\mathbf{x})\\big]+\\mathbb{E}_{\\mathbf{x}\\sim p_g}\\big[\\log(1-D_G^*(\\mathbf{x}))\\big] \\\\ \u0026=\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\left[\\log\\frac{p_r(\\mathbf{x})}{p_r(\\mathbf{x})+p_g(\\mathbf{x})}\\right]+\\mathbb{E}_{\\mathbf{x}\\sim p_g}\\left[\\log\\left(1-\\frac{p_r(\\mathbf{x})}{p_r(\\mathbf{x})+p_g(\\mathbf{x})}\\right)\\right] \\\\ \u0026=\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\left[\\log\\frac{p_r(\\mathbf{x})}{p_r(\\mathbf{x})+p_g(\\mathbf{x})}\\right]+\\mathbb{E}_{\\mathbf{x}\\sim p_g}\\left[\\log\\frac{p_g(\\mathbf{x})}{p_r(\\mathbf{x})+p_g(\\mathbf{x})}\\right] \\end{align} We will be showing that $C(G)$ achieves its global minimum iff $p_g=p_r$.\nWe begin by considering the Jensen-Shannon divergence between $p_g$ and $p_r$, denoted $D_\\text{JS}$: \\begin{align} \\hspace{-1.5cm}D_\\text{JS}(p_g\\Vert p_r)\u0026=\\frac{1}{2}D_\\text{KL}\\left(p_g\\left\\Vert\\frac{p_g+p_r}{2}\\right.\\right)+\\frac{1}{2}D_\\text{KL}\\left(p_r\\left\\Vert\\frac{p_g+p_r}{2}\\right.\\right) \\\\ \u0026=\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim p_g}\\left[\\log\\frac{2 p_r(\\mathbf{x})}{p_g(\\mathbf{x})+p_r(\\mathbf{x})}\\right]+\\frac{1}{2}\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\left[\\log\\frac{2 p_g(\\mathbf{x})}{p_g(\\mathbf{x})+p_r(\\mathbf{x})}\\right] \\\\ \u0026=\\frac{1}{2}C(G)+\\log 2, \\end{align} which implies that \\begin{equation} C(G)=2D_\\text{JS}(p_g\\Vert p_r)-2\\log 2 \\end{equation} Therefore, $C(G)$ achieves its minimum if and only if $D_\\text{JS}(p_g\\Vert p_r)$ reaches its minimum, which is zero, occurring when $p_g=p_r$.\nIt then follows when $p_g=p_r$, $C(G)$ achieves its global minimum, which is $-2\\log 2$. Training GAN We will apply the minibatch SGD method for training GAN.\nWasserstein GAN (WGAN) Wasserstein is a variation of GAN which use Wasserstein metric to measure the distance between probability distributions $p_r$ and $p_g$ instead of the Jensen-Shannon divergence.\nDifferent Distances Let $\\mathcal{X}$ be a compact metric set, $\\Sigma$ be the set of all the Borel subsets of $\\mathcal{X}$ and let $\\text{Prob}(\\mathcal{X})$ denote the space of probability measures defined on $\\mathcal{X}$. We can now define elementary distances and divergences between two probability distributions $P_r,P_g\\in\\text{Prob}(\\mathcal{X})$\nTotal Variation (TV) distance \\begin{equation} \\delta(P_r,P_g)=\\sup_{A\\in\\Sigma}\\big\\vert P_r(A)-P_g(A)\\big\\vert \\end{equation} Kullback-Leibler (KL) divergence \\begin{equation} D_\\text{KL}(P_r\\Vert P_g)=\\int_\\mathcal{X}p_r(x)\\log\\frac{p_r(x)}{p_g(x)}d\\mu(x), \\end{equation} where both $P_r,P_g$ are assumed to be absolutely continuous, and therefore admit densities, w.r.t a measure $\\mu$2. The divergence is not symmetric and infinite when there are points such that $P_g(x)=0$ and $P_r(x)\u003e0$. Jensen-Shannon (JS) divergence \\begin{equation} D_\\text{JS}(P_r\\Vert P_g)=\\frac{1}{2}D_\\text{KL}(P_r\\Vert P_m)+\\frac{1}{2}D_\\text{KL}(P_g\\Vert P_m) \\end{equation} where $P_m$ is the mixture distribution $P_m=\\frac{P_r+P_g}{2}$. The divergence is symmetric. Earth Mover (EM) distance, or Wasserstein-1 \\begin{equation} W(P_r,P_g)=\\inf_{\\gamma\\in\\Pi(P_r,P_g)}\\mathbb{E}_{(x,y)\\sim\\gamma}\\big[\\Vert x-y\\Vert_2\\big]\\label{eq:dd.1}, \\end{equation} where $\\Pi(P_r,P_g)$ is the set of all joint distribution $\\gamma(x,y)$ whose marginal distributions are $P_r$ and $P_g$. Example: Learning parallel lines Consider probability distributions defined on $\\mathbb{R}^2$:\n$P_0$ is the distribution of $(0,Y)$ where $Y\\sim\\text{Unif}(0,1)$. $P_\\theta$ is the family of distribution of $(\\theta,Y)$ with $Y\\sim\\text{Unif}(0,1)$. Our goal is to learn to move from $\\theta$ to $0$, i.e. as $\\theta$ approaches $0$, the distance or divergence $d(P_0,P_\\theta)$ tends to $0$. For each distance, or divergence we have\nTV distance. If $\\theta=0$ then $P_0=P_\\theta$, and thus \\begin{equation} \\delta(P_0,P_\\theta)=\\sup_{A\\in\\Sigma}\\vert P_0(A)-P_\\theta(A)\\vert=0 \\end{equation} If $\\theta\\neq 0$, let $\\bar{A}=\\{(0,y):y\\in[0,1]\\}$, we have \\begin{equation} \\delta(P_0,P_\\theta)=\\sup_{A\\in\\Sigma}\\vert P_0(A)-P_\\theta(A)\\vert\\geq\\vert P_0(\\bar{A})-P_\\theta(\\bar{A})\\vert=1 \\end{equation} Also, by definition we also have that $0\\leq\\delta(P_0,P_\\theta)\\leq 1$. Hence, for $\\theta\\neq 0,\\delta(P_0,P_\\theta)=1$. \\begin{equation} \\delta(P_0,P_\\theta)=\\begin{cases}0\u0026\\hspace{1cm}\\text{if }\\theta=0 \\\\ 1\u0026\\hspace{1cm}\\text{if }\\theta\\neq 0\\end{cases} \\end{equation} KL divergence. By definition, we have that \\begin{equation} D_\\text{KL}(P_0\\Vert P_\\theta)=\\begin{cases}0\u0026\\hspace{1cm}\\text{if }P_0=P_\\theta \\\\ +\\infty\u0026\\hspace{1cm}\\text{if }!x\\in\\mathbb{R}^2:P_0(x)\u003e0,P_\\theta(x)=0\\end{cases} \\end{equation} Therefore, we have that when $\\theta\\neq 0$, for any $a\\in[0,1]$, \\begin{equation} D_\\text{KL}(P_0\\Vert P_\\theta)=+\\infty, \\end{equation} since $P_0(0,a)\u003e0$ and $P_\\theta(0,a)=0$, and \\begin{equation} D_\\text{KL}(P_\\theta\\Vert P_0)=+\\infty, \\end{equation} since $P_\\theta(\\theta,a)\u003e0$ while $P_0(\\theta,a)=0$. Therefore, it follows that \\begin{equation} D_\\text{KL}(P_0\\Vert P_\\theta)=D_\\text{KL}(P_\\theta\\Vert P_0)=\\begin{cases}0\u0026\\hspace{1cm}\\text{if }\\theta=0 \\\\ +\\infty\u0026\\hspace{1cm}\\text{if }\\theta\\neq 0\\end{cases} \\end{equation} JS divergence. It is easy to see that for $\\theta=0$, we have $D_\\text{JS}(P_0\\Vert P_\\theta)=0$. Let us consider the case that $\\theta\\neq 0$. Let $P_m$ denote the mixture $\\frac{P_0+P_\\theta}{2}$, we have \\begin{equation} D_\\text{JS}(P_0\\Vert P_\\theta)=\\frac{1}{2}D_\\text{KL}(P_0\\Vert P_m)+\\frac{1}{2}D_\\text{KL}(P_\\theta\\Vert P_m) \\end{equation} Consider the first KL divergence, we have that \\begin{align} D_\\text{KL}(P_0\\Vert P_m)\u0026=\\int_{x}\\int_{0}^{1}P_0(x,y)\\log\\frac{P_0(x,y)}{P_m(x,y)}\\,dy\\,dx \\\\ \u0026\\overset{\\text{(i)}}{=}\\int_{0}^{1}P_0(0,y)\\log\\frac{2P_0(0,y)}{P_0(0,y)+P_\\theta(0,y)}\\,dy\\nonumber \\\\ \u0026+\\int_{0}^{1}\\int_{x\\neq 0}P_0(x,y)\\log\\frac{2P_0(x,y)}{P_0(x,y)+P_\\theta(x,y)}\\,dx\\,dy \\\\ \u0026\\overset{\\text{(ii)}}{=}\\int_{0}^{1}P_0(0,y)\\log 2\\,dy+\\int_{0}^{1}0\\,dy \\\\ \u0026=\\log 2\\int_{0}^{1}P_0(0,y)\\,dy \\\\ \u0026\\overset{\\text{(iii)}}{=}\\log 2, \\end{align} where In this step, we use the Fubini's theorem to exchange the order of integration. In this step, we use the fact that for any $y\\in[0,1],P_0(0,y)\u003e0$ while $P_\\theta(0,y)=0$ and for any $x\\neq 0,P_0(x,y)=0$. In this step, the expression inside the integration integrates to $1$ since $P_0$ is a probability distribution. Similarly, we also have that \\begin{equation} D_\\text{KL}(P_\\theta\\Vert P_m)=\\log 2 \\end{equation} Therefore, it follows that \\begin{equation} D_\\text{JS}(P_0\\Vert P_\\theta)=\\begin{cases}0\u0026\\hspace{1cm}\\text{if }\\theta=0 \\\\ \\log 2\u0026\\hspace{1cm}\\text{if }\\theta\\neq 0\\end{cases} \\end{equation} EM distance. In other words,\nAs $\\theta_t\\to 0$, the sequence $(P_{\\theta_t})_{t\\in\\mathbb{N}}$ converges to $P_0$ under the EM distance, but does not converge under either the TV, KL, reverse KL or JS divergences. If we consider these distances and divergences as functions of $P_0$ and $P_\\theta$, it is easily seen that EM is continuous, while none of the others is. Remark: The example shows that\nThere exist sequences of distributions that do not converge under either TV, KL, reverse KL or JS divergences but do converge under the EM distance. There are cases where EM is continuous everywhere, while all of the others are discontinuous. Convergence and continuity of Wasserstein distance Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces. A function $f:X\\mapsto Y$ is called Lipschitz continuous if there exists a constant $K\\geq 0$ such that, for all $x_1,x_2\\in X$, \\begin{equation} d_Y(f(x_1),f(x_2))\\leq K d_X(x_1,x_2) \\end{equation} In this case, $K$ is known as a Lipschitz constant for $f$, while $f$ is referred to as K-Lipschitz. If $K=1$, $f$ is called a short map (or weak contraction). And if $0\\leq K\u003c1$ and $f$ maps a metric to itself, i.e. $X=Y,$ then $f$ is called a contraction.\nThe function $f$ is called locally Lipschitz continuous if $\\forall x\\in X$, there exists a neighborhood3 $U$ of $x$ such that $f$ restricted4 to $U$ is Lipschitz continuous.\nEquivalently, if $X$ is a locally compact5 metric space, then $f$ is locally Lipschitz iff it is Lipschitz continuous on every compact subset of $X$.\nAssumption 1\nLet $g:\\mathcal{Z}\\times\\mathbb{R}^d\\mapsto\\mathcal{X}$ be locally Lipschitz between finite dimensional vector spaces6 and denote $g_\\theta(z)\\doteq g(z,\\theta)$. We say that $g$ satisfies Assumption 1 for a certain probability $p$ over $\\mathcal{Z}$ if there are local Lipschitz constants $L(\\theta,z)$ such that \\begin{equation} \\mathbb{E}_{z\\sim p}\\big[L(\\theta,z)\\big]\\leq+\\infty \\end{equation}\nTheorem 1 Let $P_r$ be a fixed distribution over $\\mathcal{X}$ and $Z$ be a r.v over another space $\\mathcal{Z}$. Let $g:\\mathcal{Z}\\times\\mathbb{R}^d\\mapsto\\mathcal{X}$ with $g_\\theta(z)\\doteq g(\\theta,z)$. And let $P_\\theta$ denote the distribution of $g_\\theta(Z)$. Then\nIf $g$ is continuous in $\\theta$, so is $W(P_r,P_\\theta)$. If $g$ is locally Lipschitz and satisfies Assumption 1, then $W(P_r,P_\\theta)$ is continuous everywhere, and differentiable almost everywhere. Statements (1) and (2) are false for the JS divergence $D_\\text{JS}(P_r\\Vert P_\\theta)$, the KL divergence $D_\\text{KL}(P_r\\Vert P_\\theta)$ and the reverse KL divergence $D_\\text{KL}(P_\\theta\\Vert P_r)$. Proof\nTODO\nWe can apply this result to neural networks, as following.\nCorollary 1\nLet $g_\\theta$ be any feedforward neural network parameterized by $\\theta$, and let $p(z)$ be a prior over $z$ such that $\\mathbb{E}_{z\\sim p(z)}\\big[\\Vert z\\Vert_2\\big]\\leq\\infty$ (e.g. Gaussian, uniform, etc). Then Assumption 1 is satisfied and therefore $W(P_r,P_\\theta)$ is continuous everywhere and differentiable almost everywhere.\nProof\nTODO\nTheorem 2 Let $P$ be a distribution on a compact space $\\mathcal{X}$ and $(P_n)_{n\\in\\mathbb{N}}$ be a sequence of distribution on $\\mathcal{X}$. Then, as $n\\to\\infty$,\nThe following statements are equivalent $\\delta(P_n,P)\\to 0$. $D_\\text{JS}(P_n\\Vert P)\\to 0$. The following statements are equivalent $W(P_n,P)\\to 0$. $P_n\\overset{\\mathcal{D}}{\\to}P$ where $\\overset{\\mathcal{D}}{\\to}$ denotes convergence in distribution for r.v.s. $D_\\text{KL}(P_n\\Vert P)\\to 0$ or $D_\\text{KL}(P\\Vert P_n)\\to 0$ imply the statements in (1). The statements in (1) imply the statements in (2). In other words, the theorem states that every distributions that converges under the TV, KL, reverse KL and JS divergences also converges under the Wasserstein distance.\nProof\nTODO\nWasserstein GAN Despite of having such nice properties, the infimum when computing EM distance \\eqref{eq:dd.1} \\begin{equation} W(P_r,P_g)=\\inf_{\\gamma\\sim\\Pi(P_r,P_g)}\\mathbb{E}_{(x,y)\\sim\\gamma}\\big[\\Vert x-y\\Vert_2\\big] \\end{equation} is however intractable. This suggests us to compute an approximation instead.\nTheorem 3 (Kantorovich-Rubinstein duality)\n\\begin{equation} W(P_r,P_\\theta)=\\sup_{\\Vert f\\Vert_L\\leq 1}\\mathbb{E}_{x\\sim P_r}[f(x)]-\\mathbb{E}_{x\\sim P_\\theta}[f(x)] \\end{equation}\nProof\nLet $\\mathcal{X}$ be a compact metric and let $\\text{Prob}(\\mathcal{X})$ denote the space of probability measures defined on $\\mathcal{X}$. Let us consider two probability distribution $P_r,P_\\theta\\in\\text{Prob}(\\mathcal{X})$ with densities $p_r,p_\\theta$ respectively. Let $\\Pi(P_r,P_\\theta)$ be the sets of all joint distribution $\\gamma(x,y)$ whose marginals are respectively $P_r$ and $P_\\theta$, i.e. \\begin{align} p_r(x)\u0026=\\int_\\mathcal{X}\\gamma(x,y)\\hspace{0.1cm}dy,\\label{eq:wgan.1} \\\\ p_\\theta(y)\u0026=\\int_\\mathcal{X}\\gamma(x,y)\\hspace{0.1cm}dx\\label{eq:wgan.2} \\end{align} Their Wasserstein distance is given as \\begin{equation} W(P_r,P_\\theta)=\\inf_{\\gamma\\in\\Pi(P_r,P_\\theta)}\\mathbb{E}_{(x,y)\\sim\\gamma}\\big[\\Vert x-y\\Vert_2\\big], \\end{equation} which is a convex optimization problem with constrains given in \\eqref{eq:wgan.1} and \\eqref{eq:wgan.2}. Let $f,g:\\mathcal{X}\\mapsto\\mathbb{R}$ be Lagrange multipliers, the Lagrangian is then given as \\begin{align} \\hspace{-1cm}\\mathcal{L}(\\gamma,f,g)\u0026=\\mathbb{E}_{(x,y)\\sim\\gamma}\\big[\\Vert x-y\\Vert_2\\big]+\\int_\\mathcal{X}\\left(p_r(x)-\\int_\\mathcal{X}\\gamma(x,y)\\hspace{0.1cm}dy\\right)f(x)\\hspace{0.1cm}dx \\nonumber\\\\ \u0026\\hspace{4cm}+\\int_\\mathcal{X}\\left(p_\\theta(y)-\\int_\\mathcal{X}\\gamma(x,y)\\hspace{0.1cm}dx\\right)g(y)\\hspace{0.1cm}dy \\\\ \u0026=\\mathbb{E}_{x\\sim P_r}\\big[f(x)\\big]+\\mathbb{E}_{y\\sim P_\\theta}\\big[g(y)\\big]+\\int_{\\mathcal{X}\\times\\mathcal{X}}\\gamma(x,y)\\Big(\\Vert x-y\\Vert_2-f(x)-g(y)\\Big)\\hspace{0.1cm}dy\\hspace{0.1cm}dx \\end{align} Applying strong duality (?? TODO), we have \\begin{equation} W(P_r,P_\\theta)=\\inf_\\gamma\\sup_{f,g}\\mathcal{L}(\\gamma,f,g)=\\sup_{f,g}\\inf_\\gamma\\mathcal{L}(\\gamma,f,g) \\end{equation}\nTODO\nIf we replace $\\Vert f\\Vert_L\\leq 1$ by $\\Vert f\\Vert_L\\leq K$ (i.e. we instead consider the supremum over K-Lipschitz functions)\nPreferences [1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Generative Adversarial Nets. NIPS, 2014.\n[2] Martin Arjovsky, Soumith Chintala, Léon Bottou. Wasserstein GAN. arXiv preprint, arXiv:1701.07875, 2017.\n[3] Alex Irpan. Read-through: Wasserstein GAN. Sorta Insightful.\n[4] Elias M. Stein \u0026 Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007.\nFootnotes Thus, in the trivial case, each of the components is an MLP. ↩︎\nA probability distribution $P\\in\\text{Prob}(\\mathcal{X})$ admits a density $p(x)$ w.r.t $\\mu$ means for all $A\\in\\Sigma$ \\begin{equation} P(A)=\\int_A p(x)d\\mu(x)\\nonumber, \\end{equation} which happens iff $P$ is absolutely continuous w.r.t $\\mu$, i.e. for all $A\\in\\Sigma$, $\\mu(A)=0$ implies $P(A)=0.$ ↩︎\nOn $\\mathbb{R}^d$, $U$ is a neighborhood of $x$ if there exists a ball $B\\subset U$ such that $x\\in B$. ↩︎\nLet $f:E\\mapsto F$ and $A\\subset E$, then the restriction of $f$ to $A$ is the function \\begin{equation} f\\vert_A:A\\mapsto F\\nonumber \\end{equation} given by $f\\vert_A(x)=f(x)$ for $x\\in A$. Generally speaking, the restriction of $f$ to $A$ is the same function as $f$, but only defined on $A$. ↩︎\n$X$ is called locally compact if every point $x\\in X$ has a compact neighborhood, i.e. there exists an open set $U$ and a compact set $K$, such that $x\\in U\\subset K.$ ↩︎\nThis happens iff $g$ is Lipschitz continuous on every compact subset of $\\mathcal{Z}\\times\\mathbb{R}^d.$ ↩︎\n","wordCount":"1795","inLanguage":"en","datePublished":"2023-05-01T13:00:00+07:00","dateModified":"2023-05-01T13:00:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/machine-learning/gan/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io/ accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>GAN</h1><div class=post-meta><span title='2023-05-01 13:00:00 +0700 +0700'>May 1, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#gan>Generative Adversarial Network (GAN)</a><ul><li><a href=#training-gan>Training GAN</a></li></ul></li><li><a href=#wasserstein-gan-wgan>Wasserstein GAN (WGAN)</a><ul><li><a href=#different-distances>Different Distances</a></li><li><a href=#example-learning-parallel-lines>Example: Learning parallel lines</a></li><li><a href=#convergence-and-continuity-of-wasserstein-distance>Convergence and continuity of Wasserstein distance</a></li><li><a href=#wasserstein-gan>Wasserstein GAN</a></li></ul></li><li><a href=#preferences>Preferences</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Generative Adversarial Networks.</p></blockquote><h2 id=gan>Generative Adversarial Network (GAN)<a hidden class=anchor aria-hidden=true href=#gan>#</a></h2><p>A <strong>generative adversarial network</strong> consists of two independent components, each of which is a neural network<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, acting as adversaries:</p><ul><li><b>Generator</b>: A generative model, denoted $G$, parameterized by $\theta_g$. The model is trained to generate fake samples as real as possible.</li><li><b>Discriminator</b>: A discriminative model, denoted $D$, parameterized by $\theta_d$. The model is trained to distinguish between the samples generated by $G$ and the training samples.</li></ul><p>To be more precise,</p><ul class=roman-list><li>Let $p_r$ denote the distribution over real data (training sample), the discriminator $D$ is trained to maximize $\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D(\mathbf{x};\theta_d)\big]$.</li><li>$G$ implicitly defines a probability distribution, which we denote as $p_g$. Let $\mathbf{z}$ denote an input noise variable, $\mathbf{z}\sim p_\mathbf{z}$ , which is the input to $G$. Thus, as assigning the correct label to data generated by $G$ and the real data, $D$ is also trained to maximize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.</li><li>At the same time, the generator $G$ is trained to minimize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.</li></ul><p>Therefore, the adversarial framework can be considered as a two-player minimax game in which we are trying to optimize the value function $V(D,G)$
\begin{equation}
\min_{\theta_g}\max_{\theta_d}V(D,G)=\mathbb{E}_{\mathbf{x}\sim p_r(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(G(\mathbf{z};\theta_g);\theta_d)\big)\big]\label{eq:gan.1}
\end{equation}</p><ul class=number-list><li>Let us consider the optimal discriminator $D$ for any given generator $G$. For $\theta_g$ fixed, we need to maximize
\begin{align}
V(D,G)&=\mathbb{E}_{\mathbf{x}\sim p_r(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(g(\mathbf{z});\theta_d)\big)\big] \\ &= \int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{z}p_\mathbf{z}(\mathbf{z})\log\big(1-D(g(\mathbf{z});\theta_d))\hspace{0.1cm}d\mathbf{z} \\ &=\int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{x}p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x} \\ &=\int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)+p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x},
\end{align}
which, with letting $\bar{\mathbf{x}}$ denote $D(\mathbf{x};\theta_d)$, can be achieved by maximizing
\begin{align}
f(\bar{\mathbf{x}})&=p_r(\mathbf{x})\log\bar{\mathbf{x}}+p_g(\mathbf{x})(1-\log\bar{\mathbf{x}})
\end{align}
Differentiating $f$ w.r.t $\bar{\mathbf{x}}$ gives us
\begin{equation}
\frac{\partial f}{\partial\bar{\mathbf{x}}}=\frac{p_r(\mathbf{x})}{\bar{\mathbf{x}}}-\frac{p_g(\mathbf{x})}{1-\bar{\mathbf{x}}}
\end{equation}
Setting the derivative to zero, we have that the optimal discriminator $D$ is given as
\begin{equation}
D_G^*(\mathbf{x})=\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})},
\end{equation}
at which $f(\bar{\mathbf{x}})$ achieves its maximum since
\begin{equation}
\frac{\partial^2 f}{\partial\bar{\mathbf{x}}^2}=-\frac{p_r(\mathbf{x})}{\bar{\mathbf{x}}^2}-\frac{p_g(\mathbf{x})}{(1-\bar{\mathbf{x}})^2}<0
\end{equation}</li><li>When $\theta_d$ is optimal and fixed, if we define
\begin{equation}
C(G)\overset{\Delta}{=}\max_{\theta_d}V(G,D)=V(G,D_G^*),
\end{equation}
the minimax game in \eqref{eq:gan.1} can be rewritten as minimizing
\begin{align}
\hspace{-0.5cm}C(G)&=\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D_G^*(G(\mathbf{z})))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{x}\sim p_g}\big[\log(1-D_G^*(\mathbf{x}))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\left(1-\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right)\right] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{p_g(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]
\end{align}
We will be showing that $C(G)$ achieves its global minimum iff $p_g=p_r$.<br>We begin by considering the <a href><b>Jensen-Shannon divergence</b></a> between $p_g$ and $p_r$, denoted $D_\text{JS}$:
\begin{align}
\hspace{-1.5cm}D_\text{JS}(p_g\Vert p_r)&=\frac{1}{2}D_\text{KL}\left(p_g\left\Vert\frac{p_g+p_r}{2}\right.\right)+\frac{1}{2}D_\text{KL}\left(p_r\left\Vert\frac{p_g+p_r}{2}\right.\right) \\ &=\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{2 p_r(\mathbf{x})}{p_g(\mathbf{x})+p_r(\mathbf{x})}\right]+\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{2 p_g(\mathbf{x})}{p_g(\mathbf{x})+p_r(\mathbf{x})}\right] \\ &=\frac{1}{2}C(G)+\log 2,
		\end{align}
which implies that
\begin{equation}
C(G)=2D_\text{JS}(p_g\Vert p_r)-2\log 2
\end{equation}
Therefore, $C(G)$ achieves its minimum if and only if $D_\text{JS}(p_g\Vert p_r)$ reaches its minimum, which is zero, occurring when $p_g=p_r$.<br>It then follows when $p_g=p_r$, $C(G)$ achieves its global minimum, which is $-2\log 2$.</li></ul><h3 id=training-gan>Training GAN<a hidden class=anchor aria-hidden=true href=#training-gan>#</a></h3><p>We will apply the minibatch SGD method for training GAN.</p><figure><img src=/images/gan/gan.png alt=GAN><figcaption></figcaption></figure><h2 id=wasserstein-gan-wgan>Wasserstein GAN (WGAN)<a hidden class=anchor aria-hidden=true href=#wasserstein-gan-wgan>#</a></h2><p>Wasserstein is a variation of GAN which use Wasserstein metric to measure the distance between probability distributions $p_r$ and $p_g$ instead of the Jensen-Shannon divergence.</p><h3 id=different-distances>Different Distances<a hidden class=anchor aria-hidden=true href=#different-distances>#</a></h3><p>Let $\mathcal{X}$ be a compact metric set, $\Sigma$ be the set of all the Borel subsets of $\mathcal{X}$ and let $\text{Prob}(\mathcal{X})$ denote the space of probability measures defined on $\mathcal{X}$. We can now define elementary distances and divergences between two probability distributions $P_r,P_g\in\text{Prob}(\mathcal{X})$</p><ul><li><strong>Total Variation (TV)</strong> distance
\begin{equation}
\delta(P_r,P_g)=\sup_{A\in\Sigma}\big\vert P_r(A)-P_g(A)\big\vert
\end{equation}</li><li><strong>Kullback-Leibler (KL)</strong> divergence
\begin{equation}
D_\text{KL}(P_r\Vert P_g)=\int_\mathcal{X}p_r(x)\log\frac{p_r(x)}{p_g(x)}d\mu(x),
\end{equation}
where both $P_r,P_g$ are assumed to be absolutely continuous, and therefore admit densities, w.r.t a measure $\mu$<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. The divergence is not symmetric and infinite when there are points such that $P_g(x)=0$ and $P_r(x)>0$.</li><li><b id=jsd>Jensen-Shannon (JS)</b> divergence
\begin{equation}
D_\text{JS}(P_r\Vert P_g)=\frac{1}{2}D_\text{KL}(P_r\Vert P_m)+\frac{1}{2}D_\text{KL}(P_g\Vert P_m)
\end{equation}
where $P_m$ is the mixture distribution $P_m=\frac{P_r+P_g}{2}$. The divergence is symmetric.</li><li><strong>Earth Mover (EM)</strong> distance, or <strong>Wasserstein-1</strong>
\begin{equation}
W(P_r,P_g)=\inf_{\gamma\in\Pi(P_r,P_g)}\mathbb{E}_{(x,y)\sim\gamma}\big[\Vert x-y\Vert_2\big]\label{eq:dd.1},
\end{equation}
where $\Pi(P_r,P_g)$ is the set of all joint distribution $\gamma(x,y)$ whose marginal distributions are $P_r$ and $P_g$.</li></ul><h3 id=example-learning-parallel-lines>Example: Learning parallel lines<a hidden class=anchor aria-hidden=true href=#example-learning-parallel-lines>#</a></h3><p>Consider probability distributions defined on $\mathbb{R}^2$:</p><ul><li>$P_0$ is the distribution of $(0,Y)$ where $Y\sim\text{Unif}(0,1)$.</li><li>$P_\theta$ is the family of distribution of $(\theta,Y)$ with $Y\sim\text{Unif}(0,1)$.</li></ul><p>Our goal is to learn to move from $\theta$ to $0$, i.e. as $\theta$ approaches $0$, the distance or divergence $d(P_0,P_\theta)$ tends to $0$. For each distance, or divergence we have</p><ul class=number-list><li><b>TV distance</b>. If $\theta=0$ then $P_0=P_\theta$, and thus
\begin{equation}
\delta(P_0,P_\theta)=\sup_{A\in\Sigma}\vert P_0(A)-P_\theta(A)\vert=0
\end{equation}
If $\theta\neq 0$, let $\bar{A}=\{(0,y):y\in[0,1]\}$, we have
\begin{equation}
\delta(P_0,P_\theta)=\sup_{A\in\Sigma}\vert P_0(A)-P_\theta(A)\vert\geq\vert P_0(\bar{A})-P_\theta(\bar{A})\vert=1
\end{equation}
Also, by definition we also have that $0\leq\delta(P_0,P_\theta)\leq 1$. Hence, for $\theta\neq 0,\delta(P_0,P_\theta)=1$.
\begin{equation}
\delta(P_0,P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ 1&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
\end{equation}</li><li><b>KL divergence</b>. By definition, we have that
\begin{equation}
D_\text{KL}(P_0\Vert P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }P_0=P_\theta \\ +\infty&\hspace{1cm}\text{if }!x\in\mathbb{R}^2:P_0(x)>0,P_\theta(x)=0\end{cases}
\end{equation}
Therefore, we have that when $\theta\neq 0$, for any $a\in[0,1]$,
\begin{equation}
D_\text{KL}(P_0\Vert P_\theta)=+\infty,
\end{equation}
since $P_0(0,a)>0$ and $P_\theta(0,a)=0$, and
\begin{equation}
D_\text{KL}(P_\theta\Vert P_0)=+\infty,
\end{equation}
since $P_\theta(\theta,a)>0$ while $P_0(\theta,a)=0$. Therefore, it follows that
\begin{equation}
D_\text{KL}(P_0\Vert P_\theta)=D_\text{KL}(P_\theta\Vert P_0)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ +\infty&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
\end{equation}</li><li><b>JS divergence</b>. It is easy to see that for $\theta=0$, we have $D_\text{JS}(P_0\Vert P_\theta)=0$. Let us consider the case that $\theta\neq 0$. Let $P_m$ denote the mixture $\frac{P_0+P_\theta}{2}$, we have
\begin{equation}
D_\text{JS}(P_0\Vert P_\theta)=\frac{1}{2}D_\text{KL}(P_0\Vert P_m)+\frac{1}{2}D_\text{KL}(P_\theta\Vert P_m)
\end{equation}
Consider the first KL divergence, we have that
\begin{align}
D_\text{KL}(P_0\Vert P_m)&=\int_{x}\int_{0}^{1}P_0(x,y)\log\frac{P_0(x,y)}{P_m(x,y)}\,dy\,dx \\ &\overset{\text{(i)}}{=}\int_{0}^{1}P_0(0,y)\log\frac{2P_0(0,y)}{P_0(0,y)+P_\theta(0,y)}\,dy\nonumber \\ &+\int_{0}^{1}\int_{x\neq 0}P_0(x,y)\log\frac{2P_0(x,y)}{P_0(x,y)+P_\theta(x,y)}\,dx\,dy \\ &\overset{\text{(ii)}}{=}\int_{0}^{1}P_0(0,y)\log 2\,dy+\int_{0}^{1}0\,dy \\ &=\log 2\int_{0}^{1}P_0(0,y)\,dy \\ &\overset{\text{(iii)}}{=}\log 2,
 \end{align}
where<ul class=roman-list><li>In this step, we use the Fubini's theorem to exchange the order of integration.</li><li>In this step, we use the fact that for any $y\in[0,1],P_0(0,y)>0$ while $P_\theta(0,y)=0$ and for any $x\neq 0,P_0(x,y)=0$.</li><li>In this step, the expression inside the integration integrates to $1$ since $P_0$ is a probability distribution.</li></ul>Similarly, we also have that
\begin{equation}
D_\text{KL}(P_\theta\Vert P_m)=\log 2
\end{equation}
Therefore, it follows that
\begin{equation}
D_\text{JS}(P_0\Vert P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ \log 2&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
\end{equation}</li><li><b>EM distance</b>.</li></ul><p>In other words,</p><ul class=number-list><li>As $\theta_t\to 0$, the sequence $(P_{\theta_t})_{t\in\mathbb{N}}$ converges to $P_0$ under the EM distance, but does not converge under either the TV, KL, reverse KL or JS divergences.</li><li>If we consider these distances and divergences as functions of $P_0$ and $P_\theta$, it is easily seen that EM is continuous, while none of the others is.</li></ul><p><strong>Remark</strong>: The example shows that</p><ul><li>There exist sequences of distributions that do not converge under either TV, KL, reverse KL or JS divergences but do converge under the EM distance.</li><li>There are cases where EM is continuous everywhere, while all of the others are discontinuous.</li></ul><h3 id=convergence-and-continuity-of-wasserstein-distance>Convergence and continuity of Wasserstein distance<a hidden class=anchor aria-hidden=true href=#convergence-and-continuity-of-wasserstein-distance>#</a></h3><p>Let $(X,d_X)$ and $(Y,d_Y)$ be two metric spaces. A function $f:X\mapsto Y$ is called <strong>Lipschitz continuous</strong> if there exists a constant $K\geq 0$ such that, for all $x_1,x_2\in X$,
\begin{equation}
d_Y(f(x_1),f(x_2))\leq K d_X(x_1,x_2)
\end{equation}
In this case, $K$ is known as a <strong>Lipschitz constant</strong> for $f$, while $f$ is referred to as <strong>K-Lipschitz</strong>. If $K=1$, $f$ is called a <strong>short map</strong> (or <strong>weak contraction</strong>). And if $0\leq K&lt;1$ and $f$ maps a metric to itself, i.e. $X=Y,$ then $f$ is called a <strong>contraction</strong>.</p><p>The function $f$ is called <strong>locally Lipschitz continuous</strong> if $\forall x\in X$, there exists a <strong>neighborhood</strong><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> $U$ of $x$ such that $f$ restricted<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> to $U$ is Lipschitz continuous.</p><p>Equivalently, if $X$ is a locally compact<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> metric space, then $f$ is <strong>locally Lipschitz</strong> iff it is Lipschitz continuous on every compact subset of $X$.</p><p><b id=asmptn1>Assumption 1</b><br><em>Let $g:\mathcal{Z}\times\mathbb{R}^d\mapsto\mathcal{X}$ be locally Lipschitz between finite dimensional vector spaces<sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> and denote $g_\theta(z)\doteq g(z,\theta)$. We say that $g$ satisfies <a href=#asmptn1>Assumption 1</a> for a certain probability $p$ over $\mathcal{Z}$ if there are local Lipschitz constants $L(\theta,z)$ such that</em>
\begin{equation}
\mathbb{E}_{z\sim p}\big[L(\theta,z)\big]\leq+\infty
\end{equation}</p><p><b id=theorem1>Theorem 1</b></br><em>Let $P_r$ be a fixed distribution over $\mathcal{X}$ and $Z$ be a r.v over another space $\mathcal{Z}$. Let $g:\mathcal{Z}\times\mathbb{R}^d\mapsto\mathcal{X}$ with $g_\theta(z)\doteq g(\theta,z)$. And let $P_\theta$ denote the distribution of $g_\theta(Z)$. Then</em></p><ul class=number-list style=font-style:italic><li id=theorem1-1>If $g$ is continuous in $\theta$, so is $W(P_r,P_\theta)$.</li><li id=theorem1-2>If $g$ is locally Lipschitz and satisfies <a href=#asmptn1>Assumption 1</a>, then $W(P_r,P_\theta)$ is continuous everywhere, and differentiable almost everywhere.</li><li>Statements <a href=#theorem1-1>(1)</a> and <a href=#theorem1-2>(2)</a> are false for the JS divergence $D_\text{JS}(P_r\Vert P_\theta)$, the KL divergence $D_\text{KL}(P_r\Vert P_\theta)$ and the reverse KL divergence $D_\text{KL}(P_\theta\Vert P_r)$.</li></ul><p><strong>Proof</strong><br>TODO</p><p>We can apply this result to neural networks, as following.</p><p><strong>Corollary 1</strong><br><em>Let $g_\theta$ be any feedforward neural network parameterized by $\theta$, and let $p(z)$ be a prior over $z$ such that $\mathbb{E}_{z\sim p(z)}\big[\Vert z\Vert_2\big]\leq\infty$ (e.g. Gaussian, uniform, etc). Then <a href=#asmptn1>Assumption 1</a> is satisfied and therefore $W(P_r,P_\theta)$ is continuous everywhere and differentiable almost everywhere.</em></p><p><strong>Proof</strong><br>TODO</p><p><b id=theorem2>Theorem 2</b></br><em>Let $P$ be a distribution on a compact space $\mathcal{X}$ and $(P_n)_{n\in\mathbb{N}}$ be a sequence of distribution on $\mathcal{X}$. Then, as $n\to\infty$,</em></p><ul class=number-list style=font-style:italic><li id=theorem2-1>The following statements are equivalent<ul class=roman-list><li>$\delta(P_n,P)\to 0$.</li><li>$D_\text{JS}(P_n\Vert P)\to 0$.</li></ul></li><li id=theorem2-2>The following statements are equivalent<ul class=roman-list><li>$W(P_n,P)\to 0$.</li><li>$P_n\overset{\mathcal{D}}{\to}P$ where $\overset{\mathcal{D}}{\to}$ denotes convergence in distribution for r.v.s.</li></ul></li><li>$D_\text{KL}(P_n\Vert P)\to 0$ or $D_\text{KL}(P\Vert P_n)\to 0$ imply the statements in <a href=#theorem2-1>(1)</a>.</li><li>The statements in <a href=#theorem2-1>(1)</a> imply the statements in <a href=#theorem2-2>(2)</a>.</li></ul><p>In other words, the theorem states that every distributions that converges under the TV, KL, reverse KL and JS divergences also converges under the Wasserstein distance.</p><p><strong>Proof</strong><br>TODO</p><h3 id=wasserstein-gan>Wasserstein GAN<a hidden class=anchor aria-hidden=true href=#wasserstein-gan>#</a></h3><p>Despite of having such nice properties, the infimum when computing EM distance \eqref{eq:dd.1}
\begin{equation}
W(P_r,P_g)=\inf_{\gamma\sim\Pi(P_r,P_g)}\mathbb{E}_{(x,y)\sim\gamma}\big[\Vert x-y\Vert_2\big]
\end{equation}
is however intractable. This suggests us to compute an approximation instead.</p><p><strong>Theorem 3</strong> (<em>Kantorovich-Rubinstein duality</em>)<br>\begin{equation}
W(P_r,P_\theta)=\sup_{\Vert f\Vert_L\leq 1}\mathbb{E}_{x\sim P_r}[f(x)]-\mathbb{E}_{x\sim P_\theta}[f(x)]
\end{equation}</p><p><strong>Proof</strong><br>Let $\mathcal{X}$ be a compact metric and let $\text{Prob}(\mathcal{X})$ denote the space of probability measures defined on $\mathcal{X}$. Let us consider two probability distribution $P_r,P_\theta\in\text{Prob}(\mathcal{X})$ with densities $p_r,p_\theta$ respectively. Let $\Pi(P_r,P_\theta)$ be the sets of all joint distribution $\gamma(x,y)$ whose marginals are respectively $P_r$ and $P_\theta$, i.e.
\begin{align}
p_r(x)&=\int_\mathcal{X}\gamma(x,y)\hspace{0.1cm}dy,\label{eq:wgan.1} \\ p_\theta(y)&=\int_\mathcal{X}\gamma(x,y)\hspace{0.1cm}dx\label{eq:wgan.2}
\end{align}
Their Wasserstein distance is given as
\begin{equation}
W(P_r,P_\theta)=\inf_{\gamma\in\Pi(P_r,P_\theta)}\mathbb{E}_{(x,y)\sim\gamma}\big[\Vert x-y\Vert_2\big],
\end{equation}
which is a convex optimization problem with constrains given in \eqref{eq:wgan.1} and \eqref{eq:wgan.2}. Let $f,g:\mathcal{X}\mapsto\mathbb{R}$ be Lagrange multipliers, the Lagrangian is then given as
\begin{align}
\hspace{-1cm}\mathcal{L}(\gamma,f,g)&=\mathbb{E}_{(x,y)\sim\gamma}\big[\Vert x-y\Vert_2\big]+\int_\mathcal{X}\left(p_r(x)-\int_\mathcal{X}\gamma(x,y)\hspace{0.1cm}dy\right)f(x)\hspace{0.1cm}dx \nonumber\\ &\hspace{4cm}+\int_\mathcal{X}\left(p_\theta(y)-\int_\mathcal{X}\gamma(x,y)\hspace{0.1cm}dx\right)g(y)\hspace{0.1cm}dy \\ &=\mathbb{E}_{x\sim P_r}\big[f(x)\big]+\mathbb{E}_{y\sim P_\theta}\big[g(y)\big]+\int_{\mathcal{X}\times\mathcal{X}}\gamma(x,y)\Big(\Vert x-y\Vert_2-f(x)-g(y)\Big)\hspace{0.1cm}dy\hspace{0.1cm}dx
\end{align}
Applying strong duality (?? TODO), we have
\begin{equation}
W(P_r,P_\theta)=\inf_\gamma\sup_{f,g}\mathcal{L}(\gamma,f,g)=\sup_{f,g}\inf_\gamma\mathcal{L}(\gamma,f,g)
\end{equation}</p><p>TODO</p><p>If we replace $\Vert f\Vert_L\leq 1$ by $\Vert f\Vert_L\leq K$ (i.e. we instead consider the supremum over K-Lipschitz functions)</p><h2 id=preferences>Preferences<a hidden class=anchor aria-hidden=true href=#preferences>#</a></h2><p>[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. <a href=http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf>Generative Adversarial Nets</a>. NIPS, 2014.</p><p>[2] Martin Arjovsky, Soumith Chintala, Léon Bottou. <a href=https://arxiv.org/abs/1701.07875>Wasserstein GAN</a>. arXiv preprint, arXiv:1701.07875, 2017.</p><p>[3] Alex Irpan. <a href=https://www.alexirpan.com/2017/02/22/wasserstein-gan.html>Read-through: Wasserstein GAN</a>. Sorta Insightful.</p><p>[4] Elias M. Stein & Rami Shakarchi. <a href=http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf>Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. Princeton University Press, 2007.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Thus, in the trivial case, each of the components is an MLP.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>A probability distribution $P\in\text{Prob}(\mathcal{X})$ admits a density $p(x)$ w.r.t $\mu$ means for all $A\in\Sigma$
\begin{equation}
P(A)=\int_A p(x)d\mu(x)\nonumber,
\end{equation}
which happens iff $P$ is absolutely continuous w.r.t $\mu$, i.e. for all $A\in\Sigma$, $\mu(A)=0$ implies $P(A)=0.$&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>On $\mathbb{R}^d$, $U$ is a neighborhood of $x$ if there exists a <a href=https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#ball>ball</a> $B\subset U$ such that $x\in B$.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Let $f:E\mapsto F$ and $A\subset E$, then the <strong>restriction of $f$ to $A$</strong> is the function
\begin{equation}
f\vert_A:A\mapsto F\nonumber
\end{equation}
given by $f\vert_A(x)=f(x)$ for $x\in A$. Generally speaking, the restriction of $f$ to $A$ is the same function as $f$, but only defined on $A$.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>$X$ is called <strong>locally compact</strong> if every point $x\in X$ has a compact neighborhood, i.e. there exists an <a href=https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#open-set>open set</a> $U$ and a <a href=https://trunghng.github.io/posts/measure-theory/measure-theory-p1/#compact-set>compact set</a> $K$, such that $x\in U\subset K.$&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>This happens iff $g$ is Lipschitz continuous on every compact subset of $\mathcal{Z}\times\mathbb{R}^d.$&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/machine-learning/>Machine-Learning</a></li><li><a href=https://trunghng.github.io/tags/generative-model/>Generative-Model</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/maddpg/><span class=title>« Prev</span><br><span>Multi-agent Deep Deterministic Policy Gradient</span>
</a><a class=next href=https://trunghng.github.io/posts/machine-learning/pgm-learning/><span class=title>Next »</span><br><span>Read-through: Probabilistic Graphical Models - Learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share GAN on x" href="https://x.com/intent/tweet/?text=GAN&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f&amp;hashtags=machine-learning%2cgenerative-model"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f&amp;title=GAN&amp;summary=GAN&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f&title=GAN"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on whatsapp" href="https://api.whatsapp.com/send?text=GAN%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on telegram" href="https://telegram.me/share/url?text=GAN&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share GAN on ycombinator" href="https://news.ycombinator.com/submitlink?t=GAN&u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fgan%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io/>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>