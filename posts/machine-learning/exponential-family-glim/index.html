<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>The Exponential Family, Generalized Linear Models | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="machine-learning,exponential-family,generalized-linear-model,probabilistic-graphical-model"><meta name=description content="
Notes on Exponential Family & Generalized Linear Models.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/machine-learning/exponential-family-glim/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="The Exponential Family, Generalized Linear Models"><meta property="og:description" content="
Notes on Exponential Family & Generalized Linear Models.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/machine-learning/exponential-family-glim/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-04-04T14:00:00+07:00"><meta property="article:modified_time" content="2022-04-04T14:00:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="The Exponential Family, Generalized Linear Models"><meta name=twitter:description content="
Notes on Exponential Family & Generalized Linear Models.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"The Exponential Family, Generalized Linear Models","item":"https://trunghng.github.io/posts/machine-learning/exponential-family-glim/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"The Exponential Family, Generalized Linear Models","name":"The Exponential Family, Generalized Linear Models","description":" Notes on Exponential Family \u0026amp; Generalized Linear Models.\n","keywords":["machine-learning","exponential-family","generalized-linear-model","probabilistic-graphical-model"],"articleBody":" Notes on Exponential Family \u0026 Generalized Linear Models.\nThe Exponential Family The exponential family of distributions is defined as family of distributions of form \\begin{align} p(x;\\eta)\u0026=h(x)\\exp\\Big[\\eta^\\text{T}T(x)-A(\\eta)\\Big],\\label{eq:ef.1} \\\\ \u0026= \\frac{1}{Z(\\eta)}h(x)\\exp\\Big[\\eta^\\text{T}T(x)\\Big] \\end{align} where\n$\\eta$ is known as the natural parameter, or canonical parameter, $T(X)$ is referred to as a sufficient statistic, $A(\\eta)$ is called the cumulant function, which can be view as the logarithm of a normalization factor (or partition function) $Z(\\eta)$, i.e. $A(\\eta)=\\log Z(\\eta)$, since integrating \\eqref{eq:ef.1} w.r.t the measure $\\nu$ gives us \\begin{equation} A(\\eta)=\\log\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx),\\label{eq:ef.2} \\end{equation} This also implies that $A(\\eta)$ will be determined once we have specified $\\nu,T(x)$ and $h(x)$. The set of parameters $\\eta$ for which the integral in \\eqref{eq:ef.2} is finite is known as the natural parameter space \\begin{equation} \\mathcal{N}=\\left\\{\\eta:\\int h(x)\\exp\\left(\\eta^\\text{T}T(x)\\right)\\nu(dx)\u003c\\infty\\right\\} \\end{equation} which explains why $\\eta$ is also referred as natural parameter. If $\\mathcal{N}$ is an non-empty open set, the exponential family is said to be a linear exponential family.\nAn exponential family is known as minimal if there are no linear constraints among the components of $\\eta$ nor are there linear constraints among the components of $T(x)$. A linear exponential family in a minimal representation is referred as regular exponential family.\nExamples Each particular choice of $\\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\\eta$. As we vary $\\eta$, we then get different distributions within this family.\nBernoulli distribution The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\\sim\\text{Bern}(\\pi)$, is given by \\begin{align} p(x;\\pi)\u0026=\\pi^x(1-\\pi)^{1-x} \\\\ \u0026=\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026=\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which can be written in the form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026=1 \\end{align} Notice that the relationship between $\\eta$ and $\\pi$ is invertible since \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}}, \\end{equation} which is the sigmoid function.\nBinomial distribution The probability mass function of a Binomial random variable $X$, denoted as $X\\sim\\text{Bin}(N,\\pi)$, is defined as \\begin{align} p(x;N,\\pi)\u0026={N\\choose x}\\pi^{x}(1-\\pi)^{1-x} \\\\ \u0026={N\\choose x}\\exp\\big[x\\log\\pi+(1-x)\\log(1-\\pi)\\big] \\\\ \u0026={N\\choose x}\\exp\\left[\\log\\left(\\frac{\\pi}{1-\\pi}\\right)x+\\log(1-\\pi)\\right], \\end{align} which is in form of an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\frac{\\pi}{1-\\pi} \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=-\\log(1-\\pi)=\\log(1+e^{\\eta}) \\\\ h(x)\u0026={N\\choose x} \\end{align} Similar to the Bernoulli case, we also have the invertible relationship between $\\eta$ and $\\pi$ as \\begin{equation} \\pi=\\frac{1}{1+e^{-\\eta}} \\end{equation}\nPoisson distribution The probability mass function of a Poisson random variable $X$, denoted as $X\\sim\\text{Pois}(\\lambda)$, is given as \\begin{align} p(x;\\lambda)\u0026=\\frac{\\lambda^x e^{-\\lambda}}{x!} \\\\ \u0026=\\frac{1}{x!}\\exp\\left(x\\log\\lambda-\\lambda\\right), \\end{align} which is also able to be written as an exponential family distribution \\eqref{eq:ef.1} with \\begin{align} \\eta\u0026=\\log\\lambda \\\\ T(x)\u0026=x \\\\ A(\\eta)\u0026=\\lambda=e^{\\eta} \\\\ h(x)\u0026=\\frac{1}{x!} \\end{align} Analogy to Bernoulli distribution, we also have that \\begin{equation} \\lambda=e^{\\eta} \\end{equation}\nGaussian distribution The (univariate) Gaussian density of a random variable $X$, denoted as $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$, is given by \\begin{align} p(x;\\mu,\\sigma^2)\u0026=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right] \\\\ \u0026=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left[\\frac{\\mu}{\\sigma^2}x-\\frac{1}{2\\sigma^2}x^2-\\frac{1}{2\\sigma^2}\\mu^2-\\log\\sigma\\right], \\end{align} which allows us to write it as an instance of the exponential family with \\begin{align} \\eta\u0026=\\left[\\begin{matrix}\\mu/\\sigma^2 \\\\ -1/2\\sigma^2\\end{matrix}\\right] \\\\ T(x)\u0026=\\left[\\begin{matrix}x\\\\ x^2\\end{matrix}\\right] \\\\ A(\\eta)\u0026=\\frac{\\mu^2}{2\\sigma^2}+\\log\\sigma=-\\frac{\\eta_1^2}{4\\eta_2}-\\frac{1}{2}\\log(-2\\eta_2) \\\\ h(x)\u0026=\\frac{1}{\\sqrt{2\\pi}} \\end{align}\nMultinomial distribution Let $\\mathbf{X}=(X_1,\\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\\mathbf{\\pi}=(\\pi_1,\\ldots,\\pi_K)$ with $\\sum_{k=1}^{K}\\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.\nThen $\\mathbf{X}$ is said to have Multinomial distribution, denoted as $\\mathbf{X}\\sim\\text{Mult}_K(N,\\boldsymbol{\\pi})$, if its probability mass function is given as with $\\sum_{k=1}^{K}x_k=1$ \\begin{align} p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\pi_1^{x_1}\\pi_2^{x_2}\\ldots\\pi_n^{x_n} \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right)\\label{eq:m.1} \\end{align} It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\\mathbf{x})$, which is \\begin{equation} \\sum_{k=1}^{K}x_k=1 \\end{equation} In order to remove this constraint, we substitute $1-\\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \\eqref{eq:m.1} be written by \\begin{align} \\hspace{-0.8cm}p(\\mathbf{x};\\boldsymbol{\\pi},N,K)\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left(\\sum_{k=1}^{K}x_k\\log\\pi_k\\right) \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{k=1}^{K-1}x_k\\log\\pi_k+\\left(1-\\sum_{k=1}^{K-1}x_k\\right)\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right] \\\\ \u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!}\\exp\\left[\\sum_{i=1}^{K-1}\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)x_i+\\log\\left(1-\\sum_{k=1}^{K-1}\\pi_k\\right)\\right]\\label{eq:m.2} \\end{align} With this representation, and also for convenience, for $i=1,\\ldots,K$ we continue by letting \\begin{equation} \\eta_i=\\log\\left(\\frac{\\pi_i}{1-\\sum_{k=1}^{K-1}\\pi_k}\\right)=\\log\\left(\\frac{\\pi_i}{\\pi_K}\\right)\\label{eq:m.3} \\end{equation} Take the exponential of both sides and summing over $K$, we have \\begin{equation} \\sum_{i=1}^{K}e^{\\eta_i}=\\frac{\\sum_{i=1}^{K}\\pi_i}{\\pi_K}=\\frac{1}{\\pi_K}\\label{eq:m.4} \\end{equation} From this result, we have that the Multinomial distribution \\eqref{eq:m.2} is therefore also a member of the exponential family with \\begin{align} \\eta\u0026=\\left[\\begin{matrix}\\log\\left(\\pi_1/\\pi_K\\right) \\\\ \\vdots \\\\ \\log\\left(\\pi_K/\\pi_K\\right)\\end{matrix}\\right] \\\\ T(\\mathbf{x})\u0026=\\left[\\begin{matrix}x_1,\\ldots,x_K\\end{matrix}\\right]^\\text{T} \\\\ A(\\eta)\u0026=-\\log\\left(1-\\sum_{i=1}^{K-1}\\pi_i\\right)=-\\log(\\pi_K)=\\log\\left(\\sum_{k=1}^{K}e^{\\eta_k}\\right) \\\\ h(\\mathbf{x})\u0026=\\frac{N!}{x_1!x_2!\\ldots x_K!} \\end{align} Additionally, substituting the result \\eqref{eq:m.4} into \\eqref{eq:m.3} gives us for $i=1,\\ldots,K$ \\begin{equation} \\eta_i=\\log\\left(\\pi_i\\sum_{k=1}^{K}e^{\\eta_k}\\right), \\end{equation} or we can express $\\boldsymbol{\\pi}$ in terms of $\\eta$ by \\begin{equation} \\pi_i=\\frac{e^{\\eta_i}}{\\sum_{k=1}^{K}e^{\\eta_k}}, \\end{equation} which is the softmax function.\nMultivariate Normal distribution For the case of a multivariate Normal r.v $\\mathbf{X}$, we have its PDF is given as \\begin{align} p(\\mathbf{x};\\boldsymbol{\\mu},\\boldsymbol{\\Sigma},K)\u0026=\\frac{1}{(2\\pi)^{K/2}\\vert\\boldsymbol{\\Sigma}\\vert^{1/2}}\\exp\\left[-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\text{T}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right] \\\\ \\end{align}\nConvexity Theorem 1: The natural space $\\mathcal{N}$ is a convex set and the cumulant function $A(\\eta)$ is a convex function. If the family is minimal, then $A(\\eta)$ is strictly convex.\nProof\nLet $\\eta_1,\\eta_2\\in\\mathcal{N}$, thus from \\eqref{eq:ef.2}, we have that \\begin{align} \\exp\\big(A(\\eta_1)\\big)\u0026=A_1, \\\\ \\exp\\big(A(\\eta_2)\\big)\u0026=A_2 \\end{align} where $A_1,A_2$ are finite.\nTo prove that $\\mathcal{N}$ is convex, we need to show that for any $\\eta=\\lambda\\eta_1+(1-\\lambda)\\eta_2$ for $0\\lt\\lambda\\lt 1$, we also have $\\eta\\in\\mathcal{N}$. From \\eqref{eq:ef.2}, and by Hölder’s inequality1, we have \\begin{align} \\exp\\big(A(\\eta)\\big)\u0026=\\int h(x)\\exp\\big(\\eta^\\text{T}T(x)\\big)\\nu(dx) \\\\ \u0026=\\int h(x)\\exp\\Big[\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big)^\\text{T}T(x)\\Big]\\nu(dx) \\\\ \u0026=\\int \\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda}\\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}\\nu(dx) \\\\ \u0026\\leq\\Bigg[\\int h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^\\lambda\\Bigg[\\int h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\nu(dx)\\Bigg]^{1-\\lambda} \\\\ \u0026=\\Big[\\exp\\big(A(\\eta_1)\\big)\\Big]^\\lambda\\Big[\\exp\\big(A(\\eta_2)\\big)\\Big]^{1-\\lambda} \\\\ \u0026=A_1^\\lambda A_2^{1-\\lambda},\\label{eq:c.1} \\end{align} which proves that $A(\\eta)$ is finite, or $\\eta\\in\\mathcal{N}$.\nMoreover, taking logarithm of both sides of \\eqref{eq:c.1} gives us \\begin{equation} \\lambda A(\\eta_1)+(1-\\lambda)A(\\eta_2)\\geq A(\\eta)=A\\big(\\lambda\\eta_1+(1-\\lambda)\\eta_2\\big), \\end{equation} which also claims the convexity of $A(\\eta)$.\nAdditionally, by Hölder’s inequality, the equality in \\eqref{eq:c.1} holds when \\begin{equation} \\Big[h(x)\\exp\\big(\\eta_2^\\text{T}T(x)\\big)\\Big]^{1-\\lambda}=c\\Big[h(x)\\exp\\big(\\eta_1^\\text{T}T(x)\\big)\\Big]^{\\lambda(1/\\lambda-1)} \\end{equation} or \\begin{equation} \\exp\\big(\\eta_2^\\text{T}T(x)\\big)=c\\exp\\big(\\eta_1^\\text{T}T(x)\\big), \\end{equation} and therefore \\begin{equation} (\\eta_2-\\eta_1)^\\text{T}T(x)=\\log c, \\end{equation} which is not minimal since $\\eta_1,\\eta_2$ are taken arbitrarily.\nMoments of sufficient statistic In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second cumulants.\nLet us first consider the first derivative of the cumulant function $A(\\eta)$. By the dominated convergence theorem, we have \\begin{align} \\frac{\\partial A(\\eta)}{\\partial\\eta^\\text{T}}\u0026=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\log\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx) \\\\ \u0026=\\frac{\\int T(x)\\exp\\big(\\eta^\\text{T}(x)\\big)h(x)\\nu(dx)}{\\int\\exp\\big(\\eta^\\text{T}T(x)\\big)h(x)\\nu(dx)} \\\\ \u0026=\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx)\\label{eq:mv.1} \\\\ \u0026=\\int T(x)p(x;\\eta)\\nu(dx) \\\\ \u0026=\\mathbb{E}[T(X)],\\label{eq:mv.2} \\end{align} which is the mean of the sufficient statistic $T(X)$.\nMoreover, taking the second derivative of cumulant function by continuing with the result \\eqref{eq:mv.1}, we have \\begin{align} \\frac{\\partial^2 A(\\eta)}{\\partial\\eta\\partial\\eta^\\text{T}}\u0026=\\frac{\\partial}{\\partial\\eta^\\text{T}}\\int T(x)\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\int T(x)\\left(T(x)-\\frac{\\partial}{\\partial\\eta^\\text{T}}A(\\eta)\\right)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\int T(x)\\big(T(x)-E(T(X))\\big)^\\text{T}\\exp\\big(\\eta^\\text{T}T(x)-A(\\eta)\\big)h(x)\\nu(dx) \\\\ \u0026=\\mathbb{E}\\left[T(X)T(X)^\\text{T}\\right]-\\mathbb{E}[T(X)]\\mathbb{E}[T(X)]^\\text{T} \\\\ \u0026=\\text{Var}[T(X)], \\end{align} which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$, or the second central moment of of $T(X)$.\nAccordingly, we have that the differentiating the cumulant function $A(\\eta)$ to $n$-order gives us the $n$-th central moment of $T(X)$.\nThe expected value of $T(X)$, $\\mu$, is also known as the mean parameter (or moment parameter). Additionally, since $\\mu=\\frac{\\partial A(\\eta)}{\\partial\\eta}$ and $A(\\eta)$ is strictly convex (due to Theorem 1), then the relationship $\\mu=\\frac{\\partial A(\\eta)}{\\partial\\eta}$ is invertible, i.e. $\\eta=\\left(\\frac{\\partial A(\\eta)}{\\partial\\eta}\\right)^{-1}(\\mu)$. More specifically, there exists a function $\\psi$ which defines the one-to-one relationship between canonical parameter $\\eta$ and moment parameter $\\mu$, i.e. \\begin{equation} \\eta=\\psi(\\mu)\\label{eq:mv.3} \\end{equation} This implies that a distribution in the exponential family can be parameterized not only by the canonical parameter $\\eta$, but also the mean parameter $\\mu$.\nSufficiency Let $X$ be a r.v and let $T(X)$ be a statistic. Suppose that the distribution of $X$ is parameterized by $\\theta$, i.e. $p(x;\\theta)$. Then $T(X)$ is said to be sufficient for $\\theta$ if there is no information in $X$ regarding $\\theta$ beyond that in $T(X)$. In other words, having observed $T(X)$, we can throw away $X$ for the purpose of inference w.r.t $\\theta$. Specifically\nIn Bayesian approach, $\\theta$ is considered as a r.v. Thus, we say that $T(X)$ is sufficient for $\\theta$ if \\begin{equation} p\\models(\\theta\\perp X\\vert T(X)), \\end{equation} which happens iff \\begin{equation} p(\\theta\\vert T(x),x)=p(\\theta\\vert T(x)) \\end{equation} In frequentist view, $\\theta$ is considered as label rather than a r.v. Thus, $T(X)$ is sufficient for $\\theta$ if the conditional distribution of $X$ given $T(X)$ is not a function of $\\theta$, i.e. \\begin{equation} p(x\\vert T(x),\\theta)=p(x\\vert T(x)) \\end{equation} For undirected models, the joint distribution can be described by a product of factors \\begin{equation} p(x,T(x),\\theta)=\\psi_1(T(x),\\theta)\\psi_2(x,T(x)), \\end{equation} where we have absorbed the partition function $Z$ in one of the potential functions. Moreover, since now $T(X)$ is a deterministic function of $X$, then $T(X)$ can be removed out from the LHS. Dividing both sides by $p(\\theta)$ yields \\begin{equation} p(x\\vert\\theta)=g(T(x),\\theta)h(x,T(x))\\label{eq:suf.1} \\end{equation} Sufficiency and the Exponential Family An important feature of the exponential family is that its sufficient statistic can be obtained by simply observed, once the distribution is written in the form \\eqref{eq:ef.1}. Recall that \\begin{equation} p(x;\\eta)=h(x)\\exp\\left[\\eta^\\text{T}T(x)-A(\\eta)\\right] \\end{equation} From \\eqref{eq:suf.1}, it follows immediately that $T(X)$ is a sufficient statistic for $\\eta$.\nMLE for Exponential Family The reduction obtainable by using a sufficient statistic $T(X)$ is particularly notable in the case of i.i.d sampling.\nConsider an i.i.d data set $\\mathcal{D}=\\{x_1,\\ldots,x_N\\}$, which is composed of $N$ independent r.v.s $X=(X_1,\\ldots,X_N)$, characterized by the sample exponential family density. The likelihood function is then given by: \\begin{align} L(\\eta;\\mathcal{D})=p(\\mathbf{x}\\vert\\eta)\u0026=\\prod_{n=1}^{N}p(x_n\\vert\\eta) \\\\ \u0026=\\prod_{n=1}^{N}h(x_n)\\exp\\big[\\eta^\\text{T}T(x_n)-A(\\eta)\\big] \\\\ \u0026=\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right]\\label{eq:mle.1} \\end{align} It is easily seen that $X$ is also an exponential distribution, with sufficient statistic $\\sum_{n=1}^{N}T(x_n)$.\nTaking the logarithm of both sides gives us the log likelihood as \\begin{equation} \\ell(\\eta)=\\log L(\\eta)=\\sum_{n=1}^{N}\\log h(x_n)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\label{eq:mle.2} \\end{equation} Consider the gradient of the log likelihood w.r.t $\\eta$, we have \\begin{align} \\nabla_\\eta\\ell(\\eta)\u0026=\\nabla_\\eta\\left[\\sum_{n=1}^{N}\\log h(x_n)+\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026=\\sum_{n=1}^{N}T(x_n)-N\\nabla_\\eta A(\\eta) \\end{align} Setting the gradient to zero, we have the value of $\\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\\eta$, denoted as $\\hat{\\eta}_\\text{ML}$ satisfies \\begin{equation} \\nabla_{\\eta}A(\\hat{\\eta}_\\text{ML})=\\frac{1}{N}\\sum_{n=1}^{N}T(x_n) \\end{equation} Finally, defining $\\mu\\doteq\\mathbb{E}\\big[T(X)\\big]$. Then by \\eqref{eq:mv.2}, we have that \\begin{equation} \\hat{\\mu}_\\text{ML}=\\frac{1}{N}\\sum_{n=1}^{N}T(x_n)\\label{eq:mle.3} \\end{equation} as the general formula for MLE of the mean parameter in the exponential family. And thus, by \\eqref{eq:mv.3}, we obtain \\begin{equation} \\hat{\\eta}_\\text{ML}=\\psi(\\hat{\\mu}_\\text{ML}) \\end{equation} It is worth noticing that the above formula involves the data only via the sufficient statistic $\\sum_{n=1}^{N}T(x_n)$.\nExample 1: From the result \\eqref{eq:mle.3}, we have that\nFor exponential family distribution with sufficient statistic $T(X)=X$, e.g. Bernoulli, Binomial, Poisson, the maximum likelihood estimate of the mean is exactly the sample mean, $\\frac{1}{N}\\sum_{n=1}^{N}x_n$. For univariate Normal distribution, which has $T(X)=\\left[\\begin{smallmatrix}X \\\\ X^2\\end{smallmatrix}\\right]$, the maximum likelihood estimate of the mean and variance are precisely the sample mean and sample variance. MLE and the KL Divergence Given a dataset $\\mathcal{D}=\\{x_1,\\ldots,x_N\\}$, the empirical distribution, denoted $\\hat{p}(x)$, is a distribution which places probability mass $1/N$ at each data point $x_n$ in $\\mathcal{D}$. In particular, \\begin{equation} \\hat{p}(x)\\doteq\\frac{1}{N}\\sum_{n=1}^{N}\\delta(x-x_n), \\end{equation} where\nin the continuous case, $\\delta(\\cdot)$ is the Dirac delta function, which has zero-valued everywhere except $0$, and integrates to $1$. in the discrete case, $\\delta(\\cdot)$ is the Kronecker delta function, which takes value of $1$ at $0$, and zero elsewhere, and thus sums to $1$. The following derivations is interchangeable between discrete and continuous circumstance by swapping between summation and integration. Thus, without loss of generality, let us consider the discrete case. Firstly, we have that \\begin{align} \\sum_x\\hat{p}(x)\\log p(x;\\theta)\u0026=\\sum_x\\frac{1}{N}\\sum_{n=1}^{N}\\delta(x-x_n)\\log p(x;\\theta) \\\\ \u0026=\\frac{1}{N}\\sum_{n=1}^{N}\\sum_x\\delta(x-x_n)\\log p(x;\\theta) \\\\ \u0026=\\frac{1}{N}\\sum_{n=1}^{N}\\log p(x;\\theta) \\\\ \u0026=\\frac{1}{N}\\ell(\\theta;\\mathcal{D}) \\end{align} where $\\ell(\\cdot)$ is the log-likelihood function.\nOn the other hand, consider the relative entropy, or KL divergence, between the empirical distribution and the model $p(x;\\theta)$, we have that \\begin{align} D_\\text{KL}(\\hat{p}(x)\\Vert p(x;\\theta))\u0026=\\sum_x\\hat{p}(x)\\log\\left(\\frac{\\hat{p}(x)}{p(x;\\theta)}\\right) \\\\ \u0026=\\sum_x\\hat{p}(x)\\log\\hat{p}(x)-\\sum_x\\hat{p}(x)\\log p(x;\\theta) \\\\ \u0026=\\mathbb{E}_{\\hat{p}}\\big[\\log\\hat{p}(x)\\big]-\\frac{1}{N}\\ell(\\theta;\\mathcal{D}) \\end{align} Since $\\mathbb{E}_\\hat{p}\\big[\\log\\hat{p}(x)\\big]$ does not depend on $\\theta$, then the value of $\\theta$ that minimizes the KL divergence to the empirical distribution is the one that maximizes the log-likelihood.\nConjugate priors Given a probability distribution $p(x\\vert\\eta)$, its prior $p(\\eta)$ is said to be conjugate to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as conjugate prior.\nFor any member of the exponential family, there exists a conjugate prior that can be written in form \\begin{equation} p(\\eta\\vert\\mathcal{X},\\theta)=f(\\mathcal{X},\\theta)\\exp(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)),\\label{eq:cp.1} \\end{equation} where $\\theta\u003e0$ and $\\mathcal{X}$ are hyperparameters.\nBy Bayes’ rule, and with the likelihood function as given in \\eqref{eq:mle.1}, the posterior distribution can be computed as \\begin{align} \u0026\\hspace{0.7cm}p(\\eta\\vert\\mathbf{X},\\mathcal{X},\\theta) \\\\ \u0026\\propto p(\\eta\\vert\\mathcal{X},\\theta)p(\\mathbf{X}\\vert\\eta) \\\\ \u0026=f(\\mathcal{X},\\theta)\\exp\\big(\\eta^\\text{T}\\mathcal{X}-\\theta A(\\eta)\\big)\\left(\\prod_{n=1}^{N}h(x_n)\\right)\\exp\\left[\\eta^\\text{T}\\left(\\sum_{n=1}^{N}T(x_n)\\right)-N A(\\eta)\\right] \\\\ \u0026\\propto\\exp\\left[\\eta^\\text{T}\\left(\\mathcal{X}+\\sum_{n=1}^{N}T(x_n)\\right)-(\\theta+N)A(\\eta)\\right], \\end{align} which is in the same form as \\eqref{eq:cp.1} and therefore claims the conjugacy.\nGeneralized Linear Models The generalized linear model (or GLIM) extends the idea of linear models in classification and regression to a more general settings using the definition of exponential family. Specifically, consider the plate model given below, which consists of two variables $X$ and $Y$ where both of them are assumed to be observed.\nFigure 2: (based on figure from Jordan's book) The graphical model representation of a generalized linear model. A GLIM makes threes assumptions about the conditional distribution $p(y\\vert x)$\nThe observed input $x$ is assumed to enter into the model via a linear combination $\\xi=\\theta^\\text{T}x$. The conditional mean $\\mu$ is represented as a function $f(\\xi)$ of the linear combination $\\xi$, where $f$ is referred as the response function, or link function. The observed output $y$ is assumed to be characterized by an exponential family distribution $p$ with conditional mean $\\mu$. Figure 3: (based on figure from Jordan's book) The relationships between the variables in a GLIM. These assumption is summarized in the above figure. Notice that the diagram also provides us an invertible mapping from the mean parameter $\\mu$ to the canonical parameter $\\eta$, which we denote as $\\eta=\\psi(\\mu)$. This allows us to use $\\eta$ to represent the exponential family distribution for $Y$.\nFormally, we assume that the output of GLIM has the form of \\begin{equation} p(y;\\eta,\\phi)=h(y,\\phi)\\exp\\left[\\frac{\\eta^\\text{T}y-A(\\eta)}{\\phi}\\right], \\end{equation} which is slightly different from the traditional definition of exponential family density by including an explicit scale parameter $\\phi$.\nAdditionally, since $\\eta=\\psi(\\mu)$ and $\\mu=f(\\xi)=f(\\theta^\\text{T}x)$, then we directly have $\\eta=\\psi(f(\\theta^\\text{T}x))$. Thus, the conditional of $y$ given $x,\\theta$ and $\\phi$ is \\begin{equation} p(y\\vert x;\\theta,\\phi)=h(y,\\phi)\\exp\\left[\\frac{y^\\text{T}\\psi(f(\\theta^\\text{T}x))-A(\\psi(\\theta^\\text{T}x))}{\\phi}\\right]\\label{eq:glim.1} \\end{equation} There are two principle choices in designing a GLIM: the choice of exponential family distribution and the choice of the response function $f(\\cdot)$.\nThe former choice strongly depends on the pattern of $Y$. In particular, class labels are naturally represented by Bernoulli or Multinomial, counts by the Poisson, integrals by the exponential or Gamma distributions.\nThe latter choice has more degree of freedom with some mild constraints, e.g. in the case of Bernoulli and Multinomial, the conditional expectation must lie between $0$ and $1$, which suggests us to use a response function $f$ whose range is $(0,1)$; while we choose the response function $f$ with range $(0,\\infty)$ for Gamma distribution, where the r.v is nonnegative. There is a particular choice called the canonical response function, which is $f(\\cdot)=\\psi^{-1}(\\cdot)$. In this case, the conditional probability in \\eqref{eq:glim.1} is simplified to \\begin{equation} p(y\\vert x;\\theta,\\phi)=h(y,\\phi)\\exp\\left[\\frac{y^\\text{T}(\\theta^\\text{T}x)-A(\\theta^\\text{T}x)}{\\phi}\\right] \\end{equation}\nMLE for Generalized Linear Models with canonical response function Consider an i.i.d dataset $\\mathcal{D}=\\{(x_1,y_1),\\ldots,(x_N,y_N)\\}$ where $y_n$ are scalars. The log-likelihood is then given as \\begin{align} \\ell(\\theta;\\mathcal{D})\u0026=\\log\\prod_{n=1}^{N}p(y_n\\vert x_n;\\theta) \\\\ \u0026=\\sum_{n=1}^{N}\\log\\Big[h(y)\\exp\\big[\\eta_n y_n-A(\\eta_n)\\big]\\Big] \\\\ \u0026=\\sum_{n=1}^{N}\\log h(y)+\\sum_{n=1}^{N}\\eta_n y_n-\\sum_{n=1}^{N}A(\\eta_n)\\label{eq:mle-glm.1} \\end{align} With canonical response function, which yields $\\eta_n=\\theta^\\text{T}x_n$ for $n=1,\\ldots,N$, the log-likelihood can be rewritten as \\begin{align} p(y\\vert x;\\theta)\u0026=\\sum_{n=1}^{N}\\log h(y)+\\sum_{n=1}^{N}\\theta^\\text{T}y_n x_n-\\sum_{n=1}^{N}A(\\eta_n) \\\\ \u0026=\\sum_{n=1}^{N}\\log h(y)+\\theta^\\text{T}\\sum_{n=1}^{N}y_n x_n-\\sum_{n=1}^{N}A(\\eta_n) \\end{align} Also, it is worth noticing that from \\eqref{eq:mle.2}, we can see that $\\sum_{n=1}^{N}x_n y_n$ is the sufficient statistic for $\\theta$.\nWe continue by taking the derivative of the log-likelihood in \\eqref{eq:mle-glm.1} w.r.t $\\theta$ instead to get a more general form \\begin{align} \\nabla_\\theta\\ell(\\theta;\\mathcal{D})\u0026=\\sum_{n=1}^{N}\\frac{d\\ell(\\theta;\\mathcal{D})}{d\\eta_n}\\nabla_\\theta\\eta_n \\\\ \u0026=\\sum_{n=1}^{N}\\left(y_n-\\frac{d A(\\eta_n)}{d\\eta_n}\\right)\\frac{d\\eta_n}{d\\mu_n}\\frac{d\\mu_n}{d\\xi_n}\\nabla_\\theta\\xi_n \\\\ \u0026=\\sum_{n=1}^{N}(y_n-\\mu_n)\\frac{d\\eta_n}{d\\mu_n}\\frac{d\\mu_n}{d\\xi_n}x_n\\label{eq:mle-glm.2} \\end{align} In using canonical response function, we have that $f=\\psi^{-1}$, thus $\\eta_n=\\xi_n$, which implies that the derivative of the log-likelihood w.r.t can be simplified as \\begin{equation} \\nabla_\\theta\\ell(\\theta;\\mathcal{D})=\\sum_{n=1}^{N}(y_n-\\mu_n)x_n\\label{eq:mle-glm.3} \\end{equation}\nOnline updating We then can use gradient ascent for estimating the parameter $\\theta$, which has the update rule: \\begin{equation} \\theta^{(t+1)}=\\theta^{(t)}+\\rho(y_n-\\mu_n^{(t)})x_n,\\label{eq:mle-glm.4} \\end{equation} where $\\mu_n^{(t)}=f({\\theta^{(t)}}^\\text{T}x_n)$ and $\\rho$ is the step size.\nNotice that if our choice of $f$ is not the canonical response, \\eqref{eq:mle-glm.4} is also a generic SGD algorithm for models throughout the GLIMs due to the fact that the derivatives of $f(\\cdot)$ and $\\psi(\\cdot)$ in \\eqref{eq:mle-glm.2}, i.e. $\\frac{d\\eta_n}{d\\mu_n}$ and $\\frac{d\\mu_n}{d\\xi_n}$, are absorbed into the step size $\\rho$.\nBatch updating To using a batch algorithm, we start by vectorizing the gradient of the log-likelihood in \\eqref{eq:mle-glm.3}, as \\begin{align} \\nabla_\\theta\\ell(\\theta;\\mathcal{D})\u0026=\\sum_{n=1}^{N}(y_n-\\mu_n)x_n \\\\ \u0026=X^\\text{T}(y-\\mu), \\end{align} where $X$ is a matrix whose rows are $x_n^\\text{T}$, and where $y,\\mu$ are vectors whose components are $y_n$ and $\\mu_n$ respectively, i.e. \\begin{equation} \\mathbf{X}=\\left[\\begin{matrix}-\\hspace{0.1cm}x_1^\\text{T}\\hspace{0.1cm}- \\\\ \\hspace{0.1cm}\\vdots\\hspace{0.1cm} \\\\ -\\hspace{0.1cm}x_N^\\text{T}\\hspace{0.1cm}-\\end{matrix}\\right],\\hspace{1cm}y=\\left[\\begin{matrix}y_1 \\\\ \\vdots \\\\ y_N\\end{matrix}\\right],\\hspace{1cm}\\mu=\\left[\\begin{matrix}\\mu_1 \\\\ \\vdots \\\\ \\mu_N\\end{matrix}\\right] \\end{equation} Additionally, let us consider the Hessian matrix by taking the second derivative of the log-likelihood \\begin{align} H_\\ell\u0026=\\frac{\\partial^2}{\\partial\\theta\\partial\\theta^\\text{T}}\\ell(\\theta;\\mathcal{D}) \\\\ \u0026=\\frac{\\partial}{\\partial\\theta^\\text{T}}\\sum_{n=1}^{N}x_n(y_n-\\mu_n) \\\\ \u0026=-\\sum_{n=1}^{N}x_n\\frac{d\\mu_n}{d\\eta_n}\\frac{\\partial\\eta_n}{\\partial\\theta^\\text{T}} \\\\ \u0026=\\sum_{n=1}^{N}x_n\\frac{d\\mu_n}{d\\eta_n}x_n^\\text{T} \\\\ \u0026=-X^\\text{T}WX, \\end{align} where $W$ is the diagonal weight matrix \\begin{equation} W=\\text{diag}\\left(\\frac{d\\mu_1}{d\\eta_1},\\ldots,\\frac{d\\mu_N}{d\\eta_N}\\right), \\end{equation} whose each diagonal entry can be computed via the second derivative of $A(\\eta_n)$.\nUsing Newton’s method, we have the update rule \\begin{align} \\theta^{(t+1)}\u0026=\\theta^{(t)}-H_\\ell^{-1}\\nabla_\\theta\\ell \\\\ \u0026=-H_\\ell^{-1}(-H_\\ell\\theta^{(t)}+\\nabla_\\theta) \\\\ \u0026=(X^\\text{T}W^{(t)}X)^{-1}\\big[X^\\text{T}W^{(t)}X\\theta^{(t)}+X^\\text{T}(y-\\mu^{(t)})\\big] \\\\ \u0026=(X^\\text{T}W^{(t)}X)^{-1}X^\\text{T}W^{(t)}z^{(t)},\\label{eq:mle-glm.5} \\end{align} where we have defined the adjusted response \\begin{equation} z^{(t)}\\doteq\\eta+\\big(W^{(t)}\\big)^{-1}(y-\\mu^{(t)}) \\end{equation} This can be understood as solving the Iteratively Reweighted Least Squares (IRLS) problem \\begin{equation} \\theta^{(t)}=\\underset{\\theta}{\\text{argmin}}(x-X\\theta)^\\text{T}\\theta(z-X\\theta) \\end{equation}\nReferences [1] M. Jordan. The Exponential Family: Basics. 2009.\n[2] Joseph K. Blitzstein \u0026 Jessica Hwang. Introduction to Probability.\n[3] Weisstein, Eric W. Hölder’s Inequalities From MathWorld–A Wolfram Web Resource.\n[4] Ian Goodfellow \u0026 Yoshua Bengio \u0026 Aaron Courville. Deep Learning. MIT Press, 2016.\nFootnotes Let $p,q\u003e1$ such that \\begin{equation*} \\frac{1}{p}+\\frac{1}{q}=1 \\end{equation*} The Hölder’s inequality for integrals states that \\begin{equation*} \\int_a^b\\vert f(x)g(x)\\vert\\hspace{0.1cm}dx\\leq\\left(\\int_a^b\\vert f(x)\\vert\\hspace{0.1cm}dx\\right)^{1/p}\\left(\\int_a^b\\vert g(x)\\vert\\hspace{0.1cm}dx\\right)^{1/q} \\end{equation*} The equality holds with \\begin{equation*} \\vert g(x)\\vert=c\\vert f(x)\\vert^{p-1} \\end{equation*} ↩︎\n","wordCount":"2782","inLanguage":"en","datePublished":"2022-04-04T14:00:00+07:00","dateModified":"2022-04-04T14:00:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/machine-learning/exponential-family-glim/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>The Exponential Family, Generalized Linear Models</h1><div class=post-meta><span title='2022-04-04 14:00:00 +0700 +0700'>April 4, 2022</span>&nbsp;·&nbsp;14 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#exp-fam>The Exponential Family</a></li><li><a href=#examples>Examples</a><ul><li><a href=#bern-dist>Bernoulli distribution</a></li><li><a href=#bin-dist>Binomial distribution</a></li><li><a href=#pois-dist>Poisson distribution</a></li><li><a href=#gauss-dist>Gaussian distribution</a></li><li><a href=#multinomial-distribution>Multinomial distribution</a></li><li><a href=#mvn-dist>Multivariate Normal distribution</a></li></ul></li><li><a href=#cvxt>Convexity</a></li><li><a href=#mmt-suff-stat>Moments of sufficient statistic</a></li><li><a href=#sufficiency>Sufficiency</a><ul><li><a href=#sufficiency-and-the-exponential-family>Sufficiency and the Exponential Family</a></li><li><a href=#mle>MLE for Exponential Family</a></li></ul></li><li><a href=#mle-and-the-kl-divergence>MLE and the KL Divergence</a></li><li><a href=#conj-prior>Conjugate priors</a></li><li><a href=#glim>Generalized Linear Models</a><ul><li><a href=#mle-for-generalized-linear-models-with-canonical-response-function>MLE for Generalized Linear Models with canonical response function</a><ul><li><a href=#online-updating>Online updating</a></li><li><a href=#batch-updating>Batch updating</a></li></ul></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Exponential Family & Generalized Linear Models.</p></blockquote><h2 id=exp-fam>The Exponential Family<a hidden class=anchor aria-hidden=true href=#exp-fam>#</a></h2><p>The <strong>exponential family</strong> of distributions is defined as family of distributions of form
\begin{align}
p(x;\eta)&=h(x)\exp\Big[\eta^\text{T}T(x)-A(\eta)\Big],\label{eq:ef.1} \\ &= \frac{1}{Z(\eta)}h(x)\exp\Big[\eta^\text{T}T(x)\Big]
\end{align}
where</p><ul><li>$\eta$ is known as the <strong>natural parameter</strong>, or <strong>canonical parameter</strong>,</li><li>$T(X)$ is referred to as a <strong>sufficient statistic</strong>,</li><li>$A(\eta)$ is called the <strong>cumulant function</strong>, which can be view as the logarithm of a normalization factor (or <strong>partition function</strong>) $Z(\eta)$, i.e. $A(\eta)=\log Z(\eta)$, since integrating \eqref{eq:ef.1} w.r.t the measure $\nu$ gives us
\begin{equation}
A(\eta)=\log\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx),\label{eq:ef.2}
\end{equation}
This also implies that $A(\eta)$ will be determined once we have specified $\nu,T(x)$ and $h(x)$.</li></ul><p>The set of parameters $\eta$ for which the integral in \eqref{eq:ef.2} is finite is known as the <strong>natural parameter space</strong>
\begin{equation}
\mathcal{N}=\left\{\eta:\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx)&lt;\infty\right\}
\end{equation}
which explains why $\eta$ is also referred as <strong>natural parameter</strong>. If $\mathcal{N}$ is an non-empty open set, the exponential family is said to be a <strong>linear exponential family</strong>.</p><p>An exponential family is known as <strong>minimal</strong> if there are no linear constraints among the components of $\eta$ nor are there linear constraints among the components of $T(x)$. A linear exponential family in a minimal representation is referred as <strong>regular exponential family</strong>.</p><h2 id=examples>Examples<a hidden class=anchor aria-hidden=true href=#examples>#</a></h2><p>Each particular choice of $\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\eta$. As we vary $\eta$, we then get different distributions within this family.</p><h3 id=bern-dist>Bernoulli distribution<a hidden class=anchor aria-hidden=true href=#bern-dist>#</a></h3><p>The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\sim\text{Bern}(\pi)$, is given by
\begin{align}
p(x;\pi)&=\pi^x(1-\pi)^{1-x} \\ &=\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &=\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which can be written in the form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\frac{\pi}{1-\pi} \\ T(x)&=x \\ A(\eta)&=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&=1
\end{align}
Notice that the relationship between $\eta$ and $\pi$ is invertible since
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}},
\end{equation}
which is the <strong>sigmoid function</strong>.</p><h3 id=bin-dist>Binomial distribution<a hidden class=anchor aria-hidden=true href=#bin-dist>#</a></h3><p>The probability mass function of a Binomial random variable $X$, denoted as $X\sim\text{Bin}(N,\pi)$, is defined as
\begin{align}
p(x;N,\pi)&={N\choose x}\pi^{x}(1-\pi)^{1-x} \\ &={N\choose x}\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &={N\choose x}\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which is in form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\frac{\pi}{1-\pi} \\ T(x)&=x \\ A(\eta)&=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&={N\choose x}
\end{align}
Similar to the Bernoulli case, we also have the invertible relationship between $\eta$ and $\pi$ as
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}}
\end{equation}</p><h3 id=pois-dist>Poisson distribution<a hidden class=anchor aria-hidden=true href=#pois-dist>#</a></h3><p>The probability mass function of a Poisson random variable $X$, denoted as $X\sim\text{Pois}(\lambda)$, is given as
\begin{align}
p(x;\lambda)&=\frac{\lambda^x e^{-\lambda}}{x!} \\ &=\frac{1}{x!}\exp\left(x\log\lambda-\lambda\right),
\end{align}
which is also able to be written as an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&=\log\lambda \\ T(x)&=x \\ A(\eta)&=\lambda=e^{\eta} \\ h(x)&=\frac{1}{x!}
\end{align}
Analogy to Bernoulli distribution, we also have that
\begin{equation}
\lambda=e^{\eta}
\end{equation}</p><h3 id=gauss-dist>Gaussian distribution<a hidden class=anchor aria-hidden=true href=#gauss-dist>#</a></h3><p>The (univariate) Gaussian density of a random variable $X$, denoted as $X\sim\mathcal{N}(\mu,\sigma^2)$, is given by
\begin{align}
p(x;\mu,\sigma^2)&=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ &=\frac{1}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right],
\end{align}
which allows us to write it as an instance of the exponential family with
\begin{align}
\eta&=\left[\begin{matrix}\mu/\sigma^2 \\ -1/2\sigma^2\end{matrix}\right] \\ T(x)&=\left[\begin{matrix}x\\ x^2\end{matrix}\right] \\ A(\eta)&=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2) \\ h(x)&=\frac{1}{\sqrt{2\pi}}
\end{align}</p><h3 id=multinomial-distribution>Multinomial distribution<a hidden class=anchor aria-hidden=true href=#multinomial-distribution>#</a></h3><p>Let $\mathbf{X}=(X_1,\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\mathbf{\pi}=(\pi_1,\ldots,\pi_K)$ with $\sum_{k=1}^{K}\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.</p><p>Then $\mathbf{X}$ is said to have Multinomial distribution, denoted as $\mathbf{X}\sim\text{Mult}_K(N,\boldsymbol{\pi})$, if its probability mass function is given as with $\sum_{k=1}^{K}x_k=1$
\begin{align}
p(\mathbf{x};\boldsymbol{\pi},N,K)&=\frac{N!}{x_1!x_2!\ldots x_K!}\pi_1^{x_1}\pi_2^{x_2}\ldots\pi_n^{x_n} \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right)\label{eq:m.1}
\end{align}
It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\mathbf{x})$, which is
\begin{equation}
\sum_{k=1}^{K}x_k=1
\end{equation}
In order to remove this constraint, we substitute $1-\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \eqref{eq:m.1} be written by
\begin{align}
\hspace{-0.8cm}p(\mathbf{x};\boldsymbol{\pi},N,K)&=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right) \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{k=1}^{K-1}x_k\log\pi_k+\left(1-\sum_{k=1}^{K-1}x_k\right)\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right] \\ &=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{i=1}^{K-1}\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)x_i+\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right]\label{eq:m.2}
\end{align}
With this representation, and also for convenience, for $i=1,\ldots,K$ we continue by letting
\begin{equation}
\eta_i=\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)=\log\left(\frac{\pi_i}{\pi_K}\right)\label{eq:m.3}
\end{equation}
Take the exponential of both sides and summing over $K$, we have
\begin{equation}
\sum_{i=1}^{K}e^{\eta_i}=\frac{\sum_{i=1}^{K}\pi_i}{\pi_K}=\frac{1}{\pi_K}\label{eq:m.4}
\end{equation}
From this result, we have that the Multinomial distribution \eqref{eq:m.2} is therefore also a member of the exponential family with
\begin{align}
\eta&=\left[\begin{matrix}\log\left(\pi_1/\pi_K\right) \\ \vdots \\ \log\left(\pi_K/\pi_K\right)\end{matrix}\right] \\ T(\mathbf{x})&=\left[\begin{matrix}x_1,\ldots,x_K\end{matrix}\right]^\text{T} \\ A(\eta)&=-\log\left(1-\sum_{i=1}^{K-1}\pi_i\right)=-\log(\pi_K)=\log\left(\sum_{k=1}^{K}e^{\eta_k}\right) \\ h(\mathbf{x})&=\frac{N!}{x_1!x_2!\ldots x_K!}
\end{align}
Additionally, substituting the result \eqref{eq:m.4} into \eqref{eq:m.3} gives us for $i=1,\ldots,K$
\begin{equation}
\eta_i=\log\left(\pi_i\sum_{k=1}^{K}e^{\eta_k}\right),
\end{equation}
or we can express $\boldsymbol{\pi}$ in terms of $\eta$ by
\begin{equation}
\pi_i=\frac{e^{\eta_i}}{\sum_{k=1}^{K}e^{\eta_k}},
\end{equation}
which is the <strong>softmax function</strong>.</p><h3 id=mvn-dist>Multivariate Normal distribution<a hidden class=anchor aria-hidden=true href=#mvn-dist>#</a></h3><p>For the case of a multivariate Normal r.v $\mathbf{X}$, we have its PDF is given as
\begin{align}
p(\mathbf{x};\boldsymbol{\mu},\boldsymbol{\Sigma},K)&=\frac{1}{(2\pi)^{K/2}\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\
\end{align}</p><h2 id=cvxt>Convexity<a hidden class=anchor aria-hidden=true href=#cvxt>#</a></h2><p><b id=theorem1>Theorem 1</b>: <em>The natural space $\mathcal{N}$ is a convex set and the cumulant function $A(\eta)$ is a convex function. If the family is minimal, then $A(\eta)$ is strictly convex.</em></p><p><strong>Proof</strong><br>Let $\eta_1,\eta_2\in\mathcal{N}$, thus from \eqref{eq:ef.2}, we have that
\begin{align}
\exp\big(A(\eta_1)\big)&=A_1, \\ \exp\big(A(\eta_2)\big)&=A_2
\end{align}
where $A_1,A_2$ are finite.</p><p>To prove that $\mathcal{N}$ is convex, we need to show that for any $\eta=\lambda\eta_1+(1-\lambda)\eta_2$ for $0\lt\lambda\lt 1$, we also have $\eta\in\mathcal{N}$. From \eqref{eq:ef.2}, and by <strong>Hölder&rsquo;s inequality</strong><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, we have
\begin{align}
\exp\big(A(\eta)\big)&=\int h(x)\exp\big(\eta^\text{T}T(x)\big)\nu(dx) \\ &=\int h(x)\exp\Big[\big(\lambda\eta_1+(1-\lambda)\eta_2\big)^\text{T}T(x)\Big]\nu(dx) \\ &=\int \Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda}\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}\nu(dx) \\ &\leq\Bigg[\int h(x)\exp\big(\eta_1^\text{T}T(x)\big)\nu(dx)\Bigg]^\lambda\Bigg[\int h(x)\exp\big(\eta_2^\text{T}T(x)\big)\nu(dx)\Bigg]^{1-\lambda} \\ &=\Big[\exp\big(A(\eta_1)\big)\Big]^\lambda\Big[\exp\big(A(\eta_2)\big)\Big]^{1-\lambda} \\ &=A_1^\lambda A_2^{1-\lambda},\label{eq:c.1}
\end{align}
which proves that $A(\eta)$ is finite, or $\eta\in\mathcal{N}$.</p><p>Moreover, taking logarithm of both sides of \eqref{eq:c.1} gives us
\begin{equation}
\lambda A(\eta_1)+(1-\lambda)A(\eta_2)\geq A(\eta)=A\big(\lambda\eta_1+(1-\lambda)\eta_2\big),
\end{equation}
which also claims the convexity of $A(\eta)$.</p><p>Additionally, by Hölder&rsquo;s inequality, the equality in \eqref{eq:c.1} holds when
\begin{equation}
\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}=c\Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda(1/\lambda-1)}
\end{equation}
or
\begin{equation}
\exp\big(\eta_2^\text{T}T(x)\big)=c\exp\big(\eta_1^\text{T}T(x)\big),
\end{equation}
and therefore
\begin{equation}
(\eta_2-\eta_1)^\text{T}T(x)=\log c,
\end{equation}
which is not minimal since $\eta_1,\eta_2$ are taken arbitrarily.</p><h2 id=mmt-suff-stat>Moments of sufficient statistic<a hidden class=anchor aria-hidden=true href=#mmt-suff-stat>#</a></h2><p>In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second <strong>cumulants</strong>.</p><p>Let us first consider the first derivative of the cumulant function $A(\eta)$. By the <strong>dominated convergence theorem</strong>, we have
\begin{align}
\frac{\partial A(\eta)}{\partial\eta^\text{T}}&=\frac{\partial}{\partial\eta^\text{T}}\log\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx) \\ &=\frac{\int T(x)\exp\big(\eta^\text{T}(x)\big)h(x)\nu(dx)}{\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx)} \\ &=\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx)\label{eq:mv.1} \\ &=\int T(x)p(x;\eta)\nu(dx) \\ &=\mathbb{E}[T(X)],\label{eq:mv.2}
\end{align}
which is the mean of the sufficient statistic $T(X)$.</p><p>Moreover, taking the second derivative of cumulant function by continuing with the result \eqref{eq:mv.1}, we have
\begin{align}
\frac{\partial^2 A(\eta)}{\partial\eta\partial\eta^\text{T}}&=\frac{\partial}{\partial\eta^\text{T}}\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\int T(x)\left(T(x)-\frac{\partial}{\partial\eta^\text{T}}A(\eta)\right)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\int T(x)\big(T(x)-E(T(X))\big)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &=\mathbb{E}\left[T(X)T(X)^\text{T}\right]-\mathbb{E}[T(X)]\mathbb{E}[T(X)]^\text{T} \\ &=\text{Var}[T(X)],
\end{align}
which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$, or the second central moment of of $T(X)$.</p><p>Accordingly, we have that the differentiating the cumulant function $A(\eta)$ to $n$-order gives us the $n$-th central moment of $T(X)$.</p><p>The expected value of $T(X)$, $\mu$, is also known as the <b id=mean-parameter>mean parameter</b> (or <strong>moment parameter</strong>). Additionally, since $\mu=\frac{\partial A(\eta)}{\partial\eta}$ and $A(\eta)$ is strictly convex (due to <a href=#theorem1>Theorem 1</a>), then the relationship $\mu=\frac{\partial A(\eta)}{\partial\eta}$ is invertible, i.e. $\eta=\left(\frac{\partial A(\eta)}{\partial\eta}\right)^{-1}(\mu)$. More specifically, there exists a function $\psi$ which defines the one-to-one relationship between canonical parameter $\eta$ and moment parameter $\mu$, i.e.
\begin{equation}
\eta=\psi(\mu)\label{eq:mv.3}
\end{equation}
This implies that a distribution in the exponential family can be parameterized not only by the canonical parameter $\eta$, but also the mean parameter $\mu$.</p><h2 id=sufficiency>Sufficiency<a hidden class=anchor aria-hidden=true href=#sufficiency>#</a></h2><p>Let $X$ be a r.v and let $T(X)$ be a statistic. Suppose that the distribution of $X$ is parameterized by $\theta$, i.e. $p(x;\theta)$. Then $T(X)$ is said to be <strong>sufficient</strong> for $\theta$ if there is no information in $X$ regarding $\theta$ beyond that in $T(X)$. In other words, having observed $T(X)$, we can throw away $X$ for the purpose of inference w.r.t $\theta$. Specifically</p><ul id=number-list><li>In Bayesian approach, $\theta$ is considered as a r.v. Thus, we say that $T(X)$ is sufficient for $\theta$ if
\begin{equation}
p\models(\theta\perp X\vert T(X)),
\end{equation}
which happens iff
\begin{equation}
p(\theta\vert T(x),x)=p(\theta\vert T(x))
\end{equation}</li><li>In frequentist view, $\theta$ is considered as label rather than a r.v. Thus, $T(X)$ is sufficient for $\theta$ if the conditional distribution of $X$ given $T(X)$ is not a function of $\theta$, i.e.
\begin{equation}
p(x\vert T(x),\theta)=p(x\vert T(x))
\end{equation}</li><li>For undirected models, the joint distribution can be described by a product of factors
\begin{equation}
p(x,T(x),\theta)=\psi_1(T(x),\theta)\psi_2(x,T(x)),
\end{equation}
where we have absorbed the partition function $Z$ in one of the potential functions. Moreover, since now $T(X)$ is a deterministic function of $X$, then $T(X)$ can be removed out from the LHS. Dividing both sides by $p(\theta)$ yields
\begin{equation}
p(x\vert\theta)=g(T(x),\theta)h(x,T(x))\label{eq:suf.1}
\end{equation}</li></ul><h3 id=sufficiency-and-the-exponential-family>Sufficiency and the Exponential Family<a hidden class=anchor aria-hidden=true href=#sufficiency-and-the-exponential-family>#</a></h3><p>An important feature of the exponential family is that its sufficient statistic can be obtained by simply observed, once the distribution is written in the form \eqref{eq:ef.1}. Recall that
\begin{equation}
p(x;\eta)=h(x)\exp\left[\eta^\text{T}T(x)-A(\eta)\right]
\end{equation}
From \eqref{eq:suf.1}, it follows immediately that $T(X)$ is a sufficient statistic for $\eta$.</p><h3 id=mle>MLE for Exponential Family<a hidden class=anchor aria-hidden=true href=#mle>#</a></h3><p>The reduction obtainable by using a sufficient statistic $T(X)$ is particularly notable in the case of i.i.d sampling.</p><p>Consider an i.i.d data set $\mathcal{D}=\{x_1,\ldots,x_N\}$, which is composed of $N$ independent r.v.s $X=(X_1,\ldots,X_N)$, characterized by the sample exponential family density. The likelihood function is then given by:
\begin{align}
L(\eta;\mathcal{D})=p(\mathbf{x}\vert\eta)&=\prod_{n=1}^{N}p(x_n\vert\eta) \\ &=\prod_{n=1}^{N}h(x_n)\exp\big[\eta^\text{T}T(x_n)-A(\eta)\big] \\ &=\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right]\label{eq:mle.1}
\end{align}
It is easily seen that $X$ is also an exponential distribution, with sufficient statistic $\sum_{n=1}^{N}T(x_n)$.</p><p>Taking the logarithm of both sides gives us the log likelihood as
\begin{equation}
\ell(\eta)=\log L(\eta)=\sum_{n=1}^{N}\log h(x_n)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\label{eq:mle.2}
\end{equation}
Consider the gradient of the log likelihood w.r.t $\eta$, we have
\begin{align}
\nabla_\eta\ell(\eta)&=\nabla_\eta\left[\sum_{n=1}^{N}\log h(x_n)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &=\sum_{n=1}^{N}T(x_n)-N\nabla_\eta A(\eta)
\end{align}
Setting the gradient to zero, we have the value of $\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\eta$, denoted as $\hat{\eta}_\text{ML}$ satisfies
\begin{equation}
\nabla_{\eta}A(\hat{\eta}_\text{ML})=\frac{1}{N}\sum_{n=1}^{N}T(x_n)
\end{equation}
Finally, defining $\mu\doteq\mathbb{E}\big[T(X)\big]$. Then by \eqref{eq:mv.2}, we have that
\begin{equation}
\hat{\mu}_\text{ML}=\frac{1}{N}\sum_{n=1}^{N}T(x_n)\label{eq:mle.3}
\end{equation}
as the general formula for MLE of the <a href=#mean-parameter>mean parameter</a> in the exponential family. And thus, by \eqref{eq:mv.3}, we obtain
\begin{equation}
\hat{\eta}_\text{ML}=\psi(\hat{\mu}_\text{ML})
\end{equation}
It is worth noticing that the above formula involves the data only via the sufficient statistic $\sum_{n=1}^{N}T(x_n)$.</p><p><strong>Example 1</strong>: From the result \eqref{eq:mle.3}, we have that</p><ul id=number-list><li>For exponential family distribution with sufficient statistic $T(X)=X$, e.g. <a href=#bern-dist>Bernoulli</a>, <a href=#bin-dist>Binomial</a>, <a href=#pois-dist>Poisson</a>, the maximum likelihood estimate of the mean is exactly the sample mean, $\frac{1}{N}\sum_{n=1}^{N}x_n$.</li><li>For <a href=#gauss-dist>univariate Normal distribution</a>, which has $T(X)=\left[\begin{smallmatrix}X \\ X^2\end{smallmatrix}\right]$, the maximum likelihood estimate of the mean and variance are precisely the sample mean and sample variance.</li></ul><h2 id=mle-and-the-kl-divergence>MLE and the KL Divergence<a hidden class=anchor aria-hidden=true href=#mle-and-the-kl-divergence>#</a></h2><p>Given a dataset $\mathcal{D}=\{x_1,\ldots,x_N\}$, the <strong>empirical distribution</strong>, denoted $\hat{p}(x)$, is a distribution which places probability mass $1/N$ at each data point $x_n$ in $\mathcal{D}$. In particular,
\begin{equation}
\hat{p}(x)\doteq\frac{1}{N}\sum_{n=1}^{N}\delta(x-x_n),
\end{equation}
where</p><ul id=number-list><li>in the continuous case, $\delta(\cdot)$ is the <b>Dirac delta</b> function, which has zero-valued everywhere except $0$, and integrates to $1$.</li><li>in the discrete case, $\delta(\cdot)$ is the <b>Kronecker delta</b> function, which takes value of $1$ at $0$, and zero elsewhere, and thus sums to $1$.</li></ul><p>The following derivations is interchangeable between discrete and continuous circumstance by swapping between summation and integration. Thus, without loss of generality, let us consider the discrete case. Firstly, we have that
\begin{align}
\sum_x\hat{p}(x)\log p(x;\theta)&=\sum_x\frac{1}{N}\sum_{n=1}^{N}\delta(x-x_n)\log p(x;\theta) \\ &=\frac{1}{N}\sum_{n=1}^{N}\sum_x\delta(x-x_n)\log p(x;\theta) \\ &=\frac{1}{N}\sum_{n=1}^{N}\log p(x;\theta) \\ &=\frac{1}{N}\ell(\theta;\mathcal{D})
\end{align}
where $\ell(\cdot)$ is the log-likelihood function.</p><p>On the other hand, consider the <strong>relative entropy</strong>, or <strong>KL divergence</strong>, between the empirical distribution and the model $p(x;\theta)$, we have that
\begin{align}
D_\text{KL}(\hat{p}(x)\Vert p(x;\theta))&=\sum_x\hat{p}(x)\log\left(\frac{\hat{p}(x)}{p(x;\theta)}\right) \\ &=\sum_x\hat{p}(x)\log\hat{p}(x)-\sum_x\hat{p}(x)\log p(x;\theta) \\ &=\mathbb{E}_{\hat{p}}\big[\log\hat{p}(x)\big]-\frac{1}{N}\ell(\theta;\mathcal{D})
\end{align}
Since $\mathbb{E}_\hat{p}\big[\log\hat{p}(x)\big]$ does not depend on $\theta$, then the value of $\theta$ that minimizes the KL divergence to the empirical distribution is the one that maximizes the log-likelihood.</p><h2 id=conj-prior>Conjugate priors<a hidden class=anchor aria-hidden=true href=#conj-prior>#</a></h2><p>Given a probability distribution $p(x\vert\eta)$, its prior $p(\eta)$ is said to be <strong>conjugate</strong> to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as <strong>conjugate prior</strong>.</p><p>For any member of the exponential family, there exists a conjugate prior that can be written in form
\begin{equation}
p(\eta\vert\mathcal{X},\theta)=f(\mathcal{X},\theta)\exp(\eta^\text{T}\mathcal{X}-\theta A(\eta)),\label{eq:cp.1}
\end{equation}
where $\theta>0$ and $\mathcal{X}$ are hyperparameters.</p><p>By Bayes&rsquo; rule, and with the likelihood function as given in \eqref{eq:mle.1}, the posterior distribution can be computed as
\begin{align}
&\hspace{0.7cm}p(\eta\vert\mathbf{X},\mathcal{X},\theta) \\ &\propto p(\eta\vert\mathcal{X},\theta)p(\mathbf{X}\vert\eta) \\ &=f(\mathcal{X},\theta)\exp\big(\eta^\text{T}\mathcal{X}-\theta A(\eta)\big)\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &\propto\exp\left[\eta^\text{T}\left(\mathcal{X}+\sum_{n=1}^{N}T(x_n)\right)-(\theta+N)A(\eta)\right],
\end{align}
which is in the same form as \eqref{eq:cp.1} and therefore claims the conjugacy.</p><h2 id=glim>Generalized Linear Models<a hidden class=anchor aria-hidden=true href=#glim>#</a></h2><p>The <strong>generalized linear model</strong> (or <strong>GLIM</strong>) extends the idea of linear models in classification and regression to a more general settings using the definition of exponential family. Specifically, consider the plate model given below, which consists of two variables $X$ and $Y$ where both of them are assumed to be observed.</p><figure><img width=20% height=20% src=/images/exponential-family-glim/glim.png alt=GLIM><figcaption><b>Figure 2</b>: (based on figure from <a href=#jordan-book>Jordan's book</a>) <b>The graphical model representation of a generalized linear model.</b></figcaption></figure><p>A GLIM makes threes assumptions about the conditional distribution $p(y\vert x)$</p><ul id=roman-list><li>The observed input $x$ is assumed to enter into the model via a linear combination $\xi=\theta^\text{T}x$.</li><li>The conditional mean $\mu$ is represented as a function $f(\xi)$ of the linear combination $\xi$, where $f$ is referred as the <b>response function</b>, or <b>link function</b>.</li><li>The observed output $y$ is assumed to be characterized by an exponential family distribution $p$ with conditional mean $\mu$.</li></ul><figure><img width=50% height=50% src=/images/exponential-family-glim/relationships-in-glim.png alt="GLIM relationships"><figcaption><b>Figure 3</b>: (based on figure from <a href=#jordan-book>Jordan's book</a>) <b>The relationships between the variables in a GLIM.</b></figcaption></figure><p>These assumption is summarized in the above figure. Notice that the diagram also provides us an invertible mapping from the mean parameter $\mu$ to the canonical parameter $\eta$, which we denote as $\eta=\psi(\mu)$. This allows us to use $\eta$ to represent the exponential family distribution for $Y$.</p><p>Formally, we assume that the output of GLIM has the form of
\begin{equation}
p(y;\eta,\phi)=h(y,\phi)\exp\left[\frac{\eta^\text{T}y-A(\eta)}{\phi}\right],
\end{equation}
which is slightly different from the traditional definition of exponential family density by including an explicit <strong>scale parameter</strong> $\phi$.</p><p>Additionally, since $\eta=\psi(\mu)$ and $\mu=f(\xi)=f(\theta^\text{T}x)$, then we directly have $\eta=\psi(f(\theta^\text{T}x))$. Thus, the conditional of $y$ given $x,\theta$ and $\phi$ is
\begin{equation}
p(y\vert x;\theta,\phi)=h(y,\phi)\exp\left[\frac{y^\text{T}\psi(f(\theta^\text{T}x))-A(\psi(\theta^\text{T}x))}{\phi}\right]\label{eq:glim.1}
\end{equation}
There are two principle choices in designing a GLIM: the choice of exponential family distribution and the choice of the response function $f(\cdot)$.</p><p>The former choice strongly depends on the pattern of $Y$. In particular, class labels are naturally represented by Bernoulli or Multinomial, counts by the Poisson, integrals by the exponential or Gamma distributions.</p><p>The latter choice has more degree of freedom with some mild constraints, e.g. in the case of Bernoulli and Multinomial, the conditional expectation must lie between $0$ and $1$, which suggests us to use a response function $f$ whose range is $(0,1)$; while we choose the response function $f$ with range $(0,\infty)$ for Gamma distribution, where the r.v is nonnegative. There is a particular choice called the <strong>canonical response function</strong>, which is $f(\cdot)=\psi^{-1}(\cdot)$. In this case, the conditional probability in \eqref{eq:glim.1} is simplified to
\begin{equation}
p(y\vert x;\theta,\phi)=h(y,\phi)\exp\left[\frac{y^\text{T}(\theta^\text{T}x)-A(\theta^\text{T}x)}{\phi}\right]
\end{equation}</p><h3 id=mle-for-generalized-linear-models-with-canonical-response-function>MLE for Generalized Linear Models with canonical response function<a hidden class=anchor aria-hidden=true href=#mle-for-generalized-linear-models-with-canonical-response-function>#</a></h3><p>Consider an i.i.d dataset $\mathcal{D}=\{(x_1,y_1),\ldots,(x_N,y_N)\}$ where $y_n$ are scalars. The log-likelihood is then given as
\begin{align}
\ell(\theta;\mathcal{D})&=\log\prod_{n=1}^{N}p(y_n\vert x_n;\theta) \\ &=\sum_{n=1}^{N}\log\Big[h(y)\exp\big[\eta_n y_n-A(\eta_n)\big]\Big] \\ &=\sum_{n=1}^{N}\log h(y)+\sum_{n=1}^{N}\eta_n y_n-\sum_{n=1}^{N}A(\eta_n)\label{eq:mle-glm.1}
\end{align}
With canonical response function, which yields $\eta_n=\theta^\text{T}x_n$ for $n=1,\ldots,N$, the log-likelihood can be rewritten as
\begin{align}
p(y\vert x;\theta)&=\sum_{n=1}^{N}\log h(y)+\sum_{n=1}^{N}\theta^\text{T}y_n x_n-\sum_{n=1}^{N}A(\eta_n) \\ &=\sum_{n=1}^{N}\log h(y)+\theta^\text{T}\sum_{n=1}^{N}y_n x_n-\sum_{n=1}^{N}A(\eta_n)
\end{align}
Also, it is worth noticing that from \eqref{eq:mle.2}, we can see that $\sum_{n=1}^{N}x_n y_n$ is the sufficient statistic for $\theta$.</p><p>We continue by taking the derivative of the log-likelihood in \eqref{eq:mle-glm.1} w.r.t $\theta$ instead to get a more general form
\begin{align}
\nabla_\theta\ell(\theta;\mathcal{D})&=\sum_{n=1}^{N}\frac{d\ell(\theta;\mathcal{D})}{d\eta_n}\nabla_\theta\eta_n \\ &=\sum_{n=1}^{N}\left(y_n-\frac{d A(\eta_n)}{d\eta_n}\right)\frac{d\eta_n}{d\mu_n}\frac{d\mu_n}{d\xi_n}\nabla_\theta\xi_n \\ &=\sum_{n=1}^{N}(y_n-\mu_n)\frac{d\eta_n}{d\mu_n}\frac{d\mu_n}{d\xi_n}x_n\label{eq:mle-glm.2}
\end{align}
In using canonical response function, we have that $f=\psi^{-1}$, thus $\eta_n=\xi_n$, which implies that the derivative of the log-likelihood w.r.t can be simplified as
\begin{equation}
\nabla_\theta\ell(\theta;\mathcal{D})=\sum_{n=1}^{N}(y_n-\mu_n)x_n\label{eq:mle-glm.3}
\end{equation}</p><h4 id=online-updating>Online updating<a hidden class=anchor aria-hidden=true href=#online-updating>#</a></h4><p>We then can use gradient ascent for estimating the parameter $\theta$, which has the update rule:
\begin{equation}
\theta^{(t+1)}=\theta^{(t)}+\rho(y_n-\mu_n^{(t)})x_n,\label{eq:mle-glm.4}
\end{equation}
where $\mu_n^{(t)}=f({\theta^{(t)}}^\text{T}x_n)$ and $\rho$ is the step size.</p><p>Notice that if our choice of $f$ is not the canonical response, \eqref{eq:mle-glm.4} is also a generic SGD algorithm for models throughout the GLIMs due to the fact that the derivatives of $f(\cdot)$ and $\psi(\cdot)$ in \eqref{eq:mle-glm.2}, i.e. $\frac{d\eta_n}{d\mu_n}$ and $\frac{d\mu_n}{d\xi_n}$, are absorbed into the step size $\rho$.</p><h4 id=batch-updating>Batch updating<a hidden class=anchor aria-hidden=true href=#batch-updating>#</a></h4><p>To using a batch algorithm, we start by vectorizing the gradient of the log-likelihood in \eqref{eq:mle-glm.3}, as
\begin{align}
\nabla_\theta\ell(\theta;\mathcal{D})&=\sum_{n=1}^{N}(y_n-\mu_n)x_n \\ &=X^\text{T}(y-\mu),
\end{align}
where $X$ is a matrix whose rows are $x_n^\text{T}$, and where $y,\mu$ are vectors whose components are $y_n$ and $\mu_n$ respectively, i.e.
\begin{equation}
\mathbf{X}=\left[\begin{matrix}-\hspace{0.1cm}x_1^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}x_N^\text{T}\hspace{0.1cm}-\end{matrix}\right],\hspace{1cm}y=\left[\begin{matrix}y_1 \\ \vdots \\ y_N\end{matrix}\right],\hspace{1cm}\mu=\left[\begin{matrix}\mu_1 \\ \vdots \\ \mu_N\end{matrix}\right]
\end{equation}
Additionally, let us consider the Hessian matrix by taking the second derivative of the log-likelihood
\begin{align}
H_\ell&=\frac{\partial^2}{\partial\theta\partial\theta^\text{T}}\ell(\theta;\mathcal{D}) \\ &=\frac{\partial}{\partial\theta^\text{T}}\sum_{n=1}^{N}x_n(y_n-\mu_n) \\ &=-\sum_{n=1}^{N}x_n\frac{d\mu_n}{d\eta_n}\frac{\partial\eta_n}{\partial\theta^\text{T}} \\ &=\sum_{n=1}^{N}x_n\frac{d\mu_n}{d\eta_n}x_n^\text{T} \\ &=-X^\text{T}WX,
\end{align}
where $W$ is the diagonal weight matrix
\begin{equation}
W=\text{diag}\left(\frac{d\mu_1}{d\eta_1},\ldots,\frac{d\mu_N}{d\eta_N}\right),
\end{equation}
whose each diagonal entry can be computed via the second derivative of $A(\eta_n)$.</p><p>Using Newton&rsquo;s method, we have the update rule
\begin{align}
\theta^{(t+1)}&=\theta^{(t)}-H_\ell^{-1}\nabla_\theta\ell \\ &=-H_\ell^{-1}(-H_\ell\theta^{(t)}+\nabla_\theta) \\ &=(X^\text{T}W^{(t)}X)^{-1}\big[X^\text{T}W^{(t)}X\theta^{(t)}+X^\text{T}(y-\mu^{(t)})\big] \\ &=(X^\text{T}W^{(t)}X)^{-1}X^\text{T}W^{(t)}z^{(t)},\label{eq:mle-glm.5}
\end{align}
where we have defined the <strong>adjusted response</strong>
\begin{equation}
z^{(t)}\doteq\eta+\big(W^{(t)}\big)^{-1}(y-\mu^{(t)})
\end{equation}
This can be understood as solving the <strong>Iteratively Reweighted Least Squares</strong> (<strong>IRLS</strong>) problem
\begin{equation}
\theta^{(t)}=\underset{\theta}{\text{argmin}}(x-X\theta)^\text{T}\theta(z-X\theta)
\end{equation}</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=jordan-book>M. Jordan. <a href=https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf>The Exponential Family: Basics</a>. 2009</span>.</p><p>[2] Joseph K. Blitzstein & Jessica Hwang. <a href=https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573>Introduction to Probability</a>.</p><p>[3] Weisstein, Eric W. <a href=https://mathworld.wolfram.com/HoeldersInequalities.html>Hölder&rsquo;s Inequalities</a> From MathWorld&ndash;A Wolfram Web Resource.</p><p>[4] Ian Goodfellow & Yoshua Bengio & Aaron Courville. <a href=https://www.deeplearningbook.org>Deep Learning</a>. MIT Press, 2016.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Let $p,q>1$ such that
\begin{equation*}
\frac{1}{p}+\frac{1}{q}=1
\end{equation*}
The <strong>Hölder&rsquo;s inequality</strong> for integrals states that
\begin{equation*}
\int_a^b\vert f(x)g(x)\vert\hspace{0.1cm}dx\leq\left(\int_a^b\vert f(x)\vert\hspace{0.1cm}dx\right)^{1/p}\left(\int_a^b\vert g(x)\vert\hspace{0.1cm}dx\right)^{1/q}
\end{equation*}
The equality holds with
\begin{equation*}
\vert g(x)\vert=c\vert f(x)\vert^{p-1}
\end{equation*}&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/machine-learning/>machine-learning</a></li><li><a href=https://trunghng.github.io/tags/exponential-family/>exponential-family</a></li><li><a href=https://trunghng.github.io/tags/generalized-linear-model/>generalized-linear-model</a></li><li><a href=https://trunghng.github.io/tags/probabilistic-graphical-model/>probabilistic-graphical-model</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/><span class=title>« Prev</span><br><span>Policy Gradient Theorem</span>
</a><a class=next href=https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/><span class=title>Next »</span><br><span>Eligible Traces</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on x" href="https://x.com/intent/tweet/?text=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f&amp;hashtags=machine-learning%2cexponential-family%2cgeneralized-linear-model%2cprobabilistic-graphical-model"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f&amp;title=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models&amp;summary=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f&title=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on whatsapp" href="https://api.whatsapp.com/send?text=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on telegram" href="https://telegram.me/share/url?text=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share The Exponential Family, Generalized Linear Models on ycombinator" href="https://news.ycombinator.com/submitlink?t=The%20Exponential%20Family%2c%20Generalized%20Linear%20Models&u=https%3a%2f%2ftrunghng.github.io%2fposts%2fmachine-learning%2fexponential-family-glim%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>