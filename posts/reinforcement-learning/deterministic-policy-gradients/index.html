<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deterministic Policy Gradients | Trung's Place</title><meta name=keywords content="reinforcement-learning,policy-gradient,actor-critic,my-rl"><meta name=description content="
Notes on Deterministic Policy Gradient Algorithms
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Deterministic Policy Gradients"><meta property="og:description" content="
Notes on Deterministic Policy Gradient Algorithms
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-02T19:26:44+07:00"><meta property="article:modified_time" content="2022-12-02T19:26:44+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Deterministic Policy Gradients"><meta name=twitter:description content="
Notes on Deterministic Policy Gradient Algorithms
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Deterministic Policy Gradients","item":"https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deterministic Policy Gradients","name":"Deterministic Policy Gradients","description":" Notes on Deterministic Policy Gradient Algorithms\n","keywords":["reinforcement-learning","policy-gradient","actor-critic","my-rl"],"articleBody":" Notes on Deterministic Policy Gradient Algorithms\nPreliminaries Consider a (infinite-horizon) Markov Decision Process (MDP), defined as a tuple of $(\\mathcal{S},\\mathcal{A},p,r,\\rho_0,\\gamma)$ where\n$\\mathcal{S}$ is the state space. $\\mathcal{A}$ is the action space. $p:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ is the transition probability distribution, i.e. $p(s,a,s’)=p(s’\\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function, and let us denote $r_{t+1}\\doteq r(S_t,A_t)$. $\\rho_0:\\mathcal{S}\\to\\mathbb{R}$ is the distribution of the initial state $s_0$. $\\gamma\\in(0,1)$ is the discount factor. Within an MPD, a policy parameterized by a vector $\\theta\\in\\mathbb{R}^n$ can be given as\nStochastic policy. $\\pi_\\theta:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$, or Deterministic policy. $\\mu_\\theta:\\mathcal{S}\\to\\mathcal{A}$. (Stochastic) Policy Gradient Theorem We continue with an assumption that the action space $\\mathcal{A}=\\mathbb{R}^m$ and the state space $\\mathcal{S}\\subset\\mathbb{R}^d$ and $\\mathcal{S}$ is compact.\nStart-state formulation Recall that in the stochastic case, $\\pi_\\theta$, the Policy Gradient Theorem states that1 \\begin{align} \\nabla_\\theta J(\\pi_\\theta)\u0026=\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\hspace{0.1cm}da\\hspace{0.1cm}ds\\label{eq:spgt.1} \\\\ \u0026=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big]\\label{eq:spgt.2} \\end{align} where\n$J(\\pi_\\theta)$ is the performance objective function, which we are trying to maximize. It is defined as the expected cumulative discounted reward from the start state \\begin{align} J(\\pi_\\theta)\u0026\\doteq\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\big[r_0^\\gamma\\big]\\label{eq:spgt.6} \\\\ \u0026=\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1}\\right] \\\\ \u0026=\\mathbb{E}_{\\rho_0,\\pi_\\theta}\\big[G_0\\big] \\\\ \u0026=\\mathbb{E}_{s_0\\sim\\rho_0}\\Big[\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s_0\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\pi(s_0)\\big],\\label{eq:spgt.3} \\end{align} where $r_t^\\gamma$ is defined as the total discounted reward from time-step $t$ onward, which is thus the return at that step \\begin{equation} G_t=r_t^\\gamma\\doteq\\sum_{t=0}^{\\infty}\\gamma^t r_{t+1} \\end{equation} The function $\\rho_\\pi(s)$ is defined as the discounted weighting of states encountered when starting at $s_0$ and following $\\pi_\\theta$ thereafter \\begin{align} \\rho_\\pi(s)\u0026\\doteq\\sum_{t=0}^{\\infty}\\gamma^t P(S_t=s\\vert s_0,\\pi_\\theta) \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})\\left(\\sum_{t=0}^{\\infty}\\gamma^t P(S_t=s\\vert\\pi_\\theta)\\right)d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\sum_{t=0}^{\\infty}\\gamma^t\\rho_0(\\bar{s})p(\\bar{s}\\to s,t,\\pi_\\theta)d\\bar{s},\\label{eq:spgt.4} \\end{align} where $p(\\bar{s}\\to s,t,\\pi_\\theta)$ is defined as the the probability of transitioning to $s$ after $t$ steps starting from $\\bar{s}$ under $\\pi_\\theta$, which implies that \\begin{equation} p(\\bar{s}\\to s,t,\\pi_\\theta)=P(S_t=s\\vert\\pi_\\theta) \\end{equation} In fact, $\\rho_\\pi(s)$ can be seen as a density function since integrating $\\rho_\\pi$ over state space $\\mathcal{S}$ gives us \\begin{align} \\int_\\mathcal{S}\\rho_\\pi(s)d s\u0026=\\int_{s\\in\\mathcal{S}}\\int_{\\bar{s}\\in\\mathcal{S}}\\sum_{t=0}^{\\infty}\\gamma^t\\rho_0(\\bar{s})p(\\bar{s}\\to s,t,\\pi_\\theta)d\\bar{s}\\hspace{0.1cm}d s \\\\ \u0026=\\int_{\\bar{s}\\in\\mathcal{S}}\\rho_0(\\bar{s})\\int_{s\\in\\mathcal{S}}\\sum_{t=0}^{\\infty}\\gamma^t p(\\bar{s}\\to s,t,\\pi_\\theta)d s\\hspace{0.1cm}d\\bar{s} \\\\ \u0026=\\int_{\\bar{s}\\in\\mathcal{S}}\\rho_0(\\bar{s})\\sum_{t=0}^{\\infty}\\gamma^t\\underbrace{\\int_{s\\in\\mathcal{S}}p(\\bar{s}\\to s,t,\\pi_\\theta)d s}_{=1}d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})\\underbrace{\\sum_{t=0}^{\\infty}\\gamma^t}_{=1}d\\bar{s} \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(\\bar{s})d\\bar{s} \\\\ \u0026=1 \\end{align} Thus, this definition of $\\rho_\\pi$ lets us write \\eqref{eq:spgt.1} as an expectation, combined with using the log-likelihood trick, we end up with \\eqref{eq:spgt.2}. Proof\nThe definition of $J(\\pi_\\theta)$ given in \\eqref{eq:spgt.3} suggests us begin by considering the gradient of the state value function w.r.t $\\theta$. For any $s\\in\\mathcal{S}$, we have \\begin{align} \\hspace{-1cm}\\nabla_\\theta V_\\pi(s)\u0026=\\nabla_\\theta\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da \\\\ \u0026=\\underbrace{\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da}_{\\psi(s)}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)da \\\\ \u0026=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\left(r(s,a)+\\int_\\mathcal{S}\\gamma p(s’\\vert s,a)V_\\pi(s’)d s’\\right)da \\\\ \u0026=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}\\gamma p(s’\\vert s,a)\\nabla_\\theta V_\\pi(s’)d s’ da \\\\ \u0026\\overset{\\text{(i)}}{=}\\psi(s)+\\int_\\mathcal{S}\\left(\\int_\\mathcal{A}\\gamma\\pi_\\theta(a\\vert s)p(s’\\vert s,a)da\\right)\\nabla_\\theta V_\\pi(s’)d s’ \\\\ \u0026=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s’)d s’\\label{eq:spgt.9} \\\\ \u0026=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\pi_\\theta)\\left(\\psi(s’)+\\int_\\mathcal{S}\\gamma p(s’\\to s’’,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s’’)d s’’\\right)d s’ \\\\ \u0026=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\pi_\\theta)\\psi(s’)d s’\\nonumber \\\\ \u0026\\hspace{2cm}+\\int_\\mathcal{S}\\int_\\mathcal{S}\\gamma^2 p(s\\to s’,1,\\pi_\\theta)p(s’\\to s’’,1,\\pi_\\theta)\\nabla_\\theta V_\\pi(s’’)d s’’\\hspace{0.1cm}d s’ \\\\ \u0026\\overset{\\text{(ii)}}{=}\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\pi_\\theta)\\psi(s’)d s’+\\int_\\mathcal{S}\\gamma^2 p(s\\to s’’,2,\\pi_\\theta)\\nabla_\\theta V_\\pi(s’’)d s’’ \\\\ \u0026=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\pi_\\theta)\\psi(s’)d s’+\\int_\\mathcal{S}\\gamma^2 p(s\\to s’’,2,\\pi_\\theta)\\psi(s’’)d s’’\\nonumber \\\\ \u0026\\hspace{2cm}+\\int_\\mathcal{S}\\gamma^3 p(s\\to s’’’,3,\\pi_\\theta)\\nabla_\\theta V_\\pi(s’’’)d s’’’ \\\\ \u0026\\hspace{0.3cm}\\vdots\\nonumber \\\\ \u0026=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\pi_\\theta)\\psi(\\tilde{s})d\\tilde{s} \\\\ \u0026=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert\\tilde{s})Q_\\pi(\\tilde{s},a)da\\hspace{0.1cm}d\\tilde{s}\\label{eq:spgt.5} \\end{align} where\nIn this step, we have used the Fubini's theorem to exchange the order of integrals. We have once again used the Fubini's theorem to exchange the order of integration combined with the identity \\begin{equation} \\int_\\mathcal{S}p(s\\to s',1,\\pi_\\theta)p(s'\\to s'',1,\\pi_\\theta)d s'=p(s\\to s'',2,\\pi_\\theta) \\end{equation} Combining \\eqref{eq:spgt.3},\\eqref{eq:spgt.4} and \\eqref{eq:spgt.5} together allows us to obtain \\begin{align} \\hspace{-1cm}\\nabla_\\theta J(\\pi_\\theta)\u0026=\\nabla_\\theta\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\pi(s_0)\\big] \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(s_0)\\nabla_\\theta V_\\pi(s_0)d s_0 \\\\ \u0026=\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\int_{s\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s\\hspace{0.1cm}d s_0 \\\\ \u0026\\overset{\\text{(i)}}{=}\\int_{s\\in\\mathcal{S}}\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s_0\\hspace{0.1cm}d s \\\\ \u0026\\overset{\\text{(ii)}}{=}\\int_{s\\in\\mathcal{S}}\\int_\\mathcal{A}\\left(\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\pi_\\theta)d s_0\\right)\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026=\\int_\\mathcal{S}\\int_\\mathcal{A}\\rho_\\pi(s)\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s, \\end{align} where in two steps (i) and (ii), we have exchanged the order of integration by respectively applying the Fubini’s theorem.$\\tag*{$\\Box$}$\nThe theorem lets us rewrite the policy gradient $\\nabla_\\theta J(\\pi_\\theta)$ in terms of which does not depend the gradient of state distribution, $\\nabla_\\theta\\rho_\\pi(s)$, despite of the fact that $J(\\pi_\\theta)$ depends on $\\rho_\\pi(s)$.\nThe above policy gradient theorem is explicitly known as the start-state policy gradient theorem (since it is given in terms of the start state distribution $\\rho_0$) defined by the policy objective function $J(\\pi_\\theta)$, given in \\eqref{eq:spgt.6}, for episodic and discounted tasks. To extend the theorem in case of continuing problems, in which the interaction between agent and the environment lasts forever, we start by giving a new definition for the $J(\\pi_\\theta)$.\nAverage-reward formulation The performance objective function in such continuing tasks is defined as the average rate of reward, or average reward, denoted $r(\\pi_\\theta)$2, while following policy $\\pi_\\theta$ \\begin{align} J(\\pi_\\theta)\\doteq r(\\pi_\\theta)\u0026\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=0}^{h}\\mathbb{E}\\big[r_{t+1}\\vert S_0,A_{0:t}\\sim\\pi_\\theta\\big] \\\\ \u0026=\\lim_{t\\to\\infty}\\mathbb{E}\\big[r_{t+1}\\vert S_0,A_{0:t}\\sim\\pi_\\theta\\big] \\\\ \u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)r(s,a)da\\hspace{0.1cm}d s, \\end{align} where $\\bar{\\rho}_\\pi(s)\\doteq\\lim_{t\\to\\infty}P(S_t=s\\vert A_{0:t}\\sim\\pi_\\theta)$ is the stationary distribution3 of states under policy $\\pi_\\theta$ which is assumed to exist and does not depend on the start state $S_0$ despite of the fact that the expectations are conditioned on $S_0$.\nAn MDP with this assumption is referred as an ergodic MDP, in which the MDP starts or any early decision made by the agent can have only a temporary effect; in the long run the expectation of being in a state depends only on the policy and the MDP transition probabilities.\nAnalogously, in continuing problems, the return at time-step $t$, $G_t$, is instead called differential return and is defined in terms of differences between rewards and the average reward $r(\\pi_\\theta)$ \\begin{equation} G_t\\doteq\\sum_{k=1}^{\\infty}\\big[r_{t+k}-r(\\pi_\\theta)\\big] \\end{equation} The value functions, which are defined as the expected return, therefore are known as differential value functions and are respectively given by \\begin{align} V_\\pi(s)\u0026\\doteq\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s\\big] \\\\ \u0026=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{k=1}^{\\infty}\\big(r_{t+k}-r(\\pi_\\theta)\\big)\\Big\\vert S_t=s\\right] \\\\ \u0026=\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s’\\vert s,a)\\big[r(s,a)-r(\\pi_\\theta)+V_\\pi(s’)\\big]d s’ da \\\\ \u0026=\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)r(s,a)da-r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s’\\vert s,a)V_\\pi(s’)d s’ da \\end{align} and \\begin{align} Q_\\pi(s,a)\u0026\\doteq\\mathbb{E}_{\\pi_\\theta}\\big[G_t\\vert S_t=s,A_t=a\\big] \\\\ \u0026=\\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{k=1}^{\\infty}\\big(r_{t+k}-r(\\pi_\\theta)\\big)\\Big\\vert S_t=s,A_t=a\\right] \\\\ \u0026=\\int_\\mathcal{S}p(s’\\vert s,a)\\left[r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a’\\vert s’)Q_\\pi(s’,a’)d a’\\right]d s’ \\\\ \u0026=r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s’\\vert s,a)\\int_\\mathcal{A}\\pi_\\theta(a’\\vert s’)Q_\\pi(s’,a’)d a’ d s’ \\\\ \u0026=r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s’\\vert s,a)V_\\pi(s’)d s' \\end{align} Now we are ready for the policy gradient theorem specified for continuing tasks, and hence is called average-reward policy gradient theorem. The theorem states that \\begin{align} \\nabla_\\theta J(\\pi_\\theta)\u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\hspace{0.1cm}da\\hspace{0.1cm}ds\\label{eq:spgt.8} \\\\ \u0026=\\mathbb{E}_{\\bar{\\rho}_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big] \\end{align} Proof\nAnalogy to the episodic case, we start with the gradient of the state value function w.r.t $\\theta$. For any $s\\in\\mathcal{S}$, we have \\begin{align} \\nabla_\\theta V_\\pi(s)\u0026=\\nabla_\\theta\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da \\\\ \u0026=\\underbrace{\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da}_{\\psi(s)}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)da \\\\ \u0026=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\left[r(s,a)-r(\\pi_\\theta)+\\int_\\mathcal{S}p(s’\\vert s,a)V_\\pi(s’)d s’\\right]da \\\\ \u0026=\\psi(s)-\\nabla_\\theta r(\\pi_\\theta)\\underbrace{\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)da}_{=1}+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s’\\vert s,a)\\nabla_\\theta V_\\pi(s’)d s’ da \\\\ \u0026=\\psi(s)-\\nabla_\\theta r(\\pi_\\theta)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s’\\vert s,a)\\nabla_\\theta V_\\pi(s’)d s’ da, \\end{align} which implies that the policy gradient can be obtained as \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\nabla_\\theta r(\\pi_\\theta)=\\psi(s)+\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_\\mathcal{S}p(s’\\vert s,a)\\nabla_\\theta V_\\pi(s’)d s’ da-\\nabla_\\theta V_\\pi(s)\\label{eq:spgt.7} \\end{equation} Using the identity \\begin{equation} \\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta J(\\pi_\\theta)d s=\\nabla_\\theta J(\\pi_\\theta)\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)d s=\\nabla_\\theta J(\\pi_\\theta), \\end{equation} we can continue to derive \\eqref{eq:spgt.7} as \\begin{align} \\hspace{-0.5cm}\\nabla_\\theta J(\\pi_\\theta)\u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s+\\int_{s\\in\\mathcal{S}}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\int_{s’\\in\\mathcal{S}}p(s’\\vert s,a)\\nabla_\\theta V_\\pi(s’)d s’ da\\hspace{0.1cm}d s\\nonumber \\\\ \u0026\\hspace{2cm}-\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta V_\\pi(s)d s \\\\ \u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s+\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s’)\\nabla_\\theta V_\\pi(s’)d s’-\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\nabla_\\theta V_\\pi(s)d s \\\\ \u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\psi(s)d s \\\\ \u0026=\\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\end{align} where the second step is due to \\eqref{eq:fn.1}.$\\tag*{$\\Box$}$\nIt can be seen that the stochastic policy gradient theorem specified in both discounted episodic and continuing settings have the same formulation. In particular, if we replace the state distribution $\\rho_\\pi$ in start-state formulation \\eqref{eq:spgt.1} by the stationary distribution $\\bar{\\rho}_\\pi$ (also with new definition of the value functions), we obtain average-reward formulation \\eqref{eq:spgt.8}. Thus, in the remaining of this note, we will be considering the episodic and discounted setting.\n(Stochastic) Actor-Critic Based on the policy gradient theorem, a (stochastic) actor-critic algorithm consists of two elements:\nActor learns a parameter $\\theta$ of the stochastic policy $\\pi_\\theta$ by iteratively update $\\theta$ by SGA using the policy gradient in \\eqref{eq:spgt.2}. Critic estimates the value function $Q_\\pi(s,a)$ by an state-action value function approximation $Q_w(s,a)$ parameterized by a vector $w$. Policy Gradient with Function Approximation Let $Q_w(s,a)$ be a function approximation parameterized by $w\\in\\mathbb{R}^n$ of the state-action value function $Q_\\pi(s,a)$ for a stochastic policy $\\pi_\\theta$ parameterized by $\\theta\\in\\mathbb{R}^n$. Then if $Q_w(s,a)$ is compatible with the policy parameterization in the sense that\n$Q_w(s,a)=\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)^\\text{T}w$. The parameters $w$ are chosen to minimize the mean-squared error (MSE) \\begin{equation} \\epsilon^2(w)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\big(Q_w(s,a)-Q_\\pi(s,a)\\big)^2\\Big] \\end{equation} then $Q_w(s,a)$ \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big], \\end{equation}\nProof\nTaking the gradient w.r.t $w$ of both sides of the equation given in property (i) gives us \\begin{equation} \\nabla_w Q_w(s,a)=\\nabla_\\theta\\log\\pi_\\theta(a\\vert s) \\end{equation} On the other hand, consider the gradient of the MSE, $\\epsilon^2(w)$, w.r.t $w$, we have \\begin{align} \\nabla_w\\epsilon^2(w)\u0026=\\nabla_w\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\big(Q_w(s,a)-Q_\\pi(s,a)\\big)^2\\Big] \\\\ \u0026=\\nabla_w\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]^2 da\\hspace{0.1cm}d s \\\\ \u0026=2\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]\\nabla_w Q_w(s,a)da\\hspace{0.1cm}d s \\\\ \u0026=2\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\big[Q_w(s,a)-Q_\\pi(s,a)\\big]\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)da\\hspace{0.1cm}d s \\\\ \u0026=2\\left(\\int_\\mathcal{S}\\rho_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)da\\hspace{0.1cm}d s-\\nabla_\\theta J(\\pi_\\theta)\\right) \\\\ \u0026=2\\left(\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big]-\\nabla_\\theta J(\\pi_\\theta)\\right) \\end{align} Moreover, property (ii) claims that this gradient w.r.t $w$ must be zero due to the fact that $w$ minimizes $\\epsilon^2(w)$. And thus, we obtain \\begin{equation} \\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{\\rho_\\pi,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_w(s,a)\\Big] \\end{equation}\nOff-policy Actor-Critic Consider off-policy methods, which learn a target policy $\\pi_\\theta$ using trajectories sampled according to a behavior policy $\\beta(a\\vert s)\\neq\\pi_\\theta(a\\vert s)$. In this setting, the performance objective is given as the value function of the target policy, averaged over $\\beta$, as \\begin{align} J_\\beta(\\pi_\\theta)\u0026=\\int_\\mathcal{S}\\rho_\\beta(s)V_\\pi(s)d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\end{align} The off-policy policy gradient then be given by utilizing importance sampling \\begin{align} \\nabla_\\theta J_\\beta(\\pi_\\theta)\u0026=\\nabla_\\theta\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\Big(\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)+\\color{red}{\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)}\\Big)da\\hspace{0.1cm}d s\\label{eq:offpac.1} \\\\ \u0026\\overset{\\text{(i)}}{\\approx}\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\nabla_\\theta\\pi_\\theta(a\\vert s)Q_\\pi(s,a)da\\hspace{0.1cm}d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\beta(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)d a\\hspace{0.1cm}d s \\\\ \u0026=\\mathbb{E}_{\\rho_\\beta,\\pi_\\theta}\\Big[\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\Big] \\\\ \u0026=\\mathbb{E}_{\\rho_\\beta,\\beta}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\beta(a\\vert s)}\\nabla_\\theta\\log\\pi_\\theta(a\\vert s)Q_\\pi(s,a)\\right], \\end{align} where to get the approximation in step (i), we have removed the $\\color{red}{\\pi_\\theta(a\\vert s)\\nabla_\\theta Q_\\pi(s,a)}$ part in \\eqref{eq:offpac.1}. This approximation is good enough to guarantee the policy improvement and eventually achieve the true local optima due to justification proofs proposed in Off-PAC paper, which stands for Off-policy Actor-Critic.\nDeterministic Policy Gradient Theorem Now let us consider the case of deterministic policy $\\mu_\\theta$, in which the performance objective function is also defined as the expected return of the start state. Thus we also have that \\begin{equation} J(\\mu_\\theta)=\\mathbb{E}_{\\rho_0,\\mu_\\theta}\\big[r_0^\\gamma\\big]=\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\mu(s_0)\\big]\\label{eq:dpgt.1} \\end{equation} The Deterministic Policy Gradient Theorem thus states that \\begin{align} \\nabla_\\theta J(\\mu_\\theta)\u0026=\\int_\\mathcal{S}\\rho_\\mu(s)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\big\\vert_{a=\\mu_\\theta(s)}d s \\\\ \u0026=\\mathbb{E}_{\\rho_\\mu}\\Big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\big\\vert_{a=\\mu_\\theta(s)}\\Big]\\label{eq:dpgt.2} \\end{align} where along with the assumption we have made in stochastic case, we additionally assume that $\\nabla_a p(s’\\vert s,a),\\mu_\\theta(s),\\nabla_\\theta\\mu_\\theta(s),\\nabla_a r(s,a)$ are continues for all $\\theta$ and $s,s’\\in\\mathcal{S},a\\in\\mathcal{A}$. These also imply the existence of $\\nabla_a Q_\\mu(s,a)$.\nProof\nThis proof will be quite similar to what we have used in the stochastic case. Specifically, also starting with the gradient of the value function w.r.t $\\theta$, we have \\begin{align} \u0026\\hspace{-0.5cm}\\nabla_\\theta V_\\mu(s)\\nonumber \\\\ \u0026\\hspace{-0.5cm}=\\nabla_\\theta Q_\\mu(s,\\mu_\\theta(s)) \\\\ \u0026\\hspace{-0.5cm}=\\nabla_\\theta\\left[r(s,\\mu_\\theta(s))+\\int_\\mathcal{S}\\gamma p(s’\\vert s,\\mu_\\theta(s))V_\\mu(s’)d s’\\right] \\\\ \u0026\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a r(s,a)\\vert_{a=\\mu_\\theta(s)}+\\nabla_\\theta\\int_\\mathcal{S}\\gamma p(s’\\vert s,\\mu_\\theta(s))V_\\mu(s’)d s’ \\\\ \u0026\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a r(s,a)\\vert_{a=\\mu_\\theta(s)}\\nonumber \\\\ \u0026\\hspace{1.5cm}+\\int_\\mathcal{S}\\gamma\\nabla_\\theta\\mu_\\theta(s)\\nabla_a p(s’\\vert s,a)\\vert_{a=\\mu_\\theta(s)}V_\\mu(s’)+\\gamma p(s’\\vert s,a)\\nabla_\\theta V_\\mu(s’)d s’ \\\\ \u0026\\hspace{-0.5cm}=\\nabla_\\theta\\mu_\\theta(s)\\nabla_a\\Big(\\underbrace{r(s,a)+\\int_\\mathcal{S}p(s’\\vert s,a)V_\\mu(s’)d s’}_{Q_\\mu(s,a)}\\Big)\\Big\\vert_{a=\\mu_\\theta(s)}+\\int_\\mathcal{S}\\gamma p(s’\\vert s,a)\\nabla_\\theta V_\\mu(s’)d s’ \\\\ \u0026\\hspace{-0.5cm}=\\underbrace{\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}}_{\\psi(s)}+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s’)d s’ \\\\ \u0026\\hspace{-0.5cm}=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s’)d s' \\end{align} which is in the same form as equation \\eqref{eq:spgt.9}. Thus after repeated unrolling, we also end up with \\begin{align} \\nabla_\\theta V_\\mu(s)\u0026=\\psi(s)+\\int_\\mathcal{S}\\gamma p(s\\to s’,1,\\mu_\\theta)\\nabla_\\theta V_\\mu(s’)d s’ \\\\ \u0026=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\mu_\\theta)\\psi(\\tilde{s})d\\tilde{s} \\\\ \u0026=\\int_\\mathcal{S}\\sum_{k=0}^{\\infty}\\gamma^k p(s\\to\\tilde{s},k,\\mu_\\theta)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d\\tilde{s}\\label{eq:dpgt.3} \\end{align} Consider the definition of $J(\\mu_\\theta)$ given in \\eqref{eq:dpgt.1}, taking gradient of both sides w.r.t $\\theta$, combined with \\eqref{eq:dpgt.3} gives us \\begin{align} \u0026\\nabla_\\theta J(\\mu_\\theta)\\nonumber \\\\ \u0026=\\nabla_\\theta\\mathbb{E}_{s_0\\sim\\rho_0}\\big[V_\\mu(s_0)\\big] \\\\ \u0026=\\int_\\mathcal{S}\\rho_0(s_0)\\nabla_\\theta V_\\mu(s_0)d s_0 \\\\ \u0026=\\int_{s_0\\in\\mathcal{S}}\\rho_0(s_0)\\int_{s\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k p(s_0\\to s,k,\\mu_\\theta)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s\\hspace{0.1cm}d s_0 \\\\ \u0026=\\int_{s\\in\\mathcal{S}}\\left(\\int_{s_0\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}\\gamma^k\\rho_0(s_0)p(s_0\\to s,k,\\mu_\\theta)d s_0\\right)\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\mu(s)\\nabla_\\theta\\mu_\\theta(s)\\nabla a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}d s, \\end{align} where in the forth step, the Fubini’s theorem has helped us exchange the order of integration.$\\tag*{$\\Box$}$\nIt is worth remarking that we can consider a deterministic policy $\\mu_\\theta$ as a special case of the stochastic policy, in which $\\pi_\\theta(\\cdot\\vert s)$ becomes the Kronecker delta function, which takes the value of $1$ at only one point $a\\in\\mathcal{A}$ and $0$ elsewhere.\nTo be more specific, in the original paper, the authors have shown that by rewriting the stochastic policy as $\\pi_{\\mu_\\theta,\\sigma}$, which is parameterized by a deterministic policy $\\mu_\\theta:\\mathcal{S}\\to\\mathcal{A}$ and a variance parameter $\\sigma$ such that for $\\sigma=0$, we have that $\\pi_{\\mu_\\theta,0}\\equiv\\mu_\\theta$; then as $\\sigma\\to 0$, they have proved that the stochastic policy gradient converges to the deterministic one.\nThis critical result allows us to apply the deterministic policy gradient to common policy gradient frameworks, for example actor-critic approaches.\nDeterministic Actor-Critic On-policy Deterministic Actor-Critic Analogous to the stochastic approach, the deterministic actor learns a parameter $\\theta$ by using SGA to iteratively update the parameter vector according to the deterministic policy gradient direction \\eqref{eq:dpgt.2} while the critic estimates the state-action value function by a function approximation $Q_w(s,a)$ using a policy evaluation method such as TD-learning.\nFor instance, a deterministic actor-critic method with a Sarsa critic has the following update in each time-step $t$ \\begin{align} \\delta_t\u0026=r_{t+1}+\\gamma Q_w(s_{t+1},a_{t+1})-Q_w(s_t,a_t) \\\\ w_{t+1}\u0026=w_t+\\alpha_w\\delta_t\\nabla_w Q_w(s_t,a_t) \\\\ \\theta_{t+1}\u0026=\\theta_t+\\alpha_\\theta\\nabla_\\theta\\mu_\\theta(s_t)\\nabla_a Q_w(s_t,a_t)\\vert_{a=\\mu_\\theta(s)}, \\end{align} where $\\delta_t$ as specified before, are known as TD errors.\nOff-policy Deterministic Actor-Critic Analogy to stochastic methods, let $\\beta(a\\vert s)$ denote the behavior policy that generates trajectories used for updating the deterministic target policy $\\mu_\\theta(s)$, the performance objective $J(\\mu_\\theta)$ is then given as \\begin{align} J_\\beta(\\mu_\\theta)\u0026=\\int_\\mathcal{S}\\rho_\\beta(s)V_\\mu(s)d s \\\\ \u0026=\\int_\\mathcal{S}\\rho_\\beta(s)Q_\\mu(s,\\mu_\\theta(s))d s \\end{align} It is noticeable that the deterministic policy allows us to explicitly replace the integration over action space $\\mathcal{A}$ by $Q_\\mu(s,\\mu_\\theta(s))$, and thus we do not need to use importance sampling in the actor. Hence, we have that \\begin{align} \\nabla_\\theta J_\\beta(\\mu_\\theta)\u0026=\\nabla_\\theta\\int_\\mathcal{S}\\rho_\\beta(s)Q_\\mu(s,\\mu_\\theta(s))d s \\\\ \u0026\\approx\\int_\\mathcal{S}\\rho_\\beta(s)\\nabla_\\theta\\mu_\\theta(a\\vert s)Q_\\mu(s,a)d s \\\\ \u0026=\\mathbb{E}_{\\rho_\\beta}\\Big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\Big] \\end{align} We can also avoid using importance sampling in critic by using Q-learning as our policy evaluation In particular, the off-policy deterministic actor-critic with a Q-learning critic has the form of \\begin{align} \\delta_t\u0026=r_{t+1}+\\gamma Q_w(s_{t+1},\\mu_\\theta(s))-Q_w(s_t,a_t)\\label{eq:opdac.1} \\\\ w_{t+1}\u0026=w_t+\\alpha_w\\delta_t\\nabla_w Q_w(s_t,a_t) \\\\ \\theta_{t+1}\u0026=\\theta_t+\\alpha_\\theta\\nabla_\\theta\\mu_\\theta(s_t)\\nabla_a Q_w(s_t,a_t)\\vert_{a_t=\\mu_\\theta(s_t)}, \\end{align} where the greedy policy, $\\underset{a}{\\text{argmax}}Q_w(s,a)$, in the usual Q-learning update has been replaced by the newly-updated deterministic policy, $\\mu_\\theta(s)$, in \\eqref{eq:opdac.1}, i.e. $\\mu_\\theta\\equiv\\mu_{\\theta_k}$.\nCompatible Function Approximation with Deterministic Policy From what we have mentioned in the stochastic case, we can also define an appropriate form of function approximation $Q_w$ which preserves the direction of true gradient.\nIn particular, A $w$-parameterized $Q_w(s,a)$ is referred as a compatible function approximator of the state-action value function $Q_\\mu$ for deterministic policy $\\mu_\\theta$ in the sense that\n$\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}=\\nabla_\\theta\\mu_\\theta(s)^\\text{T}w$. Parameters $w$ minimize the mean-squared error \\begin{equation} \\text{MSE}(\\theta,w)=\\mathbb{E}\\big[\\epsilon(\\theta,w,s)^\\text{T}\\epsilon(\\theta,w,s)\\big], \\end{equation} where \\begin{equation} \\epsilon(\\theta,w,s)\\doteq\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)} \\end{equation} then \\begin{equation} \\nabla_\\theta J(\\mu_\\theta)=\\mathbb{E}\\big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big]\\label{eq:cfad.1} \\end{equation}\nProof\nWe follow the procedure used in stochastic case. Specifically, starting with the property (i) and by definition of $\\epsilon$, we have that \\begin{align} \\nabla_w\\epsilon(\\theta,w,s)\u0026=\\nabla_w\\big[\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\big] \\\\ \u0026=\\nabla_w\\big(\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big) \\\\ \u0026=\\nabla_w\\big(\\nabla_\\theta\\mu_\\theta(s)^\\text{T}w\\big) \\\\ \u0026=\\nabla_\\theta\\mu_\\theta(s) \\end{align} Using this result, the gradient of $\\text{MSE}(\\theta,w)$ w.r.t $w$ is thus given as \\begin{align} \\nabla_w\\text{MSE}(\\theta,w)\u0026=\\nabla_w\\mathbb{E}\\big[\\epsilon(\\theta,w,s)^\\text{T}\\epsilon(\\theta,w,s)\\big] \\\\ \u0026=\\mathbb{E}\\big[2\\epsilon(\\theta,w,s)\\nabla_w\\epsilon(\\theta,w,s)\\big] \\\\ \u0026=2\\mathbb{E}\\Big[\\big(\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}-\\nabla_a Q_\\mu(s,a)\\vert_{a=\\mu_\\theta(s)}\\big)\\nabla_\\theta\\mu_\\theta(s)\\Big] \\\\ \u0026=2\\Big[\\mathbb{E}\\big[\\nabla_\\theta\\mu_\\theta(s)\\nabla_a Q_w(s,a)\\vert_{a=\\mu_\\theta(s)}\\big]-J(\\mu_\\theta)\\Big], \\end{align} which lets our claim, \\eqref{eq:cfad.1}, follows due to the property (ii), which means that $\\nabla_w\\text{MSE}(\\theta,w)=0$.$\\tag*{$\\Box$}$\nDeep Deterministic Policy Gradient References [1] David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller. Deterministic Policy Gradient Algorithms. JMLR 2014.\n[2] Richard S. Sutton, Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[3] Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. NIPS 1999.\n[4] Elias M. Stein, Rami Shakarchi. Real Analysis: Measure Theory, Integration, and Hilbert Spaces. Princeton University Press, 2007.\n[5] Thomas Degris, Martha White, Richard S. Sutton. Off-Policy Actor-Critic. ICML 2012.\n[6] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra. Continuous control with deep reinforcement learning. ICLR 2016.\nFootnotes To simplify the notation, we have let $\\rho_\\pi\\doteq\\rho_{\\pi_\\theta}$ and $Q_\\pi\\doteq Q_{\\pi_\\theta}$ implicitly. As a result, we will also denote $V_{\\pi_\\theta}$ by $V_\\pi$. ↩︎\nThe notation $r(\\pi)$ of the average reward is just a notation-abused and should not be confused with notation $r$ of the reward function. ↩︎\nThis means that if we keep selecting action according to $\\pi_\\theta$, we remains in the same state distribution $\\bar{\\rho}_\\pi$, i.e. \\begin{equation} \\int_\\mathcal{S}\\bar{\\rho}_\\pi(s)\\int_\\mathcal{A}\\pi_\\theta(a\\vert s)p(s’\\vert s,a)da\\hspace{0.1cm}d s=\\bar{\\rho}_\\pi(s’)\\label{eq:fn.1} \\end{equation} ↩︎\n","wordCount":"2450","inLanguage":"en","datePublished":"2022-12-02T19:26:44+07:00","dateModified":"2022-12-02T19:26:44+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Deterministic Policy Gradients</h1><div class=post-meta><span title='2022-12-02 19:26:44 +0700 +0700'>December 2, 2022</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preliminaries>Preliminaries</a></li><li><a href=#spg-theorem>(Stochastic) Policy Gradient Theorem</a><ul><li><a href=#start-state>Start-state formulation</a></li><li><a href=#avg-reward>Average-reward formulation</a></li></ul></li><li><a href=#stochastic-ac>(Stochastic) Actor-Critic</a><ul><li><a href=#pg-func-approx>Policy Gradient with Function Approximation</a></li><li><a href=#off-pac>Off-policy Actor-Critic</a></li></ul></li><li><a href=#dpg-theorem>Deterministic Policy Gradient Theorem</a></li><li><a href=#deterministic-ac>Deterministic Actor-Critic</a><ul><li><a href=#on-policy-deterministic-ac>On-policy Deterministic Actor-Critic</a></li><li><a href=#off-policy-deterministic-ac>Off-policy Deterministic Actor-Critic</a></li><li><a href=#compatible-func-approx-deterministic>Compatible Function Approximation with Deterministic Policy</a></li></ul></li><li><a href=#ddpg>Deep Deterministic Policy Gradient</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Deterministic Policy Gradient Algorithms</p></blockquote><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><p>Consider a (infinite-horizon) Markov Decision Process (MDP), defined as a tuple of $(\mathcal{S},\mathcal{A},p,r,\rho_0,\gamma)$ where</p><ul><li>$\mathcal{S}$ is the <strong>state space</strong>.</li><li>$\mathcal{A}$ is the <strong>action space</strong>.</li><li>$p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>transition probability distribution</strong>, i.e. $p(s,a,s&rsquo;)=p(s&rsquo;\vert s,a)$ denotes the probability of transitioning to state $s&rsquo;$ when taking action $a$ from state $s$.</li><li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>, and let us denote $r_{t+1}\doteq r(S_t,A_t)$.</li><li>$\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.</li><li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li></ul><p>Within an MPD, a policy parameterized by a vector $\theta\in\mathbb{R}^n$ can be given as</p><ul id=number-listt><li><b>Stochastic policy</b>. $\pi_\theta:\mathcal{S}\times\mathcal{A}\to[0,1]$, or</li><li><b>Deterministic policy</b>. $\mu_\theta:\mathcal{S}\to\mathcal{A}$.</li></ul><h2 id=spg-theorem>(Stochastic) Policy Gradient Theorem<a hidden class=anchor aria-hidden=true href=#spg-theorem>#</a></h2><p>We continue with an assumption that the action space $\mathcal{A}=\mathbb{R}^m$ and the state space $\mathcal{S}\subset\mathbb{R}^d$ and $\mathcal{S}$ is compact.</p><h3 id=start-state>Start-state formulation<a hidden class=anchor aria-hidden=true href=#start-state>#</a></h3><p>Recall that in the stochastic case, $\pi_\theta$, the <a href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/#policy-grad-theorem-ep><strong>Policy Gradient Theorem</strong></a> states that<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>
\begin{align}
\nabla_\theta J(\pi_\theta)&=\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)\hspace{0.1cm}da\hspace{0.1cm}ds\label{eq:spgt.1} \\ &=\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_\pi(s,a)\Big]\label{eq:spgt.2}
\end{align}
where</p><ul id=number-list><li>$J(\pi_\theta)$ is the <b>performance objective function</b>, which we are trying to maximize. It is defined as the expected cumulative discounted reward from the start state
\begin{align}
J(\pi_\theta)&\doteq\mathbb{E}_{\rho_0,\pi_\theta}\big[r_0^\gamma\big]\label{eq:spgt.6} \\ &=\mathbb{E}_{\rho_0,\pi_\theta}\left[\sum_{t=0}^{\infty}\gamma^t r_{t+1}\right] \\ &=\mathbb{E}_{\rho_0,\pi_\theta}\big[G_0\big] \\ &=\mathbb{E}_{s_0\sim\rho_0}\Big[\mathbb{E}_{\pi_\theta}\big[G_t\vert S_t=s_0\big]\Big] \\ &=\mathbb{E}_{s_0\sim\rho_0}\big[V_\pi(s_0)\big],\label{eq:spgt.3}
\end{align}
where $r_t^\gamma$ is defined as the total discounted reward from time-step $t$ onward, which is thus the return at that step
\begin{equation}
G_t=r_t^\gamma\doteq\sum_{t=0}^{\infty}\gamma^t r_{t+1}
\end{equation}</li><li>The function $\rho_\pi(s)$ is defined as the discounted weighting of states encountered when starting at $s_0$ and following $\pi_\theta$ thereafter
\begin{align}
\rho_\pi(s)&\doteq\sum_{t=0}^{\infty}\gamma^t P(S_t=s\vert s_0,\pi_\theta) \\ &=\int_\mathcal{S}\rho_0(\bar{s})\left(\sum_{t=0}^{\infty}\gamma^t P(S_t=s\vert\pi_\theta)\right)d\bar{s} \\ &=\int_\mathcal{S}\sum_{t=0}^{\infty}\gamma^t\rho_0(\bar{s})p(\bar{s}\to s,t,\pi_\theta)d\bar{s},\label{eq:spgt.4}
\end{align}
where $p(\bar{s}\to s,t,\pi_\theta)$ is defined as the the probability of transitioning to $s$ after $t$ steps starting from $\bar{s}$ under $\pi_\theta$, which implies that
\begin{equation}
p(\bar{s}\to s,t,\pi_\theta)=P(S_t=s\vert\pi_\theta)
\end{equation}
In fact, $\rho_\pi(s)$ can be seen as a density function since integrating $\rho_\pi$ over state space $\mathcal{S}$ gives us
\begin{align}
\int_\mathcal{S}\rho_\pi(s)d s&=\int_{s\in\mathcal{S}}\int_{\bar{s}\in\mathcal{S}}\sum_{t=0}^{\infty}\gamma^t\rho_0(\bar{s})p(\bar{s}\to s,t,\pi_\theta)d\bar{s}\hspace{0.1cm}d s \\ &=\int_{\bar{s}\in\mathcal{S}}\rho_0(\bar{s})\int_{s\in\mathcal{S}}\sum_{t=0}^{\infty}\gamma^t p(\bar{s}\to s,t,\pi_\theta)d s\hspace{0.1cm}d\bar{s} \\ &=\int_{\bar{s}\in\mathcal{S}}\rho_0(\bar{s})\sum_{t=0}^{\infty}\gamma^t\underbrace{\int_{s\in\mathcal{S}}p(\bar{s}\to s,t,\pi_\theta)d s}_{=1}d\bar{s} \\ &=\int_\mathcal{S}\rho_0(\bar{s})\underbrace{\sum_{t=0}^{\infty}\gamma^t}_{=1}d\bar{s} \\ &=\int_\mathcal{S}\rho_0(\bar{s})d\bar{s} \\ &=1
		\end{align}
		Thus, this definition of $\rho_\pi$ lets us write \eqref{eq:spgt.1} as an expectation, combined with using the <b>log-likelihood trick</b>, we end up with \eqref{eq:spgt.2}.</li></ul><p><strong>Proof</strong><br>The definition of $J(\pi_\theta)$ given in \eqref{eq:spgt.3} suggests us begin by considering the gradient of the state value function w.r.t $\theta$. For any $s\in\mathcal{S}$, we have
\begin{align}
\hspace{-1cm}\nabla_\theta V_\pi(s)&=\nabla_\theta\int_\mathcal{A}\pi_\theta(a\vert s)Q_\pi(s,a)da \\ &=\underbrace{\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da}_{\psi(s)}+\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta Q_\pi(s,a)da \\ &=\psi(s)+\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta\left(r(s,a)+\int_\mathcal{S}\gamma p(s&rsquo;\vert s,a)V_\pi(s&rsquo;)d s&rsquo;\right)da \\ &=\psi(s)+\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}\gamma p(s&rsquo;\vert s,a)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; da \\ &\overset{\text{(i)}}{=}\psi(s)+\int_\mathcal{S}\left(\int_\mathcal{A}\gamma\pi_\theta(a\vert s)p(s&rsquo;\vert s,a)da\right)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; \\ &=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\pi_\theta)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo;\label{eq:spgt.9} \\ &=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\pi_\theta)\left(\psi(s&rsquo;)+\int_\mathcal{S}\gamma p(s&rsquo;\to s&rsquo;&rsquo;,1,\pi_\theta)\nabla_\theta V_\pi(s&rsquo;&rsquo;)d s&rsquo;&rsquo;\right)d s&rsquo; \\ &=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\pi_\theta)\psi(s&rsquo;)d s&rsquo;\nonumber \\ &\hspace{2cm}+\int_\mathcal{S}\int_\mathcal{S}\gamma^2 p(s\to s&rsquo;,1,\pi_\theta)p(s&rsquo;\to s&rsquo;&rsquo;,1,\pi_\theta)\nabla_\theta V_\pi(s&rsquo;&rsquo;)d s&rsquo;&rsquo;\hspace{0.1cm}d s&rsquo; \\ &\overset{\text{(ii)}}{=}\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\pi_\theta)\psi(s&rsquo;)d s&rsquo;+\int_\mathcal{S}\gamma^2 p(s\to s&rsquo;&rsquo;,2,\pi_\theta)\nabla_\theta V_\pi(s&rsquo;&rsquo;)d s&rsquo;&rsquo; \\ &=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\pi_\theta)\psi(s&rsquo;)d s&rsquo;+\int_\mathcal{S}\gamma^2 p(s\to s&rsquo;&rsquo;,2,\pi_\theta)\psi(s&rsquo;&rsquo;)d s&rsquo;&rsquo;\nonumber \\ &\hspace{2cm}+\int_\mathcal{S}\gamma^3 p(s\to s&rsquo;&rsquo;&rsquo;,3,\pi_\theta)\nabla_\theta V_\pi(s&rsquo;&rsquo;&rsquo;)d s&rsquo;&rsquo;&rsquo; \\ &\hspace{0.3cm}\vdots\nonumber \\ &=\int_\mathcal{S}\sum_{k=0}^{\infty}\gamma^k p(s\to\tilde{s},k,\pi_\theta)\psi(\tilde{s})d\tilde{s} \\ &=\int_\mathcal{S}\sum_{k=0}^{\infty}\gamma^k p(s\to\tilde{s},k,\pi_\theta)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert\tilde{s})Q_\pi(\tilde{s},a)da\hspace{0.1cm}d\tilde{s}\label{eq:spgt.5}
\end{align}
where</p><ul id=roman-list><li>In this step, we have used the <b>Fubini's theorem</b> to exchange the order of integrals.</li><li>We have once again used the <b>Fubini's theorem</b> to exchange the order of integration combined with the identity
\begin{equation}
\int_\mathcal{S}p(s\to s',1,\pi_\theta)p(s'\to s'',1,\pi_\theta)d s'=p(s\to s'',2,\pi_\theta)
\end{equation}</li></ul><p>Combining \eqref{eq:spgt.3},\eqref{eq:spgt.4} and \eqref{eq:spgt.5} together allows us to obtain
\begin{align}
\hspace{-1cm}\nabla_\theta J(\pi_\theta)&=\nabla_\theta\mathbb{E}_{s_0\sim\rho_0}\big[V_\pi(s_0)\big] \\ &=\int_\mathcal{S}\rho_0(s_0)\nabla_\theta V_\pi(s_0)d s_0 \\ &=\int_{s_0\in\mathcal{S}}\rho_0(s_0)\int_{s\in\mathcal{S}}\sum_{k=0}^{\infty}\gamma^k p(s_0\to s,k,\pi_\theta)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s\hspace{0.1cm}d s_0 \\ &\overset{\text{(i)}}{=}\int_{s\in\mathcal{S}}\int_{s_0\in\mathcal{S}}\rho_0(s_0)\sum_{k=0}^{\infty}\gamma^k p(s_0\to s,k,\pi_\theta)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s_0\hspace{0.1cm}d s \\ &\overset{\text{(ii)}}{=}\int_{s\in\mathcal{S}}\int_\mathcal{A}\left(\int_{s_0\in\mathcal{S}}\rho_0(s_0)\sum_{k=0}^{\infty}\gamma^k p(s_0\to s,k,\pi_\theta)d s_0\right)\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s \\ &=\int_\mathcal{S}\int_\mathcal{A}\rho_\pi(s)\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s \\ &=\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s,
\end{align}
where in two steps (i) and (ii), we have exchanged the order of integration by respectively applying the <strong>Fubini&rsquo;s theorem</strong>.$\tag*{$\Box$}$</p><p>The theorem lets us rewrite the policy gradient $\nabla_\theta J(\pi_\theta)$ in terms of which does not depend the gradient of state distribution, $\nabla_\theta\rho_\pi(s)$, despite of the fact that $J(\pi_\theta)$ depends on $\rho_\pi(s)$.</p><p>The above policy gradient theorem is explicitly known as the <strong>start-state policy gradient theorem</strong> (since it is given in terms of the start state distribution $\rho_0$) defined by the policy objective function $J(\pi_\theta)$, given in \eqref{eq:spgt.6}, for episodic and discounted tasks. To extend the theorem in case of continuing problems, in which the interaction between agent and the environment lasts forever, we start by giving a new definition for the $J(\pi_\theta)$.</p><h3 id=avg-reward>Average-reward formulation<a hidden class=anchor aria-hidden=true href=#avg-reward>#</a></h3><p>The performance objective function in such continuing tasks is defined as the average rate of reward, or <strong>average reward</strong>, denoted $r(\pi_\theta)$<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, while following policy $\pi_\theta$
\begin{align}
J(\pi_\theta)\doteq r(\pi_\theta)&\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=0}^{h}\mathbb{E}\big[r_{t+1}\vert S_0,A_{0:t}\sim\pi_\theta\big] \\ &=\lim_{t\to\infty}\mathbb{E}\big[r_{t+1}\vert S_0,A_{0:t}\sim\pi_\theta\big] \\ &=\int_\mathcal{S}\bar{\rho}_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)r(s,a)da\hspace{0.1cm}d s,
\end{align}
where $\bar{\rho}_\pi(s)\doteq\lim_{t\to\infty}P(S_t=s\vert A_{0:t}\sim\pi_\theta)$ is the <strong>stationary distribution</strong><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> of states under policy $\pi_\theta$ which is assumed to exist and does not depend on the start state $S_0$ despite of the fact that the expectations are conditioned on $S_0$.</p><p>An MDP with this assumption is referred as an <strong>ergodic MDP</strong>, in which the MDP starts or any early decision made by the agent can have only a temporary effect; in the long run the expectation of being in a state depends only on the policy and the MDP transition probabilities.</p><p>Analogously, in continuing problems, the return at time-step $t$, $G_t$, is instead called <strong>differential return</strong> and is defined in terms of differences between rewards and the average reward $r(\pi_\theta)$
\begin{equation}
G_t\doteq\sum_{k=1}^{\infty}\big[r_{t+k}-r(\pi_\theta)\big]
\end{equation}
The value functions, which are defined as the expected return, therefore are known as <strong>differential value functions</strong> and are respectively given by
\begin{align}
V_\pi(s)&\doteq\mathbb{E}_{\pi_\theta}\big[G_t\vert S_t=s\big] \\ &=\mathbb{E}_{\pi_\theta}\left[\sum_{k=1}^{\infty}\big(r_{t+k}-r(\pi_\theta)\big)\Big\vert S_t=s\right] \\ &=\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}p(s&rsquo;\vert s,a)\big[r(s,a)-r(\pi_\theta)+V_\pi(s&rsquo;)\big]d s&rsquo; da \\ &=\int_\mathcal{A}\pi_\theta(a\vert s)r(s,a)da-r(\pi_\theta)+\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}p(s&rsquo;\vert s,a)V_\pi(s&rsquo;)d s&rsquo; da
\end{align}
and
\begin{align}
Q_\pi(s,a)&\doteq\mathbb{E}_{\pi_\theta}\big[G_t\vert S_t=s,A_t=a\big] \\ &=\mathbb{E}_{\pi_\theta}\left[\sum_{k=1}^{\infty}\big(r_{t+k}-r(\pi_\theta)\big)\Big\vert S_t=s,A_t=a\right] \\ &=\int_\mathcal{S}p(s&rsquo;\vert s,a)\left[r(s,a)-r(\pi_\theta)+\int_\mathcal{A}\pi_\theta(a&rsquo;\vert s&rsquo;)Q_\pi(s&rsquo;,a&rsquo;)d a&rsquo;\right]d s&rsquo; \\ &=r(s,a)-r(\pi_\theta)+\int_\mathcal{S}p(s&rsquo;\vert s,a)\int_\mathcal{A}\pi_\theta(a&rsquo;\vert s&rsquo;)Q_\pi(s&rsquo;,a&rsquo;)d a&rsquo; d s&rsquo; \\ &=r(s,a)-r(\pi_\theta)+\int_\mathcal{S}p(s&rsquo;\vert s,a)V_\pi(s&rsquo;)d s'
\end{align}
Now we are ready for the <strong>policy gradient theorem</strong> specified for continuing tasks, and hence is called <strong>average-reward policy gradient theorem</strong>. The theorem states that
\begin{align}
\nabla_\theta J(\pi_\theta)&=\int_\mathcal{S}\bar{\rho}_\pi(s)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)\hspace{0.1cm}da\hspace{0.1cm}ds\label{eq:spgt.8} \\ &=\mathbb{E}_{\bar{\rho}_\pi,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_\pi(s,a)\Big]
\end{align}
<strong>Proof</strong><br>Analogy to the episodic case, we start with the gradient of the state value function w.r.t $\theta$. For any $s\in\mathcal{S}$, we have
\begin{align}
\nabla_\theta V_\pi(s)&=\nabla_\theta\int_\mathcal{A}\pi_\theta(a\vert s)Q_\pi(s,a)da \\ &=\underbrace{\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da}_{\psi(s)}+\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta Q_\pi(s,a)da \\ &=\psi(s)+\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta\left[r(s,a)-r(\pi_\theta)+\int_\mathcal{S}p(s&rsquo;\vert s,a)V_\pi(s&rsquo;)d s&rsquo;\right]da \\ &=\psi(s)-\nabla_\theta r(\pi_\theta)\underbrace{\int_\mathcal{A}\pi_\theta(a\vert s)da}_{=1}+\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}p(s&rsquo;\vert s,a)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; da \\ &=\psi(s)-\nabla_\theta r(\pi_\theta)+\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}p(s&rsquo;\vert s,a)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; da,
\end{align}
which implies that the policy gradient can be obtained as
\begin{equation}
\nabla_\theta J(\pi_\theta)=\nabla_\theta r(\pi_\theta)=\psi(s)+\int_\mathcal{A}\pi_\theta(a\vert s)\int_\mathcal{S}p(s&rsquo;\vert s,a)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; da-\nabla_\theta V_\pi(s)\label{eq:spgt.7}
\end{equation}
Using the identity
\begin{equation}
\int_\mathcal{S}\bar{\rho}_\pi(s)\nabla_\theta J(\pi_\theta)d s=\nabla_\theta J(\pi_\theta)\int_\mathcal{S}\bar{\rho}_\pi(s)d s=\nabla_\theta J(\pi_\theta),
\end{equation}
we can continue to derive \eqref{eq:spgt.7} as
\begin{align}
\hspace{-0.5cm}\nabla_\theta J(\pi_\theta)&=\int_\mathcal{S}\bar{\rho}_\pi(s)\psi(s)d s+\int_{s\in\mathcal{S}}\bar{\rho}_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)\int_{s&rsquo;\in\mathcal{S}}p(s&rsquo;\vert s,a)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo; da\hspace{0.1cm}d s\nonumber \\ &\hspace{2cm}-\int_\mathcal{S}\bar{\rho}_\pi(s)\nabla_\theta V_\pi(s)d s \\ &=\int_\mathcal{S}\bar{\rho}_\pi(s)\psi(s)d s+\int_\mathcal{S}\bar{\rho}_\pi(s&rsquo;)\nabla_\theta V_\pi(s&rsquo;)d s&rsquo;-\int_\mathcal{S}\bar{\rho}_\pi(s)\nabla_\theta V_\pi(s)d s \\ &=\int_\mathcal{S}\bar{\rho}_\pi(s)\psi(s)d s \\ &=\int_\mathcal{S}\bar{\rho}_\pi(s)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s
\end{align}
where the second step is due to \eqref{eq:fn.1}.$\tag*{$\Box$}$</p><p>It can be seen that the stochastic policy gradient theorem specified in both discounted episodic and continuing settings have the same formulation. In particular, if we replace the state distribution $\rho_\pi$ in start-state formulation \eqref{eq:spgt.1} by the stationary distribution $\bar{\rho}_\pi$ (also with new definition of the value functions), we obtain average-reward formulation \eqref{eq:spgt.8}. Thus, in the remaining of this note, we will be considering the episodic and discounted setting.</p><h2 id=stochastic-ac>(Stochastic) Actor-Critic<a hidden class=anchor aria-hidden=true href=#stochastic-ac>#</a></h2><p>Based on the policy gradient theorem, a (stochastic) <strong>actor-critic algorithm</strong> consists of two elements:</p><ul id=number-list><li><b>Actor</b> learns a parameter $\theta$ of the stochastic policy $\pi_\theta$ by iteratively update $\theta$ by SGA using the policy gradient in \eqref{eq:spgt.2}.</li><li><b>Critic</b> estimates the value function $Q_\pi(s,a)$ by an state-action value function approximation $Q_w(s,a)$ parameterized by a vector $w$.</li></ul><h3 id=pg-func-approx>Policy Gradient with Function Approximation<a hidden class=anchor aria-hidden=true href=#pg-func-approx>#</a></h3><p>Let $Q_w(s,a)$ be a function approximation parameterized by $w\in\mathbb{R}^n$ of the state-action value function $Q_\pi(s,a)$ for a stochastic policy $\pi_\theta$ parameterized by $\theta\in\mathbb{R}^n$. Then if $Q_w(s,a)$ is <strong>compatible</strong> with the policy parameterization in the sense that</p><ul id=roman-list><li><span id=prop-i>$Q_w(s,a)=\nabla_\theta\log\pi_\theta(a\vert s)^\text{T}w$.</span></li><li><span id=prop-ii>The parameters $w$ are chosen to minimize the mean-squared error (MSE)</span>
\begin{equation}
\epsilon^2(w)=\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\big(Q_w(s,a)-Q_\pi(s,a)\big)^2\Big]
\end{equation}</li></ul><p>then $Q_w(s,a)$
\begin{equation}
\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_w(s,a)\Big],
\end{equation}</p><p><strong>Proof</strong><br>Taking the gradient w.r.t $w$ of both sides of the equation given in property <a href=#prop-i>(i)</a> gives us
\begin{equation}
\nabla_w Q_w(s,a)=\nabla_\theta\log\pi_\theta(a\vert s)
\end{equation}
On the other hand, consider the gradient of the MSE, $\epsilon^2(w)$, w.r.t $w$, we have
\begin{align}
\nabla_w\epsilon^2(w)&=\nabla_w\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\big(Q_w(s,a)-Q_\pi(s,a)\big)^2\Big] \\ &=\nabla_w\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)\big[Q_w(s,a)-Q_\pi(s,a)\big]^2 da\hspace{0.1cm}d s \\ &=2\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)\big[Q_w(s,a)-Q_\pi(s,a)\big]\nabla_w Q_w(s,a)da\hspace{0.1cm}d s \\ &=2\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)\big[Q_w(s,a)-Q_\pi(s,a)\big]\nabla_\theta\log\pi_\theta(a\vert s)da\hspace{0.1cm}d s \\ &=2\left(\int_\mathcal{S}\rho_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta\log\pi_\theta(a\vert s)Q_w(s,a)da\hspace{0.1cm}d s-\nabla_\theta J(\pi_\theta)\right) \\ &=2\left(\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_w(s,a)\Big]-\nabla_\theta J(\pi_\theta)\right)
\end{align}
Moreover, property <a href=#prop-ii>(ii)</a> claims that this gradient w.r.t $w$ must be zero due to the fact that $w$ minimizes $\epsilon^2(w)$. And thus, we obtain
\begin{equation}
\nabla_\theta J(\pi_\theta)=\mathbb{E}_{\rho_\pi,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_w(s,a)\Big]
\end{equation}</p><h3 id=off-pac>Off-policy Actor-Critic<a hidden class=anchor aria-hidden=true href=#off-pac>#</a></h3><p>Consider off-policy methods, which learn a target policy $\pi_\theta$ using trajectories sampled according to a behavior policy $\beta(a\vert s)\neq\pi_\theta(a\vert s)$. In this setting, the performance objective is given as the value function of the target policy, averaged over $\beta$, as
\begin{align}
J_\beta(\pi_\theta)&=\int_\mathcal{S}\rho_\beta(s)V_\pi(s)d s \\ &=\int_\mathcal{S}\rho_\beta(s)\int_\mathcal{A}\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s
\end{align}
The off-policy policy gradient then be given by utilizing importance sampling
\begin{align}
\nabla_\theta J_\beta(\pi_\theta)&=\nabla_\theta\int_\mathcal{S}\rho_\beta(s)\int_\mathcal{A}\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s \\ &=\int_\mathcal{S}\rho_\beta(s)\int_\mathcal{A}\Big(\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)+\color{red}{\pi_\theta(a\vert s)\nabla_\theta Q_\pi(s,a)}\Big)da\hspace{0.1cm}d s\label{eq:offpac.1} \\ &\overset{\text{(i)}}{\approx}\int_\mathcal{S}\rho_\beta(s)\int_\mathcal{A}\nabla_\theta\pi_\theta(a\vert s)Q_\pi(s,a)da\hspace{0.1cm}d s \\ &=\int_\mathcal{S}\rho_\beta(s)\int_\mathcal{A}\pi_\theta(a\vert s)\nabla_\theta\log\pi_\theta(a\vert s)Q_\pi(s,a)d a\hspace{0.1cm}d s \\ &=\mathbb{E}_{\rho_\beta,\pi_\theta}\Big[\nabla_\theta\log\pi_\theta(a\vert s)Q_\pi(s,a)\Big] \\ &=\mathbb{E}_{\rho_\beta,\beta}\left[\frac{\pi_\theta(a\vert s)}{\beta(a\vert s)}\nabla_\theta\log\pi_\theta(a\vert s)Q_\pi(s,a)\right],
\end{align}
where to get the approximation in step (i), we have removed the $\color{red}{\pi_\theta(a\vert s)\nabla_\theta Q_\pi(s,a)}$ part in \eqref{eq:offpac.1}. This approximation is good enough to guarantee the policy improvement and eventually achieve the true local optima due to justification proofs proposed in <a href=#offpac-paper>Off-PAC paper</a>, which stands for <strong>Off-policy Actor-Critic</strong>.</p><h2 id=dpg-theorem>Deterministic Policy Gradient Theorem<a hidden class=anchor aria-hidden=true href=#dpg-theorem>#</a></h2><p>Now let us consider the case of deterministic policy $\mu_\theta$, in which the performance objective function is also defined as the expected return of the start state. Thus we also have that
\begin{equation}
J(\mu_\theta)=\mathbb{E}_{\rho_0,\mu_\theta}\big[r_0^\gamma\big]=\mathbb{E}_{s_0\sim\rho_0}\big[V_\mu(s_0)\big]\label{eq:dpgt.1}
\end{equation}
The <strong>Deterministic Policy Gradient Theorem</strong> thus states that
\begin{align}
\nabla_\theta J(\mu_\theta)&=\int_\mathcal{S}\rho_\mu(s)\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\big\vert_{a=\mu_\theta(s)}d s \\ &=\mathbb{E}_{\rho_\mu}\Big[\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\big\vert_{a=\mu_\theta(s)}\Big]\label{eq:dpgt.2}
\end{align}
where along with the assumption we have made in stochastic case, we additionally assume that $\nabla_a p(s&rsquo;\vert s,a),\mu_\theta(s),\nabla_\theta\mu_\theta(s),\nabla_a r(s,a)$ are continues for all $\theta$ and $s,s&rsquo;\in\mathcal{S},a\in\mathcal{A}$. These also imply the existence of $\nabla_a Q_\mu(s,a)$.</p><p><strong>Proof</strong><br>This proof will be quite similar to what we have used in the stochastic case. Specifically, also starting with the gradient of the value function w.r.t $\theta$, we have
\begin{align}
&\hspace{-0.5cm}\nabla_\theta V_\mu(s)\nonumber \\ &\hspace{-0.5cm}=\nabla_\theta Q_\mu(s,\mu_\theta(s)) \\ &\hspace{-0.5cm}=\nabla_\theta\left[r(s,\mu_\theta(s))+\int_\mathcal{S}\gamma p(s&rsquo;\vert s,\mu_\theta(s))V_\mu(s&rsquo;)d s&rsquo;\right] \\ &\hspace{-0.5cm}=\nabla_\theta\mu_\theta(s)\nabla_a r(s,a)\vert_{a=\mu_\theta(s)}+\nabla_\theta\int_\mathcal{S}\gamma p(s&rsquo;\vert s,\mu_\theta(s))V_\mu(s&rsquo;)d s&rsquo; \\ &\hspace{-0.5cm}=\nabla_\theta\mu_\theta(s)\nabla_a r(s,a)\vert_{a=\mu_\theta(s)}\nonumber \\ &\hspace{1.5cm}+\int_\mathcal{S}\gamma\nabla_\theta\mu_\theta(s)\nabla_a p(s&rsquo;\vert s,a)\vert_{a=\mu_\theta(s)}V_\mu(s&rsquo;)+\gamma p(s&rsquo;\vert s,a)\nabla_\theta V_\mu(s&rsquo;)d s&rsquo; \\ &\hspace{-0.5cm}=\nabla_\theta\mu_\theta(s)\nabla_a\Big(\underbrace{r(s,a)+\int_\mathcal{S}p(s&rsquo;\vert s,a)V_\mu(s&rsquo;)d s&rsquo;}_{Q_\mu(s,a)}\Big)\Big\vert_{a=\mu_\theta(s)}+\int_\mathcal{S}\gamma p(s&rsquo;\vert s,a)\nabla_\theta V_\mu(s&rsquo;)d s&rsquo; \\ &\hspace{-0.5cm}=\underbrace{\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}}_{\psi(s)}+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\mu_\theta)\nabla_\theta V_\mu(s&rsquo;)d s&rsquo; \\ &\hspace{-0.5cm}=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\mu_\theta)\nabla_\theta V_\mu(s&rsquo;)d s'
\end{align}
which is in the same form as equation \eqref{eq:spgt.9}. Thus after repeated unrolling, we also end up with
\begin{align}
\nabla_\theta V_\mu(s)&=\psi(s)+\int_\mathcal{S}\gamma p(s\to s&rsquo;,1,\mu_\theta)\nabla_\theta V_\mu(s&rsquo;)d s&rsquo; \\ &=\int_\mathcal{S}\sum_{k=0}^{\infty}\gamma^k p(s\to\tilde{s},k,\mu_\theta)\psi(\tilde{s})d\tilde{s} \\ &=\int_\mathcal{S}\sum_{k=0}^{\infty}\gamma^k p(s\to\tilde{s},k,\mu_\theta)\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}d\tilde{s}\label{eq:dpgt.3}
\end{align}
Consider the definition of $J(\mu_\theta)$ given in \eqref{eq:dpgt.1}, taking gradient of both sides w.r.t $\theta$, combined with \eqref{eq:dpgt.3} gives us
\begin{align}
&\nabla_\theta J(\mu_\theta)\nonumber \\ &=\nabla_\theta\mathbb{E}_{s_0\sim\rho_0}\big[V_\mu(s_0)\big] \\ &=\int_\mathcal{S}\rho_0(s_0)\nabla_\theta V_\mu(s_0)d s_0 \\ &=\int_{s_0\in\mathcal{S}}\rho_0(s_0)\int_{s\in\mathcal{S}}\sum_{k=0}^{\infty}\gamma^k p(s_0\to s,k,\mu_\theta)\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}d s\hspace{0.1cm}d s_0 \\ &=\int_{s\in\mathcal{S}}\left(\int_{s_0\in\mathcal{S}}\sum_{k=0}^{\infty}\gamma^k\rho_0(s_0)p(s_0\to s,k,\mu_\theta)d s_0\right)\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}d s \\ &=\int_\mathcal{S}\rho_\mu(s)\nabla_\theta\mu_\theta(s)\nabla a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}d s,
\end{align}
where in the forth step, the <strong>Fubini&rsquo;s theorem</strong> has helped us exchange the order of integration.$\tag*{$\Box$}$</p><p>It is worth remarking that we can consider a deterministic policy $\mu_\theta$ as a special case of the stochastic policy, in which $\pi_\theta(\cdot\vert s)$ becomes the <strong>Kronecker delta function</strong>, which takes the value of $1$ at only one point $a\in\mathcal{A}$ and $0$ elsewhere.</p><p>To be more specific, in the <a href=#dpg-paper>original paper</a>, the authors have shown that by rewriting the stochastic policy as $\pi_{\mu_\theta,\sigma}$, which is parameterized by a deterministic policy $\mu_\theta:\mathcal{S}\to\mathcal{A}$ and a variance parameter $\sigma$ such that for $\sigma=0$, we have that $\pi_{\mu_\theta,0}\equiv\mu_\theta$; then as $\sigma\to 0$, they have proved that the stochastic policy gradient converges to the deterministic one.</p><p>This critical result allows us to apply the deterministic policy gradient to common policy gradient frameworks, for example actor-critic approaches.</p><h2 id=deterministic-ac>Deterministic Actor-Critic<a hidden class=anchor aria-hidden=true href=#deterministic-ac>#</a></h2><h3 id=on-policy-deterministic-ac>On-policy Deterministic Actor-Critic<a hidden class=anchor aria-hidden=true href=#on-policy-deterministic-ac>#</a></h3><p>Analogous to the stochastic approach, the deterministic actor learns a parameter $\theta$ by using SGA to iteratively update the parameter vector according to the deterministic policy gradient direction \eqref{eq:dpgt.2} while the critic estimates the state-action value function by a function approximation $Q_w(s,a)$ using a policy evaluation method such as <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/>TD-learning</a>.</p><p>For instance, a deterministic actor-critic method with a <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#sarsa>Sarsa</a> critic has the following update in each time-step $t$
\begin{align}
\delta_t&=r_{t+1}+\gamma Q_w(s_{t+1},a_{t+1})-Q_w(s_t,a_t) \\ w_{t+1}&=w_t+\alpha_w\delta_t\nabla_w Q_w(s_t,a_t) \\ \theta_{t+1}&=\theta_t+\alpha_\theta\nabla_\theta\mu_\theta(s_t)\nabla_a Q_w(s_t,a_t)\vert_{a=\mu_\theta(s)},
\end{align}
where $\delta_t$ as specified before, are known as TD errors.</p><h3 id=off-policy-deterministic-ac>Off-policy Deterministic Actor-Critic<a hidden class=anchor aria-hidden=true href=#off-policy-deterministic-ac>#</a></h3><p>Analogy to stochastic methods, let $\beta(a\vert s)$ denote the behavior policy that generates trajectories used for updating the deterministic target policy $\mu_\theta(s)$, the performance objective $J(\mu_\theta)$ is then given as
\begin{align}
J_\beta(\mu_\theta)&=\int_\mathcal{S}\rho_\beta(s)V_\mu(s)d s \\ &=\int_\mathcal{S}\rho_\beta(s)Q_\mu(s,\mu_\theta(s))d s
\end{align}
It is noticeable that the deterministic policy allows us to explicitly replace the integration over action space $\mathcal{A}$ by $Q_\mu(s,\mu_\theta(s))$, and thus we do not need to use importance sampling in the actor. Hence, we have that
\begin{align}
\nabla_\theta J_\beta(\mu_\theta)&=\nabla_\theta\int_\mathcal{S}\rho_\beta(s)Q_\mu(s,\mu_\theta(s))d s \\ &\approx\int_\mathcal{S}\rho_\beta(s)\nabla_\theta\mu_\theta(a\vert s)Q_\mu(s,a)d s \\ &=\mathbb{E}_{\rho_\beta}\Big[\nabla_\theta\mu_\theta(s)\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}\Big]
\end{align}
We can also avoid using importance sampling in critic by using Q-learning
as our policy evaluation In particular, the off-policy deterministic actor-critic with a Q-learning critic has the form of
\begin{align}
\delta_t&=r_{t+1}+\gamma Q_w(s_{t+1},\mu_\theta(s))-Q_w(s_t,a_t)\label{eq:opdac.1} \\ w_{t+1}&=w_t+\alpha_w\delta_t\nabla_w Q_w(s_t,a_t) \\ \theta_{t+1}&=\theta_t+\alpha_\theta\nabla_\theta\mu_\theta(s_t)\nabla_a Q_w(s_t,a_t)\vert_{a_t=\mu_\theta(s_t)},
\end{align}
where the greedy policy, $\underset{a}{\text{argmax}}Q_w(s,a)$, in the usual Q-learning update has been replaced by the newly-updated deterministic policy, $\mu_\theta(s)$, in \eqref{eq:opdac.1}, i.e. $\mu_\theta\equiv\mu_{\theta_k}$.</p><h3 id=compatible-func-approx-deterministic>Compatible Function Approximation with Deterministic Policy<a hidden class=anchor aria-hidden=true href=#compatible-func-approx-deterministic>#</a></h3><p>From what we have mentioned in the stochastic case, we can also define an appropriate form of function approximation $Q_w$ which preserves the direction of true gradient.</p><p>In particular, A $w$-parameterized $Q_w(s,a)$ is referred as a <strong>compatible function approximator</strong> of the state-action value function $Q_\mu$ for deterministic policy $\mu_\theta$ in the sense that</p><ul id=roman-list><li><span id=prop-i-det>$\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}=\nabla_\theta\mu_\theta(s)^\text{T}w$.</span></li><li><span id=prop-ii-det>Parameters $w$ minimize the mean-squared error</span>
\begin{equation}
\text{MSE}(\theta,w)=\mathbb{E}\big[\epsilon(\theta,w,s)^\text{T}\epsilon(\theta,w,s)\big],
\end{equation}
where
\begin{equation}
\epsilon(\theta,w,s)\doteq\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}-\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}
\end{equation}</li></ul><p>then
\begin{equation}
\nabla_\theta J(\mu_\theta)=\mathbb{E}\big[\nabla_\theta\mu_\theta(s)\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}\big]\label{eq:cfad.1}
\end{equation}</p><p><strong>Proof</strong><br>We follow the procedure used in stochastic case. Specifically, starting with the property <a href=#prop-i-det>(i)</a> and by definition of $\epsilon$, we have that
\begin{align}
\nabla_w\epsilon(\theta,w,s)&=\nabla_w\big[\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}-\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}\big] \\ &=\nabla_w\big(\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}\big) \\ &=\nabla_w\big(\nabla_\theta\mu_\theta(s)^\text{T}w\big) \\ &=\nabla_\theta\mu_\theta(s)
\end{align}
Using this result, the gradient of $\text{MSE}(\theta,w)$ w.r.t $w$ is thus given as
\begin{align}
\nabla_w\text{MSE}(\theta,w)&=\nabla_w\mathbb{E}\big[\epsilon(\theta,w,s)^\text{T}\epsilon(\theta,w,s)\big] \\ &=\mathbb{E}\big[2\epsilon(\theta,w,s)\nabla_w\epsilon(\theta,w,s)\big] \\ &=2\mathbb{E}\Big[\big(\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}-\nabla_a Q_\mu(s,a)\vert_{a=\mu_\theta(s)}\big)\nabla_\theta\mu_\theta(s)\Big] \\ &=2\Big[\mathbb{E}\big[\nabla_\theta\mu_\theta(s)\nabla_a Q_w(s,a)\vert_{a=\mu_\theta(s)}\big]-J(\mu_\theta)\Big],
\end{align}
which lets our claim, \eqref{eq:cfad.1}, follows due to the property <a href=#prop-ii-det>(ii)</a>, which means that $\nabla_w\text{MSE}(\theta,w)=0$.$\tag*{$\Box$}$</p><h2 id=ddpg>Deep Deterministic Policy Gradient<a hidden class=anchor aria-hidden=true href=#ddpg>#</a></h2><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=dpg-paper>David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, Martin Riedmiller. <a href=http://proceedings.mlr.press/v32/silver14.pdf>Deterministic Policy Gradient Algorithms</a>. JMLR 2014</span>.</p><p>[2] Richard S. Sutton, Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[3] Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour. <a href=https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html>Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>. NIPS 1999.</p><p>[4] Elias M. Stein, Rami Shakarchi. <a href=http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf>Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. Princeton University Press, 2007.</p><p>[5] <span id=offpac-paper>Thomas Degris, Martha White, Richard S. Sutton. <a href=https://icml.cc/2012/papers/268.pdf>Off-Policy Actor-Critic</a>. ICML 2012</span>.</p><p>[6] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra. <a href=https://arxiv.org/pdf/1509.02971.pdf>Continuous control with deep reinforcement learning</a>. ICLR 2016.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>To simplify the notation, we have let $\rho_\pi\doteq\rho_{\pi_\theta}$ and $Q_\pi\doteq Q_{\pi_\theta}$ implicitly. As a result, we will also denote $V_{\pi_\theta}$ by $V_\pi$.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The notation $r(\pi)$ of the <strong>average reward</strong> is just a notation-abused and should not be confused with notation $r$ of the reward function.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>This means that if we keep selecting action according to $\pi_\theta$, we remains in the same state distribution $\bar{\rho}_\pi$, i.e.
\begin{equation}
\int_\mathcal{S}\bar{\rho}_\pi(s)\int_\mathcal{A}\pi_\theta(a\vert s)p(s&rsquo;\vert s,a)da\hspace{0.1cm}d s=\bar{\rho}_\pi(s&rsquo;)\label{eq:fn.1}
\end{equation}&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/policy-gradient/>policy-gradient</a></li><li><a href=https://trunghng.github.io/tags/actor-critic/>actor-critic</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/machine-learning/pgm-representation/><span class=title>« Prev</span><br><span>Probabilistic Graphical Model - Representation</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/trpo/><span class=title>Next »</span><br><span>Trust Region Policy Optimization</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on twitter" href="https://twitter.com/intent/tweet/?text=Deterministic%20Policy%20Gradients&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f&hashtags=reinforcement-learning%2cpolicy-gradient%2cactor-critic%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f&title=Deterministic%20Policy%20Gradients&summary=Deterministic%20Policy%20Gradients&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f&title=Deterministic%20Policy%20Gradients"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on whatsapp" href="https://api.whatsapp.com/send?text=Deterministic%20Policy%20Gradients%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deterministic Policy Gradients on telegram" href="https://telegram.me/share/url?text=Deterministic%20Policy%20Gradients&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeterministic-policy-gradients%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>