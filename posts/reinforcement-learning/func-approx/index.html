<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Function Approximation | Trung's Place</title><meta name=keywords content="reinforcement-learning,function-approximation,td-learning,importance-sampling,my-rl"><meta name=description content="
All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list{counter-reset:section}#roman-list,#number-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-.75em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Function Approximation"><meta property="og:description" content="
All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/func-approx/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-11T15:26:00+07:00"><meta property="article:modified_time" content="2022-02-11T15:26:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Function Approximation"><meta name=twitter:description content="
All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Function Approximation","item":"https://trunghng.github.io/posts/reinforcement-learning/func-approx/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Function Approximation","name":"Function Approximation","description":" All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.\n","keywords":["reinforcement-learning","function-approximation","td-learning","importance-sampling","my-rl"],"articleBody":" All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.\nOn-policy Methods So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.\nValue-function Approximation All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value” (or update target) for that state \\begin{equation} s\\mapsto u, \\end{equation} where $s$ is the state updated and $u$ is the update target that $s$’s estimated value is shifted toward.\nFor example,\nthe MC update for value prediction is: $S_t\\mapsto G_t$. the TD(0) update for value prediction is: $S_t\\mapsto R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)$. the $n$-step TD update is: $S_t\\mapsto G_{t:t+n}$. and in the DP, policy-evaluation update, $s\\mapsto\\mathbb{E}\\big[R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)\\vert S_t=s\\big]$, an arbitrary $s$ is updated. Each update $s\\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process function approximation.\nThe Prediction Objective In contrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is impossible to find the exact value function of all states. And moreover, an update at one state also affects many others.\nHence, it is necessary to specify a state distribution $\\mu(s)\\geq0,\\sum_s\\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\\hat{v}(s,\\mathbf{w})$ and the true value $v_\\pi(s)$) in each state $s$. Weighting this over the state space $\\mathcal{S}$ by $\\mu$, we obtain a natural objective function, called the Mean Squared Value Error, denoted as $\\overline{\\text{VE}}$: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})\\doteq\\sum_{s\\in\\mathcal{S}}\\mu(s)\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} The distribution $\\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divided by total amount of visits). Under on-policy training this is called the on-policy distribution.\nIn continuing tasks, the on-policy distribution is the stationary distribution under $\\pi$. In episodic tasks, the on-policy distribution depends on how the initial states are chosen. Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode \\begin{equation} \\eta(s)=h(s)+\\sum_\\bar{s}\\eta(\\bar{s})\\sum_a\\pi(a\\vert\\bar{s})p(s\\vert\\bar{s},a),\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} This system of equation can be solved for the expected number of visits $\\eta(s)$. The on-policy distribution is then \\begin{equation} \\mu(s)=\\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)},\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} Gradient-based algorithms To solve the least squares problem, we are going to use a popular method, named Gradient descent.\nSay, consider a differentiable function $J(\\mathbf{w})$ of parameter vector $\\mathbf{w}$.\nThe gradient of $J(\\mathbf{w})$ w.r.t $\\mathbf{w}$ is defined to be \\begin{equation} \\nabla_{\\mathbf{w}}J(\\mathbf{w})=\\left(\\begin{smallmatrix}\\dfrac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}_1} \\\\ \\vdots \\\\ \\dfrac{\\partial J(\\mathbf{w})}{\\partial\\mathbf{w}_d}\\end{smallmatrix}\\right) \\end{equation} The idea of Gradient descent is to minimize the objective function $J(\\mathbf{w})$, we repeatedly move $\\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\\nabla_\\mathbf{w}J(\\mathbf{w})$.\nThus, we have the update rule of Gradient descent: \\begin{equation} \\mathbf{w}:=\\mathbf{w}-\\dfrac{1}{2}\\alpha\\nabla_\\mathbf{w}J(\\mathbf{w}), \\end{equation} where $\\alpha$ is a positive step-size parameter.\nStochastic-gradient Apply gradient descent to our problem, which is we have to find the minimization of \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})=\\sum_{s\\in\\mathcal{S}}\\mu(s)\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} Since $\\mu(s)$ is the state distribution over state space $\\mathcal{S}$, we can rewrite $\\overline{\\text{VE}}$ as \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w})=\\mathbb{E}_{s\\sim\\mu}\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]^2 \\end{equation} By the update we have defined earlier, in each step, we need to decrease $\\mathbf{w}$ by an amount of \\begin{equation} \\Delta\\mathbf{w}=-\\dfrac{1}{2}\\alpha\\nabla_\\mathbf{w}\\overline{\\text{VE}}(\\mathbf{w})=\\alpha\\mathbb{E}\\Big[v_\\pi(s)-\\hat{v}(s,\\mathbf{w})\\Big]\\nabla_\\mathbf{w}\\hat{v}(s,\\mathbf{w}) \\end{equation} Assume that, on each step, we observe a new example $S_t\\mapsto v_\\pi(S_t)$ consisting of a state $S_t$ and its true value under the policy $\\pi$.\nUsing Stochastic Gradient descent (SGD), we adjust the weight vector after each example by a small amount in the direction that would most reduce the error on that example: \\begin{align} \\mathbf{w}_{t+1}\u0026\\doteq\\mathbf{w}_t-\\frac{1}{2}\\alpha\\nabla_\\mathbf{w}\\big[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbf{w}_t)\\big]^2 \\\\ \u0026=\\mathbf{w}_t+\\alpha\\big[v_\\pi(S_t)-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\label{eq:sg.1} \\end{align} When the target output, here denoted as $U_t\\in\\mathbb{R}$, of the $t$-th training example, $S_t\\mapsto U_t$, is not the true value, $v_\\pi(S_t)$, but some approximation to it, we cannot perform the exact update \\eqref{eq:sg.1} since $v_\\pi(S_t)$ is unknown, but we can approximate it by substituting $U_t$ in place of $v_\\pi(S_t)$. This yield the following general SGD method for state-value prediction: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[U_t-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t)\\label{eq:sg.2} \\end{equation} If $U_t$ is an unbiased estimate of $v_\\pi(S_t)$, i.e., $\\mathbb{E}\\left[U_t\\vert S_t=s\\right]=v_\\pi(S_t)$, for each $t$, then $\\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic conditions for decreasing $\\alpha$.\nIn particular, since the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t\\doteq G_t$, we have that the SGD version of Monte Carlo state-value prediction, \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[G_t-\\hat{v}(S_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} is guaranteed to converge to a local optimal point.\nWe have the pseudocode of the algorithm\nSemi-gradient If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\\sum_{a,s’,r}\\pi(a\\vert S_t)p(s’,r\\vert S_t,a)\\left[r+\\gamma\\hat{v}(s’,\\mathbf{w}_t)\\right]$, which all depend on the current value of the weight vector $\\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.\nSuch methods are called semi-gradient since they include only a part of the gradient.\nLinear Function Approximation One of the most crucial special cases of function approximation is that in which the approximate function, $\\hat{v}(\\cdot,\\mathbf{w})$, is a linear function of the weight vector, $\\mathbf{w}$.\nCorresponding to every state $s$, there is a real-valued vector $\\mathbf{x}(s)\\doteq\\left(x_1(s),x_2(s),\\dots,x_d(s)\\right)$, with the same number of components with $\\mathbf{w}$.\nLinear Methods Linear methods approximate value function by the inner product between $\\mathbf{w}$ and $\\mathbf{x}(s)$: \\begin{equation} \\hat{v}(s,\\mathbf{w})\\doteq\\mathbf{w}^\\text{T}\\mathbf{x}(s)=\\sum_{i=1}^{d}w_ix_i(s)\\label{eq:lm.1} \\end{equation} The vector $\\mathbf{x}(s)$ is called a feature vector representing state $s$, i.e., $x_i:\\mathcal{S}\\to\\mathbb{R}$.\nFor linear methods, features are basis functions because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.\nFrom \\eqref{eq:lm.1}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\\mathbf{w}$ is \\begin{equation} \\nabla_\\mathbf{w}\\hat{v}(s,\\mathbf{w})=\\mathbf{x}(s) \\end{equation} Thus, with linear approximation, the SGD update can be rewrite as \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\left[G_t-\\hat{v}(S_t,\\mathbf{w}_t)\\right]\\mathbf{x}(S_t) \\end{equation}\nIn the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.\nThe gradient MC algorithm in the previous section converges to the global optimum of the $\\overline{\\text{VE}}$ under linear function approximation if $\\alpha$ is reduced over time according to the usual conditions. In particular, it converges to the fixed point, called $\\mathbf{w}_{\\text{MC}}$, with: \\begin{align} \\nabla_{\\mathbf{w}_{\\text{MC}}}\\mathbb{E}\\left[\\big(G_t-v_{\\mathbf{w}_{\\text{MC}}}(S_t)\\big)^2\\right]\u0026=0 \\\\ \\mathbb{E}\\Big[\\big(G_t-v_{\\mathbf{w}_{\\text{MC}}}(S_t)\\big)\\mathbf{x}_t\\Big]\u0026=0 \\\\ \\mathbb{E}\\Big[(G_t-\\mathbf{x}_t^\\text{T}\\mathbf{w}_{\\text{MC}})\\mathbf{x}_t\\Big]\u0026=0 \\\\ \\mathbb{E}\\left[G_t\\mathbf{x}_t-\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\mathbf{w}_{\\text{MC}}\\right]\u0026=0 \\\\ \\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]\\mathbf{w}_\\text{MC}\u0026=\\mathbb{E}\\left[G_t\\mathbf{x}_t\\right] \\\\ \\mathbf{w}_\\text{MC}\u0026=\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[G_t\\mathbf{x}_t\\right] \\end{align} The semi-gradient TD algorithm also converges under linear approximation. Recall that, at each time $t$, the semi-gradient TD update is \\begin{align} \\mathbf{w}_{t+1}\u0026\\doteq\\mathbf{w}_t+\\alpha\\left(R_{t+1}+\\gamma\\mathbf{w}_t^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}_t^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t \\\\ \u0026=\\mathbf{w}_t+\\alpha\\left(R_{t+1}\\mathbf{x}_t-\\mathbf{x}_t(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1})^\\text{T}\\mathbf{w}_t\\right), \\end{align} where $\\mathbf{x}_t=\\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\\mathbf{w}_t$, the expected next weight vector can be written as \\begin{equation} \\mathbb{E}\\left[\\mathbf{w}_{t+1}\\vert\\mathbf{w}_t\\right]=\\mathbf{w}_t+\\alpha\\left(\\mathbf{b}-\\mathbf{A}\\mathbf{w}_t\\right),\\label{eq:lm.2} \\end{equation} where \\begin{align} \\mathbf{b}\u0026\\doteq\\mathbb{E}\\left[R_{t+1}\\mathbf{x}_t\\right]\\in\\mathbb{R}^d, \\\\ \\mathbf{A}\u0026\\doteq\\mathbb{E}\\left[\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right]\\in\\mathbb{R}^d\\times\\mathbb{R}^d\\label{eq:lm.3} \\end{align} From \\eqref{eq:lm.2}, it is easily seen that if the system converges, it must converges to the weight vector $\\mathbf{w}_{\\text{TD}}$ at which \\begin{align} \\mathbf{b}-\\mathbf{A}\\mathbf{w}_{\\text{TD}}\u0026=\\mathbf{0} \\\\ \\mathbf{w}_{\\text{TD}}\u0026=\\mathbf{A}^{-1}\\mathbf{b} \\end{align} This quantity, $\\mathbf{w}_{\\text{TD}}$, is called the TD fixed point. And in fact, linear semi-gradient TD(0) converges to this point1. At the TD fixed point, it has also been proven (in the continuing case) that $\\overline{\\text{VE}}$ is within a bounded expansion of the lowest possible error, while the Monte Carlo solutions minimize the value error $\\overline{\\text{VE}}$: \\begin{equation} \\overline{\\text{VE}}(\\mathbf{w}_{\\text{TD}})\\leq\\dfrac{1}{1-\\gamma}\\overline{\\text{VE}}(\\mathbf{w}_{\\text{MC}})=\\dfrac{1}{1-\\gamma}\\min_{\\mathbf{w}}\\overline{\\text{VE}}(\\mathbf{w}) \\end{equation} Based on the tabular $n$-step TD we have defined before, applying the semi-gradient method, we have the function approximation version of its, called semi-gradient $\\boldsymbol{n}$-step TD, can be defined as: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\left[G_{t:t+n}-\\hat{v}(S_t,\\mathbf{w}_{t+n-1})\\right]\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\leq t\\lt T \\end{equation} where the $n$-step return is generalized from the tabular version: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{v}(S_{t+n},\\mathbf{w}_{t+n-1}),\\hspace{1cm}0\\geq t\\geq T-n \\end{equation} We therefore have the pseudocode of the semi-gradient $n$-step TD algorithm.\nFeature Construction There are various ways to define features. The simplest way is to use each variable directly as a basis function along with a constant function, i.e., setting: \\begin{equation} x_0(s)=1;\\hspace{1cm}x_i(s)=s_i,0\\leq i\\leq d \\end{equation} However, most interesting value functions are too complex to be represented in this way. This scheme therefore was generalized into the polynomial basis.\nPolynomial Basis Suppose each state $s$ corresponds to $d$ numbers, $s_1,s_2\\dots,s_d$, with each $s_i\\in\\mathbb{R}$. For this $d$-dimensional state space, each order-$n$ polynomial basis feature $x_i$ can be written as \\begin{equation} x_i(s)=\\prod_{j=1}^{d}s_j^{c_{i,j}}, \\end{equation} where each $c_{i,j}\\in\\{0,1,\\dots,n\\}$ for an integer $n\\geq 0$. These features make up the order-$n$ polynomial basis for dimension $d$, which contains $(n+1)^d$ different features.\nFourier Basis The Univariate Fourier Series Fourier series is applied widely in Mathematics to approximate a periodic function2. For example:\nFigure 1: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\\theta)=\\frac{4\\sin\\theta}{\\pi},f_2(\\theta)=\\frac{4\\sin 3\\theta}{3\\pi},f_3(\\theta)=\\frac{4\\sin 5\\theta}{5\\pi}$ and $f_4(\\theta)=\\frac{4\\sin 7\\theta}{7\\pi}$. The code can be found here In particular, the $n$-degree Fourier expansion of $f$ with period $\\tau$ is \\begin{equation} \\bar{f}(x)=\\dfrac{a_0}{2}+\\sum_{k=1}^{n}\\left[a_k\\cos\\left(k\\frac{2\\pi}{\\tau}x\\right)+b_k\\left(k\\frac{2\\pi}{\\tau}x\\right)\\right], \\end{equation} where \\begin{align} a_k\u0026=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\cos\\left(\\frac{2\\pi kx}{\\tau}\\right)\\hspace{0.1cm}dx, \\\\ b_k\u0026=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi kx}{\\tau}\\right)\\hspace{0.1cm}dx \\end{align} In the RL setting, $f$ is unknown so we cannot compute $a_0,\\dots,a_n$ and $b_1,\\dots,b_n$, but we can instead treat them as parameters in a linear function approximation scheme, with \\begin{equation} \\phi_i(x)=\\begin{cases}1 \u0026\\text{if }i=0 \\\\ \\cos\\left(\\frac{(i+1)\\pi x}{\\tau}\\right) \u0026\\text{if }i\u003e0,i\\text{ odd} \\\\ \\sin\\left(\\frac{i\\pi x}{\\tau}\\right) \u0026\\text{if }i\u003e0,i\\text{ even}\\end{cases} \\end{equation} Thus, a full $n$-th order Fourier approximation to a one-dimensional value function results in a linear function approximation with $2n+1$ terms.\nEven, Odd and Non-Periodic Functions If $f$ is known to be even (i.e., $f(x)=f(-x)$), then $\\forall i\u003e0$, we have: \\begin{align} b_i\u0026=\\frac{2}{\\tau}\\int_{0}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx \\\\ \u0026=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x-\\tau)\\sin\\left(\\frac{2\\pi ix}{\\tau}-2\\pi i\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{\\tau/2}^{\\tau}f(x-\\tau)\\sin\\left(\\frac{2\\pi i(x-\\tau)}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026=\\frac{2}{\\tau}\\left[\\int_{0}^{\\tau/2}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx+\\int_{-\\tau/2}^{0}f(x)\\sin\\left(\\frac{2\\pi ix}{\\tau}\\right)\\hspace{0.1cm}dx\\right] \\\\ \u0026=0, \\end{align} so the $\\sin$ terms can be dropped, which reduces the terms required for an $n$-th order Fourier approximation to $n+1$.\nSimilarly, if $f$ is known to be odd (i.e., $f(x)=-f(-x)$), then $\\forall i\u003e0, a_i=0$, so we can omit the $\\cos$ terms.\nHowever, in general, value functions are not even, odd, or periodic (or known to be in advance). In such cases, if $f$ is defined over a bounded interval with length, let us assume, $\\tau$, or without loss of generality, $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, but only project the input variable to $\\left[0,\\frac{\\tau}{2}\\right]$. This results in a function periodic on $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, but unconstrained on $\\left(0,\\frac{\\tau}{2}\\right]$. We are now free to choose whether or not the function is even or odd over $\\left[-\\frac{\\tau}{2},\\frac{\\tau}{2}\\right]$, and can drop half of the terms in the approximation.\nIn general, we expect it will be better to use the “half-even” approximation and drop the $\\sin$ terms because this causes only a slight discontinuity at the origin. Thus, we can define the univariate $n$-th order Fourier basis as: \\begin{equation} x_i(s)=\\cos(i\\pi s), \\end{equation} for $i=0,\\dots,n$.\nThe Multivariate Fourier Series The $n$-order Fourier expansion of the multivariate function $F$ with period $\\tau$ in $d$ dimensions is \\begin{equation} \\overline{F}(\\mathbf{x})=\\sum_\\mathbf{c}\\left[a_\\mathbf{c}\\cos\\left(\\frac{2\\pi}{\\tau}\\mathbf{c}\\cdot\\mathbf{x}\\right)+b_\\mathbf{c}\\sin\\left(\\frac{2\\pi}{\\tau}\\mathbf{c}\\cdot\\mathbf{x}\\right)\\right], \\end{equation} where $\\mathbf{c}=(c_1,\\dots,c_d)^\\text{T},c_i\\in\\left[0,\\dots,n\\right],1\\leq i\\leq d$.\nThis results in $2(n+1)^d$ basis functions for an $n$-th order full Fourier approximation to a value function in $d$ dimensions, which can be reduced to $(n+1)^d$ if we drop either the $sin$ or $cos$ terms for each variable as described above. Thus, we can define the $n$-th order Fourier basis in the multi-dimensional case as:\nSuppose each state $s$ corresponds to a vector of $d$ numbers, $\\mathbf{s}=(s_1,\\dots,s_d)^\\text{T}$, with each $s_i\\in[0,1]$. The $i$-th feature in the order-$n$ Fourier cosine basis can then be written as: \\begin{equation} x_i(s)=\\cos\\left(\\pi\\mathbf{s}^\\text{T}\\mathbf{c}^i\\right), \\end{equation} where $\\mathbf{c}=(c_1^i,\\dots,c_d^i)^\\text{T}$, with $c_j^i\\in\\{0,\\dots,n\\}$ for $j=1,\\dots,d$ and $i=0,\\dots,(n+1)^d$.\nThis defines a feature for each of the $(n+1)^d$ possible integer vector $\\mathbf{c}^i$. The inner product $\\mathbf{s}^\\text{T}\\mathbf{c}^i$ has the effect of assigning an integer in $\\{0,\\dots,n\\}$ to each dimension of $\\mathbf{s}$. As in the one-dimensional case, this integer determines the feature’s frequency along that dimension. The feature thus can be shifted and scaled to suit the bounded state space of a particular application.\nFigure 2: Fourier basis vs Polynomial basis on the 1000-state random walk\n(RL book - Example 9.2).\nThe code can be found here Coarse Coding Figure 3: Using linear function approximation based on coarse coding to learn a one-dimensional square-wave function (RL book - Example 9.3).\nThe code can be found here Tile Coding Figure 4: Gradient Monte Carlo with single tiling and with multiple tilings on the 1000-state random walk\n(RL book - Example 9.2).\nThe code can be found here Radial Basis Functions Another common scheme is Radial Basis Functions (RBFs). RBFs are the natural generalization of coarse coding to continuous valued features. Rather than each feature taking either $0$ or $1$, it can be anything within $[0,1]$, reflecting various degrees to which the feature is present.\nA typical RBF feature, $x_i$, has a Gaussian response $x_i(s)$ dependent only on the distance between the state, $s$, and the feature’s prototypical or center state, $c_i$, and relative to the feature’s width, $\\sigma_i$: \\begin{equation} x_i(s)\\doteq\\exp\\left(\\frac{\\Vert s-c_i\\Vert^2}{2\\sigma_i^2}\\right) \\end{equation} The figures below shows a one-dimensional example with a Euclidean distance metric.\nFigure 5: One-dimensional RBFs Least-Squares TD Recall when using TD(0) with linear function approximation, $v_\\mathbf{w}(s)=\\mathbf{w}^\\text{T}\\mathbf{x}(s)$, we need to find a point $\\mathbf{w}$ such that \\begin{equation} \\mathbb{E}\\Big[\\big(R_{t+1}+\\gamma v_\\mathbf{w}(S_{t+1})-v_{\\mathbf{w}}(S_t)\\big)\\mathbf{x}_t\\Big]=\\mathbf{0}\\label{eq:lstd.1} \\end{equation} or \\begin{equation} \\mathbb{E}\\Big[R_{t+1}\\mathbf{x}_t-\\mathbf{x}_t(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1})^\\text{T}\\mathbf{w}_t\\Big]=\\mathbf{0} \\end{equation} We found out that the solution is: \\begin{equation} \\mathbf{w}_{\\text{TD}}=\\mathbf{A}^{-1}\\mathbf{b}, \\end{equation} where \\begin{align} \\mathbf{A}\u0026\\doteq\\mathbb{E}\\left[\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right], \\\\ \\mathbf{b}\u0026\\doteq\\mathbb{E}\\left[R_{t+1}\\mathbf{x}_t\\right] \\end{align} Instead of computing these expectations over all possible states and all possible transitions that could happen, we now only care about the things that did happen. In particular, we now consider the empirical loss of \\eqref{eq:lstd.1}, as: \\begin{equation} \\frac{1}{t}\\sum_{k=0}^{t-1}\\big(R_{k+1}+\\gamma v_\\mathbf{w}(S_{k+1})-v_{\\mathbf{w}}(S_k)\\big)\\mathbf{x}_i=\\mathbf{0}\\label{eq:lstd.2} \\end{equation} By the law of large numbers3, when $t\\to\\infty$, \\eqref{eq:lstd.2} converges to its expectation, which is \\eqref{eq:lstd.1}. Hence, we now just have to compute the estimate of $\\mathbf{w}_{\\text{TD}}$, called $\\mathbf{w}_{\\text{LSTD}}$ (as LSTD stands for Least-Squares TD), which is defined as: \\begin{equation} \\mathbf{w}_{\\text{LSTD}}\\doteq\\left(\\sum_{k=0}^{t-1}\\mathbf{x}_i\\left(\\mathbf{x}_k-\\gamma\\mathbf{x}_{k+1}\\right)^\\text{T}\\right)^{-1}\\left(\\sum_{k=1}^{t-1}R_{k+1}\\mathbf{x}_k\\right)\\label{eq:lstd.3} \\end{equation} In other words, our work is to compute estimates $\\widehat{\\mathbf{A}}_t$ and $\\widehat{\\mathbf{b}}_t$ of $\\mathbf{A}$ and $\\mathbf{b}$: \\begin{align} \\widehat{\\mathbf{A}}_t\u0026\\doteq\\sum_{k=0}^{t-1}\\mathbf{x}_k\\left(\\mathbf{x}_k-\\gamma\\mathbf{x}_{k+1}\\right)^\\text{T}+\\varepsilon\\mathbf{I};\\label{eq:lstd.4} \\\\ \\widehat{\\mathbf{b}}_t\u0026\\doteq\\sum_{k=0}^{t-1}R_{k+1}\\mathbf{x}_k,\\label{eq:lstd.5} \\end{align} where $\\mathbf{I}$ is the identity matrix, and $\\varepsilon\\mathbf{I}$, for some small $\\varepsilon\u003e0$, ensures that $\\widehat{\\mathbf{A}}_t$ is always invertible. Thus, \\eqref{eq:lstd.3} can be rewritten as: \\begin{equation} \\mathbf{w}_{\\text{LSTD}}\\doteq\\widehat{\\mathbf{A}}_t^{-1}\\widehat{\\mathbf{b}}_t \\end{equation} The two approximations in \\eqref{eq:lstd.4} and \\eqref{eq:lstd.5} could be implemented incrementally using the same technique we used to apply earlier so that they can be done in constant time per step. Even so, the update for $\\widehat{\\mathbf{A}}_t$ would have the computational complexity of $O(d^2)$, and so is its memory required to hold the $\\widehat{\\mathbf{A}}_t$ matrix.\nThis leads to a problem that our next step, which is the computation of the inverse $\\widehat{\\mathbf{A}}_t^{-1}$ of $\\widehat{\\mathbf{A}}_t$, is going to be $O(d^3)$. Fortunately, with the so-called Sherman-Morrison formula, an inverse of our special form matrix - a sum of outer products - can also be updated incrementally with only $O(d^2)$ computations, as \\begin{align} \\widehat{\\mathbf{A}}_t^{-1}\u0026=\\left(\\widehat{\\mathbf{A}}_t+\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\right)^{-1} \\\\ \u0026=\\widehat{\\mathbf{A}}_{t-1}^{-1}-\\frac{\\widehat{\\mathbf{A}}_{t-1}^{-1}\\mathbf{x}_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\widehat{\\mathbf{A}}_{t-1}^{-1}}{1+\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)^\\text{T}\\widehat{\\mathbf{A}}_{t-1}^{-1}\\mathbf{x}_t}, \\end{align} for $t\u003e0$, with $\\mathbf{\\widehat{A}}_0\\doteq\\varepsilon\\mathbf{I}$.\nFor the estimate $\\widehat{\\mathbf{b}}_t$ of $\\mathbf{b}$, it can be updated using naive approach: \\begin{equation} \\widehat{\\mathbf{b}}_{t+1}=\\widehat{\\mathbf{b}}_t+R_{t+1}\\mathbf{x}_t \\end{equation} The pseudocode for LSTD is given below\nEpisodic Semi-gradient Sarsa We now consider the control problem, with parametric approximation of the action-value function $\\hat{q}(s,a,\\mathbf{w})\\approx q_*(s,a)$, where $\\mathbf{w}\\in\\mathbb{R}^d$ is a finite-dimensional weight vector.\nSimilar to the prediction problem, we can apply semi-gradient methods in solving the control problem. The difference is rather than considering training examples of the form $S_t\\mapsto U_t$, we now consider examples of the form $S_t,A_t\\mapsto U_t$.\nFrom \\eqref{eq:sg.2}, we can derive the general SGD update for action-value prediction as \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[U_t-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:esgs.1} \\end{equation} The update for the one-step Sarsa method therefore would be \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\big[R_{t+1}+\\gamma\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:esgs.2} \\end{equation} We call this method episodic semi-gradient one-step Sarsa.\nTo form the control method, we need to couple the action-value\nThe following figure illustrates the cost-to-go function $\\max_a\\hat{q}(s,a,\\mathbf{w})$ learned during one run of the semi-gradient Sarsa on Mountain Car task.\nFigure 6: Cost-to-go learned during one run of Semi-gradient Sarsa on Mountain Car problem\n(RL book - Example 10.1).\nThe code can be found here Episodic Semi-gradient $\\boldsymbol{n}$-step Sarsa Similar to how we defined the one-step Sarsa version of semi-gradient, we can replace the update target in \\eqref{eq:esgs.1} by an $n$-step return, \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}),\\label{eq:esgnss.1} \\end{equation} for $t+n\\lt T$, with $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$, as usual, to obtain the semi-gradient $n$-step Sarsa update: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} for $0\\leq t\\lt T$. The pseudocode is given below.\nThe figure below shows how the $n$-step ($8$-step in particular) tends to learn faster than the one-step algorithm.\nFigure 7: Performance of one-step vs 8-step Semi-gradient Sarsa on Mountain Car task\n(RL book).\nThe code can be found here Average Reward We now consider a new setting for continuing tasks - alongside the episodic and discounted settings - average reward.\nIn the average-reward setting, the quality of a policy $\\pi$ is defined as the average rate of reward, or simply average reward, while following that policy, which we denote as $r(\\pi)$: \\begin{align} r(\\pi)\u0026\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=1}^{h}\\mathbb{E}\\Big[R_t\\vert S_0,A_{0:t-1}\\sim\\pi\\Big] \\\\ \u0026=\\lim_{t\\to\\infty}\\mathbb{E}\\Big[R_t\\vert S_0,A_{0:t-1}\\sim\\pi\\Big] \\\\ \u0026=\\sum_s\\mu_\\pi(s)\\sum_a\\pi(a\\vert s)\\sum_{s’,r}p(s’,r\\vert s,a)r, \\end{align} where:\nthe expectations are conditioned on the initial state $S_0$, and on the subsequent action $A_0,A_1,\\dots,A_{t-1}$, being taken according to $\\pi$; $\\mu_\\pi$ is the steady-state distribution, \\begin{equation} \\mu_\\pi\\doteq\\lim_{t\\to\\infty}P\\left(S_t=s\\vert A_{0:t-1}\\sim\\pi\\right), \\end{equation} which is assumed to exist for any $\\pi$ and to be independent of $S_0$. The steady state distribution is the special distribution under which, if we select actions according to $\\pi$, we remain in the same distribution. That is, for which \\begin{equation} \\sum_s\\mu_\\pi(x)\\sum_a\\pi(a\\vert s)p(s’\\vert s,a)=\\mu_\\pi(s’) \\end{equation} In the average-reward setting, returns are defined in terms of differences between rewards and the average reward: \\begin{equation} G_t\\doteq R_{t+1}-r(\\pi)+R_{t+2}-r(\\pi)+R_{t+3}-r(\\pi)+\\dots\\label{eq:ar.1} \\end{equation} This is known as the differential return, and the corresponding value functions are known as differential value functions, $v_\\pi(s)$ and $q_\\pi(s,a)$, which are defined in the same way as we have done before: \\begin{align} v_\\pi(s)\u0026\\doteq\\mathbb{E}\\big[G_t\\vert S_t=s\\big]; \\\\ q_\\pi(s,a)\u0026\\doteq\\mathbb{E}\\big[G_t\\vert S_t=s,A_t=a\\big], \\end{align} and similarly for $v_{*}$ and $q_{*}$. Likewise, differential value functions also have Bellman equations, with some modifications by replacing all discounted factor $\\gamma$ and replacing all rewards, $r$, by the difference between the reward and the true average reward, $r-r(\\pi)$, as: \\begin{align} \u0026v_\\pi(s)=\\sum_a\\pi(a|s)\\sum_{r,s’}p(r,s’|s,a)\\left[r-r(\\pi)+v_\\pi(s’)\\right], \\\\ \u0026q_\\pi(s,a)=\\sum_{r,s’}p(s’,r|s,a)\\left[r-r(\\pi)+\\sum_{a’}\\pi(a’|s’)q_\\pi(s’,a’)\\right], \\\\ \u0026v_{*}(s)=\\max_a\\sum_{r,s’}p(s’,r|s,a)\\left[r-\\max_\\pi r(\\pi)+v_{*}(s’)\\right], \\\\ \u0026q_{*}(s,a)=\\sum_{r,s’}p(s’,r|s,a)\\left[r-\\max_\\pi r(\\pi)+\\max_{a’}q_{*}(s’,a’)\\right] \\end{align}\nDifferential Semi-gradient Sarsa There is also a differential form of the two TD errors: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}_{t+1}+\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t), \\end{equation} and \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}_{t+1}+\\hat{q}(S_{t+1},A_{t+1},\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t),\\label{eq:dsgs.1} \\end{equation} where $\\bar{R}_t$ is an estimate at time $t$ of the average reward $r(\\pi)$.\nWith these alternative definitions, most of our algorithms and many theoretical results carry through to the average-reward setting without change.\nFor example, the average reward version of semi-gradient Sarsa is defined just as in \\eqref{eq:esgs.2} except with the differential version of the TD error \\eqref{eq:dsgs.1}: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_t)\\label{eq:dsgs.2} \\end{equation} The pseudocode of the algorithm is then given below.\nDifferential Semi-gradient $\\boldsymbol{n}$-step Sarsa To derive the $n$-step version of \\eqref{eq:dsgs.2}, we use the same update rule, except with an $n$-step version of the TD error.\nFirst, we need to define the $n$-step differential return, with function approximation, by combining the idea of \\eqref{eq:esgnss.1} and \\eqref{eq:ar.1} together, as: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}-\\bar{R}_{t+1}+R_{t+2}-\\bar{R}_{t+2}+\\dots+R_{t+n}-\\bar{R}_{t+n}+\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}), \\end{equation} where $\\bar{R}$ is an estimate of $r(\\pi),n\\geq 1$, $t+n\\lt T$; $G_{t:t+n}\\doteq G_t$ if $t+n\\geq T$ as usual. The $n$-step TD error is then \\begin{equation} \\delta_t\\doteq G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}) \\end{equation} The pseudocode of the algorithm is then given below.\nOff-policy Methods We now consider off-policy methods with function approximation.\nSemi-gradient To derive the semi-gradient form of off-policy tabular methods we have known, we simply replace the update to an array ($V$ or $Q$) to an update to a weight vector $\\mathbf{w}$, using the approximate value function $\\hat{v}$ or $\\hat{q}$ and its gradient.\nRecall that in off-policy learning we seek to learn a value function for a target policy $\\pi$, given data due to a different behavior policy $b$.\nMany of these algorithms use the per-step importance sampling ratio: \\begin{equation} \\rho_t\\doteq\\rho_{t:t}=\\dfrac{\\pi(A_t|S_t)}{b(A_t|S_t)} \\end{equation}\nIn particular, for state-value functions, the one-step algorithm is semi-gradient off-policy TD(0) has the update rule: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\rho_t\\delta_t\\nabla_\\mathbf{w}\\hat{v}(S_t,\\mathbf{w}_t),\\label{eq:opsg.1} \\end{equation} where\nIf the problem is episodic and discounted, we have: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} If the problem is continuing and undiscounted using average reward, we have: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}+\\hat{v}(S_{t+1},\\mathbf{w}_t)-\\hat{v}(S_t,\\mathbf{w}_t) \\end{equation} For action values, the one-step algorithm is semi-gradient Expected Sarsa, which has the update rule: \\begin{equation} \\mathbf{w}_{t+1}\\doteq\\mathbf{w}_t+\\alpha\\delta_t\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}), \\end{equation} with\nEpisodic tasks: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma\\sum_a\\pi(a\\vert S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} Continuing tasks: \\begin{equation} \\delta_t\\doteq R_{t+1}-\\bar{R}+\\sum_a\\pi(a\\vert S_{t+1})\\hat{q}(S_{t+1},a,\\mathbf{w}_t)-\\hat{q}(S_t,A_t,\\mathbf{w}_t) \\end{equation} With multi-step algorithms, we begin with semi-gradient $\\boldsymbol{n}$-step Expected Sarsa, which has the update rule: \\begin{equation} \\hspace{-0.8cm}\\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\rho_{t+1}\\dots\\rho_{t+n-1}\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} where $\\rho_k=1$ for $k\\geq T$ and $G_{t:n}\\doteq G_t$ if $t+n\\geq T$, and with\nEpisodic tasks: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}) \\end{equation} Continuing tasks: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}-\\bar{R}_t+\\dots+R_{t+n}-\\bar{R}_{t+n-1}+\\hat{q}(S_{t+n},A_{t+n},\\mathbf{w}_{t+n-1}), \\end{equation} For the semi-gradient version of $n$-step tree-backup, called semi-gradient $\\boldsymbol{n}$-step tree-backup, the update rule is: \\begin{equation} \\mathbf{w}_{t+n}\\doteq\\mathbf{w}_{t+n-1}+\\alpha\\big[G_{t:t+n}-\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1})\\big]\\nabla_\\mathbf{w}\\hat{q}(S_t,A_t,\\mathbf{w}_{t+n-1}), \\end{equation} where \\begin{equation} G_{t:t+n}\\doteq\\hat{q}(S_t,A_t,\\mathbf{w}_{t-1})+\\sum_{k=t}^{t+n-1}\\delta_k\\prod_{i=t+1}^{k}\\gamma\\pi(A_i|S_i), \\end{equation} with $\\delta_t$ is defined similar to the case of semi-gradient Expected Sarsa.\nResidual Bellman Update Gradient-TD In this section, we will be considering SGD methods for minimizing the $\\overline{\\text{PBE}}$.\nRewrite the objective $\\overline{\\text{PBE}}$ in matrix terms, we have: \\begin{align} \\overline{\\text{PBE}}(\\mathbf{w})\u0026=\\left\\Vert\\Pi\\bar{\\delta}_\\mathbf{w}\\right\\Vert_{\\mu}^{2} \\\\ \u0026=\\left(\\Pi\\bar{\\delta}_\\mathbf{w}\\right)^\\text{T}\\mathbf{D}\\Pi\\bar{\\delta}_\\mathbf{w} \\\\ \u0026=\\bar{\\delta}_\\mathbf{w}^\\text{T}\\Pi^\\text{T}\\mathbf{D}\\Pi\\bar{\\delta}_\\mathbf{w} \\\\ \u0026=\\bar{\\delta}_\\mathbf{w}^\\text{T}\\mathbf{D}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w} \\\\ \u0026=\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right)^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right), \\end{align} where in the fourth step, we use the property of projection operation4 and the identity \\begin{equation} \\Pi^\\text{T}\\mathbf{D}\\Pi=\\mathbf{D}\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D} \\end{equation} Thus, the gradient w.r.t weight vector $\\mathbf{w}$ is \\begin{equation} \\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})=2\\nabla_\\mathbf{w}\\left[\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right]^\\text{T}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}\\right)\\label{eq:gt.1} \\end{equation}\nTo turn this into an SGD method, we have to sample something on every time step that has this gradient as its expected value. Let $\\mu$ be the distribution of states visited under the behavior policy. The last factor of \\eqref{eq:gt.1} can be written as: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\bar{\\delta}_\\mathbf{w}=\\sum_s\\mu(s)\\mathbf{x}(s)\\bar{\\delta}_\\mathbf{w}=\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right], \\end{equation} which is the expectation of the semi-gradient TD(0) update \\eqref{eq:opsg.1}. The first factor of \\eqref{eq:gt.1}, which is the transpose of the gradient of this update, then can also be written as: \\begin{align} \\nabla_\\mathbf{w}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]^\\text{T}\u0026=\\mathbb{E}\\left[\\rho_t\\nabla_\\mathbf{w}\\delta_t^\\text{T}\\mathbf{x}_t^\\text{T}\\right] \\\\ \u0026=\\mathbb{E}\\left[\\rho_t\\nabla_\\mathbf{w}\\left(R_{t+1}+\\gamma\\mathbf{w}^\\text{T}\\mathbf{x}_{t+1}-\\mathbf{w}^\\text{T}\\mathbf{x}_t\\right)^\\text{T}\\mathbf{x}_t^\\text{T}\\right] \\\\ \u0026=\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right] \\end{align} And the middle factor, without the inverse operation, can also be written as: \\begin{equation} \\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}=\\sum_a\\mu(s)\\mathbf{x}_s\\mathbf{x}_s^\\text{T}=\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right] \\end{equation} Substituting these expectations back to \\eqref{eq:gt.1}, we obtain: \\begin{equation} \\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w})=2\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\label{eq:gt.2} \\end{equation}\nHere, we use the Gradient-TD to estimate and store the product of the second two factors in \\eqref{eq:gt.2}, denoted as $\\mathbf{v}$: \\begin{equation} \\mathbf{v}\\approx\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right],\\label{eq:gt.3} \\end{equation} which is the solution of the linear least-squares problem that tries to approximate $\\rho_t\\delta_t$ from the features. The SGD for incrementally finding the vector $\\mathbf{v}$ that minimizes the expected squared error $\\left(\\mathbf{v}^\\text{T}\\mathbf{x}_t\\right)^2$ is known as the Least Mean Square (LMS) rule (here augmented with an IS ratio): \\begin{equation} \\mathbf{v}_{t+1}\\doteq\\mathbf{v}_t+\\beta\\rho_t\\left(\\delta_t-\\mathbf{v}^\\text{T}\\mathbf{x}_t\\right)\\mathbf{x}_t, \\end{equation} where $\\beta\u003e0$ is a step-size parameter.\nWith a given stored estimate $\\mathbf{v}_t$ approximating \\eqref{eq:gt.3}, we can apply SGD update to the parameter $\\mathbf{w}_t$: \\begin{align} \\mathbf{w}_{t+1}\u0026=\\mathbf{w}_t-\\frac{1}{2}\\alpha\\nabla_\\mathbf{w}\\overline{\\text{PBE}}(\\mathbf{w}_t) \\\\ \u0026=\\mathbf{w}_t-\\frac{1}{2}\\alpha 2\\mathbb{E}\\left[\\rho_t\\left(\\gamma\\mathbf{x}_{t+1}-\\mathbf{x}_t\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026=\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\label{eq:gt.4} \\\\ \u0026\\approx\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbf{v}_t \\\\ \u0026\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t\\mathbf{v}_t \\end{align} This algorithm is called GTD2. From \\eqref{eq:gt.4}, we can also continue to derive as: \\begin{align} \\mathbf{w}_{t+1}\u0026=\\mathbf{w}_t+\\alpha\\mathbb{E}\\left[\\rho_t\\left(\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\right)\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\rho_t\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right] \\\\ \u0026=\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\rho_t\\delta_t\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\mathbb{E}\\left[\\mathbf{x}_t\\mathbf{x}_t^\\text{T}\\right]^{-1}\\mathbb{E}\\left[\\rho_t\\delta_t\\mathbf{x}_t\\right]\\right) \\\\ \u0026\\approx\\mathbf{w}_t+\\alpha\\left(\\mathbb{E}\\left[\\mathbf{x}_t\\rho_t\\delta_t\\right]-\\gamma\\mathbb{E}\\left[\\rho_t\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\right]\\right)\\mathbf{v}_t \\\\ \u0026\\approx\\mathbf{w}_t+\\alpha\\rho_t\\left(\\delta_t\\mathbf{x}_t-\\gamma\\mathbf{x}_{t+1}\\mathbf{x}_t^\\text{T}\\mathbf{v}_t\\right) \\end{align} This algorithm is known as TD(0) with gradient correction (TDC), or as GTD(0).\nEmphatic-TD References [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Deepmind x UCL. Reinforcement Learning Lecture Series 2021.\n[3] Richard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3, 9–44, 1988.\n[4] Konidaris, G. \u0026 Osentoski, S. \u0026 Thomas, P.. Value Function Approximation in Reinforcement Learning Using the Fourier Basis. AAAI Conference on Artificial Intelligence, North America, aug. 2011.\n[5] Joseph K. Blitzstein \u0026 Jessica Hwang. Introduction to Probability.\n[6] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes Proof We have \\eqref{eq:lm.2} can be written as \\begin{equation*} \\mathbb{E}\\left[\\mathbf{w}_{t+1}\\vert\\mathbf{w}_t\\right]=\\left(\\mathbf{I}-\\alpha\\mathbf{A}\\right)\\mathbf{w}_t+\\alpha\\mathbf{b} \\end{equation*} The idea of the proof is prove that the matrix $\\mathbf{A}$ in \\eqref{eq:lm.3} is a positive definite matrix5, since $\\mathbf{w}_t$ will be reduced toward zero whenever $\\mathbf{A}$ is positive definite.\nFor linear TD(0), in the continuing case with $\\gamma\u003c1$, the matrix $\\mathbf{A}$ can be written as \\begin{align} \\mathbf{A}\u0026=\\sum_s\\mu(s)\\sum_a\\pi(a\\vert s)\\sum_{r,s’}p(r,s’\\vert s,a)\\mathbf{x}(s)\\big(\\mathbf{x}(s)-\\gamma\\mathbf{x}(s’)\\big)^\\text{T}\\nonumber \\\\ \u0026=\\sum_s\\mu(s)\\sum_{s’}p(s’\\vert s)\\mathbf{x}(s)\\big(\\mathbf{x}(s)-\\gamma\\mathbf{x}(s’)\\big)^\\text{T}\\nonumber \\\\ \u0026=\\sum_s\\mu(s)\\mathbf{x}(s)\\Big(\\mathbf{x}(s)-\\gamma\\sum_{s’}p(s’\\vert s)\\mathbf{x}(s’)\\Big)^\\text{T}\\nonumber \\\\ \u0026=\\mathbf{X}^\\text{T}\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})\\mathbf{X},\\label{eq:lm.4} \\end{align} where\n$\\mu(s)$ is the stationary distribution under $\\pi$; $p(s’\\vert s)$ is the probability transition from $s$ to $s’$ under policy $\\pi$; $\\mathbf{P}$ is the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix of these probabilities; $\\mathbf{D}$ is the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix with the $\\mu(s)$ on its diagonal; $\\mathbf{X}$ is the $\\vert\\mathcal{S}\\vert\\times d$ matrix with $\\mathbf{x}(s)$ as its row. Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ in \\eqref{eq:lm.4}.\nTo continue proving the positive definiteness of $\\mathbf{A}$, we use two lemmas:\nLemma 1: A square matrix $\\mathbf{A}$ is positive definite if the symmetric matrix $\\mathbf{S}=\\mathbf{A}+\\mathbf{A}^\\text{T}$ is positive definite. Lemma 2: If $\\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\\mathbf{A}$ is positive definite. With these lemmas, plus since $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\\mathbf{P}$ is a stochastic matrix and $\\gamma\u003c1$. Thus the problem remains to show that the column sums are nonnegative. Let $\\mathbf{1}$ denote the column vector with all components equal to $1$ and $\\boldsymbol{\\mu}(s)$ denote the vectorized version of $\\mu(s)$: i.e., $\\boldsymbol{\\mu}\\in\\mathbb{R}^{\\vert\\mathcal{S}\\vert}$. Thus, $\\boldsymbol{\\mu}=\\mathbf{P}^\\text{T}\\boldsymbol{\\mu}$ since $\\mu(s)$ is the stationary distribution. We have: \\begin{align*} \\mathbf{1}^\\text{T}\\mathbf{D}\\left(\\mathbf{I}-\\gamma\\mathbf{P}\\right)\u0026=\\boldsymbol{\\mu}^\\text{T}\\left(\\mathbf{I}-\\gamma\\mathbf{P}\\right) \\\\ \u0026=\\boldsymbol{\\mu}^\\text{T}-\\gamma\\boldsymbol{\\mu}^\\text{T}\\mathbf{P} \\\\ \u0026=\\boldsymbol{\\mu}^\\text{T}-\\gamma\\boldsymbol{\\mu}^\\text{T} \\\\ \u0026=\\left(1-\\gamma\\right)\\boldsymbol{\\mu}^\\text{T}, \\end{align*} which implies that the column sums of $\\mathbf{D}(\\mathbf{I}-\\gamma\\mathbf{P})$ are positive. ↩︎\nA function $f$ is periodic with period $\\tau$ if \\begin{equation*} f(x+\\tau)=f(x), \\end{equation*} for all $x$. ↩︎\nConsider i.i.d r.v.s $X_1,X_2,\\dots$ with finite mean $\\mu$ and finite variance $\\sigma^2$. For all positive integer $n$, let: \\begin{equation*} \\overline{X}_n\\doteq\\frac{X_1+\\dots+X_n}{n} \\end{equation*} be the sample mean of $X_1$ through $X_n$. As $n\\to\\infty$, the sample mean $\\overline{X}_n$ converges to the true mean $\\mu$, with probability $1$. ↩︎\nFor a linear function approximator, the projection is linear, which implies that it can be represented as an $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ matrix: \\begin{equation*} \\Pi\\doteq\\mathbf{X}\\left(\\mathbf{X}^\\text{T}\\mathbf{D}\\mathbf{X}\\right)^{-1}\\mathbf{X}^\\text{T}\\mathbf{D}, \\end{equation*} where $\\mathbf{D}$ denotes the $\\vert\\mathcal{S}\\vert\\times\\vert\\mathcal{S}\\vert$ diagonal matrix with the $\\mu(s)$ on the diagonal, and $\\mathbf{X}$ denotes the $\\vert\\mathcal{S}\\vert\\times d$ matrix whose rows are the feature vectors $\\mathbf{x}(s)^\\text{T}$, one for each state $s$. ↩︎\nA $n\\times n$ matrix $A$ is called positive definite if and only if for any non-zero vector $\\mathbf{x}\\in\\mathbb{R}^n$, we always have \\begin{equation*} \\mathbf{x}^\\text{T}\\mathbf{A}\\mathbf{x}\u003e0 \\end{equation*} ↩︎\n","wordCount":"4311","inLanguage":"en","datePublished":"2022-02-11T15:26:00+07:00","dateModified":"2022-02-11T15:26:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/func-approx/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Function Approximation</h1><div class=post-meta><span title='2022-02-11 15:26:00 +0700 +0700'>February 11, 2022</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#on-policy-methods>On-policy Methods</a><ul><li><a href=#value-func-approx>Value-function Approximation</a></li><li><a href=#pred-obj>The Prediction Objective</a></li><li><a href=#grad-algs>Gradient-based algorithms</a><ul><li><a href=#stochastic-grad>Stochastic-gradient</a></li><li><a href=#on-policy-semi-grad>Semi-gradient</a></li></ul></li><li><a href=#lin-func-approx>Linear Function Approximation</a><ul><li><a href=#lin-methods>Linear Methods</a></li><li><a href=#feature-cons>Feature Construction</a><ul><li><a href=#polynomial>Polynomial Basis</a></li><li><a href=#fourier>Fourier Basis</a><ul><li><a href=#uni-fourier-series>The Univariate Fourier Series</a></li><li><a href=#even-odd-non-periodic-func>Even, Odd and Non-Periodic Functions</a></li><li><a href=#mult-fourier-series>The Multivariate Fourier Series</a></li></ul></li><li><a href=#coarse-coding>Coarse Coding</a></li><li><a href=#tile-coding>Tile Coding</a></li><li><a href=#rbf>Radial Basis Functions</a></li></ul></li></ul></li><li><a href=#lstd>Least-Squares TD</a></li><li><a href=#ep-semi-grad-sarsa>Episodic Semi-gradient Sarsa</a></li><li><a href=#ep-semi-grad-n-step-sarsa>Episodic Semi-gradient $\boldsymbol{n}$-step Sarsa</a></li><li><a href=#avg-reward>Average Reward</a><ul><li><a href=#dif-semi-grad-sarsa>Differential Semi-gradient Sarsa</a></li><li><a href=#dif-semi-grad-n-step-sarsa>Differential Semi-gradient $\boldsymbol{n}$-step Sarsa</a></li></ul></li></ul></li><li><a href=#off-policy-methods>Off-policy Methods</a><ul><li><a href=#off-policy-semi-grad>Semi-gradient</a></li><li><a href=#residual-bellman-update>Residual Bellman Update</a></li><li><a href=#grad-td>Gradient-TD</a></li><li><a href=#em-td>Emphatic-TD</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>All of the tabular methods we have been considering so far might scale well within a small state space. However, when dealing with Reinforcement Learning problems in continuous state space, an exact solution is nearly impossible to find. But instead, an approximated answer could be found.</p></blockquote><h2 id=on-policy-methods>On-policy Methods<a hidden class=anchor aria-hidden=true href=#on-policy-methods>#</a></h2><p>So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.</p><h3 id=value-func-approx>Value-function Approximation<a hidden class=anchor aria-hidden=true href=#value-func-approx>#</a></h3><p>All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a &ldquo;backed-up value&rdquo; (or <strong>update target</strong>) for that state
\begin{equation}
s\mapsto u,
\end{equation}
where $s$ is the state updated and $u$ is the update target that $s$&rsquo;s estimated value is shifted toward.</p><p>For example,</p><ul><li>the MC update for value prediction is: $S_t\mapsto G_t$.</li><li>the TD(0) update for value prediction is: $S_t\mapsto R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)$.</li><li>the $n$-step TD update is: $S_t\mapsto G_{t:t+n}$.</li><li>and in the DP, policy-evaluation update, $s\mapsto\mathbb{E}\big[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\vert S_t=s\big]$, an arbitrary $s$ is updated.</li></ul><p>Each update $s\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process <strong>function approximation</strong>.</p><h3 id=pred-obj>The Prediction Objective<a hidden class=anchor aria-hidden=true href=#pred-obj>#</a></h3><p>In contrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is impossible to find the exact value function of all states. And moreover, an update at one state also affects many others.</p><p>Hence, it is necessary to specify a state distribution $\mu(s)\geq0,\sum_s\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_\pi(s)$) in each state $s$. Weighting this over the state space $\mathcal{S}$ by $\mu$, we obtain a natural objective function, called the <strong>Mean Squared Value Error</strong>, denoted as $\overline{\text{VE}}$:
\begin{equation}
\overline{\text{VE}}(\mathbf{w})\doteq\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
The distribution $\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divided by total amount of visits). Under on-policy training this is called the <strong>on-policy distribution</strong>.</p><ul><li>In continuing tasks, the on-policy distribution is the stationary distribution under $\pi$.</li><li>In episodic tasks, the on-policy distribution depends on how the initial states are chosen.<ul><li>Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode
\begin{equation}
\eta(s)=h(s)+\sum_\bar{s}\eta(\bar{s})\sum_a\pi(a\vert\bar{s})p(s\vert\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
This system of equation can be solved for the expected number of visits $\eta(s)$. The on-policy distribution is then
\begin{equation}
\mu(s)=\frac{\eta(s)}{\sum_{s&rsquo;}\eta(s&rsquo;)},\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}</li></ul></li></ul><h3 id=grad-algs>Gradient-based algorithms<a hidden class=anchor aria-hidden=true href=#grad-algs>#</a></h3><p>To solve the least squares problem, we are going to use a popular method, named <strong>Gradient descent</strong>.</p><p>Say, consider a differentiable function $J(\mathbf{w})$ of parameter vector $\mathbf{w}$.</p><p>The gradient of $J(\mathbf{w})$ w.r.t $\mathbf{w}$ is defined to be
\begin{equation}
\nabla_{\mathbf{w}}J(\mathbf{w})=\left(\begin{smallmatrix}\dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_1} \\ \vdots \\ \dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_d}\end{smallmatrix}\right)
\end{equation}
The idea of Gradient descent is to minimize the objective function $J(\mathbf{w})$, we repeatedly move $\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\nabla_\mathbf{w}J(\mathbf{w})$.</p><p>Thus, we have the update rule of Gradient descent:
\begin{equation}
\mathbf{w}:=\mathbf{w}-\dfrac{1}{2}\alpha\nabla_\mathbf{w}J(\mathbf{w}),
\end{equation}
where $\alpha$ is a positive step-size parameter.</p><h4 id=stochastic-grad>Stochastic-gradient<a hidden class=anchor aria-hidden=true href=#stochastic-grad>#</a></h4><p>Apply gradient descent to our problem, which is we have to find the minimization of
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
Since $\mu(s)$ is the state distribution over state space $\mathcal{S}$, we can rewrite $\overline{\text{VE}}$ as
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\mathbb{E}_{s\sim\mu}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
By the update we have defined earlier, in each step, we need to decrease $\mathbf{w}$ by an amount of
\begin{equation}
\Delta\mathbf{w}=-\dfrac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{VE}}(\mathbf{w})=\alpha\mathbb{E}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})
\end{equation}
Assume that, on each step, we observe a new example $S_t\mapsto v_\pi(S_t)$ consisting of a state $S_t$ and its true value under the policy $\pi$.</p><p>Using <strong>Stochastic Gradient descent (SGD)</strong>, we adjust the weight vector after each example by a small amount in the direction that would most reduce the error on that example:
\begin{align}
\mathbf{w}_{t+1}&\doteq\mathbf{w}_t-\frac{1}{2}\alpha\nabla_\mathbf{w}\big[v_\pi(S_t)-\hat{v}(S_t,\mathbf{w}_t)\big]^2 \\ &=\mathbf{w}_t+\alpha\big[v_\pi(S_t)-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\label{eq:sg.1}
\end{align}
When the target output, here denoted as $U_t\in\mathbb{R}$, of the $t$-th training example, $S_t\mapsto U_t$, is not the true value, $v_\pi(S_t)$, but some approximation to it, we cannot perform the exact update \eqref{eq:sg.1} since $v_\pi(S_t)$ is unknown, but we can approximate it by substituting $U_t$ in place of $v_\pi(S_t)$. This yield the following general SGD method for state-value prediction:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[U_t-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\label{eq:sg.2}
\end{equation}
If $U_t$ is an <em>unbiased estimate</em> of $v_\pi(S_t)$, i.e., $\mathbb{E}\left[U_t\vert S_t=s\right]=v_\pi(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic conditions for decreasing $\alpha$.</p><p>In particular, since the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t\doteq G_t$, we have that the SGD version of Monte Carlo state-value prediction,
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[G_t-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
is guaranteed to converge to a local optimal point.</p><p>We have the pseudocode of the algorithm</p><figure><img src=/images/func-approx/sgd-mc.png alt="SGD Monte Carlo" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=on-policy-semi-grad>Semi-gradient<a hidden class=anchor aria-hidden=true href=#on-policy-semi-grad>#</a></h4><p>If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\sum_{a,s&rsquo;,r}\pi(a\vert S_t)p(s&rsquo;,r\vert S_t,a)\left[r+\gamma\hat{v}(s&rsquo;,\mathbf{w}_t)\right]$, which all depend on the current value of the weight vector $\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.</p><p>Such methods are called <strong>semi-gradient</strong> since they include only a part of the gradient.</p><figure><img src=/images/func-approx/semi-grad-td.png alt="Semi-gradient TD(0)" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=lin-func-approx>Linear Function Approximation<a hidden class=anchor aria-hidden=true href=#lin-func-approx>#</a></h3><p>One of the most crucial special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot,\mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$.</p><p>Corresponding to every state $s$, there is a real-valued vector $\mathbf{x}(s)\doteq\left(x_1(s),x_2(s),\dots,x_d(s)\right)$, with the same number of components with $\mathbf{w}$.</p><h4 id=lin-methods>Linear Methods<a hidden class=anchor aria-hidden=true href=#lin-methods>#</a></h4><p>Linear methods approximate value function by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:
\begin{equation}
\hat{v}(s,\mathbf{w})\doteq\mathbf{w}^\text{T}\mathbf{x}(s)=\sum_{i=1}^{d}w_ix_i(s)\label{eq:lm.1}
\end{equation}
The vector $\mathbf{x}(s)$ is called a <em>feature vector</em> representing state $s$, i.e., $x_i:\mathcal{S}\to\mathbb{R}$.</p><p>For linear methods, features are <em>basis functions</em> because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.</p><p>From \eqref{eq:lm.1}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})=\mathbf{x}(s)
\end{equation}
Thus, with linear approximation, the SGD update can be rewrite as
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t-\hat{v}(S_t,\mathbf{w}_t)\right]\mathbf{x}(S_t)
\end{equation}</p><p>In the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.</p><ul><li>The gradient MC algorithm in the previous section converges to the global optimum of the $\overline{\text{VE}}$ under linear function approximation if $\alpha$ is reduced over time according to the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#stochastic-approx-condition>usual conditions</a>. In particular, it converges to the fixed point, called $\mathbf{w}_{\text{MC}}$, with:
\begin{align}
\nabla_{\mathbf{w}_{\text{MC}}}\mathbb{E}\left[\big(G_t-v_{\mathbf{w}_{\text{MC}}}(S_t)\big)^2\right]&=0 \\ \mathbb{E}\Big[\big(G_t-v_{\mathbf{w}_{\text{MC}}}(S_t)\big)\mathbf{x}_t\Big]&=0 \\ \mathbb{E}\Big[(G_t-\mathbf{x}_t^\text{T}\mathbf{w}_{\text{MC}})\mathbf{x}_t\Big]&=0 \\ \mathbb{E}\left[G_t\mathbf{x}_t-\mathbf{x}_t\mathbf{x}_t^\text{T}\mathbf{w}_{\text{MC}}\right]&=0 \\ \mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]\mathbf{w}_\text{MC}&=\mathbb{E}\left[G_t\mathbf{x}_t\right] \\ \mathbf{w}_\text{MC}&=\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[G_t\mathbf{x}_t\right]
\end{align}</li><li>The semi-gradient TD algorithm also converges under linear approximation.<ul><li>Recall that, at each time $t$, the semi-gradient TD update is
\begin{align}
\mathbf{w}_{t+1}&\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &=\mathbf{w}_t+\alpha\left(R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\text{T}\mathbf{w}_t\right),
\end{align}
where $\mathbf{x}_t=\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\mathbf{w}_t$, the expected next weight vector can be written as
\begin{equation}
\mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\mathbf{w}_t+\alpha\left(\mathbf{b}-\mathbf{A}\mathbf{w}_t\right),\label{eq:lm.2}
\end{equation}
where
\begin{align}
\mathbf{b}&\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]\in\mathbb{R}^d, \\ \mathbf{A}&\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\text{T}\right]\in\mathbb{R}^d\times\mathbb{R}^d\label{eq:lm.3}
\end{align}
From \eqref{eq:lm.2}, it is easily seen that if the system converges, it must converges to the weight vector $\mathbf{w}_{\text{TD}}$ at which
\begin{align}
\mathbf{b}-\mathbf{A}\mathbf{w}_{\text{TD}}&=\mathbf{0} \\ \mathbf{w}_{\text{TD}}&=\mathbf{A}^{-1}\mathbf{b}
\end{align}
This quantity, $\mathbf{w}_{\text{TD}}$, is called the <strong>TD fixed point</strong>. And in fact, linear semi-gradient TD(0) converges to this point<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</li><li>At the TD fixed point, it has also been proven (in the continuing case) that $\overline{\text{VE}}$ is within a bounded expansion of the lowest possible error, while the Monte Carlo solutions minimize the value error $\overline{\text{VE}}$:
\begin{equation}
\overline{\text{VE}}(\mathbf{w}_{\text{TD}})\leq\dfrac{1}{1-\gamma}\overline{\text{VE}}(\mathbf{w}_{\text{MC}})=\dfrac{1}{1-\gamma}\min_{\mathbf{w}}\overline{\text{VE}}(\mathbf{w})
\end{equation}</li></ul></li></ul><p>Based on the tabular <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-td-update>$n$-step TD</a> we have defined before, applying the semi-gradient method, we have the function approximation version of its, called <span id=semi-grad-n-step-td-update><strong>semi-gradient $\boldsymbol{n}$-step TD</strong></span>, can be defined as:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}-\hat{v}(S_t,\mathbf{w}_{t+n-1})\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\lt T
\end{equation}
where the $n$-step return is generalized from the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-return>tabular version</a>:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}0\geq t\geq T-n
\end{equation}
We therefore have the pseudocode of the semi-gradient $n$-step TD algorithm.</p><figure><img src=/images/func-approx/semi-grad-n-step-td.png alt="Semi-gradient n-step TD" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=feature-cons>Feature Construction<a hidden class=anchor aria-hidden=true href=#feature-cons>#</a></h4><p>There are various ways to define features. The simplest way is to use each variable directly as a basis function along with a constant function, i.e., setting:
\begin{equation}
x_0(s)=1;\hspace{1cm}x_i(s)=s_i,0\leq i\leq d
\end{equation}
However, most interesting value functions are too complex to be represented in this way. This scheme therefore was generalized into the polynomial basis.</p><h5 id=polynomial>Polynomial Basis<a hidden class=anchor aria-hidden=true href=#polynomial>#</a></h5><p>Suppose each state $s$ corresponds to $d$ numbers, $s_1,s_2\dots,s_d$, with each $s_i\in\mathbb{R}$. For this $d$-dimensional state space, each order-$n$ polynomial basis feature $x_i$ can be written as
\begin{equation}
x_i(s)=\prod_{j=1}^{d}s_j^{c_{i,j}},
\end{equation}
where each $c_{i,j}\in\{0,1,\dots,n\}$ for an integer $n\geq 0$. These features make up the order-$n$ polynomial basis for dimension $d$, which contains $(n+1)^d$ different features.</p><h5 id=fourier>Fourier Basis<a hidden class=anchor aria-hidden=true href=#fourier>#</a></h5><h6 id=uni-fourier-series>The Univariate Fourier Series<a hidden class=anchor aria-hidden=true href=#uni-fourier-series>#</a></h6><p><strong>Fourier series</strong> is applied widely in Mathematics to approximate a periodic function<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. For example:</p><figure><img src=/images/func-approx/fourier_series.gif alt="Fourier series visualization" width=480 height=360px style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\theta)=\frac{4\sin\theta}{\pi},f_2(\theta)=\frac{4\sin 3\theta}{3\pi},f_3(\theta)=\frac{4\sin 5\theta}{5\pi}$ and $f_4(\theta)=\frac{4\sin 7\theta}{7\pi}$. The code can be found <a href=https://github.com/trunghng/maths-visualization/blob/main/fourier-series/fourier_series.py target=_blank>here</a></figcaption></figure><p>In particular, the $n$-degree Fourier expansion of $f$ with period $\tau$ is
\begin{equation}
\bar{f}(x)=\dfrac{a_0}{2}+\sum_{k=1}^{n}\left[a_k\cos\left(k\frac{2\pi}{\tau}x\right)+b_k\left(k\frac{2\pi}{\tau}x\right)\right],
\end{equation}
where
\begin{align}
a_k&=\frac{2}{\tau}\int_{0}^{\tau}f(x)\cos\left(\frac{2\pi kx}{\tau}\right)\hspace{0.1cm}dx, \\ b_k&=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi kx}{\tau}\right)\hspace{0.1cm}dx
\end{align}
In the RL setting, $f$ is unknown so we cannot compute $a_0,\dots,a_n$ and $b_1,\dots,b_n$, but we can instead treat them as parameters in a linear function approximation scheme, with
\begin{equation}
\phi_i(x)=\begin{cases}1 &\text{if }i=0 \\ \cos\left(\frac{(i+1)\pi x}{\tau}\right) &\text{if }i>0,i\text{ odd} \\ \sin\left(\frac{i\pi x}{\tau}\right) &\text{if }i>0,i\text{ even}\end{cases}
\end{equation}
Thus, a full $n$-th order Fourier approximation to a one-dimensional value function results in a linear function approximation with $2n+1$ terms.</p><h6 id=even-odd-non-periodic-func>Even, Odd and Non-Periodic Functions<a hidden class=anchor aria-hidden=true href=#even-odd-non-periodic-func>#</a></h6><p>If $f$ is known to be <em>even</em> (i.e., $f(x)=f(-x)$), then $\forall i>0$, we have:
\begin{align}
b_i&=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx \\ &=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx+\int_{\tau/2}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx\right] \\ &=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi ix}{\tau}-2\pi i\right)\hspace{0.1cm}dx\right] \\ &=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi i(x-\tau)}{\tau}\right)\hspace{0.1cm}dx\right] \\ &=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx+\int_{-\tau/2}^{0}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\hspace{0.1cm}dx\right] \\ &=0,
\end{align}
so the $\sin$ terms can be dropped, which reduces the terms required for an $n$-th order Fourier approximation to $n+1$.</p><p>Similarly, if $f$ is known to be <em>odd</em> (i.e., $f(x)=-f(-x)$), then $\forall i>0, a_i=0$, so we can omit the $\cos$ terms.</p><p>However, in general, value functions are not even, odd, or periodic (or known to be in advance). In such cases, if $f$ is defined over a bounded interval with length, let us assume, $\tau$, or without loss of generality, $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, but only project the input variable to $\left[0,\frac{\tau}{2}\right]$. This results in a function periodic on $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, but unconstrained on $\left(0,\frac{\tau}{2}\right]$. We are now free to choose whether or not the function is even or odd over $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, and can drop half of the terms in the approximation.</p><p>In general, we expect it will be better to use the &ldquo;half-even&rdquo; approximation and drop the $\sin$ terms because this causes only a slight discontinuity at the origin. Thus, we can define the univariate $n$-th order Fourier basis as:
\begin{equation}
x_i(s)=\cos(i\pi s),
\end{equation}
for $i=0,\dots,n$.</p><h6 id=mult-fourier-series>The Multivariate Fourier Series<a hidden class=anchor aria-hidden=true href=#mult-fourier-series>#</a></h6><p>The $n$-order Fourier expansion of the multivariate function $F$ with period $\tau$ in $d$ dimensions is
\begin{equation}
\overline{F}(\mathbf{x})=\sum_\mathbf{c}\left[a_\mathbf{c}\cos\left(\frac{2\pi}{\tau}\mathbf{c}\cdot\mathbf{x}\right)+b_\mathbf{c}\sin\left(\frac{2\pi}{\tau}\mathbf{c}\cdot\mathbf{x}\right)\right],
\end{equation}
where $\mathbf{c}=(c_1,\dots,c_d)^\text{T},c_i\in\left[0,\dots,n\right],1\leq i\leq d$.</p><p>This results in $2(n+1)^d$ basis functions for an $n$-th order full Fourier approximation to a value function in $d$ dimensions, which can be reduced to $(n+1)^d$ if we drop either the $sin$ or $cos$ terms for each variable as described above. Thus, we can define the $n$-th order Fourier basis in the multi-dimensional case as:</p><p>Suppose each state $s$ corresponds to a vector of $d$ numbers, $\mathbf{s}=(s_1,\dots,s_d)^\text{T}$, with each $s_i\in[0,1]$. The $i$-th feature in the order-$n$ Fourier cosine basis can then be written as:
\begin{equation}
x_i(s)=\cos\left(\pi\mathbf{s}^\text{T}\mathbf{c}^i\right),
\end{equation}
where $\mathbf{c}=(c_1^i,\dots,c_d^i)^\text{T}$, with $c_j^i\in\{0,\dots,n\}$ for $j=1,\dots,d$ and $i=0,\dots,(n+1)^d$.</p><p>This defines a feature for each of the $(n+1)^d$ possible integer vector $\mathbf{c}^i$. The inner product $\mathbf{s}^\text{T}\mathbf{c}^i$ has the effect of assigning an integer in $\{0,\dots,n\}$ to each dimension of $\mathbf{s}$. As in the one-dimensional case, this integer determines the feature&rsquo;s frequency along that dimension. The feature thus can be shifted and scaled to suit the bounded state space of a particular application.</p><figure><img src=/images/func-approx/gradient_mc_bases.png alt="Fourier basis vs polynomial basis" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: Fourier basis vs Polynomial basis on the 1000-state random walk<br><span>(<a href=#rl-book>RL book</a> - Example 9.2).</span><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-09/random_walk.py target=_blank>here</a></figcaption></figure><h5 id=coarse-coding>Coarse Coding<a hidden class=anchor aria-hidden=true href=#coarse-coding>#</a></h5><figure><img src=/images/func-approx/square_wave_function.png alt="Square wave function approximated using Coarse Coding" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 3</b>: Using linear function approximation based on coarse coding to learn a one-dimensional square-wave function<br><span>(<a href=#rl-book>RL book</a> - Example 9.3).</span><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-09/square_wave.py target=_blank>here</a></figcaption></figure><h5 id=tile-coding>Tile Coding<a hidden class=anchor aria-hidden=true href=#tile-coding>#</a></h5><figure><img src=/images/func-approx/gradient_mc_tile_coding.png alt="Gradient MC with tile coding" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 4</b>: Gradient Monte Carlo with single tiling and with multiple tilings on the 1000-state random walk<br><span>(<a href=#rl-book>RL book</a> - Example 9.2).</span><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-09/random_walk.py target=_blank>here</a></figcaption></figure><h5 id=rbf>Radial Basis Functions<a hidden class=anchor aria-hidden=true href=#rbf>#</a></h5><p>Another common scheme is <strong>Radial Basis Functions (RBFs)</strong>. RBFs are the natural generalization of coarse coding to continuous valued features. Rather than each feature taking either $0$ or $1$, it can be anything within $[0,1]$, reflecting various degrees to which the feature is present.</p><p>A typical RBF feature, $x_i$, has a Gaussian response $x_i(s)$ dependent only on the distance between the state, $s$, and the feature&rsquo;s prototypical or center state, $c_i$, and relative to the feature&rsquo;s width, $\sigma_i$:
\begin{equation}
x_i(s)\doteq\exp\left(\frac{\Vert s-c_i\Vert^2}{2\sigma_i^2}\right)
\end{equation}
The figures below shows a one-dimensional example with a Euclidean distance metric.</p><figure><img src=/images/func-approx/1-d-rbf.png alt="one-dimensional RBFs" style=display:block;margin-left:auto;margin-right:auto;width:300px;height:100px><figcaption style=text-align:center;font-style:italic><b>Figure 5</b>: One-dimensional RBFs</figcaption></figure><h3 id=lstd>Least-Squares TD<a hidden class=anchor aria-hidden=true href=#lstd>#</a></h3><p>Recall when using TD(0) with linear function approximation, $v_\mathbf{w}(s)=\mathbf{w}^\text{T}\mathbf{x}(s)$, we need to find a point $\mathbf{w}$ such that
\begin{equation}
\mathbb{E}\Big[\big(R_{t+1}+\gamma v_\mathbf{w}(S_{t+1})-v_{\mathbf{w}}(S_t)\big)\mathbf{x}_t\Big]=\mathbf{0}\label{eq:lstd.1}
\end{equation}
or
\begin{equation}
\mathbb{E}\Big[R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\text{T}\mathbf{w}_t\Big]=\mathbf{0}
\end{equation}
We found out that the solution is:
\begin{equation}
\mathbf{w}_{\text{TD}}=\mathbf{A}^{-1}\mathbf{b},
\end{equation}
where
\begin{align}
\mathbf{A}&\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\text{T}\right], \\ \mathbf{b}&\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]
\end{align}
Instead of computing these expectations over all possible states and all possible transitions that could happen, we now only care about the things that did happen. In particular, we now consider the empirical loss of \eqref{eq:lstd.1}, as:
\begin{equation}
\frac{1}{t}\sum_{k=0}^{t-1}\big(R_{k+1}+\gamma v_\mathbf{w}(S_{k+1})-v_{\mathbf{w}}(S_k)\big)\mathbf{x}_i=\mathbf{0}\label{eq:lstd.2}
\end{equation}
By the law of large numbers<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>, when $t\to\infty$, \eqref{eq:lstd.2} converges to its expectation, which is \eqref{eq:lstd.1}. Hence, we now just have to compute the estimate of $\mathbf{w}_{\text{TD}}$, called $\mathbf{w}_{\text{LSTD}}$ (as LSTD stands for <strong>Least-Squares TD</strong>), which is defined as:
\begin{equation}
\mathbf{w}_{\text{LSTD}}\doteq\left(\sum_{k=0}^{t-1}\mathbf{x}_i\left(\mathbf{x}_k-\gamma\mathbf{x}_{k+1}\right)^\text{T}\right)^{-1}\left(\sum_{k=1}^{t-1}R_{k+1}\mathbf{x}_k\right)\label{eq:lstd.3}
\end{equation}
In other words, our work is to compute estimates $\widehat{\mathbf{A}}_t$ and $\widehat{\mathbf{b}}_t$ of $\mathbf{A}$ and $\mathbf{b}$:
\begin{align}
\widehat{\mathbf{A}}_t&\doteq\sum_{k=0}^{t-1}\mathbf{x}_k\left(\mathbf{x}_k-\gamma\mathbf{x}_{k+1}\right)^\text{T}+\varepsilon\mathbf{I};\label{eq:lstd.4} \\ \widehat{\mathbf{b}}_t&\doteq\sum_{k=0}^{t-1}R_{k+1}\mathbf{x}_k,\label{eq:lstd.5}
\end{align}
where $\mathbf{I}$ is the identity matrix, and $\varepsilon\mathbf{I}$, for some small $\varepsilon>0$, ensures that $\widehat{\mathbf{A}}_t$ is always invertible. Thus, \eqref{eq:lstd.3} can be rewritten as:
\begin{equation}
\mathbf{w}_{\text{LSTD}}\doteq\widehat{\mathbf{A}}_t^{-1}\widehat{\mathbf{b}}_t
\end{equation}
The two approximations in \eqref{eq:lstd.4} and \eqref{eq:lstd.5} could be implemented incrementally using the same <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#incremental-method>technique</a> we used to apply earlier so that they can be done in constant time per step. Even so, the update for $\widehat{\mathbf{A}}_t$ would have the computational complexity of $O(d^2)$, and so is its memory required to hold the $\widehat{\mathbf{A}}_t$ matrix.</p><p>This leads to a problem that our next step, which is the computation of the inverse $\widehat{\mathbf{A}}_t^{-1}$ of $\widehat{\mathbf{A}}_t$, is going to be $O(d^3)$. Fortunately, with the so-called <strong>Sherman-Morrison formula</strong>, an inverse of our special form matrix - a sum of outer products - can also be updated incrementally with only $O(d^2)$ computations, as
\begin{align}
\widehat{\mathbf{A}}_t^{-1}&=\left(\widehat{\mathbf{A}}_t+\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\text{T}\right)^{-1} \\ &=\widehat{\mathbf{A}}_{t-1}^{-1}-\frac{\widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\text{T}\widehat{\mathbf{A}}_{t-1}^{-1}}{1+\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\text{T}\widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_t},
\end{align}
for $t>0$, with $\mathbf{\widehat{A}}_0\doteq\varepsilon\mathbf{I}$.</p><p>For the estimate $\widehat{\mathbf{b}}_t$ of $\mathbf{b}$, it can be updated using naive approach:
\begin{equation}
\widehat{\mathbf{b}}_{t+1}=\widehat{\mathbf{b}}_t+R_{t+1}\mathbf{x}_t
\end{equation}
The pseudocode for LSTD is given below</p><figure><img src=/images/func-approx/lstd.png alt=LSTD style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=ep-semi-grad-sarsa>Episodic Semi-gradient Sarsa<a hidden class=anchor aria-hidden=true href=#ep-semi-grad-sarsa>#</a></h3><p>We now consider the control problem, with parametric approximation of the action-value function $\hat{q}(s,a,\mathbf{w})\approx q_*(s,a)$, where $\mathbf{w}\in\mathbb{R}^d$ is a finite-dimensional weight vector.</p><p>Similar to the prediction problem, we can apply semi-gradient methods in solving the control problem. The difference is rather than considering training examples of the form $S_t\mapsto U_t$, we now consider examples of the form $S_t,A_t\mapsto U_t$.</p><p>From \eqref{eq:sg.2}, we can derive the general SGD update for action-value prediction as
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[U_t-\hat{q}(S_t,A_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\label{eq:esgs.1}
\end{equation}
The update for the one-step Sarsa method therefore would be
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\label{eq:esgs.2}
\end{equation}
We call this method <strong>episodic semi-gradient one-step Sarsa</strong>.</p><p>To form the control method, we need to couple the action-value</p><figure><img src=/images/func-approx/ep-semi-grad-sarsa.png alt="Episodic Semi-gradient Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>The following figure illustrates the cost-to-go function $\max_a\hat{q}(s,a,\mathbf{w})$ learned during one run of the semi-gradient Sarsa on Mountain Car task.</p><figure><img src=/images/func-approx/mountain-car-ep-semi-grad-sarsa.png alt="Semi-gradient Sarsa on Mountain Car task" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 6</b>: Cost-to-go learned during one run of Semi-gradient Sarsa on Mountain Car problem<br><span>(<a href=#rl-book>RL book</a> - Example 10.1).</span><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-10/mountain_car.py target=_blank>here</a></figcaption></figure><h3 id=ep-semi-grad-n-step-sarsa>Episodic Semi-gradient $\boldsymbol{n}$-step Sarsa<a hidden class=anchor aria-hidden=true href=#ep-semi-grad-n-step-sarsa>#</a></h3><p>Similar to how we defined the one-step Sarsa version of semi-gradient, we can replace the update target in \eqref{eq:esgs.1} by an <span id=n-step-return>$n$-step return</span>,
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\label{eq:esgnss.1}
\end{equation}
for $t+n\lt T$, with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$, as usual, to obtain the <strong>semi-gradient $n$-step Sarsa</strong> update:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
for $0\leq t\lt T$. The pseudocode is given below.</p><figure><img src=/images/func-approx/ep-semi-grad-n-step-sarsa.png alt="Episodic Semi-gradient n-step Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>The figure below shows how the $n$-step ($8$-step in particular) tends to learn faster than the one-step algorithm.</p><figure><img src=/images/func-approx/mountain-car-ep-semi-grad-n-step-sarsa.png alt="one-step vs 8-step Semi-gradient Sarsa on Mountain Car task" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 7</b>: Performance of one-step vs 8-step Semi-gradient Sarsa on Mountain Car task<br><span>(<a href=#rl-book>RL book</a>).</span><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-10/mountain_car.py target=_blank>here</a></figcaption></figure><br><h3 id=avg-reward>Average Reward<a hidden class=anchor aria-hidden=true href=#avg-reward>#</a></h3><p>We now consider a new setting for continuing tasks - alongside the episodic and discounted settings - <strong>average reward</strong>.</p><p>In the average-reward setting, the quality of a policy $\pi$ is defined as the average rate of reward, or simply <strong>average reward</strong>, while following that policy, which we denote as $r(\pi)$:
\begin{align}
r(\pi)&\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}\Big[R_t\vert S_0,A_{0:t-1}\sim\pi\Big] \\ &=\lim_{t\to\infty}\mathbb{E}\Big[R_t\vert S_0,A_{0:t-1}\sim\pi\Big] \\ &=\sum_s\mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s&rsquo;,r}p(s&rsquo;,r\vert s,a)r,
\end{align}
where:</p><ul><li>the expectations are conditioned on the initial state $S_0$, and on the subsequent action $A_0,A_1,\dots,A_{t-1}$, being taken according to $\pi$;</li><li>$\mu_\pi$ is the steady-state distribution,
\begin{equation}
\mu_\pi\doteq\lim_{t\to\infty}P\left(S_t=s\vert A_{0:t-1}\sim\pi\right),
\end{equation}
which is assumed to exist for any $\pi$ and to be independent of $S_0$.</li></ul><p>The steady state distribution is the special distribution under which, if we select actions according to $\pi$, we remain in the same distribution. That is, for which
\begin{equation}
\sum_s\mu_\pi(x)\sum_a\pi(a\vert s)p(s&rsquo;\vert s,a)=\mu_\pi(s&rsquo;)
\end{equation}
In the average-reward setting, returns are defined in terms of differences between rewards and the average reward:
<tag id=differential-return>\begin{equation}
G_t\doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\dots\label{eq:ar.1}
\end{equation}</tag>
This is known as the <strong>differential return</strong>, and the corresponding value functions are known as <strong>differential value functions</strong>, $v_\pi(s)$ and $q_\pi(s,a)$, which are defined in the same way as we have done before:
\begin{align}
v_\pi(s)&\doteq\mathbb{E}\big[G_t\vert S_t=s\big]; \\ q_\pi(s,a)&\doteq\mathbb{E}\big[G_t\vert S_t=s,A_t=a\big],
\end{align}
and similarly for $v_{*}$ and $q_{*}$. Likewise, differential value functions also have Bellman equations, with some modifications by replacing all discounted factor $\gamma$ and replacing all rewards, $r$, by the difference between the reward and the true average reward, $r-r(\pi)$, as:
\begin{align}
&v_\pi(s)=\sum_a\pi(a|s)\sum_{r,s&rsquo;}p(r,s&rsquo;|s,a)\left[r-r(\pi)+v_\pi(s&rsquo;)\right], \\ &q_\pi(s,a)=\sum_{r,s&rsquo;}p(s&rsquo;,r|s,a)\left[r-r(\pi)+\sum_{a&rsquo;}\pi(a&rsquo;|s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)\right], \\ &v_{*}(s)=\max_a\sum_{r,s&rsquo;}p(s&rsquo;,r|s,a)\left[r-\max_\pi r(\pi)+v_{*}(s&rsquo;)\right], \\ &q_{*}(s,a)=\sum_{r,s&rsquo;}p(s&rsquo;,r|s,a)\left[r-\max_\pi r(\pi)+\max_{a&rsquo;}q_{*}(s&rsquo;,a&rsquo;)\right]
\end{align}</p><h4 id=dif-semi-grad-sarsa>Differential Semi-gradient Sarsa<a hidden class=anchor aria-hidden=true href=#dif-semi-grad-sarsa>#</a></h4><p>There is also a differential form of the two <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#td_error>TD errors</a>:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}_{t+1}+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
and
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}_{t+1}+\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t),\label{eq:dsgs.1}
\end{equation}
where $\bar{R}_t$ is an estimate at time $t$ of the average reward $r(\pi)$.</p><p>With these alternative definitions, most of our algorithms and many theoretical results carry through to the average-reward setting without change.</p><p>For example, the average reward version of semi-gradient Sarsa is defined just as in \eqref{eq:esgs.2} except with the differential version of the TD error \eqref{eq:dsgs.1}:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\label{eq:dsgs.2}
\end{equation}
The pseudocode of the algorithm is then given below.</p><figure><img src=/images/func-approx/dif-semi-grad-sarsa.png alt="Differential Semi-gradient Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=dif-semi-grad-n-step-sarsa>Differential Semi-gradient $\boldsymbol{n}$-step Sarsa<a hidden class=anchor aria-hidden=true href=#dif-semi-grad-n-step-sarsa>#</a></h4><p>To derive the $n$-step version of \eqref{eq:dsgs.2}, we use the same update rule, except with an $n$-step version of the TD error.</p><p>First, we need to define the $n$-step differential return, with function approximation, by combining the idea of \eqref{eq:esgnss.1} and \eqref{eq:ar.1} together, as:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}-\bar{R}_{t+1}+R_{t+2}-\bar{R}_{t+2}+\dots+R_{t+n}-\bar{R}_{t+n}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),
\end{equation}
where $\bar{R}$ is an estimate of $r(\pi),n\geq 1$, $t+n\lt T$; $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ as usual. The $n$-step TD error is then
\begin{equation}
\delta_t\doteq G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w})
\end{equation}
The pseudocode of the algorithm is then given below.</p><figure><img src=/images/func-approx/dif-semi-grad-n-step-sarsa.png alt="Differential Semi-gradient n-step Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h2 id=off-policy-methods>Off-policy Methods<a hidden class=anchor aria-hidden=true href=#off-policy-methods>#</a></h2><p>We now consider off-policy methods with function approximation.</p><h3 id=off-policy-semi-grad>Semi-gradient<a hidden class=anchor aria-hidden=true href=#off-policy-semi-grad>#</a></h3><p>To derive the semi-gradient form of off-policy tabular methods we have known, we simply replace the update to an array ($V$ or $Q$) to an update to a weight vector $\mathbf{w}$, using the approximate value function $\hat{v}$ or $\hat{q}$ and its gradient.</p><p>Recall that in off-policy learning we seek to learn a value function for a <em>target policy</em> $\pi$, given data due to a different <em>behavior policy</em> $b$.</p><p>Many of these algorithms use the per-step importance sampling ratio:
\begin{equation}
\rho_t\doteq\rho_{t:t}=\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}</p><p>In particular, for state-value functions, the one-step algorithm is <strong>semi-gradient off-policy TD(0)</strong> has the update rule:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\rho_t\delta_t\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\label{eq:opsg.1}
\end{equation}
where</p><ul id=number-list><li>If the problem is episodic and discounted, we have:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}</li><li>If the problem is continuing and undiscounted using average reward, we have:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}</li></ul><p>For action values, the one-step algorithm is <strong>semi-gradient Expected Sarsa</strong>, which has the update rule:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}),
\end{equation}
with</p><ul id=number-list><li>Episodic tasks:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\sum_a\pi(a\vert S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}</li><li>Continuing tasks:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}+\sum_a\pi(a\vert S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}</li></ul><p>With multi-step algorithms, we begin with <strong>semi-gradient $\boldsymbol{n}$-step Expected Sarsa</strong>, which has the update rule:
\begin{equation}
\hspace{-0.8cm}\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\rho_{t+1}\dots\rho_{t+n-1}\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
where $\rho_k=1$ for $k\geq T$ and $G_{t:n}\doteq G_t$ if $t+n\geq T$, and with</p><ul id=number-list><li>Episodic tasks:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1})
\end{equation}</li><li>Continuing tasks:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}-\bar{R}_t+\dots+R_{t+n}-\bar{R}_{t+n-1}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),
\end{equation}</li></ul><p>For the semi-gradient version of <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#n-step-tree-backup>$n$-step tree-backup</a>, called <strong>semi-gradient $\boldsymbol{n}$-step tree-backup</strong>, the update rule is:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
where
\begin{equation}
G_{t:t+n}\doteq\hat{q}(S_t,A_t,\mathbf{w}_{t-1})+\sum_{k=t}^{t+n-1}\delta_k\prod_{i=t+1}^{k}\gamma\pi(A_i|S_i),
\end{equation}
with $\delta_t$ is defined similar to the case of <strong>semi-gradient Expected Sarsa</strong>.</p><h3 id=residual-bellman-update>Residual Bellman Update<a hidden class=anchor aria-hidden=true href=#residual-bellman-update>#</a></h3><h3 id=grad-td>Gradient-TD<a hidden class=anchor aria-hidden=true href=#grad-td>#</a></h3><p>In this section, we will be considering SGD methods for minimizing the $\overline{\text{PBE}}$.</p><p>Rewrite the objective $\overline{\text{PBE}}$ in matrix terms, we have:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&=\left\Vert\Pi\bar{\delta}_\mathbf{w}\right\Vert_{\mu}^{2} \\ &=\left(\Pi\bar{\delta}_\mathbf{w}\right)^\text{T}\mathbf{D}\Pi\bar{\delta}_\mathbf{w} \\ &=\bar{\delta}_\mathbf{w}^\text{T}\Pi^\text{T}\mathbf{D}\Pi\bar{\delta}_\mathbf{w} \\ &=\bar{\delta}_\mathbf{w}^\text{T}\mathbf{D}\mathbf{X}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w} \\ &=\left(\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w}\right)^\text{T}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\left(\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w}\right),
\end{align}
where in the fourth step, we use the property of projection operation<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> and the identity
\begin{equation}
\Pi^\text{T}\mathbf{D}\Pi=\mathbf{D}\mathbf{X}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\text{T}\mathbf{D}
\end{equation}
Thus, the gradient w.r.t weight vector $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})=2\nabla_\mathbf{w}\left[\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w}\right]^\text{T}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\left(\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w}\right)\label{eq:gt.1}
\end{equation}</p><p>To turn this into an SGD method, we have to sample something on every time step that has this gradient as its expected value. Let $\mu$ be the distribution of states visited under the behavior policy. The last factor of \eqref{eq:gt.1} can be written as:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{D}\bar{\delta}_\mathbf{w}=\sum_s\mu(s)\mathbf{x}(s)\bar{\delta}_\mathbf{w}=\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right],
\end{equation}
which is the expectation of the semi-gradient TD(0) update \eqref{eq:opsg.1}. The first factor of \eqref{eq:gt.1}, which is the transpose of the gradient of this update, then can also be written as:
\begin{align}
\nabla_\mathbf{w}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]^\text{T}&=\mathbb{E}\left[\rho_t\nabla_\mathbf{w}\delta_t^\text{T}\mathbf{x}_t^\text{T}\right] \\ &=\mathbb{E}\left[\rho_t\nabla_\mathbf{w}\left(R_{t+1}+\gamma\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\right)^\text{T}\mathbf{x}_t^\text{T}\right] \\ &=\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\text{T}\right]
\end{align}
And the middle factor, without the inverse operation, can also be written as:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}=\sum_a\mu(s)\mathbf{x}_s\mathbf{x}_s^\text{T}=\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]
\end{equation}
Substituting these expectations back to \eqref{eq:gt.1}, we obtain:
\begin{equation}
\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})=2\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\text{T}\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\label{eq:gt.2}
\end{equation}</p><p>Here, we use the <strong>Gradient-TD</strong> to estimate and store the product of the second two factors in \eqref{eq:gt.2}, denoted as $\mathbf{v}$:
\begin{equation}
\mathbf{v}\approx\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right],\label{eq:gt.3}
\end{equation}
which is the solution of the linear least-squares problem that tries to approximate $\rho_t\delta_t$ from the features. The SGD for incrementally finding the vector $\mathbf{v}$ that minimizes the expected squared error $\left(\mathbf{v}^\text{T}\mathbf{x}_t\right)^2$ is known as the <strong>Least Mean Square (LMS)</strong> rule (here augmented with an IS ratio):
\begin{equation}
\mathbf{v}_{t+1}\doteq\mathbf{v}_t+\beta\rho_t\left(\delta_t-\mathbf{v}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t,
\end{equation}
where $\beta>0$ is a step-size parameter.</p><p>With a given stored estimate $\mathbf{v}_t$ approximating \eqref{eq:gt.3}, we can apply SGD update to the parameter $\mathbf{w}_t$:
\begin{align}
\mathbf{w}_{t+1}&=\mathbf{w}_t-\frac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w}_t) \\ &=\mathbf{w}_t-\frac{1}{2}\alpha 2\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\text{T}\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &=\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\text{T}\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\label{eq:gt.4} \\ &\approx\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\text{T}\right]\mathbf{v}_t \\ &\approx\mathbf{w}_t+\alpha\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t\mathbf{v}_t
\end{align}
This algorithm is called <strong>GTD2</strong>. From \eqref{eq:gt.4}, we can also continue to derive as:
<span id=tdc>\begin{align}
\mathbf{w}_{t+1}&=\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\text{T}\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\rho_t\mathbf{x}_t\mathbf{x}_t^\text{T}\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\text{T}\right]\right)\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\text{T}\right]\right)\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\rho_t\delta_t\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\text{T}\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\text{T}\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\right) \\ &\approx\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\rho_t\delta_t\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\text{T}\right]\right)\mathbf{v}_t \\ &\approx\mathbf{w}_t+\alpha\rho_t\left(\delta_t\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\mathbf{x}_t^\text{T}\mathbf{v}_t\right)
\end{align}</span>
This algorithm is known as <strong>TD(0) with gradient correction (TDC)</strong>, or as <strong>GTD(0)</strong>.</p><h3 id=em-td>Emphatic-TD<a hidden class=anchor aria-hidden=true href=#em-td>#</a></h3><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</span></p><p>[2] Deepmind x UCL. <a href=https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021>Reinforcement Learning Lecture Series 2021</a>.</p><p>[3] Richard S. Sutton. <a href=doi:10.1007/bf00115009>Learning to predict by the methods of temporal differences</a>. Machine Learning, 3, 9–44, 1988.</p><p>[4] Konidaris, G. & Osentoski, S. & Thomas, P.. <a href=https://dl.acm.org/doi/10.5555/2900423.2900483>Value Function Approximation in Reinforcement Learning Using the Fourier Basis</a>. AAAI Conference on Artificial Intelligence, North America, aug. 2011.</p><p>[5] Joseph K. Blitzstein & Jessica Hwang. <a href=https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573>Introduction to Probability</a>.</p><p>[6] Shangtong Zhang. <a href=https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>Reinforcement Learning: An Introduction implementation</a>. Github.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><span id=td-fixed-pt-proof><strong>Proof</strong></span><br>We have \eqref{eq:lm.2} can be written as
\begin{equation*}
\mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\left(\mathbf{I}-\alpha\mathbf{A}\right)\mathbf{w}_t+\alpha\mathbf{b}
\end{equation*}
The idea of the proof is prove that the matrix $\mathbf{A}$ in \eqref{eq:lm.3} is a positive definite matrix<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>, since $\mathbf{w}_t$ will be reduced toward zero whenever $\mathbf{A}$ is positive definite.<br>For linear TD(0), in the continuing case with $\gamma&lt;1$, the matrix $\mathbf{A}$ can be written as
\begin{align}
\mathbf{A}&=\sum_s\mu(s)\sum_a\pi(a\vert s)\sum_{r,s&rsquo;}p(r,s&rsquo;\vert s,a)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s&rsquo;)\big)^\text{T}\nonumber \\ &=\sum_s\mu(s)\sum_{s&rsquo;}p(s&rsquo;\vert s)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s&rsquo;)\big)^\text{T}\nonumber \\ &=\sum_s\mu(s)\mathbf{x}(s)\Big(\mathbf{x}(s)-\gamma\sum_{s&rsquo;}p(s&rsquo;\vert s)\mathbf{x}(s&rsquo;)\Big)^\text{T}\nonumber \\ &=\mathbf{X}^\text{T}\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})\mathbf{X},\label{eq:lm.4}
\end{align}
where</p><ul><li>$\mu(s)$ is the stationary distribution under $\pi$;</li><li>$p(s&rsquo;\vert s)$ is the probability transition from $s$ to $s&rsquo;$ under policy $\pi$;</li><li>$\mathbf{P}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of these probabilities;</li><li>$\mathbf{D}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on its diagonal;</li><li>$\mathbf{X}$ is the $\vert\mathcal{S}\vert\times d$ matrix with $\mathbf{x}(s)$ as its row.</li></ul><p>Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ in \eqref{eq:lm.4}.</p><p>To continue proving the positive definiteness of $\mathbf{A}$, we use two lemmas:</p><ul id=number-list style=font-style:italic><li><b>Lemma 1</b>: A square matrix $\mathbf{A}$ is positive definite if the symmetric matrix $\mathbf{S}=\mathbf{A}+\mathbf{A}^\text{T}$ is positive definite.</li><li><b>Lemma 2</b>: If $\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\mathbf{A}$ is positive definite.</li></ul>With these lemmas, plus since $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\mathbf{P}$ is a stochastic matrix and $\gamma<1$. Thus the problem remains to show that the column sums are nonnegative.<p>Let $\mathbf{1}$ denote the column vector with all components equal to $1$ and $\boldsymbol{\mu}(s)$ denote the vectorized version of $\mu(s)$: i.e., $\boldsymbol{\mu}\in\mathbb{R}^{\vert\mathcal{S}\vert}$. Thus, $\boldsymbol{\mu}=\mathbf{P}^\text{T}\boldsymbol{\mu}$ since $\mu(s)$ is the stationary distribution. We have:
\begin{align*}
\mathbf{1}^\text{T}\mathbf{D}\left(\mathbf{I}-\gamma\mathbf{P}\right)&=\boldsymbol{\mu}^\text{T}\left(\mathbf{I}-\gamma\mathbf{P}\right) \\ &=\boldsymbol{\mu}^\text{T}-\gamma\boldsymbol{\mu}^\text{T}\mathbf{P} \\ &=\boldsymbol{\mu}^\text{T}-\gamma\boldsymbol{\mu}^\text{T} \\ &=\left(1-\gamma\right)\boldsymbol{\mu}^\text{T},
\end{align*}
which implies that the column sums of $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ are positive.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>A function $f$ is periodic with period $\tau$ if
\begin{equation*}
f(x+\tau)=f(x),
\end{equation*}
for all $x$.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Consider i.i.d r.v.s $X_1,X_2,\dots$ with finite mean $\mu$ and finite variance $\sigma^2$. For all positive integer $n$, let:
\begin{equation*}
\overline{X}_n\doteq\frac{X_1+\dots+X_n}{n}
\end{equation*}
be the <em>sample mean</em> of $X_1$ through $X_n$. As $n\to\infty$, the sample mean $\overline{X}_n$ converges to the true mean $\mu$, with probability $1$.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>For a linear function approximator, the projection is linear, which implies that it can be represented as an $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix:
\begin{equation*}
\Pi\doteq\mathbf{X}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\text{T}\mathbf{D},
\end{equation*}
where $\mathbf{D}$ denotes the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on the diagonal, and $\mathbf{X}$ denotes the $\vert\mathcal{S}\vert\times d$ matrix whose rows are the feature vectors $\mathbf{x}(s)^\text{T}$, one for each state $s$.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>A $n\times n$ matrix $A$ is called <strong>positive definite</strong> if and only if for any non-zero vector $\mathbf{x}\in\mathbb{R}^n$, we always have
\begin{equation*}
\mathbf{x}^\text{T}\mathbf{A}\mathbf{x}>0
\end{equation*}&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/function-approximation/>function-approximation</a></li><li><a href=https://trunghng.github.io/tags/td-learning/>td-learning</a></li><li><a href=https://trunghng.github.io/tags/importance-sampling/>importance-sampling</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/eligible-traces/><span class=title>« Prev</span><br><span>Eligible Traces</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/><span class=title>Next »</span><br><span>Temporal-Difference Learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on twitter" href="https://twitter.com/intent/tweet/?text=Function%20Approximation&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f&hashtags=reinforcement-learning%2cfunction-approximation%2ctd-learning%2cimportance-sampling%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f&title=Function%20Approximation&summary=Function%20Approximation&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f&title=Function%20Approximation"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on whatsapp" href="https://api.whatsapp.com/send?text=Function%20Approximation%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Function Approximation on telegram" href="https://telegram.me/share/url?text=Function%20Approximation&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ffunc-approx%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>