<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Solving MDPs with Dynamic Programming | Trung's Place</title><meta name=keywords content="reinforcement-learning,dynamic-programming,my-rl"><meta name=description content="
In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a{text-decoration:none}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list{counter-reset:section}#roman-list,#number-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-.75em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Solving MDPs with Dynamic Programming"><meta property="og:description" content="
In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-25T15:30:00+07:00"><meta property="article:modified_time" content="2021-07-25T15:30:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Solving MDPs with Dynamic Programming"><meta name=twitter:description content="
In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Solving MDPs with Dynamic Programming","item":"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Solving MDPs with Dynamic Programming","name":"Solving MDPs with Dynamic Programming","description":" In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.\n","keywords":["reinforcement-learning","dynamic-programming","my-rl"],"articleBody":" In two previous notes, MDPs and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with Dynamic Programming.\nWhat is Dynamic Programming? Dynamic Programming (DP) is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.\nFigure 1: Using Dynamic Programming to find the shortest path in graph Dynamic Programming applied in Markov Decision Processes DP is a very general method for solving problems having two properties: Optimal substructure.\n- Principle of optimality applies.\n- Optimal solution can be decomposed into sub-problems. Overlapping sub-problems.\n- Sub-problems recur many times.\n- Solutions can be cached and reused. MDPs satisfy both properties since: Bellman equation gives recursive decomposition. Value function stores and reuses solutions. DP assumes the model is already known. Policy Evaluation Recall from the definition of Bellman equation that, for all $s\\in\\mathcal{S}$, \\begin{equation} v_\\pi(s)\\doteq\\sum_a\\pi(a|s)\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_\\pi(s’)\\right]\\tag{1}\\label{1} \\end{equation} If the environment’s dynamics are completely known, then \\eqref{1} is a system of $\\vert\\mathcal{S}\\vert$ linear equations in $\\vert\\mathcal{S}\\vert$ unknowns. We can use iterative methods to solve this problem.\nConsider a sequence of approximate value functions $v_0,v_1,\\dots$, each mapping $\\mathcal{S}^+\\to\\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\\pi$, we have an update rule: \\begin{align} v_{k+1}(s)\u0026\\doteq\\mathbb{E}_\\pi\\left[R_{t+1}+\\gamma v_k(S_{k+1})\\vert S_t=s\\right] \\\\ \u0026=\\sum_a\\pi(a|s)\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_k(s’)\\right] \\end{align} for all $s\\in\\mathcal{S}$. Thanks to Banach’s fixed points theorem and as we have mentioned in that note, we have that the sequence $\\{v_k\\}\\to v_\\pi$ as $k\\to\\infty$. This algorithm is called iterative policy evaluation.\nWe have the backup diagram for this update.\nFigure 2: Backup diagram for Iterative policy evaluation update When implementing iterative policy evaluation method, for all $s\\in\\mathcal{S}$, we can use:\nOne array to store the value functions, and update them \"in-place\" (asynchronous DP) \\begin{equation} \\color{red}{v(s)}\\leftarrow\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\color{red}{v(s')}\\right] \\end{equation} Two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (synchronous DP) \\begin{align} \\color{red}{v_{new}(s)}\u0026\\leftarrow\\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\color{red}{v_{old}(s')}\\right] \\\\ \\color{red}{v_{old}}\u0026\\leftarrow\\color{red}{v_{new}} \\end{align} Here is the pseudocode of the in-place iterative policy evaluation, given a policy $\\pi$, for estimating $V\\approx v_\\pi$\nPolicy Improvement The reason why we compute the value function for a given policy $\\pi$ is to find better policies. Given the computed value function $v_\\pi$ for an deterministic policy $\\pi$, we already know how good it is for a state $s$ to choose action $a=\\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\\neq\\pi$, will it be better?\nIn particular, in state $s$, selecting action $a$ and thereafter following the policy $\\pi$, we have: \\begin{align} q_\\pi(s,a)\u0026\\doteq\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a\\right]\\tag{2}\\label{2} \\\\ \u0026=\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_\\pi(s’)\\right] \\end{align} Theorem (Policy improvement)\nLet $\\pi,\\pi’$ be any pair of deterministic policies such that, for all $s\\in\\mathcal{S}$, \\begin{equation} q_\\pi(s,\\pi’(s))\\geq v_\\pi(s)\\tag{3}\\label{3} \\end{equation} Then $\\pi’\\geq\\pi$, which means for all $s\\in\\mathcal{S}$, we have $v_{\\pi’}(s)\\geq v_\\pi(s)$.\nProof\nDeriving \\eqref{3} combined with \\eqref{2}, we have1: \\begin{align} v_\\pi(s)\u0026\\leq q_\\pi(s,\\pi’(s)) \\\\ \u0026=\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=\\pi’(s)\\right]\\tag{by \\eqref{2}} \\\\ \u0026=\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right] \\\\ \u0026\\leq\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma q_\\pi(S_{t+1},\\pi’(S_{t+1}))|S_t=s\\right]\\tag{by \\eqref{3}} \\\\ \u0026=\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma\\mathbb{E}_{\\pi’}\\left[R_{t+2}+\\gamma v_\\pi(S_{t+2})|S_{t+1},A_{t+1}=\\pi’(S_{t+1})\\right]|S_t=s\\right] \\\\ \u0026=\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 v_\\pi(S_{t+2})|S_t=s\\right] \\\\ \u0026\\leq\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 v_\\pi(S_{t+3})|S_t=s\\right] \\\\ \u0026\\quad\\vdots \\\\ \u0026\\leq\\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+\\dots|S_t=s\\right] \\\\ \u0026=v_{\\pi’}(s) \\end{align}\nConsider the new greedy policy, $\\pi’$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\\pi$, given by \\begin{align} \\pi’(s)\u0026\\doteq\\underset{a}{\\text{argmax}}\\hspace{0.1cm}q_\\pi(s,a) \\\\ \u0026=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=a\\right]\\tag{4}\\label{4} \\\\ \u0026=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_\\pi(s’)\\right] \\end{align} By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.\nSuppose the new greedy policy, $\\pi’$, is as good as, but not better than, $\\pi$. Or in other words, $v_\\pi=v_{\\pi’}$. And from \\eqref{4}, we have for all $s\\in\\mathcal{S}$, \\begin{align} v_{\\pi’}(s)\u0026=\\max_a\\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi’}(S_{t+1})|S_t=s,A_t=a\\right] \\\\ \u0026=\\max_a\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_{\\pi’}(s’)\\right] \\end{align} which is the Bellman optimality equation for action-value function. And therefore, $v_{\\pi’}$ must be $v_*$. Hence, policy improvement must give us a strictly better policy except when the original one is already optimal2.\nPolicy Iteration Once we have obtained a better policy, $\\pi’$, by improving a policy $\\pi$ using $v_\\pi$, we can repeat the same process by computing $v_{\\pi’}$, and improve it to yield an even better $\\pi’’$. Repeating it again and again, we get an iterative procedure to improve the policy \\begin{equation} \\pi_0\\xrightarrow[]{\\text{evaluation}}v_{\\pi_0}\\xrightarrow[]{\\text{improvement}}\\pi_1\\xrightarrow[]{\\text{evaluation}}v_{\\pi_1}\\xrightarrow[]{\\text{improvement}}\\pi_2\\xrightarrow[]{\\text{evaluation}}\\dots\\xrightarrow[]{\\text{improvement}}\\pi_*\\xrightarrow[]{\\text{evaluation}}v_* \\end{equation} Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations. This algorithm is called policy iteration. And here is the pseudocode of the policy iteration.\nAn example of using policy iteration on the Jack’s rental problem (RL book - example 4.2)\nFigure 3: Policy Iteration on Jack's car rental task. The code can be found here Value Iteration When using policy iteration, each of its iterations involves policy evaluation, which requires multiple sweeps through the state set, and thus affects the computation performance.\nPolicy evaluation step of policy iteration, in fact, can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called value iteration, which follows this update: \\begin{align} v_{k+1}\u0026\\doteq\\max_a\\mathbb{E}\\left[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s,A_t=a\\right] \\\\ \u0026=\\max_a\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma v_k(s’)\\right], \\end{align} for all $s\\in\\mathcal{S}$. Once again, thanks to Banach’s fixed point theorem, for an arbitrary $v_0$, we have that the sequence $\\{v_k\\}\\to v_*$ as $k\\to\\infty$.\nWe have the backup diagram for this update3.\nFigure 4: Backup diagram of Value Iteration update And here is the pseudocode of the value iteration.\nExample - Gambler’s Problem (This example is taken from RL book - example 4.3).\nLet’s say you are a gambler, who decides to bet on the outcomes of sequence of coin flips. On each flip, you have to decide how many dollars, in integer, you will bet. Each time you win, when the coin comes up head, the amount of money you get is exactly the same as the money that you staked on that flip. Same it goes in the tail case, you will lose that amount of dollars. The game ends when you reach your goal, let’s assume, $\\$100$, or when your hands are empty, $\\$0$. This task can be formulated as undiscounted, episodic, finite MDP. The state is your capital, $s\\in\\{1,2,\\dots,99\\}$; the actions are stakes, $a\\in\\{0,1,\\dots,\\min\\left(s,100-s\\right)\\}$. The reward is zero on all trainsitions except those on which you reach your goal, when it is $+1$. And we also assume that the probability of the coin coming up heads, $p_h=0.4$.\nSolution code\nThe code can be found here.\nimport numpy as np import matplotlib.pyplot as plt GOAL = 100 #For convenience, we introduce 2 dummy states: 0 and terminal state states = np.arange(0, GOAL + 1) rewards = {'terminal': 1, 'non-terminal': 0} HEAD_PROB = 0.4 GAMMA = 1 # discount factor def value_iteration(theta): V = np.zeros(states.shape) V_set = [] policy = np.zeros(V.shape) while True: delta = 0 V_set.append(V.copy()) for state in states[1:GOAL]: old_value = V[state].copy() actions = np.arange(0, min(state, GOAL - state) + 1) new_value = 0 for action in actions: next_head_state = states[state] + action next_tail_state = states[state] - action head_reward = rewards['terminal'] if next_head_state == GOAL else rewards['non-terminal'] tail_reward = rewards['non-terminal'] value = HEAD_PROB * (head_reward + GAMMA * V[next_head_state]) + \\ (1 - HEAD_PROB) * (tail_reward + GAMMA * V[next_tail_state]) if value \u003e new_value: new_value = value V[state] = new_value delta = max(delta, abs(old_value - V[state])) print('Max value changed: ', delta) if delta \u003c theta: V_set.append(V) break for state in states[1:GOAL]: values = [] actions = np.arange(min(state, 100 - state) + 1) for action in actions: next_head_state = states[state] + action next_tail_state = states[state] - action head_reward = rewards['terminal'] if next_head_state == GOAL else rewards['non-terminal'] tail_reward = rewards['non-terminal'] values.append(HEAD_PROB * (head_reward + GAMMA * V[next_head_state]) + (1 - HEAD_PROB) * (tail_reward + GAMMA * V[next_tail_state])) policy[state] = actions[np.argmax(np.round(values[1:], 4)) + 1] return V_set, policy if __name__ == '__main__': theta = 1e-13 value_funcs, optimal_policy = value_iteration(theta) optimal_value = value_funcs[-1] print(optimal_value) plt.figure(figsize=(10, 20)) plt.subplot(211) for sweep, value in enumerate(value_funcs): plt.plot(value, label='sweep {}'.format(sweep)) plt.xlabel('Capital') plt.ylabel('Value estimates') plt.legend(loc='best') plt.subplot(212) plt.scatter(states, optimal_policy) plt.xlabel('Capital') plt.ylabel('Final policy (stake)') plt.savefig('./gambler.png') plt.close() And here is our results after running the code\nFigure 5: Gambler's Problem solved by Value Iteration Result Generalized Policy Iteration The Generalized Policy Iteration (GPI) algorithm refers to the idea of combining policy evaluation and policy improvement together to improve the original policy.\nIn GPI, the value function is repeatedly driven toward the true value of the current policy and at the same time the policy is being improved optimality with respect to its value function, as in the following diagram.\nFigure 6: Generalized Policy Iteration Once it reaches the stationary state (when both evaluation and improvement no long produce any updates), then the current value function and policy must be optimal.\nThe evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They competing in the sense that on the one hand, making policy greedy w.r.t the value function typically makes value function incorrect for the new policy. And on the other hand, approximating the value function closer to the true value of the policy typically forces the policy is no longer to be greedy. But in the long run, they two processes cooperate to find a single joint solution: the optimal value function and an optimal policy.\nFigure 7: Interaction between the evaluation and improvement processes in GPI References [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] David Silver. UCL course on RL.\n[3] Csaba Szepesvári. Algorithms for Reinforcement Learning.\n[4] A. Lazaric. Markov Decision Processes and Dynamic Programming.\n[5] Wikipedia. Dynamic Programming.\n[6] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\n[7] Policy Improvement theorem.\nFootnotes In the third step, the expression \\begin{equation} \\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right] \\end{equation} means ‘’the discounted expected value when starting in state $s$, choosing action according to $\\pi’$ for the next time step, and following $\\pi$ thereafter\". And so on for the two, or n next steps. Therefore, we have that: \\begin{equation} \\mathbb{E}_{\\pi’}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s\\right]=\\mathbb{E}\\left[R_{t+1}+\\gamma v_\\pi(S_{t+1})|S_t=s,A_t=\\pi’(s)\\right] \\end{equation} ↩︎\nThe idea of policy improvement also extends to stochastic policies. ↩︎\nValue iteration can be used in conjunction with action-value function, which takes the following update: \\begin{align} q_{k+1}(s,a)\u0026\\doteq\\mathbb{E}\\left[R_{t+1}+\\gamma\\max_{a’}q_k(S_{t+1},a’)|S_t=s,A_t=a\\right] \\\\ \u0026=\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma\\max_{a’}q_k(s’,a’)\\right] \\end{align} Yep, that’s right, the sequence $\\{q_k\\}\\to q_*$ as $k\\to\\infty$ at a geometric rate thanks to Banach’s fixed point theorem. ↩︎\n","wordCount":"1742","inLanguage":"en","datePublished":"2021-07-25T15:30:00+07:00","dateModified":"2021-07-25T15:30:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Solving MDPs with Dynamic Programming</h1><div class=post-meta><span title='2021-07-25 15:30:00 +0700 +0700'>July 25, 2021</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-dynamic-programming>What is Dynamic Programming?</a></li><li><a href=#dynamic-programming-applied-in-markov-decision-processes>Dynamic Programming applied in Markov Decision Processes</a><ul><li><a href=#policy-evaluation>Policy Evaluation</a></li><li><a href=#policy-improvement>Policy Improvement</a></li><li><a href=#policy-iteration>Policy Iteration</a></li><li><a href=#value-iteration>Value Iteration</a><ul><li><a href=#example>Example - Gambler&rsquo;s Problem</a></li></ul></li><li><a href=#gpi>Generalized Policy Iteration</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>In two previous notes, <a href=https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/><strong>MDPs and Bellman equations</strong></a> and <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations were defined and how they worked. In this note, we are going to find the solution for the MDP framework with <strong>Dynamic Programming</strong>.</p></blockquote><h2 id=what-is-dynamic-programming>What is Dynamic Programming?<a hidden class=anchor aria-hidden=true href=#what-is-dynamic-programming>#</a></h2><p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p><figure><img src=/images/dp-in-mdp/dp.png alt="dynamic programming" width=360 height=200px style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: Using Dynamic Programming to find the shortest path in graph</figcaption></figure><h2 id=dynamic-programming-applied-in-markov-decision-processes>Dynamic Programming applied in Markov Decision Processes<a hidden class=anchor aria-hidden=true href=#dynamic-programming-applied-in-markov-decision-processes>#</a></h2><ul><li>DP is a very general method for solving problems having two properties:<ul id=roman-list><li><b>Optimal substructure</b>.<br>- Principle of optimality applies.<br>- Optimal solution can be decomposed into sub-problems.</li><li><b>Overlapping sub-problems</b>.<br>- Sub-problems recur many times.<br>- Solutions can be cached and reused.</li></ul></li><li>MDPs satisfy both properties since:<ul><li>Bellman equation gives recursive decomposition.</li><li>Value function stores and reuses solutions.</li></ul></li><li>DP assumes the model is already known.</li></ul><h3 id=policy-evaluation>Policy Evaluation<a hidden class=anchor aria-hidden=true href=#policy-evaluation>#</a></h3><p>Recall from the definition of <a href=https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/#bellman-equations>Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_\pi(s&rsquo;)\right]\tag{1}\label{1}
\end{equation}
If the environment&rsquo;s dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br>Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &=\sum_a\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_k(s&rsquo;)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/#banach-fixed-pts-theorem>Banach&rsquo;s fixed points theorem</a> and as we have mentioned in that note, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <strong>iterative policy evaluation</strong>.<br>We have the backup diagram for this update.</p><figure><img src=/images/dp-in-mdp/backup-iterative-policy-evaluation.png alt="Backup diagram for iterative policy evalution update" style=display:block;margin-left:auto;margin-right:auto;width:360px;height:200px><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: Backup diagram for Iterative policy evaluation update</figcaption></figure><p>When implementing <strong>iterative policy evaluation</strong> method, for all $s\in\mathcal{S}$, we can use:</p><ul id=number-list><li>One array to store the value functions, and update them "in-place" (<b>asynchronous DP</b>)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\color{red}{v(s')}\right]
\end{equation}</li><li>Two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (<b>synchronous DP</b>)
\begin{align}
\color{red}{v_{new}(s)}&\leftarrow\sum_a\pi(a|s)\sum_{s',r}p(s',r|s,a)\left[r+\color{red}{v_{old}(s')}\right] \\ \color{red}{v_{old}}&\leftarrow\color{red}{v_{new}}
   \end{align}</li></ul><p>Here is the pseudocode of the <strong>in-place iterative policy evaluation</strong>, given a policy $\pi$, for estimating $V\approx v_\pi$</p><figure><img src=/images/dp-in-mdp/iterative-policy-evaluation.png alt="iterative policy evalution pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><h3 id=policy-improvement>Policy Improvement<a hidden class=anchor aria-hidden=true href=#policy-improvement>#</a></h3><p>The reason why we compute the value function for a given policy $\pi$ is to find better policies. Given the computed value function $v_\pi$ for an deterministic policy $\pi$, we already know how good it is for a state $s$ to choose action $a=\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\neq\pi$, will it be better?<br>In particular, in state $s$, selecting action $a$ and thereafter following the policy $\pi$, we have:
\begin{align}
q_\pi(s,a)&\doteq\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{2}\label{2} \\ &=\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_\pi(s&rsquo;)\right]
\end{align}
<strong>Theorem</strong> (<em>Policy improvement</em>)<br>Let $\pi,\pi&rsquo;$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,
\begin{equation}
q_\pi(s,\pi&rsquo;(s))\geq v_\pi(s)\tag{3}\label{3}
\end{equation}
Then $\pi&rsquo;\geq\pi$, which means for all $s\in\mathcal{S}$, we have $v_{\pi&rsquo;}(s)\geq v_\pi(s)$.</p><p><strong>Proof</strong><br>Deriving \eqref{3} combined with \eqref{2}, we have<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:
\begin{align}
v_\pi(s)&\leq q_\pi(s,\pi&rsquo;(s)) \\ &=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi&rsquo;(s)\right]\tag{by \eqref{2}} \\ &=\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right] \\ &\leq\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi&rsquo;(S_{t+1}))|S_t=s\right]\tag{by \eqref{3}} \\ &=\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma\mathbb{E}_{\pi&rsquo;}\left[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi&rsquo;(S_{t+1})\right]|S_t=s\right] \\ &=\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\right] \\ &\leq\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|S_t=s\right] \\ &\quad\vdots \\ &\leq\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\dots|S_t=s\right] \\ &=v_{\pi&rsquo;}(s)
\end{align}</p><p>Consider the new <strong>greedy policy</strong>, $\pi&rsquo;$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\pi$, given by
\begin{align}
\pi&rsquo;(s)&\doteq\underset{a}{\text{argmax}}\hspace{0.1cm}q_\pi(s,a) \\ &=\underset{a}{\text{argmax}}\hspace{0.1cm}\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{4}\label{4} \\ &=\underset{a}{\text{argmax}}\hspace{0.1cm}\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_\pi(s&rsquo;)\right]
\end{align}
By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.<br>Suppose the new greedy policy, $\pi&rsquo;$, is as good as, but not better than, $\pi$. Or in other words, $v_\pi=v_{\pi&rsquo;}$. And from \eqref{4}, we have for all $s\in\mathcal{S}$,
\begin{align}
v_{\pi&rsquo;}(s)&=\max_a\mathbb{E}\left[R_{t+1}+\gamma v_{\pi&rsquo;}(S_{t+1})|S_t=s,A_t=a\right] \\ &=\max_a\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_{\pi&rsquo;}(s&rsquo;)\right]
\end{align}
which is the Bellman optimality equation for action-value function. And therefore, $v_{\pi&rsquo;}$ must be $v_*$. Hence, <strong>policy improvement</strong> must give us a strictly better policy except when the original one is already optimal<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><h3 id=policy-iteration>Policy Iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h3><p>Once we have obtained a better policy, $\pi&rsquo;$, by improving a policy $\pi$ using $v_\pi$, we can repeat the same process by computing $v_{\pi&rsquo;}$, and improve it to yield an even better $\pi&rsquo;&rsquo;$. Repeating it again and again, we get an iterative procedure to improve the policy
\begin{equation}
\pi_0\xrightarrow[]{\text{evaluation}}v_{\pi_0}\xrightarrow[]{\text{improvement}}\pi_1\xrightarrow[]{\text{evaluation}}v_{\pi_1}\xrightarrow[]{\text{improvement}}\pi_2\xrightarrow[]{\text{evaluation}}\dots\xrightarrow[]{\text{improvement}}\pi_*\xrightarrow[]{\text{evaluation}}v_*
\end{equation}
Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations.
This algorithm is called <strong>policy iteration</strong>. And here is the pseudocode of the policy iteration.</p><figure><img src=/images/dp-in-mdp/policy-iteration.png alt="policy iteration pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><p>An example of using policy iteration on the Jack&rsquo;s rental problem (<a href=%22#rl-book%22><strong>RL book - example 4.2</strong></a>)</p><figure><img src=/images/dp-in-mdp/jackscar.png alt="Using policy iteration on Jack's car rental problem" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 3</b>: Policy Iteration on Jack's car rental task. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-04/jackscar.py target=_blank>here</a></figcaption></figure><h3 id=value-iteration>Value Iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h3><p>When using <em>policy iteration</em>, each of its iterations involves policy evaluation, which requires multiple sweeps through the state set, and thus affects the computation performance.<br>Policy evaluation step of policy iteration, in fact, can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called <strong>value iteration</strong>, which follows this update:
\begin{align}
v_{k+1}&\doteq\max_a\mathbb{E}\left[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a\right] \\ &=\max_a\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma v_k(s&rsquo;)\right],
\end{align}
for all $s\in\mathcal{S}$. Once again, thanks to <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/#banach-fixed-pts-theorem>Banach&rsquo;s fixed point theorem</a>, for an arbitrary $v_0$, we have that the sequence $\{v_k\}\to v_*$ as $k\to\infty$.<br>We have the backup diagram for this update<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><figure><img src=/images/dp-in-mdp/backup-value-iteration.png alt="Backup diagram of value iteration update" style=display:block;margin-left:auto;margin-right:auto;width:360px;height:200px><figcaption style=text-align:center;font-style:italic><b>Figure 4</b>: Backup diagram of Value Iteration update</figcaption></figure><p>And here is the pseudocode of the value iteration.</p><figure><img src=/images/dp-in-mdp/value-iteration.png alt="value iteration pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><h4 id=example>Example - Gambler&rsquo;s Problem<a hidden class=anchor aria-hidden=true href=#example>#</a></h4><p>(This example is taken from <a href=#rl-book><strong>RL book - example 4.3</strong></a>).</p><p>Let&rsquo;s say you are a gambler, who decides to bet on the outcomes of sequence of coin flips. On each flip, you have to decide how many dollars, in integer, you will bet. Each time you win, when the coin comes up head, the amount of money you get is exactly the same as the money that you staked on that flip. Same it goes in the tail case, you will lose that amount of dollars. The game ends when you reach your goal, let&rsquo;s assume, $\$100$, or when your hands are empty, $\$0$. This task can be formulated as undiscounted, episodic, finite MDP. The state is your capital, $s\in\{1,2,\dots,99\}$; the actions are stakes, $a\in\{0,1,\dots,\min\left(s,100-s\right)\}$. The reward is zero on all trainsitions except those on which you reach your goal, when it is $+1$.
And we also assume that the probability of the coin coming up heads, $p_h=0.4$.</p><p><strong>Solution code</strong><br>The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-04/gambler.py>here</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>GOAL</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl><span class=c1>#For convenience, we introduce 2 dummy states: 0 and terminal state</span>
</span></span><span class=line><span class=cl><span class=n>states</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>GOAL</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>rewards</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;terminal&#39;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;non-terminal&#39;</span><span class=p>:</span> <span class=mi>0</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=n>HEAD_PROB</span> <span class=o>=</span> <span class=mf>0.4</span>
</span></span><span class=line><span class=cl><span class=n>GAMMA</span> <span class=o>=</span> <span class=mi>1</span>  <span class=c1># discount factor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>value_iteration</span><span class=p>(</span><span class=n>theta</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>states</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>V_set</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>policy</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>V</span><span class=o>.</span><span class=n>shape</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>delta</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>V_set</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>V</span><span class=o>.</span><span class=n>copy</span><span class=p>())</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>state</span> <span class=ow>in</span> <span class=n>states</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=n>GOAL</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=n>old_value</span> <span class=o>=</span> <span class=n>V</span><span class=p>[</span><span class=n>state</span><span class=p>]</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>actions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>GOAL</span> <span class=o>-</span> <span class=n>state</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>new_value</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=n>actions</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>next_head_state</span> <span class=o>=</span> <span class=n>states</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>+</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>                <span class=n>next_tail_state</span> <span class=o>=</span> <span class=n>states</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>-</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>                <span class=n>head_reward</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;terminal&#39;</span><span class=p>]</span> <span class=k>if</span> <span class=n>next_head_state</span> <span class=o>==</span> <span class=n>GOAL</span> <span class=k>else</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;non-terminal&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>tail_reward</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;non-terminal&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>                <span class=n>value</span> <span class=o>=</span> <span class=n>HEAD_PROB</span> <span class=o>*</span> <span class=p>(</span><span class=n>head_reward</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>V</span><span class=p>[</span><span class=n>next_head_state</span><span class=p>])</span> <span class=o>+</span> \
</span></span><span class=line><span class=cl>                    <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>HEAD_PROB</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>tail_reward</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>V</span><span class=p>[</span><span class=n>next_tail_state</span><span class=p>])</span>
</span></span><span class=line><span class=cl>                <span class=k>if</span> <span class=n>value</span> <span class=o>&gt;</span> <span class=n>new_value</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                    <span class=n>new_value</span> <span class=o>=</span> <span class=n>value</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=n>V</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>=</span> <span class=n>new_value</span>
</span></span><span class=line><span class=cl>            <span class=n>delta</span> <span class=o>=</span> <span class=nb>max</span><span class=p>(</span><span class=n>delta</span><span class=p>,</span> <span class=nb>abs</span><span class=p>(</span><span class=n>old_value</span> <span class=o>-</span> <span class=n>V</span><span class=p>[</span><span class=n>state</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>            <span class=nb>print</span><span class=p>(</span><span class=s1>&#39;Max value changed: &#39;</span><span class=p>,</span> <span class=n>delta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>delta</span> <span class=o>&lt;</span> <span class=n>theta</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>V_set</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>V</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>state</span> <span class=ow>in</span> <span class=n>states</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=n>GOAL</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>actions</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=nb>min</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=mi>100</span> <span class=o>-</span> <span class=n>state</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=n>actions</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>next_head_state</span> <span class=o>=</span> <span class=n>states</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>+</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>            <span class=n>next_tail_state</span> <span class=o>=</span> <span class=n>states</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>-</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>            <span class=n>head_reward</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;terminal&#39;</span><span class=p>]</span> <span class=k>if</span> <span class=n>next_head_state</span> <span class=o>==</span> <span class=n>GOAL</span> <span class=k>else</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;non-terminal&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>tail_reward</span> <span class=o>=</span> <span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;non-terminal&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>values</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>HEAD_PROB</span> <span class=o>*</span> <span class=p>(</span><span class=n>head_reward</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>V</span><span class=p>[</span><span class=n>next_head_state</span><span class=p>])</span> <span class=o>+</span>
</span></span><span class=line><span class=cl>                          <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>HEAD_PROB</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=n>tail_reward</span> <span class=o>+</span> <span class=n>GAMMA</span> <span class=o>*</span> <span class=n>V</span><span class=p>[</span><span class=n>next_tail_state</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>        <span class=n>policy</span><span class=p>[</span><span class=n>state</span><span class=p>]</span> <span class=o>=</span> <span class=n>actions</span><span class=p>[</span><span class=n>np</span><span class=o>.</span><span class=n>argmax</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=n>values</span><span class=p>[</span><span class=mi>1</span><span class=p>:],</span> <span class=mi>4</span><span class=p>))</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>V_set</span><span class=p>,</span> <span class=n>policy</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>theta</span> <span class=o>=</span> <span class=mf>1e-13</span>
</span></span><span class=line><span class=cl>    <span class=n>value_funcs</span><span class=p>,</span> <span class=n>optimal_policy</span> <span class=o>=</span> <span class=n>value_iteration</span><span class=p>(</span><span class=n>theta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>optimal_value</span> <span class=o>=</span> <span class=n>value_funcs</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>optimal_value</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>figure</span><span class=p>(</span><span class=n>figsize</span><span class=o>=</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=mi>20</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>211</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>sweep</span><span class=p>,</span> <span class=n>value</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>value_funcs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>value</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;sweep </span><span class=si>{}</span><span class=s1>&#39;</span><span class=o>.</span><span class=n>format</span><span class=p>(</span><span class=n>sweep</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Capital&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Value estimates&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>(</span><span class=n>loc</span><span class=o>=</span><span class=s1>&#39;best&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>subplot</span><span class=p>(</span><span class=mi>212</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>scatter</span><span class=p>(</span><span class=n>states</span><span class=p>,</span> <span class=n>optimal_policy</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Capital&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Final policy (stake)&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;./gambler.png&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><p>And here is our results after running the code</p><figure><img src=/images/dp-in-mdp/gambler.png alt=gambler style=display:block;margin-left:auto;margin-right:auto;width:450px;height:900px><figcaption style=text-align:center;font-style:italic><b>Figure 5</b>: Gambler's Problem solved by Value Iteration Result</figcaption></figure><h3 id=gpi>Generalized Policy Iteration<a hidden class=anchor aria-hidden=true href=#gpi>#</a></h3><p>The <strong>Generalized Policy Iteration (GPI)</strong> algorithm refers to the idea of combining policy evaluation and policy improvement together to improve the original policy.<br>In GPI, the value function is repeatedly driven toward the true value of the current policy and at the same time the policy is being improved optimality with respect to its value function, as in the following diagram.</p><figure><img src=/images/dp-in-mdp/gpi.png alt=GPI style=display:block;margin-left:auto;margin-right:auto;width:200px;height:320px><figcaption style=text-align:center;font-style:italic><b>Figure 6</b>: Generalized Policy Iteration</figcaption></figure><br><p>Once it reaches the stationary state (when both evaluation and improvement no long produce any updates), then the current value function and policy must be optimal.<br>The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They competing in the sense that on the one hand, making policy greedy w.r.t the value function typically makes value function incorrect for the new policy. And on the other hand, approximating the value function closer to the true value of the policy typically forces the policy is no longer to be greedy. But in the long run, they two processes cooperate to find a single joint solution: the optimal value function and an optimal policy.</p><figure><img src=/images/dp-in-mdp/gpi-rel.png alt="GPI interaction" style=display:block;margin-left:auto;margin-right:auto;width:360px;height:200px><figcaption style=text-align:center;font-style:italic><b>Figure 7</b>: Interaction between the evaluation and improvement processes in GPI</figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto</span>. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[2] David Silver. <a href=https://www.davidsilver.uk/teaching/>UCL course on RL</a>.</p><p>[3] Csaba Szepesvári. <a href=https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924>Algorithms for Reinforcement Learning</a>.</p><p>[4] A. Lazaric. <a href=http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf>Markov Decision Processes and Dynamic Programming</a>.</p><p>[5] Wikipedia. <a href=https://en.wikipedia.org/wiki/Dynamic_programming>Dynamic Programming</a>.</p><p>[6] Shangtong Zhang. <a href=https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>Reinforcement Learning: An Introduction implementation</a>. Github.</p><p>[7] <a href=https://stats.stackexchange.com/a/258783>Policy Improvement theorem</a>.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>In the third step, the expression
\begin{equation}
\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]
\end{equation}
means &lsquo;&rsquo;the discounted expected value when starting in state $s$, choosing action according to $\pi&rsquo;$ for the next time step, and following $\pi$ thereafter". And so on for the two, or n next steps. Therefore, we have that:
\begin{equation}
\mathbb{E}_{\pi&rsquo;}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi&rsquo;(s)\right]
\end{equation}&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The idea of policy improvement also extends to stochastic policies.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Value iteration can be used in conjunction with action-value function, which takes the following update:
\begin{align}
q_{k+1}(s,a)&\doteq\mathbb{E}\left[R_{t+1}+\gamma\max_{a&rsquo;}q_k(S_{t+1},a&rsquo;)|S_t=s,A_t=a\right] \\ &=\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma\max_{a&rsquo;}q_k(s&rsquo;,a&rsquo;)\right]
\end{align}
Yep, that&rsquo;s right, the sequence $\{q_k\}\to q_*$ as $k\to\infty$ at a geometric rate thanks to <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/#banach-fixed-pts-theorem>Banach&rsquo;s fixed point theorem</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/dynamic-programming/>dynamic-programming</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/><span class=title>« Prev</span><br><span>Monte Carlo Methods in Reinforcement Learning</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/><span class=title>Next »</span><br><span>Optimal Policy Existence</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on twitter" href="https://twitter.com/intent/tweet/?text=Solving%20MDPs%20with%20Dynamic%20Programming&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f&hashtags=reinforcement-learning%2cdynamic-programming%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f&title=Solving%20MDPs%20with%20Dynamic%20Programming&summary=Solving%20MDPs%20with%20Dynamic%20Programming&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f&title=Solving%20MDPs%20with%20Dynamic%20Programming"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on whatsapp" href="https://api.whatsapp.com/send?text=Solving%20MDPs%20with%20Dynamic%20Programming%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Solving MDPs with Dynamic Programming on telegram" href="https://telegram.me/share/url?text=Solving%20MDPs%20with%20Dynamic%20Programming&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdp-in-mdp%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>