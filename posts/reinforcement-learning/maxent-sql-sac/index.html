<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="reinforcement-learning,deep-reinforcement-learning,policy-gradient,actor-critic,q-learning,model-free,my-rl"><meta name=description content="
Notes on Entropy-Regularized Reinforcement Learning via SQL & SAC
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic"><meta property="og:description" content="
Notes on Entropy-Regularized Reinforcement Learning via SQL & SAC
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-27T13:46:09+07:00"><meta property="article:modified_time" content="2022-12-27T13:46:09+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic"><meta name=twitter:description content="
Notes on Entropy-Regularized Reinforcement Learning via SQL & SAC
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Maximum Entropy Reinforcement Learning via Soft Q-learning \u0026 Soft Actor-Critic","item":"https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Maximum Entropy Reinforcement Learning via Soft Q-learning \u0026 Soft Actor-Critic","name":"Maximum Entropy Reinforcement Learning via Soft Q-learning \u0026 Soft Actor-Critic","description":" Notes on Entropy-Regularized Reinforcement Learning via SQL \u0026amp; SAC\n","keywords":["reinforcement-learning","deep-reinforcement-learning","policy-gradient","actor-critic","q-learning","model-free","my-rl"],"articleBody":" Notes on Entropy-Regularized Reinforcement Learning via SQL \u0026 SAC\nEntropy-Regularized Reinforcement Learning Consider an infinite-horizon Markov Decision Process (MDP), defined as a tuple $(\\mathcal{S},\\mathcal{A},p,r,\\gamma)$, where\n$\\mathcal{S}$ is the state space. $\\mathcal{A}$ is the action space. $p:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to[0,1]$ is the transition probability distribution, i.e. $p(s,a,s’)=p(s’\\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function, and let us denote $r_t\\doteq r(s_t,a_t)$ for simplicity. $\\gamma\\in(0,1)$ is the discount factor. To consider entropy regularization setting, we first recall some basics in standard RL, then extend them into the maximum entropy framework.\nObjective Function Regularly, with discounted infinite-horizon MDP, our objective is to maximize the expected cumulative rewards \\begin{equation} J_\\text{std}(\\pi)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t r_t\\right] \\end{equation} In Entropy-Regularized RL, or Maximum Entropy RL framework, we wish to maximize the expected entropy-augmented return \\begin{equation} J_\\text{MaxEnt}(\\pi)=\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\right],\\label{eq:mr.1} \\end{equation} where $\\alpha\u003e0$ is the temperature parameter determines the relative importance of the entropy with the rewards, and thus controls the stochasticity of the optimal policy, and $\\mathcal{H}(\\pi(\\cdot\\vert s_t))$ is the entropy of the policy $\\pi$ at state $s_t$, which is calculated as $\\mathcal{H}(\\pi(\\cdot\\vert s_t))=-\\log\\pi(\\cdot\\vert s_t)$.\nThe corresponding optimal policy of the maximum entropy objective is then given by \\begin{align} \\pi^*\u0026=\\underset{\\pi}{\\text{argmax}}\\hspace{0.1cm}J_\\text{MaxEnt}(\\pi) \\\\ \u0026=\\underset{\\pi}{\\text{argmax}}\\hspace{0.1cm}\\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\right] \\end{align}\nSoft Value Functions In standard RL, value functions are referred to be the expected returns. Thus, the state-value function and state-action value function in maximum entropy framework could be defined as the expected entropy-augmented returns. Specifically, by adding an entropy term, the state-value function is then referred as soft state value function given by \\begin{equation} V_\\pi(s)=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\Big\\vert s_0=s\\right], \\end{equation} and analogously the soft state-action value function, or soft Q-function is given as \\begin{equation} Q_\\pi(s,a)=\\mathbb{E}_{\\pi,p}\\left[r_0+\\sum_{t=1}^{\\infty}\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)\\Big\\vert s_0=s,a_0=a\\right] \\end{equation} It is worth remarking that those definitions imply that \\begin{align} V_\\pi(s)\u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\Big)\\label{eq:svf.1} \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi}\\big[Q_\\pi(s,a)-\\alpha\\log\\pi(a\\vert s)\\big], \\end{align} and \\begin{align} Q_\\pi(s,a)=r(s,a)+\\gamma\\mathbb{E}_{s’\\sim p}\\big[V_\\pi(s’)\\big]\\label{eq:svf.2} \\end{align}\nSoft Bellman Backup Operators In standard RL, let $\\mathcal{T}_\\pi$ be the Bellman operator1, with which we can compute the expected returns by one-step lookahead, i.e. \\begin{align} (\\mathcal{T}_\\pi V_\\pi)(s)\u0026=\\sum_a\\pi(a\\vert s)\\Big[r(s,a)+\\gamma V_\\pi(s’)\\Big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[r(s,a)+\\gamma\\mathbb{E}_{s’\\sim p}\\big[V_\\pi(s’)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi,s’\\sim p}\\Big[r(s,a)+\\gamma V_\\pi(s’)\\Big]\\label{eq:sbo.1} \\end{align} and \\begin{align} (\\mathcal{T}_\\pi Q_\\pi)(s,a)\u0026=r(s,a)+\\gamma\\sum_{s’,a’}p(s’\\vert s,a)\\pi(a’\\vert s’)Q_\\pi(s’,a’) \\\\ \u0026=r(s,a)+\\gamma\\mathbb{E}_{s’\\sim p,a’\\sim\\pi}\\Big[Q_\\pi(s’,a’)\\Big] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\Big[r(s,a)+\\gamma\\mathbb{E}_{a’\\sim\\pi}\\big[Q_\\pi(s’,a’)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s’\\sim p,a’\\sim\\pi}\\Big[r(s,a)+\\gamma Q_\\pi(s’,a’)\\Big] \\end{align} Repeatedly applying $\\mathcal{T}_\\pi$ operator $n$ times yields the $n$-step Bellman operator, denoted $\\mathcal{T}_\\pi^{(n)}$, which allows us to compute the expected returns with $n$-step lookahead, i.e. \\begin{align} (\\mathcal{T}_\\pi^{(n)}V_\\pi)(s)\u0026=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t r_t+\\gamma^n V_\\pi(s_n)\\Bigg\\vert s_0=s\\right], \\\\ (\\mathcal{T}_\\pi^{(n)}Q_\\pi)(s,a)\u0026=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t r_t+\\gamma^n Q_\\pi(s_n,a_n)\\Bigg\\vert s_0=s,a_0=a\\right] \\end{align} We can generalize the Bellman operator $\\mathcal{T}_\\pi$ to the maximum entropy setting to get a recursive relationship between successive value functions. Specifically, by adding an entropy term, \\eqref{eq:sbo.1} can be rewritten as \\begin{equation} (\\mathcal{T}_\\pi V_\\pi)(s)=\\mathbb{E}_{a\\sim\\pi,s’\\sim p}\\Big[r(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)+\\gamma V_\\pi(s’)\\Big], \\end{equation} and analogously we have \\begin{align} (\\mathcal{T}_\\pi Q_\\pi)(s,a)\u0026=r(s,a)+\\gamma\\mathbb{E}_{s’\\sim p}\\Big[\\mathbb{E}_{a’\\sim\\pi}\\big[Q_\\pi(s’,a’)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s’)\\big)\\Big]\\label{eq:sbo.2} \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\Big[r(s,a)+\\gamma\\Big(\\mathbb{E}_{a’\\sim\\pi}\\big[Q_\\pi(s’,a’)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s’)\\big)\\Big)\\Big] \\\\ \u0026=\\mathbb{E}_{s’\\sim p,a’\\sim\\pi}\\Big[r(s,a)+\\gamma\\Big(Q_\\pi(s’,a’)+\\alpha H\\big(\\pi(\\cdot\\vert s’)\\big)\\Big)\\Big] \\end{align} And also, the $n$-step Bellman operators generalize for entropy regularization framework are given by \\begin{equation} (\\mathcal{T}_\\pi^{(n)}V_\\pi)(s)=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{n-1}\\gamma^t\\Big(r_t+\\gamma H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)+\\gamma^n V_\\pi(s_n)\\Bigg\\vert s_0=s\\right],\\label{eq:sbo.3} \\end{equation} and \\begin{align} \\hspace{-0.8cm}\u0026(\\mathcal{T}_\\pi^{(n)}Q_\\pi)(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\nonumber \\\\ \\hspace{-0.8cm}\u0026=\\mathbb{E}_{\\pi,p}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\Big(r_t+\\alpha H\\big(\\pi(\\cdot\\vert s_t)\\big)\\Big)+\\gamma^n\\Big(Q_\\pi(s_n,a_n)+\\alpha H\\big(\\pi(\\cdot\\vert s_n)\\big)\\Big)\\Bigg\\vert s_0=s,a_0=a\\right] \\end{align} The above $n$-step Bellman operator for state-action value can also be deduced by combining \\eqref{eq:sbo.3} with the result \\eqref{eq:svf.1}.\nGreedy Policy Recall that in standard setting, the greedy policy for state-action value function $Q$ are defined as a deterministic policy that selects the greedy action in the sense that maximizes the state-action value function, i.e. \\begin{equation} \\pi_\\text{g}(s)\\doteq\\underset{a}{\\text{argmax}}\\hspace{0.1cm}Q(s,a) \\end{equation} With entropy-regularized, the greedy policy instead maximizes the entropy-augmented value function, and thus given in stochastic form that for some $s\\in\\mathcal{S}$ \\begin{align} \\pi_\\text{g}(\\cdot\\vert s)\u0026\\doteq\\underset{\\pi}{\\text{argmax}}\\hspace{0.1cm}\\mathbb{E}_{a\\sim\\pi}\\Big[Q(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\label{eq:gp.1} \\\\ \u0026=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q(s,\\cdot)\\right)}{\\mathbb{E}_{a’\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a’)\\right)\\right]}, \\end{align} where $\\tilde{\\pi}$ is some “reference” policy, and thus the denominator is acting as a normalizing constant since it is independent of $\\pi$.\nTo verify this, we begin by considering2 \\begin{align} \\hspace{-1.2cm}H\\big(\\pi(\\cdot\\vert s)\\big)\u0026=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\mathbb{E}_{a\\sim\\pi}\\big[\\log\\pi_\\text{g}(a\\vert s)\\big] \\\\ \u0026=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\mathbb{E}_{a\\sim\\pi}\\left[\\frac{1}{\\alpha}Q(s,a)-\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right]\\right] \\\\ \u0026=-D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)-\\frac{1}{\\alpha}\\mathbb{E}_{a\\sim\\pi}\\big[Q(s,a)\\big]+\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right],\\label{eq:gp.2} \\end{align} where $D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)$ denotes the KL divergence between $\\pi(\\cdot\\vert s)$ and $\\pi_\\text{g}(\\cdot\\vert s)$. The result \\eqref{eq:gp.2} implies that \\begin{equation} \\hspace{-1.2cm}\\mathbb{E}_{a\\sim\\pi}\\big[Q(s,a)\\big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)=-\\alpha D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)+\\alpha\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right] \\end{equation} Since $\\alpha\\log\\mathbb{E}_{a\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q(s,a)\\right)\\right]$ does not depend on $\\pi$ and $\\alpha\\in[0,1]$, the policy $\\pi$ that maximizes LHS is the one that minimizes $D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\pi_\\text{g}(\\cdot\\vert s)\\big)$, which proves our claim due to the fact that KL divergence between two distributions is $\\geq 0$ with equality holds when they are identical.\nBackup Operators for Greedy Policy It has been shown that we can also define backup operators for value functions corresponding to greedy policy $\\pi_\\text{g}$. In particular, we have \\begin{align} \\hspace{-1.2cm}(\\mathcal{T}Q_{\\pi_\\text{g}})(s,a)\u0026=\\mathbb{E}_{s’\\sim p}\\Big[r(s,a)+\\gamma\\Big(\\mathbb{E}_{a’\\sim\\pi_\\text{g}}\\big[Q_{\\pi_\\text{g}}(s’,a’)\\big]+\\alpha H\\big(\\pi_\\text{g}(\\cdot\\vert s’)\\big)\\Big)\\Big] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\left[r(s,a)+\\gamma\\left(\\mathbb{E}_{a’\\sim\\pi_\\text{g}}\\big[Q_{\\pi_\\text{g}}(s’,a’)\\big]-\\alpha\\sum_{a’}\\pi_\\text{g}(a’\\vert s’)\\log\\pi_\\text{g}(a’\\vert s’)\\right)\\right] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\left[r(s,a)+\\gamma\\sum_{a’}\\pi_\\text{g}(a’\\vert s’)\\big(Q_{\\pi_\\text{g}}(s’,a’)-\\alpha\\log\\pi_\\text{g}(a’\\vert s’)\\big)\\right] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\left[r(s,a)+\\gamma\\sum_{a’}\\pi_\\text{g}(a’\\vert s’)\\alpha\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s’,\\tilde{a})\\right)\\right]\\right] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\Bigg[r(s,a)+\\gamma\\alpha\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s’,\\tilde{a})\\right)\\right]\\Bigg] \\\\ \u0026=\\mathbb{E}_{s’\\sim p}\\Bigg[r(s,a)+\\gamma\\alpha\\log\\mathbb{E}_{a’\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s’,a’)\\right)\\right]\\Bigg]\\label{eq:bogp.1} \\end{align} where in the fifth step, we use the fact that $\\log\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{g}}(s’,\\tilde{a})\\right)\\right]$ is independent of $a’$, which allows us to do the summation of $\\pi_\\text{g}$ over the action space $\\mathcal{A}$, i.e. \\begin{equation} \\sum_{a’}\\pi_\\text{g}(a’\\vert s’)=1 \\end{equation}\nSoft Policy Iteration In standard RL, the Bellman operator provides useful facts to apply the dynamic programming method - policy iteration, which alternates between policy evaluation and policy improvement processes, and eventually we end up with the optimal policy. More importantly, it has been proved that we can also apply the method to entropy-regularized RL.\nSoft Policy Evaluation In policy evaluation process, we wish to compute the value of a given policy according to the entropy regularization objective \\eqref{eq:mr.1}. This can be found by an iterative method.\nLemma 1 (Soft Policy Evaluation). Consider the Bellman backup operator $\\mathcal{T}_\\pi$ specified in \\eqref{eq:sbo.2} and a mapping $Q^{(0)}:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ with $\\vert\\mathcal{A}\\vert\\lt\\infty$, and define $Q^{(k+1)}=\\mathcal{T}_\\pi Q^{(k)}$. The resulting sequence $\\{Q^{(k)}\\}_{k=0,1\\ldots,}$ will converge as $k\\to\\infty$.\nProof\nLet $r_\\pi(s,a)\\doteq r(s,a)+\\gamma\\mathbb{E}_{s’\\sim p}\\big[\\alpha H\\big(\\pi(\\cdot\\vert s’)\\big)\\big]$ denote the entropy augmented reward, the update rule can be rewritten as \\begin{equation} Q^{(k+1)}(s,a)=r_\\pi(s,a)+\\gamma\\mathbb{E}_{s’\\sim p,a’\\sim\\pi}\\big[Q^{(k)}(s’,a’)\\big] \\end{equation} Since $\\vert\\mathcal{A}\\vert\u003c\\infty$, we have that $r_\\pi(s,a)$ is bounded. Analogy to the (standard) policy evaluation, we then can prove that $\\mathcal{T}_\\pi$ is a contraction mapping and then by using the Banach’s fixed point theorem, we can show that $\\{Q^{(k)}\\}_{k=0,1,\\ldots}$ eventually converges to a fixed point, which we call it the soft Q-value of $\\pi$.\nSoft Policy Improvement Analogously, the (standard) policy improvement step can be generalized to entropy regularizing as:\nLemma 2 (Soft Policy Improvement) Let $\\Pi$ be some set of policies, $\\pi_\\text{old}\\in\\Pi$ be some policy and for each $s_t$ let $\\pi_\\text{new}$ be defined as \\begin{equation} \\pi_\\text{new}(\\cdot\\vert s_t)=\\underset{\\pi’\\in\\Pi}{\\text{argmin}}D_\\text{KL}\\left(\\pi’(\\cdot\\vert s_t)\\Bigg\\Vert\\frac{\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,\\cdot)\\right)}{\\mathbb{E}_{a_t\\sim\\tilde{\\pi}_\\text{old}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,a_t)\\right)\\right]}\\right) \\end{equation} Then $Q_{\\pi_\\text{new}}(s_t,a_t)\\geq Q_{\\pi_\\text{old}}(s_t,a_t)$ for all $(s_t,a_t)\\in\\mathcal{S}\\times\\mathcal{A}$ with $\\vert\\mathcal{A}\\vert\\lt\\infty$.\nProof\nAs KL divergence between two distribution reaches its minimum when those two distributions are identical, we then have that for each $s_t\\in\\mathcal{S}$ \\begin{equation} \\pi_\\text{new}(\\cdot\\vert s_t)=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,\\cdot)\\right)}{\\mathbb{E}_{a_t\\sim\\tilde{\\pi}_\\text{old}}\\left[\\exp\\left(\\frac{1}{\\alpha}Q_{\\pi_\\text{old}}(s_t,a_t)\\right)\\right]}, \\end{equation} which is the greedy policy $\\pi_\\text{g}$, and thus by \\eqref{eq:gp.1}, for all $s_t\\in\\mathcal{S}$, we have \\begin{align} \\mathbb{E}_{a_t\\sim\\pi_\\text{new}}\\big[Q_{\\pi_\\text{old}}(s_t,a_t)\\big]+\\alpha H\\big(\\pi_\\text{new}(\\cdot\\vert s_t)\\big)\u0026\\geq\\mathbb{E}_{a_t\\sim\\pi_\\text{old}}\\big[Q_{\\pi_\\text{old}}(s_t,a_t)\\big]+\\alpha H\\big(\\pi_\\text{old}(\\cdot\\vert s_t)\\big) \\\\ \u0026=V_{\\pi_\\text{old}}(s_t) \\end{align} Therefore, combined with \\eqref{eq:svf.2} we obtain \\begin{align} Q_{\\pi_\\text{old}}(s_t,a_t)\u0026=r_t+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\big[V_{\\pi_\\text{old}}(s_{t+1})\\big] \\\\ \u0026\\leq r_t+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\Big[\\mathbb{E}_{a_{t+1}\\sim\\pi_\\text{new}}\\big[Q_{\\pi_\\text{old}}(s_{t+1},a_{t+1})\\big]+\\alpha H\\big(\\pi_\\text{new}(\\cdot\\vert s_{t+1})\\big)\\Big] \\\\ \u0026\\hspace{0.3cm}\\vdots \\\\ \u0026\\leq Q_{\\pi_\\text{new}}(s_t,a_t) \\end{align}\nNow we are ready to specify the policy iteration for entropy-regularized RL.\nTheorem 3 (Soft Policy Iteration) Repeated application of soft policy evaluation and soft policy improvement to any $\\pi\\in\\Pi$ converges to a policy $\\pi^*$ such that $Q_{\\pi^∗}(s_t,a_t)\\geq Q_\\pi(s_t,a_t)$ for all $\\pi\\in\\Pi$ and for all $(s_t,a_t)\\in\\mathcal{S}\\times\\mathcal{A}$ , assuming $\\vert\\mathcal{A}\\vert\\lt\\infty$.\nProof\nThis can be easily proved since the soft Bellman operator $\\mathcal{T}_\\pi$ defined in \\eqref{eq:sbo.2} and backup operator $\\mathcal{T}$ given in \\eqref{eq:bogp.1} both are contractions.\nSoft Actor-Critic In large scale problems, it is impractical to run either policy evaluation or policy improvement until convergence. It is then necessary to use an approximation version of soft policy iteration, which we call Soft Actor-Critic, or SAC. SAC instead uses function approximators (neural network function approximation is our choice) for both the policy and the soft Q-function, and rather than alternating between evaluation and improvement to convergence, using SGD to optimize both networks.\nThese following are key components of SAC method:\nOne policy $\\pi_\\phi$ and two soft Q-functions $Q_{\\theta_1},Q_{\\theta_2}$. Utilizing shared target Q-networks, $Q_{\\overline{\\theta}_1},Q_{\\overline{\\theta}_2}$, whose parameters are soft updated due to \\begin{equation} \\bar{\\theta}_i\\leftarrow\\tau\\theta_i+(1-\\tau)\\bar{\\theta}_i,\\hspace{2cm}i=\\{1,2\\} \\end{equation} where $\\tau\\in(0,1]$ and close to $0$. Off-policy training with samples from a replay buffer $\\mathcal{D}$ to minimize correlations between samples. The rollout phase is given as for each step $t$ \\begin{align*} \u0026a_t\\sim\\pi_\\phi(a_t\\vert s_t) \\\\ \u0026s_{t+1}\\sim p(s_{t+1}\\vert s_t,a_t) \\\\ \u0026\\mathcal{D}\\leftarrow\\mathcal{D}\\cup\\{s_t,a_t,r(s_t,a_t),s_{t+1},d_t\\}, \\end{align*} where $d_t$ informs whether $s_{t+1}$ is the terminal state. The Q-function parameters are trained to minimize the Mean Square Bellman Error (MSBE) \\begin{equation} J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim\\mathcal{D}}\\left[\\frac{1}{2}\\Big(y_t-Q_\\theta(s_t,a_t)\\Big)^2\\right], \\end{equation} where $y_t$ is the TD target at step $t$, which is given by \\begin{align} y_t\u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p}\\big[V_\\overline{\\theta}(s_{t+1})\\big]\\label{eq:sac.6} \\\\ \u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p,a_{t+1}\\sim\\pi_\\phi}\\Big[Q_\\overline{\\theta}(s_{t+1},a_{t+1})+\\alpha H\\big(\\pi_\\phi(\\cdot\\vert s_{t+1})\\big)\\Big] \\\\ \u0026=r(s_t,a_t)+\\gamma\\mathbb{E}_{s_{t+1}\\sim p,a_{t+1}\\sim\\pi_\\phi}\\Big[Q_\\overline{\\theta}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\Big], \\end{align} which, as an expectation, can be approximated with samples from replay buffer $\\mathcal{D}$ (i.e. $s_{t+1}\\sim\\mathcal{D}$ since $s_{t+1}\\sim p(s_{t+1}\\vert s_t,a_t)$ while $(s_t,a_t)\\sim\\mathcal{D}$, which is the reason why we attached $s_{t+1}$ in the replay buffer $\\mathcal{D}$ as in the key point (3)) with current policy $\\pi_\\phi$ (i.e. $a_{t+1}\\sim\\pi_\\phi$): \\begin{equation} y_t\\approx r(s_t,a_t)+\\gamma\\big(Q_\\overline{\\theta}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\big) \\end{equation} Therefore, the loss function for Q-networks at step $t$ is given by \\begin{equation} J_Q(\\theta)=\\mathbb{E}_{(s_t,a_t,r,s_{t+1},d_t)\\sim\\mathcal{D},a_{t+1}\\sim\\pi_\\phi}\\left[\\frac{1}{2}\\big(y(r,s_{t+1},a_{t+1},d_t)-Q_\\theta(s_t,a_t)\\big)^2\\right],\\label{eq:sac.1} \\end{equation} where \\begin{equation} y(r,s_{t+1},a_{t+1},d_t)=r+\\gamma(1-d_t)\\big(Q_\\overline{\\theta}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\big)\\label{eq:sac.2} \\end{equation} The loss function $J_Q(\\theta)$ in \\eqref{eq:sac.1} then can be optimized according to SGD using \\begin{equation} \\hat{\\nabla}_\\theta J_Q(\\theta)=\\nabla_\\theta Q_\\theta(s_t,a_t)\\big(y_t-Q_\\theta(s_t,a_t)\\big),\\label{eq:sac.3} \\end{equation} where $y_t$ is the TD target at time-step $t$, which can be computed according \\eqref{eq:sac.2}. The policy, in each state, acts greedily as \\eqref{eq:gp.1}, which maximizes the expected entropy-augmented return, which is $V_\\pi(s)$ \\begin{align} V_\\pi(s)\u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big)\\Big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q_\\pi(s,a)-\\alpha\\log\\pi(a\\vert s)\\Big] \\end{align} Hence, the policy parameters $\\phi$ can be learned by directly maximizing \\begin{align} J_\\pi(\\phi)\u0026=\\mathbb{E}_{s_t\\sim\\mathcal{D}}\\Big[\\mathbb{E}_{a_t\\sim\\pi_\\phi}\\big[Q_\\theta(s_t,a_t)-\\alpha\\log\\pi_\\phi(a_t\\vert s_t)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s_t\\sim\\mathcal{D},a_t\\sim\\pi_\\phi}\\Big[Q_\\theta(s_t,a_t)-\\alpha\\log\\pi_\\phi(a_t\\vert s_t)\\Big]\\label{eq:sac.4} \\end{align} Since Q-function is represented by a neural network and can be differentiated, in SAC paper, the authors make use of the reparameterization trick to reduce variance. In particular, samples are obtained according to \\begin{equation} a_\\phi(s_t,\\epsilon_t)=\\text{tanh}(\\mu_\\phi(s_t)+\\sigma_\\phi(s_t)\\odot\\epsilon_t) \\end{equation} where $\\epsilon_t\\sim\\mathcal{N}(0,I)$ is a spherical Gaussian noise, $\\mu_\\phi$ and $\\sigma_\\phi$ are defined as given in the next key point. The loss function in \\eqref{eq:sac.4} then can be rewritten as \\begin{equation} J_\\pi(\\phi)=\\mathbb{E}_{s_t\\sim\\mathcal{D},\\epsilon_t\\sim\\mathcal{N}(0,I)}\\Big[Q_\\theta\\big(s_t,a_\\phi(s_t,\\epsilon_t)\\big)-\\alpha\\log\\pi_\\phi\\big(a_\\phi(s_t,\\epsilon_t)\\vert s_t\\big)\\Big], \\end{equation} which rather than taking the expectation over actions ($a_t\\sim\\pi_\\phi$, depends on $\\phi$), computing over noise ($\\epsilon_t\\sim\\mathcal{N}(0,I)$, depends on nothing). This function can be optimized with SGD with \\begin{equation} \\hspace{-0.9cm}\\hat{\\nabla}_\\phi J_\\pi(\\phi)=\\big(\\nabla_{a_t}Q_\\theta(s_t,a_t)-\\alpha\\nabla_{a_t}\\log\\pi_\\phi(a_t\\vert s_t)\\big)\\nabla_\\phi a_\\phi(s_t,\\epsilon_t)-\\alpha\\nabla_\\phi\\log\\pi_\\phi(a_t\\vert s_t),\\label{eq:sac.5} \\end{equation} where $a_t$ is evaluated at $a_\\phi(s_t,\\epsilon_t)$. For continuous action space tasks, a stochastic policy is usually given in form of a diagonal Gaussian, i.e. \\begin{equation} \\pi_\\phi(\\cdot\\vert s)=\\mathcal{N}(\\mu_\\phi(s),\\Sigma_\\phi(s))=\\mathcal{N}(\\mu_\\phi(s),\\sigma_\\theta(s)^2 I)=\\mu_\\phi(s)+\\sigma_\\phi(s)\\mathcal{N}(0,I) \\end{equation} hence, when sampling from $\\pi_\\phi$, let $\\epsilon_t\\sim\\mathcal{N}(0,I)$ be a vector of spherical Gaussian noise, an action $a_t\\sim\\mathcal{N}(\\mu_\\theta(s),\\sigma_\\phi(s)^2 I)$ can be computed as \\begin{equation} a_t=\\mu_\\phi(s_t)+\\sigma_\\phi(s_t)\\odot\\epsilon_t, \\end{equation} where $\\odot$ denotes the elementwise product of two vectors.\nSince the normal distribution taking range of $(-\\infty,\\infty)$, it is necessary to bound the policy to a finite interval, which can be performed by applying a squashing function (e.g. $\\text{tanh}$, sigmoid, etc) to the Gaussian samples. For instance, the $\\text{tanh}$ function converts support of $(-\\infty,\\infty)$ into $(-1,1)$. Two soft Q-functions $Q_{\\theta_1},Q_{\\theta_2}$ are trained independently to optimize $J_Q(\\theta_1),J_Q(\\theta_2)$ respectively. Also, the minimum of the soft Q-functions is used in \\eqref{eq:sac.3} and \\eqref{eq:sac.5} instead, i.e. \\begin{align} \\hat{\\nabla}_\\theta J_Q(\\theta)\u0026=\\nabla_\\theta Q_\\theta(s_t,a_t)\\big(y_t-Q_\\theta(s_t,a_t)\\big), \\\\ \\hat{\\nabla}_\\phi J_\\pi(\\phi)\u0026=\\big(\\nabla_{a_t}\\min_{i=1,2}Q_{\\theta_i}(s_t,a_t)-\\alpha\\nabla_{a_t}\\log\\pi_\\phi(a_t\\vert s_t)\\big)\\nabla_\\phi a_\\phi(s_t,\\epsilon_t)\\nonumber \\\\ \u0026\\hspace{2cm}-\\alpha\\nabla_\\phi\\log\\pi_\\phi(a_t\\vert s_t) \\end{align} where \\begin{equation} y_t=r+\\gamma(1-d_t)\\left(\\min_{i=1,2}Q_{\\overline{\\theta}_i}(s_{t+1},a_{t+1})-\\alpha\\log\\pi_\\phi(a_{t+1}\\vert s_{t+1})\\right) \\end{equation} Instead of considering entropy coefficient $\\alpha$ as a constant, in the latter version of SAC, authors treated it as a parameter and can be optimized due to the loss function \\begin{equation} J(\\alpha)=\\mathbb{E}_{a_t\\sim\\pi_t}\\big[-\\alpha\\log\\pi_t(a_t\\vert s_t)-\\alpha\\bar{H}\\big]\\label{eq:sac.7} \\end{equation} where $\\pi_t$ denotes the current policy at time-step $t$ and $\\bar{H}$ is target entropy value, usually is set as $-\\text{dim}(\\mathcal{A})$. Pseudocode for our final algorithm is given below.\nDiscrete SAC In discrete-action setting, the policy $\\pi_\\phi(a\\vert s)$ can be consider as a PFM (probability mass function), instead of a density function in the continuous case. This gives rise to some necessary changes:\nWe use a Categorical policy instead of a diagonal Gaussian, i.e. $\\pi:\\mathcal{S}\\to[0,1]^{\\vert\\mathcal{A}\\vert}$. The PMF-form policy allows us to compute the soft value function directly instead of using Monte Carlo sampling, i.e. \\begin{equation} V_\\pi(s)=\\sum_{a\\in\\mathcal{A}}\\pi(a\\vert s)\\big[Q_\\pi(s,a)-\\alpha\\log\\pi(a\\vert s)\\big],\\label{eq:dsac.1} \\end{equation} which reduces the variance in Monte Carlo estimate of the $V_\\overline{\\theta}(s_{t+1})$ given in \\eqref{eq:sac.6} that is used to compute the loss function of Q-networks $J_Q(\\theta)$.\nIt is then more efficient to make a modification to the soft Q-function: The soft Q-function returns a value for each possible action rather than simply the action provided as an input, i.e. $Q:\\mathcal{S}\\to\\mathbb{R}^{\\vert\\mathcal{A}\\vert}$.\nThis change, along with the previous one, lets us rewrite the calculation for soft state-value given in \\eqref{eq:dsac.1} as \\begin{equation} V_\\pi(s)=\\pi(s)^\\text{T}\\big[Q_\\pi(s)-\\alpha\\log\\pi(s)\\big] \\end{equation} Analogously, the entropy coefficient loss changes from \\eqref{eq:sac.7} to \\begin{equation} J(\\alpha)=\\pi_t(s_t)^\\text{T}\\big[-\\alpha\\log\\pi_t(s_t)-\\alpha\\bar{H}\\big] \\end{equation} Since the policy $\\pi$ now returns the exact action, we are able to compute the expectation directly. Thus, it is unnecessary to use the reparameterization trick in the calculation for the loss function $J_\\pi(\\phi)$. It is then given by \\begin{equation} J_\\pi(\\phi)=\\mathbb{E}_{s_t\\sim\\mathcal{D}}\\Big[\\pi_\\phi(s_t)^\\text{T}\\big[Q_\\theta(s_t)-\\alpha\\log\\pi_\\phi(s_t)\\big]\\Big] \\end{equation} Soft Q-learning References [1] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine. Reinforcement Learning with Deep Energy-Based Policies. ICML, 2017.\n[2] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. arXiv preprint, arXiv:1812.05905, 2018.\n[3] Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine. Soft Actor-Critic Algorithms and Applications. arXiv preprint, arXiv:1812.05905, 2019.\n[4] Brian D. Ziebart. Modeling purposeful adaptive behavior with the principle of maximum causal entropy. PhD Thesis, Carnegie Mellon University, 2010.\n[5] John Schulman, Xi Chen, Pieter Abbeel. Equivalence Between Policy Gradients and Soft Q-Learning. arXiv preprint arXiv:1704.06440, 2018.\n[6] Csaba Szepesvári. Algorithms for Reinforcement Learning. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2010.\n[7] Richard S. Sutton, Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[8] Josh Achiam. Spinning Up in Deep Reinforcement Learning. SpinningUp2018, 2018.\n[9] Petros Christodoulou. Soft Actor-Critic for Discrete Action Settings. arXiv preprint, arXiv:1910.07207.\nFootnotes With an abuse of notation, $\\mathcal{T}_\\pi$ implicitly represents two mappings $\\mathcal{T}_\\pi:\\mathcal{S}\\to\\mathcal{S}$ and $\\mathcal{T}_\\pi’:\\mathcal{S}\\times\\mathcal{A}\\to\\mathcal{S}\\times\\mathcal{A}$. ↩︎\nThe formula for greedy policy can also be derived by another way. First off, consider the objective we have \\begin{align*} J(\\pi(\\cdot\\vert s))\u0026=\\mathbb{E}_{a\\sim\\pi}\\Big[Q(s,a)\\Big]+\\alpha H\\big(\\pi(\\cdot\\vert s)\\big) \\\\ \u0026=\\sum_{a\\sim\\pi}\\pi(a\\vert s)Q(s,a)-\\sum_{a\\sim\\pi}\\pi(a\\vert s)\\log\\pi(a\\vert s) \\\\ \u0026=\\sum_{a}\\pi(a\\vert s)\\big(Q(s,a)-\\alpha\\log\\pi(a\\vert s)\\big) \\end{align*} Thus, the partial derivative of the $J(\\pi(\\cdot\\vert s))$ w.r.t $\\pi(\\bar{a}\\vert s)$ for some $\\bar{a}\\in\\mathcal{A}$ is given as \\begin{align*} \\nabla_{\\pi(\\bar{a}\\vert s)}J(\\pi(\\cdot\\vert s))\u0026=\\nabla_{\\pi(\\bar{a}\\vert s)}\\sum_a\\pi(a\\vert s)\\Big[Q(s,a)-\\alpha\\log\\pi(a\\vert s)\\Big] \\\\ \u0026=Q(s,\\bar{a})-\\alpha\\log\\pi(\\bar{a}\\vert s)-\\alpha\\pi(\\bar{a}\\vert s)\\cdot\\frac{1}{\\pi(\\bar{a}\\vert s)} \\\\ \u0026=Q(s,\\bar{a})-\\alpha\\log\\pi(\\bar{a}\\vert s)-\\alpha \\end{align*} Setting the derivative to zero yields \\begin{equation*} \\pi(\\bar{a}\\vert s)=\\frac{\\exp\\left(\\frac{1}{\\alpha}Q(s,\\bar{a})\\right)}{\\exp\\alpha} \\end{equation*} ↩︎\n","wordCount":"2309","inLanguage":"en","datePublished":"2022-12-27T13:46:09+07:00","dateModified":"2022-12-27T13:46:09+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/maxent-sql-sac/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://trunghng.github.io>Home</a>&nbsp;»&nbsp;<a href=https://trunghng.github.io/posts/>Posts</a></div><h1 class=post-title>Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic</h1><div class=post-meta><span title='2022-12-27 13:46:09 +0700 +0700'>December 27, 2022</span>&nbsp;·&nbsp;11 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#maxent-rl>Entropy-Regularized Reinforcement Learning</a><ul><li><a href=#objective-func>Objective Function</a></li><li><a href=#soft-val-funcs>Soft Value Functions</a></li><li><a href=#soft-bellman-op>Soft Bellman Backup Operators</a></li><li><a href=#greedy-policy>Greedy Policy</a></li><li><a href=#backup-op-greedy-policy>Backup Operators for Greedy Policy</a></li></ul></li><li><a href=#soft-policy-iter>Soft Policy Iteration</a><ul><li><a href=#soft-policy-eval>Soft Policy Evaluation</a></li><li><a href=#soft-policy-imp>Soft Policy Improvement</a></li></ul></li><li><a href=#sac>Soft Actor-Critic</a><ul><li><a href=#discrete-sac>Discrete SAC</a></li></ul></li><li><a href=#sql>Soft Q-learning</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on Entropy-Regularized Reinforcement Learning via SQL & SAC</p></blockquote><h2 id=maxent-rl>Entropy-Regularized Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#maxent-rl>#</a></h2><p>Consider an infinite-horizon Markov Decision Process (MDP), defined as a tuple $(\mathcal{S},\mathcal{A},p,r,\gamma)$, where</p><ul><li>$\mathcal{S}$ is the <strong>state space</strong>.</li><li>$\mathcal{A}$ is the <strong>action space</strong>.</li><li>$p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>transition probability distribution</strong>, i.e. $p(s,a,s&rsquo;)=p(s&rsquo;\vert s,a)$ denotes the probability of transitioning to state $s&rsquo;$ when taking action $a$ from state $s$.</li><li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>, and let us denote $r_t\doteq r(s_t,a_t)$ for simplicity.</li><li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li></ul><p>To consider entropy regularization setting, we first recall some basics in standard RL, then extend them into the maximum entropy framework.</p><h3 id=objective-func>Objective Function<a hidden class=anchor aria-hidden=true href=#objective-func>#</a></h3><p>Regularly, with discounted infinite-horizon MDP, our objective is to maximize the expected cumulative rewards
\begin{equation}
J_\text{std}(\pi)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r_t\right]
\end{equation}
In <strong>Entropy-Regularized RL</strong>, or <strong>Maximum Entropy RL</strong> framework, we wish to maximize the expected <strong>entropy-augmented return</strong>
\begin{equation}
J_\text{MaxEnt}(\pi)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t\Big(r_t+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big)\right],\label{eq:mr.1}
\end{equation}
where $\alpha>0$ is the temperature parameter determines the relative importance of the entropy with the rewards, and thus controls the stochasticity of the optimal policy, and $\mathcal{H}(\pi(\cdot\vert s_t))$ is the entropy of the policy $\pi$ at state $s_t$, which is calculated as $\mathcal{H}(\pi(\cdot\vert s_t))=-\log\pi(\cdot\vert s_t)$.</p><p>The corresponding optimal policy of the maximum entropy objective is then given by
\begin{align}
\pi^*&=\underset{\pi}{\text{argmax}}\hspace{0.1cm}J_\text{MaxEnt}(\pi) \\ &=\underset{\pi}{\text{argmax}}\hspace{0.1cm}\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t\Big(r_t+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big)\right]
\end{align}</p><h3 id=soft-val-funcs>Soft Value Functions<a hidden class=anchor aria-hidden=true href=#soft-val-funcs>#</a></h3><p>In standard RL, value functions are referred to be the expected returns. Thus, the state-value function and state-action value function in maximum entropy framework could be defined as the expected entropy-augmented returns. Specifically, by adding an entropy term, the state-value function is then referred as <strong>soft state value function</strong> given by
\begin{equation}
V_\pi(s)=\mathbb{E}_{\pi,p}\left[\sum_{t=0}^{\infty}\gamma^t\Big(r_t+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big)\Big\vert s_0=s\right],
\end{equation}
and analogously the <strong>soft state-action value function</strong>, or <strong>soft Q-function</strong> is given as
\begin{equation}
Q_\pi(s,a)=\mathbb{E}_{\pi,p}\left[r_0+\sum_{t=1}^{\infty}\Big(r_t+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big)\Big\vert s_0=s,a_0=a\right]
\end{equation}
It is worth remarking that those definitions imply that
\begin{align}
V_\pi(s)&=\mathbb{E}_{a\sim\pi}\Big[Q_\pi(s,a)\Big]+\alpha H\big(\pi(\cdot\vert s)\Big)\label{eq:svf.1} \\ &=\mathbb{E}_{a\sim\pi}\big[Q_\pi(s,a)-\alpha\log\pi(a\vert s)\big],
\end{align}
and
\begin{align}
Q_\pi(s,a)=r(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p}\big[V_\pi(s&rsquo;)\big]\label{eq:svf.2}
\end{align}</p><h3 id=soft-bellman-op>Soft Bellman Backup Operators<a hidden class=anchor aria-hidden=true href=#soft-bellman-op>#</a></h3><p>In standard RL, let $\mathcal{T}_\pi$ be the Bellman operator<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, with which we can compute the expected returns by one-step lookahead, i.e.
\begin{align}
(\mathcal{T}_\pi V_\pi)(s)&=\sum_a\pi(a\vert s)\Big[r(s,a)+\gamma V_\pi(s&rsquo;)\Big] \\ &=\mathbb{E}_{a\sim\pi}\Big[r(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p}\big[V_\pi(s&rsquo;)\big]\Big] \\ &=\mathbb{E}_{a\sim\pi,s&rsquo;\sim p}\Big[r(s,a)+\gamma V_\pi(s&rsquo;)\Big]\label{eq:sbo.1}
\end{align}
and
\begin{align}
(\mathcal{T}_\pi Q_\pi)(s,a)&=r(s,a)+\gamma\sum_{s&rsquo;,a&rsquo;}p(s&rsquo;\vert s,a)\pi(a&rsquo;\vert s&rsquo;)Q_\pi(s&rsquo;,a&rsquo;) \\ &=r(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p,a&rsquo;\sim\pi}\Big[Q_\pi(s&rsquo;,a&rsquo;)\Big] \\ &=\mathbb{E}_{s&rsquo;\sim p}\Big[r(s,a)+\gamma\mathbb{E}_{a&rsquo;\sim\pi}\big[Q_\pi(s&rsquo;,a&rsquo;)\big]\Big] \\ &=\mathbb{E}_{s&rsquo;\sim p,a&rsquo;\sim\pi}\Big[r(s,a)+\gamma Q_\pi(s&rsquo;,a&rsquo;)\Big]
\end{align}
Repeatedly applying $\mathcal{T}_\pi$ operator $n$ times yields the $n$-step Bellman operator, denoted $\mathcal{T}_\pi^{(n)}$, which allows us to compute the expected returns with $n$-step lookahead, i.e.
\begin{align}
(\mathcal{T}_\pi^{(n)}V_\pi)(s)&=\mathbb{E}_{\pi,p}\left[\sum_{t=0}^{n-1}\gamma^t r_t+\gamma^n V_\pi(s_n)\Bigg\vert s_0=s\right], \\ (\mathcal{T}_\pi^{(n)}Q_\pi)(s,a)&=\mathbb{E}_{\pi,p}\left[\sum_{t=0}^{n-1}\gamma^t r_t+\gamma^n Q_\pi(s_n,a_n)\Bigg\vert s_0=s,a_0=a\right]
\end{align}
We can generalize the Bellman operator $\mathcal{T}_\pi$ to the maximum entropy setting to get a recursive relationship between successive value functions. Specifically, by adding an entropy term, \eqref{eq:sbo.1} can be rewritten as
\begin{equation}
(\mathcal{T}_\pi V_\pi)(s)=\mathbb{E}_{a\sim\pi,s&rsquo;\sim p}\Big[r(s,a)+\alpha H\big(\pi(\cdot\vert s)\big)+\gamma V_\pi(s&rsquo;)\Big],
\end{equation}
and analogously we have
\begin{align}
(\mathcal{T}_\pi Q_\pi)(s,a)&=r(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p}\Big[\mathbb{E}_{a&rsquo;\sim\pi}\big[Q_\pi(s&rsquo;,a&rsquo;)\big]+\alpha H\big(\pi(\cdot\vert s&rsquo;)\big)\Big]\label{eq:sbo.2} \\ &=\mathbb{E}_{s&rsquo;\sim p}\Big[r(s,a)+\gamma\Big(\mathbb{E}_{a&rsquo;\sim\pi}\big[Q_\pi(s&rsquo;,a&rsquo;)\big]+\alpha H\big(\pi(\cdot\vert s&rsquo;)\big)\Big)\Big] \\ &=\mathbb{E}_{s&rsquo;\sim p,a&rsquo;\sim\pi}\Big[r(s,a)+\gamma\Big(Q_\pi(s&rsquo;,a&rsquo;)+\alpha H\big(\pi(\cdot\vert s&rsquo;)\big)\Big)\Big]
\end{align}
And also, the $n$-step Bellman operators generalize for entropy regularization framework are given by
\begin{equation}
(\mathcal{T}_\pi^{(n)}V_\pi)(s)=\mathbb{E}_{\pi,p}\left[\sum_{t=0}^{n-1}\gamma^t\Big(r_t+\gamma H\big(\pi(\cdot\vert s_t)\big)\Big)+\gamma^n V_\pi(s_n)\Bigg\vert s_0=s\right],\label{eq:sbo.3}
\end{equation}
and
\begin{align}
\hspace{-0.8cm}&(\mathcal{T}_\pi^{(n)}Q_\pi)(s,a)+\alpha H\big(\pi(\cdot\vert s)\big)\nonumber \\ \hspace{-0.8cm}&=\mathbb{E}_{\pi,p}\left[\sum_{t=0}^{\infty}\gamma^t\Big(r_t+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big)+\gamma^n\Big(Q_\pi(s_n,a_n)+\alpha H\big(\pi(\cdot\vert s_n)\big)\Big)\Bigg\vert s_0=s,a_0=a\right]
\end{align}
The above $n$-step Bellman operator for state-action value can also be deduced by combining \eqref{eq:sbo.3} with the result \eqref{eq:svf.1}.</p><h3 id=greedy-policy>Greedy Policy<a hidden class=anchor aria-hidden=true href=#greedy-policy>#</a></h3><p>Recall that in standard setting, the <strong>greedy policy</strong> for state-action value function $Q$ are defined as a deterministic policy that selects the greedy action in the sense that maximizes the state-action value function, i.e.
\begin{equation}
\pi_\text{g}(s)\doteq\underset{a}{\text{argmax}}\hspace{0.1cm}Q(s,a)
\end{equation}
With entropy-regularized, the greedy policy instead maximizes the entropy-augmented value function, and thus given in stochastic form that for some $s\in\mathcal{S}$
\begin{align}
\pi_\text{g}(\cdot\vert s)&\doteq\underset{\pi}{\text{argmax}}\hspace{0.1cm}\mathbb{E}_{a\sim\pi}\Big[Q(s,a)\Big]+\alpha H\big(\pi(\cdot\vert s)\big)\label{eq:gp.1} \\ &=\frac{\exp\left(\frac{1}{\alpha}Q(s,\cdot)\right)}{\mathbb{E}_{a&rsquo;\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q(s,a&rsquo;)\right)\right]},
\end{align}
where $\tilde{\pi}$ is some &ldquo;reference&rdquo; policy, and thus the denominator is acting as a normalizing constant since it is independent of $\pi$.</p><p>To verify this, we begin by considering<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
\begin{align}
\hspace{-1.2cm}H\big(\pi(\cdot\vert s)\big)&=-D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)-\mathbb{E}_{a\sim\pi}\big[\log\pi_\text{g}(a\vert s)\big] \\ &=-D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)-\mathbb{E}_{a\sim\pi}\left[\frac{1}{\alpha}Q(s,a)-\log\mathbb{E}_{a\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q(s,a)\right)\right]\right] \\ &=-D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)-\frac{1}{\alpha}\mathbb{E}_{a\sim\pi}\big[Q(s,a)\big]+\log\mathbb{E}_{a\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q(s,a)\right)\right],\label{eq:gp.2}
\end{align}
where $D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)$ denotes the KL divergence between $\pi(\cdot\vert s)$ and $\pi_\text{g}(\cdot\vert s)$. The result \eqref{eq:gp.2} implies that
\begin{equation}
\hspace{-1.2cm}\mathbb{E}_{a\sim\pi}\big[Q(s,a)\big]+\alpha H\big(\pi(\cdot\vert s)\big)=-\alpha D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)+\alpha\log\mathbb{E}_{a\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q(s,a)\right)\right]
\end{equation}
Since $\alpha\log\mathbb{E}_{a\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q(s,a)\right)\right]$ does not depend on $\pi$ and $\alpha\in[0,1]$, the policy $\pi$ that maximizes LHS is the one that minimizes $D_\text{KL}\big(\pi(\cdot\vert s)\Vert\pi_\text{g}(\cdot\vert s)\big)$, which proves our claim due to the fact that KL divergence between two distributions is $\geq 0$ with equality holds when they are identical.</p><h3 id=backup-op-greedy-policy>Backup Operators for Greedy Policy<a hidden class=anchor aria-hidden=true href=#backup-op-greedy-policy>#</a></h3><p>It has been shown that we can also define backup operators for value functions corresponding to greedy policy $\pi_\text{g}$. In particular, we have
\begin{align}
\hspace{-1.2cm}(\mathcal{T}Q_{\pi_\text{g}})(s,a)&=\mathbb{E}_{s&rsquo;\sim p}\Big[r(s,a)+\gamma\Big(\mathbb{E}_{a&rsquo;\sim\pi_\text{g}}\big[Q_{\pi_\text{g}}(s&rsquo;,a&rsquo;)\big]+\alpha H\big(\pi_\text{g}(\cdot\vert s&rsquo;)\big)\Big)\Big] \\ &=\mathbb{E}_{s&rsquo;\sim p}\left[r(s,a)+\gamma\left(\mathbb{E}_{a&rsquo;\sim\pi_\text{g}}\big[Q_{\pi_\text{g}}(s&rsquo;,a&rsquo;)\big]-\alpha\sum_{a&rsquo;}\pi_\text{g}(a&rsquo;\vert s&rsquo;)\log\pi_\text{g}(a&rsquo;\vert s&rsquo;)\right)\right] \\ &=\mathbb{E}_{s&rsquo;\sim p}\left[r(s,a)+\gamma\sum_{a&rsquo;}\pi_\text{g}(a&rsquo;\vert s&rsquo;)\big(Q_{\pi_\text{g}}(s&rsquo;,a&rsquo;)-\alpha\log\pi_\text{g}(a&rsquo;\vert s&rsquo;)\big)\right] \\ &=\mathbb{E}_{s&rsquo;\sim p}\left[r(s,a)+\gamma\sum_{a&rsquo;}\pi_\text{g}(a&rsquo;\vert s&rsquo;)\alpha\log\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{g}}(s&rsquo;,\tilde{a})\right)\right]\right] \\ &=\mathbb{E}_{s&rsquo;\sim p}\Bigg[r(s,a)+\gamma\alpha\log\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{g}}(s&rsquo;,\tilde{a})\right)\right]\Bigg] \\ &=\mathbb{E}_{s&rsquo;\sim p}\Bigg[r(s,a)+\gamma\alpha\log\mathbb{E}_{a&rsquo;\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{g}}(s&rsquo;,a&rsquo;)\right)\right]\Bigg]\label{eq:bogp.1}
\end{align}
where in the fifth step, we use the fact that $\log\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{g}}(s&rsquo;,\tilde{a})\right)\right]$ is independent of $a&rsquo;$, which allows us to do the summation of $\pi_\text{g}$ over the action space $\mathcal{A}$, i.e.
\begin{equation}
\sum_{a&rsquo;}\pi_\text{g}(a&rsquo;\vert s&rsquo;)=1
\end{equation}</p><h2 id=soft-policy-iter>Soft Policy Iteration<a hidden class=anchor aria-hidden=true href=#soft-policy-iter>#</a></h2><p>In standard RL, the Bellman operator provides useful facts to apply the dynamic programming method - <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-iter>policy iteration</a>, which alternates between <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-eval>policy evaluation</a> and <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-imp>policy improvement</a> processes, and eventually we end up with the optimal policy. More importantly, it has been <a href=#sql-apper>proved</a> that we can also apply the method to entropy-regularized RL.</p><h3 id=soft-policy-eval>Soft Policy Evaluation<a hidden class=anchor aria-hidden=true href=#soft-policy-eval>#</a></h3><p>In <strong>policy evaluation</strong> process, we wish to compute the value of a given policy according to the entropy regularization objective \eqref{eq:mr.1}. This can be found by an iterative method.</p><p><strong>Lemma 1</strong> (Soft Policy Evaluation). <em>Consider the Bellman backup operator $\mathcal{T}_\pi$ specified in \eqref{eq:sbo.2} and a mapping $Q^{(0)}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ with $\vert\mathcal{A}\vert\lt\infty$, and define $Q^{(k+1)}=\mathcal{T}_\pi Q^{(k)}$. The resulting sequence $\{Q^{(k)}\}_{k=0,1\ldots,}$ will converge as $k\to\infty$</em>.</p><p><strong>Proof</strong><br>Let $r_\pi(s,a)\doteq r(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p}\big[\alpha H\big(\pi(\cdot\vert s&rsquo;)\big)\big]$ denote the entropy augmented reward, the update rule can be rewritten as
\begin{equation}
Q^{(k+1)}(s,a)=r_\pi(s,a)+\gamma\mathbb{E}_{s&rsquo;\sim p,a&rsquo;\sim\pi}\big[Q^{(k)}(s&rsquo;,a&rsquo;)\big]
\end{equation}
Since $\vert\mathcal{A}\vert&lt;\infty$, we have that $r_\pi(s,a)$ is bounded. Analogy to the <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/>(standard) policy evaluation</a>, we then can prove that $\mathcal{T}_\pi$ is a <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/#contractions>contraction mapping</a> and then by using the <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/#banach-fixed-pts-theorem><strong>Banach&rsquo;s fixed point theorem</strong></a>, we can show that $\{Q^{(k)}\}_{k=0,1,\ldots}$ eventually converges to a fixed point, which we call it the <strong>soft Q-value</strong> of $\pi$.</p><h3 id=soft-policy-imp>Soft Policy Improvement<a hidden class=anchor aria-hidden=true href=#soft-policy-imp>#</a></h3><p>Analogously, the (standard) policy improvement step can be generalized to entropy regularizing as:</p><p><strong>Lemma 2</strong> (Soft Policy Improvement) <em>Let $\Pi$ be some set of policies, $\pi_\text{old}\in\Pi$ be some policy and for each $s_t$ let $\pi_\text{new}$ be defined as
\begin{equation}
\pi_\text{new}(\cdot\vert s_t)=\underset{\pi&rsquo;\in\Pi}{\text{argmin}}D_\text{KL}\left(\pi&rsquo;(\cdot\vert s_t)\Bigg\Vert\frac{\exp\left(\frac{1}{\alpha}Q_{\pi_\text{old}}(s_t,\cdot)\right)}{\mathbb{E}_{a_t\sim\tilde{\pi}_\text{old}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{old}}(s_t,a_t)\right)\right]}\right)
\end{equation}
Then $Q_{\pi_\text{new}}(s_t,a_t)\geq Q_{\pi_\text{old}}(s_t,a_t)$ for all $(s_t,a_t)\in\mathcal{S}\times\mathcal{A}$ with $\vert\mathcal{A}\vert\lt\infty$</em>.</p><p><strong>Proof</strong><br>As KL divergence between two distribution reaches its minimum when those two distributions are identical, we then have that for each $s_t\in\mathcal{S}$
\begin{equation}
\pi_\text{new}(\cdot\vert s_t)=\frac{\exp\left(\frac{1}{\alpha}Q_{\pi_\text{old}}(s_t,\cdot)\right)}{\mathbb{E}_{a_t\sim\tilde{\pi}_\text{old}}\left[\exp\left(\frac{1}{\alpha}Q_{\pi_\text{old}}(s_t,a_t)\right)\right]},
\end{equation}
which is the greedy policy $\pi_\text{g}$, and thus by \eqref{eq:gp.1}, for all $s_t\in\mathcal{S}$, we have
\begin{align}
\mathbb{E}_{a_t\sim\pi_\text{new}}\big[Q_{\pi_\text{old}}(s_t,a_t)\big]+\alpha H\big(\pi_\text{new}(\cdot\vert s_t)\big)&\geq\mathbb{E}_{a_t\sim\pi_\text{old}}\big[Q_{\pi_\text{old}}(s_t,a_t)\big]+\alpha H\big(\pi_\text{old}(\cdot\vert s_t)\big) \\ &=V_{\pi_\text{old}}(s_t)
\end{align}
Therefore, combined with \eqref{eq:svf.2} we obtain
\begin{align}
Q_{\pi_\text{old}}(s_t,a_t)&=r_t+\gamma\mathbb{E}_{s_{t+1}\sim p}\big[V_{\pi_\text{old}}(s_{t+1})\big] \\ &\leq r_t+\gamma\mathbb{E}_{s_{t+1}\sim p}\Big[\mathbb{E}_{a_{t+1}\sim\pi_\text{new}}\big[Q_{\pi_\text{old}}(s_{t+1},a_{t+1})\big]+\alpha H\big(\pi_\text{new}(\cdot\vert s_{t+1})\big)\Big] \\ &\hspace{0.3cm}\vdots \\ &\leq Q_{\pi_\text{new}}(s_t,a_t)
\end{align}</p><p>Now we are ready to specify the policy iteration for entropy-regularized RL.</p><p><strong>Theorem 3</strong> (Soft Policy Iteration) <em>Repeated application of soft policy evaluation and soft policy improvement to any $\pi\in\Pi$ converges to a policy $\pi^*$ such that $Q_{\pi^∗}(s_t,a_t)\geq Q_\pi(s_t,a_t)$ for all $\pi\in\Pi$ and for all $(s_t,a_t)\in\mathcal{S}\times\mathcal{A}$ , assuming $\vert\mathcal{A}\vert\lt\infty$</em>.</p><p><strong>Proof</strong><br>This can be easily proved since the soft Bellman operator $\mathcal{T}_\pi$ defined in \eqref{eq:sbo.2} and backup operator $\mathcal{T}$ given in \eqref{eq:bogp.1} both are contractions.</p><h2 id=sac>Soft Actor-Critic<a hidden class=anchor aria-hidden=true href=#sac>#</a></h2><p>In large scale problems, it is impractical to run either policy evaluation or policy improvement until convergence. It is then necessary to use an approximation version of soft policy iteration, which we call <strong>Soft Actor-Critic</strong>, or <strong>SAC</strong>. SAC instead uses function approximators (neural network function approximation is our choice) for both the policy and the soft Q-function, and rather than alternating between evaluation and improvement to convergence, using SGD to optimize both networks.</p><p>These following are key components of SAC method:</p><ul id=number-list><li>One policy $\pi_\phi$ and two soft Q-functions $Q_{\theta_1},Q_{\theta_2}$.</li><li>Utilizing shared target Q-networks, $Q_{\overline{\theta}_1},Q_{\overline{\theta}_2}$, whose parameters are soft updated due to
\begin{equation}
\bar{\theta}_i\leftarrow\tau\theta_i+(1-\tau)\bar{\theta}_i,\hspace{2cm}i=\{1,2\}
\end{equation}
where $\tau\in(0,1]$ and close to $0$.</li><li id=key-pt-3>Off-policy training with samples from a replay buffer $\mathcal{D}$ to minimize correlations between samples. The rollout phase is given as for each step $t$
\begin{align*}
&a_t\sim\pi_\phi(a_t\vert s_t) \\ &s_{t+1}\sim p(s_{t+1}\vert s_t,a_t) \\ &\mathcal{D}\leftarrow\mathcal{D}\cup\{s_t,a_t,r(s_t,a_t),s_{t+1},d_t\},
\end{align*}
where $d_t$ informs whether $s_{t+1}$ is the terminal state.</li><li>The Q-function parameters are trained to minimize the Mean Square Bellman Error (MSBE)
\begin{equation}
J_Q(\theta)=\mathbb{E}_{(s_t,a_t)\sim\mathcal{D}}\left[\frac{1}{2}\Big(y_t-Q_\theta(s_t,a_t)\Big)^2\right],
\end{equation}
where $y_t$ is the TD target at step $t$, which is given by
\begin{align}
y_t&=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p}\big[V_\overline{\theta}(s_{t+1})\big]\label{eq:sac.6} \\ &=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p,a_{t+1}\sim\pi_\phi}\Big[Q_\overline{\theta}(s_{t+1},a_{t+1})+\alpha H\big(\pi_\phi(\cdot\vert s_{t+1})\big)\Big] \\ &=r(s_t,a_t)+\gamma\mathbb{E}_{s_{t+1}\sim p,a_{t+1}\sim\pi_\phi}\Big[Q_\overline{\theta}(s_{t+1},a_{t+1})-\alpha\log\pi_\phi(a_{t+1}\vert s_{t+1})\Big],
\end{align}
which, as an expectation, can be approximated with samples from replay buffer $\mathcal{D}$ (i.e. $s_{t+1}\sim\mathcal{D}$ since $s_{t+1}\sim p(s_{t+1}\vert s_t,a_t)$ while $(s_t,a_t)\sim\mathcal{D}$, which is the reason why we attached $s_{t+1}$ in the replay buffer $\mathcal{D}$ as in the key point <a href=#key-pt-3>(3)</a>) with current policy $\pi_\phi$ (i.e. $a_{t+1}\sim\pi_\phi$):
\begin{equation}
y_t\approx r(s_t,a_t)+\gamma\big(Q_\overline{\theta}(s_{t+1},a_{t+1})-\alpha\log\pi_\phi(a_{t+1}\vert s_{t+1})\big)
\end{equation}
Therefore, the loss function for Q-networks at step $t$ is given by
\begin{equation}
J_Q(\theta)=\mathbb{E}_{(s_t,a_t,r,s_{t+1},d_t)\sim\mathcal{D},a_{t+1}\sim\pi_\phi}\left[\frac{1}{2}\big(y(r,s_{t+1},a_{t+1},d_t)-Q_\theta(s_t,a_t)\big)^2\right],\label{eq:sac.1}
\end{equation}
where
\begin{equation}
y(r,s_{t+1},a_{t+1},d_t)=r+\gamma(1-d_t)\big(Q_\overline{\theta}(s_{t+1},a_{t+1})-\alpha\log\pi_\phi(a_{t+1}\vert s_{t+1})\big)\label{eq:sac.2}
\end{equation}
The loss function $J_Q(\theta)$ in \eqref{eq:sac.1} then can be optimized according to SGD using
\begin{equation}
\hat{\nabla}_\theta J_Q(\theta)=\nabla_\theta Q_\theta(s_t,a_t)\big(y_t-Q_\theta(s_t,a_t)\big),\label{eq:sac.3}
\end{equation}
where $y_t$ is the TD target at time-step $t$, which can be computed according \eqref{eq:sac.2}.</li><li>The policy, in each state, acts greedily as \eqref{eq:gp.1}, which maximizes the expected entropy-augmented return, which is $V_\pi(s)$
\begin{align}
V_\pi(s)&=\mathbb{E}_{a\sim\pi}\Big[Q_\pi(s,a)+\alpha H\big(\pi(\cdot\vert s)\big)\Big] \\ &=\mathbb{E}_{a\sim\pi}\Big[Q_\pi(s,a)-\alpha\log\pi(a\vert s)\Big]
\end{align}
Hence, the policy parameters $\phi$ can be learned by directly maximizing
\begin{align}
J_\pi(\phi)&=\mathbb{E}_{s_t\sim\mathcal{D}}\Big[\mathbb{E}_{a_t\sim\pi_\phi}\big[Q_\theta(s_t,a_t)-\alpha\log\pi_\phi(a_t\vert s_t)\big]\Big] \\ &=\mathbb{E}_{s_t\sim\mathcal{D},a_t\sim\pi_\phi}\Big[Q_\theta(s_t,a_t)-\alpha\log\pi_\phi(a_t\vert s_t)\Big]\label{eq:sac.4}
\end{align}
Since Q-function is represented by a neural network and can be differentiated, in <a href=#sac-paper>SAC paper</a>, the authors make use of the <b>reparameterization trick</b> to reduce variance. In particular, samples are obtained according to
\begin{equation}
a_\phi(s_t,\epsilon_t)=\text{tanh}(\mu_\phi(s_t)+\sigma_\phi(s_t)\odot\epsilon_t)
\end{equation}
where $\epsilon_t\sim\mathcal{N}(0,I)$ is a spherical Gaussian noise, $\mu_\phi$ and $\sigma_\phi$ are defined as given in the <a href=#action-sample>next key point</a>. The loss function in \eqref{eq:sac.4} then can be rewritten as
\begin{equation}
J_\pi(\phi)=\mathbb{E}_{s_t\sim\mathcal{D},\epsilon_t\sim\mathcal{N}(0,I)}\Big[Q_\theta\big(s_t,a_\phi(s_t,\epsilon_t)\big)-\alpha\log\pi_\phi\big(a_\phi(s_t,\epsilon_t)\vert s_t\big)\Big],
\end{equation}
which rather than taking the expectation over actions ($a_t\sim\pi_\phi$, depends on $\phi$), computing over noise ($\epsilon_t\sim\mathcal{N}(0,I)$, depends on nothing). This function can be optimized with SGD with
\begin{equation}
\hspace{-0.9cm}\hat{\nabla}_\phi J_\pi(\phi)=\big(\nabla_{a_t}Q_\theta(s_t,a_t)-\alpha\nabla_{a_t}\log\pi_\phi(a_t\vert s_t)\big)\nabla_\phi a_\phi(s_t,\epsilon_t)-\alpha\nabla_\phi\log\pi_\phi(a_t\vert s_t),\label{eq:sac.5}
\end{equation}
where $a_t$ is evaluated at $a_\phi(s_t,\epsilon_t)$.</li><li><span id=action-sample></span>For continuous action space tasks, a stochastic policy is usually given in form of a diagonal Gaussian, i.e.
\begin{equation}
\pi_\phi(\cdot\vert s)=\mathcal{N}(\mu_\phi(s),\Sigma_\phi(s))=\mathcal{N}(\mu_\phi(s),\sigma_\theta(s)^2 I)=\mu_\phi(s)+\sigma_\phi(s)\mathcal{N}(0,I)
\end{equation}
hence, when sampling from $\pi_\phi$, let $\epsilon_t\sim\mathcal{N}(0,I)$ be a vector of spherical Gaussian noise, an action $a_t\sim\mathcal{N}(\mu_\theta(s),\sigma_\phi(s)^2 I)$ can be computed as
\begin{equation}
a_t=\mu_\phi(s_t)+\sigma_\phi(s_t)\odot\epsilon_t,
\end{equation}
where $\odot$ denotes the elementwise product of two vectors.<br>Since the normal distribution taking range of $(-\infty,\infty)$, it is necessary to bound the policy to a finite interval, which can be performed by applying a <b>squashing function</b> (e.g. $\text{tanh}$, sigmoid, etc) to the Gaussian samples. For instance, the $\text{tanh}$ function converts support of $(-\infty,\infty)$ into $(-1,1)$.</li><li>Two soft Q-functions $Q_{\theta_1},Q_{\theta_2}$ are trained independently to optimize $J_Q(\theta_1),J_Q(\theta_2)$ respectively. Also, the minimum of the soft Q-functions is used in \eqref{eq:sac.3} and \eqref{eq:sac.5} instead, i.e.
\begin{align}
\hat{\nabla}_\theta J_Q(\theta)&=\nabla_\theta Q_\theta(s_t,a_t)\big(y_t-Q_\theta(s_t,a_t)\big), \\ \hat{\nabla}_\phi J_\pi(\phi)&=\big(\nabla_{a_t}\min_{i=1,2}Q_{\theta_i}(s_t,a_t)-\alpha\nabla_{a_t}\log\pi_\phi(a_t\vert s_t)\big)\nabla_\phi a_\phi(s_t,\epsilon_t)\nonumber \\ &\hspace{2cm}-\alpha\nabla_\phi\log\pi_\phi(a_t\vert s_t)
\end{align}
where
\begin{equation}
y_t=r+\gamma(1-d_t)\left(\min_{i=1,2}Q_{\overline{\theta}_i}(s_{t+1},a_{t+1})-\alpha\log\pi_\phi(a_{t+1}\vert s_{t+1})\right)
\end{equation}</li><li>Instead of considering entropy coefficient $\alpha$ as a constant, in the <a href=#sac-paper-new>latter version</a> of SAC, authors treated it as a parameter and can be optimized due to the loss function
\begin{equation}
J(\alpha)=\mathbb{E}_{a_t\sim\pi_t}\big[-\alpha\log\pi_t(a_t\vert s_t)-\alpha\bar{H}\big]\label{eq:sac.7}
\end{equation}
where $\pi_t$ denotes the current policy at time-step $t$ and $\bar{H}$ is target entropy value, usually is set as $-\text{dim}(\mathcal{A})$.</li></ul><p>Pseudocode for our final algorithm is given below.</p><figure><img src=/images/maxent-sql-sac/sac.png alt=SAC></figure><h3 id=discrete-sac>Discrete SAC<a hidden class=anchor aria-hidden=true href=#discrete-sac>#</a></h3><p>In discrete-action setting, the policy $\pi_\phi(a\vert s)$ can be consider as a PFM (probability mass function), instead of a density function in the continuous case. This gives rise to some necessary changes:</p><ul id=number-list><li>We use a Categorical policy instead of a diagonal Gaussian, i.e. $\pi:\mathcal{S}\to[0,1]^{\vert\mathcal{A}\vert}$.</li><li>The PMF-form policy allows us to compute the soft value function directly instead of using Monte Carlo sampling, i.e.
\begin{equation}
V_\pi(s)=\sum_{a\in\mathcal{A}}\pi(a\vert s)\big[Q_\pi(s,a)-\alpha\log\pi(a\vert s)\big],\label{eq:dsac.1}
\end{equation}
which reduces the variance in Monte Carlo estimate of the $V_\overline{\theta}(s_{t+1})$ given in \eqref{eq:sac.6} that is used to compute the loss function of Q-networks $J_Q(\theta)$.<br>It is then more efficient to make a modification to the soft Q-function: The soft Q-function returns a value for each possible action rather than simply the action provided as an input, i.e. $Q:\mathcal{S}\to\mathbb{R}^{\vert\mathcal{A}\vert}$.<br>This change, along with the previous one, lets us rewrite the calculation for soft state-value given in \eqref{eq:dsac.1} as
\begin{equation}
V_\pi(s)=\pi(s)^\text{T}\big[Q_\pi(s)-\alpha\log\pi(s)\big]
\end{equation}</li><li>Analogously, the entropy coefficient loss changes from \eqref{eq:sac.7} to
\begin{equation}
J(\alpha)=\pi_t(s_t)^\text{T}\big[-\alpha\log\pi_t(s_t)-\alpha\bar{H}\big]
\end{equation}</li><li>Since the policy $\pi$ now returns the exact action, we are able to compute the expectation directly. Thus, it is unnecessary to use the reparameterization trick in the calculation for the loss function $J_\pi(\phi)$. It is then given by
\begin{equation}
J_\pi(\phi)=\mathbb{E}_{s_t\sim\mathcal{D}}\Big[\pi_\phi(s_t)^\text{T}\big[Q_\theta(s_t)-\alpha\log\pi_\phi(s_t)\big]\Big]
\end{equation}</li></ul><h2 id=sql>Soft Q-learning<a hidden class=anchor aria-hidden=true href=#sql>#</a></h2><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=sql-paper>Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine. <a href=https://dl.acm.org/doi/10.5555/3305381.3305521>Reinforcement Learning with Deep Energy-Based Policies</a>. ICML, 2017</span>.</p><p>[2] <span id=sac-paper>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. <a href=https://arxiv.org/abs/1801.01290>Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</a>. arXiv preprint, arXiv:1812.05905, 2018.</span></p><p>[3] <span id=sac-paper-new>Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine. <a href=https://arxiv.org/abs/1812.05905>Soft Actor-Critic Algorithms and Applications</a>. arXiv preprint, arXiv:1812.05905, 2019.</span></p><p>[4] Brian D. Ziebart. <a href=https://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf>Modeling purposeful adaptive behavior with the principle of maximum causal entropy</a>. PhD Thesis, Carnegie Mellon University, 2010.</p><p>[5] John Schulman, Xi Chen, Pieter Abbeel. <a href=https://arxiv.org/abs/1704.06440>Equivalence Between Policy Gradients and Soft Q-Learning</a>. arXiv preprint arXiv:1704.06440, 2018.</p><p>[6] Csaba Szepesvári. <a href=https://doi.org/10.1007/978-3-031-01551-9>Algorithms for Reinforcement Learning</a>. Synthesis Lectures on Artificial Intelligence and Machine Learning, 2010.</p><p>[7] Richard S. Sutton, Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[8] Josh Achiam. <a href=https://spinningup.openai.com/>Spinning Up in Deep Reinforcement Learning</a>. SpinningUp2018, 2018.</p><p>[9] Petros Christodoulou. <a href=https://arxiv.org/abs/1910.07207>Soft Actor-Critic for Discrete Action Settings</a>. arXiv preprint, arXiv:1910.07207.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>With an abuse of notation, $\mathcal{T}_\pi$ implicitly represents two mappings $\mathcal{T}_\pi:\mathcal{S}\to\mathcal{S}$ and $\mathcal{T}_\pi&rsquo;:\mathcal{S}\times\mathcal{A}\to\mathcal{S}\times\mathcal{A}$.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The formula for greedy policy can also be derived by another way. First off, consider the objective we have
\begin{align*}
J(\pi(\cdot\vert s))&=\mathbb{E}_{a\sim\pi}\Big[Q(s,a)\Big]+\alpha H\big(\pi(\cdot\vert s)\big) \\ &=\sum_{a\sim\pi}\pi(a\vert s)Q(s,a)-\sum_{a\sim\pi}\pi(a\vert s)\log\pi(a\vert s) \\ &=\sum_{a}\pi(a\vert s)\big(Q(s,a)-\alpha\log\pi(a\vert s)\big)
\end{align*}
Thus, the partial derivative of the $J(\pi(\cdot\vert s))$ w.r.t $\pi(\bar{a}\vert s)$ for some $\bar{a}\in\mathcal{A}$ is given as
\begin{align*}
\nabla_{\pi(\bar{a}\vert s)}J(\pi(\cdot\vert s))&=\nabla_{\pi(\bar{a}\vert s)}\sum_a\pi(a\vert s)\Big[Q(s,a)-\alpha\log\pi(a\vert s)\Big] \\ &=Q(s,\bar{a})-\alpha\log\pi(\bar{a}\vert s)-\alpha\pi(\bar{a}\vert s)\cdot\frac{1}{\pi(\bar{a}\vert s)} \\ &=Q(s,\bar{a})-\alpha\log\pi(\bar{a}\vert s)-\alpha
\end{align*}
Setting the derivative to zero yields
\begin{equation*}
\pi(\bar{a}\vert s)=\frac{\exp\left(\frac{1}{\alpha}Q(s,\bar{a})\right)}{\exp\alpha}
\end{equation*}&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/deep-reinforcement-learning/>deep-reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/policy-gradient/>policy-gradient</a></li><li><a href=https://trunghng.github.io/tags/actor-critic/>actor-critic</a></li><li><a href=https://trunghng.github.io/tags/q-learning/>q-learning</a></li><li><a href=https://trunghng.github.io/tags/model-free/>model-free</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/machine-learning/cat-reparam-gumbel-softmax-concrete-dist/><span class=title>« Prev</span><br><span>Categorical Reparameterization with Gumbel-Softmax & Concrete Distribution</span>
</a><a class=next href=https://trunghng.github.io/posts/machine-learning/pgm-representation/><span class=title>Next »</span><br><span>Read-through: Probabilistic Graphical Models - Representation</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on x" href="https://x.com/intent/tweet/?text=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f&amp;hashtags=reinforcement-learning%2cdeep-reinforcement-learning%2cpolicy-gradient%2cactor-critic%2cq-learning%2cmodel-free%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f&amp;title=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic&amp;summary=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f&title=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on whatsapp" href="https://api.whatsapp.com/send?text=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on telegram" href="https://telegram.me/share/url?text=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic on ycombinator" href="https://news.ycombinator.com/submitlink?t=Maximum%20Entropy%20Reinforcement%20Learning%20via%20Soft%20Q-learning%20%26%20Soft%20Actor-Critic&u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmaxent-sql-sac%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>