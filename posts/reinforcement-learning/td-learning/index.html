<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Temporal-Difference Learning | Trung's Place</title><meta name=keywords content="reinforcement-learning,td-learning,importance-sampling,q-learning,my-rl"><meta name=description content="
So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a{text-decoration:none}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list{counter-reset:section}#roman-list,#number-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-.75em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Temporal-Difference Learning"><meta property="og:description" content="
So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/td-learning/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-01-31T16:55:00+07:00"><meta property="article:modified_time" content="2022-01-31T16:55:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Temporal-Difference Learning"><meta name=twitter:description content="
So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Temporal-Difference Learning","item":"https://trunghng.github.io/posts/reinforcement-learning/td-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Temporal-Difference Learning","name":"Temporal-Difference Learning","description":" So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.\n","keywords":["reinforcement-learning","td-learning","importance-sampling","q-learning","my-rl"],"articleBody":" So far in this series, we have gone through the ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-difference (TD) learning is our answer.\nTD(0) As usual, we approach this new method by considering the prediction problem.\nTD Prediction Borrowing the idea of Monte Carlo, TD methods learn from episodes of experience to solve the prediction problem. The simplest TD method is TD(0) (or one-step TD)1, which has the update form: \\begin{equation} V(S_t)\\leftarrow V(S_t)+\\alpha\\left[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\right]\\label{eq:tp.1}, \\end{equation} where $\\alpha\\gt 0$ is step size of the update. Here is pseudocode of the TD(0) method\nRecall that in Monte Carlo method, or even in its trivial form, constant-$\\alpha$ MC, which has the update form: \\begin{equation} V(S_t)\\leftarrow V(S_t)+\\alpha\\left[G_t-V(S_t)\\right]\\label{eq:tp.2}, \\end{equation} we have to wait until the end of the episode, when the return $G_t$ is determined. However, with TD(0), we can do the update immediately in the next time step $t+1$.\nAs we can see in \\eqref{eq:tp.1} and \\eqref{eq:tp.2}, both TD and MC updates look ahead to a sample successor state (or state-action pair), use the value of the successor and the corresponding reward in order to update the value of the current state (or state-action pair). This kind of updates is called sample update, which differs from expected update used by DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.\nOther than the sampling of Monte Carlo, TD methods also use the bootstrapping of DP. Because similar to DP, TD(0) is also a bootstrapping method, since the target in its update is $R_{t+1}+\\gamma V(S_{t+1})$.\nThe quantity inside bracket in \\eqref{eq:tp.1} is called TD error, denoted as $\\delta$: \\begin{equation} \\delta_t\\doteq R_{t+1}+\\gamma V(S_{t+1})-V(S_t) \\end{equation} If the array $V$ does not change during the episode (as in MC), then the MC error can be written as a sum of TD errors \\begin{align} G_t-V(S_t)\u0026=R_{t+1}+\\gamma G_{t+1}-V(S_t)+\\gamma V(S_{t+1})-\\gamma V(S_{t+1}) \\\\ \u0026=\\delta_t+\\gamma\\left(G_{t+1}-V(S_{t+1})\\right) \\\\ \u0026=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\left(G_{t+2}-V(S_{t+2})\\right) \\\\ \u0026=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}\\left(G_T-V(S_T)\\right) \\\\ \u0026=\\delta_t+\\gamma\\delta_{t+1}+\\gamma^2\\delta_{t+2}+\\dots+\\gamma^{T-t-1}\\delta_{T-1}+\\gamma^{T-t}(0-0) \\\\ \u0026=\\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k \\end{align}\nAdvantages over MC \u0026 DP With how TD is established, these are some advantages of its over MC and DP:\nOnly experience is required. Can be fully incremental: Can make update before knowing the final outcome. Requires less memory. Requires less peak computation. TD(0) does converge to $v_\\pi$, in the mean for a sufficient small $\\alpha$, and with probability of $1$ if $\\alpha$ decreases according to the stochastic approximation condition \\begin{equation} \\sum_{n=1}^{\\infty}\\alpha_n(a)=\\infty\\hspace{1cm}\\text{and}\\hspace{1cm}\\sum_{n=1}^{\\infty}\\alpha_n^2(a)\u003c\\infty, \\end{equation} where $\\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$-th selection of action $a$.\nOptimality of TD(0) Under batch training, TD(0) converges to the optimal maximum likelihood estimate. The convergence and optimality proofs can be found in this paper.\nFigure 1: Performance of TD(0) and constant-$\\alpha$ MC under batch training on the random walk task. The code can be found here TD Control We begin solving the control problem with an on-policy TD method. Recall that in on-policy methods, we evaluate or improve the policy $\\pi$ used to make decision.\nSarsa As mentioned in MC methods, when the model is not available, we have to learn an action-value function rather than a state-value function. Or in other words, we need to estimate $q_\\pi(s,a)$ for the current policy $\\pi$ and $\\forall s,a$. Thus, instead of considering transitions from state to state in order to learn the value of states, we now take transitions from state-action pair to state-action pair into account so as to learn the value of state-action pairs.\nSimilarly, we’ve got the TD update for the action-value function case: \\begin{equation} Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\right] \\end{equation} This update is done after every transition from a non-terminal state $S_t$ to its successor $S_{t+1}$ \\begin{equation} \\left(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\\right) \\end{equation} And if $S_{t+1}$ is terminal (i.e., $S_{t+1}=S_T$), then $Q(S_{t+1},A_{t+1})=0$. The name Sarsa of the method is taken based on acronym of the quintuple.\nAs usual when working on on-policy control problem, we apply the idea of GPI: \\begin{equation} \\pi_0\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_0}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_1\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_1}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_2\\overset{\\small \\text{E}}{\\rightarrow}\\dots\\overset{\\small \\text{I}}{\\rightarrow}\\pi_*\\overset{\\small \\text{E}}{\\rightarrow}q_* \\end{equation} However this time, instead, we use it with TD methods. Which is, we continually estimate $q_\\pi$ for the behavior policy $\\pi$, and at the same time change $\\pi$ toward greediness w.r.t $q_\\pi$. That gives us the following pseudocode of the Sarsa control algorithm\nQ-learning We now turn our move to an off-policy method, which evaluates or improves a policy different from that used to generate the data.\nThe method we are talking about is called Q-learning, which was first introduced by Watkins, in which the update on $Q$-value has the form: \\begin{equation} Q(S_t,A_t)\\leftarrow Q(S_t,A_t)+\\alpha\\left[R_{t+1}+\\gamma\\max_a Q(S_{t+1},a)-Q(S_t,A_t)\\right]\\label{eq:ql.1} \\end{equation} In this case, the learned action-value function, $Q$, directly approximates optimal action-value function $q_*$, independent of the policy being followed. Down below is pseudocode of the Q-learning.\nExample: Cliffwalking - Sarsa vs Q-learning (This example is taken from Example 6.6, Reinforcement Learning: An Introduction book.)\nSay that we have an agent in a gridworld, which is an undiscounted, episodic task described by the above image. Start and goal states are denoted as \"S\" and \"G\" respectively. Agent can take up, down, left or right action. All the actions lead to a reward of $-1$, except for cliff region, into which stepping gives a reward of $-100$. We will be solving this problem with Q-learning and Sarsa with $\\varepsilon$-greedy action selection, for $\\varepsilon=0.1$. Solution\nThe source code can be found here.\nWe begin by importing necessary packages we will be using\nimport numpy as np import matplotlib.pyplot as plt from tqdm import tqdm Our first step is to define the environment, gridworld with a cliff, which is constructed by height, width, cliff region, start state, goal state, actions and rewards.\nclass GridWorld: def __init__(self, height, width, start_state, goal_state, cliff): ''' Initialization function Params ------ height: int gridworld's height width: int gridworld's width start_state: [int, int] gridworld's start state goal_state: [int, int] gridworld's goal state cliff: list\u003c[int, int]\u003e gridworld's cliff region ''' self.height = height self.width = width self.start_state = start_state self.goal_state = goal_state self.cliff = cliff self.actions = [(-1, 0), (1, 0), (0, 1), (0, -1)] self.rewards = {'cliff': -100, 'non-cliff': -1} The gridworld also needs some helper functions. is_terminal() function checks whether the current state is the goal state; take_action() takes an state and action as inputs and returns next state and corresponding reward while get_action_idx() gives us the index of action from action list. Putting all these functions inside GridWorld’s body, we have:\ndef is_terminal(self, state): ''' Whether state @state is the goal state Params ------ state: [int, int] current state ''' return state == self.goal_state def take_action(self, state, action): ''' Take action @action at state @state Params ------ state: [int, int] current state action: (int, int) action taken Return ------ (next_state, reward): ([int, int], int) a tuple of next state and reward ''' next_state = [state[0] + action[0], state[1] + action[1]] next_state = [max(0, next_state[0]), max(0, next_state[1])] next_state = [min(self.height - 1, next_state[0]), min(self.width - 1, next_state[1])] if next_state in self.cliff: reward = self.rewards['cliff'] next_state = self.start_state else: reward = self.rewards['non-cliff'] return next_state, reward def get_action_idx(self, action): ''' Get index of action in action list Params ------ action: (int, int) action ''' return self.actions.index(action) Next, we define the $\\varepsilon$-greedy function used by our methods in epsilon_greedy() function.\ndef epsilon_greedy(grid_world, epsilon, Q, state): ''' Choose action according to epsilon-greedy policy Params: ------- grid_world: GridWorld epsilon: float Q: np.ndarray action-value function state: [int, int] current state Return ------ action: (int, int) ''' if np.random.binomial(1, epsilon): action_idx = np.random.randint(len(grid_world.actions)) action = grid_world.actions[action_idx] else: values = Q[state[0], state[1], :] action_idx = np.random.choice([action_ for action_, value_ in enumerate(values) if value_ == np.max(values)]) action = grid_world.actions[action_idx] return action It’s time for our main course, Q-learning and Sarsa.\ndef q_learning(Q, grid_world, epsilon, alpha, gamma): ''' Q-learning Params ------ Q: np.ndarray action-value function grid_world: GridWorld epsilon: float alpha: float step size gamma: float discount factor ''' state = grid_world.start_state rewards = 0 while not grid_world.is_terminal(state): action = epsilon_greedy(grid_world, epsilon, Q, state) next_state, reward = grid_world.take_action(state, action) rewards += reward action_idx = grid_world.get_action_idx(action) Q[state[0], state[1], action_idx] += alpha * (reward + gamma * \\ np.max(Q[next_state[0], next_state[1], :]) - Q[state[0], state[1], action_idx]) state = next_state return rewards def sarsa(Q, grid_world, epsilon, alpha, gamma): ''' Sarsa Params ------ Q: np.ndarray action-value function grid_world: GridWorld epsilon: float alpha: float step size gamma: float discount factor ''' state = grid_world.start_state action = epsilon_greedy(grid_world, epsilon, Q, state) rewards = 0 while not grid_world.is_terminal(state): next_state, reward = grid_world.take_action(state, action) rewards += reward next_action = epsilon_greedy(grid_world, epsilon, Q, next_state) action_idx = grid_world.get_action_idx(action) next_action_idx = grid_world.get_action_idx(next_action) Q[state[0], state[1], action_idx] += alpha * (reward + gamma * Q[next_state[0], \\ next_state[1], next_action_idx] - Q[state[0], state[1], action_idx]) state = next_state action = next_action return rewards And lastly, wrapping everything together in the main function, we have\nif __name__ == '__main__': height = 4 width = 13 start_state = [3, 0] goal_state = [3, 12] cliff = [[3, x] for x in range(1, 12)] grid_world = GridWorld(height, width, start_state, goal_state, cliff) n_runs = 50 n_eps = 500 epsilon = 0.1 alpha = 0.5 gamma = 1 Q = np.zeros((height, width, len(grid_world.actions))) rewards_q_learning = np.zeros(n_eps) rewards_sarsa = np.zeros(n_eps) for _ in tqdm(range(n_runs)): Q_q_learning = Q.copy() Q_sarsa = Q.copy() for ep in range(n_eps): rewards_q_learning[ep] += q_learning(Q_q_learning, grid_world, epsilon, alpha, gamma) rewards_sarsa[ep] += sarsa(Q_sarsa, grid_world, epsilon, alpha, gamma) rewards_q_learning /= n_runs rewards_sarsa /= n_runs plt.plot(rewards_q_learning, label='Q-Learning') plt.plot(rewards_sarsa, label='Sarsa') plt.xlabel('Episodes') plt.ylabel('Sum of rewards during episode') plt.ylim([-100, 0]) plt.legend() plt.savefig('./cliff_walking.png') plt.close() This is our result after completing running the code.\nExpected Sarsa In the update \\eqref{eq:ql.1} of Q-learning, rather than taking the maximum over next state-action pairs, we use the expected value to consider how likely each action is under the current policy. That means, we instead have the following update rule for $Q$-value: \\begin{align} Q(S_t,A_t)\u0026\\leftarrow Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma\\mathbb{E}_\\pi\\big[Q(S_{t+1},A_{t+1}\\vert S_{t+1})\\big]-Q(S_t,A_t)\\Big] \\\\ \u0026\\leftarrow Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma\\sum_a\\pi(a|S_{t+1})Q(S_{t+1}|a)-Q(S_t,A_t)\\Big] \\end{align} However, given the next state, $S_{t+1}$, this algorithms move deterministically in the same direction as Sarsa moves in expectation. Thus, this method is also called Expected Sarsa.\nExpected Sarsa performs better than Sarsa since it eliminates the variance due to the randomization in selecting $A_{t+1}$. Which also means that it takes expected Sarsa more resource than Sarsa.\nDouble Q-learning Maximization Bias Consider a set of $M$ random variables $X=\\{X_1,\\dots,X_M\\}$. Say that we are interested in maximizing expected value of the r.v.s in $X$: \\begin{equation} \\max_{i=1,\\dots,M}\\mathbb{E}(X_i)\\label{eq:mb.1} \\end{equation} This value can be approximated by constructing approximations for $\\mathbb{E}(X_i)$ for all $i$. Let \\begin{equation} S=\\bigcup_{i=1}^{M}S_i \\end{equation} denote a set of samples, where $S_i$ is the subset containing samples for the variables $X_i$, and assume that the samples in $S_i$ are i.i.d. Unbiased estimates for the expected values can be obtained by computing the sample average for each variable: \\begin{equation} \\mathbb{E}(X_i)=\\mathbb{E}(\\mu_i)\\approx\\mu_i(S)\\doteq\\frac{1}{\\vert S_i\\vert}\\sum_{s\\in S_i}s, \\end{equation} where $\\mu_i$ is an estimator for variable $X_i$. This approximation is unbiased since every sample $s\\in S_i$ is an unbiased estimate for the value of $\\mathbb{E}(X_i)$. Thus, \\eqref{eq:mb.1} can be approximated by: \\begin{equation} \\max_{i=1,\\dots,M}\\mathbb{E}(X_i)=\\max_{i=1,\\dots,M}\\mathbb{E}(\\mu_i)\\approx\\max_{i=1,\\dots,M}\\mu_i(S) \\end{equation} Let $f_i$, $F_i$ denote the PDF and CDF of $X_i$ and $f_i^\\mu, F_i^\\mu$ denote the PDF and CDF of $\\mu_i$ respectively. Hence we have that \\begin{align} \\mathbb{E}(X_i)\u0026=\\int_{-\\infty}^{\\infty}x f_i(x)\\hspace{0.1cm}dx;\\hspace{0.5cm}F_i(x)=P(X_i\\leq x)=\\int_{-\\infty}^{\\infty}f_i(x)\\hspace{0.1cm}dx \\\\ \\mathbb{E}(\\mu_i)\u0026=\\int_{-\\infty}^{\\infty}x f_i^\\mu(x)\\hspace{0.1cm}dx;\\hspace{0.5cm}F_i^\\mu(x)=P(\\mu_i\\leq x)=\\int_{-\\infty}^{\\infty}f_i^\\mu(x)\\hspace{0.1cm}dx \\end{align} With these notations, considering the maximal estimator $\\mu_i$, which is distributed by some PDF $f_{\\max}^{\\mu}$, we have: \\begin{align} F_{\\max}^{\\mu}\u0026\\doteq P(\\max_i \\mu_i\\leq x) \\\\ \u0026=P(\\mu_1\\leq x;\\dots;\\mu_M\\leq x) \\\\ \u0026=\\prod_{i=1}^{M}P(\\mu_i\\leq x) \\\\ \u0026=\\prod_{i=1}^{M}F_i^\\mu(x) \\end{align} The value $\\max_i\\mu_i(S)$ is an unbiased estimate of $\\mathbb{E}(\\max_i\\mu_i)$, which is given by \\begin{align} \\mathbb{E}\\left(\\max_i\\mu_i\\right) \u0026=\\int_{-\\infty}^{\\infty}x f_{\\max}^{\\mu}(x)\\hspace{0.1cm}dx \\\\ \u0026=\\int_{-\\infty}^{\\infty}x\\frac{d}{dx}\\left(\\prod_{i=1}^{M}F_i^\\mu(x)\\right)\\hspace{0.1cm}dx \\\\ \u0026=\\sum_{i=1}^M\\int_{-\\infty}^{\\infty}f_i^\\mu(x)\\prod_{j\\neq i}^{M}F_i^\\mu(x)\\hspace{0.1cm}dx \\end{align} However, as can be seen in \\eqref{eq:mb.1}, the order of expectation and maximization is the other way around. This leads to the result that $\\max_i\\mu_i(S)$ is a biased estimate of $\\max_i\\mathbb{E}(X_i)$\nA Solution The reason why maximization bias happens is we are using the same samples to decide which action is the best (highest reward one) and also to estimate its action-value. To overcome this situation, Hasselt (2010) proposed an alternative method that uses two set of estimators instead, $\\mu^A=\\{\\mu_1^A,\\dots,\\mu_M^A\\}$ and $\\mu^B=\\{\\mu_1^B,\\dots,\\mu_M^B\\}$. The method is thus also called double estimators.\nSpecifically, we use these two sets to learn two independent estimates, called $Q^A$ and $Q^B$, each is an estimate of the true value $q(a)$, for all $a\\in\\mathcal{A}$.\n$\\boldsymbol{n}$-step TD From the definition of one-step TD, we can formalize the idea into a more general, n-step TD. Once again, first off, we will be considering the prediction problem.\n$\\boldsymbol{n}$-step TD Prediction Recall that in one-step TD, the update is based on the next reward, bootstrapping2 from the value of the state at one step later. In particular, the target of the update is $R_{t+1}+\\gamma V_t(S_{t+1})$, which we are going to denote as $G_{t:t+1}$, or one-step return: \\begin{equation} G_{t:t+1}\\doteq R_{t+1}+\\gamma V_t(S_{t+1}) \\end{equation} where $V_t:\\mathcal{S}\\to\\mathbb{R}$ is the estimate at time step $t$ of $v_\\pi$. Thus, rather than taking into account one step later, in two-step TD, it makes sense to consider the rewards in two steps further, combined with the value function of the state at two step later. In other words, the target of two-step update is the two-step return: \\begin{equation} G_{t:t+2}\\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 V_{t+1}(S_{t+2}) \\end{equation} Similarly, the target of $n$-step update is the $\\boldsymbol{n}$-step return: \\begin{equation} G_{t:t+n}\\doteq R_{t+1}+\\gamma R_{t+2}+\\dots+\\gamma^{n-1}R_{t+n}+\\gamma^n V_{t+n-1}(S_{t+n}) \\end{equation} for all $n,t$ such that $n\\geq 1$ and $0\\leq t","wordCount":"4444","inLanguage":"en","datePublished":"2022-01-31T16:55:00+07:00","dateModified":"2022-01-31T16:55:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/td-learning/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Temporal-Difference Learning</h1><div class=post-meta><span title='2022-01-31 16:55:00 +0700 +0700'>January 31, 2022</span>&nbsp;·&nbsp;21 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#td0>TD(0)</a><ul><li><a href=#td-prediction>TD Prediction</a><ul><li><a href=#adv-over-mc-dp>Advantages over MC & DP</a></li><li><a href=#opt-td0>Optimality of TD(0)</a></li></ul></li><li><a href=#td-control>TD Control</a><ul><li><a href=#sarsa>Sarsa</a></li><li><a href=#q-learning>Q-learning</a><ul><li><a href=#eg-cliffwalking>Example: Cliffwalking - Sarsa vs Q-learning</a></li></ul></li><li><a href=#exp-sarsa>Expected Sarsa</a></li><li><a href=#double-q-learning>Double Q-learning</a><ul><li><a href=#max-bias>Maximization Bias</a></li><li><a href=#sol>A Solution</a></li></ul></li></ul></li></ul></li><li><a href=#n-step-td>$\boldsymbol{n}$-step TD</a><ul><li><a href=#n-step-td-prediction>$\boldsymbol{n}$-step TD Prediction</a><ul><li><a href=#eg-random-walk>Example: Random Walk</a></li></ul></li><li><a href=#n-step-td-control>$\boldsymbol{n}$-step TD Control</a><ul><li><a href=#n-step-sarsa>$\boldsymbol{n}$-step Sarsa</a></li></ul></li><li><a href=#off-policy-n-step-td>Off-policy $\boldsymbol{n}$-step TD</a><ul><li><a href=#n-step-td-is>$\boldsymbol{n}$-step TD with Importance Sampling</a></li><li><a href=#per-decision-control-variates>Per-decision Methods with Control Variates</a></li><li><a href=#n-step-tree-backup>$\boldsymbol{n}$-step Tree Backup</a></li><li><a href=#n-step-q-sigma>$\boldsymbol{n}$-step $Q(\sigma)$</a></li></ul></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>So far in this <a href=https://trunghng.github.io/tags/my-rl/>series</a>, we have gone through the ideas of <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/><strong>dynamic programming</strong> (DP)</a> and <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/><strong>Monte Carlo</strong></a>. What will happen if we combine these ideas together? <strong>Temporal-difference (TD) learning</strong> is our answer.</p></blockquote><h2 id=td0>TD(0)<a hidden class=anchor aria-hidden=true href=#td0>#</a></h2><p>As usual, we approach this new method by considering the prediction problem.</p><h3 id=td-prediction>TD Prediction<a hidden class=anchor aria-hidden=true href=#td-prediction>#</a></h3><p>Borrowing the idea of Monte Carlo, TD methods learn from episodes of experience to solve the <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#fn:2>prediction problem</a>. The simplest TD method is <strong>TD(0)</strong> (or <strong>one-step TD</strong>)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right]\label{eq:tp.1},
\end{equation}
where $\alpha\gt 0$ is step size of the update. Here is pseudocode of the TD(0) method</p><figure><img src=/images/td-learning/td0.png alt=TD(0) style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>Recall that in <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#mc-prediction>Monte Carlo method</a>, or even in its trivial form, <strong>constant-$\alpha$ MC</strong>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[G_t-V(S_t)\right]\label{eq:tp.2},
\end{equation}
we have to wait until the end of the episode, when the return $G_t$ is determined. However, with TD(0), we can do the update immediately in the next time step $t+1$.</p><p>As we can see in \eqref{eq:tp.1} and \eqref{eq:tp.2}, both TD and MC updates look ahead to a sample successor state (or state-action pair), use the value of the successor and the corresponding reward in order to update the value of the current state (or state-action pair). This kind of updates is called <strong>sample update</strong>, which differs from <strong>expected update</strong> used by DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.</p><p>Other than the sampling of Monte Carlo, TD methods also use the bootstrapping of DP. Because similar to <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-evaluation>DP</a>, TD(0) is also a bootstrapping method, since the target in its update is $R_{t+1}+\gamma V(S_{t+1})$.</p><p>The quantity inside bracket in \eqref{eq:tp.1} is called <span id=td_error><strong>TD error</strong></span>, denoted as $\delta$:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\end{equation}
If the array $V$ does not change during the episode (as in MC), then the MC error can be written as a sum of TD errors
\begin{align}
G_t-V(S_t)&=R_{t+1}+\gamma G_{t+1}-V(S_t)+\gamma V(S_{t+1})-\gamma V(S_{t+1}) \\ &=\delta_t+\gamma\left(G_{t+1}-V(S_{t+1})\right) \\ &=\delta_t+\gamma\delta_{t+1}+\gamma^2\left(G_{t+2}-V(S_{t+2})\right) \\ &=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}\left(G_T-V(S_T)\right) \\ &=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0) \\ &=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k
\end{align}</p><h4 id=adv-over-mc-dp>Advantages over MC & DP<a hidden class=anchor aria-hidden=true href=#adv-over-mc-dp>#</a></h4><p>With how TD is established, these are some advantages of its over MC and DP:</p><ul><li>Only experience is required.</li><li>Can be fully incremental:<ul><li>Can make update before knowing the final outcome.</li><li>Requires less memory.</li><li>Requires less peak computation.</li></ul></li></ul><p>TD(0) does converge to $v_\pi$, in the mean for a sufficient small $\alpha$, and with probability of $1$ if $\alpha$ decreases according to the <span id=stochastic-approx-condition><strong>stochastic approximation condition</strong></span>
\begin{equation}
\sum_{n=1}^{\infty}\alpha_n(a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty,
\end{equation}
where $\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$-th selection of action $a$.</p><h4 id=opt-td0>Optimality of TD(0)<a hidden class=anchor aria-hidden=true href=#opt-td0>#</a></h4><p>Under batch training, TD(0) converges to the optimal maximum likelihood estimate. The convergence and optimality proofs can be found in this <a href=#td-convergence>paper</a>.</p><figure><img src=/images/td-learning/random_walk_batch_updating.png alt="TD(0) vs constant-alpha MC" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: Performance of TD(0) and constant-$\alpha$ MC under batch training on the random walk task. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-06/random-walk.py target=_blank>here</a></figcaption></figure><h3 id=td-control>TD Control<a hidden class=anchor aria-hidden=true href=#td-control>#</a></h3><p>We begin solving the control problem with an on-policy TD method. Recall that in on-policy methods, we evaluate or improve the policy $\pi$ used to make decision.</p><h4 id=sarsa>Sarsa<a hidden class=anchor aria-hidden=true href=#sarsa>#</a></h4><p>As mentioned in <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#mc-est-action-value>MC methods</a>, when the model is not available, we have to learn an action-value function rather than a state-value function. Or in other words, we need to estimate $q_\pi(s,a)$ for the current policy $\pi$ and $\forall s,a$. Thus, instead of considering transitions from state to state in order to learn the value of states, we now take transitions from state-action pair to state-action pair into account so as to learn the value of state-action pairs.</p><p>Similarly, we&rsquo;ve got the TD update for the action-value function case:
\begin{equation}
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\right]
\end{equation}
This update is done after every transition from a non-terminal state $S_t$ to its successor $S_{t+1}$
\begin{equation}
\left(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\right)
\end{equation}
And if $S_{t+1}$ is terminal (i.e., $S_{t+1}=S_T$), then $Q(S_{t+1},A_{t+1})=0$. The name <strong>Sarsa</strong> of the method is taken based on acronym of the quintuple.</p><p>As usual when working on on-policy control problem, we apply the idea of <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#gpi>GPI</a>:
\begin{equation}
\pi_0\overset{\small \text{E}}{\rightarrow}q_{\pi_0}\overset{\small \text{I}}{\rightarrow}\pi_1\overset{\small \text{E}}{\rightarrow}q_{\pi_1}\overset{\small \text{I}}{\rightarrow}\pi_2\overset{\small \text{E}}{\rightarrow}\dots\overset{\small \text{I}}{\rightarrow}\pi_*\overset{\small \text{E}}{\rightarrow}q_*
\end{equation}
However this time, instead, we use it with TD methods. Which is, we continually estimate $q_\pi$ for the behavior policy $\pi$, and at the same time change $\pi$ toward greediness w.r.t $q_\pi$. That gives us the following pseudocode of the Sarsa control algorithm</p><figure><img src=/images/td-learning/sarsa.png alt=Sarsa style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=q-learning>Q-learning<a hidden class=anchor aria-hidden=true href=#q-learning>#</a></h4><p>We now turn our move to an off-policy method, which evaluates or improves a policy different from that used to generate the data.<br>The method we are talking about is called <strong>Q-learning</strong>, which was first introduced by <a href=#q-learning-watkins>Watkins</a>, in which the update on $Q$-value has the form:
\begin{equation}
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\max_a Q(S_{t+1},a)-Q(S_t,A_t)\right]\label{eq:ql.1}
\end{equation}
In this case, the learned action-value function, $Q$, directly approximates optimal action-value function $q_*$, independent of the policy being followed. Down below is pseudocode of the Q-learning.</p><figure><img src=/images/td-learning/q-learning.png alt=Q-learning style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h5 id=eg-cliffwalking>Example: Cliffwalking - Sarsa vs Q-learning<a hidden class=anchor aria-hidden=true href=#eg-cliffwalking>#</a></h5><p>(This example is taken from <strong>Example 6.6</strong>, <a href=#rl-book><strong>Reinforcement Learning: An Introduction</strong></a> book.)</p><figure><img src=/images/td-learning/cliff-walking-eg.png alt="Cliff Walking example" style=display:block;margin-left:auto;margin-right:auto;width:500px><figcaption style=text-align:center;font-style:italic></figcaption></figure>Say that we have an agent in a gridworld, which is an undiscounted, episodic task described by the above image. Start and goal states are denoted as "S" and "G" respectively. Agent can take up, down, left or right action. All the actions lead to a reward of $-1$, except for cliff region, into which stepping gives a reward of $-100$. We will be solving this problem with Q-learning and Sarsa with $\varepsilon$-greedy action selection, for $\varepsilon=0.1$.<p><strong>Solution</strong><br>The source code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-06/cliff_walking.py>here</a>.</p><p>We begin by importing necessary packages we will be using</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm</span> <span class=kn>import</span> <span class=n>tqdm</span>
</span></span></code></pre></div><p>Our first step is to define the environment, gridworld with a cliff, which is constructed by height, width, cliff region, start state, goal state, actions and rewards.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>GridWorld</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>start_state</span><span class=p>,</span> <span class=n>goal_state</span><span class=p>,</span> <span class=n>cliff</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Initialization function
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        height: int
</span></span></span><span class=line><span class=cl><span class=s1>            gridworld&#39;s height
</span></span></span><span class=line><span class=cl><span class=s1>        width: int
</span></span></span><span class=line><span class=cl><span class=s1>            gridworld&#39;s width
</span></span></span><span class=line><span class=cl><span class=s1>        start_state: [int, int]
</span></span></span><span class=line><span class=cl><span class=s1>            gridworld&#39;s start state
</span></span></span><span class=line><span class=cl><span class=s1>        goal_state: [int, int]
</span></span></span><span class=line><span class=cl><span class=s1>            gridworld&#39;s goal state
</span></span></span><span class=line><span class=cl><span class=s1>        cliff: list&lt;[int, int]&gt;
</span></span></span><span class=line><span class=cl><span class=s1>            gridworld&#39;s cliff region
</span></span></span><span class=line><span class=cl><span class=s1>    	&#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>height</span> <span class=o>=</span> <span class=n>height</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>width</span> <span class=o>=</span> <span class=n>width</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>start_state</span> <span class=o>=</span> <span class=n>start_state</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>goal_state</span> <span class=o>=</span> <span class=n>goal_state</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>cliff</span> <span class=o>=</span> <span class=n>cliff</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actions</span> <span class=o>=</span> <span class=p>[(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>),</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>),</span> <span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span> <span class=o>=</span> <span class=p>{</span><span class=s1>&#39;cliff&#39;</span><span class=p>:</span> <span class=o>-</span><span class=mi>100</span><span class=p>,</span> <span class=s1>&#39;non-cliff&#39;</span><span class=p>:</span> <span class=o>-</span><span class=mi>1</span><span class=p>}</span>
</span></span></code></pre></div><p>The gridworld also needs some helper functions. <code>is_terminal()</code> function checks whether the current state is the goal state; <code>take_action()</code> takes an state and action as inputs and returns next state and corresponding reward while <code>get_action_idx()</code> gives us the index of action from action list. Putting all these functions inside <code>GridWorld</code>&rsquo;s body, we have:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_terminal</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Whether state @state is the goal state
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        state: [int, int]
</span></span></span><span class=line><span class=cl><span class=s1>            current state
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>state</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>goal_state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>take_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Take action @action at state @state
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        state: [int, int]
</span></span></span><span class=line><span class=cl><span class=s1>            current state
</span></span></span><span class=line><span class=cl><span class=s1>        action: (int, int)
</span></span></span><span class=line><span class=cl><span class=s1>            action taken
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Return
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        (next_state, reward): ([int, int], int)
</span></span></span><span class=line><span class=cl><span class=s1>            a tuple of next state and reward
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span> <span class=o>=</span> <span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>+</span> <span class=n>action</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span> <span class=o>+</span> <span class=n>action</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span> <span class=o>=</span> <span class=p>[</span><span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>max</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>])]</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span> <span class=o>=</span> <span class=p>[</span><span class=nb>min</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>height</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>min</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>width</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>])]</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>next_state</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>cliff</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;cliff&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>next_state</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>start_state</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=s1>&#39;non-cliff&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>get_action_idx</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Get index of action in action list
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        action: (int, int)
</span></span></span><span class=line><span class=cl><span class=s1>            action
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>actions</span><span class=o>.</span><span class=n>index</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span></code></pre></div><p>Next, we define the $\varepsilon$-greedy function used by our methods in <code>epsilon_greedy()</code> function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>epsilon_greedy</span><span class=p>(</span><span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Choose action according to epsilon-greedy policy
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params:
</span></span></span><span class=line><span class=cl><span class=s1>    -------
</span></span></span><span class=line><span class=cl><span class=s1>    grid_world: GridWorld
</span></span></span><span class=line><span class=cl><span class=s1>    epsilon: float
</span></span></span><span class=line><span class=cl><span class=s1>    Q: np.ndarray
</span></span></span><span class=line><span class=cl><span class=s1>        action-value function
</span></span></span><span class=line><span class=cl><span class=s1>    state: [int, int]
</span></span></span><span class=line><span class=cl><span class=s1>        current state
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Return
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    action: (int, int)
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>action_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>randint</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>grid_world</span><span class=o>.</span><span class=n>actions</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>actions</span><span class=p>[</span><span class=n>action_idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>values</span> <span class=o>=</span> <span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>:]</span>
</span></span><span class=line><span class=cl>        <span class=n>action_idx</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>([</span><span class=n>action_</span> <span class=k>for</span> <span class=n>action_</span><span class=p>,</span> <span class=n>value_</span> 
</span></span><span class=line><span class=cl>            <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>values</span><span class=p>)</span> <span class=k>if</span> <span class=n>value_</span> <span class=o>==</span> <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>values</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>actions</span><span class=p>[</span><span class=n>action_idx</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>action</span>
</span></span></code></pre></div><p>It&rsquo;s time for our main course, Q-learning and Sarsa.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>q_learning</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Q-learning
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    Q: np.ndarray
</span></span></span><span class=line><span class=cl><span class=s1>        action-value function
</span></span></span><span class=line><span class=cl><span class=s1>    grid_world: GridWorld
</span></span></span><span class=line><span class=cl><span class=s1>    epsilon: float
</span></span></span><span class=line><span class=cl><span class=s1>    alpha: float
</span></span></span><span class=line><span class=cl><span class=s1>        step size
</span></span></span><span class=line><span class=cl><span class=s1>    gamma: float
</span></span></span><span class=line><span class=cl><span class=s1>        discount factor
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>start_state</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>(</span><span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>epsilon_greedy</span><span class=p>(</span><span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>take_action</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>        <span class=n>action_idx</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>get_action_idx</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action_idx</span><span class=p>]</span> <span class=o>+=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>reward</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> \
</span></span><span class=line><span class=cl>            <span class=n>np</span><span class=o>.</span><span class=n>max</span><span class=p>(</span><span class=n>Q</span><span class=p>[</span><span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=p>:])</span> <span class=o>-</span> <span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action_idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rewards</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>sarsa</span><span class=p>(</span><span class=n>Q</span><span class=p>,</span> <span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Sarsa
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    Q: np.ndarray
</span></span></span><span class=line><span class=cl><span class=s1>        action-value function
</span></span></span><span class=line><span class=cl><span class=s1>    grid_world: GridWorld
</span></span></span><span class=line><span class=cl><span class=s1>    epsilon: float
</span></span></span><span class=line><span class=cl><span class=s1>    alpha: float
</span></span></span><span class=line><span class=cl><span class=s1>        step size
</span></span></span><span class=line><span class=cl><span class=s1>    gamma: float
</span></span></span><span class=line><span class=cl><span class=s1>        discount factor
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>start_state</span>
</span></span><span class=line><span class=cl>    <span class=n>action</span> <span class=o>=</span> <span class=n>epsilon_greedy</span><span class=p>(</span><span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=ow>not</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>(</span><span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>take_action</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>+=</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>        <span class=n>next_action</span> <span class=o>=</span> <span class=n>epsilon_greedy</span><span class=p>(</span><span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>Q</span><span class=p>,</span> <span class=n>next_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>action_idx</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>get_action_idx</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>next_action_idx</span> <span class=o>=</span> <span class=n>grid_world</span><span class=o>.</span><span class=n>get_action_idx</span><span class=p>(</span><span class=n>next_action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action_idx</span><span class=p>]</span> <span class=o>+=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>reward</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>Q</span><span class=p>[</span><span class=n>next_state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> \
</span></span><span class=line><span class=cl>            <span class=n>next_state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>next_action_idx</span><span class=p>]</span> <span class=o>-</span> <span class=n>Q</span><span class=p>[</span><span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>],</span> <span class=n>action_idx</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span><span class=line><span class=cl>        <span class=n>action</span> <span class=o>=</span> <span class=n>next_action</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>rewards</span>
</span></span></code></pre></div><p>And lastly, wrapping everything together in the main function, we have</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>height</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>    <span class=n>width</span> <span class=o>=</span> <span class=mi>13</span>
</span></span><span class=line><span class=cl>    <span class=n>start_state</span> <span class=o>=</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>goal_state</span> <span class=o>=</span> <span class=p>[</span><span class=mi>3</span><span class=p>,</span> <span class=mi>12</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>cliff</span> <span class=o>=</span> <span class=p>[[</span><span class=mi>3</span><span class=p>,</span> <span class=n>x</span><span class=p>]</span> <span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>12</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>    <span class=n>grid_world</span> <span class=o>=</span> <span class=n>GridWorld</span><span class=p>(</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=n>start_state</span><span class=p>,</span> <span class=n>goal_state</span><span class=p>,</span> <span class=n>cliff</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>n_runs</span> <span class=o>=</span> <span class=mi>50</span>
</span></span><span class=line><span class=cl>    <span class=n>n_eps</span> <span class=o>=</span> <span class=mi>500</span>
</span></span><span class=line><span class=cl>    <span class=n>epsilon</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>    <span class=n>alpha</span> <span class=o>=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>    <span class=n>gamma</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>height</span><span class=p>,</span> <span class=n>width</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid_world</span><span class=o>.</span><span class=n>actions</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards_q_learning</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards_sarsa</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>n_eps</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>n_runs</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>Q_q_learning</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>Q_sarsa</span> <span class=o>=</span> <span class=n>Q</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>ep</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_eps</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards_q_learning</span><span class=p>[</span><span class=n>ep</span><span class=p>]</span> <span class=o>+=</span> <span class=n>q_learning</span><span class=p>(</span><span class=n>Q_q_learning</span><span class=p>,</span> <span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards_sarsa</span><span class=p>[</span><span class=n>ep</span><span class=p>]</span> <span class=o>+=</span> <span class=n>sarsa</span><span class=p>(</span><span class=n>Q_sarsa</span><span class=p>,</span> <span class=n>grid_world</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>rewards_q_learning</span> <span class=o>/=</span> <span class=n>n_runs</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards_sarsa</span> <span class=o>/=</span> <span class=n>n_runs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>rewards_q_learning</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Q-Learning&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>rewards_sarsa</span><span class=p>,</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;Sarsa&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=s1>&#39;Episodes&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Sum of rewards during episode&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylim</span><span class=p>([</span><span class=o>-</span><span class=mi>100</span><span class=p>,</span> <span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;./cliff_walking.png&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><p>This is our result after completing running the code.</p><figure><img src=/images/td-learning/cliff_walking.png alt="Q-learning vs Sarsa on Cliff walking" style=display:block;margin-left:auto;margin-right:auto;width:500px><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=exp-sarsa>Expected Sarsa<a hidden class=anchor aria-hidden=true href=#exp-sarsa>#</a></h4><p>In the update \eqref{eq:ql.1} of Q-learning, rather than taking the maximum over next state-action pairs, we use the expected value to consider how likely each action is under the current policy. That means, we instead have the following update rule for $Q$-value:
\begin{align}
Q(S_t,A_t)&\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\mathbb{E}_\pi\big[Q(S_{t+1},A_{t+1}\vert S_{t+1})\big]-Q(S_t,A_t)\Big] \\ &\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1}|a)-Q(S_t,A_t)\Big]
\end{align}
However, given the next state, $S_{t+1}$, this algorithms move <em>deterministically</em> in the same direction as Sarsa moves in <em>expectation</em>. Thus, this method is also called <strong>Expected Sarsa</strong>.</p><p>Expected Sarsa performs better than Sarsa since it eliminates the variance due to the randomization in selecting $A_{t+1}$. Which also means that it takes expected Sarsa more resource than Sarsa.</p><h4 id=double-q-learning>Double Q-learning<a hidden class=anchor aria-hidden=true href=#double-q-learning>#</a></h4><h5 id=max-bias>Maximization Bias<a hidden class=anchor aria-hidden=true href=#max-bias>#</a></h5><p>Consider a set of $M$ random variables $X=\{X_1,\dots,X_M\}$. Say that we are interested in maximizing expected value of the r.v.s in $X$:
\begin{equation}
\max_{i=1,\dots,M}\mathbb{E}(X_i)\label{eq:mb.1}
\end{equation}
This value can be approximated by constructing approximations for $\mathbb{E}(X_i)$ for all $i$. Let
\begin{equation}
S=\bigcup_{i=1}^{M}S_i
\end{equation}
denote a set of samples, where $S_i$ is the subset containing samples for the variables $X_i$, and assume that the samples in $S_i$ are i.i.d. Unbiased estimates for the expected values can be obtained by computing the sample average for each variable:
\begin{equation}
\mathbb{E}(X_i)=\mathbb{E}(\mu_i)\approx\mu_i(S)\doteq\frac{1}{\vert S_i\vert}\sum_{s\in S_i}s,
\end{equation}
where $\mu_i$ is an estimator for variable $X_i$. This approximation is unbiased since every sample $s\in S_i$ is an unbiased estimate for the value of $\mathbb{E}(X_i)$. Thus, \eqref{eq:mb.1} can be approximated by:
\begin{equation}
\max_{i=1,\dots,M}\mathbb{E}(X_i)=\max_{i=1,\dots,M}\mathbb{E}(\mu_i)\approx\max_{i=1,\dots,M}\mu_i(S)
\end{equation}
Let $f_i$, $F_i$ denote the PDF and CDF of $X_i$ and $f_i^\mu, F_i^\mu$ denote the PDF and CDF of $\mu_i$ respectively. Hence we have that
\begin{align}
\mathbb{E}(X_i)&=\int_{-\infty}^{\infty}x f_i(x)\hspace{0.1cm}dx;\hspace{0.5cm}F_i(x)=P(X_i\leq x)=\int_{-\infty}^{\infty}f_i(x)\hspace{0.1cm}dx \\ \mathbb{E}(\mu_i)&=\int_{-\infty}^{\infty}x f_i^\mu(x)\hspace{0.1cm}dx;\hspace{0.5cm}F_i^\mu(x)=P(\mu_i\leq x)=\int_{-\infty}^{\infty}f_i^\mu(x)\hspace{0.1cm}dx
\end{align}
With these notations, considering the maximal estimator $\mu_i$, which is distributed by some PDF $f_{\max}^{\mu}$, we have:
\begin{align}
F_{\max}^{\mu}&\doteq P(\max_i \mu_i\leq x) \\ &=P(\mu_1\leq x;\dots;\mu_M\leq x) \\ &=\prod_{i=1}^{M}P(\mu_i\leq x) \\ &=\prod_{i=1}^{M}F_i^\mu(x)
\end{align}
The value $\max_i\mu_i(S)$ is an unbiased estimate of $\mathbb{E}(\max_i\mu_i)$, which is given by
\begin{align}
\mathbb{E}\left(\max_i\mu_i\right) &=\int_{-\infty}^{\infty}x f_{\max}^{\mu}(x)\hspace{0.1cm}dx \\ &=\int_{-\infty}^{\infty}x\frac{d}{dx}\left(\prod_{i=1}^{M}F_i^\mu(x)\right)\hspace{0.1cm}dx \\ &=\sum_{i=1}^M\int_{-\infty}^{\infty}f_i^\mu(x)\prod_{j\neq i}^{M}F_i^\mu(x)\hspace{0.1cm}dx
\end{align}
However, as can be seen in \eqref{eq:mb.1}, the order of expectation and maximization is the other way around. This leads to the result that $\max_i\mu_i(S)$ is a biased estimate of $\max_i\mathbb{E}(X_i)$</p><h5 id=sol>A Solution<a hidden class=anchor aria-hidden=true href=#sol>#</a></h5><p>The reason why maximization bias happens is we are using the same samples to decide which action is the best (highest reward one) and also to estimate its action-value. To overcome this situation, Hasselt (2010) proposed an alternative method that uses two set of estimators instead, $\mu^A=\{\mu_1^A,\dots,\mu_M^A\}$ and $\mu^B=\{\mu_1^B,\dots,\mu_M^B\}$. The method is thus also called <strong>double estimators</strong>.</p><p>Specifically, we use these two sets to learn two independent estimates, called $Q^A$ and $Q^B$, each is an estimate of the true value $q(a)$, for all $a\in\mathcal{A}$.</p><figure><img src=/images/td-learning/double-q-learning.png alt="Double Q-learning" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h2 id=n-step-td>$\boldsymbol{n}$-step TD<a hidden class=anchor aria-hidden=true href=#n-step-td>#</a></h2><p>From the definition of <em>one-step TD</em>, we can formalize the idea into a more general, <strong>n-step TD</strong>. Once again, first off, we will be considering the prediction problem.</p><h3 id=n-step-td-prediction>$\boldsymbol{n}$-step TD Prediction<a hidden class=anchor aria-hidden=true href=#n-step-td-prediction>#</a></h3><p>Recall that in <em>one-step TD</em>, the update is based on the next reward, bootstrapping<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> from the value of the state at one step later. In particular,
the target of the update is $R_{t+1}+\gamma V_t(S_{t+1})$, which we are going to denote as $G_{t:t+1}$, or <em>one-step return</em>:
\begin{equation}
G_{t:t+1}\doteq R_{t+1}+\gamma V_t(S_{t+1})
\end{equation}
where $V_t:\mathcal{S}\to\mathbb{R}$ is the estimate at time step $t$ of $v_\pi$. Thus, rather than taking into account one step later, in <em>two-step TD</em>, it makes sense to consider the rewards in two steps further, combined with the value function of the state at two step later. In other words, the target of two-step update is the <em>two-step return</em>:
\begin{equation}
G_{t:t+2}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 V_{t+1}(S_{t+2})
\end{equation}
Similarly, the target of $n$-step update is the <span id=n-step-return><strong>$\boldsymbol{n}$-step return</strong></span>:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t&lt;T-n$. If $t+n\geq T$, then all the missing terms are taken as zero, and the <em>n-step return</em> defined to be equal to the full return:
\begin{equation}
G_{t:t+n}=G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots+\gamma^{T-t-1}R_T,\label{eq:nstp.1}
\end{equation}
which is the target of the Monte Carlo update.</p><p>Hence, the <span id=n-step-td-update><strong>$\boldsymbol{n}$-step TD</strong></span> method can be defined as:
\begin{equation}
V_{t+n}(S_t)\doteq V_{t+n-1}(S_t)+\alpha\left[G_{t:t+n}-V_{t+n-1}(S_t)\right],
\end{equation}
for $0\leq t&lt;T$, while the values for all other states remain unchanged: $V_{t+n}(s)=V_{t+n-1}(s),\forall s\neq S_t$. Pseudocode of the algorithm is given right below.</p><figure><img src=/images/td-learning/n-step-td.png alt="n-step TD" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>From \eqref{eq:nstp.1} combined with this definition of <em>$n$-step TD</em> method, it is easily seen that by changing the value of $n$ from $1$ to $\infty$, we obtain a corresponding spectrum ranging from <em>one-step TD method</em> to <em>Monte Carlo method</em>.</p><figure><img src=/images/td-learning/n-step-td-diagram.png alt="Backup diagram of n-step TD" style=display:block;margin-left:auto;margin-right:auto;width:450px;height:370px><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: The backup diagram of $n$-step TD methods</figcaption></figure><h4 id=eg-random-walk>Example: Random Walk<a hidden class=anchor aria-hidden=true href=#eg-random-walk>#</a></h4><p>(This example is taken from <strong>Example 7.1</strong>, <a href=#rl-book><strong>Reinforcement Learning: An Introduction</strong></a> book; the random process image is created based on the figure from <a href=#random_walk><strong>Singd & Sutton</strong></a>).</p><p>Suppose we have a random process as following</p><figure><img src=/images/td-learning/random_process.png alt="Random process" style=display:block;margin-left:auto;margin-right:auto;width:620px;height:120px><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>Specifically, the reward is zero everywhere except the transitions into terminal states: the transition from State 2 to State 1 (with reward of $-1$) and the transition from State 20 to State 21 (with reward of $1$). The discount factor $\gamma$ is $1$. The initial value estimates are $0$ for all states. We will implement $n$-step TD method for $n\in\{1,2,4,\dots,512\}$ and step size $\alpha\in\{0,0.2,0.4,\dots,1\}$. The walk starts at State 10.</p><p><strong>Solution</strong><br>The source code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-07/random_walk.py>here</a>.</p><p>As usual, we need these packages for our implementation.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm</span> <span class=kn>import</span> <span class=n>tqdm</span>
</span></span></code></pre></div><p>First off, we need to define our environment, the random walk process. The <code>is_terminal()</code> function is used to check whether the state considered is a terminal state, while the <code>take_action()</code> function itself returns the next state and corresponding reward given the current state and the action taken.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RandomWalk</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Random walk environment
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>n_states</span><span class=p>,</span> <span class=n>start_state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span> <span class=o>=</span> <span class=n>n_states</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>states</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>n_states</span> <span class=o>+</span> <span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>start_state</span> <span class=o>=</span> <span class=n>start_state</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>end_states</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>,</span> <span class=n>n_states</span> <span class=o>+</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>actions</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>action_prob</span> <span class=o>=</span> <span class=mf>0.5</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span> <span class=o>=</span> <span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>is_terminal</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Whether state @state is an end state
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        state: int
</span></span></span><span class=line><span class=cl><span class=s1>            current state
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>state</span> <span class=ow>in</span> <span class=bp>self</span><span class=o>.</span><span class=n>end_states</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>take_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>        Take action @action at state @state
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Params
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        state: int
</span></span></span><span class=line><span class=cl><span class=s1>            current state
</span></span></span><span class=line><span class=cl><span class=s1>        action: int
</span></span></span><span class=line><span class=cl><span class=s1>            action taken
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>        Return
</span></span></span><span class=line><span class=cl><span class=s1>        ------
</span></span></span><span class=line><span class=cl><span class=s1>        (next_state, reward): (int, int)
</span></span></span><span class=line><span class=cl><span class=s1>            a tuple of next state and reward
</span></span></span><span class=line><span class=cl><span class=s1>        &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>        <span class=n>next_state</span> <span class=o>=</span> <span class=n>state</span> <span class=o>+</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>next_state</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>elif</span> <span class=n>next_state</span> <span class=o>==</span> <span class=bp>self</span><span class=o>.</span><span class=n>n_states</span> <span class=o>+</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>reward</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span>
</span></span></code></pre></div><p>To calculate the RMSE, we need to compute the true value of states, which can be achieved with the help of <code>get_true_value()</code> function. Here we apply Bellman equations to calculate the true value of states.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>get_true_value</span><span class=p>(</span><span class=n>random_walk</span><span class=p>,</span> <span class=n>gamma</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Calculate true value of @random_walk by Bellman equations
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    random_walk: RandomWalk
</span></span></span><span class=line><span class=cl><span class=s1>    gamma: float
</span></span></span><span class=line><span class=cl><span class=s1>        discount factor
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>P</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span><span class=p>,</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>true_value</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span> <span class=o>+</span> <span class=mi>2</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>state</span> <span class=ow>in</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>states</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>next_states</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>rewards</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>action</span> <span class=ow>in</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>actions</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>next_state</span> <span class=o>=</span> <span class=n>state</span> <span class=o>+</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>            <span class=n>next_states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>next_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>next_state</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>reward</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>elif</span> <span class=n>next_state</span> <span class=o>==</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span> <span class=o>+</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>reward</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>2</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>reward</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>state_</span><span class=p>,</span> <span class=n>reward_</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>next_states</span><span class=p>,</span> <span class=n>rewards</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>(</span><span class=n>state_</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>P</span><span class=p>[</span><span class=n>state</span> <span class=o>-</span> <span class=mi>1</span><span class=p>,</span> <span class=n>state_</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>action_prob</span> <span class=o>*</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=n>r</span><span class=p>[</span><span class=n>state_</span><span class=p>]</span> <span class=o>=</span> <span class=n>reward_</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>    <span class=n>u</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span><span class=p>,</span> <span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>u</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>action_prob</span> <span class=o>*</span> <span class=mi>1</span> <span class=o>*</span> <span class=p>(</span><span class=o>-</span><span class=mi>1</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>u</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>action_prob</span> <span class=o>*</span> <span class=mi>1</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>rewards</span><span class=p>[</span><span class=mi>2</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>r</span> <span class=o>=</span> <span class=n>r</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>true_value</span><span class=p>[</span><span class=mi>1</span><span class=p>:</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>inv</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>identity</span><span class=p>(</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span><span class=p>)</span> <span class=o>-</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>P</span><span class=p>)</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=mf>0.5</span> <span class=o>*</span> <span class=p>(</span><span class=n>P</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>r</span><span class=p>)</span> <span class=o>+</span> <span class=n>u</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>true_value</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>true_value</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>true_value</span>
</span></span></code></pre></div><p>In this random walk experiment, we simply use random policy as our action selection.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>random_policy</span><span class=p>(</span><span class=n>random_walk</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    Choose an action randomly
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    random_walk: RandomWalk
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=n>random_walk</span><span class=o>.</span><span class=n>actions</span><span class=p>)</span>
</span></span></code></pre></div><p>Now it is time to implement our algorithm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>n_step_temporal_difference</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>random_walk</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>    n-step TD
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>    Params
</span></span></span><span class=line><span class=cl><span class=s1>    ------
</span></span></span><span class=line><span class=cl><span class=s1>    V: np.ndarray
</span></span></span><span class=line><span class=cl><span class=s1>        value function
</span></span></span><span class=line><span class=cl><span class=s1>    n: int
</span></span></span><span class=line><span class=cl><span class=s1>        number of steps
</span></span></span><span class=line><span class=cl><span class=s1>    alpha: float
</span></span></span><span class=line><span class=cl><span class=s1>        step size
</span></span></span><span class=line><span class=cl><span class=s1>    random_walk: RandomWalk
</span></span></span><span class=line><span class=cl><span class=s1>    &#39;&#39;&#39;</span>
</span></span><span class=line><span class=cl>    <span class=n>state</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>start_state</span>
</span></span><span class=line><span class=cl>    <span class=n>states</span> <span class=o>=</span> <span class=p>[</span><span class=n>state</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>T</span> <span class=o>=</span> <span class=nb>float</span><span class=p>(</span><span class=s1>&#39;inf&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>t</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>rewards</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span> <span class=c1># dummy reward to save the next reward as R_{t+1}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=kc>True</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>t</span> <span class=o>&lt;</span> <span class=n>T</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>action</span> <span class=o>=</span> <span class=n>random_policy</span><span class=p>(</span><span class=n>random_walk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>next_state</span><span class=p>,</span> <span class=n>reward</span> <span class=o>=</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>take_action</span><span class=p>(</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>states</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>next_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>rewards</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>reward</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>(</span><span class=n>next_state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>T</span> <span class=o>=</span> <span class=n>t</span> <span class=o>+</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=n>tau</span> <span class=o>=</span> <span class=n>t</span> <span class=o>-</span> <span class=n>n</span> <span class=o>+</span> <span class=mi>1</span> <span class=c1># updated state&#39;s time</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>tau</span> <span class=o>&gt;=</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>G</span> <span class=o>=</span> <span class=mi>0</span> <span class=c1># return</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>tau</span> <span class=o>+</span> <span class=mi>1</span><span class=p>,</span> <span class=nb>min</span><span class=p>(</span><span class=n>tau</span> <span class=o>+</span> <span class=n>n</span><span class=p>,</span> <span class=n>T</span><span class=p>)</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                <span class=n>G</span> <span class=o>+=</span> <span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=n>gamma</span><span class=p>,</span> <span class=n>i</span> <span class=o>-</span> <span class=n>tau</span> <span class=o>-</span> <span class=mi>1</span><span class=p>)</span> <span class=o>*</span> <span class=n>rewards</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>tau</span> <span class=o>+</span> <span class=n>n</span> <span class=o>&lt;</span> <span class=n>T</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>G</span> <span class=o>+=</span> <span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=n>gamma</span><span class=p>,</span> <span class=n>n</span><span class=p>)</span> <span class=o>*</span> <span class=n>V</span><span class=p>[</span><span class=n>states</span><span class=p>[</span><span class=n>tau</span> <span class=o>+</span> <span class=n>n</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=ow>not</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>(</span><span class=n>states</span><span class=p>[</span><span class=n>tau</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>                <span class=n>V</span><span class=p>[</span><span class=n>states</span><span class=p>[</span><span class=n>tau</span><span class=p>]]</span> <span class=o>+=</span> <span class=n>alpha</span> <span class=o>*</span> <span class=p>(</span><span class=n>G</span> <span class=o>-</span> <span class=n>V</span><span class=p>[</span><span class=n>states</span><span class=p>[</span><span class=n>tau</span><span class=p>]])</span>
</span></span><span class=line><span class=cl>        <span class=n>t</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>tau</span> <span class=o>==</span> <span class=n>T</span> <span class=o>-</span> <span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>        <span class=n>state</span> <span class=o>=</span> <span class=n>next_state</span>
</span></span></code></pre></div><p>As usual, we are going illustrate our result in the main function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>n_states</span> <span class=o>=</span> <span class=mi>19</span>
</span></span><span class=line><span class=cl>    <span class=n>start_state</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>    <span class=n>gamma</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>random_walk</span> <span class=o>=</span> <span class=n>RandomWalk</span><span class=p>(</span><span class=n>n_states</span><span class=p>,</span> <span class=n>start_state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>true_value</span> <span class=o>=</span> <span class=n>get_true_value</span><span class=p>(</span><span class=n>random_walk</span><span class=p>,</span> <span class=n>gamma</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>episodes</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>    <span class=n>runs</span> <span class=o>=</span> <span class=mi>100</span>
</span></span><span class=line><span class=cl>    <span class=n>ns</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>10</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=n>alphas</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mf>1.1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>errors</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=nb>len</span><span class=p>(</span><span class=n>ns</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>alphas</span><span class=p>)))</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>n_i</span><span class=p>,</span> <span class=n>n</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>ns</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>alpha_i</span><span class=p>,</span> <span class=n>alpha</span> <span class=ow>in</span> <span class=nb>enumerate</span><span class=p>(</span><span class=n>alphas</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>runs</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>                <span class=n>V</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span> <span class=o>+</span> <span class=mi>2</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                <span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>episodes</span><span class=p>):</span>
</span></span><span class=line><span class=cl>                    <span class=n>n_step_temporal_difference</span><span class=p>(</span><span class=n>V</span><span class=p>,</span> <span class=n>n</span><span class=p>,</span> <span class=n>alpha</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>random_walk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>                    <span class=n>rmse</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>sqrt</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>sum</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=n>V</span> <span class=o>-</span> <span class=n>true_value</span><span class=p>,</span> <span class=mi>2</span><span class=p>)</span> <span class=o>/</span> <span class=n>random_walk</span><span class=o>.</span><span class=n>n_states</span><span class=p>))</span>
</span></span><span class=line><span class=cl>                    <span class=n>errors</span><span class=p>[</span><span class=n>n_i</span><span class=p>,</span> <span class=n>alpha_i</span><span class=p>]</span> <span class=o>+=</span> <span class=n>rmse</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>errors</span> <span class=o>/=</span> <span class=n>episodes</span> <span class=o>*</span> <span class=n>runs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>ns</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>plt</span><span class=o>.</span><span class=n>plot</span><span class=p>(</span><span class=n>alphas</span><span class=p>,</span> <span class=n>errors</span><span class=p>[</span><span class=n>i</span><span class=p>,</span> <span class=p>:],</span> <span class=n>label</span><span class=o>=</span><span class=s1>&#39;n = </span><span class=si>%d</span><span class=s1>&#39;</span> <span class=o>%</span> <span class=p>(</span><span class=n>ns</span><span class=p>[</span><span class=n>i</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>xlabel</span><span class=p>(</span><span class=sa>r</span><span class=s1>&#39;$\alpha$&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylabel</span><span class=p>(</span><span class=s1>&#39;Average RMS error&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>ylim</span><span class=p>([</span><span class=mf>0.25</span><span class=p>,</span> <span class=mf>0.55</span><span class=p>])</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>legend</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;./random_walk.png&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><p>This is our result after completing running the code.</p><figure><img src=/images/td-learning/random_walk.png alt="Random Walk with n-step TD" style=display:block;margin-left:auto;margin-right:auto;width:500px><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=n-step-td-control>$\boldsymbol{n}$-step TD Control<a hidden class=anchor aria-hidden=true href=#n-step-td-control>#</a></h3><p>Similarly, we can apply $n$-step TD methods to control task. In particular, we will combine the idea of $n$-step update with Sarsa, a control method we previously have defined above.</p><h4 id=n-step-sarsa>$\boldsymbol{n}$-step Sarsa<a hidden class=anchor aria-hidden=true href=#n-step-sarsa>#</a></h4><p>As usual, to apply our method to control problem, rather than taking into account states, we instead consider state-action pairs $s,a$ in order to learn the value functions, $q_\pi(s,a)$, of them.<br>Recall that the target in <em>one-step Sarsa</em> update is
\begin{equation}
G_{t:t+1}\doteq R_{t+1}+\gamma Q_t(S_{t+1},A_{t+1})
\end{equation}
Similar to what we have done in the previous part of <a href=#n-step-td-prediction>$n$-step TD Prediction</a>, we can redefine the new target of our $n$-step update
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1} R_{t+n}+\gamma^n Q_{t+n-1}(S_{t+n},A_{t+n}),
\end{equation}
for $n\geq 0,0\leq t&lt;T-n$, with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$. The <strong>$\boldsymbol{n}$-step Sarsa</strong> is then can be defined as:
\begin{equation}
Q_{t+n}(S_t,A_t)\doteq Q_{t+n-1}(S_t,A_t)+\alpha\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\right],\hspace{1cm}0\leq t&lt;T,\label{eq:nss.1}
\end{equation}
while the values of all other state-action pairs remain unchanged: $Q_{t+n}(s,a)=Q_{t+n-1}(s,a)$, for all $s,a$ such that $s\neq S_t$ or $a\neq A_t$.</p><p>From this definition of $n$-step Sarsa, we can easily derive the multiple step version of Expected Sarsa, called <strong>$\boldsymbol{n}$-step Expected Sarsa</strong>.
\begin{equation}
Q_{t+n}(S_t,A_t)\doteq Q_{t+n-1}(S_t,A_t)+\alpha\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\right],\hspace{1cm}0\leq t&lt;T,
\end{equation}
which has the same rule as \eqref{eq:nss.1}, except that the target of the update in this case is defined as:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\overline{V}_{t+n-1}(S_{t+n}),\hspace{1cm}t+n&lt;T,\label{eq:nss.2}
\end{equation}
with $G_{t:t+n}=G_t$ for $t+n\geq T$, where $\overline{V}_t(s)$ is the <span id=expected-approximate-value><strong>expected approximate value</strong></span> of state $s$, using the estimated action value at time $t$, under the target policy $\pi$:
\begin{equation}
\overline{V}_t(s)\doteq\sum_a\pi(a|s)Q_t(s,a),\hspace{1cm}\forall s\in\mathcal{S}\label{eq:nss.3}
\end{equation}
If $s$ is terminal, then its expected approximate value is defined to be zero.</p><p>Pseudocode of the $n$-step Sarsa algorithm is given right below.</p><figure><img src=/images/td-learning/n-step-sarsa.png alt="n-step Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>When taking the value of $n$ from $1$ to $\infty$, similarly, we also obtain a corresponding spectrum ranging from <em>one-step Sarsa</em> to <em>Monte Carlo</em>.</p><figure><img src=/images/td-learning/n-step-td-state-action-diagram.png alt="Backup diagram of n-step TD for state-action values" style=display:block;margin-left:auto;margin-right:auto;width:570px;height:370px><figcaption style=text-align:center;font-style:italic><b>Figure 3</b>: The backup diagram of $n$-step methods for state-action values</figcaption></figure><h3 id=off-policy-n-step-td>Off-policy $\boldsymbol{n}$-step TD<a hidden class=anchor aria-hidden=true href=#off-policy-n-step-td>#</a></h3><p>Recall that off-policy methods are ones that learn the value function of a <em>target policy</em>, $\pi$, while follows a <em>behavior policy</em>, $b$. In this section, we will be considering an off-policy $n$-step TD, or in specifically, $n$-step TD using <strong>Importance Sampling</strong><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><h4 id=n-step-td-is>$\boldsymbol{n}$-step TD with Importance Sampling<a hidden class=anchor aria-hidden=true href=#n-step-td-is>#</a></h4><p>In $n$-step methods, returns are constructed over $n$ steps, so we are interested in the relative probability of just those $n$ actions. Thus, by weighting updates by <em>importance sampling ratio</em>, $\rho_{t:t+n-1}$, which is the relative probability under the two policies $\pi$ and $b$ of taking $n$ actions from $A_t$ to $A_{t+n-1}$:
\begin{equation}
\rho_{t:h}\doteq\prod_{k=t}^{\min(h,T-1)}\frac{\pi(A_k|S_k)}{b(A_k|S_k)},
\end{equation}
we can get the <strong>off-policy $\boldsymbol{n}$-step TD</strong> method.
\begin{equation}
V_{t+n}(S_t)\doteq V_{t+n-1}(S_t)+\alpha\rho_{t:t+n-1}\left[G_{t:t+n}-V_{t+n-1}(S_t)\right],\hspace{1cm}0\leq t&lt;T
\end{equation}
Similarly, we have the <strong>off-policy $\boldsymbol{n}$-step Sarsa</strong> method.
\begin{equation}
Q_{t+n}(S_t,A_t)\doteq Q_{t+n-1}(S_t,A_t)+\alpha\rho_{t:t+n-1}\left[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\right],\hspace{0.5cm}0\leq t &lt;T\label{eq:nsti.1}
\end{equation}
The <strong>off-policy $\boldsymbol{n}$-step Expected Sarsa</strong> uses the same update as \eqref{eq:nsti.1} except that it uses $\rho_{t+1:t+n-1}$ as its importance sampling ratio instead of $\rho_{t+1:t+n}$ and also has \eqref{eq:nss.2} as its target.</p><p>Following is pseudocode of the off-policy $n$-step Sarsa.</p><figure><img src=/images/td-learning/off-policy-n-step-sarsa.png alt="Off-policy n-step Sarsa" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=per-decision-control-variates>Per-decision Methods with Control Variates<a hidden class=anchor aria-hidden=true href=#per-decision-control-variates>#</a></h4><p>Recall that in the note of <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/>Monte Carlo Methods</a>, to reduce the variance even in the absence of discounting (i.e., $\gamma=1$), we used a method called <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#per-decision-is><strong>Per-decision Importance Sampling</strong></a>. So how about we use it with multi-step off-policy TD methods?</p><p>We begin rewriting the $n$-step return ending at horizon $h$ as:
\begin{equation}
G_{t:h}=R_{t+1}+\gamma G_{t+1:h},\hspace{1cm}1\lt h\lt T,
\end{equation}
where $G_{h:h}\doteq V_{h-1}(S_h)$.</p><p>Since we are following a policy $b$ that is not the same as the target policy $\pi$, all of the resulting experience, including the first reward $R_{t+1}$ and the next state $S_{t+1}$ must be weighted by the importance sampling ratio for time $t$, $\rho_t=\frac{\pi(A_t\vert S_t)}{b(A_t\vert S_t)}$. And moreover, to avoid the high variance when the $n$-step return is zero (resulting when the action at time $t$ would never be select by $\pi$, which leads to $\rho_t=0$), we define the $n$-step return ending at horizon $h$ for the off-policy state-value prediction as:
<span id=n-step-return-control-variate-state-value>\begin{equation}
G_{t:h}\doteq\rho_t\left(R_{t+1}+\gamma G_{t+1:h}\right)+(1-\rho_t)V_{h-1}(S_t),\hspace{1cm}1\lt h\lt T\label{eq:pdcv.1}
\end{equation}</span>
where $G_{h:h}\doteq V_{h-1}(S_h)$. The second term of \eqref{eq:pdcv.1}, $(1-\rho_t)V_{h-1}(S_t)$, is called <strong>control variate</strong>, which has the expected value of $0$, and then does not change the expected update.</p><p>For state-action values, the off-policy definition of the $n$-step return ending at horizon $h$ can be defined as:
<span id=n-step-return-control-variate-action-value>\begin{align}
G_{t:h}&\doteq R_{t+1}+\gamma\left(\rho_{t+1}G_{t+1:h}+\overline{V}_{h-1}(S_{t+1})-\rho_{t+1}Q_{h-1}(S_{t+1},A_{t+1})\right) \\ &=R_{t+1}+\gamma\rho_{t+1}\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\big)+\gamma\overline{V}_{h-1}(S_{t+1}),\hspace{1cm}t\lt h\leq T\label{eq:pdcv.2}
\end{align}</span>
If $h\lt T$, the recursion ends with $G_{h:h}\doteq Q_{h-1}(S_h,A_h)$, whereas, if $h\geq T$, the recursion ends with $G_{T-1:h}\doteq R_T$.</p><h4 id=n-step-tree-backup>$\boldsymbol{n}$-step Tree Backup<a hidden class=anchor aria-hidden=true href=#n-step-tree-backup>#</a></h4><p>So, is there possibly an off-policy method without the use of importance sampling? Yes, and of them is called <strong>Tree-backup</strong>.</p><p>The idea of tree-backup update is to start with the target of the one-step update, which is defined as the first reward plus the discounted estimated value of the next state. This estimated value is computed as the weighted sum of estimated action values. Each weight corresponding to an action is proportional to its probability of occurrence. In particular, the target of one-step tree-backup update is:
\begin{equation}
G_{t:t+1}\doteq R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q_t(S_{t+1},a),\hspace{1cm}t&lt;T-1
\end{equation}
which is the same as that of Expected Sarsa. With two-step update, for a certain action $A_{t+1}$ taken according to the behavior policy, $b$ (i.e. $b(A_{t+1}|S_{t+1})=1$), one step later, the estimated value of the next state similarly now, can be computed as:
\begin{equation}
\pi(A_{t+1}|S_{t+1})\Big(R_{t+2}+\gamma\pi(a|S_{t+2})Q_{t+1}(S_{t+2},a)\Big)
\end{equation}
The target of two-step update, which also is defined as sum of the first reward received plus the discounted estimated value of the next state therefore, can be computed as
\begin{align}
G_{t:t+2}&\doteq R_{t+1}+\gamma\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q_{t+1}(S_{t+1},a) \\ &\hspace{1cm}+\gamma\pi(A_{t+1}|S_{t+1})\Big(R_{t+2}+\gamma\pi(a|S_{t+2})Q_{t+1}(S_{t+2},a)\Big) \\&=R_{t+1}+\gamma\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q_{t+1}(S_{t+1},a)+\gamma\pi(A_{t+1}|S_{t+1})G_{t+1:t+2},
\end{align}
for $t&lt;T-2$. Hence, the target of the $n$-step tree-backup update recursively can be defined as:
<span id=n-step-tree-backup-return>\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q_{t+n-1}(S_{t+1},a)+\gamma\pi(A_{t+1}|S_{t+1})G_{t+1:t+n}\label{eq:nstb.1}
\end{equation}</span>
for $t&lt;T-1,n\geq 2$. The $n$-step tree-backup update can be illustrated through the following diagram</p><figure><img src=/images/td-learning/3-step-tree-backup.png alt="3-step tree-backup" style=display:block;margin-left:auto;margin-right:auto;width:110px;height:375px><figcaption style=text-align:center;font-style:italic><b>Figure 4</b>: The backup diagram of 3-step tree-backup</figcaption></figure><p>With this definition of the target, we now can define our <strong>$\boldsymbol{n}$-step tree-backup</strong> method as:
\begin{equation}
Q_{t+n}(S_t,A_t)\doteq Q_{t+n-1}(S_t,A_t)+\alpha\Big[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\Big],\hspace{1cm}0\leq t&lt;T
\end{equation}
while the values of all other state-action pairs remain unchanged: $Q_{t+n}(s,a)=Q_{t+n-1}(s,a)$, for all $s,a$ such that $s\neq S_t$ or $a\neq A_t$. Pseudocode of the n-step tree-backup algorithm is given below.</p><figure><img src=/images/td-learning/n-step-tree-backup.png alt="n-step tree-backup" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=n-step-q-sigma>$\boldsymbol{n}$-step $Q(\sigma)$<a hidden class=anchor aria-hidden=true href=#n-step-q-sigma>#</a></h4><p>In updating the action-value functions, if we choose always to sample, we would obtain Sarsa, whereas if we choose never to sample, we would get the tree-backup algorithm. Expected Sarsa would be the case where we choose to sample for all steps except for the last one.
An possible unifying method is choosing on a state-by-state basis whether to sample or not.</p><p>We begin by rewriting the tree-backup $n$-step return \eqref{eq:nstb.1} in terms of the horizon $h=t+n$ and the expected approximate value $\overline{V}$ \eqref{eq:nss.3}:
\begin{align}
\hspace{-0.6cm}G_{t:h}&=R_{t+1}+\gamma\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})Q_{h-1}(S_{t+1},a)+\gamma\pi(A_{t+1}|S_{t+1})G_{t+1:h} \\ &=R_{t+1}+\gamma\overline{V}_{h-1}(S_{t+1})-\gamma\pi(A_{t+1}|S_{t+1})Q_{h-1}(S_{t+1},A_{t+1})+\gamma\pi(A_{t+1}|S_{t+1})G_{t+1:h} \\ &=R_{t+1}+\gamma\pi(A_{t+1}|S_{t+1})\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\big)+\gamma\overline{V}_{h-1}(S_{t+1}),
\end{align}
which is exactly the same as the $n$-step return for Sarsa with control variates \eqref{eq:pdcv.2} except that the importance-sampling ratio $\rho_{t+1}$ has been replaced with the action probability $\pi(A_{t+1}|S_{t+1})$.</p><p>Let $\sigma_t\in[0,1]$ denote the degree of sampling on step $t$, with $\sigma=1$ denoting full sampling and $\sigma=0$ denoting a pure expectation with no sampling. The r.v $\sigma_t$ might be set as a function of the state, action or state-action pair at time $t$.</p><figure><img src=/images/td-learning/n-step-q-sigma-backup.png alt="Backup diagrams of n-step Sarsa, Tree-backup, Expected Sarsa, Q(sigma)" style=display:block;margin-left:auto;margin-right:auto;width:530px;height:370px><figcaption style=text-align:center;font-style:italic><b>Figure 5</b>: The backup diagrams of $n$-step methods for state-action values: Sarsa, Tree-backup, Expected Sarsa, $Q(\sigma)$</figcaption></figure><p>With the definition of $\sigma_t$, we can define the $n$-step return ending at horizon $h$ of the $Q(\sigma)$ as:
\begin{align}
G_{t:h}&\doteq R_{t+1}+\gamma\Big(\sigma_{t+1}\rho_{t+1}+(1-\rho_{t+1})\pi(A_{t+1}|S_{t+1})\Big)\Big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\Big)\nonumber \\ &\hspace{2cm}+\gamma\overline{V}_{h-1}(S_{t+1}),
\end{align}
for $t\lt h\leq T$. The recursion ends with $G_{h:h}\doteq Q_{h-1}(S_h,A_h)$ if $h\lt T$, or with $G_{T-1:T}\doteq R_T$ if $h=T$. Then we use the off-policy $n$-step Sarsa update \eqref{eq:nsti.1}, which produces the pseudocode below.</p><figure><img src=/images/td-learning/n-step-q-sigma.png alt="n-step Q(sigma)" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018</span>.</p><p>[2] <span id=td-convergence>Richard S. Sutton. <a href=https://doi.org/10.1007/BF00115009>Learning to predict by the methods of temporal differences</a>. Mach Learn 3, 9–44, 1988.</span></p><p>[3] <span id=q-learning-watkins>Chris Watkins. <a href=https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards>Learning from Delayed Rewards</a>. PhD Thesis, 1989.</span></p><p>[4] Hado Hasselt. <a href=https://papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html>Double Q-learning</a>. NIPS 2010.</p><p>[5] Shangtong Zhang. <a href=https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>Reinforcement Learning: An Introduction implementation</a>. Github.</p><p>[6] <span id=random_walk>Singh, S.P., Sutton, R.S. <a href=https://doi.org/10.1007/BF00114726>Reinforcement learning with replacing eligibility traces</a>. Mach Learn 22, 123–158, 1996.</span></p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>It is a special case of <a href=#n-step-td>n-step TD</a> and TD($\lambda$).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Bootstrapping is to update estimates of the value functions of states based on estimates of value functions of other states.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>For the definition of Importance Sampling method, you can read more in this <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/#is>section</a>.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/td-learning/>td-learning</a></li><li><a href=https://trunghng.github.io/tags/importance-sampling/>importance-sampling</a></li><li><a href=https://trunghng.github.io/tags/q-learning/>q-learning</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/><span class=title>« Prev</span><br><span>Function Approximation</span></a>
<a class=next href=https://trunghng.github.io/posts/optimization/cvx-sets-funcs/><span class=title>Next »</span><br><span>Convex sets, convex functions</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on twitter" href="https://twitter.com/intent/tweet/?text=Temporal-Difference%20Learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f&hashtags=reinforcement-learning%2ctd-learning%2cimportance-sampling%2cq-learning%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f&title=Temporal-Difference%20Learning&summary=Temporal-Difference%20Learning&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f&title=Temporal-Difference%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on whatsapp" href="https://api.whatsapp.com/send?text=Temporal-Difference%20Learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Temporal-Difference Learning on telegram" href="https://telegram.me/share/url?text=Temporal-Difference%20Learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftd-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>