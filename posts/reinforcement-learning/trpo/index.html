<!doctype html><html lang=en dir=auto><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")</script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Trust Region Policy Optimization | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="deep-reinforcement-learning,policy-gradient,model-free,my-rl"><meta name=description content="
Notes on policy optimization using trust region method.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/trpo/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://trunghng.github.io/posts/reinforcement-learning/trpo/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")}</script><meta property="og:title" content="Trust Region Policy Optimization"><meta property="og:description" content="
Notes on policy optimization using trust region method.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/trpo/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-23T15:26:00+07:00"><meta property="article:modified_time" content="2022-11-23T15:26:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Trust Region Policy Optimization"><meta name=twitter:description content="
Notes on policy optimization using trust region method.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Trust Region Policy Optimization","item":"https://trunghng.github.io/posts/reinforcement-learning/trpo/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Trust Region Policy Optimization","name":"Trust Region Policy Optimization","description":" Notes on policy optimization using trust region method.\n","keywords":["deep-reinforcement-learning","policy-gradient","model-free","my-rl"],"articleBody":" Notes on policy optimization using trust region method.\nPreliminaries Markov Decision Processes An infinite-horizon discounted Markov Decision Process (MDP) is defined as the tuple $(\\mathcal{S},\\mathcal{A},P,r,\\rho_0,\\gamma)$, where\n$\\mathcal{S}$ is a finite set of states, or state space. $\\mathcal{A}$ is a finite set of actions, or action space. $P:\\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{S}\\to\\mathbb{R}$ is the transition probability distribution, i.e. $P(s,a,s’)=P(s’\\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$. $r:\\mathcal{S}\\times\\mathcal{A}\\to\\mathbb{R}$ is the reward function. $\\rho_0:\\mathcal{S}\\to\\mathbb{R}$ is the distribution of the initial state $s_0$. $\\gamma\\in(0,1)$ is the discount factor. A policy, denoted $\\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to\\{0,1\\}$ (or $\\pi:\\mathcal{S}\\to\\mathcal{A}$) or stochastic $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$. Here, we consider the stochastic policy only.\nWe continue by letting $\\eta(\\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\\pi$ thereafter \\begin{equation} \\eta(\\pi)=\\mathbb{E}_{s_0,a_0,\\ldots}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right], \\end{equation} where \\begin{equation} s_0\\sim\\rho_0(s_0),\\hspace{1cm}a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t) \\end{equation} For a policy $\\pi$, the state value function, denoted as $V_\\pi$, of a state $s\\in\\mathcal{S}$ measures how good it is for the agent to be in $s$, and the action value function, referred as $Q_\\pi$, of a state-action pair $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as \\begin{align} V_\\pi(s_t)\u0026=\\mathbb{E}_{a_t,s_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\\\ Q_\\pi(s_t,a_t)\u0026=\\mathbb{E}_{s_{t+1},a_{t+1},\\ldots}\\left[\\sum_{k=0}^{\\infty}\\gamma^k r(s_{t+k},a_{t+k})\\right], \\end{align} where \\begin{equation} a_t\\sim\\pi(a_t\\vert s_t),\\hspace{1cm}s_{t+1}\\sim P(s_{t+1}\\vert s_t,a_t)\\hspace{1cm}t\\geq 0 \\end{equation} Along with these value functions, we will also define the advantage function for $\\pi$, denoted $A_\\pi$, given as \\begin{equation} A_\\pi(s_t,a_t)=Q_\\pi(s_t,a_t)-V_\\pi(s_t) \\end{equation}\nCoupling \u0026 Total variation distance Consider two probability measures $\\mu$ and $\\nu$ on a probability space $(\\Omega,\\mathcal{F},P)$. One refers a coupling of $\\mu$ and $\\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\\mu$ and $\\nu$.\nSpecifically, if $p$ is a joint distribution of $X,Y$ on $\\Omega$, then it implies that \\begin{align} \\sum_{y\\in\\Omega}p(x,y)\u0026=\\sum_{y\\in\\Omega}P(X=x,Y=y)=P(X=x)=\\mu(x) \\\\ \\sum_{x\\in\\Omega}p(x,y)\u0026=\\sum_{x\\in\\Omega}P(X=x,Y=y)=P(Y=y)=\\nu(y) \\end{align} For probability distributions $\\mu$ and $\\nu$ on $\\Omega$ as above, the total variation distance between $\\mu$ and $\\nu$, denoted $\\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}$, is defined by \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}\\doteq\\sup_{A\\subset\\Omega}\\big\\vert\\mu(A)-\\nu(A)\\big\\vert \\end{equation} Proposition 1\nLet $\\mu$ and $\\nu$ be probability distributions on $\\Omega$, we then have \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\frac{1}{2}\\sum_{x\\in\\Omega}\\big\\vert\\mu(x)-\\nu(x)\\big\\vert \\end{equation} Proof\nLet $B=\\{x:\\mu(x)\\geq\\nu(x)\\}$ and let $A\\subset\\Omega$. We have \\begin{align} \\mu(A)-\\nu(A)\u0026=\\mu(A\\cap B)+\\mu(A\\cap B^c)-\\nu(A\\cap B)-\\nu(A\\cap B^c) \\\\ \u0026\\leq\\mu(A\\cap B)-\\nu(A\\cap B) \\\\ \u0026\\leq\\mu(B)-\\nu(B) \\end{align} Analogously, we also have \\begin{equation} \\nu(A)-\\mu(A)\\leq\\nu(B^c)-\\mu(B^c) \\end{equation} Hence, combining these results gives us \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\frac{1}{2}\\left(\\mu(B)-\\nu(B)+\\nu(B^c)-\\mu(B^c)\\right)=\\frac{1}{2}\\sum_{x\\in\\Omega}\\big\\vert\\mu(x)-\\nu(x)\\big\\vert \\end{equation} This proof also implies that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\sum_{x\\in\\Omega;,\\mu(x)\\geq\\nu(x)}\\mu(x)-\\nu(x) \\end{equation} Proposition 2\nLet $\\mu$ and $\\nu$ be two probability measures defined in a probability space $\\Omega$, we then have that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\inf_{(X,Y)\\text{ coupling of }\\mu,\\nu}P(X\\neq Y) \\end{equation} Proof\nFor any $A\\subset\\Omega$ and for any coupling $(X,Y)$ of $\\mu$ and $\\nu$ we have \\begin{align} \\mu(A)-\\nu(A)\u0026=P(X\\in A)-P(Y\\in A) \\\\ \u0026=P(X\\in A,Y\\notin A)+P(X\\in A,Y\\in A)-P(Y\\in A) \\\\ \u0026\\leq P(X\\in A,Y\\notin A) \\\\ \u0026\\leq P(X\\neq Y), \\end{align} which implies that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=\\sup_{A’\\subset\\Omega}\\big\\vert\\mu(A’)-\\nu(A’)\\big\\vert\\leq P(X\\neq Y)\\leq\\inf_{(X,Y)\\text{ coupling of }\\mu,\\nu}P(X\\neq Y) \\end{equation} Thus, it suffices to construct a coupling $(X,Y)$ of $\\mu$ and $\\nu$ such that \\begin{equation} \\big\\Vert\\mu-\\nu\\big\\Vert_\\text{TV}=P(X\\neq Y) \\end{equation}\nPolicy improvement We begin by proving an identity that expresses the expected return $\\eta(\\tilde{\\pi})$ of a policy $\\tilde{\\pi}$ in terms of the advantage over another policy $\\pi$, accumulated over time steps.\nLemma 3\nGiven two policies $\\pi,\\tilde{\\pi}$, we have \\begin{equation} \\eta(\\tilde{\\pi})=\\eta(\\pi)+\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t A_\\pi(s_t,a_t)\\right]\\label{eq:pi.1} \\end{equation} Proof\nBy definition of advantage function $A_\\pi$ of policy $\\pi$, we have \\begin{align} \\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t A_\\pi(s_t,a_t)\\right]\u0026=\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\left(Q_\\pi(s_t,a_t)-V_\\pi(s_t)\\right)\\right] \\\\ \u0026=\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t\\big(r(s_t,a_t)+\\gamma V_\\pi(s_{t+1})-V_\\pi(s_t)\\big)\\right] \\\\ \u0026=\\mathbb{E}_{\\tilde{\\pi}}\\left[-V_\\pi(s_0)+\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right] \\\\ \u0026=-\\mathbb{E}_{s_0}\\big[V_\\pi(s_0)\\big]+\\mathbb{E}_{\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^t r(s_t,a_t)\\right] \\\\ \u0026=-\\eta(\\pi)+\\eta(\\tilde{\\pi}), \\end{align} where in the third step, since $\\gamma\\in(0,1)$ as $t\\to\\infty$, we have that $\\gamma^t V_\\pi(s_{t+1})\\to 0$.\nLet $\\rho_\\pi$ be the unnormalized discounted visitation frequencies for state $s$: \\begin{equation} \\rho_\\pi(s)\\doteq P(s_0=s)+\\gamma P(s_1=s)+\\gamma^2 P(s_2=s)+\\ldots \\end{equation} where $s_0\\sim\\rho_0$ and the actions are chosen according to $\\pi$. This allows us to rewrite \\eqref{eq:pi.1} as \\begin{align} \\eta(\\tilde{\\pi})\u0026=\\eta(\\pi)+\\sum_{t=0}^{\\infty}\\sum_{s}P(s_t=s\\vert\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a\\vert s)\\gamma^t A_\\pi(s,a) \\\\ \u0026=\\eta(\\pi)+\\sum_{s}\\sum_{t=0}^{\\infty}\\gamma^t P(s_t=s\\vert\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a) \\\\ \u0026=\\eta(\\pi)+\\sum_{s}\\rho_\\tilde{\\pi}(s)\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\label{eq:pi.2} \\end{align} This result implies that any policy update $\\pi\\to\\tilde{\\pi}$ that has a nonnegative expected advantage at every state $s$, i.e. $\\sum_{a}\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\geq 0$, is guaranteed to make an improvement on $\\eta$ (or unchanged in case the expected advantage take the value of zero for every $s$). By letting $\\tilde{\\pi}$ be the deterministic policy that \\begin{equation} \\tilde{\\pi}(s)=\\underset{a}{\\text{argmax}}\\hspace{0.1cm}A_\\pi(s,a), \\end{equation} we obtain the policy improvement result used in policy iteration.\nHowever, there are cases when \\eqref{eq:pi.2} is difficult to be optimized, especially when the expected advantage is negative, i.e. $\\sum_a\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)$, due to estimation and approximation error in the approximate setting. We instead consider a local approximation to $\\eta$: \\begin{equation} L_\\pi(\\tilde{\\pi})=\\eta(\\pi)+\\sum_s\\rho_\\pi(s)\\sum_a\\tilde{\\pi}(a\\vert s)A_\\pi(s,a)\\label{eq:pi.5} \\end{equation}\nIf $\\pi$ is a policy parameterized by $\\theta$, in which $\\pi_\\theta(a\\vert s)$ s differentiable w.r.t $\\theta$, we then have for any parameter value $\\theta_0$ \\begin{align} L_{\\pi_{\\theta_0}}(\\pi_{\\theta_0})\u0026=\\eta(\\pi_{\\theta_0}) \\\\ \\nabla_\\theta L_{\\pi_{\\theta_0}}(\\pi_\\theta)\\big\\vert_{\\theta=\\theta_0}\u0026=\\nabla_\\theta\\eta(\\pi_\\theta)\\big\\vert_{\\theta=\\theta_0},\\label{eq:pi.6} \\end{align} which suggests that a sufficiently small step $\\pi_{\\theta_0}\\to\\tilde{\\pi}$ that leads to an improvement on $L_{\\pi_{\\theta_\\text{old}}}$ will also make an improvement on $\\eta$.\nTo measure the improvement on updating $\\pi_\\text{old}\\to\\pi_\\text{new}$, we choose the total variance distance metric, as defined above with an observation that each policy $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0,1]$ can be viewed as a distribution function defined on $\\mathcal{S}\\times\\mathcal{A}$. Thus, those results and definitions mentioned above for probability measures $\\mu$ and $\\nu$ defined on $\\Omega$ can also be applied to policies $\\pi$ and $\\tilde{\\pi}$ specified on $\\mathcal{S}\\times\\mathcal{A}$.\nIn addition, we need to define some notations:\nLet \\begin{equation} \\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_{\\text{TV}}^{\\text{max}}\\doteq\\max_s\\big\\Vert\\pi(\\cdot\\vert s)-\\tilde{\\pi}(\\cdot\\vert s)\\big\\Vert_\\text{TV} \\end{equation} A policy pair $(\\pi,\\tilde{\\pi})$ is referred as $\\alpha$-coupled if it defines a joint distribution $(a,\\tilde{a})\\vert s$ such that \\begin{equation} P(a\\neq\\tilde{a}\\vert s)\\leq\\alpha,\\hspace{1cm}\\forall s \\end{equation} $\\pi$ and $\\tilde{\\pi}$ will respectively denote the marginal distributions of $a$ and $\\tilde{a}$.\nProposition 4\nLet $(\\pi,\\tilde{\\pi})$ be $\\alpha$-coupled policy pair, for all $s$, we have \\begin{equation} \\big\\vert\\bar{A}(s)\\big\\vert\\leq 2\\alpha\\max_{s,\\tilde{a}}\\big\\vert A_\\pi(s,\\tilde{a})\\big\\vert, \\end{equation} where $\\bar{A}(s)$ is the expected advantage of $\\tilde{\\pi}$ over $\\pi$ at state $s$, given as \\begin{equation} \\bar{A}(s)=\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})\\big] \\end{equation} Proof\nBy definition of the advantage function, it is easily noticed that $\\mathbb{E}_{a\\sim\\pi}\\big[A_\\pi(s,a)\\big]=0$, which lets us obtain \\begin{align} \\bar{A}(s)\u0026=\\mathbb{E}_{\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})\\big] \\\\ \u0026=\\mathbb{E}_{a\\sim\\pi,\\tilde{a}\\sim\\tilde{\\pi}}\\big[A_\\pi(s,\\tilde{a})-A_\\pi(s,a)\\big] \\\\ \u0026=P(a\\neq\\tilde{a}\\vert s)\\mathbb{E}_{a\\sim\\pi,\\tilde{a}\\sim\\tilde{\\pi}\\vert a\\neq\\tilde{a}}\\big[A_\\pi(s,\\tilde{a})-A_\\pi(s,a)\\big], \\end{align} which by definition of $\\alpha$-coupling implies that \\begin{equation} \\big\\vert\\bar{A}(s)\\big\\vert\\leq\\alpha\\cdot 2\\max_{s,\\tilde{a}}\\big\\vert A_\\pi(s,\\tilde{a})\\big\\vert \\end{equation} Theorem 5\nLet $\\alpha=\\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_\\text{TV}^\\text{max}$. The following holds \\begin{equation} \\eta(\\tilde{\\pi})\\geq L_\\pi(\\tilde{\\pi})-\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^2, \\end{equation} where \\begin{equation} \\epsilon=\\max_{s,a}\\big\\vert A_\\pi(s,a)\\big\\vert \\end{equation} Proof\nOn the other hand, by Pinsker’s inequality, which bounds the total variation distance in terms of the Kullback-Leibler divergence, denoted $D_\\text{KL}$, we have that \\begin{equation} \\big\\Vert\\pi-\\tilde{\\pi}\\big\\Vert_\\text{TV}^2\\leq\\frac{1}{2}D_\\text{KL}(\\pi\\Vert\\tilde{\\pi})\\leq D_\\text{KL}(\\pi\\Vert\\tilde{\\pi}),\\label{eq:pi.3} \\end{equation} since $D_\\text{KL}(\\cdot\\Vert\\cdot)\\geq 0$. Thus, let \\begin{equation} D_\\text{KL}^\\text{max}(\\pi,\\tilde{\\pi})\\doteq\\max_s D_\\text{KL}\\big(\\pi(\\cdot\\vert s)\\Vert\\tilde{\\pi}(\\cdot\\vert s)\\big), \\end{equation} with the result \\eqref{eq:pi.3} and by Theorem 5, we have \\begin{equation} \\eta(\\tilde{\\pi})\\geq L_\\pi(\\tilde{\\pi})-CD_\\text{KL}^\\text{max}(\\pi,\\tilde{\\pi}),\\label{eq:pi.4} \\end{equation} where \\begin{equation} C=\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2} \\end{equation} The policy improvement bound \\eqref{eq:pi.4} allows us to specify a policy iteration, as given in the following pseudocode\nIt is worth noticing that \\eqref{eq:pi.4} allows the policy iteration above to guarantee to generating a sequence of non-decreasing expected returns \\begin{equation} \\eta(\\pi_0)\\leq\\eta(\\pi_1)\\leq\\eta(\\pi_2)\\leq\\ldots \\end{equation} To see this, let \\begin{equation} M_i(\\pi)\\doteq L_{\\pi_i}(\\pi)-CD_\\text{KL}^\\text{max}(\\pi_i,\\pi), \\end{equation} by \\eqref{eq:pi.4}, we then have \\begin{equation} \\eta(\\pi_{i+1})\\geq M_i(\\pi_{i+1}), \\end{equation} which implies that \\begin{equation} \\eta(\\pi_{i+1})-\\eta(\\pi_i)=\\eta(\\pi_{i+1})-M_i(\\pi_i)\\geq M_i(\\pi_{i+1})-M_i(\\pi_i) \\end{equation} Parameterized Policy Optimization by Trust Region We now consider the policy optimization problem in which the policy is parameterized by $\\theta$.\nWe begin by simplifying notations. In particular, let $\\eta(\\theta)\\doteq\\eta(\\pi_\\theta)$, let $L_\\theta(\\tilde{\\theta})\\doteq L_{\\pi_\\theta}(\\pi_\\tilde{\\theta})$ and $D_\\text{KL}(\\theta\\Vert\\tilde{\\theta})\\doteq D_\\text{KL}(\\pi_\\theta\\Vert\\pi_\\tilde{\\theta})$, which allows us to represent \\begin{equation} D_\\text{KL}^\\text{max}(\\theta,\\tilde{\\theta})\\doteq D_\\text{KL}^\\text{max}(\\pi_\\theta,\\pi_\\tilde{\\theta})=\\max_s D_\\text{KL}\\big(\\pi_\\theta(\\cdot\\vert s)\\Vert\\pi_\\tilde{\\theta}(\\cdot\\vert s)\\big) \\end{equation} Also let $\\theta_\\text{old}$ denote the previous policy parameters that we want to improve. Hence, by the previous section, we have \\begin{equation} \\eta(\\theta)\\geq L_{\\theta_\\text{old}}(\\theta)-CD_\\text{KL}^\\text{max}(\\theta_\\text{old},\\theta), \\end{equation} where the equality holds at $\\theta=\\theta_\\text{old}$. This means, we get a guaranteed improvement to the true objective function $\\eta$ by solving the following optimization problem \\begin{equation} \\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\big[L_{\\theta_\\text{old}}(\\theta)-CD_\\text{KL}^\\text{max}(\\theta_\\text{old},\\theta)\\big] \\end{equation} To speed up the algorithm, we make some robust modification. Specifically, we instead solve a trust region problem: \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026\\hspace{0.2cm}L_{\\theta_\\text{old}}(\\theta)\\nonumber \\\\ \\text{s.t.}\u0026\\hspace{0.2cm}\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\leq\\delta,\\label{eq:ppo.1} \\end{align} where $\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}$ is the average KL divergence, given as \\begin{equation} \\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big] \\end{equation} Let us pay attention to our objective function, $L_{\\theta_\\text{old}}(\\theta)$, for a while. By the definition of $L$, given in \\eqref{eq:pi.5}, combined with using an importance sampling estimator, we can rewrite the objective function of \\eqref{eq:ppo.1} as \\begin{align} L_{\\theta_\\text{old}}(\\theta)\u0026=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\sum_a\\pi_\\theta(a\\vert s)A_{\\theta_\\text{old}}(s,a) \\\\ \u0026=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{align} where $A_{\\theta_\\text{old}}\\doteq A_{\\pi_{\\theta_\\text{old}}}$; and $q$ represents the sampling distribution. The trust region problem now is given as \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026\\hspace{0.2cm}\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\\\ \\text{s.t.}\u0026\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big]\\leq\\delta \\end{align} which is thus equivalent to12 \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\\\ \\text{s.t.}\u0026\\hspace{0.2cm}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\Big]\\leq\\delta\\label{eq:ppo.2} \\end{align}\nSolving the optimization problem Let us take a closer look on how to solve this trust region constrained optimization problem. We begin by letting \\begin{equation} \\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\left[\\frac{\\pi_\\theta(a\\vert s)}{\\pi_{\\theta_\\text{old}}(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{equation} Consider Taylor expansion of the objective function $\\mathcal{L}_{\\theta_\\text{old}}(\\theta)$ about $\\theta=\\theta_\\text{old}$ to the first order, we thus can linearly approximate the objective function by the policy gradient, $\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})$, as \\begin{align} \\mathcal{L}_{\\theta_\\text{old}}(\\theta)\u0026\\approx\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]+(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026\\overset{\\text{(i)}}{=}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\mathcal{L}_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026\\overset{\\text{(ii)}}{=}(\\theta-\\theta_\\text{old})^\\text{T}\\left[\\frac{1}{1-\\gamma}\\nabla_\\theta L_{\\theta_\\text{old}}(\\theta)\\big\\vert_{\\theta=\\theta_\\text{old}}\\right] \\\\ \u0026\\overset{\\text{(iii)}}{=}\\frac{1}{1-\\gamma}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026\\underset{\\max_\\theta}{\\propto}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}},\\label{eq:st.1} \\end{align} where\nThis step is due to definition of advantage function for a policy $\\pi$, we have that $\\mathbb{E}_{a\\sim\\pi}\\big[A_\\pi(s,a)\\big]=0$, which implies that \\begin{align} \\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]\u0026=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\mathbb{E}_{a\\sim\\pi_{\\theta_\\text{old}}}\\big[A_{\\theta_\\text{old}}(s,a)\\big]\\Big] \\\\ \u0026=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\big[0\\big]=0 \\end{align} This step uses the same logic as we have used in \\eqref{eq:fn.1}. This step is due to \\eqref{eq:pi.6}. To get an approximation of the constraint, we fist consider the Taylor expansion of the KL divergence $D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)$ about $\\theta=\\theta_\\text{old}$ to the second order, which, given a state $s$, gives us a quadratic approximation: \\begin{align} \u0026\\hspace{-0.7cm}D_\\text{KL}\\big(\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)\\Vert\\pi_\\theta(\\cdot\\vert s)\\big)\\nonumber \\\\ \u0026=\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)-\\log\\pi_\\theta(\\cdot\\vert s)\\Big] \\\\ \u0026\\approx\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Bigg[\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)-\\Big(\\log\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)+(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nonumber \\\\ \u0026\\hspace{2cm}+\\left.\\frac{1}{2}(\\theta_\\text{old}-\\theta)^\\text{T}\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}(\\theta_\\text{old}-\\theta)\\right)\\Bigg] \\\\ \u0026\\overset{\\text{(i)}}{=}-\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\left[\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}(\\theta-\\theta_\\text{old})\\right] \\\\ \u0026\\overset{\\text{(ii)}}{=}\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right),\\label{eq:st.2} \\end{align} where\nBy chain rule, we have \\begin{align} \\hspace{-0.7cm}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\left[(\\theta_\\text{old}-\\theta)^\\text{T}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\right]\u0026=\\sum_s\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)(\\theta_\\text{old}-\\theta)^\\text{T}\\frac{\\nabla_\\theta\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}}{\\pi_{\\theta_\\text{old}}(\\cdot\\vert s)} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\sum_s\\nabla_\\theta\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\left.\\left(\\nabla_\\theta\\sum_s\\pi_\\theta(\\cdot\\vert s)\\right)\\right\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}(\\nabla_\\theta 1)\\big\\vert_{\\theta=\\theta_\\text{old}} \\\\ \u0026=(\\theta_\\text{old}-\\theta)^\\text{T}\\mathbf{0}=0 \\end{align} This step goes with same logic as we have used in Natural evolution strategies, which let us claim that \\begin{equation} -\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta^2\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\Big]=\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big] \\end{equation} Given the Taylor series approximation \\eqref{eq:st.2}, we can locally approximate $\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)$ as \\begin{align} \u0026\\hspace{-0.5cm}\\overline{D}_\\text{KL}^{\\rho_{\\theta_\\text{old}}}(\\theta_\\text{old},\\theta)\\nonumber \\\\ \u0026\\approx\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{\\pi_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right)\\right] \\\\ \u0026=\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big]\\left(\\theta-\\theta_\\text{old}\\right) \\\\ \u0026=\\frac{1}{2}(\\theta-\\theta_\\text{old})^\\text{T}\\mathbf{F}(\\theta-\\theta_\\text{old}),\\label{eq:st.3} \\end{align} where the matrix \\begin{equation} \\mathbf{F}\\doteq\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\Big[\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}\\nabla_\\theta\\log\\pi_\\theta(\\cdot\\vert s)\\big\\vert_{\\theta=\\theta_\\text{old}}^\\text{T}\\Big] \\end{equation} is referred as the Fisher information matrix, which, worthy remarking, is symmetric.\nAs acquired results \\eqref{eq:st.1} and \\eqref{eq:st.3}, we yield an approximate problem \\begin{align} \\underset{\\theta}{\\text{maximize}}\u0026\\hspace{0.2cm}\\Delta\\theta^\\text{T}g=\\tilde{\\mathcal{L}}(\\theta)\\nonumber \\\\ \\text{s.t.}\u0026\\hspace{0.2cm}\\frac{1}{2}\\Delta\\theta^\\text{T}\\mathbf{F}\\Delta\\theta\\leq\\delta,\\label{eq:st.4} \\end{align} where we have let $\\Delta\\theta\\doteq\\theta-\\theta_\\text{old}$ and $g\\doteq\\nabla_\\theta\\eta(\\pi_{\\theta_\\text{old}})\\big\\vert_{\\theta=\\theta_\\text{old}}$ denote the policy gradient to simplify our notations.\nNatural policy gradient Consider the problem \\eqref{eq:st.4}, we have the Lagrangian3 associated with the our constrained optimization problem is given by \\begin{equation} \\bar{\\mathcal{L}}(\\Delta\\theta,\\lambda)=-\\Delta\\theta^\\text{T}g+\\lambda\\left(\\frac{1}{2}\\Delta\\theta^\\text{T}\\mathbf{F}\\Delta\\theta-\\delta\\right) \\end{equation} which can be minimized w.r.t $\\Delta\\theta$ by taking the gradient of the Lagrangian w.r.t $\\Delta\\theta$ \\begin{equation} \\nabla_{\\Delta\\theta}\\overline{\\mathcal{L}}(\\Delta\\theta,\\lambda)=-g+\\lambda\\mathbf{F}\\Delta\\theta, \\end{equation} and setting this gradient to zero, which yields \\begin{equation} \\Delta\\theta=\\frac{1}{\\lambda}\\mathbf{F}^{-1}g\\label{eq:np.1} \\end{equation} The dual function4 then is given by \\begin{align} \\overline{g}(\\lambda)\u0026=-\\frac{1}{\\lambda}g^\\text{T}\\mathbf{F}^{-1}g+\\frac{1}{2\\lambda}g^\\text{T}\\mathbf{F}^{-1}\\mathbf{F}\\mathbf{F}^{-1}g-\\lambda\\delta \\\\ \u0026=-\\frac{1}{2\\lambda}g^\\text{T}\\mathbf{F}^{-1}g-\\lambda\\delta, \\end{align} Letting the gradient of dual function w.r.t $\\lambda$ \\begin{equation} \\nabla_\\lambda\\overline{g}(\\lambda)=\\frac{1}{2}g^\\text{T}\\mathbf{F}^{-1}g\\cdot\\frac{1}{\\lambda^2}-\\delta \\end{equation} be zero and solving for $\\lambda$ gives us the Lagrange multiplier that maximizes $\\overline{g}$, which is \\begin{equation} \\lambda=\\sqrt{\\frac{g^\\text{T}\\mathbf{F}^{-1}g}{2\\delta}} \\end{equation} The vector $\\Delta\\theta$ given in \\eqref{eq:np.1} that solves the optimization problem \\eqref{eq:st.4} defines the a search direction, which is the direction of the natural policy gradient, i.e. $\\tilde{\\nabla}_\\theta\\tilde{\\mathcal{L}}(\\theta)=\\mathbf{F}^{-1}g$.\nHence, using this gradient to iteratively update $\\theta$ gives us \\begin{equation} \\theta_{k+1}:=\\theta_k+\\lambda^{-1}\\tilde{\\nabla}_\\theta\\tilde{\\mathcal{L}}(\\theta)=\\theta_k+\\sqrt{\\frac{2\\delta}{g^\\text{T}\\mathbf{F}^{-1}g}}\\mathbf{F}^{-1}g\\label{eq:np.2} \\end{equation}\nLine search A problem with the above algorithm is that there exist approximation errors because of the Taylor expansion we have used. This consequently might not give us an improvement of the objective or the updated $\\pi_\\theta$ may not satisfy the KL constraint due to taking large steps.\nTo overcome this, we use a line search by adding an exponential decay to the update rule \\eqref{eq:np.2} that \\begin{equation} \\theta_{k+1}:=\\theta_k+\\alpha^j\\sqrt{\\frac{2\\delta}{g^\\text{T}\\mathbf{F}^{-1}g}}\\mathbf{F}^{-1}g,\\label{eq:ls.1} \\end{equation} where $\\alpha\\in(0,1)$ is the decay coefficient and $j$ is the smallest nonnegative integer that make an improvement on the objective, while let $\\pi_{\\theta_{k+1}}$ satisfy the KL constraint as well.\nCompute $\\mathbf{F}^{-1}g$ Since both the step size and direction of the update \\eqref{eq:ls.1} relate to $\\mathbf{F}^{-1}g$, it is then necessary to take into account the computation of this product.\nRather than computing the inverse $\\mathbf{F}^{-1}$ of the Fisher information matrix, then multiply it with the gradient vector $g$ to obtain the natural gradient $\\mathbf{F}^{-1}g$, we find a vector $x$ such that \\begin{equation} \\mathbf{F}x=g,\\label{eq:cfig.1} \\end{equation} which implies that $x=\\mathbf{F}^{-1}g$.\nThe problem now remains to solve the linear equation \\eqref{eq:cfig.1}, which can be approximately solved by Conjugate gradient method with a predefined number of iterations.\nSampled-based estimation The objective and constraint functions of \\eqref{eq:ppo.2} can be approximated using Monte Carlo simulation. Following are two possible sampling approaches to construct the estimated objective and constraint functions.\nSingle path This sampling scheme has the following procedure\nSample $s_0\\sim\\rho_0$ to get a set of $m$ start states $\\mathcal{S}_0=\\{s_0^{(1)},\\ldots,s_0^{(m)}\\}$. For each $s_0^{(i)}\\in\\mathcal{S}_0$, generate a trajectory $\\tau^{(i)}=\\big(s_0^{(i)},a_0^{(i)},s_1^{(i)},a_1^{(i)},\\ldots,s_{T-1}^{(i)},a_{T-1}^{(i)},s_T^{(i)}\\big)$ by rolling out the policy $\\pi_{\\theta_\\text{old}}$ for $T$ steps. Thus $q(a^{(i)}\\vert s^{(i)})=\\pi_{\\theta_\\text{old}}(a^{(i)}\\vert s^{(i)})$. At each state-action pair $(s_t^{(i)},a_t^{(i)})$, compute the action-value function $Q_{\\theta_\\text{old}}(s,a)$ by taking the discounted sum of future rewards along $\\tau^{(i)}$. Vine This sampling approach follows the following process\nSample $s_0\\sim\\rho_0$ and simulate the policy $\\pi_{\\theta_i}$ to generate $m$ trajectories. Choose a rollout set, which is a subset $s_1,\\ldots,s_N$ of $N$ states along the trajectories. For each state $s_n$ with $1\\leq n\\leq N$, sample $K$ actions according to $a_{n,k}\\sim q(\\cdot\\vert s_n)$, where $q(\\cdot\\vert s_n)$ includes the support of $\\pi_{\\theta_i}(\\cdot\\vert s_n)$. For each action $a_{n,k}$, estimate $\\hat{Q}_{\\theta_i}(s_n,a_{n,k})$ by performing a rollout starting from $s_n$ and taking action $a_{n,k}$ Given the estimated action-value function, $\\hat{Q}_{\\theta_i}(s_n,a_{n,k})$, for each state-action pair $(s_n,a_{n,k})$, compute the estimator, $L_n(\\theta)$, of $L_{\\theta_\\text{old}}$ at state $s_n$ as: For small, finite action spaces, in which generating a rollout for every possible action from a given state is possible, thus \\begin{equation} L_n(\\theta)=\\sum_{k=1}^{K}\\pi_\\theta(a_k\\vert s_n)\\hat{Q}(s_n,a_k), \\end{equation} where $\\mathcal{A}=\\{a_1,\\ldots,a_K\\}$ is the action space. For large or continuous state spaces, use importance sampling \\begin{equation} L_n(\\theta)=\\frac{\\sum_{k=1}^{K}\\frac{\\pi_\\theta(a_{n,k}\\vert s_n)}{\\pi_{\\theta_\\text{old}}(a_{n,k}\\vert s_n)}\\hat{Q}(s_n,a_{n,k})}{\\sum_{k=1}^{K}\\frac{\\pi_\\theta(a_{n,k}\\vert s_n)}{\\pi_{\\theta_\\text{old}}(a_{n,k}\\vert s_n)}}, \\end{equation} assuming that $K$ actions $a_{n,1},\\ldots,a_{n,K}$ are performed from state $s_n$. Average over $s_n\\sim\\rho(\\pi)$ to obtain an estimator for $L_{\\theta_\\text{old}}$, as well the policy gradient. References [1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. Trust Region Policy Optimization. ICML'15, pp 1889–1897, 2015.\n[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. Markov chains and mixing times. American Mathematical Society, 2009.\n[3] Sham Kakade, John Langford. Approximately optimal approximate reinforcement learning. ICML'2, pp. 267–274, 2002.\n[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. Proximal Policy Optimization Algorithms. arXiv:1707.06347, 2017.\n[5] Stephen Boyd \u0026 Lieven Vandenberghe. Convex Optimization. Cambridge UP, 2004.\n[6] Sham Kakade. A Natural Policy Gradient. NIPS 2001.\nFootnotes To be more specific, by definition of the advantage, i.e. $A_{\\theta_\\text{old}}(s,a)=Q_{\\theta_\\text{old}}(s,a)-V_{\\theta_\\text{old}}(s)$, we have: \\begin{align} \\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\u0026=\\mathbb{E}_{s\\sim\\rho_{\\text{old}}}\\left[\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\right]\\nonumber \\\\ \u0026\\underset{\\max_\\theta}{\\propto}\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\right]\\label{eq:fn.1} \\\\ \u0026=\\sum_s\\rho_{\\theta_\\text{old}}(s)\\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]\\nonumber \\end{align} where we have used the notation \\begin{equation} \\text{LHS}\\underset{\\max_\\theta}{\\propto}\\text{RHS}\\nonumber \\end{equation} to denote that the problem $\\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\text{LHS}$ is equivalent to $\\underset{\\theta}{\\text{maximize}}\\hspace{0.2cm}\\text{RHS}$. Also, the second step comes from definition of $\\rho_\\pi$, i.e. for $s_0\\sim\\rho_0$ and the actions are chosen according to $\\pi$, we have \\begin{equation*} \\rho_\\pi(s)=P(s_0=s)+\\gamma P(s_1=s)+\\gamma^2 P(s_2=s)+\\ldots, \\end{equation*} which implies that by summing across all $s$, we obtain \\begin{align*} \\sum_{s}\\rho_\\pi(s)\u0026=\\sum_s P(s_0=s)+\\gamma\\sum_s P(s_1=s)+\\gamma^2\\sum_s P(s_2=s)+\\ldots \\\\ \u0026=1+\\gamma+\\gamma^2+\\ldots \\\\ \u0026=\\frac{1}{1-\\gamma} \\end{align*} ↩︎\nIn the original TRPO paper, the authors used the state-action value function $Q_{\\theta_\\text{old}}$ rather than the advantage $A_{\\theta_\\text{old}}$ since by definition, $A_{\\theta_\\text{old}}(s,a)=Q_{\\theta_\\text{old}}(s,a)-V_{\\theta_\\text{old}}(s)$, which lets us obtain \\begin{align*} \u0026\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}Q_{\\theta_\\text{old}}(s,a)\\right] \\\\ \u0026=\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}\\big(A_{\\theta_\\text{old}}(s,a)+V_{\\theta_\\text{old}}(s)\\big)\\right] \\\\ \u0026=\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]+\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\left[V_{\\theta_\\text{old}}(s)\\sum_{a}\\pi_\\theta(a\\vert s)\\right] \\\\ \u0026=\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right]+\\mathbb{E}_{s\\sim\\rho_{\\theta_\\text{old}}}\\big[V_{\\theta_\\text{old}}(s)\\big] \\\\ \u0026\\underset{\\max_\\theta}{\\propto}\\mathbb{E}_{s\\sim\\rho_{\\text{old}},a\\sim q}\\left[\\frac{\\pi_\\theta(a\\vert s)}{q(a\\vert s)}A_{\\theta_\\text{old}}(s,a)\\right] \\end{align*} ↩︎\nThe Lagrangian, $\\bar{\\mathcal{L}}$, here should not be confused with the objective function $\\mathcal{L}$ due to their notations. It was just a notation-abused problem, in which normally the Lagrangian is denoted as $\\mathcal{L}$, which has been already used. ↩︎\nThe dual function is usually denoted by $g$, which has unfortunately been taken by the policy gradient. Thus, we abuse the notation once again by representing it as $\\overline{g}$. ↩︎\n","wordCount":"2534","inLanguage":"en","datePublished":"2022-11-23T15:26:00+07:00","dateModified":"2022-11-23T15:26:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/trpo/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io/ accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Trust Region Policy Optimization</h1><div class=post-meta><span title='2022-11-23 15:26:00 +0700 +0700'>November 23, 2022</span>&nbsp;·&nbsp;12 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#preliminaries>Preliminaries</a><ul><li><a href=#mdp>Markov Decision Processes</a></li><li><a href=#coupling-tvd>Coupling & Total variation distance</a></li></ul></li><li><a href=#policy-imp>Policy improvement</a></li><li><a href=#param-policy-opt>Parameterized Policy Optimization by Trust Region</a><ul><li><a href=#solve-tr>Solving the optimization problem</a><ul><li><a href=#ntr-pg>Natural policy gradient</a></li><li><a href=#line-search>Line search</a></li><li><a href=#compute-f-inv-g>Compute $\mathbf{F}^{-1}g$</a></li></ul></li></ul></li><li><a href=#sampled-bsd-est>Sampled-based estimation</a><ul><li><a href=#sgl>Single path</a></li><li><a href=#vine>Vine</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on policy optimization using trust region method.</p></blockquote><h2 id=preliminaries>Preliminaries<a hidden class=anchor aria-hidden=true href=#preliminaries>#</a></h2><h3 id=mdp>Markov Decision Processes<a hidden class=anchor aria-hidden=true href=#mdp>#</a></h3><p>An infinite-horizon discounted <strong>Markov Decision Process</strong> (<strong>MDP</strong>) is defined as the tuple $(\mathcal{S},\mathcal{A},P,r,\rho_0,\gamma)$, where</p><ul><li>$\mathcal{S}$ is a finite set of states, or <strong>state space</strong>.</li><li>$\mathcal{A}$ is a finite set of actions, or <strong>action space</strong>.</li><li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$ is the <strong>transition probability distribution</strong>, i.e. $P(s,a,s&rsquo;)=P(s&rsquo;\vert s,a)$ denotes the probability of transitioning to state $s&rsquo;$ when taking action $a$ from state $s$.</li><li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>.</li><li>$\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.</li><li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li></ul><p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$. Here, we consider the stochastic policy only.</p><p>We continue by letting $\eta(\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\pi$ thereafter
\begin{equation}
\eta(\pi)=\mathbb{E}_{s_0,a_0,\ldots}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right],
\end{equation}
where
\begin{equation}
s_0\sim\rho_0(s_0),\hspace{1cm}a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)
\end{equation}
For a policy $\pi$, the <strong>state value function</strong>, denoted as $V_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$, and the <strong>action value function</strong>, referred as $Q_\pi$, of a state-action pair $(s,a)\in\mathcal{S}\times\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as
\begin{align}
V_\pi(s_t)&=\mathbb{E}_{a_t,s_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right], \\ Q_\pi(s_t,a_t)&=\mathbb{E}_{s_{t+1},a_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right],
\end{align}
where
\begin{equation}
a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)\hspace{1cm}t\geq 0
\end{equation}
Along with these value functions, we will also define the <strong>advantage function</strong> for $\pi$, denoted $A_\pi$, given as
\begin{equation}
A_\pi(s_t,a_t)=Q_\pi(s_t,a_t)-V_\pi(s_t)
\end{equation}</p><h3 id=coupling-tvd>Coupling & Total variation distance<a hidden class=anchor aria-hidden=true href=#coupling-tvd>#</a></h3><p>Consider two probability measures $\mu$ and $\nu$ on a probability space $(\Omega,\mathcal{F},P)$. One refers a <strong>coupling</strong> of $\mu$ and $\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\mu$ and $\nu$.</p><p>Specifically, if $p$ is a joint distribution of $X,Y$ on $\Omega$, then it implies that
\begin{align}
\sum_{y\in\Omega}p(x,y)&=\sum_{y\in\Omega}P(X=x,Y=y)=P(X=x)=\mu(x) \\ \sum_{x\in\Omega}p(x,y)&=\sum_{x\in\Omega}P(X=x,Y=y)=P(Y=y)=\nu(y)
\end{align}
For probability distributions $\mu$ and $\nu$ on $\Omega$ as above, the <strong>total variation distance</strong> between $\mu$ and $\nu$, denoted $\big\Vert\mu-\nu\big\Vert_\text{TV}$, is defined by
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}\doteq\sup_{A\subset\Omega}\big\vert\mu(A)-\nu(A)\big\vert
\end{equation}
<strong>Proposition 1</strong><br>Let $\mu$ and $\nu$ be probability distributions on $\Omega$, we then have
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
<strong>Proof</strong><br>Let $B=\{x:\mu(x)\geq\nu(x)\}$ and let $A\subset\Omega$. We have
\begin{align}
\mu(A)-\nu(A)&=\mu(A\cap B)+\mu(A\cap B^c)-\nu(A\cap B)-\nu(A\cap B^c) \\ &\leq\mu(A\cap B)-\nu(A\cap B) \\ &\leq\mu(B)-\nu(B)
\end{align}
Analogously, we also have
\begin{equation}
\nu(A)-\mu(A)\leq\nu(B^c)-\mu(B^c)
\end{equation}
Hence, combining these results gives us
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\left(\mu(B)-\nu(B)+\nu(B^c)-\mu(B^c)\right)=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
This proof also implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sum_{x\in\Omega;,\mu(x)\geq\nu(x)}\mu(x)-\nu(x)
\end{equation}
<strong>Proposition 2</strong><br>Let $\mu$ and $\nu$ be two probability measures defined in a probability space $\Omega$, we then have that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
<strong>Proof</strong><br>For any $A\subset\Omega$ and for any coupling $(X,Y)$ of $\mu$ and $\nu$ we have
\begin{align}
\mu(A)-\nu(A)&=P(X\in A)-P(Y\in A) \\ &=P(X\in A,Y\notin A)+P(X\in A,Y\in A)-P(Y\in A) \\ &\leq P(X\in A,Y\notin A) \\ &\leq P(X\neq Y),
\end{align}
which implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sup_{A&rsquo;\subset\Omega}\big\vert\mu(A&rsquo;)-\nu(A&rsquo;)\big\vert\leq P(X\neq Y)\leq\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
Thus, it suffices to construct a coupling $(X,Y)$ of $\mu$ and $\nu$ such that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=P(X\neq Y)
\end{equation}</p><h2 id=policy-imp>Policy improvement<a hidden class=anchor aria-hidden=true href=#policy-imp>#</a></h2><p>We begin by proving an identity that expresses the expected return $\eta(\tilde{\pi})$ of a policy $\tilde{\pi}$ in terms of the advantage over another policy $\pi$, accumulated over time steps.</p><p><strong>Lemma 3</strong><br>Given two policies $\pi,\tilde{\pi}$, we have
\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]\label{eq:pi.1}
\end{equation}
<strong>Proof</strong><br>By definition of advantage function $A_\pi$ of policy $\pi$, we have
\begin{align}
\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]&=\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\left(Q_\pi(s_t,a_t)-V_\pi(s_t)\right)\right] \\ &=\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\big(r(s_t,a_t)+\gamma V_\pi(s_{t+1})-V_\pi(s_t)\big)\right] \\ &=\mathbb{E}_{\tilde{\pi}}\left[-V_\pi(s_0)+\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\ &=-\mathbb{E}_{s_0}\big[V_\pi(s_0)\big]+\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\ &=-\eta(\pi)+\eta(\tilde{\pi}),
\end{align}
where in the third step, since $\gamma\in(0,1)$ as $t\to\infty$, we have that $\gamma^t V_\pi(s_{t+1})\to 0$.</p><p>Let $\rho_\pi$ be the unnormalized discounted visitation frequencies for state $s$:
\begin{equation}
\rho_\pi(s)\doteq P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots
\end{equation}
where $s_0\sim\rho_0$ and the actions are chosen according to $\pi$. This allows us to rewrite \eqref{eq:pi.1} as
\begin{align}
\eta(\tilde{\pi})&=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)\gamma^t A_\pi(s,a) \\ &=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma^t P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a) \\ &=\eta(\pi)+\sum_{s}\rho_\tilde{\pi}(s)\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.2}
\end{align}
This result implies that any policy update $\pi\to\tilde{\pi}$ that has a nonnegative expected advantage at every state $s$, i.e. $\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\geq 0$, is guaranteed to make an improvement on $\eta$ (or unchanged in case the expected advantage take the value of zero for every $s$). By letting $\tilde{\pi}$ be the deterministic policy that
\begin{equation}
\tilde{\pi}(s)=\underset{a}{\text{argmax}}\hspace{0.1cm}A_\pi(s,a),
\end{equation}
we obtain the <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-improvement><strong>policy improvement</strong></a> result used in <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-iteration><strong>policy iteration</strong></a>.</p><p>However, there are cases when \eqref{eq:pi.2} is difficult to be optimized, especially when the expected advantage is negative, i.e. $\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)$, due to estimation and approximation error in the approximate setting. We instead consider a local approximation to $\eta$:
\begin{equation}
L_\pi(\tilde{\pi})=\eta(\pi)+\sum_s\rho_\pi(s)\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.5}
\end{equation}</p><p>If $\pi$ is a policy parameterized by $\theta$, in which $\pi_\theta(a\vert s)$ s differentiable w.r.t $\theta$, we then have for any parameter value $\theta_0$
\begin{align}
L_{\pi_{\theta_0}}(\pi_{\theta_0})&=\eta(\pi_{\theta_0}) \\ \nabla_\theta L_{\pi_{\theta_0}}(\pi_\theta)\big\vert_{\theta=\theta_0}&=\nabla_\theta\eta(\pi_\theta)\big\vert_{\theta=\theta_0},\label{eq:pi.6}
\end{align}
which suggests that a sufficiently small step $\pi_{\theta_0}\to\tilde{\pi}$ that leads to an improvement on $L_{\pi_{\theta_\text{old}}}$ will also make an improvement on $\eta$.</p><p>To measure the improvement on updating $\pi_\text{old}\to\pi_\text{new}$, we choose the total variance distance metric, as defined above with an observation that each policy $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$ can be viewed as a distribution function defined on $\mathcal{S}\times\mathcal{A}$. Thus, those results and definitions mentioned above for probability measures $\mu$ and $\nu$ defined on $\Omega$ can also be applied to policies $\pi$ and $\tilde{\pi}$ specified on $\mathcal{S}\times\mathcal{A}$.</p><p>In addition, we need to define some notations:</p><ul class=number-list><li>Let
\begin{equation}
\big\Vert\pi-\tilde{\pi}\big\Vert_{\text{TV}}^{\text{max}}\doteq\max_s\big\Vert\pi(\cdot\vert s)-\tilde{\pi}(\cdot\vert s)\big\Vert_\text{TV}
\end{equation}</li><li>A policy pair $(\pi,\tilde{\pi})$ is referred as <b>$\alpha$-coupled</b> if it defines a joint distribution $(a,\tilde{a})\vert s$ such that
\begin{equation}
P(a\neq\tilde{a}\vert s)\leq\alpha,\hspace{1cm}\forall s
\end{equation}
$\pi$ and $\tilde{\pi}$ will respectively denote the marginal distributions of $a$ and $\tilde{a}$.<br><br><b>Proposition 4</b><br>Let $(\pi,\tilde{\pi})$ be $\alpha$-coupled policy pair, for all $s$, we have
\begin{equation}
\big\vert\bar{A}(s)\big\vert\leq 2\alpha\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert,
\end{equation}
where $\bar{A}(s)$ is the expected advantage of $\tilde{\pi}$ over $\pi$ at state $s$, given as
\begin{equation}
\bar{A}(s)=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big]
\end{equation}
<b>Proof</b><br>By definition of the advantage function, it is easily noticed that $\mathbb{E}_{a\sim\pi}\big[A_\pi(s,a)\big]=0$, which lets us obtain
\begin{align}
\bar{A}(s)&=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big] \\ &=\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big] \\ &=P(a\neq\tilde{a}\vert s)\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}\vert a\neq\tilde{a}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big],
\end{align}
which by definition of $\alpha$-coupling implies that
\begin{equation}
\big\vert\bar{A}(s)\big\vert\leq\alpha\cdot 2\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert
\end{equation}</li></ul><p><strong>Theorem 5</strong><br>Let $\alpha=\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^\text{max}$. The following holds
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-\frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2,
\end{equation}
where
\begin{equation}
\epsilon=\max_{s,a}\big\vert A_\pi(s,a)\big\vert
\end{equation}
<strong>Proof</strong></p><p>On the other hand, by <strong>Pinsker&rsquo;s inequality</strong>, which bounds the total variation distance in terms of the <strong>Kullback-Leibler divergence</strong>, denoted $D_\text{KL}$, we have that
\begin{equation}
\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^2\leq\frac{1}{2}D_\text{KL}(\pi\Vert\tilde{\pi})\leq D_\text{KL}(\pi\Vert\tilde{\pi}),\label{eq:pi.3}
\end{equation}
since $D_\text{KL}(\cdot\Vert\cdot)\geq 0$. Thus, let
\begin{equation}
D_\text{KL}^\text{max}(\pi,\tilde{\pi})\doteq\max_s D_\text{KL}\big(\pi(\cdot\vert s)\Vert\tilde{\pi}(\cdot\vert s)\big),
\end{equation}
with the result \eqref{eq:pi.3} and by <strong>Theorem 5</strong>, we have
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-CD_\text{KL}^\text{max}(\pi,\tilde{\pi}),\label{eq:pi.4}
\end{equation}
where
\begin{equation}
C=\frac{4\epsilon\gamma}{(1-\gamma)^2}
\end{equation}
The policy improvement bound \eqref{eq:pi.4} allows us to specify a policy iteration, as given in the following pseudocode</p><figure><img src=/images/trpo/policy-iteration-nondec-exp-return.png alt="Non-decreasing expected return policy iteration"></figure>It is worth noticing that \eqref{eq:pi.4} allows the policy iteration above to guarantee to generating a sequence of non-decreasing expected returns
\begin{equation}
\eta(\pi_0)\leq\eta(\pi_1)\leq\eta(\pi_2)\leq\ldots
\end{equation}
To see this, let
\begin{equation}
M_i(\pi)\doteq L_{\pi_i}(\pi)-CD_\text{KL}^\text{max}(\pi_i,\pi),
\end{equation}
by \eqref{eq:pi.4}, we then have
\begin{equation}
\eta(\pi_{i+1})\geq M_i(\pi_{i+1}),
\end{equation}
which implies that
\begin{equation}
\eta(\pi_{i+1})-\eta(\pi_i)=\eta(\pi_{i+1})-M_i(\pi_i)\geq M_i(\pi_{i+1})-M_i(\pi_i)
\end{equation}<h2 id=param-policy-opt>Parameterized Policy Optimization by Trust Region<a hidden class=anchor aria-hidden=true href=#param-policy-opt>#</a></h2><p>We now consider the policy optimization problem in which the policy is parameterized by $\theta$.</p><p>We begin by simplifying notations. In particular, let $\eta(\theta)\doteq\eta(\pi_\theta)$, let $L_\theta(\tilde{\theta})\doteq L_{\pi_\theta}(\pi_\tilde{\theta})$ and $D_\text{KL}(\theta\Vert\tilde{\theta})\doteq D_\text{KL}(\pi_\theta\Vert\pi_\tilde{\theta})$, which allows us to represent
\begin{equation}
D_\text{KL}^\text{max}(\theta,\tilde{\theta})\doteq D_\text{KL}^\text{max}(\pi_\theta,\pi_\tilde{\theta})=\max_s D_\text{KL}\big(\pi_\theta(\cdot\vert s)\Vert\pi_\tilde{\theta}(\cdot\vert s)\big)
\end{equation}
Also let $\theta_\text{old}$ denote the previous policy parameters that we want to improve. Hence, by the previous section, we have
\begin{equation}
\eta(\theta)\geq L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta),
\end{equation}
where the equality holds at $\theta=\theta_\text{old}$. This means, we get a guaranteed improvement to the true objective function $\eta$ by solving the following optimization problem
\begin{equation}
\underset{\theta}{\text{maximize}}\hspace{0.2cm}\big[L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta)\big]
\end{equation}
To speed up the algorithm, we make some robust modification. Specifically, we instead solve a <strong>trust region problem</strong>:
\begin{align}
\underset{\theta}{\text{maximize}}&\hspace{0.2cm}L_{\theta_\text{old}}(\theta)\nonumber \\ \text{s.t.}&\hspace{0.2cm}\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\leq\delta,\label{eq:ppo.1}
\end{align}
where $\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}$ is the average KL divergence, given as
\begin{equation}
\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\doteq\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]
\end{equation}
Let us pay attention to our objective function, $L_{\theta_\text{old}}(\theta)$, for a while. By the definition of $L$, given in \eqref{eq:pi.5}, combined with using an <a href=https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/#likelihood-ratio-pg-is>importance sampling estimator</a>, we can rewrite the objective function of \eqref{eq:ppo.1} as
\begin{align}
L_{\theta_\text{old}}(\theta)&=\sum_s\rho_{\theta_\text{old}}(s)\sum_a\pi_\theta(a\vert s)A_{\theta_\text{old}}(s,a) \\ &=\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]
\end{align}
where $A_{\theta_\text{old}}\doteq A_{\pi_{\theta_\text{old}}}$; and $q$ represents the sampling distribution. The trust region problem now is given as
\begin{align}
\underset{\theta}{\text{maximize}}&\hspace{0.2cm}\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\nonumber \\ \text{s.t.}&\hspace{0.2cm}\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]\leq\delta
\end{align}
which is thus equivalent to<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>
\begin{align}
\underset{\theta}{\text{maximize}}&\hspace{0.2cm}\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\nonumber \\ \text{s.t.}&\hspace{0.2cm}\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]\leq\delta\label{eq:ppo.2}
\end{align}</p><h3 id=solve-tr>Solving the optimization problem<a hidden class=anchor aria-hidden=true href=#solve-tr>#</a></h3><p>Let us take a closer look on how to solve this trust region constrained optimization problem. We begin by letting
\begin{equation}
\mathcal{L}_{\theta_\text{old}}(\theta)\doteq\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim\pi_{\theta_\text{old}}}\left[\frac{\pi_\theta(a\vert s)}{\pi_{\theta_\text{old}}(a\vert s)}A_{\theta_\text{old}}(s,a)\right]
\end{equation}
Consider Taylor expansion of the objective function $\mathcal{L}_{\theta_\text{old}}(\theta)$ about $\theta=\theta_\text{old}$ to the first order, we thus can linearly approximate the objective function by the policy gradient, $\nabla_\theta\eta(\pi_{\theta_\text{old}})$, as
\begin{align}
\mathcal{L}_{\theta_\text{old}}(\theta)&\approx\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim\pi_{\theta_\text{old}}}\big[A_{\theta_\text{old}}(s,a)\big]+(\theta-\theta_\text{old})^\text{T}\nabla_\theta\mathcal{L}_{\theta_\text{old}}(\theta)\big\vert_{\theta=\theta_\text{old}} \\ &\overset{\text{(i)}}{=}(\theta-\theta_\text{old})^\text{T}\nabla_\theta\mathcal{L}_{\theta_\text{old}}(\theta)\big\vert_{\theta=\theta_\text{old}} \\ &\overset{\text{(ii)}}{=}(\theta-\theta_\text{old})^\text{T}\left[\frac{1}{1-\gamma}\nabla_\theta L_{\theta_\text{old}}(\theta)\big\vert_{\theta=\theta_\text{old}}\right] \\ &\overset{\text{(iii)}}{=}\frac{1}{1-\gamma}(\theta-\theta_\text{old})^\text{T}\nabla_\theta\eta(\pi_{\theta_\text{old}})\big\vert_{\theta=\theta_\text{old}} \\ &\underset{\max_\theta}{\propto}(\theta-\theta_\text{old})^\text{T}\nabla_\theta\eta(\pi_{\theta_\text{old}})\big\vert_{\theta=\theta_\text{old}},\label{eq:st.1}
\end{align}
where</p><ul class=roman-list><li>This step is due to definition of advantage function for a policy $\pi$, we have that $\mathbb{E}_{a\sim\pi}\big[A_\pi(s,a)\big]=0$, which implies that
\begin{align}
\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim\pi_{\theta_\text{old}}}\big[A_{\theta_\text{old}}(s,a)\big]&=\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[\mathbb{E}_{a\sim\pi_{\theta_\text{old}}}\big[A_{\theta_\text{old}}(s,a)\big]\Big] \\ &=\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\big[0\big]=0
\end{align}</li><li>This step uses the same logic as we have used in \eqref{eq:fn.1}.</li><li>This step is due to \eqref{eq:pi.6}.</li></ul><p>To get an approximation of the constraint, we fist consider the Taylor expansion of the KL divergence $D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)$ about $\theta=\theta_\text{old}$ to the second order, which, given a state $s$, gives us a quadratic approximation:
\begin{align}
&\hspace{-0.7cm}D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\nonumber \\ &=\mathbb{E}_{\pi_{\theta_\text{old}}}\Big[\log\pi_{\theta_\text{old}}(\cdot\vert s)-\log\pi_\theta(\cdot\vert s)\Big] \\ &\approx\mathbb{E}_{\pi_{\theta_\text{old}}}\Bigg[\log\pi_{\theta_\text{old}}(\cdot\vert s)-\Big(\log\pi_{\theta_\text{old}}(\cdot\vert s)+(\theta-\theta_\text{old})^\text{T}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nonumber \\ &\hspace{2cm}+\left.\frac{1}{2}(\theta_\text{old}-\theta)^\text{T}\nabla_\theta^2\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}(\theta_\text{old}-\theta)\right)\Bigg] \\ &\overset{\text{(i)}}{=}-\mathbb{E}_{\pi_{\theta_\text{old}}}\left[\frac{1}{2}(\theta-\theta_\text{old})^\text{T}\nabla_\theta^2\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}(\theta-\theta_\text{old})\right] \\ &\overset{\text{(ii)}}{=}\frac{1}{2}(\theta-\theta_\text{old})^\text{T}\mathbb{E}_{\pi_{\theta_\text{old}}}\Big[\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}^\text{T}\Big]\left(\theta-\theta_\text{old}\right),\label{eq:st.2}
\end{align}
where</p><ul class=roman-list><li>By chain rule, we have
\begin{align}
\hspace{-0.7cm}\mathbb{E}_{\pi_{\theta_\text{old}}}\left[(\theta_\text{old}-\theta)^\text{T}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\right]&=\sum_s\pi_{\theta_\text{old}}(\cdot\vert s)(\theta_\text{old}-\theta)^\text{T}\frac{\nabla_\theta\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}}{\pi_{\theta_\text{old}}(\cdot\vert s)} \\ &=(\theta_\text{old}-\theta)^\text{T}\sum_s\nabla_\theta\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}} \\ &=(\theta_\text{old}-\theta)^\text{T}\left.\left(\nabla_\theta\sum_s\pi_\theta(\cdot\vert s)\right)\right\vert_{\theta=\theta_\text{old}} \\ &=(\theta_\text{old}-\theta)^\text{T}(\nabla_\theta 1)\big\vert_{\theta=\theta_\text{old}} \\ &=(\theta_\text{old}-\theta)^\text{T}\mathbf{0}=0
\end{align}</li><li>This step goes with same logic as we have used in <a href=https://trunghng.github.io/posts/evolution-strategy/nes/#derivation-ii target=_blank><b>Natural evolution strategies</b></a>, which let us claim that
\begin{equation}
-\mathbb{E}_{\pi_{\theta_\text{old}}}\Big[\nabla_\theta^2\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\Big]=\mathbb{E}_{\pi_{\theta_\text{old}}}\Big[\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}^\text{T}\Big]
\end{equation}</li></ul><p>Given the Taylor series approximation \eqref{eq:st.2}, we can locally approximate $\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)$ as
\begin{align}
&\hspace{-0.5cm}\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\nonumber \\ &\approx\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[\frac{1}{2}(\theta-\theta_\text{old})^\text{T}\mathbb{E}_{\pi_{\theta_\text{old}}}\Big[\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}^\text{T}\Big]\left(\theta-\theta_\text{old}\right)\right] \\ &=\frac{1}{2}(\theta-\theta_\text{old})^\text{T}\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}^\text{T}\Big]\left(\theta-\theta_\text{old}\right) \\ &=\frac{1}{2}(\theta-\theta_\text{old})^\text{T}\mathbf{F}(\theta-\theta_\text{old}),\label{eq:st.3}
\end{align}
where the matrix
\begin{equation}
\mathbf{F}\doteq\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}\nabla_\theta\log\pi_\theta(\cdot\vert s)\big\vert_{\theta=\theta_\text{old}}^\text{T}\Big]
\end{equation}
is referred as the <strong>Fisher information matrix</strong>, which, worthy remarking, is symmetric.</p><p>As acquired results \eqref{eq:st.1} and \eqref{eq:st.3}, we yield an approximate problem
\begin{align}
\underset{\theta}{\text{maximize}}&\hspace{0.2cm}\Delta\theta^\text{T}g=\tilde{\mathcal{L}}(\theta)\nonumber \\ \text{s.t.}&\hspace{0.2cm}\frac{1}{2}\Delta\theta^\text{T}\mathbf{F}\Delta\theta\leq\delta,\label{eq:st.4}
\end{align}
where we have let $\Delta\theta\doteq\theta-\theta_\text{old}$ and $g\doteq\nabla_\theta\eta(\pi_{\theta_\text{old}})\big\vert_{\theta=\theta_\text{old}}$ denote the policy gradient to simplify our notations.</p><h4 id=ntr-pg>Natural policy gradient<a hidden class=anchor aria-hidden=true href=#ntr-pg>#</a></h4><p>Consider the problem \eqref{eq:st.4}, we have the <strong>Lagrangian</strong><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> associated with the our constrained optimization problem is given by
\begin{equation}
\bar{\mathcal{L}}(\Delta\theta,\lambda)=-\Delta\theta^\text{T}g+\lambda\left(\frac{1}{2}\Delta\theta^\text{T}\mathbf{F}\Delta\theta-\delta\right)
\end{equation}
which can be minimized w.r.t $\Delta\theta$ by taking the gradient of the Lagrangian w.r.t $\Delta\theta$
\begin{equation}
\nabla_{\Delta\theta}\overline{\mathcal{L}}(\Delta\theta,\lambda)=-g+\lambda\mathbf{F}\Delta\theta,
\end{equation}
and setting this gradient to zero, which yields
\begin{equation}
\Delta\theta=\frac{1}{\lambda}\mathbf{F}^{-1}g\label{eq:np.1}
\end{equation}
The <strong>dual function</strong><sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> then is given by
\begin{align}
\overline{g}(\lambda)&=-\frac{1}{\lambda}g^\text{T}\mathbf{F}^{-1}g+\frac{1}{2\lambda}g^\text{T}\mathbf{F}^{-1}\mathbf{F}\mathbf{F}^{-1}g-\lambda\delta \\ &=-\frac{1}{2\lambda}g^\text{T}\mathbf{F}^{-1}g-\lambda\delta,
\end{align}
Letting the gradient of dual function w.r.t $\lambda$
\begin{equation}
\nabla_\lambda\overline{g}(\lambda)=\frac{1}{2}g^\text{T}\mathbf{F}^{-1}g\cdot\frac{1}{\lambda^2}-\delta
\end{equation}
be zero and solving for $\lambda$ gives us the Lagrange multiplier that maximizes $\overline{g}$, which is
\begin{equation}
\lambda=\sqrt{\frac{g^\text{T}\mathbf{F}^{-1}g}{2\delta}}
\end{equation}
The vector $\Delta\theta$ given in \eqref{eq:np.1} that solves the optimization problem \eqref{eq:st.4} defines the a search direction, which is the direction of the <strong>natural policy gradient</strong>, i.e. $\tilde{\nabla}_\theta\tilde{\mathcal{L}}(\theta)=\mathbf{F}^{-1}g$.</p><p>Hence, using this gradient to iteratively update $\theta$ gives us
\begin{equation}
\theta_{k+1}:=\theta_k+\lambda^{-1}\tilde{\nabla}_\theta\tilde{\mathcal{L}}(\theta)=\theta_k+\sqrt{\frac{2\delta}{g^\text{T}\mathbf{F}^{-1}g}}\mathbf{F}^{-1}g\label{eq:np.2}
\end{equation}</p><h4 id=line-search>Line search<a hidden class=anchor aria-hidden=true href=#line-search>#</a></h4><p>A problem with the above algorithm is that there exist approximation errors because of the Taylor expansion we have used. This consequently might not give us an improvement of the objective or the updated $\pi_\theta$ may not satisfy the KL constraint due to taking large steps.</p><p>To overcome this, we use a <strong>line search</strong> by adding an exponential decay to the update rule \eqref{eq:np.2} that
\begin{equation}
\theta_{k+1}:=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^\text{T}\mathbf{F}^{-1}g}}\mathbf{F}^{-1}g,\label{eq:ls.1}
\end{equation}
where $\alpha\in(0,1)$ is the <strong>decay coefficient</strong> and $j$ is the smallest nonnegative integer that make an improvement on the objective, while let $\pi_{\theta_{k+1}}$ satisfy the KL constraint as well.</p><h4 id=compute-f-inv-g>Compute $\mathbf{F}^{-1}g$<a hidden class=anchor aria-hidden=true href=#compute-f-inv-g>#</a></h4><p>Since both the step size and direction of the update \eqref{eq:ls.1} relate to $\mathbf{F}^{-1}g$, it is then necessary to take into account the computation of this product.</p><p>Rather than computing the inverse $\mathbf{F}^{-1}$ of the Fisher information matrix, then multiply it with the gradient vector $g$ to obtain the natural gradient $\mathbf{F}^{-1}g$, we find a vector $x$ such that
\begin{equation}
\mathbf{F}x=g,\label{eq:cfig.1}
\end{equation}
which implies that $x=\mathbf{F}^{-1}g$.</p><p>The problem now remains to solve the linear equation \eqref{eq:cfig.1}, which can be approximately solved by <strong>Conjugate gradient</strong> method with a predefined number of iterations.</p><h2 id=sampled-bsd-est>Sampled-based estimation<a hidden class=anchor aria-hidden=true href=#sampled-bsd-est>#</a></h2><p>The objective and constraint functions of \eqref{eq:ppo.2} can be approximated using Monte Carlo simulation. Following are two possible sampling approaches to construct the estimated objective and constraint functions.</p><h3 id=sgl>Single path<a hidden class=anchor aria-hidden=true href=#sgl>#</a></h3><p>This sampling scheme has the following procedure</p><ul class=number-list><li>Sample $s_0\sim\rho_0$ to get a set of $m$ start states $\mathcal{S}_0=\{s_0^{(1)},\ldots,s_0^{(m)}\}$.</li><li>For each $s_0^{(i)}\in\mathcal{S}_0$, generate a trajectory $\tau^{(i)}=\big(s_0^{(i)},a_0^{(i)},s_1^{(i)},a_1^{(i)},\ldots,s_{T-1}^{(i)},a_{T-1}^{(i)},s_T^{(i)}\big)$ by rolling out the policy $\pi_{\theta_\text{old}}$ for $T$ steps. Thus $q(a^{(i)}\vert s^{(i)})=\pi_{\theta_\text{old}}(a^{(i)}\vert s^{(i)})$.</li><li>At each state-action pair $(s_t^{(i)},a_t^{(i)})$, compute the action-value function $Q_{\theta_\text{old}}(s,a)$ by taking the discounted sum of future rewards along $\tau^{(i)}$.</li></ul><h3 id=vine>Vine<a hidden class=anchor aria-hidden=true href=#vine>#</a></h3><p>This sampling approach follows the following process</p><ul class=number-list><li>Sample $s_0\sim\rho_0$ and simulate the policy $\pi_{\theta_i}$ to generate $m$ trajectories.</li><li>Choose a rollout set, which is a subset $s_1,\ldots,s_N$ of $N$ states along the trajectories.</li><li>For each state $s_n$ with $1\leq n\leq N$, sample $K$ actions according to $a_{n,k}\sim q(\cdot\vert s_n)$, where $q(\cdot\vert s_n)$ includes the support of $\pi_{\theta_i}(\cdot\vert s_n)$.</li><li>For each action $a_{n,k}$, estimate $\hat{Q}_{\theta_i}(s_n,a_{n,k})$ by performing a rollout starting from $s_n$ and taking action $a_{n,k}$</li><li>Given the estimated action-value function, $\hat{Q}_{\theta_i}(s_n,a_{n,k})$, for each state-action pair $(s_n,a_{n,k})$, compute the estimator, $L_n(\theta)$, of $L_{\theta_\text{old}}$ at state $s_n$ as:<ul class=roman-list><li>For small, finite action spaces, in which generating a rollout for every possible action from a given state is possible, thus
\begin{equation}
L_n(\theta)=\sum_{k=1}^{K}\pi_\theta(a_k\vert s_n)\hat{Q}(s_n,a_k),
\end{equation}
where $\mathcal{A}=\{a_1,\ldots,a_K\}$ is the action space.</li><li>For large or continuous state spaces, use importance sampling
\begin{equation}
L_n(\theta)=\frac{\sum_{k=1}^{K}\frac{\pi_\theta(a_{n,k}\vert s_n)}{\pi_{\theta_\text{old}}(a_{n,k}\vert s_n)}\hat{Q}(s_n,a_{n,k})}{\sum_{k=1}^{K}\frac{\pi_\theta(a_{n,k}\vert s_n)}{\pi_{\theta_\text{old}}(a_{n,k}\vert s_n)}},
\end{equation}
assuming that $K$ actions $a_{n,1},\ldots,a_{n,K}$ are performed from state $s_n$.</li></ul></li><li>Average over $s_n\sim\rho(\pi)$ to obtain an estimator for $L_{\theta_\text{old}}$, as well the policy gradient.</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. <a href=https://dl.acm.org/doi/10.5555/3045118.3045319>Trust Region Policy Optimization</a>. ICML'15, pp 1889–1897, 2015.</p><p>[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. <a href=https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf>Markov chains and mixing times</a>. American Mathematical Society, 2009.</p><p>[3] Sham Kakade, John Langford. <a href=https://dl.acm.org/doi/10.5555/645531.656005>Approximately optimal approximate reinforcement learning</a>. ICML'2, pp. 267–274, 2002.</p><p>[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. <a href=https://arxiv.org/abs/1707.06347>Proximal Policy Optimization Algorithms</a>. arXiv:1707.06347, 2017.</p><p>[5] Stephen Boyd & Lieven Vandenberghe. <a href=http://www.stanford.edu/%E2%88%BCboyd/cvxbook/>Convex Optimization</a>. Cambridge UP, 2004.</p><p>[6] Sham Kakade. <a href=https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf>A Natural Policy Gradient</a>. NIPS 2001.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>To be more specific, by definition of the advantage, i.e. $A_{\theta_\text{old}}(s,a)=Q_{\theta_\text{old}}(s,a)-V_{\theta_\text{old}}(s)$, we have:
\begin{align}
\mathbb{E}_{s\sim\rho_{\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]&=\mathbb{E}_{s\sim\rho_{\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]\nonumber \\ &\underset{\max_\theta}{\propto}\frac{1}{1-\gamma}\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]\label{eq:fn.1} \\ &=\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\nonumber
\end{align}
where we have used the notation
\begin{equation}
\text{LHS}\underset{\max_\theta}{\propto}\text{RHS}\nonumber
\end{equation}
to denote that the problem $\underset{\theta}{\text{maximize}}\hspace{0.2cm}\text{LHS}$ is equivalent to $\underset{\theta}{\text{maximize}}\hspace{0.2cm}\text{RHS}$. Also, the second step comes from definition of $\rho_\pi$, i.e. for $s_0\sim\rho_0$ and the actions are chosen according to $\pi$, we have
\begin{equation*}
\rho_\pi(s)=P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots,
\end{equation*}
which implies that by summing across all $s$, we obtain
\begin{align*}
\sum_{s}\rho_\pi(s)&=\sum_s P(s_0=s)+\gamma\sum_s P(s_1=s)+\gamma^2\sum_s P(s_2=s)+\ldots \\ &=1+\gamma+\gamma^2+\ldots \\ &=\frac{1}{1-\gamma}
\end{align*}&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>In the original TRPO paper, the authors used the state-action value function $Q_{\theta_\text{old}}$ rather than the advantage $A_{\theta_\text{old}}$ since by definition, $A_{\theta_\text{old}}(s,a)=Q_{\theta_\text{old}}(s,a)-V_{\theta_\text{old}}(s)$, which lets us obtain
\begin{align*}
&\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}Q_{\theta_\text{old}}(s,a)\right] \\ &=\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}\big(A_{\theta_\text{old}}(s,a)+V_{\theta_\text{old}}(s)\big)\right] \\ &=\mathbb{E}_{s\sim\rho_{\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]+\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[V_{\theta_\text{old}}(s)\sum_{a}\pi_\theta(a\vert s)\right] \\ &=\mathbb{E}_{s\sim\rho_{\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]+\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\big[V_{\theta_\text{old}}(s)\big] \\ &\underset{\max_\theta}{\propto}\mathbb{E}_{s\sim\rho_{\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]
\end{align*}&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>The Lagrangian, $\bar{\mathcal{L}}$, here should not be confused with the objective function $\mathcal{L}$ due to their notations. It was just a notation-abused problem, in which normally the Lagrangian is denoted as $\mathcal{L}$, which has been already used.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>The <strong>dual function</strong> is usually denoted by $g$, which has unfortunately been taken by the policy gradient. Thus, we abuse the notation once again by representing it as $\overline{g}$.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/deep-reinforcement-learning/>Deep-Reinforcement-Learning</a></li><li><a href=https://trunghng.github.io/tags/policy-gradient/>Policy-Gradient</a></li><li><a href=https://trunghng.github.io/tags/model-free/>Model-Free</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>My-Rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/deterministic-policy-gradients/><span class=title>« Prev</span><br><span>Deterministic Policy Gradients</span>
</a><a class=next href=https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/><span class=title>Next »</span><br><span>Deep Q-learning</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on x" href="https://x.com/intent/tweet/?text=Trust%20Region%20Policy%20Optimization&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f&amp;hashtags=deep-reinforcement-learning%2cpolicy-gradient%2cmodel-free%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f&amp;title=Trust%20Region%20Policy%20Optimization&amp;summary=Trust%20Region%20Policy%20Optimization&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f&title=Trust%20Region%20Policy%20Optimization"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on whatsapp" href="https://api.whatsapp.com/send?text=Trust%20Region%20Policy%20Optimization%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on telegram" href="https://telegram.me/share/url?text=Trust%20Region%20Policy%20Optimization&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Trust Region Policy Optimization on ycombinator" href="https://news.ycombinator.com/submitlink?t=Trust%20Region%20Policy%20Optimization&u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2ftrpo%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io/>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>