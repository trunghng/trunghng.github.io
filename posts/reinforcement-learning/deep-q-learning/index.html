<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deep Q-learning | Trung's Place</title><meta name=keywords content="deep-reinforcement-learning,function-approximation,q-learning,my-rl"><meta name=description content="
Notes on DQN.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a{text-decoration:none}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list{counter-reset:section}#roman-list,#number-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-.75em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Deep Q-learning"><meta property="og:description" content="
Notes on DQN.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-11-18T15:26:00+07:00"><meta property="article:modified_time" content="2022-11-18T15:26:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Deep Q-learning"><meta name=twitter:description content="
Notes on DQN.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Deep Q-learning","item":"https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deep Q-learning","name":"Deep Q-learning","description":" Notes on DQN.\n","keywords":["deep-reinforcement-learning","function-approximation","q-learning","my-rl"],"articleBody":" Notes on DQN.\nQ-value iteration Recall that in the note Markov Decision Processes, Bellman equations, we have defined the state-value function for a policy $\\pi$ to measure how good the state $s$ is, given as \\begin{equation} V_\\pi(s)=\\sum_{a}\\pi(a\\vert s)\\sum_{s’}P(s’\\vert s,a)\\big[R(s,a,s’)+\\gamma V_\\pi(s’)\\big] \\end{equation} From the definition of $V_\\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$: \\begin{equation} V^*(s)=\\max_{a}\\sum_{s’}P(s’\\vert s,a)\\big[R(s,a,s’)+\\gamma V^*(s’)\\big],\\label{eq:qvi.1} \\end{equation} which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s’$.\nThen, with Dynamic programming, we can solve \\eqref{eq:qvi.1} by an iterative method, called value iteration, given as \\begin{equation} V_{t+1}(s)=\\max_{a}\\sum_{s’}P(s’\\vert s,a)\\big[R(s,a,s’)+\\gamma V_t(s’)\\big]\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\\{V_t\\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the Banach’s fixed point theorem, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.\nDetails for value iteration method can be seen in the following pseudocode.\nRemember that along with the state-value function $V_\\pi(s)$, we have also defined the action-value function, or Q-values for a policy $\\pi$, denoted $Q$, given by \\begin{align} Q_\\pi(s,a)\u0026=\\sum_{s’}P(s’\\vert s,a)\\left[R(s,a,s’)+\\gamma\\sum_{a’}\\pi(a’\\vert s’)Q_\\pi(s’,a’)\\right] \\\\ \u0026=\\sum_{s’}P(s’\\vert s,a)\\big[R(s,a,s’)+\\gamma V_\\pi(s’)\\big] \\end{align} which measures how good it is to be in state $s$ and take action $a$.\nAnalogously, we also have the Bellman equation for the optimal action-value function, given as \\begin{align} Q^*(s,a)\u0026=\\sum_{s’}P(s’\\vert s,a)\\left[R(s,a,s’)+\\gamma\\max_{a’}Q^*(s’,a’)\\right]\\label{eq:qvi.2} \\\\ \u0026=\\sum_{s’}P(s’\\vert s,a)\\big[R(s,a,s’)+\\gamma V^*(s’)\\big]\\label{eq:qvi.3} \\end{align} The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\\pi^*$, thereafter.\nEquation \\eqref{eq:qvi.3} allows us to write \\begin{equation} V^*(s)=\\max_a Q^*(s,a) \\end{equation} Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \\eqref{eq:qvi.2}, called Q-value iteration. The method is given by the update rule \\begin{equation} Q_{t+1}(s,a)=\\sum_{s’}P(s’\\vert s,a)\\left[R(s,a,s’)+\\gamma\\max_{a’}Q_t(s’,a’)\\right]\\label{eq:qvi.4} \\end{equation} This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.\nQ-learning The update formula \\eqref{eq:qvi.4} can be rewritten as an expected update \\begin{equation} Q_{t+1}(s,a)=\\mathbb{E}_{s’\\sim P(s’\\vert s,a)}\\left[R(s,a,s’)+\\gamma\\max_{a’}Q_t(s’,a’)\\right]\\label{eq:ql.1} \\end{equation} It is noticeable that the above update rule requires the transition model $P(s’\\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \\eqref{eq:ql.1} can be approximated by sampling, as\nAt a state, taking (sampling) action $a$ (e.g. due to an $\\varepsilon$-greedy policy), we get the next state: \\begin{equation} s'\\sim P(s'\\vert s,a) \\end{equation} Consider the old estimate $Q_t(s,a)$. Consider the new sample estimate (target): \\begin{equation} Q_\\text{target}=R(s,a,s')+\\gamma\\max_{a'}Q_t(s',a')\\label{eq:ql.2} \\end{equation} Append the new estimate into a running average to iteratively update Q-values: \\begin{align} Q_{t+1}(s,a)\u0026=(1-\\alpha)Q_t(s,a)+\\alpha Q_\\text{target} \\\\ \u0026=(1-\\alpha)Q_t(s,a)+\\alpha\\left[R(s,a,s')+\\gamma\\max_{a'}Q_t(s',a')\\right] \\end{align} This update rule is in form of a stochastic process, and thus, is guaranteed to converge to the optimal $Q^*$, under the stochastic approximation conditions for the learning rate $\\alpha$. \\begin{equation} \\sum_{t=1}^{\\infty}\\alpha_t(s,a)=\\infty\\hspace{1cm}\\text{and}\\hspace{1cm}\\sum_{t=1}^{\\infty}\\alpha_t^2(s,a)\u003c\\infty,\\label{eq:ql.3} \\end{equation} for all $(s,a)\\in\\mathcal{S}\\times\\mathcal{A}$.\nThe method is so called Q-learning, with pseudocode given below.\nNeural networks with Q-learning As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an approximated solution.\nIn particular, we have tried to find an approximated action-value function $Q_\\boldsymbol{\\theta}(s,a)$, parameterized by a learnable vector $\\boldsymbol{\\theta}$, of the action-value function $Q(s,a)$, as \\begin{equation} Q_\\boldsymbol{\\theta}(s,a) \\end{equation} Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\\boldsymbol{\\theta}$ so as to minimize the loss function \\begin{equation} L(\\boldsymbol{\\theta})=\\mathbb{E}_{s,a\\sim\\mu(\\cdot)}\\Big[\\big(Q(s,a)-Q_\\boldsymbol{\\theta}(s,a)\\big)^2\\Big] \\end{equation} The resulting SGD update had the form \\begin{align} \\boldsymbol{\\theta}_{t+1}\u0026=\\boldsymbol{\\theta}_t-\\frac{1}{2}\\alpha\\nabla_\\boldsymbol{\\theta}\\big[Q(s_t,a_t)-Q_\\boldsymbol{\\theta}(s_t,a_t)\\big]^2 \\\\ \u0026=\\boldsymbol{\\theta}_t+\\alpha\\big[Q(s_t,a_t)-Q_\\boldsymbol{\\theta}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.1} \\end{align} However, we could not perform the exact update \\eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $y_t$, which can be any approximation of $Q(s_t,a_t)$1: \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[y_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.2} \\end{equation}\nLinear function approximation Recall that, we have applied linear methods as our function approximators: \\begin{equation} Q_\\boldsymbol{\\theta}(s,a)=\\boldsymbol{\\theta}^\\text{T}\\mathbf{f}(s,a), \\end{equation} where $\\mathbf{f}(s,a)$ represents the feature vector, (or basis functions) of the state-action pair $(s,a)$. Linear function approximation allowed us to rewrite \\eqref{eq:nql.2} in a simplified form \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[y_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\mathbf{f}(s_t,a_t)\\label{eq:nql.3} \\end{equation} The corresponding SGD method for Q-learning and Q-learning with linear function approximation are respectively given in form of \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\left[R(s_t,a_t,s_{t+1})+\\gamma\\max_{a’}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a’)-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\right]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t)\\label{eq:nql.4} \\end{equation} and \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\left[R(s_t,a_t,s_{t+1})+\\gamma\\max_{a’}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a’)-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\right]\\mathbf{f}(s_t,a_t),\\label{eq:nql.5} \\end{equation} which both replace the $Q_\\text{target}$ in \\eqref{eq:ql.2} by the one parameterized by $\\boldsymbol{\\theta}$ \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\max_{a’}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a’) \\end{equation} However, in updating $\\boldsymbol{\\theta}_ {t+1}$, these methods both use the bootstrapping target: \\begin{equation} R(s_t,a_t,s_{t+1})+\\gamma\\max_{a’}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a’), \\end{equation} which depends on the current value $\\boldsymbol{\\theta}_t$, and thus will be biased. As a consequence, \\eqref{eq:nql.4} does not guarantee to converge2.\nSuch methods are known as semi-gradient since they take into account the effect of changing the weight vector $\\boldsymbol{\\theta}_t$ on the estimate, but ignore its effect on the target.\nDeep Q-learning On the other hands, we have already known that a neural network with particular settings for hidden layers and activation functions can approximate any continuous functions on a compact subsets of $\\mathbb{R}^n$, so how about using it with the Q-learning algorithm?\nSpecifically, we will be using neural network with weight $\\boldsymbol{\\theta}$ as a function approximator for Q-learning update. The network is referred as Q-network, as the whole algorithm is so-called Deep Q-learning, and the agent is known as DQN in short for Deep Q-network.\nThe Q-network can be trained by minimizing a sequence of loss function $L_t(\\boldsymbol{\\theta}_t)$ that changes at each iteration $t$: \\begin{equation} L_t(\\boldsymbol{\\theta}_t)=\\mathbb{E}_{s,a\\sim\\rho(\\cdot)}\\Big[\\big(y_t-Q_{\\boldsymbol{\\theta}_t}(s,a)\\big)^2\\Big],\\label{eq:dqn.1} \\end{equation} where \\begin{equation} y_t=\\mathbb{E}_{s’\\sim\\mathcal{E}}\\left[R(s,a,s’)+\\gamma\\max_{a’}Q_{\\boldsymbol{\\theta}_{t-1}}(s’,a’)\\vert s,a\\right] \\end{equation} is the target in iteration $t$, which follows as in \\eqref{eq:nql.3}; and where $\\rho(s,a)$ is referred as the behavior policy.\nThe TD target $y_t$ can approximated as \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\max_{a’}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a’) \\end{equation} To stabilize learning, DQN applies the following mechanisms.\nExperience replay Along with Q-network, the authors of deep-Q learning also introduce a technique called experience replay, which utilizes data efficiency and at the same time reduces the variance of the updates.\nIn particular, at each time step $t$, the experience, $e_t$, defined as \\begin{equation} e_t=(s_t,a_t,r_t,s_{t+1}) \\end{equation} is added into a set $\\mathcal{D}$ of size $N$, which is sampled uniformly at the training time to apply Q-learning updates. This method provides some advantages:\nEach experience $e_t$ can be used in many weight updates. Uniformly sampling from $\\mathcal{D}$ cancels out the correlations between consecutive experiences, i.e. $e_t, e_{t+1}$. Target network DQN introduces a target network $\\hat{Q}$ parameterized by $\\boldsymbol{\\theta}^-$to generate the TD target $y_t$ in \\eqref{eq:dqn.1} as \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\max_{a’}\\hat{Q}_{\\boldsymbol{\\theta}_t^-}(s_{t+1},a’)\\label{eq:tn.1} \\end{equation} The target network $\\hat{Q}$ is cloned from $Q$ every $C$ Q-learning update steps, i.e. $\\boldsymbol{\\theta}^-\\leftarrow\\boldsymbol{\\theta}$.\nImproved variants Double deep Q-learning As stated before that the Q-learning method could lead to over optimistic value estimates. Moreover, Q-learning with function approximation, such as DQN, has also been proved to induce maximization bias. These results are due to that in Q-learning and DQN, the $\\max$ operator uses the same values to both select and evaluate an action.\nTo reduce the overoptimism effect due to overestimation in DQN, we use a double estimator version of deep Q-learning, called Double Deep-Q learning, as we have used double Q-learning to mitigate the maximization bias in Q-learning.\nThe double DQN agent is similar to DQN, except that it replaces the target \\eqref{eq:tn.1}, which can be rewritten as: \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\hat{Q}_{\\boldsymbol{\\theta}_t^-}\\left(s_{t+1},\\underset{a}{\\text{argmax}}\\hspace{0.1cm}\\hat{Q}_{\\boldsymbol{\\theta}_t^-}(s_{t+1},a)\\right), \\end{equation} with \\begin{equation} y_t=R(s_t,a_t,s_{t+1})+\\gamma\\hat{Q}_{\\boldsymbol{\\theta}_t^-}\\left(s_{t+1},\\underset{a}{\\text{argmax}}\\hspace{0.1cm}Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a)\\right) \\end{equation}\nPrioritized replay Dueling network Rainbow References [1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. On the Convergence of Stochastic Iterative Dynamic Programming Algorithms. A.I. Memo No. 1441, 1993.\n[2] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[3] Pieter Abbeel. Foundations of Deep RL Series, YouTube, 2021.\n[4] Vlad Mnih, et al. Playing Atari with Deep Reinforcement Learning, 2013.\n[5] Vlad Mnih, et al. Human Level Control Through Deep Reinforcement Learning. Nature, 2015.\n[6] Hado van Hasselt, Arthur Guez, David Silver. Deep Reinforcement Learning with Double Q-learning. AAAI16, 2016.\n[7] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. arXiv:1511.06581, 2015.\n[8] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. Prioritized Experience Replay. arXiv:1511.05952, 2016.\n[9] Taisuke Kobayashi, Wendyam Eric Lionel Ilboudo. t-Soft Update of Target Network for Deep Reinforcement Learning. arXiv:2008.10861, 2020.\n[10] Zhikang T. Wang, Masahito Ueda. Convergent and Efficient Deep Q Network Algorithm. arXiv:2106.15419, 2022.\nFootnotes In Monte Carlo control, the update target $y_t$ is chosen as the full return $G_t$, i.e. \\begin{equation*} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[G_t-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{equation*} and in (episodic on-policy) TD control methods, we use the TD target as the choice for $y_t$, i.e. for one-step TD methods such as one-step Sarsa, the update rule for $\\boldsymbol{\\theta}$ is given as \\begin{align*} \\boldsymbol{\\theta}_{t+1}\u0026=\\boldsymbol{\\theta}_t+\\alpha\\big[G_{t:t+1}-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t) \\\\ \u0026=\\boldsymbol{\\theta}_t+\\alpha\\big[R(s_t,a_t,s_{t+1})+\\gamma Q_{\\boldsymbol{\\theta}_t}(s_{t+1},a_{t+1})-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{align*} and for $n$-step TD method, for instance, $n$-step Sarsa, we instead have \\begin{equation*} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}_t+\\alpha\\big[G_{t:t+n}-Q_{\\boldsymbol{\\theta}_t}(s_t,a_t)\\big]\\nabla_\\boldsymbol{\\theta}Q_\\boldsymbol{\\theta}(s_t,a_t), \\end{equation*} where \\begin{equation*} G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+\\ldots+\\gamma^{n-1}R_{t+n}+\\gamma^n Q_{\\boldsymbol{\\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\\hspace{1cm}t+n","wordCount":"1496","inLanguage":"en","datePublished":"2022-11-18T15:26:00+07:00","dateModified":"2022-11-18T15:26:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/deep-q-learning/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Deep Q-learning</h1><div class=post-meta><span title='2022-11-18 15:26:00 +0700 +0700'>November 18, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#q-value-iter>Q-value iteration</a></li><li><a href=#q-learning>Q-learning</a></li><li><a href=#nn-q-learning>Neural networks with Q-learning</a><ul><li><a href=#lin-func-approx>Linear function approximation</a></li><li><a href=#dqn>Deep Q-learning</a><ul><li><a href=#exp-replay>Experience replay</a></li><li><a href=#target-net>Target network</a></li></ul></li></ul></li><li><a href=#imp-vars>Improved variants</a><ul><li><a href=#double-dqn>Double deep Q-learning</a></li><li><a href=#prior-rep>Prioritized replay</a></li><li><a href=#duel-net>Dueling network</a></li><li><a href=#rainbow>Rainbow</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Notes on DQN.</p></blockquote><h2 id=q-value-iter>Q-value iteration<a hidden class=anchor aria-hidden=true href=#q-value-iter>#</a></h2><p>Recall that in the note <a href=https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\big[R(s,a,s&rsquo;)+\gamma V_\pi(s&rsquo;)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\big[R(s,a,s&rsquo;)+\gamma V^*(s&rsquo;)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s&rsquo;$.</p><p>Then, with <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#value-iteration><strong>value iteration</strong></a>, given as
\begin{equation}
V_{t+1}(s)=\max_{a}\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\big[R(s,a,s&rsquo;)+\gamma V_t(s&rsquo;)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_t\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/><strong>Banach&rsquo;s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.</p><p>Details for value iteration method can be seen in the following pseudocode.</p><figure><img src=/images/deep-q-learning/value-iteration.png alt="value iteration pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&=\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\left[R(s,a,s&rsquo;)+\gamma\sum_{a&rsquo;}\pi(a&rsquo;\vert s&rsquo;)Q_\pi(s&rsquo;,a&rsquo;)\right] \\ &=\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\big[R(s,a,s&rsquo;)+\gamma V_\pi(s&rsquo;)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p><p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&=\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\left[R(s,a,s&rsquo;)+\gamma\max_{a&rsquo;}Q^*(s&rsquo;,a&rsquo;)\right]\label{eq:qvi.2} \\ &=\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\big[R(s,a,s&rsquo;)+\gamma V^*(s&rsquo;)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p><p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{t+1}(s,a)=\sum_{s&rsquo;}P(s&rsquo;\vert s,a)\left[R(s,a,s&rsquo;)+\gamma\max_{a&rsquo;}Q_t(s&rsquo;,a&rsquo;)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p><figure><img src=/images/deep-q-learning/q-value-iteration.png alt="value iteration pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption></figcaption></figure><h2 id=q-learning>Q-learning<a hidden class=anchor aria-hidden=true href=#q-learning>#</a></h2><p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{t+1}(s,a)=\mathbb{E}_{s&rsquo;\sim P(s&rsquo;\vert s,a)}\left[R(s,a,s&rsquo;)+\gamma\max_{a&rsquo;}Q_t(s&rsquo;,a&rsquo;)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(s&rsquo;\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \eqref{eq:ql.1} can be approximated by sampling, as</p><ul id=number-list><li>At a state, taking (sampling) action $a$ (e.g. due to an $\varepsilon$-greedy policy), we get the next state:
\begin{equation}
s'\sim P(s'\vert s,a)
\end{equation}</li><li>Consider the old estimate $Q_t(s,a)$.</li><li>Consider the new sample estimate (target):
\begin{equation}
Q_\text{target}=R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\label{eq:ql.2}
\end{equation}</li><li>Append the new estimate into a running average to iteratively update Q-values:
\begin{align}
Q_{t+1}(s,a)&=(1-\alpha)Q_t(s,a)+\alpha Q_\text{target} \\ &=(1-\alpha)Q_t(s,a)+\alpha\left[R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\right]
\end{align}</li></ul><p>This update rule is in form of a <strong>stochastic process</strong>, and thus, is <a href=#q-learning-td-convergence>guaranteed to converge</a> to the optimal $Q^*$, under the <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#stochastic-approx-condition>stochastic approximation conditions</a> for the learning rate $\alpha$.
\begin{equation}
\sum_{t=1}^{\infty}\alpha_t(s,a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{t=1}^{\infty}\alpha_t^2(s,a)&lt;\infty,\label{eq:ql.3}
\end{equation}
for all $(s,a)\in\mathcal{S}\times\mathcal{A}$.</p><p>The method is so called <strong>Q-learning</strong>, with pseudocode given below.</p><h2 id=nn-q-learning>Neural networks with Q-learning<a hidden class=anchor aria-hidden=true href=#nn-q-learning>#</a></h2><p>As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/>approximated solution</a>.</p><p>In particular, we have tried to find an approximated action-value function $Q_\boldsymbol{\theta}(s,a)$, parameterized by a learnable vector $\boldsymbol{\theta}$, of the action-value function $Q(s,a)$, as
\begin{equation}
Q_\boldsymbol{\theta}(s,a)
\end{equation}
Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\boldsymbol{\theta}$ so as to minimize the loss function
\begin{equation}
L(\boldsymbol{\theta})=\mathbb{E}_{s,a\sim\mu(\cdot)}\Big[\big(Q(s,a)-Q_\boldsymbol{\theta}(s,a)\big)^2\Big]
\end{equation}
The resulting SGD update had the form
\begin{align}
\boldsymbol{\theta}_{t+1}&=\boldsymbol{\theta}_t-\frac{1}{2}\alpha\nabla_\boldsymbol{\theta}\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]^2 \\ &=\boldsymbol{\theta}_t+\alpha\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.1}
\end{align}
However, we could not perform the exact update \eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $y_t$, which can be any approximation of $Q(s_t,a_t)$<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>:
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.2}
\end{equation}</p><h3 id=lin-func-approx>Linear function approximation<a hidden class=anchor aria-hidden=true href=#lin-func-approx>#</a></h3><p>Recall that, we have applied <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#lin-func-approx>linear methods</a> as our function approximators:
\begin{equation}
Q_\boldsymbol{\theta}(s,a)=\boldsymbol{\theta}^\text{T}\mathbf{f}(s,a),
\end{equation}
where $\mathbf{f}(s,a)$ represents the <strong>feature vector</strong>, (or <strong>basis functions</strong>) of the state-action pair $(s,a)$.
Linear function approximation allowed us to rewrite \eqref{eq:nql.2} in a simplified form
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\mathbf{f}(s_t,a_t)\label{eq:nql.3}
\end{equation}
The corresponding SGD method for Q-learning and Q-learning with linear function approximation are respectively given in form of
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a&rsquo;}Q_{\boldsymbol{\theta}_t}(s_{t+1},a&rsquo;)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.4}
\end{equation}
and
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a&rsquo;}Q_{\boldsymbol{\theta}_t}(s_{t+1},a&rsquo;)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\mathbf{f}(s_t,a_t),\label{eq:nql.5}
\end{equation}
which both replace the $Q_\text{target}$ in \eqref{eq:ql.2} by the one parameterized by $\boldsymbol{\theta}$
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a&rsquo;}Q_{\boldsymbol{\theta}_t}(s_{t+1},a&rsquo;)
\end{equation}
However, in updating $\boldsymbol{\theta}_
{t+1}$, these methods both use the <strong>bootstrapping target</strong>:
\begin{equation}
R(s_t,a_t,s_{t+1})+\gamma\max_{a&rsquo;}Q_{\boldsymbol{\theta}_t}(s_{t+1},a&rsquo;),
\end{equation}
which depends on the current value $\boldsymbol{\theta}_t$, and thus will be biased. As a consequence, \eqref{eq:nql.4} does not guarantee to converge<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><p>Such methods are known as <strong>semi-gradient</strong> since they take into account the effect of changing the weight vector $\boldsymbol{\theta}_t$ on the estimate, but ignore its effect on the target.</p><h3 id=dqn>Deep Q-learning<a hidden class=anchor aria-hidden=true href=#dqn>#</a></h3><p>On the other hands, we have already known that a <strong>neural network</strong> with particular settings for hidden layers and activation functions can approximate <a href=https://trunghng.github.io/posts/machine-learning/neural-nets/#unv-approx>any</a> continuous functions on a compact subsets of $\mathbb{R}^n$, so how about using it with the Q-learning algorithm?</p><p>Specifically, we will be using neural network with weight $\boldsymbol{\theta}$ as a function approximator for Q-learning update. The network is referred as <strong>Q-network</strong>, as the whole algorithm is so-called <strong>Deep Q-learning</strong>, and the agent is known as <strong>DQN</strong> in short for <strong>Deep Q-network</strong>.</p><p>The Q-network can be trained by minimizing a sequence of loss function $L_t(\boldsymbol{\theta}_t)$ that changes at each iteration $t$:
\begin{equation}
L_t(\boldsymbol{\theta}_t)=\mathbb{E}_{s,a\sim\rho(\cdot)}\Big[\big(y_t-Q_{\boldsymbol{\theta}_t}(s,a)\big)^2\Big],\label{eq:dqn.1}
\end{equation}
where
\begin{equation}
y_t=\mathbb{E}_{s&rsquo;\sim\mathcal{E}}\left[R(s,a,s&rsquo;)+\gamma\max_{a&rsquo;}Q_{\boldsymbol{\theta}_{t-1}}(s&rsquo;,a&rsquo;)\vert s,a\right]
\end{equation}
is the target in iteration $t$, which follows as in \eqref{eq:nql.3}; and where $\rho(s,a)$ is referred as the behavior policy.</p><p>The TD target $y_t$ can approximated as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\max_{a&rsquo;}Q_{\boldsymbol{\theta}_t}(s_{t+1},a&rsquo;)
\end{equation}
To stabilize learning, DQN applies the following mechanisms.</p><h4 id=exp-replay>Experience replay<a hidden class=anchor aria-hidden=true href=#exp-replay>#</a></h4><p>Along with Q-network, the authors of deep-Q learning also introduce a technique called <strong>experience replay</strong>, which utilizes data efficiency and at the same time reduces the variance of the updates.</p><p>In particular, at each time step $t$, the <strong>experience</strong>, $e_t$, defined as
\begin{equation}
e_t=(s_t,a_t,r_t,s_{t+1})
\end{equation}
is added into a set $\mathcal{D}$ of size $N$, which is sampled uniformly at the training time to apply Q-learning updates. This method provides some advantages:</p><ul id=number-list><li>Each experience $e_t$ can be used in many weight updates.</li><li>Uniformly sampling from $\mathcal{D}$ cancels out the correlations between consecutive experiences, i.e. $e_t, e_{t+1}$.</li></ul><h4 id=target-net>Target network<a hidden class=anchor aria-hidden=true href=#target-net>#</a></h4><p>DQN introduces a <strong>target network</strong> $\hat{Q}$ parameterized by $\boldsymbol{\theta}^-$to generate the TD target $y_t$ in \eqref{eq:dqn.1} as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a&rsquo;}\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a&rsquo;)\label{eq:tn.1}
\end{equation}
The target network $\hat{Q}$ is cloned from $Q$ every $C$ Q-learning update steps, i.e. $\boldsymbol{\theta}^-\leftarrow\boldsymbol{\theta}$.</p><h2 id=imp-vars>Improved variants<a hidden class=anchor aria-hidden=true href=#imp-vars>#</a></h2><h3 id=double-dqn>Double deep Q-learning<a hidden class=anchor aria-hidden=true href=#double-dqn>#</a></h3><p>As stated <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#max-bias>before</a> that the Q-learning method could lead to over optimistic value estimates. Moreover, Q-learning with function approximation, such as DQN, has also been <a href=#double-dqn-paper>proved</a> to induce maximization bias. These results are due to that in Q-learning and DQN, the $\max$ operator uses the same values to both select and evaluate an action.</p><p>To reduce the overoptimism effect due to overestimation in DQN, we use a double estimator version of deep Q-learning, called <strong>Double Deep-Q learning</strong>, as we have used double Q-learning to mitigate the maximization bias in Q-learning.</p><p>The <strong>double DQN</strong> agent is similar to DQN, except that it replaces the target \eqref{eq:tn.1}, which can be rewritten as:
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\hspace{0.1cm}\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a)\right),
\end{equation}
with
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\hspace{0.1cm}Q_{\boldsymbol{\theta}_t}(s_{t+1},a)\right)
\end{equation}</p><h3 id=prior-rep>Prioritized replay<a hidden class=anchor aria-hidden=true href=#prior-rep>#</a></h3><h3 id=duel-net>Dueling network<a hidden class=anchor aria-hidden=true href=#duel-net>#</a></h3><h3 id=rainbow>Rainbow<a hidden class=anchor aria-hidden=true href=#rainbow>#</a></h3><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=q-learning-td-convergence>Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href=https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps>On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</span></p><p>[2] Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[3] Pieter Abbeel. <a href="https://youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL Series</a>, YouTube, 2021.</p><p>[4] Vlad Mnih, et al. <a href=https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf>Playing Atari with Deep Reinforcement Learning</a>, 2013.</p><p>[5] Vlad Mnih, et al. <a href=https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning>Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p><p>[6] <span id=double-dqn-paper>Hado van Hasselt, Arthur Guez, David Silver. <a href=https://arxiv.org/abs/1509.06461>Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</span></p><p>[7] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. <a href=https://arxiv.org/abs/1511.06581>Dueling Network Architectures for Deep Reinforcement Learning</a>. arXiv:1511.06581, 2015.</p><p>[8] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. <a href=https://arxiv.org/abs/1511.05952>Prioritized Experience Replay</a>. arXiv:1511.05952, 2016.</p><p>[9] Taisuke Kobayashi, Wendyam Eric Lionel Ilboudo. <a href=https://arxiv.org/abs/2008.10861>t-Soft Update of Target Network for Deep Reinforcement Learning</a>. arXiv:2008.10861, 2020.</p><p>[10] Zhikang T. Wang, Masahito Ueda. <a href=https://arxiv.org/abs/2106.15419>Convergent and Efficient Deep Q Network Algorithm</a>. arXiv:2106.15419, 2022.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>In <strong>Monte Carlo control</strong>, the update target $y_t$ is chosen as the <strong>full return</strong> $G_t$, i.e.
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
and in (episodic on-policy) TD control methods, we use the <strong>TD target</strong> as the choice for $y_t$, i.e. for one-step TD methods such as <strong>one-step Sarsa</strong>, the update rule for $\boldsymbol{\theta}$ is given as
\begin{align*}
\boldsymbol{\theta}_{t+1}&=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+1}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t) \\ &=\boldsymbol{\theta}_t+\alpha\big[R(s_t,a_t,s_{t+1})+\gamma Q_{\boldsymbol{\theta}_t}(s_{t+1},a_{t+1})-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{align*}
and for $n$-step TD method, for instance, <strong>$n$-step Sarsa</strong>, we instead have
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+n}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
where
\begin{equation*}
G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^n Q_{\boldsymbol{\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\hspace{1cm}t+n&lt;T
\end{equation*}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ and where $R_{t+1}\doteq R(s_t,a_t,s_{t+1})$.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The semi-gradient TD methods with linear function approximation, e.g. \eqref{eq:nql.5}, are guaranteed to converge to the <strong>TD fixed point</strong> due to the <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#td-fixed-pt-proof>result</a> we have proved.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/deep-reinforcement-learning/>deep-reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/function-approximation/>function-approximation</a></li><li><a href=https://trunghng.github.io/tags/q-learning/>q-learning</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/trpo/><span class=title>« Prev</span><br><span>Trust Region Policy Optimization</span></a>
<a class=next href=https://trunghng.github.io/posts/evolution-strategy/nes/><span class=title>Next »</span><br><span>Natural Evolution Strategies</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on twitter" href="https://twitter.com/intent/tweet/?text=Deep%20Q-learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f&hashtags=deep-reinforcement-learning%2cfunction-approximation%2cq-learning%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f&title=Deep%20Q-learning&summary=Deep%20Q-learning&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f&title=Deep%20Q-learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on whatsapp" href="https://api.whatsapp.com/send?text=Deep%20Q-learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Deep Q-learning on telegram" href="https://telegram.me/share/url?text=Deep%20Q-learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fdeep-q-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2022 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>