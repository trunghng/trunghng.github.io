<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Monte Carlo Methods in Reinforcement Learning | Trung's Place</title><meta name=keywords content="reinforcement-learning,monte-carlo,importance-sampling,my-rl"><meta name=description content="
Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}figcaption{font-size:14px}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;font-size:12px;width:70%;text-align:center;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Monte Carlo Methods in Reinforcement Learning"><meta property="og:description" content="
Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-08-21T13:03:00+07:00"><meta property="article:modified_time" content="2021-08-21T13:03:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Monte Carlo Methods in Reinforcement Learning"><meta name=twitter:description content="
Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Monte Carlo Methods in Reinforcement Learning","item":"https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Monte Carlo Methods in Reinforcement Learning","name":"Monte Carlo Methods in Reinforcement Learning","description":" Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.\n","keywords":["reinforcement-learning","monte-carlo","importance-sampling","my-rl"],"articleBody":" Recall that when using Dynamic Programming algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.\nMonte Carlo Methods Monte Carlo, named after a casino in Monaco, simulates complex probabilistic events using simple random events, such as tossing a pair of dice to simulate the casino‚Äôs overall business model.\nFigure 1: Using Monte Carlo method to approximate the value of $\\pi$. The code can be found here Monte Carlo methods have been used in several different tasks:\nSimulating a system and its probability distribution \\begin{equation} x\\sim\\pi(x) \\end{equation} Estimating a quantity through Monte Carlo integration \\begin{equation} c=\\mathbb{E}_\\pi\\left[f(x)\\right]=\\int\\pi(x)f(x)\\,dx \\end{equation} Optimizing a target function to find its modes (maxima or minima) \\begin{equation} x^*=\\text{argmax}\\,\\pi(x) \\end{equation} Learning a parameters from a training set to optimize some loss functions, such as the maximum likelihood estimation from a set of examples $\\{x_i,i=1,2,\\dots,M\\}$ \\begin{equation} \\Theta^*=\\text{argmax}\\sum_{i=1}^{M}\\log p(x_i;\\Theta) \\end{equation} Visualizing the energy landscape of a target function. Monte Carlo Methods in Reinforcement Learning Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns. Here, we define Monte Carlo methods only for episodic tasks. Or in other words, they learn from complete episodes of experience.\nMonte Carlo Prediction1 Since the value of a state $v_\\pi(s)=\\mathbb{E}_\\pi\\left[G_t|S_t=s\\right]$ is defined as the expectation of the return when the process is started from the given state $s$, an obvious way of estimating this value from experience is to compute observed mean returns after visits to that state. As more returns are observed, the average should converge to the expected value. This is an instance of the so-called Monte Carlo method.\nIn particular, suppose we wish to estimate $v_\\pi(s)$ given a set of episodes obtained by following $\\pi$ and passing through $s$. Each time state $s$ appears in an episode, we call it a visit to $s$. There are two types of Monte Carlo methods:\nFirst-visit MC. The first time $s$ is visited in an episode is referred as the first visit to $s$. The method estimates $v_\\pi(s)$ as the average of the returns that have followed the first visit to $s$. Every-visit MC. The method estimates $v_\\pi(s)$ as the average of the returns that have followed all visits to to $s$. The sample mean return for state $s$ is computed as \\begin{equation} v_\\pi(s)=\\dfrac{\\sum_{t=1}^{T}ùüô\\left(S_t=s\\right)G_t}{\\sum_{t=1}^{T}ùüô\\left(S_t=s\\right)}, \\end{equation} where $ùüô(\\cdot)$ is an indicator function. In the case of first-visit MC, $ùüô\\left(S_t=s\\right)$ returns $1$ only in the first time $s$ is encountered in an episode. And for every-visit MC, $ùüô\\left(S_t=s\\right)$ gives value of $1$ every time $s$ is visited.\nFollowing is pseudocode of first-visit MC prediction, for estimating $V\\approx v_\\pi$\nFirst-visit MC vs. every-visit MC Both methods converge to $v_\\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $\\frac{1}{\\sqrt{n}}$, where $n$ is the number of returns averaged.\nFigure 2: Summary of Statistical Results comparing first-visit and every-visit MC method Monte Carlo Control2 Monte Carlo Estimation of Action Values When model is not available, it is particular useful to estimate action values rather than state values (which alone are insufficient to determine a policy). We must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$. To achieve this, we first consider the policy evaluation problem for action values.\nSimilar to when using MC method to estimate $v_\\pi(s)$, we can use both first-visit MC and every-visit MC to approximate the value of $q_\\pi(s,a)$. The only thing we need to keep in mind is, in this case, we work with visits to a state-action pair rather than to a state. Likewise, we define two types of MC methods for estimating $q_\\pi(s,a)$:\nFirst-visit MC: estimates $q_\\pi(s,a)$ as the average of the returns following the first time in each episode that the state $s$ was visited and the action $a$ was selected. Every-visit MC: estimates $q_\\pi(s,a)$ as the average of the returns that have followed all the visits to state-action pair $(s,a)$. Exploring Starts However, here we must exercise exploration. Because many state-action pairs may never be visited, and if $\\pi$ is a deterministic policy, then returns of only single one action for each state will be observed. That leads to the consequence that the other actions will not be evaluated since there are no returns to average.\nThere is one way to achieve this, which is called exploring starts - an assumption that assumes the episodes start in a state-action pair, and that every pair has a nonzero probability of being selected as the start. This assumption assures that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.\nMonte Carlo Policy Iteration To learn the optimal policy by MC, we apply the idea of GPI: \\begin{equation} \\pi_0\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_0}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_1\\overset{\\small \\text{E}}{\\rightarrow}q_{\\pi_1}\\overset{\\small \\text{I}}{\\rightarrow}\\pi_2\\overset{\\small \\text{E}}{\\rightarrow}\\dots\\overset{\\small \\text{I}}{\\rightarrow}\\pi_*\\overset{\\small \\text{E}}{\\rightarrow}q_* \\end{equation} Specifically,\nPolicy evaluation (denoted $\\overset{\\small\\text{E}}{\\rightarrow}$): estimates action value function $q_\\pi(s,a)$ using the episode generated from $s, a$, following by current policy $\\pi$ \\begin{equation} q_\\pi(s,a)=\\dfrac{\\sum_{t=1}^{T}ùüô\\left(S_t=s,A_t=a\\right)G_t}{\\sum_{t=1}^{T}ùüô\\left(S_t=s,A_t=a\\right)} \\end{equation} Policy improvement (denoted $\\overset{\\small\\text{I}}{\\rightarrow}$): makes the policy greedy with the current value function (action value function in this case) \\begin{equation} \\pi(s)\\doteq\\underset{a\\in\\mathcal{A(s)}}{\\text{argmax}},q(s,a) \\end{equation} The policy improvement can be done by constructing each $\\pi_{k+1}$ as the greedy policy w.r.t $q_{\\pi_k}$ because \\begin{align} q_{\\pi_k}\\left(s,\\pi_{k+1}(s)\\right)\u0026=q_{\\pi_k}\\left(s,\\underset{a}{\\text{argmax}},q_{\\pi_k}(s,a)\\right) \\\\ \u0026=\\max_a q_{\\pi_k}(s,a) \\\\ \u0026\\geq q_{\\pi_k}\\left(s,\\pi_k(s)\\right) \\\\ \u0026\\geq v_{\\pi_k}(s) \\end{align} Therefore, by the policy improvement theorem, we have that $\\pi_{k+1}\\geq\\pi_k$. Figure 3: MC policy iteration To solve this problem with Monte Carlo policy iteration, in the 1998 version of ‚ÄúReinforcement Learning: An Introduction‚Äù, authors of the book introduced Monte Carlo ES (MCES), for Monte Carlo with exploring starts.\nIn MCES, value function is approximated by simulated returns and a greedy policy is selected at each iteration. Although MCES does not converge to any sub-optimal policy, the convergence to optimal fixed point is still an open question. For solutions in particular settings, you can check out some results like Tsitsiklis (2002), Chen (2018), Liu (2020).\nDown below is pseudocode of the Monte Carlo ES.\nOn-policy Monte Carlo Control3 In the previous section, we used the assumption of exploring starts (ES) to design a Monte Carlo control method called MCES. In this part, without making that impractical assumption, we will be talking about another Monte Carlo control method.\nIn on-policy control methods, the policy is generally soft (i.e. $\\pi(a|s)\u003e0,\\forall s\\in\\mathcal{S},a\\in\\mathcal{A(s)}$, but gradually shifted closer and closer to a deterministic optimal policy). We can not simply improve the policy by following a greedy policy, since no exploration will take place. Then to get rid of ES, we use the on-policy MC method with $\\varepsilon$-greedy policies, e.g, most of the time they choose an action that maximal estimated action value, but with probability of $\\varepsilon$ they instead select an action at random. Specifically,\n$Pr(\\small\\textit{non-greedy action})=\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ $Pr(\\small\\textit{greedy action})=1-\\varepsilon+\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ The $\\varepsilon$-greedy policies are examples of $\\varepsilon$-soft policies, defined as ones for which $\\pi(a\\vert s)\\geq\\frac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}$ for all states and actions, for some $\\varepsilon\u003e0$. Among $\\varepsilon$-soft policies, $\\varepsilon$-greedy policies are in some sense those that closest to greedy.\nWe have that any $\\varepsilon$-greedy policy w.r.t $q_\\pi$ is an improvement over any $\\varepsilon$-soft policy is assured by the policy improvement theorem.\nProof\nLet $\\pi‚Äô$ be the $\\varepsilon$-greedy. The conditions of the policy improvement theorem apply because for any $s\\in\\mathcal{S}$, we have: \\begin{align} q_\\pi\\left(s,\\pi‚Äô(s)\\right)\u0026=\\sum_a\\pi‚Äô(a|s)q_\\pi(s,a) \\\\ \u0026=\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a)+(1-\\varepsilon)\\max_a q_\\pi(s,a) \\\\ \u0026\\geq\\dfrac{\\varepsilon}{\\vert\\mathcal{A(s)}\\vert}\\sum_a q_\\pi(s,a)+(1-\\varepsilon)\\sum_a\\dfrac{\\pi(a|s)-\\frac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}}{1-\\varepsilon}q_\\pi(s,a) \\\\ \u0026=\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a)+\\sum_a\\pi(a|s)q_\\pi(s,a)-\\dfrac{\\varepsilon}{\\vert\\mathcal{A}(s)\\vert}\\sum_a q_\\pi(s,a) \\\\ \u0026=v_\\pi(s) \\end{align} where in the third step, we have used the fact that the latter $\\sum$ is a weighted average over $q_\\pi(s,a)$. Thus, by the theorem, $\\pi‚Äô\\geq\\pi$. The equality holds when both $\\pi‚Äô$ and $\\pi$ are optimal policies among the $\\varepsilon$-soft ones.\nPseudocode of the complete algorithm is given below.\nOff-policy Monte Carlo Prediction4 When working with control methods, we have to solve a dilemma about exploitation and exploration. In other words, we have to evaluate a policy from episodes generated by following an exploratory policy.\nA straightforward way to solve this problem is to use two different policies, one that is learned about and becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy is being learned about is called the target policy, whereas behavior policy is the one which is used to generate behavior.\nIn this section, we will be considering the off-policy method on prediction task, on which both target (denoted as $\\pi$) and behavior (denoted as $b$) policies are fixed and given. Particularly, we wish to estimate $v_\\pi$ or $q_\\pi$ from episodes retrieved from following another policy $b$, where $\\pi\\neq b$.\nAssumption of Coverage In order to use episodes from $b$ to estimate values for $\\pi$, we require that every action taken under $\\pi$ is also taken, at least occasionally, under $b$. That means, we assume that $\\pi(a|s)\u003e0$ implies $b(s|a)\u003e0$, which leads to a result that $b$ must be stochastic, while $\\pi$ may be deterministic since $\\pi\\neq b$. This is the assumption of coverage.\nImportance Sampling Let $X$ be a variable (or set of variables) that takes on values in some space $\\textit{Val}(X)$. Importance sampling (IS) is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$, typically called the target distribution. We can estimate this expectation by generating samples $x[1],\\dots,x[M]$ from $P$, and then estimating \\begin{equation} \\mathbb{E}_P\\left[f\\right]\\approx\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m]) \\end{equation} In some cases, it might be impossible or computationally very expensive to generate samples from $P$, we instead prefer to generate samples from a different distribution, $Q$, known as the proposal distribution (or sampling distribution).\nUnnormalized Importance Sampling.\nIf we generate samples from $Q$ instead of $P$, we cannot simply average the $f$-value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that \\begin{align} \\mathbb{E}_{P(X)}\\left[f(X)\\right]\u0026=\\sum_x f(x)P(x) \\\\ \u0026=\\sum_x Q(x)f(x)\\dfrac{P(x)}{Q(x)} \\\\ \u0026=\\mathbb{E}_{Q(X)}\\left[f(X)\\dfrac{P(X)}{Q(X)}\\right]\\tag{1}\\label{1} \\end{align} Based on this observation \\eqref{1}, we can use the standard estimator for expectations relative to $Q$. We generate a set of sample $\\mathcal{D}=\\{x[1],\\dots,x[M]\\}$ from $Q$, and then estimate: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m])\\dfrac{P(x[m])}{Q(x[m])}\\tag{2}\\label{2}, \\end{equation} where $\\hat{\\mathbb{E}}$ denotes empirical expectation. We call this estimator the unnormalized importance sampling estimator, this method is also often called unweighted importance sampling. The factor $\\frac{P(x[m])}{Q(x[m])}$ (denoted as $w(x[m])$) can be viewed as a correction weight to the term $f(x[m])$, which we would have used had $Q$ been our target distribution. Normalized Importance Sampling.\nIn many situations, we have that $P$ is known only up to a normalizing constant $Z$. Particularly, what we have access to is a distribution $\\tilde{P}(X)=ZP(X)$. Thus, rather than to define the weights relative to $P$ as above, we define: \\begin{equation} w(X)\\doteq\\dfrac{\\tilde{P}(X)}{Q(X)} \\end{equation} We have that the weight $w(X)$ is a random variable, and has expected value equal to $Z$: \\begin{equation} \\mathbb{E}_{Q(X)}\\left[w(X)\\right]=\\sum_x Q(x)\\dfrac{\\tilde{P}(x)}{Q(x)}=\\sum_x\\tilde{P}(x)=Z \\end{equation} Hence, this quantity is the normalizing constant of the distribution $\\tilde{P}$. We can now rewrite \\eqref{1} as: \\begin{align} \\mathbb{E}_{P(X)}\\left[f(X)\\right]\u0026=\\sum_x P(x)f(x) \\\\ \u0026=\\sum_x Q(x)f(x)\\dfrac{P(x)}{Q(x)} \\\\ \u0026=\\dfrac{1}{Z}\\sum_x Q(x)f(x)\\dfrac{\\tilde{P}(x)}{Q(x)} \\\\ \u0026=\\dfrac{1}{Z}\\mathbb{E}_{Q(X)}\\left[f(X)w(X)\\right] \\\\ \u0026=\\dfrac{\\mathbb{E}_{Q(X)}\\left[f(X)w(X)\\right]}{\\mathbb{E}_{Q(X)}\\left[w(X)\\right]}\\tag{3}\\label{3} \\end{align} We can use an empirical estimator for both the numerator and denominator. Given $M$ samples $\\mathcal{D}=\\{x[1],\\dots,x[M]\\}$ from $Q$, we can estimate: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{\\sum_{m=1}^{M}f(x[m])w(x[m])}{\\sum_{m=1}^{M}w(x[m])}\\tag{4}\\label{4} \\end{equation} We call this estimator the normalized importance sampling estimator (or weighted importance sampling estimator). Off-policy Monte Carlo Prediction via Importance Sampling We apply IS to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the importance sampling ratio (which we denoted as $w$ as above, but now we change the notation to $\\rho$ in order to follows the book).\nThe probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\\dots,S_T$, occurring under any policy $\\pi$ given starting state $s$ is: \\begin{align} Pr(A_t,S_{t+1},\\dots,S_T|S_t,A_{t:T-1}\\sim\\pi)\u0026=\\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\\dots p(S_T|S_{T-1},A_{T-1}) \\\\ \u0026=\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_k,A_k) \\end{align} Thus, the importance sampling ratio as we defined is: \\begin{equation} \\rho_{t:T-1}\\doteq\\dfrac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+1}|S_t,A_t)}{\\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_t,A_t)}=\\prod_{k=1}^{T-1}\\dfrac{\\pi(A_k|S_k)}{b(A_k|S_k)} \\end{equation} which depends only on the two policies and the sequence, not on the MDP.\nSince $v_b(s)=\\mathbb{E}\\left[G_t|S_t=s\\right]$, then we have \\begin{equation} \\mathbb{E}\\left[\\rho_{t:T-1}G_t|S_t=s\\right]=v_\\pi(s) \\end{equation} To estimate $v_\\pi(s)$, we simply scale the returns by the ratios and average the results: \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{\\vert\\mathcal{T}(s)\\vert},\\tag{5}\\label{5} \\end{equation} where $\\mathcal{T}(s)$ is the set of all states in which $s$ is visited (only for every-visit). For a first-visit,$\\mathcal{T}(s)$ would only include time steps that were first visits to $s$ within their episodes. $T(t)$ denotes the first time of termination following time $t$, and $G_t$ denotes the return after $t$ up through $T(t)$.\nWhen importance sampling is done as simple average in this way, we call it ordinary importance sampling (OIS) (which corresponds to unweighted importance sampling in the previous section).\nAnd the one corresponding to weighted importance sampling (WIS), which uses a weighted average, is defined as: \\begin{equation} V(s)\\doteq\\dfrac{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\mathcal{T}(s)}\\rho_{t:T(t)-1}},\\tag{6}\\label{6} \\end{equation} or zero if the denominator is zero.\nIncremental Implementation for Off-policy MC Prediction using IS Incremental Method Incremental method is a way of updating averages with small, constant computation required to process each new reward instead of maintaining a record of all the rewards and then performing this computation whenever the estimated value was needed. It follows the general rule: \\begin{equation} NewEstimate\\leftarrow OldEstimate+StepSize\\left[Target-OldEstimate\\right] \\end{equation}\nApplying to Off-policy MC Prediction using IS In ordinary IS, the returns are scaled by the IS ratio $\\rho_{t:T(t)-1}$, then simply averaged, as in \\eqref{5}. Thus, it‚Äôs easy to apply incremental method to OIS.\nFor WIS, as in the equation \\eqref{6}, we have to form a weighted average of the returns, and a slightly different incremental incremental algorithm is required. Suppose we have a sequence of returns $G_1,G_2,\\dots,G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$, e.g. $W_i=\\rho_{t_i:T(t_i)}$. We wish to form the estimate \\begin{equation} V_n\\doteq\\dfrac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k},\\hspace{1cm}n\\geq2 \\end{equation} and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first $n$ returns. The update rule for $V_n$ is \\begin{equation} V_{n+1}\\doteq V_n+\\dfrac{W_n}{C_n}\\big[G_n-V_n\\big],\\hspace{1cm}n\\geq1, \\end{equation} and \\begin{equation} C_{n+1}\\doteq C_n+W_{n+1}, \\end{equation} where $C_0=0$. And here is pseudocode of our algorithm.\nOff-policy Monte Carlo Control Similarly, we develop the algorithm for off-policy MC control, based on GPI and WIS, for estimating $\\pi_*$ and $q_*$, which is shown below.\nThe target policy $\\pi\\approx\\pi_*$ is the greedy policy w.r.t $Q$, which is an estimate of $q_\\pi$. The behavior policy, $b$, can be anything, but in order to assure convergence of $\\pi$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be guaranteed by choosing $b$ to be $\\varepsilon$-soft.\nThe policy $\\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$, which may change between or even within episodes.\nExample - Racetrack (This example is taken from Exercise 5.12, Reinforcement Learning: An Introduction book.)\nProblem\nConsider driving a race car around a turn like that shown in Figure 4. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car‚Äôs location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).\nFigure 4: A turn for the racetrack task Solution code\nThe source code can be found here.\nWe begin by importing some useful packages.\nimport numpy as np import matplotlib.pyplot as plt from tqdm import tqdm Next, we define our environment\nclass RaceTrack: def __init__(self, grid): self.NOISE = 0 self.MAX_VELOCITY = 4 self.MIN_VELOCITY = 0 self.starting_line = [] self.track = None self.car_position = None self.actions = [[-1,-1],[-1,0],[-1,1],[0,-1],[0,0],[0,1],[1,-1],[1,0],[1,1]] self._load_track(grid) self._generate_start_state() self.velocity = np.array([0, 0], dtype=np.int16) def reset(self): self._generate_start_state() self.velocity = np.array([0, 0], dtype=np.int16) def get_state(self): return self.car_position.copy(), self.velocity.copy() def _generate_start_state(self): index = np.random.choice(len(self.starting_line)) self.car_position = np.array(self.starting_line[index]) def take_action(self, action): if self.is_terminal(): return 0 self._update_state(action) return -1 def _update_state(self, action): # update velocity # with probability of 0.1, keep the velocity unchanged if not np.random.binomial(1, 0.1): self.velocity += np.array(action, dtype=np.int16) self.velocity = np.minimum(self.velocity, self.MAX_VELOCITY) self.velocity = np.maximum(self.velocity, self.MIN_VELOCITY) # update car position for tstep in range(0, self.MAX_VELOCITY + 1): t = tstep / self.MAX_VELOCITY position = self.car_position + np.round(self.velocity * t).astype(np.int16) if self.track[position[0], position[1]] == -1: self.reset() return if self.track[position[0], position[1]] == 2: self.car_position = position self.velocity = np.array([0, 0], dtype=np.int16) return self.car_position = position def _load_track(self, grid): y_len, x_len = len(grid), len(grid[0]) self.track = np.zeros((x_len, y_len), dtype=np.int16) for y in range(y_len): for x in range(x_len): pt = grid[y][x] if pt == 'W': self.track[x, y] = -1 elif pt == 'o': self.track[x, y] = 1 elif pt == '-': self.track[x, y] = 0 else: self.track[x, y] = 2 # rotate the track in order to sync the track with actions self.track = np.fliplr(self.track) for y in range(y_len): for x in range(x_len): if self.track[x, y] == 0: self.starting_line.append((x, y)) def is_terminal(self): return self.track[self.car_position[0], self.car_position[1]] == 2 We continue by defining our behavior policy and algorithm.\ndef behavior_policy(track, state): index = np.random.choice(len(track.actions)) return np.array(track.actions[index]) def off_policy_MC_control(episodes, gamma, grid): x_len, y_len = len(grid[0]), len(grid) Q = np.zeros((x_len, y_len, 5, 5, 3, 3)) - 40 C = np.zeros((x_len, y_len, 5, 5, 3, 3)) pi = np.zeros((x_len, y_len, 5, 5, 1, 2), dtype=np.int16) track = RaceTrack(grid) # for epsilon-soft greedy policy epsilon = 0.1 for ep in tqdm(range(episodes)): track.reset() trajectory = [] while not track.is_terminal(): state = track.get_state() s_x, s_y = state[0][0], state[0][1] s_vx, s_vy = state[1][0], state[1][1] if not np.random.binomial(1, epsilon): action = pi[s_x, s_y, s_vx, s_vy, 0] else: action = behavior_policy(track, state) reward = track.take_action(action) trajectory.append([state, action, reward]) G = 0 W = 1 while len(trajectory) \u003e 0: state, action, reward = trajectory.pop() G = gamma * G + reward sp_x, sp_y, sv_x, sv_y = state[0][0], state[0][1], state[1][0], state[1][1] a_x, a_y = action s_a = (sp_x, sp_y, sv_x, sv_y, a_x, a_y) C[s_a] += W Q[s_a] += W/C[s_a]*(G-Q[s_a]) q_max = -1e5 a_max = None for act in track.actions: sa_max = sp_x, sp_y, sv_x, sv_y, act[0], act[1] if Q[sa_max] \u003e q_max: q_max = Q[sa_max] a_max = act pi[sp_x, sp_y, sv_x, sv_y, 0] = a_max if not np.array_equal(pi[sp_x, sp_y, sv_x, sv_y, 0], action): break W *= 1/(1-epsilon+epsilon/9) return pi And wrapping everything up with the main function.\nif __name__ == '__main__': gamma = 0.9 episodes = 10000 grid = ['WWWWWWWWWWWWWWWWWW', 'WWWWooooooooooooo+', 'WWWoooooooooooooo+', 'WWWoooooooooooooo+', 'WWooooooooooooooo+', 'Woooooooooooooooo+', 'Woooooooooooooooo+', 'WooooooooooWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WoooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWooooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWoooooooWWWWWWWW', 'WWWWooooooWWWWWWWW', 'WWWWooooooWWWWWWWW', 'WWWW------WWWWWWWW'] policy = off_policy_MC_control(episodes, gamma, grid) track_ = RaceTrack(grid) x_len, y_len = len(grid[0]), len(grid) trace = np.zeros((x_len, y_len)) for _ in range(1000): state = track_.get_state() sp_x, sp_y, sv_x, sv_y = state[0][0], state[0][1], state[1][0], state[1][1] trace[sp_x, sp_y] += 1 action = policy[sp_x, sp_y, sv_x, sv_y, 0] reward = track_.take_action(action) if track_.is_terminal(): break trace = (trace \u003e 0).astype(np.float32) trace += track_.track plt.imshow(np.flipud(trace.T)) plt.savefig('./racetrack_off_policy_control.png') plt.close() We end up with this result after running the code.\nFigure 5: Example - Racetrack's result Discounting-aware Importance Sampling Recall that in the above section, we defined the estimator for $\\mathbb{E}_P[f]$ as: \\begin{equation} \\hat{\\mathbb{E}}_\\mathcal{D}(f)=\\dfrac{1}{M}\\sum_{m=1}^{M}f(x[m])\\dfrac{P(x[m])}{Q(x[m])} \\end{equation} This estimator is unbiased because each of the samples it averages is unbiased: \\begin{equation} \\mathbb{E}_{Q}\\left[\\dfrac{P(x[m])}{Q(x[m])}f(x[m])\\right]=\\int_x Q(x)\\dfrac{P(x)}{Q(x)}f(x)\\hspace{0.1cm}dx=\\int_x P(x)f(x)\\hspace{0.1cm}dx=\\mathbb{E}_{P}\\left[f(x[m])\\right] \\end{equation} This IS estimate is unfortunately often of unnecessarily high variance. To be more specific, for example, the episodes last 100 steps and $\\gamma=0$. Then $G_0=R_1$ will be weighted by \\begin{equation} \\rho_{0:99}=\\dfrac{\\pi(A_0|S_0)}{b(A_0|S_0)}\\dots\\dfrac{\\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})} \\end{equation} but actually, it really needs to be weighted by $\\rho_{0:1}=\\frac{\\pi(A_0|S_0)}{b(A_0|S_0)}$. The other 99 factors $\\frac{\\pi(A_1|S_1)}{b(A_1|S_1)}\\dots\\frac{\\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}$ are irrelevant because after the first reward, the return has already been determined. These later factors are all independent of the return and of expected value $1$; they do not change the expected update, but they add enormously to its variance. They could even make the variance infinite in some cases.\nFigure 6: Infinite variance when using OIS (Eg5.5 - RL: An Introduction book). The code can be found here One of the methods used to avoid this large extraneous variance is discounting-aware IS. The idea is to think of discounting as determining a probability of termination or, equivalently, a degree of partial termination.\nWe begin by defining flat partial returns: \\begin{equation} \\bar{G}_{t:h}\\doteq R_{t+1}+R_{t+2}+\\dots+R_h,\\hspace{1cm}0\\leq t","wordCount":"4101","inLanguage":"en","datePublished":"2021-08-21T13:03:00+07:00","dateModified":"2021-08-21T13:03:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Monte Carlo Methods in Reinforcement Learning</h1><div class=post-meta><span title='2021-08-21 13:03:00 +0700 +0700'>August 21, 2021</span>&nbsp;¬∑&nbsp;20 min&nbsp;¬∑&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#mc-methods>Monte Carlo Methods</a></li><li><a href=#mc-rl>Monte Carlo Methods in Reinforcement Learning</a><ul><li><a href=#mc-prediction>Monte Carlo Prediction</a><ul><li><a href=#first-mc-every-mc>First-visit MC vs. every-visit MC</a></li></ul></li><li><a href=#mc-control>Monte Carlo Control</a><ul><li><a href=#mc-est-action-value>Monte Carlo Estimation of Action Values</a><ul><li><a href=#es>Exploring Starts</a></li></ul></li><li><a href=#mc-policy-iteration>Monte Carlo Policy Iteration</a></li></ul></li><li><a href=#on-policy-mc-control>On-policy Monte Carlo Control</a></li><li><a href=#off-policy-mc-pred>Off-policy Monte Carlo Prediction</a><ul><li><a href=#coverage>Assumption of Coverage</a></li><li><a href=#is>Importance Sampling</a></li><li><a href=#is-off-policy>Off-policy Monte Carlo Prediction via Importance Sampling</a></li><li><a href=#imp-off-policy-is>Incremental Implementation for Off-policy MC Prediction using IS</a><ul><li><a href=#incremental-method>Incremental Method</a></li><li><a href=#applying-off-policy-is>Applying to Off-policy MC Prediction using IS</a></li></ul></li></ul></li><li><a href=#off-policy-mc-control>Off-policy Monte Carlo Control</a><ul><li><a href=#example>Example - Racetrack</a></li></ul></li><li><a href=#discounting-aware-is>Discounting-aware Importance Sampling</a></li><li><a href=#per-decision-is>Per-decision Importance Sampling</a></li></ul></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Recall that when using <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/><strong>Dynamic Programming</strong></a> algorithms to solve RL problems, we made an assumption about the complete knowledge of the environment. With <strong>Monte Carlo</strong> methods, we only require <strong>experience</strong> - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.</p></blockquote><h2 id=mc-methods>Monte Carlo Methods<a hidden class=anchor aria-hidden=true href=#mc-methods>#</a></h2><p><strong>Monte Carlo</strong>, named after a casino in Monaco, simulates complex probabilistic events using simple random events, such as tossing a pair of dice to simulate the casino&rsquo;s overall business model.</p><figure><img src=/images/monte-carlo-in-rl/mc-pi.gif alt="monte carlo method" style=display:block;margin-left:auto;margin-right:auto;width:480;height:360px><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: Using Monte Carlo method to approximate the value of $\pi$. The code can be found <a href=https://github.com/trunghng/maths-visualization/blob/main/monte-carlo/monte_carlo_pi.py target=_blank>here</a></figcaption></figure><br><p>Monte Carlo methods have been used in several different tasks:</p><ul id=number-list><li>Simulating a system and its probability distribution
\begin{equation}
x\sim\pi(x)
\end{equation}</li><li>Estimating a quantity through Monte Carlo integration
\begin{equation}
c=\mathbb{E}_\pi\left[f(x)\right]=\int\pi(x)f(x)\,dx
\end{equation}</li><li>Optimizing a target function to find its modes (maxima or minima)
\begin{equation}
x^*=\text{argmax}\,\pi(x)
\end{equation}</li><li>Learning a parameters from a training set to optimize some loss functions, such as the maximum likelihood estimation from a set of examples $\{x_i,i=1,2,\dots,M\}$
\begin{equation}
\Theta^*=\text{argmax}\sum_{i=1}^{M}\log p(x_i;\Theta)
\end{equation}</li><li>Visualizing the energy landscape of a target function.</li></ul><h2 id=mc-rl>Monte Carlo Methods in Reinforcement Learning<a hidden class=anchor aria-hidden=true href=#mc-rl>#</a></h2><p><strong>Monte Carlo</strong> (<strong>MC</strong>) methods are ways of solving the reinforcement learning problem based on averaging sample returns. Here, we define Monte Carlo methods only for episodic tasks. Or in other words, they learn from complete episodes of experience.</p><h3 id=mc-prediction>Monte Carlo Prediction<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup><a hidden class=anchor aria-hidden=true href=#mc-prediction>#</a></h3><p>Since the value of a state $v_\pi(s)=\mathbb{E}_\pi\left[G_t|S_t=s\right]$ is defined as the expectation of the return when the process is started from the given state $s$, an obvious way of estimating this value from experience is to compute observed mean returns after visits to that state. As more returns are observed, the average should converge to the expected value. This is an instance of the so-called <em>Monte Carlo method</em>.</p><p>In particular, suppose we wish to estimate $v_\pi(s)$ given a set of episodes obtained by following $\pi$ and passing through $s$. Each time state $s$ appears in an episode, we call it a <em>visit</em> to $s$. There are two types of Monte Carlo methods:</p><ul id=number-list><li><b>First-visit MC</b>.<ul><li>The first time $s$ is visited in an episode is referred as the <b>first visit</b> to $s$.</li><li>The method estimates $v_\pi(s)$ as the average of the returns that have followed the <b>first visit</b> to $s$.</li></ul><li><b>Every-visit MC</b>.<ul><li>The method estimates $v_\pi(s)$ as the average of the returns that have followed all visits to to $s$.</li></ul></li></ul><p>The sample mean return for state $s$ is computed as
\begin{equation}
v_\pi(s)=\dfrac{\sum_{t=1}^{T}ùüô\left(S_t=s\right)G_t}{\sum_{t=1}^{T}ùüô\left(S_t=s\right)},
\end{equation}
where $ùüô(\cdot)$ is an indicator function. In the case of <em>first-visit MC</em>, $ùüô\left(S_t=s\right)$ returns $1$ only in the first time $s$ is encountered in an episode. And for <em>every-visit MC</em>, $ùüô\left(S_t=s\right)$ gives value of $1$ every time $s$ is visited.</p><p>Following is pseudocode of <strong>first-visit MC prediction</strong>, for estimating $V\approx v_\pi$</p><figure><img src=/images/monte-carlo-in-rl/mc-prediction.png alt="iterative policy evaluation pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h4 id=first-mc-every-mc>First-visit MC vs. every-visit MC<a hidden class=anchor aria-hidden=true href=#first-mc-every-mc>#</a></h4><p>Both methods converge to $v_\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $\frac{1}{\sqrt{n}}$, where $n$ is the number of returns averaged.</p><figure><img src=/images/monte-carlo-in-rl/first-visit-every-visit.png alt="first-visit MC vs every-visit MC" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: Summary of Statistical Results comparing first-visit and every-visit MC method</figcaption></figure><br><h3 id=mc-control>Monte Carlo Control<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup><a hidden class=anchor aria-hidden=true href=#mc-control>#</a></h3><h4 id=mc-est-action-value>Monte Carlo Estimation of Action Values<a hidden class=anchor aria-hidden=true href=#mc-est-action-value>#</a></h4><p>When model is not available, it is particular useful to estimate <em>action values</em> rather than <em>state values</em> (which alone are insufficient to determine a policy). We must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$. To achieve this, we first consider the policy evaluation problem for action values.</p><p>Similar to when using MC method to estimate $v_\pi(s)$, we can use both first-visit MC and every-visit MC to approximate the value of $q_\pi(s,a)$. The only thing we need to keep in mind is, in this case, we work with visits to a state-action pair rather than to a state. Likewise, we define two types of MC methods for estimating $q_\pi(s,a)$:</p><ul id=number-list><li><b>First-visit MC</b>: estimates $q_\pi(s,a)$ as the average of the returns following the first time in each episode that the state $s$ was visited and the action $a$ was selected.</li><li><b>Every-visit MC</b>: estimates $q_\pi(s,a)$ as the average of the returns that have followed all the visits to state-action pair $(s,a)$.</li></ul><h5 id=es>Exploring Starts<a hidden class=anchor aria-hidden=true href=#es>#</a></h5><p>However, here we must exercise <strong>exploration</strong>. Because many state-action pairs may never be visited, and if $\pi$ is a deterministic policy, then returns of only single one action for each state will be observed. That leads to the consequence that the other actions will not be evaluated since there are no returns to average.</p><p>There is one way to achieve this, which is called <strong>exploring starts</strong> - an assumption that assumes the episodes <em>start in a state-action pair</em>, and that every pair has a <em>nonzero</em> probability of being selected as the start. This assumption assures that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.</p><h4 id=mc-policy-iteration>Monte Carlo Policy Iteration<a hidden class=anchor aria-hidden=true href=#mc-policy-iteration>#</a></h4><p>To learn the optimal policy by MC, we apply the idea of <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#gpi>GPI</a>:
\begin{equation}
\pi_0\overset{\small \text{E}}{\rightarrow}q_{\pi_0}\overset{\small \text{I}}{\rightarrow}\pi_1\overset{\small \text{E}}{\rightarrow}q_{\pi_1}\overset{\small \text{I}}{\rightarrow}\pi_2\overset{\small \text{E}}{\rightarrow}\dots\overset{\small \text{I}}{\rightarrow}\pi_*\overset{\small \text{E}}{\rightarrow}q_*
\end{equation}
Specifically,</p><ol><li><strong>Policy evaluation</strong> (denoted $\overset{\small\text{E}}{\rightarrow}$): estimates action value function $q_\pi(s,a)$ using the episode generated from $s, a$, following by current policy $\pi$
\begin{equation}
q_\pi(s,a)=\dfrac{\sum_{t=1}^{T}ùüô\left(S_t=s,A_t=a\right)G_t}{\sum_{t=1}^{T}ùüô\left(S_t=s,A_t=a\right)}
\end{equation}</li><li><strong>Policy improvement</strong> (denoted $\overset{\small\text{I}}{\rightarrow}$): makes the policy <strong>greedy</strong> with the current value function (action value function in this case)
\begin{equation}
\pi(s)\doteq\underset{a\in\mathcal{A(s)}}{\text{argmax}},q(s,a)
\end{equation}
The policy improvement can be done by constructing each $\pi_{k+1}$ as the greedy policy w.r.t $q_{\pi_k}$ because
\begin{align}
q_{\pi_k}\left(s,\pi_{k+1}(s)\right)&=q_{\pi_k}\left(s,\underset{a}{\text{argmax}},q_{\pi_k}(s,a)\right) \\ &=\max_a q_{\pi_k}(s,a) \\ &\geq q_{\pi_k}\left(s,\pi_k(s)\right) \\ &\geq v_{\pi_k}(s)
\end{align}
Therefore, by the <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-improvement>policy improvement theorem</a>, we have that $\pi_{k+1}\geq\pi_k$.</li></ol><figure><img src=/images/monte-carlo-in-rl/gpi.png alt=GPI width=150 height=150px style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 3</b>: MC policy iteration</figcaption></figure><p>To solve this problem with Monte Carlo policy iteration, in the 1998 version of &ldquo;<strong>Reinforcement Learning: An Introduction</strong>&rdquo;, authors of the book introduced <strong>Monte Carlo ES</strong> (<strong>MCES</strong>), for Monte Carlo with <em>exploring starts</em>.</p><p>In MCES, value function is approximated by simulated returns and a greedy policy is selected at each iteration. Although MCES does not converge to any sub-optimal policy, the convergence to optimal fixed point is still an open question. For solutions in particular settings, you can check out some results like Tsitsiklis (2002), Chen (2018), Liu (2020).<br>Down below is pseudocode of the Monte Carlo ES.</p><figure><img src=/images/monte-carlo-in-rl/mces.png alt="monte carlo es pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=on-policy-mc-control>On-policy Monte Carlo Control<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup><a hidden class=anchor aria-hidden=true href=#on-policy-mc-control>#</a></h3><p>In the previous section, we used the assumption of <a href=#es>exploring starts</a> (ES) to design a Monte Carlo control method called MCES. In this part, without making that impractical assumption, we will be talking about another Monte Carlo control method.</p><p>In <strong>on-policy control methods</strong>, the policy is generally <strong>soft</strong> (i.e. $\pi(a|s)>0,\forall s\in\mathcal{S},a\in\mathcal{A(s)}$, but gradually shifted closer and closer to a deterministic optimal policy). We can not simply improve the policy by following a greedy policy, since no exploration will take place. Then to get rid of ES, we use the on-policy MC method with $\varepsilon$-<em>greedy</em> policies, e.g, most of the time they choose an action that maximal estimated action value, but with probability of $\varepsilon$ they instead select an action at random. Specifically,</p><ul><li>$Pr(\small\textit{non-greedy action})=\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$</li><li>$Pr(\small\textit{greedy action})=1-\varepsilon+\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$</li></ul><p>The $\varepsilon$-greedy policies are examples of $\varepsilon$-<em>soft</em> policies, defined as ones for which $\pi(a\vert s)\geq\frac{\varepsilon}{\vert\mathcal{A(s)}\vert}$ for all states and actions, for some $\varepsilon>0$. Among $\varepsilon$-soft policies, $\varepsilon$-greedy policies are in some sense those that closest to greedy.</p><p>We have that any $\varepsilon$-greedy policy w.r.t $q_\pi$ is an <em>improvement</em> over any $\varepsilon$-soft policy is assured by the <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/#policy-improvement>policy improvement theorem</a>.</p><p><strong>Proof</strong><br>Let $\pi&rsquo;$ be the $\varepsilon$-greedy. The conditions of the policy improvement theorem apply because for any $s\in\mathcal{S}$, we have:
\begin{align}
q_\pi\left(s,\pi&rsquo;(s)\right)&=\sum_a\pi&rsquo;(a|s)q_\pi(s,a) \\ &=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\max_a q_\pi(s,a) \\ &\geq\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\sum_a\dfrac{\pi(a|s)-\frac{\varepsilon}{\vert\mathcal{A}(s)\vert}}{1-\varepsilon}q_\pi(s,a) \\ &=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)-\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a) \\ &=v_\pi(s)
\end{align}
where in the third step, we have used the fact that the latter $\sum$ is a weighted average over $q_\pi(s,a)$. Thus, by the theorem, $\pi&rsquo;\geq\pi$. The equality holds when both $\pi&rsquo;$ and $\pi$ are optimal policies among the $\varepsilon$-soft ones.</p><p>Pseudocode of the complete algorithm is given below.</p><figure><img src=/images/monte-carlo-in-rl/on-policy-mc-control.png alt="monte carlo es pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=off-policy-mc-pred>Off-policy Monte Carlo Prediction<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup><a hidden class=anchor aria-hidden=true href=#off-policy-mc-pred>#</a></h3><p>When working with control methods, we have to solve a dilemma about <strong>exploitation</strong> and <strong>exploration</strong>. In other words, we have to evaluate a policy from episodes generated by following an exploratory policy.</p><p>A straightforward way to solve this problem is to use two different policies, one that is learned about and becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy is being learned about is called the <em>target policy</em>, whereas <em>behavior policy</em> is the one which is used to generate behavior.</p><p>In this section, we will be considering the off-policy method on prediction task, on which both target (denoted as $\pi$) and behavior (denoted as $b$) policies are fixed and given. Particularly, we wish to estimate $v_\pi$ or $q_\pi$ from episodes retrieved from following another policy $b$, where $\pi\neq b$.</p><h4 id=coverage>Assumption of Coverage<a hidden class=anchor aria-hidden=true href=#coverage>#</a></h4><p>In order to use episodes from $b$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $b$. That means, we assume that $\pi(a|s)>0$ implies $b(s|a)>0$, which leads to a result that $b$ must be stochastic, while $\pi$ may be deterministic since $\pi\neq b$. This is the assumption of <strong>coverage</strong>.</p><h4 id=is>Importance Sampling<a hidden class=anchor aria-hidden=true href=#is>#</a></h4><p>Let $X$ be a variable (or set of variables) that takes on values in some space $\textit{Val}(X)$. <strong>Importance sampling</strong> (IS) is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$, typically called the <strong>target distribution</strong>. We can estimate this expectation by generating samples $x[1],\dots,x[M]$ from $P$, and then estimating
\begin{equation}
\mathbb{E}_P\left[f\right]\approx\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])
\end{equation}
In some cases, it might be impossible or computationally very expensive to generate samples from $P$, we instead prefer to generate samples from a different distribution, $Q$, known as the <strong>proposal distribution</strong> (or <strong>sampling distribution</strong>).</p><ul id=number-list><li><b>Unnormalized Importance Sampling</b>.<br>If we generate samples from $Q$ instead of $P$, we cannot simply average the $f$-value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&=\sum_x f(x)P(x) \\ &=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &=\mathbb{E}_{Q(X)}\left[f(X)\dfrac{P(X)}{Q(X)}\right]\tag{1}\label{1}
\end{align}
Based on this observation \eqref{1}, we can use the standard estimator for expectations relative to $Q$. We generate a set of sample $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, and then estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])\dfrac{P(x[m])}{Q(x[m])}\tag{2}\label{2},
\end{equation}
where $\hat{\mathbb{E}}$ denotes empirical expectation. We call this estimator the <b>unnormalized importance sampling estimator</b>, this method is also often called <b>unweighted importance sampling</b>. The factor $\frac{P(x[m])}{Q(x[m])}$ (denoted as $w(x[m])$) can be viewed as a correction weight to the term $f(x[m])$, which we would have used had $Q$ been our target distribution.</li><li><b>Normalized Importance Sampling</b>.<br>In many situations, we have that $P$ is known only up to a normalizing constant $Z$. Particularly, what we have access to is a distribution $\tilde{P}(X)=ZP(X)$.
Thus, rather than to define the weights relative to $P$ as above, we define:
\begin{equation}
w(X)\doteq\dfrac{\tilde{P}(X)}{Q(X)}
\end{equation}
We have that the weight $w(X)$ is a random variable, and has expected value equal to $Z$:
\begin{equation}
\mathbb{E}_{Q(X)}\left[w(X)\right]=\sum_x Q(x)\dfrac{\tilde{P}(x)}{Q(x)}=\sum_x\tilde{P}(x)=Z
\end{equation}
Hence, this quantity is the normalizing constant of the distribution $\tilde{P}$. We can now rewrite \eqref{1} as:
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&=\sum_x P(x)f(x) \\ &=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &=\dfrac{1}{Z}\sum_x Q(x)f(x)\dfrac{\tilde{P}(x)}{Q(x)} \\ &=\dfrac{1}{Z}\mathbb{E}_{Q(X)}\left[f(X)w(X)\right] \\ &=\dfrac{\mathbb{E}_{Q(X)}\left[f(X)w(X)\right]}{\mathbb{E}_{Q(X)}\left[w(X)\right]}\tag{3}\label{3}
\end{align}
We can use an empirical estimator for both the numerator and denominator. Given $M$ samples $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, we can estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{\sum_{m=1}^{M}f(x[m])w(x[m])}{\sum_{m=1}^{M}w(x[m])}\tag{4}\label{4}
\end{equation}
We call this estimator the <b>normalized importance sampling estimator</b> (or <b>weighted importance sampling estimator</b>).</li></ul><h4 id=is-off-policy>Off-policy Monte Carlo Prediction via Importance Sampling<a hidden class=anchor aria-hidden=true href=#is-off-policy>#</a></h4><p>We apply IS to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the <strong>importance sampling ratio</strong> (which we denoted as $w$ as above, but now we change the notation to $\rho$ in order to follows the book).</p><p>The probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\dots,S_T$, occurring under any policy $\pi$ given starting state $s$ is:
\begin{align}
Pr(A_t,S_{t+1},\dots,S_T|S_t,A_{t:T-1}\sim\pi)&=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\dots p(S_T|S_{T-1},A_{T-1}) \\ &=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)
\end{align}
Thus, the importance sampling ratio as we defined is:
\begin{equation}
\rho_{t:T-1}\doteq\dfrac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_t,A_t)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_t,A_t)}=\prod_{k=1}^{T-1}\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}
\end{equation}
which depends only on the two policies and the sequence, not on the MDP.</p><p>Since $v_b(s)=\mathbb{E}\left[G_t|S_t=s\right]$, then we have
\begin{equation}
\mathbb{E}\left[\rho_{t:T-1}G_t|S_t=s\right]=v_\pi(s)
\end{equation}
To estimate $v_\pi(s)$, we simply scale the returns by the ratios and average the results:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\vert\mathcal{T}(s)\vert},\tag{5}\label{5}
\end{equation}
where $\mathcal{T}(s)$ is the set of all states in which $s$ is visited (only for every-visit). For a first-visit,$\mathcal{T}(s)$ would only include time steps that were first visits to $s$ within their episodes. $T(t)$ denotes the first time of termination following time $t$, and $G_t$ denotes the return after $t$ up through $T(t)$.</p><p>When importance sampling is done as simple average in this way, we call it <strong>ordinary importance sampling</strong> (OIS) (which corresponds to <strong>unweighted importance sampling</strong> in the previous section).</p><p>And the one corresponding to <strong>weighted importance sampling</strong> (WIS), which uses a weighted average, is defined as:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}},\tag{6}\label{6}
\end{equation}
or zero if the denominator is zero.</p><h4 id=imp-off-policy-is>Incremental Implementation for Off-policy MC Prediction using IS<a hidden class=anchor aria-hidden=true href=#imp-off-policy-is>#</a></h4><h5 id=incremental-method>Incremental Method<a hidden class=anchor aria-hidden=true href=#incremental-method>#</a></h5><p><strong>Incremental method</strong> is a way of updating averages with small, constant computation required to process each new reward instead of maintaining a record of all the rewards and then performing this computation whenever the estimated value was needed. It follows the general rule:
\begin{equation}
NewEstimate\leftarrow OldEstimate+StepSize\left[Target-OldEstimate\right]
\end{equation}</p><h5 id=applying-off-policy-is>Applying to Off-policy MC Prediction using IS<a hidden class=anchor aria-hidden=true href=#applying-off-policy-is>#</a></h5><p>In ordinary IS, the returns are scaled by the IS ratio $\rho_{t:T(t)-1}$, then simply averaged, as in \eqref{5}. Thus, it&rsquo;s easy to apply incremental method to OIS.</p><p>For WIS, as in the equation \eqref{6}, we have to form a weighted average of the returns, and a slightly different incremental incremental algorithm is required.
Suppose we have a sequence of returns $G_1,G_2,\dots,G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$, e.g. $W_i=\rho_{t_i:T(t_i)}$. We wish to form the estimate
\begin{equation}
V_n\doteq\dfrac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},\hspace{1cm}n\geq2
\end{equation}
and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first $n$ returns. The update rule for $V_n$ is
\begin{equation}
V_{n+1}\doteq V_n+\dfrac{W_n}{C_n}\big[G_n-V_n\big],\hspace{1cm}n\geq1,
\end{equation}
and
\begin{equation}
C_{n+1}\doteq C_n+W_{n+1},
\end{equation}
where $C_0=0$. And here is pseudocode of our algorithm.</p><figure><img src=/images/monte-carlo-in-rl/off-policy-mc-prediction.png alt="off-policy MC prediction pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><h3 id=off-policy-mc-control>Off-policy Monte Carlo Control<a hidden class=anchor aria-hidden=true href=#off-policy-mc-control>#</a></h3><p>Similarly, we develop the algorithm for off-policy MC control, based on GPI and WIS, for estimating $\pi_*$ and $q_*$, which is shown below.</p><figure><img src=/images/monte-carlo-in-rl/off-policy-mc-control.png alt="off-policy MC control pseudocode" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic></figcaption></figure><p>The target policy $\pi\approx\pi_*$ is the greedy policy w.r.t $Q$, which is an estimate of $q_\pi$. The behavior policy, $b$, can be anything, but in order to assure convergence of $\pi$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be guaranteed by choosing $b$ to be $\varepsilon$-soft.</p><p>The policy $\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$, which may change between or even within episodes.</p><h4 id=example>Example - Racetrack<a hidden class=anchor aria-hidden=true href=#example>#</a></h4><p>(This example is taken from <strong>Exercise 5.12</strong>, <a href=#rl-book><strong>Reinforcement Learning: An Introduction</strong></a> book.)</p><p><strong>Problem</strong><br>Consider driving a race car around a turn like that shown in <em><strong>Figure 4</strong></em>. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car&rsquo;s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).</p><figure><img src=/images/monte-carlo-in-rl/racetrack.png alt=racetrack width=200 height=300px style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 4</b>: A turn for the racetrack task</figcaption></figure><br><p><strong>Solution code</strong><br>The source code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-05/racetrack.py>here</a>.</p><p>We begin by importing some useful packages.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>tqdm</span> <span class=kn>import</span> <span class=n>tqdm</span>
</span></span></code></pre></div><p>Next, we define our environment</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>RaceTrack</span><span class=p>:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grid</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>NOISE</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>MAX_VELOCITY</span> <span class=o>=</span> <span class=mi>4</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>MIN_VELOCITY</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>starting_line</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>track</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>car_position</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>actions</span> <span class=o>=</span> <span class=p>[[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>],[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>0</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>],[</span><span class=mi>0</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=mi>0</span><span class=p>,</span><span class=mi>1</span><span class=p>],[</span><span class=mi>1</span><span class=p>,</span><span class=o>-</span><span class=mi>1</span><span class=p>],[</span><span class=mi>1</span><span class=p>,</span><span class=mi>0</span><span class=p>],[</span><span class=mi>1</span><span class=p>,</span><span class=mi>1</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>_load_track</span><span class=p>(</span><span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>_generate_start_state</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>reset</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>_generate_start_state</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>get_state</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>car_position</span><span class=o>.</span><span class=n>copy</span><span class=p>(),</span> <span class=bp>self</span><span class=o>.</span><span class=n>velocity</span><span class=o>.</span><span class=n>copy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>_generate_start_state</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>index</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>starting_line</span><span class=p>))</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>car_position</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>starting_line</span><span class=p>[</span><span class=n>index</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>take_action</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>():</span>
</span></span><span class=line><span class=cl>			<span class=k>return</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>_update_state</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=o>-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>_update_state</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=c1># update velocity</span>
</span></span><span class=line><span class=cl>		<span class=c1># with probability of 0.1, keep the velocity unchanged</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=ow>not</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mf>0.1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>+=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>action</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>minimum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>velocity</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>MAX_VELOCITY</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>velocity</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>MIN_VELOCITY</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>		<span class=c1># update car position</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>tstep</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>MAX_VELOCITY</span> <span class=o>+</span> <span class=mi>1</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=n>t</span> <span class=o>=</span> <span class=n>tstep</span> <span class=o>/</span> <span class=bp>self</span><span class=o>.</span><span class=n>MAX_VELOCITY</span>
</span></span><span class=line><span class=cl>			<span class=n>position</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>car_position</span> <span class=o>+</span> <span class=n>np</span><span class=o>.</span><span class=n>round</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>*</span> <span class=n>t</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>position</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>position</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span> <span class=o>==</span> <span class=o>-</span><span class=mi>1</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=bp>self</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>				<span class=k>return</span>
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>position</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>position</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=bp>self</span><span class=o>.</span><span class=n>car_position</span> <span class=o>=</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>				<span class=bp>self</span><span class=o>.</span><span class=n>velocity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([</span><span class=mi>0</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>				<span class=k>return</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>car_position</span> <span class=o>=</span> <span class=n>position</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>_load_track</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>grid</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>y_len</span><span class=p>,</span> <span class=n>x_len</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>),</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>track</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>y</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>y_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>x_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>				<span class=n>pt</span> <span class=o>=</span> <span class=n>grid</span><span class=p>[</span><span class=n>y</span><span class=p>][</span><span class=n>x</span><span class=p>]</span>
</span></span><span class=line><span class=cl>				<span class=k>if</span> <span class=n>pt</span> <span class=o>==</span> <span class=s1>&#39;W&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span>
</span></span><span class=line><span class=cl>				<span class=k>elif</span> <span class=n>pt</span> <span class=o>==</span> <span class=s1>&#39;o&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>				<span class=k>elif</span> <span class=n>pt</span> <span class=o>==</span> <span class=s1>&#39;-&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>				<span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span> <span class=o>=</span> <span class=mi>2</span>
</span></span><span class=line><span class=cl>		<span class=c1># rotate the track in order to sync the track with actions</span>
</span></span><span class=line><span class=cl>		<span class=bp>self</span><span class=o>.</span><span class=n>track</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>fliplr</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>for</span> <span class=n>y</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>y_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>			<span class=k>for</span> <span class=n>x</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>x_len</span><span class=p>):</span>
</span></span><span class=line><span class=cl>				<span class=k>if</span> <span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>]</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=bp>self</span><span class=o>.</span><span class=n>starting_line</span><span class=o>.</span><span class=n>append</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>y</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>def</span> <span class=nf>is_terminal</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>track</span><span class=p>[</span><span class=bp>self</span><span class=o>.</span><span class=n>car_position</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=bp>self</span><span class=o>.</span><span class=n>car_position</span><span class=p>[</span><span class=mi>1</span><span class=p>]]</span> <span class=o>==</span> <span class=mi>2</span>
</span></span></code></pre></div><p>We continue by defining our behavior policy and algorithm.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>behavior_policy</span><span class=p>(</span><span class=n>track</span><span class=p>,</span> <span class=n>state</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>index</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>choice</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>track</span><span class=o>.</span><span class=n>actions</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>track</span><span class=o>.</span><span class=n>actions</span><span class=p>[</span><span class=n>index</span><span class=p>])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>off_policy_MC_control</span><span class=p>(</span><span class=n>episodes</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>grid</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>Q</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span> <span class=o>-</span> <span class=mi>40</span>
</span></span><span class=line><span class=cl>	<span class=n>C</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>3</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=n>pi</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>5</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>),</span> <span class=n>dtype</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>int16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>track</span> <span class=o>=</span> <span class=n>RaceTrack</span><span class=p>(</span><span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=c1># for epsilon-soft greedy policy</span>
</span></span><span class=line><span class=cl>	<span class=n>epsilon</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>ep</span> <span class=ow>in</span> <span class=n>tqdm</span><span class=p>(</span><span class=nb>range</span><span class=p>(</span><span class=n>episodes</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>		<span class=n>track</span><span class=o>.</span><span class=n>reset</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=n>trajectory</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>		<span class=k>while</span> <span class=ow>not</span> <span class=n>track</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>():</span>
</span></span><span class=line><span class=cl>			<span class=n>state</span> <span class=o>=</span> <span class=n>track</span><span class=o>.</span><span class=n>get_state</span><span class=p>()</span>
</span></span><span class=line><span class=cl>			<span class=n>s_x</span><span class=p>,</span> <span class=n>s_y</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>			<span class=n>s_vx</span><span class=p>,</span> <span class=n>s_vy</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=ow>not</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>binomial</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>epsilon</span><span class=p>):</span>
</span></span><span class=line><span class=cl>				<span class=n>action</span> <span class=o>=</span> <span class=n>pi</span><span class=p>[</span><span class=n>s_x</span><span class=p>,</span> <span class=n>s_y</span><span class=p>,</span> <span class=n>s_vx</span><span class=p>,</span> <span class=n>s_vy</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>			<span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=n>action</span> <span class=o>=</span> <span class=n>behavior_policy</span><span class=p>(</span><span class=n>track</span><span class=p>,</span> <span class=n>state</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>reward</span> <span class=o>=</span> <span class=n>track</span><span class=o>.</span><span class=n>take_action</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>trajectory</span><span class=o>.</span><span class=n>append</span><span class=p>([</span><span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span><span class=p>])</span>
</span></span><span class=line><span class=cl>		<span class=n>G</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>		<span class=n>W</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=k>while</span> <span class=nb>len</span><span class=p>(</span><span class=n>trajectory</span><span class=p>)</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>			<span class=n>state</span><span class=p>,</span> <span class=n>action</span><span class=p>,</span> <span class=n>reward</span> <span class=o>=</span> <span class=n>trajectory</span><span class=o>.</span><span class=n>pop</span><span class=p>()</span>
</span></span><span class=line><span class=cl>			<span class=n>G</span> <span class=o>=</span> <span class=n>gamma</span> <span class=o>*</span> <span class=n>G</span> <span class=o>+</span> <span class=n>reward</span>
</span></span><span class=line><span class=cl>			<span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>			<span class=n>a_x</span><span class=p>,</span> <span class=n>a_y</span> <span class=o>=</span> <span class=n>action</span>
</span></span><span class=line><span class=cl>			<span class=n>s_a</span> <span class=o>=</span> <span class=p>(</span><span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span><span class=p>,</span> <span class=n>a_x</span><span class=p>,</span> <span class=n>a_y</span><span class=p>)</span>
</span></span><span class=line><span class=cl>			<span class=n>C</span><span class=p>[</span><span class=n>s_a</span><span class=p>]</span> <span class=o>+=</span> <span class=n>W</span>
</span></span><span class=line><span class=cl>			<span class=n>Q</span><span class=p>[</span><span class=n>s_a</span><span class=p>]</span> <span class=o>+=</span> <span class=n>W</span><span class=o>/</span><span class=n>C</span><span class=p>[</span><span class=n>s_a</span><span class=p>]</span><span class=o>*</span><span class=p>(</span><span class=n>G</span><span class=o>-</span><span class=n>Q</span><span class=p>[</span><span class=n>s_a</span><span class=p>])</span>
</span></span><span class=line><span class=cl>			<span class=n>q_max</span> <span class=o>=</span> <span class=o>-</span><span class=mf>1e5</span>
</span></span><span class=line><span class=cl>			<span class=n>a_max</span> <span class=o>=</span> <span class=kc>None</span>
</span></span><span class=line><span class=cl>			<span class=k>for</span> <span class=n>act</span> <span class=ow>in</span> <span class=n>track</span><span class=o>.</span><span class=n>actions</span><span class=p>:</span>
</span></span><span class=line><span class=cl>				<span class=n>sa_max</span> <span class=o>=</span> <span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span><span class=p>,</span> <span class=n>act</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=n>act</span><span class=p>[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>				<span class=k>if</span> <span class=n>Q</span><span class=p>[</span><span class=n>sa_max</span><span class=p>]</span> <span class=o>&gt;</span> <span class=n>q_max</span><span class=p>:</span>
</span></span><span class=line><span class=cl>					<span class=n>q_max</span> <span class=o>=</span> <span class=n>Q</span><span class=p>[</span><span class=n>sa_max</span><span class=p>]</span>
</span></span><span class=line><span class=cl>					<span class=n>a_max</span> <span class=o>=</span> <span class=n>act</span>
</span></span><span class=line><span class=cl>			<span class=n>pi</span><span class=p>[</span><span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span> <span class=o>=</span> <span class=n>a_max</span>
</span></span><span class=line><span class=cl>			<span class=k>if</span> <span class=ow>not</span> <span class=n>np</span><span class=o>.</span><span class=n>array_equal</span><span class=p>(</span><span class=n>pi</span><span class=p>[</span><span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span><span class=p>,</span> <span class=mi>0</span><span class=p>],</span> <span class=n>action</span><span class=p>):</span>
</span></span><span class=line><span class=cl>				<span class=k>break</span>
</span></span><span class=line><span class=cl>			<span class=n>W</span> <span class=o>*=</span> <span class=mi>1</span><span class=o>/</span><span class=p>(</span><span class=mi>1</span><span class=o>-</span><span class=n>epsilon</span><span class=o>+</span><span class=n>epsilon</span><span class=o>/</span><span class=mi>9</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=k>return</span> <span class=n>pi</span>
</span></span></code></pre></div><p>And wrapping everything up with the main function.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>	<span class=n>gamma</span> <span class=o>=</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl>	<span class=n>episodes</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>	<span class=n>grid</span> <span class=o>=</span> <span class=p>[</span><span class=s1>&#39;WWWWWWWWWWWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWWooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;Woooooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;Woooooooooooooooo+&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WooooooooooWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WoooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWooooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWoooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWWooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWWooooooWWWWWWWW&#39;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>          <span class=s1>&#39;WWWW------WWWWWWWW&#39;</span><span class=p>]</span>
</span></span><span class=line><span class=cl>	<span class=n>policy</span> <span class=o>=</span> <span class=n>off_policy_MC_control</span><span class=p>(</span><span class=n>episodes</span><span class=p>,</span> <span class=n>gamma</span><span class=p>,</span> <span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>track_</span> <span class=o>=</span> <span class=n>RaceTrack</span><span class=p>(</span><span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=nb>len</span><span class=p>(</span><span class=n>grid</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>trace</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>((</span><span class=n>x_len</span><span class=p>,</span> <span class=n>y_len</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> <span class=n>_</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>		<span class=n>state</span> <span class=o>=</span> <span class=n>track_</span><span class=o>.</span><span class=n>get_state</span><span class=p>()</span>
</span></span><span class=line><span class=cl>		<span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span> <span class=o>=</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>0</span><span class=p>][</span><span class=mi>1</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>0</span><span class=p>],</span> <span class=n>state</span><span class=p>[</span><span class=mi>1</span><span class=p>][</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>trace</span><span class=p>[</span><span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>]</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>		<span class=n>action</span> <span class=o>=</span> <span class=n>policy</span><span class=p>[</span><span class=n>sp_x</span><span class=p>,</span> <span class=n>sp_y</span><span class=p>,</span> <span class=n>sv_x</span><span class=p>,</span> <span class=n>sv_y</span><span class=p>,</span> <span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>		<span class=n>reward</span> <span class=o>=</span> <span class=n>track_</span><span class=o>.</span><span class=n>take_action</span><span class=p>(</span><span class=n>action</span><span class=p>)</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=n>track_</span><span class=o>.</span><span class=n>is_terminal</span><span class=p>():</span>
</span></span><span class=line><span class=cl>			<span class=k>break</span>
</span></span><span class=line><span class=cl>	<span class=n>trace</span> <span class=o>=</span> <span class=p>(</span><span class=n>trace</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>)</span><span class=o>.</span><span class=n>astype</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>trace</span> <span class=o>+=</span> <span class=n>track_</span><span class=o>.</span><span class=n>track</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>imshow</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>flipud</span><span class=p>(</span><span class=n>trace</span><span class=o>.</span><span class=n>T</span><span class=p>))</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>savefig</span><span class=p>(</span><span class=s1>&#39;./racetrack_off_policy_control.png&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>	<span class=n>plt</span><span class=o>.</span><span class=n>close</span><span class=p>()</span>
</span></span></code></pre></div><p>We end up with this result after running the code.</p><figure><img src=/images/monte-carlo-in-rl/racetrack-result.png alt="racetrack's result" style=display:block;margin-left:auto;margin-right:auto;width:450px;height:400px><figcaption style=text-align:center;font-style:italic><b>Figure 5</b>: Example - Racetrack's result</figcaption></figure><h3 id=discounting-aware-is>Discounting-aware Importance Sampling<a hidden class=anchor aria-hidden=true href=#discounting-aware-is>#</a></h3><p>Recall that in the above <a href=#is>section</a>, we defined the estimator for $\mathbb{E}_P[f]$ as:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])\dfrac{P(x[m])}{Q(x[m])}
\end{equation}
This estimator is unbiased because each of the samples it averages is unbiased:
\begin{equation}
\mathbb{E}_{Q}\left[\dfrac{P(x[m])}{Q(x[m])}f(x[m])\right]=\int_x Q(x)\dfrac{P(x)}{Q(x)}f(x)\hspace{0.1cm}dx=\int_x P(x)f(x)\hspace{0.1cm}dx=\mathbb{E}_{P}\left[f(x[m])\right]
\end{equation}
This IS estimate is unfortunately often of unnecessarily high variance. To be more specific, for example, the episodes last 100 steps and $\gamma=0$. Then $G_0=R_1$ will be weighted by
\begin{equation}
\rho_{0:99}=\dfrac{\pi(A_0|S_0)}{b(A_0|S_0)}\dots\dfrac{\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}
\end{equation}
but actually, it really needs to be weighted by
$\rho_{0:1}=\frac{\pi(A_0|S_0)}{b(A_0|S_0)}$.
The other 99 factors $\frac{\pi(A_1|S_1)}{b(A_1|S_1)}\dots\frac{\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}$ are irrelevant because after the first reward, the return has already been determined. These later factors are all independent of the return and of expected value $1$; they do not change the expected update, but they add enormously to its variance. They could even make the variance <strong>infinite</strong> in some cases.</p><figure><img src=/images/monte-carlo-in-rl/inf-var.png alt="infinite variance" style=display:block;margin-left:auto;margin-right:auto><figcaption style=text-align:center;font-style:italic><b>Figure 6</b>: Infinite variance when using OIS (Eg5.5 - RL: An Introduction book). The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-05/infinite-variance.py target=_blank>here</a></figcaption></figure><p>One of the methods used to avoid this large extraneous variance is <strong>discounting-aware IS</strong>. The idea is to think of discounting as determining a probability of termination or, equivalently, a <em>degree</em> of partial termination.</p><p>We begin by defining <strong>flat partial returns</strong>:
\begin{equation}
\bar{G}_{t:h}\doteq R_{t+1}+R_{t+2}+\dots+R_h,\hspace{1cm}0\leq t&lt;h\leq T,
\end{equation}
where <em>flat</em> denotes the absence of discounting, and <em>partial</em> denotes that these returns do not extend all the way to termination but instead stop at $h$, called the <strong>horizon</strong>. The conventional full return $G_t$ can be viewed as a <strong>sum of flat partial returns</strong>:
\begin{align}
G_t&\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots+\gamma^{T-t-1}R_T \\ &=(1-\gamma)R_{t+1} \\ &\hspace{0.5cm}+(1-\gamma)\gamma(R_{t+1}+R_{t+2}) \\ &\hspace{0.5cm}+(1-\gamma)\gamma^2(R_{t+1}+R_{t+2}+R_{t+3}) \\ &\hspace{0.7cm}\vdots \\ &\hspace{0.5cm}+(1-\gamma)\gamma^{T-t-2}(R_{t+1}+R_{t+2}+\dots+R_{T-1}) \\ &\hspace{0.5cm}+\gamma^{T-t-1}(R_{t+1}+R_{t+2}+\dots+R_T) \\ &=(1-\gamma)\sum_{h=t+1}^{T-1}\left(\gamma^{h-t-1}\bar{G}_{t:h}\right)+\gamma^{T-t-1}\bar{G}_{t:T}
\end{align}
Now we need to scale the <em>flat partial returns</em> by an <strong>IS ratio</strong> that is similarly truncated. As $\bar{G}_{t:h}$ only involves rewards up to a horizon $h$, we only need the ratio of the probabilities up to $h$. We define:</p><ol><li><strong>Discounting-aware OIS</strong> estimator
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\bar{G}_{t:h}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar{G}_{t:T(t)}\right]}{\vert\mathcal{T}(s)\vert}
\end{equation}</li><li><strong>Discounting-aware WIS</strong> estimator
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\bar{G}_{t:h}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar{G}_{t:T(t)}\right]}{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\right]}
\end{equation}
These two estimators take into account the discount rate $\gamma$ but have no effect if $\gamma=1$.</li></ol><h3 id=per-decision-is>Per-decision Importance Sampling<a hidden class=anchor aria-hidden=true href=#per-decision-is>#</a></h3><p>There is another way beside discounting-aware that may be able to reduce variance, even if $\gamma=1$.</p><p>Recall that in the off-policy estimator \eqref{5} and \eqref{6}, each term of the sum in the numerator is itself a sum:
\begin{align}
\rho_{t:T-1}G_t&=\rho_{t:T-1}\left(R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-t-1}R_T\right) \\ &=\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\tag{7}\label{7}
\end{align}
We have that
\begin{equation}
\rho_{t:T-1}R_{t+k}=\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\dfrac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\dots\dfrac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+k}
\end{equation}
Of all these factors, only the first $k$ factors, $\frac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\frac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}$, and the last (the reward $R_{t+k}$) are related. All the others are for event that occurred after the reward. Moreover, we have that
\begin{equation}
\mathbb{E}\left[\dfrac{\pi(A_i|S_i)}{b(A_i|S_i)}\right]\doteq\sum_a b(a|S_i)\dfrac{\pi(a|S_i)}{b(a|S_i)}=1
\end{equation}
Therefore, we obtain
\begin{align}
\mathbb{E}\Big[\rho_{t:T-1}R_{t+k}\Big]&=\mathbb{E}\left[\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\dfrac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\right]\mathbb{E}\left[\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}\right]\dots\mathbb{E}\left[\dfrac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}\right] \\ &=\mathbb{E}\Big[\rho_{t:t+k-1}R_{t+k}\Big].1\dots 1 \\ &=\mathbb{E}\Big[\rho_{t:t+k-1}R_{t+k}\Big]
\end{align}
Plug the result we just got into the expectation of \eqref{7}, we have
\begin{align}
\mathbb{E}\Big[\rho_{t:T-1}G_t\Big]&=\mathbb{E}\Big[\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\Big] \\ &=\mathbb{E}\Big[\rho_{t:t}R_{t+1}+\gamma\rho_{t:t+1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\Big] \\ &=\mathbb{E}\Big[\tilde{G}_t\Big],
\end{align}
where $\tilde{G}_t=\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T$.</p><p>We call this idea <strong>per-decision IS</strong>. Hence, we develop <strong>per-decision OIS</strong> estimator, using $\tilde{G}_t$:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\tilde{G}_t}{\vert\mathcal{T}(s)\vert}
\end{equation}</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto</span>. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[2] Adrian Barbu & Song-Chun Zhu. <a href=https://link.springer.com/book/10.1007/978-981-13-2971-5>Monte Carlo Methods</a>.</p><p>[3] David Silver. <a href=https://www.davidsilver.uk/teaching/>UCL course on RL</a>.</p><p>[4] Csaba SzepesvaÃÅri. <a href=https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924>Algorithms for Reinforcement Learning</a>.</p><p>[5] Singh, S.P., Sutton, R.S. <a href=https://doi.org/10.1007/BF00114726>Reinforcement learning with replacing eligibility traces</a>. Mach Learn 22, 123‚Äì158, 1996.</p><p>[6] John N. Tsitsiklis. <a href=https://www.mit.edu/~jnt/Papers/J089-02-jnt-optimistic.pdf>On the Convergence of Optimistic Policy Iteration</a>. Journal of Machine Learning Research 3 (2002) 59‚Äì72.</p><p>[7] Yuanlong Chen. <a href=https://arxiv.org/abs/1808.08763>On the convergence of optimistic policy iteration for stochastic shortest path problem</a>, arXiv:1808.08763, 2018.</p><p>[8] Jun Liu. <a href=https://arxiv.org/abs/2007.10916>On the Convergence of Reinforcement Learning with Monte Carlo Exploring Starts</a>. arXiv:2007.10916, 2020.</p><p>[9] Daphne Koller & Nir Friedman. <a href=https://mitpress.mit.edu/books/probabilistic-graphical-models>Probabilistic Graphical Models: Principles and Techniques</a>.</p><p>[10] A. Rupam Mahmood, Hado P. van Hasselt, Richard S. Sutton. <a href=https://papers.nips.cc/paper/2014/hash/be53ee61104935234b174e62a07e53cf-Abstract.html>Weighted importance sampling for off-policy learning with linear function approximation</a>. Advances in Neural Information Processing Systems 27 (NIPS 2014).</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>A prediction task in RL is where we are given a policy and our goal is to measure how well it performs.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Along with prediction, a control task in RL is where the policy is not fixed, and our goal is to find the optimal policy.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>On-policy is a category of RL algorithms that attempts to evaluate or improve the policy that is used to make decisions.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>In contrast to on-policy, off-policy methods evaluate or improve a policy different from that used to generate the data.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/monte-carlo/>monte-carlo</a></li><li><a href=https://trunghng.github.io/tags/importance-sampling/>importance-sampling</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/calculus/infinite-series-of-constants/><span class=title>¬´ Prev</span><br><span>Infinite Series of Constants</span></a>
<a class=next href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/><span class=title>Next ¬ª</span><br><span>Solving MDPs with Dynamic Programming</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on twitter" href="https://twitter.com/intent/tweet/?text=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f&hashtags=reinforcement-learning%2cmonte-carlo%2cimportance-sampling%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f&title=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning&summary=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f&title=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on whatsapp" href="https://api.whatsapp.com/send?text=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Monte Carlo Methods in Reinforcement Learning on telegram" href="https://telegram.me/share/url?text=Monte%20Carlo%20Methods%20in%20Reinforcement%20Learning&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmonte-carlo-in-rl%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>