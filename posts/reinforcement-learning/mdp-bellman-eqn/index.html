<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Markov Decision Processes, Bellman equations | Trung's Place</title><meta name=keywords content="reinforcement-learning,bellman-equation,my-rl"><meta name=description content="
You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.bc1149f4a72aa4858d3a9f71462f75e5884ffe8073ea9d6d5761d5663d651e20.css integrity="sha256-vBFJ9KcqpIWNOp9xRi915YhP/oBz6p1tV2HVZj1lHiA=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Markov Decision Processes, Bellman equations"><meta property="og:description" content="
You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-06-27T08:00:00+07:00"><meta property="article:modified_time" content="2021-06-27T08:00:00+07:00"><meta property="og:site_name" content="Trung's Place"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Markov Decision Processes, Bellman equations"><meta name=twitter:description content="
You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Markov Decision Processes, Bellman equations","item":"https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Markov Decision Processes, Bellman equations","name":"Markov Decision Processes, Bellman equations","description":" You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.\n","keywords":["reinforcement-learning","bellman-equation","my-rl"],"articleBody":" You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.\nWhat is Reinforcement Learning? Say, there is an unknown environment that we’re trying to put an agent on. By interacting with the agent through taking actions that gives rise to rewards continually, the agent learns a policy that maximize the cumulative rewards.\nReinforcement Learning (RL), roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called policy) for the agent from experimental trials and relative simple feedback received. With the optimal policy, the agent is capable to actively adapt to the environment to maximize future rewards. Markov Decision Processes (MDPs) Markov decision processes (MDPs) formally describe an environment for RL. And almost all RL problems can be formalized as MDPs.\nDefinition (MDP)\nA Markov Decision Process is a tuple $⟨\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma⟩$\n$\\mathcal{S}$ is a set of states called state space $\\mathcal{A}$ is a set of actions called action space $\\mathcal{P}$ is a state transition probability matrix\n$$\\mathcal{P}^a_{ss’}=P(S_{t+1}=s’|S_t=s,A_t=a)$$ $\\mathcal{R}$ is a reward function\n$$\\mathcal{R}^a_s=\\mathbb{E}\\left[R_{t+1}|S_t=s,A_t=a\\right]$$ $\\gamma\\in[0, 1]$ is a discount factor for future reward MDP is an extension of a Markov chain. If only one action exists for each state, and all rewards are the same, an MDP reduces to a Markov chain. All states in MDP has Markov property, referring to the fact that the current state captures all relevant information from the history. \\begin{equation} P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\\dots,S_t) \\end{equation}\nReturn In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the expected return.\nDefinition (Return)\nThe return $G_t$ is the total discounted reward from $t$ \\begin{equation} G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\dots=\\sum_{k=0}^{\\infty}\\gamma^k R_{t+k+1}, \\end{equation} where $\\gamma\\in[0,1]$ is called discount rate (or discount factor).\nThe discount rate $\\gamma$ determines the present value of future rewards: a reward received k time steps in the future is worth only $\\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\\rightarrow\\infty$ then $\\gamma^k\\rightarrow 0$.\nPolicy Policy, which is denoted as $\\pi$, is the behavior function of the agent. $\\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either deterministic or stochastic.\nDeterministic policy:\t$\\quad\\pi(s)=a$ Stochastic policy: $\\quad\\pi(a|s)=P(A_t=a|S_t=s)$ Value Function Value function measures how good a particular state is (or how good it is to perform a given action in a given state).\nDefinition (state-value function)\nThe state-value function of a state $s$ under a policy $\\pi$, denoted as $v_\\pi(s)$, is the expected return starting from state $s$ and following $\\pi$ thereafter: \\begin{equation} v_\\pi(s)=\\mathbb{E}_\\pi[G_t|S_t=s] \\end{equation}\nDefinition (action-value function)\nSimilarly, we define the value of taking action $a$ in state $s$ under a policy $\\pi$, denoted as $q_\\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\\pi$: \\begin{equation} q_\\pi(s,a)=\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] \\end{equation}\nSince we follow the policy $\\pi$, we have that \\begin{equation} v_\\pi(s)=\\sum_{a\\in\\mathcal{A}}q_\\pi(s,a)\\pi(a|s) \\end{equation}\nOptimal Policy and Optimal Value Function For finite MDPs (finite state and action space), we can precisely define an optimal policy. Value functions define a partial ordering over policies. A policy $\\pi$ is defined to be better than or equal to a policy $\\pi’$ if its expected return is greater than or equal to that of $\\pi’$ for all states. In other words, \\begin{equation} \\pi\\geq\\pi’\\iff v_\\pi(s)\\geq v_{\\pi’} \\forall s\\in\\mathcal{S} \\end{equation}\nTheorem (Optimal policy)\nFor any MDP, there exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, \\begin{equation} \\pi_*\\geq\\pi,\\forall\\pi \\end{equation}\nThe proof of the above theorem is provided in another note since we need some additional tools to do that.\nThere may be more than one optimal policy, they share the same state-value function, called optimal state-value function though. \\begin{equation} v_*(s)=\\max_{\\pi}v_\\pi(s) \\end{equation} Optimal policies also share the same action-value function, called optimal action-value function \\begin{equation} q_*(s,a)=\\max_{\\pi}q_\\pi(s,a) \\end{equation}\nBellman Equations A fundamental property of value functions used throughout RL is that they satisfy recursive relationships \\begin{align} v_\\pi(s)\u0026\\doteq \\mathbb{E}_\\pi[G_t|S_t=s] \\\\\u0026=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s] \\\\\u0026=\\sum_{s’,r,g’,a}p(s’,r,g’,a|s)(r+\\gamma g’) \\\\\u0026=\\sum_{a}p(a|s)\\sum_{s’,r,g’}p(s’,r,g’|a,s)(r+\\gamma g’) \\\\\u0026=\\sum_{a}\\pi(a|s)\\sum_{s’,r,g’}p(s’,r|a,s)p(g’|s’,r,a,s)(r+\\gamma g’) \\\\\u0026=\\sum_{a}\\pi(a|s)\\sum_{s’,r}p(s’,r|a,s)\\sum_{g’}p(g’|s’)(r+\\gamma g’) \\\\\u0026=\\sum_{a}\\pi(a|s)\\sum_{s’,r}p(s’,r|a,s)\\left[r+\\gamma\\sum_{g’}p(g’|s’)g’\\right] \\\\\u0026=\\sum_{a}\\pi(a|s)\\sum_{s’,r}p(s’,r|a,s)\\left[r+\\gamma v_\\pi(s’)\\right], \\end{align} where $p(s’,r|s,a)=P(S_{t+1}=s’,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the Bellman equation for $v_\\pi(s)$. It expresses a relationship between the value state $s$, $v_\\pi(s)$ and the values of its successor states $s’$, $v_\\pi(s’)$.\nSimilarly, we define the Bellman equation for $q_\\pi(s,a)$ \\begin{align} q_\\pi(s,a)\u0026\\doteq\\mathbb{E}_\\pi[G_t|S_t=s,A_t=a] \\\\\u0026=\\mathbb{E}_\\pi[R_t+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\u0026=\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma\\sum_{a’}\\pi(a’|s’)q_\\pi(s’,a’)\\right] \\end{align}\nBellman Backup Diagram Backup diagram of state-value function and action-value function respectively Figure 1: Backup diagram of state-value function Figure 2: Backup diagram of action-value function Bellman Optimality Equations Since $v_*$ is the value function for a policy, it must satisfy the Bellman equation for state-values. Moreover, it is also the optimal value function, then we have \\begin{align} v_*(s)\u0026=\\max_{a\\in\\mathcal{A(s)}}q_{\\pi_*}(s,a) \\\\\u0026=\\max_{a}\\mathbb{E}_{\\pi_*}[G_t|S_t=s,A_t=a] \\\\\u0026=\\max_{a}\\mathbb{E}_{\\pi_*}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\u0026=\\max_{a}\\mathbb{E}[R_{t+1}+\\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\\\\u0026=\\max_{a}\\sum_{s’,r}p(s’,r|s,a)[r+\\gamma v_*(s’)] \\end{align} The last two equations are two forms of the Bellman optimality equation for $v_*$. Similarly, we have the Bellman optimality equation for $q_*$ \\begin{align} q_*(s,a)\u0026=\\mathbb{E}\\left[R_{t+1}+\\gamma\\max_{a’}q_*(S_{t+1},a’)|S_t=s,A_t=a\\right] \\\\\u0026=\\sum_{s’,r}p(s’,r|s,a)\\left[r+\\gamma\\max_{a’}q_*(s’,a’)\\right] \\end{align}\nBackup diagram for $v_*$ and $q_*$ Figure 3: Backup diagram of optimal value functions References [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] David Silver. UCL course on RL.\n[3] Lilian Weng A (Long) Peek into Reinforcement Learning. Lil’Log, 2018.\n[4] AlphaGo. Deepmind.\n","wordCount":"925","inLanguage":"en","datePublished":"2021-06-27T08:00:00+07:00","dateModified":"2021-06-27T08:00:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/mdp-bellman-eqn/"},"publisher":{"@type":"Organization","name":"Trung's Place","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Trung's Place (Alt + H)"><img src=https://trunghng.github.io/images/others/pokeball.png alt aria-label=logo height=27>Trung's Place</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Markov Decision Processes, Bellman equations</h1><div class=post-meta><span title='2021-06-27 08:00:00 +0700 +0700'>June 27, 2021</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-reinforcement-learning>What is Reinforcement Learning?</a></li><li><a href=#markov-decision-processes-mdps>Markov Decision Processes (MDPs)</a><ul><li><a href=#return>Return</a></li><li><a href=#policy>Policy</a></li><li><a href=#value-function>Value Function</a></li><li><a href=#optimal-policy-and-optimal-value-function>Optimal Policy and Optimal Value Function</a></li></ul></li><li><a href=#bellman-equations>Bellman Equations</a><ul><li><a href=#bellman-backup-diagram>Bellman Backup Diagram</a></li><li><a href=#bellman-optimality-equations>Bellman Optimality Equations</a></li><li><a href=#backup-diagram-for-v_-and-q_>Backup diagram for $v_*$ and $q_*$</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>You may have known or heard vaguely about a computer program called <strong>AlphaGo</strong> - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called <strong>self-play</strong> against its other instances, with <strong>Reinforcement Learning</strong>.</p></blockquote><h2 id=what-is-reinforcement-learning>What is Reinforcement Learning?<a hidden class=anchor aria-hidden=true href=#what-is-reinforcement-learning>#</a></h2><p>Say, there is an unknown <strong>environment</strong> that we&rsquo;re trying to put an <strong>agent</strong> on. By interacting with the <strong>agent</strong> through taking <strong>actions</strong> that gives rise to <strong>rewards</strong> continually, the <strong>agent</strong> learns a <strong>policy</strong> that maximize the cumulative <strong>rewards</strong>.<br><strong>Reinforcement Learning (RL)</strong>, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called <strong>policy</strong>) for the <strong>agent</strong> from experimental trials and relative simple feedback received. With the optimal <strong>policy</strong>, the <strong>agent</strong> is capable to actively adapt to the environment to maximize future <strong>rewards</strong>.<figure><img src=/images/mdp-bellman-eqn/robot.png alt=RL style=display:block;margin-left:auto;margin-right:auto;width:450px;height:330px><figcaption style=text-align:center;font-style:italic></figcaption></figure></p><h2 id=markov-decision-processes-mdps>Markov Decision Processes (MDPs)<a hidden class=anchor aria-hidden=true href=#markov-decision-processes-mdps>#</a></h2><p><strong>Markov decision processes (MDPs)</strong> formally describe an environment for <strong>RL</strong>. And almost all <strong>RL</strong> problems can be formalized as <strong>MDPs</strong>.</p><p><strong>Definition (MDP)</strong><br>A <strong>Markov Decision Process</strong> is a tuple $⟨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma⟩$</p><ul><li>$\mathcal{S}$ is a set of states called <em>state space</em></li><li>$\mathcal{A}$ is a set of actions called <em>action space</em></li><li>$\mathcal{P}$ is a state transition probability matrix<br>$$\mathcal{P}^a_{ss&rsquo;}=P(S_{t+1}=s&rsquo;|S_t=s,A_t=a)$$</li><li>$\mathcal{R}$ is a reward function<br>$$\mathcal{R}^a_s=\mathbb{E}\left[R_{t+1}|S_t=s,A_t=a\right]$$</li><li>$\gamma\in[0, 1]$ is a discount factor for future reward</li></ul><p><strong>MDP</strong> is an extension of a <a href=https://trunghng.github.io/posts/probability-statistics/markov-chain/><strong>Markov chain</strong></a>. If only one action exists for each state, and all rewards are the same, an <strong>MDP</strong> reduces to a <em>Markov chain</em>. All states in <strong>MDP</strong> has <a href=https://trunghng.github.io/posts/probability-statistics/markov-chain/#markov-property>Markov property</a>, referring to the fact that the current state captures all relevant information from the history.
\begin{equation}
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\dots,S_t)
\end{equation}</p><h3 id=return>Return<a hidden class=anchor aria-hidden=true href=#return>#</a></h3><p>In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the <strong>expected return</strong>.</p><p><strong>Definition</strong> (<em>Return</em>)<br>The <strong>return</strong> $G_t$ is the total discounted reward from $t$
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called <strong>discount rate</strong> (or <strong>discount factor</strong>).</p><p>The discount rate $\gamma$ determines the present value of future rewards: a reward received
k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.</p><h3 id=policy>Policy<a hidden class=anchor aria-hidden=true href=#policy>#</a></h3><p><strong>Policy</strong>, which is denoted as $\pi$, is the behavior function of the agent. $\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either <em>deterministic</em> or <em>stochastic</em>.</p><ul><li><em>Deterministic policy</em>: $\quad\pi(s)=a$</li><li><em>Stochastic policy</em>: $\quad\pi(a|s)=P(A_t=a|S_t=s)$</li></ul><h3 id=value-function>Value Function<a hidden class=anchor aria-hidden=true href=#value-function>#</a></h3><p><strong>Value function</strong> measures <em>how good</em> a particular state is (or <em>how good</em> it is to perform a given action in a given state).</p><p><strong>Definition</strong> (<em>state-value function</em>)<br>The <strong>state-value function</strong> of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]
\end{equation}</p><p><strong>Definition</strong> (<em>action-value function</em>)<br>Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
\end{equation}</p><p>Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}</p><h3 id=optimal-policy-and-optimal-value-function>Optimal Policy and Optimal Value Function<a hidden class=anchor aria-hidden=true href=#optimal-policy-and-optimal-value-function>#</a></h3><p>For finite MDPs (finite state and action space), we can precisely define an <strong>optimal policy</strong>. <em>Value functions</em> define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi&rsquo;$ if its expected return is greater than or equal to that of $\pi&rsquo;$ for all states. In other words,
\begin{equation}
\pi\geq\pi&rsquo;\iff v_\pi(s)\geq v_{\pi&rsquo;} \forall s\in\mathcal{S}
\end{equation}</p><p><strong>Theorem</strong> (<em>Optimal policy</em>)<br>For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal to all other policies,
\begin{equation}
\pi_*\geq\pi,\forall\pi
\end{equation}</p><p>The proof of the above theorem is provided in another <a href=https://trunghng.github.io/posts/reinforcement-learning/optimal-policy-existence/>note</a> since we need some additional tools to do that.</p><p>There may be more than one <strong>optimal policy</strong>, they share the same <em>state-value function</em>, called <strong>optimal state-value function</strong> though.
\begin{equation}
v_*(s)=\max_{\pi}v_\pi(s)
\end{equation}
<strong>Optimal policies</strong> also share the same <em>action-value function</em>, called <strong>optimal action-value function</strong>
\begin{equation}
q_*(s,a)=\max_{\pi}q_\pi(s,a)
\end{equation}</p><h2 id=bellman-equations>Bellman Equations<a hidden class=anchor aria-hidden=true href=#bellman-equations>#</a></h2><p>A fundamental property of <em>value functions</em> used throughout RL is that they satisfy recursive relationships
\begin{align}
v_\pi(s)&\doteq \mathbb{E}_\pi[G_t|S_t=s] \\&=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s] \\&=\sum_{s&rsquo;,r,g&rsquo;,a}p(s&rsquo;,r,g&rsquo;,a|s)(r+\gamma g&rsquo;) \\&=\sum_{a}p(a|s)\sum_{s&rsquo;,r,g&rsquo;}p(s&rsquo;,r,g&rsquo;|a,s)(r+\gamma g&rsquo;) \\&=\sum_{a}\pi(a|s)\sum_{s&rsquo;,r,g&rsquo;}p(s&rsquo;,r|a,s)p(g&rsquo;|s&rsquo;,r,a,s)(r+\gamma g&rsquo;) \\&=\sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|a,s)\sum_{g&rsquo;}p(g&rsquo;|s&rsquo;)(r+\gamma g&rsquo;) \\&=\sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|a,s)\left[r+\gamma\sum_{g&rsquo;}p(g&rsquo;|s&rsquo;)g&rsquo;\right] \\&=\sum_{a}\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|a,s)\left[r+\gamma v_\pi(s&rsquo;)\right],
\end{align}
where $p(s&rsquo;,r|s,a)=P(S_{t+1}=s&rsquo;,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the <strong>Bellman equation for</strong> $v_\pi(s)$. It expresses a relationship between the value state $s$, $v_\pi(s)$ and the values of its successor states $s&rsquo;$, $v_\pi(s&rsquo;)$.</p><p>Similarly, we define the <strong>Bellman equation for</strong> $q_\pi(s,a)$
\begin{align}
q_\pi(s,a)&\doteq\mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\&=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\&=\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma\sum_{a&rsquo;}\pi(a&rsquo;|s&rsquo;)q_\pi(s&rsquo;,a&rsquo;)\right]
\end{align}</p><h3 id=bellman-backup-diagram>Bellman Backup Diagram<a hidden class=anchor aria-hidden=true href=#bellman-backup-diagram>#</a></h3><p>Backup diagram of <em>state-value function</em> and <em>action-value function</em> respectively<div style=display:flex><figure><img src=/images/mdp-bellman-eqn/state.png alt=State style=float:left;margin-left:auto;margin-right:auto;width:auto;height:210px><figcaption style=text-align:center;font-style:italic><b>Figure 1</b>: Backup diagram of state-value function</figcaption></figure><figure><img src=/images/mdp-bellman-eqn/action.png alt=Action style=float:left;margin-left:auto;margin-right:auto;width:auto;height:210px><figcaption style=text-align:center;font-style:italic><b>Figure 2</b>: Backup diagram of action-value function</figcaption></figure></div></p><h3 id=bellman-optimality-equations>Bellman Optimality Equations<a hidden class=anchor aria-hidden=true href=#bellman-optimality-equations>#</a></h3><p>Since $v_*$ is the value function for a policy, it must satisfy the <em>Bellman equation for state-values</em>. Moreover, it is also the optimal value function, then we have
\begin{align}
v_*(s)&=\max_{a\in\mathcal{A(s)}}q_{\pi_*}(s,a) \\&=\max_{a}\mathbb{E}_{\pi_*}[G_t|S_t=s,A_t=a] \\&=\max_{a}\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\&=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\&=\max_{a}\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)[r+\gamma v_*(s&rsquo;)]
\end{align}
The last two equations are two forms of the <em>Bellman optimality equation for</em> $v_*$. Similarly, we have the <em>Bellman optimality equation for</em> $q_*$
\begin{align}
q_*(s,a)&=\mathbb{E}\left[R_{t+1}+\gamma\max_{a&rsquo;}q_*(S_{t+1},a&rsquo;)|S_t=s,A_t=a\right] \\&=\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\left[r+\gamma\max_{a&rsquo;}q_*(s&rsquo;,a&rsquo;)\right]
\end{align}</p><h3 id=backup-diagram-for-v_-and-q_>Backup diagram for $v_*$ and $q_*$<a hidden class=anchor aria-hidden=true href=#backup-diagram-for-v_-and-q_>#</a></h3><figure><img src=/images/mdp-bellman-eqn/opt.png alt="Backup diagram for optimal value functions" style=display:block;margin-left:auto;margin-right:auto;width:630px;height:210px><figcaption style=text-align:center;font-style:italic><b>Figure 3</b>: Backup diagram of optimal value functions</figcaption></figure><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p><p>[2] David Silver. <a href=https://www.davidsilver.uk/teaching/>UCL course on RL</a>.</p><p>[3] Lilian Weng <a href=https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html>A (Long) Peek into Reinforcement Learning</a>. Lil&rsquo;Log, 2018.</p><p>[4] <a href=https://deepmind.com/research/case-studies/alphago-the-story-so-far>AlphaGo</a>. Deepmind.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/bellman-equation/>bellman-equation</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/measure-theory/measure/><span class=title>« Prev</span><br><span>Measures</span></a>
<a class=next href=https://trunghng.github.io/posts/probability-statistics/markov-chain/><span class=title>Next »</span><br><span>Markov Chain</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on twitter" href="https://twitter.com/intent/tweet/?text=Markov%20Decision%20Processes%2c%20Bellman%20equations&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f&hashtags=reinforcement-learning%2cbellman-equation%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f&title=Markov%20Decision%20Processes%2c%20Bellman%20equations&summary=Markov%20Decision%20Processes%2c%20Bellman%20equations&source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f&title=Markov%20Decision%20Processes%2c%20Bellman%20equations"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on whatsapp" href="https://api.whatsapp.com/send?text=Markov%20Decision%20Processes%2c%20Bellman%20equations%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Markov Decision Processes, Bellman equations on telegram" href="https://telegram.me/share/url?text=Markov%20Decision%20Processes%2c%20Bellman%20equations&url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fmdp-bellman-eqn%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://trunghng.github.io>Trung's Place</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>