<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Planning & Learning | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="reinforcement-learning,dyna,q-learning,my-rl"><meta name=description content="
Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/planning-learning/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.e9f4bcad0f9fc853201ee998afd06c07a01cb19320ff7cb62155b43ffdb33cea.css integrity="sha256-6fS8rQ+fyFMgHumYr9BsB6AcsZMg/3y2IVW0P/2zPOo=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://trunghng.github.io/posts/reinforcement-learning/planning-learning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")</script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:active{box-shadow:none;color:#4682b4}.post-content a:hover{color:skyblue}.post-content a:visited{color:#00008b}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}.roman-list,.number-list,.alpha-list{counter-reset:section;margin-bottom:10px}.roman-list>li{list-style:none;position:relative}.number-list>li{list-style:none;position:relative}.alpha-list>li{list-style:none;position:relative}.roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}.number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}.alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}#non-style-list{margin-bottom:10px;margin-left:0}#non-style-list>li{position:relative}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script async src="https://www.googletagmanager.com/gtag/js?id=G-GF0KK4E3F0"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GF0KK4E3F0")}</script><meta property="og:title" content="Planning & Learning"><meta property="og:description" content="
Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/planning-learning/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-19T14:09:00+07:00"><meta property="article:modified_time" content="2022-05-19T14:09:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Planning & Learning"><meta name=twitter:description content="
Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Planning \u0026 Learning","item":"https://trunghng.github.io/posts/reinforcement-learning/planning-learning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Planning \u0026 Learning","name":"Planning \u0026 Learning","description":" Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.\n","keywords":["reinforcement-learning","dyna","q-learning","my-rl"],"articleBody":" Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.\nModels \u0026 Planning Models A model of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.\nWhen the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.\nIf a model produces a description of all possibilities and their probabilities, we call it distribution model. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin. On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it sample model. Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to simulate the environment in order to produce simulated experience.\nPlanning Planning in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment \\begin{equation} \\text{model}\\hspace{0.5cm}\\xrightarrow[]{\\hspace{1cm}\\text{planning}\\hspace{1cm}}\\hspace{0.5cm}\\text{policy} \\end{equation} There are two types of planning:\nState-space planning is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas: Involving computing value functions as a key intermediate step toward improving the policy. Computing value functions by updates or backup applied to simulated experience. \\begin{equation} \\text{model}\\xrightarrow[]{\\hspace{1.5cm}}\\text{simulated experience}\\xrightarrow[]{\\hspace{0.3cm}\\text{backups}\\hspace{0.3cm}}\\text{backups}\\xrightarrow[]{\\hspace{1.5cm}}\\text{policy} \\end{equation} Plan-space planning is a search through the space of plans. Plan-space planning methods consist of evolutionary methods and partial-order planning, in which the ordering of steps is not completely determined at all states of planning. Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.\nFor instance, following is pseudocode of a planning method, called random-sample one-step tabular Q-planning, based on one-step tabular Q-learning, and on random samples from a sample model.\nDyna Within a planning agent, experience plays at least two roles:\nmodel learning: improving the model; direct reinforcement learning (RL): improving the value function and policy The figure below illustrates the possible relationships between experience, model, value functions and policy.\nFigure 1: (taken from RL book) The possible relationships between experience, model, values and policy. Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called indirect RL), which involved in planning.\ndirect RL: simpler, not affected by bad models; indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions. Dyna-Q Dyna-Q is the method having all of the processes shown in the diagram in Figure 1 - planning, acting, model-learning and direct RL - all occurring continually:\nthe planning method is the random-sample one-step tabular Q-planning in the previous section; the direct RL method is the one-step tabular Q-learning; the model-learning method is also table-based and assumes the environment is deterministic. After each transition $S_t,A_t\\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.\nDuring planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.\nFollowing is the general architecture of Dyna methods, of which Dyna-Q is an instance.\nFigure 2: (taken from RL book) The general Dyna Architecture. In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.\nPseudocode of Dyna-Q method is shown below.\nExample (This example is taken from RL book - example 8.1.)\nConsider a gridworld with some obstacles, called “maze” in this example, shown in the figure below.\nFigure 3: (taken from RL book) A maze with some obstacles. As usual, four action, $\\text{up}, \\text{down}, \\text{right}$ and $\\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state.\nThe task is discounted, episodic with $\\gamma=0.95$.\nFigure 4: Using Dyna-Q with different setting of number of planning steps on the maze. The code can be found here. Dyna-Q+ Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.\nFigure 5: (taken from RL book) A maze before and after change. With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps.\nIn this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an exploration bonus:\nKeeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment. An special bonus reward is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting: \\begin{equation} r+\\kappa\\sqrt{\\tau}, \\end{equation} for a small (time weight) $\\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\\tau$ time steps. The agent actually plans how to visit long unvisited state-action pairs. The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.\nFigure 6: Average performance of Dyna-Q and Dyna-Q+ on blocking maze. The code can be found here. We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.\nFigure 7: (taken from RL book) A maze before and after change. Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.\nFigure 8: Average performance of Dyna-Q and Dyna-Q+ on shortcut maze. The code can be found here. It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).\nThe reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.\nPrioritized Sweeping Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.\nPseudocode of prioritized sweeping is shown below.\nFigure 9: Using prioritized sweeping on mazes. The code can be found here. Heuristic Search Preferences [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Richard S. Sutton. Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.\n[3] Harm van Seijen \u0026 Richard S. Sutton. Efficient planning in MDPs by small backups. Proceedings of the 30th International Conference on Machine Learning (ICML 2013), 2013.\n[4] Shangtong Zhang. Reinforcement Learning: An Introduction implementation. Github.\nFootnotes","wordCount":"1490","inLanguage":"en","datePublished":"2022-05-19T14:09:00+07:00","dateModified":"2022-05-19T14:09:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/planning-learning/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io/ accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.png alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Planning & Learning</h1><div class=post-meta><span title='2022-05-19 14:09:00 +0700 +0700'>May 19, 2022</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#models-planning>Models & Planning</a><ul><li><a href=#models>Models</a></li><li><a href=#planning>Planning</a></li></ul></li><li><a href=#dyna>Dyna</a><ul><li><a href=#dyna-q>Dyna-Q</a><ul><li><a href=#dyna-q-eg>Example</a></li></ul></li><li><a href=#dyna-q-plus>Dyna-Q+</a></li></ul></li><li><a href=#prioritized-sweeping>Prioritized Sweeping</a></li><li><a href=#heuristic-search>Heuristic Search</a></li><li><a href=#preferences>Preferences</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>Recall that when using <a href=https://trunghng.github.io/posts/reinforcement-learning/dp-in-mdp/>dynamic programming (DP) method</a> in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with <a href=https://trunghng.github.io/posts/reinforcement-learning/monte-carlo-in-rl/>Monte Carlo methods</a> and <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/>temporal-difference learning</a>, the models are unnecessary. Such methods with requirement of a model like the case of DP is called <strong>model-based</strong>, while methods without using a model is called <strong>model-free</strong>. Model-based methods primarily rely on <strong>planning</strong>; and model-free methods, on the other hand, primarily rely on <strong>learning</strong>.</p></blockquote><h2 id=models-planning>Models & Planning<a hidden class=anchor aria-hidden=true href=#models-planning>#</a></h2><h3 id=models>Models<a hidden class=anchor aria-hidden=true href=#models>#</a></h3><p>A <strong>model</strong> of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.</p><p>When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.</p><ul><li>If a model produces a description of all possibilities and their probabilities, we call it <strong>distribution model</strong>. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.</li><li>On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it <strong>sample model</strong>.</li></ul><p>Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to <strong>simulate</strong> the environment in order to produce <strong>simulated experience</strong>.</p><h3 id=planning>Planning<a hidden class=anchor aria-hidden=true href=#planning>#</a></h3><p><strong>Planning</strong> in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:</p><ul><li><strong>State-space planning</strong> is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:<ul><li>Involving computing value functions as a key intermediate step toward improving the policy.</li><li>Computing value functions by updates or backup applied to simulated experience.
\begin{equation}
\text{model}\xrightarrow[]{\hspace{1.5cm}}\text{simulated experience}\xrightarrow[]{\hspace{0.3cm}\text{backups}\hspace{0.3cm}}\text{backups}\xrightarrow[]{\hspace{1.5cm}}\text{policy}
\end{equation}</li></ul></li><li><strong>Plan-space planning</strong> is a search through the space of plans.<ul><li>Plan-space planning methods consist of <strong>evolutionary methods</strong> and <strong>partial-order planning</strong>, in which the ordering of steps is not completely determined at all states of planning.</li></ul></li></ul><p>Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.</p><p>For instance, following is pseudocode of a planning method, called <strong>random-sample one-step tabular Q-planning</strong>, based on <a href=https://trunghng.github.io/posts/reinforcement-learning/td-learning/#q-learning>one-step tabular Q-learning</a>, and on random samples from a sample model.</p><figure><img src=/images/planning-learning/rand-samp-one-step-q-planning.png alt="Random-sample one-step Q-planning"></figure><h2 id=dyna>Dyna<a hidden class=anchor aria-hidden=true href=#dyna>#</a></h2><p>Within a planning agent, experience plays at least two roles:</p><ul><li><strong>model learning</strong>: improving the model;</li><li><strong>direct reinforcement learning (RL)</strong>: improving the value function and policy</li></ul><p>The figure below illustrates the possible relationships between experience, model, value functions and policy.</p><figure><img src=/images/planning-learning/exp-model-value-policy.png alt="Exp, model, values and policy relationships" width=45% height=45%><figcaption><b>Figure 1</b>: (taken from <a href=#rl-book>RL book</a>) <b>The possible relationships between experience, model, values and policy</b>.</figcaption></figure><p>Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called <strong>indirect RL</strong>), which involved in planning.</p><ul><li>direct RL: simpler, not affected by bad models;</li><li>indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions.</li></ul><h3 id=dyna-q>Dyna-Q<a hidden class=anchor aria-hidden=true href=#dyna-q>#</a></h3><p><strong>Dyna-Q</strong> is the method having all of the processes shown in the diagram in <em><strong>Figure 1</strong></em> - planning, acting, model-learning and direct RL - all occurring continually:</p><ul><li>the <em>planning</em> method is the random-sample one-step tabular Q-planning in the previous section;</li><li>the <em>direct RL</em> method is the one-step tabular Q-learning;</li><li>the <em>model-learning</em> method is also table-based and assumes the environment is deterministic.</li></ul><p>After each transition $S_t,A_t\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.</p><p>During planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.</p><p>Following is the general architecture of Dyna methods, of which Dyna-Q is an instance.</p><figure><img src=/images/planning-learning/dyna-arch.png alt="Dyna architecture" width=50% height=50%><figcaption style=text-align:center><b>Figure 2</b>: (taken from <a href=#rl-book>RL book</a>) <b>The general Dyna Architecture</b>.</figcaption></figure><p>In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.</p><p>Pseudocode of Dyna-Q method is shown below.</p><figure><img src=/images/planning-learning/tabular-dyna-q.png alt="Tabular Dyna-Q"></figure><h4 id=dyna-q-eg>Example<a hidden class=anchor aria-hidden=true href=#dyna-q-eg>#</a></h4><p>(This example is taken from <a href=#rl-book>RL book</a> - example 8.1.)</p><p>Consider a gridworld with some obstacles, called &ldquo;maze&rdquo; in this example, shown in the figure below.</p><figure><img src=/images/planning-learning/dyna-maze.png alt="Dyna maze" width=50% height=50%><figcaption style=text-align:center><b>Figure 3</b>: (taken from <a href=#rl-book>RL book</a>) <b>A maze with some obstacles</b>.</figcaption></figure><p>As usual, four action, $\text{up}, \text{down}, \text{right}$ and $\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state.</p><p>The task is discounted, episodic with $\gamma=0.95$.</p><figure><img src=/images/planning-learning/dyna-maze-dyna-q.png alt="Dyna maze solved with Dyna-Q" width=80% height=80%><figcaption><b>Figure 4</b>: <b>Using Dyna-Q with different setting of number of planning steps on the maze.</b> The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py target=_blank>here</a>.</figcaption></figure><h3 id=dyna-q-plus>Dyna-Q+<a hidden class=anchor aria-hidden=true href=#dyna-q-plus>#</a></h3><p>Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.</p><figure><img src=/images/planning-learning/blocking-maze.png alt="Blocking maze" width=80% height=80%><figcaption style=text-align:center><b>Figure 5</b>: (taken from <a href=#rl-book>RL book</a>) <b>A maze before and after change</b>.</figcaption></figure><p>With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps.</p><p>In this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an <strong>exploration bonus</strong>:</p><ul><li>Keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment.</li><li>An special <strong>bonus reward</strong> is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting:
\begin{equation}
r+\kappa\sqrt{\tau},
\end{equation}
for a small (time weight) $\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\tau$ time steps.</li><li>The agent actually plans how to visit long unvisited state-action pairs.</li></ul><p>The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.</p><figure><img src=/images/planning-learning/blocking-maze-dyna-q-qplus.png alt="Dyna-Q, Dyna-Q+ on blocking maze" width=80% height=80%><figcaption><b>Figure 6</b>: <b>Average performance of Dyna-Q and Dyna-Q+ on blocking maze.</b> The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py target=_blank>here</a>.</figcaption></figure><p>We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.</p><figure><img src=/images/planning-learning/shortcut-maze.png alt="shortcut maze" width=80% height=80%><figcaption style=text-align:center><b>Figure 7</b>: (taken from <a href=#rl-book>RL book</a>) <b>A maze before and after change</b>.</figcaption></figure><p>Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.</p><figure><img src=/images/planning-learning/shortcut-maze-dyna-q-qplus.png alt="Dyna-Q, Dyna-Q+ on blocking maze" width=80% height=80%><figcaption><b>Figure 8</b>: <b>Average performance of Dyna-Q and Dyna-Q+ on shortcut maze</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py target=_blank>here</a>.</figcaption></figure><p>It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).</p><p>The reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.</p><h2 id=prioritized-sweeping>Prioritized Sweeping<a hidden class=anchor aria-hidden=true href=#prioritized-sweeping>#</a></h2><p>Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.</p><p>Pseudocode of prioritized sweeping is shown below.</p><figure><img src=/images/planning-learning/prioritized-sweeping.png alt="Prioritized sweeping"></figure><figure><img src=/images/planning-learning/dyna-maze-prioritized-sweeping.png alt="Prioritized sweeping on dyna maze" width=80% height=80%><figcaption style=text-align:center><b>Figure 9</b>: <b>Using prioritized sweeping on mazes</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py target=_blank>here</a>.</figcaption></figure><h2 id=heuristic-search>Heuristic Search<a hidden class=anchor aria-hidden=true href=#heuristic-search>#</a></h2><h2 id=preferences>Preferences<a hidden class=anchor aria-hidden=true href=#preferences>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</span></p><p>[2] Richard S. Sutton. <a href=https://doi.org/10.1016/B978-1-55860-141-3.50030-4>Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming</a>. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.</p><p>[3] Harm van Seijen & Richard S. Sutton. <a href=https://proceedings.mlr.press/v28/vanseijen13.pdf>Efficient planning in MDPs by small backups</a>. Proceedings
of the 30th International Conference on Machine Learning (ICML 2013), 2013.</p><p>[4] Shangtong Zhang. <a href=https://github.com/ShangtongZhang/reinforcement-learning-an-introduction>Reinforcement Learning: An Introduction implementation</a>. Github.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>Reinforcement-Learning</a></li><li><a href=https://trunghng.github.io/tags/dyna/>Dyna</a></li><li><a href=https://trunghng.github.io/tags/q-learning/>Q-Learning</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>My-Rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/likelihood-ratio-pg-is/><span class=title>« Prev</span><br><span>Likelihood Ratio Policy Gradient via Importance Sampling</span>
</a><a class=next href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/><span class=title>Next »</span><br><span>Policy Gradient Theorem</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on x" href="https://x.com/intent/tweet/?text=Planning%20%26%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f&amp;hashtags=reinforcement-learning%2cdyna%2cq-learning%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f&amp;title=Planning%20%26%20Learning&amp;summary=Planning%20%26%20Learning&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f&title=Planning%20%26%20Learning"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on whatsapp" href="https://api.whatsapp.com/send?text=Planning%20%26%20Learning%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on telegram" href="https://telegram.me/share/url?text=Planning%20%26%20Learning&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Planning & Learning on ycombinator" href="https://news.ycombinator.com/submitlink?t=Planning%20%26%20Learning&u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fplanning-learning%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io/>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>