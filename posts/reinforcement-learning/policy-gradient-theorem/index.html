<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Policy Gradient Theorem | Littleroot</title>
<script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><meta name=keywords content="reinforcement-learning,policy-gradient,actor-critic,function-approximation,my-rl"><meta name=description content="
So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.
"><meta name=author content="Trung H. Nguyen"><link rel=canonical href=https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://trunghng.github.io/images/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://trunghng.github.io/images/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://trunghng.github.io/images/favicon/favicon-32x32.png><link rel=apple-touch-icon href=https://trunghng.github.io/images/favicon/apple-touch-icon.png><link rel=mask-icon href=https://trunghng.github.io/images/favicon/android-chrome-512x512.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type=text/x-mathjax-config>
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><style>.post-content{text-align:justify;font-size:15px;font-family:"goudy bookletter 1911",sans-serif}.post-content h1,h2,h3,h4,h5,h6{text-align:left}.post-content a,.post-content a:link,.post-content a:visited,.post-content a:hover,.post-content a:active{box-shadow:none;font-weight:700;color:#4682b4}.post-content ol,.post-content ul{margin-left:10px}.post-content li>ol,.post-content li>ul{margin-left:30px}#roman-list,#number-list,#alpha-list{counter-reset:section;margin-bottom:10px}#roman-list>li{list-style:none;position:relative}#number-list>li{list-style:none;position:relative}#alpha-list>li{list-style:none;position:relative}#roman-list>li:before{counter-increment:section;content:"(" counter(section,lower-roman)") ";position:absolute;left:-2em}#number-list>li:before{counter-increment:section;content:"(" counter(section,decimal)") ";position:absolute;left:-2em}#alpha-list>li:before{counter-increment:section;content:"(" counter(section,lower-alpha)") ";position:absolute;left:-2em}.toc{font-size:15px}.post-footer{font-size:15px}.post-content figure>img{display:block;margin-left:auto;margin-right:auto}.post-content figure>figcaption{all:revert;text-align:justify;font-size:12px;font-style:italic;width:70%;margin-left:15%}.post-content figure>figcaption>p{all:revert}.post-content h3{font-size:28px}.post-content h4{font-size:24px}.post-content h5{font-size:20px}.post-content h6{font-size:16px}</style><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Policy Gradient Theorem"><meta property="og:description" content="
So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.
"><meta property="og:type" content="article"><meta property="og:url" content="https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/"><meta property="og:image" content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-04T14:00:00+07:00"><meta property="article:modified_time" content="2022-05-04T14:00:00+07:00"><meta property="og:site_name" content="Littleroot"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://trunghng.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Policy Gradient Theorem"><meta name=twitter:description content="
So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Posts","item":"https://trunghng.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Policy Gradient Theorem","item":"https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Policy Gradient Theorem","name":"Policy Gradient Theorem","description":" So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\\boldsymbol{\\theta}$, that can select actions without consulting a value function by updating $\\boldsymbol{\\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\\boldsymbol{\\theta}$. Such methods are called policy gradient methods.\n","keywords":["reinforcement-learning","policy-gradient","actor-critic","function-approximation","my-rl"],"articleBody":" So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\\boldsymbol{\\theta}$, that can select actions without consulting a value function by updating $\\boldsymbol{\\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\\boldsymbol{\\theta}$. Such methods are called policy gradient methods.\nPolicy approximation In policy gradient methods, the policy $\\pi$ can be parameterized in any way, as long as $\\pi(a\\vert s,\\boldsymbol{\\theta})$ is differentiable w.r.t $\\boldsymbol{\\theta}$.\nFor discrete action space $\\mathcal{A}$, a common choice of parameterization is to use parameterized numerical preferences $h(s,a,\\boldsymbol{\\theta})\\in\\mathbb{R}$ for each state-action pair. Then, the actions with the highest preferences in each state are given the highest probabilities of being selected, for instance, according to an exponential softmax distribution \\begin{equation} \\pi(a\\vert s,\\boldsymbol{\\theta})\\doteq\\frac{e^{h(s,a,\\boldsymbol{\\theta})}}{\\sum_b e^{h(s,b,\\boldsymbol{\\theta})}} \\end{equation} We refer this policy approximation as softmax in action preferences.\nThe action preferences $h$ can be linear: \\begin{equation} h(s,a,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a), \\end{equation} where $\\mathbf{x}(s,a)\\in\\mathbb{R}^{d’}$ is the feature vector corresponding to state-action pair $(s,a)$. Or $h$ could also be calculated by a neural network.\nPolicy Gradient for Episodic Problems We begin by considering episodic case, for which we define the performance measure $J(\\boldsymbol{\\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have: \\begin{equation} J(\\boldsymbol{\\theta})\\doteq v_{\\pi_\\boldsymbol{\\theta}}(s_0),\\label{eq:pge.1} \\end{equation} where $v_{\\pi_\\boldsymbol{\\theta}}$ is the true value function for $\\pi_\\boldsymbol{\\theta}$, the policy parameterized by $\\boldsymbol{\\theta}$.\nIn policy gradient methods, our goal is to learn a policy $\\pi_{\\boldsymbol{\\theta}^*}$ with a parameter vector $\\boldsymbol{\\theta}^*$ that maximizes the performance measure $J(\\boldsymbol{\\theta})$. Using gradient ascent, we iteratively update $\\boldsymbol{\\theta}$ by \\begin{equation} \\boldsymbol{\\theta}_{t+1}=\\boldsymbol{\\theta}+\\alpha\\nabla J(\\boldsymbol{\\theta}_t), \\end{equation} where $\\alpha\u003e0$ is the learning rate. By \\eqref{eq:pge.1}, it is noticeable that $\\nabla J(\\theta)$ depends on the state distribution, which generates the start state $s_0$, which is unfortunately unknown.\nHowever, the following theorem claims that we can express the gradient $\\nabla J(\\boldsymbol{\\theta})$ in a form not involving the state distribution.\nThe Policy Gradient Theorem Theorem 1: The policy gradient theorem for the episodic case establishes that \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}),\\label{eq:pgte.1} \\end{equation} where $\\pi$ represents the policy corresponding to parameter vector $\\boldsymbol{\\theta}$.\nProof\nWe have that the gradient of the state-value function w.r.t $\\boldsymbol{\\theta}$ can be written in terms of the action-value function, for any $s\\in\\mathcal{S}$, as: \\begin{align} \\hspace{-1.2cm}\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\u0026=\\nabla_\\boldsymbol{\\theta}\\Big[\\sum_a\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\Big],\\hspace{1cm}\\forall s\\in\\mathcal{S} \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}q_\\pi(s,a)\\Big] \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(s|a)q_\\pi(a,s)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}\\sum_{s’,r}p(s’,r|s,a)\\big(r+v_\\pi(s’)\\big)\\Big] \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s’}p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)\\Big] \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s’}p(s’|s,a)\\sum_{a’}\\big(\\nabla_\\boldsymbol{\\theta}\\pi(s’|a’,\\boldsymbol{\\theta})q_\\pi(s’,a’) \\\\ \u0026\\hspace{2cm}+\\pi(a’|s’,\\boldsymbol{\\theta})\\sum_{s''}p(s''\\vert s’,a’)\\nabla_\\boldsymbol{\\theta}v_\\pi(s'')\\big)\\Big] \\\\ \u0026=\\sum_{x\\in\\mathcal{S}}\\sum_{k=0}^{\\infty}P(s\\to x,k,\\pi)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a), \\end{align} After repeated unrolling as in the fifth step, where $P(s\\to x,k,\\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\\pi$. It is then immediate that: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026=\\nabla_\\boldsymbol{\\theta}v_\\pi(s_0) \\\\ \u0026=\\sum_s\\Big(\\sum_{k=0}^{\\infty}P(s_0\\to s,k,\\pi)\\Big)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026=\\sum_s\\eta(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026=\\sum_{s’}\\eta(s’)\\sum_s\\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)}\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026=\\sum_{s’}\\eta(s’)\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\\\ \u0026\\propto\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a), \\end{align} where $\\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode: \\begin{equation} \\eta(s)=h(s)+\\sum_{\\bar{s}}\\eta(\\bar{s})\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s|\\bar{s},a),\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation} where $h(s)$ denotes the probability that an episode begins in each state $s$; $\\bar{s}$ denotes a preceding state of $s$. This leads to the result that we have used in the fifth step: \\begin{equation} \\mu(s)=\\frac{\\eta(s)}{\\sum_{s’}\\eta(s’)},\\hspace{1cm}\\forall s\\in\\mathcal{S} \\end{equation}\nREINFORCE Notice that in Theorem 1, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\\mu(s)$) under the target policy $\\pi$. Therefore, we can rewrite \\eqref{eq:pgte.1} as: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026\\propto\\sum_s\\mu(s)\\sum_a q_\\pi(s,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}) \\\\ \u0026=\\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})\\right]\\label{eq:reinforce.1} \\end{align} Using SGD on maximizing $J(\\boldsymbol{\\theta})$ gives us the update rule: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha\\sum_a\\hat{q}(S_t,a,\\mathbf{w})\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta}), \\end{equation} where $\\hat{q}$ is some learned approximation to $q_\\pi$ with $\\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called all-actions method because its update involves all of the actions.\nContinue our derivation in \\eqref{eq:reinforce.1}, we have: \\begin{align} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026=\\mathbb{E}_\\pi\\left[\\sum_a q_\\pi(S_t,a)\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})\\right] \\\\ \u0026=\\mathbb{E}_\\pi\\left[\\sum_a\\pi(a|S_t,\\boldsymbol{\\theta})q_\\pi(S_t,a)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(a|S_t,\\boldsymbol{\\theta})}{\\pi(a|S_t,\\boldsymbol{\\theta})}\\right] \\\\ \u0026=\\mathbb{E}_\\pi\\left[q_\\pi(S_t,A_t)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta}}\\right] \\\\ \u0026=\\mathbb{E}_\\pi\\left[G_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\right], \\end{align} where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\\sim\\pi$; and in the fourth step, we have used the identity \\begin{equation} \\mathbb{E}_\\pi\\left[G_t|S_t,A_t\\right]=q_\\pi(S_t,A_t) \\end{equation} With this gradient, we have the SGD update for time step $t$, called the REINFORCE update, is then: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha G_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:reinforce.2} \\end{equation} Pseudocode of the algorithm is given below.\nThe vector \\begin{equation} \\frac{\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})}{\\pi(a|s,\\boldsymbol{\\theta})}=\\nabla_\\boldsymbol{\\theta}\\log\\pi(a|s,\\boldsymbol{\\theta}) \\end{equation} in \\eqref{eq:reinforce.2} is called the eligibility vector.\nConsider using soft-max in action preferences with linear action preferences, which means that: \\begin{equation} \\pi(a|s,\\boldsymbol{\\theta})\\doteq\\dfrac{\\exp\\Big[h(s,a,\\boldsymbol{\\theta})\\Big]}{\\sum_b\\exp\\Big[h(s,b,\\boldsymbol{\\theta})\\Big]}, \\end{equation} where the preferences $h(s,a,\\boldsymbol{\\theta})$ is defined as: \\begin{equation} h(s,a,\\boldsymbol{\\theta})=\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a) \\end{equation} Using the chain rule we can rewrite the eligibility vector as: \\begin{align} \\nabla_\\boldsymbol{\\theta}\\log\\pi(a|s,\\boldsymbol{\\theta})\u0026=\\nabla_\\boldsymbol{\\theta}\\log{\\frac{\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a)\\Big]}{\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big]}} \\\\ \u0026=\\nabla_\\boldsymbol{\\theta}\\Big(\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,a)\\Big)-\\nabla_\\boldsymbol{\\theta}\\log\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big] \\\\ \u0026=\\mathbf{x}(s,a)-\\dfrac{\\sum_b\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b)\\Big]\\mathbf{x}(s,b)}{\\sum_{b’}\\exp\\Big[\\boldsymbol{\\theta}^\\text{T}\\mathbf{x}(s,b’)\\Big]} \\\\ \u0026=\\mathbf{x}(s,a)-\\sum_b\\pi(b|s,\\boldsymbol{\\theta})\\mathbf{x}(s,b) \\end{align}\nA result when using REINFORCE to solve the short-corridor problem (RL book, Example 13.1) is shown below.\nFigure 1: REINFORCE on short-corridor problem. The code can be found here. REINFORCE with Baseline The policy gradient theorem \\eqref{eq:pgte.1} can be generalized to include a comparison of the action value to an arbitrary baseline $b(s)$: \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\\propto\\sum_s\\mu(s)\\sum_a\\Big(q_\\pi(s,a)-b(s)\\Big)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta}) \\end{equation} The baseline can be any function, even a r.v, as long as it is independent with $a$. The equation is valid because: \\begin{align} \\sum_a b(s)\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})\u0026=b(s)\\nabla_\\boldsymbol{\\theta}\\sum_a\\pi(a|s,\\boldsymbol{\\theta}) \\\\ \u0026=b(s)\\nabla_\\boldsymbol{\\theta}1=0 \\end{align} Using the derivation steps analogous to REINFORCE, we end up with another version of REINFORCE that includes a general baseline: \\begin{equation} \\boldsymbol{\\theta}_{t+1}\\doteq\\boldsymbol{\\theta}_t+\\alpha\\Big(G_t-b(s)\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:rb.1} \\end{equation} One natural baseline choice is the estimate of the state value, $\\hat{v}(S_t,\\mathbf{w})$, with $\\mathbf{w}\\in\\mathbb{R}^d$ is the weight vector of its. Using this baseline, we have pseudocode of the generalization with baseline of REINFORCE algorithm \\eqref{eq:rb.1} given below.\nAdding a baseline to REINFORCE lets the agent learn much faster, as illustrated in the following figure.\nFigure 2: REINFORCE versus REINFORCE with baseline on short-corridor problem. The code can be found here. Actor-Critic Methods In Reinforcement Learning, methods that learn both policy and value function at the same time are called actor-critic methods, in which actor refers to the learned policy and critic is a reference to the learned value function. Although the REINFORCE with Baseline method in the previous section learns both policy and value function, but it is not an actor-critic method. Because its state-value function is used as a baseline, not as a critic, which is used for bootstrapping.\nWe begin by considering one-step actor-critic methods. One-step actor-critic methods replace the full return, $G_t$, of REINFORCE \\eqref{eq:rb.1} with the one-step return, $G_{t:t+1}$: \\begin{align} \\boldsymbol{\\theta}_{t+1}\u0026\\doteq\\boldsymbol{\\theta}_t+\\alpha\\Big(G_{t:t+1}-\\hat{v}(S_t,\\mathbf{w})\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})}\\label{eq:acm.1} \\\\ \u0026=\\boldsymbol{\\theta}_t+\\alpha\\Big(R_{t+1}+\\hat{v}(S_{t+1},\\mathbf{w})-\\hat{v}(S_t,\\mathbf{w})\\Big)\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})} \\\\ \u0026=\\boldsymbol{\\theta}_t+\\alpha\\delta_t\\frac{\\nabla_\\boldsymbol{\\theta}\\pi(A_t|S_t,\\boldsymbol{\\theta})}{\\pi(A_t|S_t,\\boldsymbol{\\theta})} \\end{align} The natural state-value function learning method to pair with this is semi-gradient TD(0), which produces the pseudocode given below.\nTo generalize the one-step methods to the forward view of $n$-step methods and then to $\\lambda$-return, in \\eqref{eq:acm.1}, we simply replace the one-step return, $G_{t+1}$, by the $n$-step return, $G_{t:t+n}$, and the $\\lambda$-return, $G_t^\\lambda$, respectively.\nIn order to obtain the backward view of the $\\lambda$-return algorithm, we use separately eligible traces for the actor and critic, as in the pseudocode given below.\nPolicy Gradient with Continuing Problems In the continuing tasks, we define the performance measure in terms of average-reward, as: \\begin{align} J(\\boldsymbol{\\theta})\\doteq r(\\pi)\u0026\\doteq\\lim_{h\\to\\infty}\\frac{1}{h}\\sum_{t=1}^{h}\\mathbb{E}\\Big[R_t\\big|S_0,A_{0:1}\\sim\\pi\\Big] \\\\ \u0026=\\lim_{t\\to\\infty}\\mathbb{E}\\Big[R_t|S_0,A_{0:1}\\sim\\pi\\Big] \\\\ \u0026=\\sum_s\\mu(s)\\sum_a\\pi(a|s)\\sum_{s’,r}p(s’,r|s,a)r, \\end{align} where $\\mu$ is the steady-state distribution under $\\pi$, $\\mu(s)\\doteq\\lim_{t\\to\\infty}P(S_t=s|A_{0:t}\\sim\\pi)$ which is assumed to exist and to be independent of $S_0$; and we also have that: \\begin{equation} \\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s’|s,a)=\\mu(s’),\\hspace{1cm}\\forall s’\\in\\mathcal{S} \\end{equation} Recall that in continuing tasks with average-reward setting, we use the differential return, which is defined in terms of differences between rewards and the average reward: \\begin{equation} G_t\\doteq R_{t+1}-r(\\pi)+R_{t+2}-r(\\pi)+R_{t+3}-r(\\pi)+\\dots\\label{eq:pgc.1} \\end{equation} And thus, we also use the differential version of value functions, which are defined as usual except that they use the differential return \\eqref{eq:pgc.1}: \\begin{align} v_\\pi(s)\u0026\\doteq\\mathbb{E}_\\pi\\left[G_t|S_t=s\\right] \\\\ q_\\pi(s,a)\u0026\\doteq\\mathbb{E}_\\pi\\left[G_t|S_t=s,A_t=s\\right] \\end{align}\nThe Policy Gradient Theorem Theorem 2: The policy gradient theorem for continuing case with average-reward states that \\begin{equation} \\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s)q_\\pi(s,a) \\end{equation}\nProof\nWe have that the gradient of the state-value function w.r.t $\\boldsymbol{\\theta}$ can be written, for any $s\\in\\mathcal{S}$, as: \\begin{align} \\hspace{-1cm}\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\u0026=\\boldsymbol{\\theta}\\Big[\\sum_a\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\Big],\\hspace{1cm}\\forall s\\in\\mathcal{S} \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}q_\\pi(s,a)\\Big] \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\nabla_\\boldsymbol{\\theta}\\sum_{s’,r}p(s’,r|s,a)\\big(r-r(\\boldsymbol{\\theta})+v_\\pi(s’)\\big)\\Big] \\\\ \u0026=\\sum_a\\Bigg[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\Big[-\\nabla_\\boldsymbol{\\theta}r(\\boldsymbol{\\theta})+\\sum_{s’}p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)\\Big]\\Bigg] \\end{align} Thus, the gradient of the performance measure w.r.t $\\boldsymbol{\\theta}$ is: \\begin{align} \\hspace{-1cm}\\nabla_\\boldsymbol{\\theta}J(\\boldsymbol{\\theta})\u0026=\\nabla_\\boldsymbol{\\theta}r(\\boldsymbol{\\theta}) \\\\ \u0026=\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s’}p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)\\Big]-\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026=\\sum_s\\mu(s)\\Bigg(\\sum_a\\Big[\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026\\hspace{2cm}+\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s’}p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)\\Big]-\\nabla_\\boldsymbol{\\theta}v_\\pi(s)\\Bigg) \\\\ \u0026=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026\\hspace{2cm}+\\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})\\sum_{s’}p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)\\nonumber \\\\ \u0026\\hspace{2cm}+\\sum_{s’}\\sum_s\\mu(s)\\sum_a\\pi(a|s,\\boldsymbol{\\theta})p(s’|s,a)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a)+\\sum_{s’}\\mu(s’)\\nabla_\\boldsymbol{\\theta}v_\\pi(s’)-\\sum_s\\mu(s)\\nabla_\\boldsymbol{\\theta}v_\\pi(s) \\\\ \u0026=\\sum_s\\mu(s)\\sum_a\\nabla_\\boldsymbol{\\theta}\\pi(a|s,\\boldsymbol{\\theta})q_\\pi(s,a) \\end{align}\nPolicy Parameterization for Continuous Actions For tasks having continuous action space with an infinite number of actions, instead of computing learned probabilities for each action, we can learn statistics of the probability distribution.\nIn particular, to produce a policy parameterization, the policy can be defined as the Normal distribution over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state, as given: \\begin{equation} \\pi(a|s,\\boldsymbol{\\theta})\\doteq\\frac{1}{\\sigma(s,\\boldsymbol{\\theta})\\sqrt{2\\pi}}\\exp\\left(-\\frac{(a-\\mu(s,\\boldsymbol{\\theta}))^2}{2\\sigma(s,\\boldsymbol{\\theta})^2}\\right), \\end{equation} where $\\mu:\\mathcal{S}\\times\\mathbb{R}^{d’}\\to\\mathbb{R}$ and $\\sigma:\\mathcal{S}\\times\\mathbb{R}^{d’}\\to\\mathbb{R}^+$ are two parameterized function approximators.\nWe continue by dividing the policy’s parameter vector, $\\boldsymbol{\\theta}=[\\boldsymbol{\\theta}_\\mu, \\boldsymbol{\\theta}_\\sigma]^\\text{T}$, into two parts: one part, $\\boldsymbol{\\theta}_\\mu$, is used for the approximation of the mean and the other, $\\boldsymbol{\\theta}_\\sigma$, is used for the approximation of the standard deviation.\nThe mean, $\\mu$, can be approximated as a linear function, while the standard deviation, $\\sigma$, must always be positive, which should be approximated as the exponential of a linear function, as: \\begin{align} \\mu(s,\\boldsymbol{\\theta})\u0026\\doteq\\boldsymbol{\\theta}_\\mu^\\text{T}\\mathbf{x}_\\mu(s) \\\\ \\sigma(s,\\boldsymbol{\\theta})\u0026\\doteq\\exp\\Big(\\boldsymbol{\\theta}_\\sigma^\\text{T}\\mathbf{x}_\\sigma(s)\\Big), \\end{align} where $\\mathbf{x}_\\mu(s)$ and $\\mathbf{x}_\\sigma(s)$ are state feature vectors corresponding to each approximator.\nReferences [1] Richard S. Sutton \u0026 Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press, 2018.\n[2] Deepmind x UCL. Reinforcement Learning Lecture Series 2021. Deepmind, 2021.\n[3] Richard S. Sutton \u0026 David McAllester \u0026 Satinder Singh \u0026 Yishay Mansour. Policy Gradient Methods for Reinforcement Learning with Function Approximation. NIPS 1999.\nFootnotes","wordCount":"1518","inLanguage":"en","datePublished":"2022-05-04T14:00:00+07:00","dateModified":"2022-05-04T14:00:00+07:00","author":{"@type":"Person","name":"Trung H. Nguyen"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://trunghng.github.io/posts/reinforcement-learning/policy-gradient-theorem/"},"publisher":{"@type":"Organization","name":"Littleroot","logo":{"@type":"ImageObject","url":"https://trunghng.github.io/images/favicon/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://trunghng.github.io accesskey=h title="Littleroot (Alt + H)"><img src=https://trunghng.github.io/images/others/littleroottown.gif alt aria-label=logo height=27>Littleroot</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://trunghng.github.io/ title=Posts><span>Posts</span></a></li><li><a href=https://trunghng.github.io/archives/ title=Archive><span>Archive</span></a></li><li><a href=https://trunghng.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://trunghng.github.io/about/ title=About><span>About</span></a></li><li><a href=https://trunghng.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Policy Gradient Theorem</h1><div class=post-meta><span title='2022-05-04 14:00:00 +0700 +0700'>May 4, 2022</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Trung H. Nguyen</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#policy-approx>Policy approximation</a></li><li><a href=#policy-grad-ep>Policy Gradient for Episodic Problems</a><ul><li><a href=#policy-grad-theorem-ep>The Policy Gradient Theorem</a></li><li><a href=#reinforce>REINFORCE</a></li><li><a href=#reinforce-baseline>REINFORCE with Baseline</a></li><li><a href=#actor-critic-methods>Actor-Critic Methods</a></li></ul></li><li><a href=#policy-grad-cont>Policy Gradient with Continuing Problems</a><ul><li><a href=#policy-grad-theorem-cont>The Policy Gradient Theorem</a></li></ul></li><li><a href=#policy-prm-cont-actions>Policy Parameterization for Continuous Actions</a></li><li><a href=#references>References</a></li><li><a href=#footnotes>Footnotes</a></li></ul></nav></div></details></div><div class=post-content><blockquote><p>So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a <strong>parameterized policy</strong>, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called <strong>policy gradient methods</strong>.</p></blockquote><h2 id=policy-approx>Policy approximation<a hidden class=anchor aria-hidden=true href=#policy-approx>#</a></h2><p>In <strong>policy gradient</strong> methods, the policy $\pi$ can be parameterized in any way, as long as $\pi(a\vert s,\boldsymbol{\theta})$ is differentiable w.r.t $\boldsymbol{\theta}$.</p><p>For discrete action space $\mathcal{A}$, a common choice of parameterization is to use <strong>parameterized numerical preferences</strong> $h(s,a,\boldsymbol{\theta})\in\mathbb{R}$ for each state-action pair. Then, the actions with the highest preferences in each state are given the highest probabilities of being selected, for instance, according to an exponential softmax distribution
\begin{equation}
\pi(a\vert s,\boldsymbol{\theta})\doteq\frac{e^{h(s,a,\boldsymbol{\theta})}}{\sum_b e^{h(s,b,\boldsymbol{\theta})}}
\end{equation}
We refer this policy approximation as <strong>softmax in action preferences</strong>.</p><p>The action preferences $h$ can be linear:
\begin{equation}
h(s,a,\boldsymbol{\theta})=\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a),
\end{equation}
where $\mathbf{x}(s,a)\in\mathbb{R}^{d&rsquo;}$ is the feature vector corresponding to state-action pair $(s,a)$. Or $h$ could also be calculated by a neural network.</p><h2 id=policy-grad-ep>Policy Gradient for Episodic Problems<a hidden class=anchor aria-hidden=true href=#policy-grad-ep>#</a></h2><p>We begin by considering episodic case, for which we define the performance measure $J(\boldsymbol{\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have:
\begin{equation}
J(\boldsymbol{\theta})\doteq v_{\pi_\boldsymbol{\theta}}(s_0),\label{eq:pge.1}
\end{equation}
where $v_{\pi_\boldsymbol{\theta}}$ is the true value function for $\pi_\boldsymbol{\theta}$, the policy parameterized by $\boldsymbol{\theta}$.</p><p>In policy gradient methods, our goal is to learn a policy $\pi_{\boldsymbol{\theta}^*}$ with a parameter vector $\boldsymbol{\theta}^*$ that maximizes the performance measure $J(\boldsymbol{\theta})$. Using gradient ascent, we iteratively update $\boldsymbol{\theta}$ by
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}+\alpha\nabla J(\boldsymbol{\theta}_t),
\end{equation}
where $\alpha>0$ is the learning rate. By \eqref{eq:pge.1}, it is noticeable that $\nabla J(\theta)$ depends on the state distribution, which generates the start state $s_0$, which is unfortunately unknown.</p><p>However, the following theorem claims that we can express the gradient $\nabla J(\boldsymbol{\theta})$ in a form not involving the state distribution.</p><h3 id=policy-grad-theorem-ep>The Policy Gradient Theorem<a hidden class=anchor aria-hidden=true href=#policy-grad-theorem-ep>#</a></h3><p><span id=theorem1><strong>Theorem 1</strong></span>: <em>The <strong>policy gradient theorem</strong> for the episodic case establishes that</em>
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}),\label{eq:pgte.1}
\end{equation}
<em>where $\pi$ represents the policy corresponding to parameter vector $\boldsymbol{\theta}$.</em></p><p><strong>Proof</strong><br>We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written in terms of the action-value function, for any $s\in\mathcal{S}$, as:
\begin{align}
\hspace{-1.2cm}\nabla_\boldsymbol{\theta}v_\pi(s)&=\nabla_\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(s|a)q_\pi(a,s)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\big(r+v_\pi(s&rsquo;)\big)\Big] \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s&rsquo;}p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)\Big] \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s&rsquo;}p(s&rsquo;|s,a)\sum_{a&rsquo;}\big(\nabla_\boldsymbol{\theta}\pi(s&rsquo;|a&rsquo;,\boldsymbol{\theta})q_\pi(s&rsquo;,a&rsquo;) \\ &\hspace{2cm}+\pi(a&rsquo;|s&rsquo;,\boldsymbol{\theta})\sum_{s''}p(s''\vert s&rsquo;,a&rsquo;)\nabla_\boldsymbol{\theta}v_\pi(s'')\big)\Big] \\ &=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty}P(s\to x,k,\pi)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
After repeated unrolling as in the fifth step, where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$. It is then immediate that:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&=\nabla_\boldsymbol{\theta}v_\pi(s_0) \\ &=\sum_s\Big(\sum_{k=0}^{\infty}P(s_0\to s,k,\pi)\Big)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &=\sum_s\eta(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &=\sum_{s&rsquo;}\eta(s&rsquo;)\sum_s\frac{\eta(s)}{\sum_{s&rsquo;}\eta(s&rsquo;)}\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &=\sum_{s&rsquo;}\eta(s&rsquo;)\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &\propto\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode:
\begin{equation}
\eta(s)=h(s)+\sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|s,\boldsymbol{\theta})p(s|\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
where $h(s)$ denotes the probability that an episode begins in each state $s$; $\bar{s}$ denotes a preceding state of $s$. This leads to the result that we have used in the fifth step:
\begin{equation}
\mu(s)=\frac{\eta(s)}{\sum_{s&rsquo;}\eta(s&rsquo;)},\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}</p><h3 id=reinforce>REINFORCE<a hidden class=anchor aria-hidden=true href=#reinforce>#</a></h3><p>Notice that in <a href=#theorem1>Theorem 1</a>, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\mu(s)$) under the target policy $\pi$. Therefore, we can rewrite \eqref{eq:pgte.1} as:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}) \\ &=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right]\label{eq:reinforce.1}
\end{align}
Using SGD on maximizing $J(\boldsymbol{\theta})$ gives us the update rule:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\sum_a\hat{q}(S_t,a,\mathbf{w})\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta}),
\end{equation}
where $\hat{q}$ is some learned approximation to $q_\pi$ with $\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called <strong>all-actions</strong> method because its update involves all of the actions.</p><p>Continue our derivation in \eqref{eq:reinforce.1}, we have:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right] \\ &=\mathbb{E}_\pi\left[\sum_a\pi(a|S_t,\boldsymbol{\theta})q_\pi(S_t,a)\frac{\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})}{\pi(a|S_t,\boldsymbol{\theta})}\right] \\ &=\mathbb{E}_\pi\left[q_\pi(S_t,A_t)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right] \\ &=\mathbb{E}_\pi\left[G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\right],
\end{align}
where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\sim\pi$; and in the fourth step, we have used the identity
\begin{equation}
\mathbb{E}_\pi\left[G_t|S_t,A_t\right]=q_\pi(S_t,A_t)
\end{equation}
With this gradient, we have the SGD update for time step $t$, called the <strong>REINFORCE</strong> update, is then:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\label{eq:reinforce.2}
\end{equation}
Pseudocode of the algorithm is given below.</p><figure><img src=/images/policy-gradient-theorem/reinforce.png alt=REINFORCE></figure><p>The vector
\begin{equation}
\frac{\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})}{\pi(a|s,\boldsymbol{\theta})}=\nabla_\boldsymbol{\theta}\log\pi(a|s,\boldsymbol{\theta})
\end{equation}
in \eqref{eq:reinforce.2} is called the <strong>eligibility vector</strong>.</p><p>Consider using <strong>soft-max in action preferences</strong> with linear action preferences, which means that:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\dfrac{\exp\Big[h(s,a,\boldsymbol{\theta})\Big]}{\sum_b\exp\Big[h(s,b,\boldsymbol{\theta})\Big]},
\end{equation}
where the preferences $h(s,a,\boldsymbol{\theta})$ is defined as:
\begin{equation}
h(s,a,\boldsymbol{\theta})=\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)
\end{equation}
Using the chain rule we can rewrite the eligibility vector as:
\begin{align}
\nabla_\boldsymbol{\theta}\log\pi(a|s,\boldsymbol{\theta})&=\nabla_\boldsymbol{\theta}\log{\frac{\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)\Big]}{\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big]}} \\ &=\nabla_\boldsymbol{\theta}\Big(\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)\Big)-\nabla_\boldsymbol{\theta}\log\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big] \\ &=\mathbf{x}(s,a)-\dfrac{\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big]\mathbf{x}(s,b)}{\sum_{b&rsquo;}\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b&rsquo;)\Big]} \\ &=\mathbf{x}(s,a)-\sum_b\pi(b|s,\boldsymbol{\theta})\mathbf{x}(s,b)
\end{align}</p><p>A result when using REINFORCE to solve the short-corridor problem (<a href=#rl-book>RL book</a>, Example 13.1) is shown below.</p><figure><img src=/images/policy-gradient-theorem/short-corridor-reinforce.png alt="REINFORCE on short-corridor"><figcaption><b>Figure 1</b>: <b>REINFORCE on short-corridor problem</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-13/short_corridor.py target=_blank>here</a>.</figcaption></figure><h3 id=reinforce-baseline>REINFORCE with Baseline<a hidden class=anchor aria-hidden=true href=#reinforce-baseline>#</a></h3><p>The policy gradient theorem \eqref{eq:pgte.1} can be generalized to include a comparison of the action value to an arbitrary <strong>baseline</strong> $b(s)$:
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a\Big(q_\pi(s,a)-b(s)\Big)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})
\end{equation}
The baseline can be any function, even a r.v, as long as it is independent with $a$. The equation is valid because:
\begin{align}
\sum_a b(s)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})&=b(s)\nabla_\boldsymbol{\theta}\sum_a\pi(a|s,\boldsymbol{\theta}) \\ &=b(s)\nabla_\boldsymbol{\theta}1=0
\end{align}
Using the derivation steps analogous to REINFORCE, we end up with another version of REINFORCE that includes a general baseline:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\Big(G_t-b(s)\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\label{eq:rb.1}
\end{equation}
One natural baseline choice is the estimate of the state value, $\hat{v}(S_t,\mathbf{w})$, with $\mathbf{w}\in\mathbb{R}^d$ is the weight vector of its. Using this baseline, we have pseudocode of the generalization with baseline of REINFORCE algorithm \eqref{eq:rb.1} given below.</p><figure><img src=/images/policy-gradient-theorem/reinforce-baseline.png alt="REINFORCE with Baseline"></figure><p>Adding a baseline to REINFORCE lets the agent learn much faster, as illustrated in the following figure.</p><figure><img src=/images/policy-gradient-theorem/short-corridor-reinforce-baseline.png alt="REINFORCE, REINFORCE with baseline on short-corridor"><figcaption><b>Figure 2</b>: <b>REINFORCE versus REINFORCE with baseline on short-corridor problem</b>. The code can be found <a href=https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-13/short_corridor.py target=_blank>here</a>.</figcaption></figure><h3 id=actor-critic-methods>Actor-Critic Methods<a hidden class=anchor aria-hidden=true href=#actor-critic-methods>#</a></h3><p>In Reinforcement Learning, methods that learn both policy and value function at the same time are called <strong>actor-critic methods</strong>, in which <strong>actor</strong> refers to the learned policy and <strong>critic</strong> is a reference to the learned value function. Although the REINFORCE with Baseline method in the previous section learns both policy and value function, but it is not an actor-critic method. Because its state-value function is used as a baseline, not as a critic, which is used for bootstrapping.</p><p>We begin by considering one-step actor-critic methods. One-step actor-critic methods replace the full return, $G_t$, of REINFORCE \eqref{eq:rb.1} with the one-step return, $G_{t:t+1}$:
\begin{align}
\boldsymbol{\theta}_{t+1}&\doteq\boldsymbol{\theta}_t+\alpha\Big(G_{t:t+1}-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\label{eq:acm.1} \\ &=\boldsymbol{\theta}_t+\alpha\Big(R_{t+1}+\hat{v}(S_{t+1},\mathbf{w})-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})} \\ &=\boldsymbol{\theta}_t+\alpha\delta_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}
\end{align}
The natural state-value function learning method to pair with this is semi-gradient TD(0), which produces the pseudocode given below.</p><figure><img src=/images/policy-gradient-theorem/one-step-actor-critic.png alt="One-step Actor-Critic"></figure><p>To generalize the one-step methods to the forward view of $n$-step methods and then to $\lambda$-return, in \eqref{eq:acm.1}, we simply replace the one-step return, $G_{t+1}$, by the $n$-step return, $G_{t:t+n}$, and the $\lambda$-return, $G_t^\lambda$, respectively.</p><p>In order to obtain the backward view of the $\lambda$-return algorithm, we use separately eligible traces for the actor and critic, as in the pseudocode given below.</p><figure><img src=/images/policy-gradient-theorem/actor-critic-eligible-traces.png alt="Actor-Critic with Eligible Traces"></figure><h2 id=policy-grad-cont>Policy Gradient with Continuing Problems<a hidden class=anchor aria-hidden=true href=#policy-grad-cont>#</a></h2><p>In the continuing tasks, we define the performance measure in terms of <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#avg-reward>average-reward</a>, as:
\begin{align}
J(\boldsymbol{\theta})\doteq r(\pi)&\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}\Big[R_t\big|S_0,A_{0:1}\sim\pi\Big] \\ &=\lim_{t\to\infty}\mathbb{E}\Big[R_t|S_0,A_{0:1}\sim\pi\Big] \\ &=\sum_s\mu(s)\sum_a\pi(a|s)\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)r,
\end{align}
where $\mu$ is the steady-state distribution under $\pi$, $\mu(s)\doteq\lim_{t\to\infty}P(S_t=s|A_{0:t}\sim\pi)$ which is assumed to exist and to be independent of $S_0$; and we also have that:
\begin{equation}
\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s&rsquo;|s,a)=\mu(s&rsquo;),\hspace{1cm}\forall s&rsquo;\in\mathcal{S}
\end{equation}
Recall that in continuing tasks with average-reward setting, we use the <a href=https://trunghng.github.io/posts/reinforcement-learning/func-approx/#differential-return>differential return</a>, which is defined in terms of differences between rewards and the average reward:
\begin{equation}
G_t\doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\dots\label{eq:pgc.1}
\end{equation}
And thus, we also use the differential version of value functions, which are defined as usual except that they use the differential return \eqref{eq:pgc.1}:
\begin{align}
v_\pi(s)&\doteq\mathbb{E}_\pi\left[G_t|S_t=s\right] \\ q_\pi(s,a)&\doteq\mathbb{E}_\pi\left[G_t|S_t=s,A_t=s\right]
\end{align}</p><h3 id=policy-grad-theorem-cont>The Policy Gradient Theorem<a hidden class=anchor aria-hidden=true href=#policy-grad-theorem-cont>#</a></h3><p><strong>Theorem 2</strong>: <em>The policy gradient theorem for continuing case with average-reward states that</em>
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)
\end{equation}</p><p><strong>Proof</strong><br>We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written, for any $s\in\mathcal{S}$, as:
\begin{align}
\hspace{-1cm}\nabla_\boldsymbol{\theta}v_\pi(s)&=\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s&rsquo;,r}p(s&rsquo;,r|s,a)\big(r-r(\boldsymbol{\theta})+v_\pi(s&rsquo;)\big)\Big] \\ &=\sum_a\Bigg[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\Big[-\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta})+\sum_{s&rsquo;}p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)\Big]\Bigg]
\end{align}
Thus, the gradient of the performance measure w.r.t $\boldsymbol{\theta}$ is:
\begin{align}
\hspace{-1cm}\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&=\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta}) \\ &=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s&rsquo;}p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s) \\ &=\sum_s\mu(s)\Bigg(\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\nonumber \\ &\hspace{2cm}+\pi(a|s,\boldsymbol{\theta})\sum_{s&rsquo;}p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s)\Bigg) \\ &=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\nonumber \\ &\hspace{2cm}+\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})\sum_{s&rsquo;}p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\nonumber \\ &\hspace{2cm}+\sum_{s&rsquo;}\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s&rsquo;|s,a)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\sum_{s&rsquo;}\mu(s&rsquo;)\nabla_\boldsymbol{\theta}v_\pi(s&rsquo;)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)
\end{align}</p><h2 id=policy-prm-cont-actions>Policy Parameterization for Continuous Actions<a hidden class=anchor aria-hidden=true href=#policy-prm-cont-actions>#</a></h2><p>For tasks having continuous action space with an infinite number of actions, instead of computing learned probabilities for each action, we can learn statistics of the probability distribution.</p><p>In particular, to produce a policy parameterization, the policy can be defined as the <a href=https://trunghng.github.io/posts/probability-statistics/gaussian-dist-gaussian-bn/>Normal distribution</a> over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state, as given:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\frac{1}{\sigma(s,\boldsymbol{\theta})\sqrt{2\pi}}\exp\left(-\frac{(a-\mu(s,\boldsymbol{\theta}))^2}{2\sigma(s,\boldsymbol{\theta})^2}\right),
\end{equation}
where $\mu:\mathcal{S}\times\mathbb{R}^{d&rsquo;}\to\mathbb{R}$ and $\sigma:\mathcal{S}\times\mathbb{R}^{d&rsquo;}\to\mathbb{R}^+$ are two parameterized function approximators.</p><p>We continue by dividing the policy&rsquo;s parameter vector, $\boldsymbol{\theta}=[\boldsymbol{\theta}_\mu, \boldsymbol{\theta}_\sigma]^\text{T}$, into two parts: one part, $\boldsymbol{\theta}_\mu$, is used for the approximation of the mean and the other, $\boldsymbol{\theta}_\sigma$, is used for the approximation of the standard deviation.</p><p>The mean, $\mu$, can be approximated as a linear function, while the standard deviation, $\sigma$, must always be positive, which should be approximated as the exponential of a linear function, as:
\begin{align}
\mu(s,\boldsymbol{\theta})&\doteq\boldsymbol{\theta}_\mu^\text{T}\mathbf{x}_\mu(s) \\ \sigma(s,\boldsymbol{\theta})&\doteq\exp\Big(\boldsymbol{\theta}_\sigma^\text{T}\mathbf{x}_\sigma(s)\Big),
\end{align}
where $\mathbf{x}_\mu(s)$ and $\mathbf{x}_\sigma(s)$ are state feature vectors corresponding to each approximator.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><p>[1] <span id=rl-book>Richard S. Sutton & Andrew G. Barto. <a href=https://mitpress.mit.edu/books/reinforcement-learning-second-edition>Reinforcement Learning: An Introduction</a>. MIT press, 2018.</span></p><p>[2] Deepmind x UCL. <a href=https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021>Reinforcement Learning Lecture Series 2021</a>. Deepmind, 2021.</p><p>[3] Richard S. Sutton & David McAllester & Satinder Singh & Yishay Mansour. <a href=https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html>Policy Gradient Methods for Reinforcement Learning with Function Approximation</a>. NIPS 1999.</p><h2 id=footnotes>Footnotes<a hidden class=anchor aria-hidden=true href=#footnotes>#</a></h2></div><footer class=post-footer><ul class=post-tags><li><a href=https://trunghng.github.io/tags/reinforcement-learning/>reinforcement-learning</a></li><li><a href=https://trunghng.github.io/tags/policy-gradient/>policy-gradient</a></li><li><a href=https://trunghng.github.io/tags/actor-critic/>actor-critic</a></li><li><a href=https://trunghng.github.io/tags/function-approximation/>function-approximation</a></li><li><a href=https://trunghng.github.io/tags/my-rl/>my-rl</a></li></ul><nav class=paginav><a class=prev href=https://trunghng.github.io/posts/reinforcement-learning/planning-learning/><span class=title>« Prev</span><br><span>Planning & Learning</span>
</a><a class=next href=https://trunghng.github.io/posts/machine-learning/exponential-family-glim/><span class=title>Next »</span><br><span>The Exponential Family, Generalized Linear Models</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on x" href="https://x.com/intent/tweet/?text=Policy%20Gradient%20Theorem&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f&amp;hashtags=reinforcement-learning%2cpolicy-gradient%2cactor-critic%2cfunction-approximation%2cmy-rl"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f&amp;title=Policy%20Gradient%20Theorem&amp;summary=Policy%20Gradient%20Theorem&amp;source=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on reddit" href="https://reddit.com/submit?url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f&title=Policy%20Gradient%20Theorem"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on whatsapp" href="https://api.whatsapp.com/send?text=Policy%20Gradient%20Theorem%20-%20https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on telegram" href="https://telegram.me/share/url?text=Policy%20Gradient%20Theorem&amp;url=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Policy Gradient Theorem on ycombinator" href="https://news.ycombinator.com/submitlink?t=Policy%20Gradient%20Theorem&u=https%3a%2f%2ftrunghng.github.io%2fposts%2freinforcement-learning%2fpolicy-gradient-theorem%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=trunghng/trunghng.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://trunghng.github.io>Littleroot</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>