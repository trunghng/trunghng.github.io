I"Ú$<blockquote>
  <p>In two previous posts, <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a> and <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with <strong>Dynamic Programming</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-dp">What is Dynamic Programming?</a></li>
  <li><a href="#dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</a>
    <ul>
      <li><a href="#policy-evaluation">Policy Evaluation</a></li>
      <li><a href="#policy-improvement">Policy Improvement</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h4 id="what-is-dp">What is Dynamic Programming?</h4>
<p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p>

<h4 id="dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</h4>
<p>DP is a very general method for solving problems having two properties:</p>
<ul>
  <li><em>Optimal substructure</em>
    <ul>
      <li>Principle of optimality applies.</li>
      <li>Optimal solution can be decomposed into sub-problems.</li>
    </ul>
  </li>
  <li><em>Overlapping sub-problems</em>
    <ul>
      <li>Sub-problems recur many times.</li>
      <li>Solutions can be cached and reused.</li>
    </ul>
  </li>
</ul>

<p>MDPs satisfy both properties since:</p>
<ul>
  <li>Bellman equation gives recursive decomposition.</li>
  <li>Value function stores and reuses solutions.</li>
</ul>

<h5 id="policy-evaluation">Policy Evaluation</h5>
<p>Recall from the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html/#bellman-equations">Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]\tag{1}\label{1}
\end{equation}
If the environment‚Äôs dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br />
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;=\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_k(s‚Äô)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html/#banach-fixed-pts">Banach‚Äôs fixed points theorem</a> and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <em>Iterative policy evaluation</em>.<br />
We have the backup diagram for this update</p>

<figure>
	<img src="/assets/images/2021-07-25/iterative-policy-evaluation.png" alt="Backup diagram for iterative policy evalution update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Backup diagram for Iterative policy evaluation update</figcaption>
</figure>

<p>When implementing <em>iterative policy evaluation</em>, for all $s\in\mathcal{S}$, we can use:</p>
<ul>
  <li>one array to store the value functions, and update them ‚Äò‚Äòin-place‚Äù (<em>asynchronous DP</em>)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v(s‚Äô)}\right]
\end{equation}</li>
  <li>two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (<em>synchronous DP</em>)
\begin{align}
\color{red}{v_{new}(s)}&amp;\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v_{old}(s‚Äô)}\right]\\ \color{red}{v_{old}}&amp;\leftarrow\color{red}{v_{new}}
\end{align}
Here is the pseudocode of the <em>in-place iterative policy evaluation</em>, given a policy $\pi$, for estimating $V\approx v_\pi$</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">iterative_policy_evaluation</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">threshold</span><span class="p">):</span>
    <span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">=</span> <span class="n">arbitrary_values</span>
    <span class="n">V</span><span class="p">(</span><span class="n">terminal_state</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">S</span><span class="p">:</span>
                <span class="n">v</span> <span class="o">=</span> <span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                <span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="o">=</span> <span class="n">sum_over_</span><span class="p">{</span><span class="n">a</span><span class="p">}[</span><span class="n">pi</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">s</span><span class="p">)</span> <span class="o">*</span> <span class="n">sum_over_</span><span class="p">{</span><span class="n">s_prime</span><span class="p">,</span><span class="n">r</span><span class="p">}[</span><span class="n">p</span><span class="p">(</span><span class="n">s_prime</span><span class="p">,</span><span class="n">r</span><span class="p">,</span><span class="n">s</span><span class="p">,</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">V</span><span class="p">(</span><span class="n">s_prime</span><span class="p">))]]</span>
                <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="n">absolute_of_</span><span class="p">{</span><span class="n">v</span> <span class="o">-</span> <span class="n">V</span><span class="p">(</span><span class="n">s</span><span class="p">)})</span>
        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="k">break</span></code></pre></figure>

<h4 id="policy-improvement">Policy Improvement</h4>
<p>The reason why we compute the value function for a given policy $\pi$ is to find better policies. Given the computed value function $v_\pi$ for an deterministic policy $\pi$, we already know how good it is for a state $s$ to choose action $a=\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\neq\pi$, will it be better?<br />
In particular, in state $s$, selecting action $a$ and thereafter following the policy $\pi$, we have:
\begin{align}
q_\pi(s,a)&amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{2}\label{2} \\ &amp;=\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]
\end{align}
<strong>Theorem</strong> (<em>Policy improvement</em>)<br />
Let $\pi,\pi‚Äô$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,
\begin{equation}
q_\pi(s,\pi‚Äô(s))\geq v_\pi(s)\tag{3}\label{3}
\end{equation}
Then $\pi‚Äô\geq\pi$, which means for all $s\in\mathcal{S}$, we have $v_{\pi‚Äô}(s)\geq v_\pi(s)$.</p>

<p><strong>Proof</strong><br />
Deriving \eqref{3} combined with \eqref{2}, we have:
\begin{align}
v_\pi(s)&amp;\leq q_\pi(s,\pi‚Äô(s)) \\ &amp;=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi‚Äô(s)\right]\tag{by \eqref{2}} \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right][^1] \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi‚Äô(S_{t+1}))|S_t=s\right]\tag{by \eqref{3}} \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma\mathbb{E}_{\pi‚Äô}\left[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi‚Äô(S_{t+1})\right]|S_t=s\right] \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\right] \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|S_t=s\right] \\ &amp;\quad\vdots \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\dots|S_t=s\right] \\ &amp;=v_{\pi‚Äô}(s)\blacksquare
\end{align}
\hfill\blacksquare</p>

:ET