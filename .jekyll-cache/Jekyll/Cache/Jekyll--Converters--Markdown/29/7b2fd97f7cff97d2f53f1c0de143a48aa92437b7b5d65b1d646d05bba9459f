I"²0<blockquote>
  <p>A note on Gaussian distribution.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#gauss-dist">Gaussian (Normal) distribution</a>
    <ul>
      <li><a href="#std-norm">Standard Normal</a></li>
    </ul>
  </li>
  <li><a href="#mvn">Multivariate Normal distribution</a>
    <ul>
      <li><a href="#bvn">Bivariate Normal</a></li>
    </ul>
  </li>
  <li><a href="#prop-cov">Properties of the covariance matrix</a>
    <ul>
      <li><a href="#sym-cov">Symmetric</a></li>
      <li><a href="#re-cov">Real eigenvalues</a></li>
      <li><a href="#proj-ev-cov">Projection onto eigenvectors</a></li>
    </ul>
  </li>
  <li><a href="#geo-int">Geometrical interpretation</a></li>
  <li><a href="#cond-gauss-dist">Conditional Gaussian distribution</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p>
<h2 id="gauss-dist">Gaussian (Normal) Distribution</h2>
<p>A random variable $X$ is said to be <strong>Gaussian</strong> or to have the <strong>Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)\tag{1}\label{1}
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$.</p>

<h3 id="std-normal">Standard Normal</h3>
<p>When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution <strong>Standard Normal</strong>.
\begin{equation}
X\sim\mathcal{N}(0,1)\tag{2}\label{2}
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2},\tag{3}\label{3}
\end{equation}
and
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\,dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt\tag{4}\label{4}
\end{equation}
Below are some illustrations of Normal distribution.</p>
<figure>
	<img src="/assets/images/2021-11-22/normal.png" alt="normal distribution" style="display: block; margin-left: auto; margin-right: auto; width:  900px; height: 380px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/gauss-dist.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h2 id="mvn">Multivariate Normal Distribution</h2>
<p>A $k$-dimensional random vector $\mathbf{X}=\left(X_1,\dots,X_D\right)^\text{T}$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_DX_D
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_D$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\tag{5}\label{5}
\end{equation}
where
\begin{equation}
\boldsymbol{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^\text{T}=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^\text{T}\tag{6}\label{6}
\end{equation}
is the $D$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{D\times D}$ with
\begin{equation}
\boldsymbol{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)\tag{7}\label{7}
\end{equation}
We also have that $\boldsymbol{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Thus, the PDF of an MVN is defined as
\begin{equation}
f_\mathbf{X}(x_1,\ldots,x_D)=\dfrac{1}{(2\pi)^{D/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]
\end{equation}
With this idea, <em>Standard Normal</em> distribution in multi-dimensional case can be defined as a Gaussian with mean $\boldsymbol{\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\boldsymbol{\Sigma}=\mathbf{I}_{D\times D}$.</p>

<h3 id="bvn">Bivariate Normal</h3>
<p>When the number of dimensions in $\mathbf{X}$, $D=2$, this special case of MVN is called the <strong>Bivariate Normal (BVN)</strong>.</p>

<p>An example of an BVN, $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$, is shown as following.</p>
<figure>
	<img src="/assets/images/2021-11-22/bvn.png" alt="monte carlo method" style="display: block; margin-left: auto; margin-right: auto; width: 750px; height: 350px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/mvn.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h2 id="prop-cov">Properties of the covariance matrix</h2>

<h3 id="sym-cov">Symmetric</h3>
<p>With the definition \eqref{7} of the covariance matrix $\boldsymbol{\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\boldsymbol{\Sigma}$ is symmetric.</p>

<p>To prove this property, first off consider a square matrix $\mathbf{S}$, we have it can be written by
\begin{equation}
\mathbf{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2}+\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}=\mathbf{S}_\text{S}+\mathbf{S}_\text{A},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2},\hspace{2cm}\mathbf{S}_\text{A}=\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}
\end{equation}
It is easily seen that $\mathbf{S}_\text{S}$ is symmetric because the $\{i,j\}$ element of its equal to the $\{j,i\}$ element due to
\begin{equation}
(\mathbf{S}_\text{S})_{ij}=\frac{(\mathbf{S})_{ij}+(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}+(\mathbf{S})_{ji}}{2}=(\mathbf{S}_\text{S})_{ji}
\end{equation}
On the other hand, the matrix $\mathbf{S}_\text{A}$ is anti-symmetric since
\begin{equation}
(\mathbf{S}_\text{A})_{ij}=\frac{(\mathbf{S})_{ij}-(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}-(\mathbf{S})_{ji}}{2}=-(\mathbf{S}_\text{A})_{ji}
\end{equation}
Consider the density of a distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, we have that $\boldsymbol{\Sigma}$ is square and so is its inverse $\boldsymbol{\Sigma}^{-1}$. Therefore we can express $\boldsymbol{\Sigma}^{-1}$ as a sum of a symmetric matrix $\boldsymbol{\Sigma}_\text{S}$ with an anti-symmetric matrix $\boldsymbol{\Sigma}_\text{A}$
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A}
\end{equation}
We have that the density of the distribution is given by
\begin{align}
f(\mathbf{x})&amp;=\frac{1}{(2\pi)^{D/2}\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;\propto\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;=\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}(\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A})(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;\propto\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}+\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}\right] \\ &amp;=\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}\right]
\end{align}
where in the forth step, we have defined $\mathbf{v}\doteq\mathbf{x}-\boldsymbol{\mu}$, and where in the fifth-step, the result obtained was due to
\begin{align}
\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}&amp;=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i(\boldsymbol{\Sigma}_\text{A})_{ij}\mathbf{v}_j \\ &amp;=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i-(\boldsymbol{\Sigma}_\text{A})_{ji}\mathbf{v}_j \\ &amp;=-\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}
\end{align}
which implies that $\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}=0$.</p>

<p>Thus, when computing th{e density, the symmetric part of $\boldsymbol{\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\boldsymbol{\Sigma}^{-1}$ is symmetric, which means that $\boldsymbol{\Sigma}$ is also symmetric.</p>

<h3 id="re-cov">Real eigenvalues</h3>
<p>Consider an eigenvector, eigenvalue pair $(\mathbf{x},\lambda)$ of covariance matrix $\boldsymbol{\Sigma}$, we have
\begin{equation}
\boldsymbol{\Sigma}\mathbf{x}=\lambda\mathbf{x}\tag{8}\label{8}
\end{equation}
Since $\boldsymbol{\Sigma}\in\mathbb{R}^{D\times D}$, we have $\boldsymbol{\Sigma}=\overline{\boldsymbol{\Sigma}}$. Conjugate both sides of the equation above we have
\begin{equation}
\boldsymbol{\Sigma}\overline{\mathbf{x}}=\overline{\lambda}\overline{\mathbf{x}},\tag{9}\label{9}
\end{equation}
Since $\boldsymbol{\Sigma}$ is symmetric, we have $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}^\text{T}$. Taking the transpose of both sides of \eqref{9} gives us
\begin{equation}
\overline{\mathbf{x}}^\text{T}\boldsymbol{\Sigma}=\overline{\lambda}\overline{\mathbf{x}}^\text{T}\tag{10}\label{10}
\end{equation}
Continuing by taking dot product of both sides of \eqref{10} with $\mathbf{x}$ lets us obtain
\begin{equation}
\overline{\mathbf{x}}^\text{T}\boldsymbol{\Sigma}\mathbf{x}=\overline{\lambda}\overline{\mathbf{x}}^\text{T}\mathbf{x}\tag{11}\label{11}
\end{equation}
On the other hand, take dot product of $\overline{\mathbf{x}}^\text{T}$ with both sides of \eqref{8}, we have
\begin{equation}
\overline{\mathbf{x}}^\text{T}\boldsymbol{\Sigma}\mathbf{x}=\lambda\overline{\mathbf{x}}^\text{T}\mathbf{x}
\end{equation}
which by eqref{11} implies that
\begin{equation}
\overline{\lambda}\overline{\mathbf{x}}^\text{T}\mathbf{x}=\lambda\overline{\mathbf{x}}^\text{T}\mathbf{x},
\end{equation}
or
\begin{equation}
(\lambda-\overline{\lambda})\overline{\mathbf{x}}^\text{T}\mathbf{x}=0
\end{equation}
We have that
\begin{equation}
\overline{\mathbf{x}}^\text{T}\mathbf{x}=\sum_{k=1}^{D}
\end{equation}
where we denote complex eigenvector $\mathbf{x}$ as $\mathbf{x}=(a_1+i b_1,\ldots,a_D+i b_D)^\text{T}$, which implies that its complex conjugate $\overline{\mathbf{x}}$ can be written as $\mathbf{x}=(a_1+i b_1,\ldots,a_D+i b_D)^\text{T}$.</p>

<h3 id="projection-onto-eigenvectors">Projection onto eigenvectors</h3>

<h2 id="geo-int">Geometrical interpretation</h2>

<h2 id="cond-gauss-dist">Conditional Gaussian distribution</h2>

<h2 id="references">References</h2>
<p>[1] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a>.</p>

<p>[2] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY.</p>

<p>[3] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The definition of covariance matrix $\boldsymbol{\Sigma}$ can be rewritten as
\begin{equation}
\boldsymbol{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation}
Let $\mathbf{z}\in\mathbb{R}^D$, we have
\begin{equation}
\Var(\mathbf{z}^\text{T}\mathbf{X})=\mathbf{z}^\text{T}\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^\text{T}\boldsymbol{\Sigma}\mathbf{z}
\end{equation}
And since $\Var(\mathbf{z}^\text{T}\mathbf{X})\geq0$, we also have that $\mathbf{z}^\text{T}\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\boldsymbol{\Sigma}$ is a positive semi-definite matrix.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET