I"<blockquote>
  <p>Materials were taken mostly from <a href="#bishops-book">Bishop’s book</a>.
<!-- excerpt-end --></p>
  <ul>
    <li><a href="#preliminaries">Preliminaries</a>
      <ul>
        <li><a href="#ind-basis-dim">Independence, basis, dimension</a>
          <ul>
            <li><a href="#lin-ind">Linear independence</a></li>
            <li><a href="#basis">Basis of a vector space</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#lin-models-regression">Linear models for Regression</a>
      <ul>
        <li><a href="#lin-basis-func-models">Linear basis function models</a>
          <ul>
            <li></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#references">References</a></li>
    <li><a href="#footnotes">Footnotes</a></li>
  </ul>
</blockquote>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="ind-basis-dim">Independence, basis, dimension</h3>

<h4 id="lin-ind">Linear independence</h4>
<p>The sequence of vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ is said to be <strong>linearly independent</strong> if
\begin{equation}
c_1\mathbf{x}_1+\ldots+c_n\mathbf{x}_n=\mathbf{0}
\end{equation}
only when $c_1,\ldots,c_n$ are all zero.</p>

<p>Considering those $n$ vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ as $n$ columns of a matrix $\mathbf{A}$
\begin{equation}
\mathbf{A}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{x}_1 &amp; \ldots &amp; \mathbf{x}_n \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
we have that the columns of $\mathbf{A}$ are linearly independent when
\begin{equation}
\mathbf{A}\mathbf{x}=\mathbf{0}\hspace{0.5cm}\Leftrightarrow\hspace{0.5cm}\mathbf{x}=\mathbf{0},
\end{equation}
or in other words, the rank of $\mathbf{A}$ equal to the number of columns of $\mathbf{A}$.</p>

<h4 id="basis">Basis of a vector space</h4>
<p>We say that vectors $\mathbf{v}_1,\ldots,\mathbf{v}_k$ span a space $S$ when the space consists of all combinations of those vectors. In this case, $S$ is the smallest space containing those vectors.</p>

<p>A <strong>basis</strong> for a vector space $S$ is a sequence of vectors $\mathbf{v}_1,\ldots,\mathbf{v}_d$ having two properties:
• v1, v2, …vd are independent
• v1, v2, …vd span the vector space.
The basis of a space tells us everything we need to know about that space.</p>

<h2 id="lin-models-regression">Linear models for Regression</h2>
<p>Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\mathbf{x}$ of input variables.</p>

<h3 id="linear-basis-function-models">Linear basis function models</h3>

<p id="lin-basis-func-models">The simplest linear model used for regression tasks is <strong>linear regression</strong>, which is defined as a linear combination of the input variables
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+\ldots+w_Dx_D,\tag{1}\label{1}
\end{equation}
where $\mathbf{x}=(x_1,\ldots,x_D)^\intercal$ is the input variables, while $w_i$’s are the parameters parameterizing the space of linear function mapping from the input space $\mathcal{X}$ of $\mathbf{x}$ to $\mathcal{Y}$.</p>

<p>We can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\mathbf{x}$, as
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1\phi_1(\mathbf{x})+\ldots+w_{M-1}\phi_{M-1}(\mathbf{x})=w_0+\sum_{i=1}^{M-1}w_i\phi_i(\mathbf{x}),\tag{2}\label{2}
\end{equation}
where $\phi_i(\mathbf{x})$’s are called the <strong>basis functions</strong>; $w_0$ is called a <strong>bias parameter</strong>. By letting $w_0$ be a coefficient corresponding to a dummy basis function $\phi_0(\mathbf{x})=1$, \eqref{2} can be written in a more convenient way
\begin{equation}
y(\mathbf{x},\mathbf{w})=\sum_{i=0}^{M-1}w_i\phi_i(\mathbf{x})=\mathbf{w}^\intercal\mathbf{\phi}(\mathbf{x}),\tag{3}\label{3}
\end{equation}
where $\mathbf{w}=(w_0,\ldots,w_{M-1})^\intercal$ and $\mathbf{\phi}=(\phi_0,\ldots,\phi_{M-1})^\intercal$, with $\phi_0(\cdot)=1$.
There are various choices of basis functions</p>
<ul id="number-list">
	<li>
		<b>Polynomial basis</b>
		\begin{equation}
		\phi_i(x)=x^i
		\end{equation}
	</li>
	<li>
		<b>Gaussian basis function</b>.
		\begin{equation}
		\phi_i(x)=\exp\left(-\frac{(x-\mu_i)^2}{2\sigma^2}\right)
		\end{equation}
	</li>
</ul>

<h2 id="references">References</h2>
<p>[1] <span id="bishops-book">Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY.</span></p>

<p>[2] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>.</p>

<p>[3] MIT 18.06. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Linear Algebra</a>.</p>

<h2 id="footnotes">Footnotes</h2>
:ET