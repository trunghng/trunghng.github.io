I"S<blockquote>
  <p>Materials were taken mostly from <a href="#bishops-book">Bishop’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#lin-models-regression">Linear models for Regression</a>
    <ul>
      <li><a href="#lin-basis-func-models">Linear basis function models</a>
        <ul>
          <li><a href="#ind-basis-dim">Independence, basis, dimension</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lin-models-regression">Linear models for Regression</h2>
<p>Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\mathbf{x}$ of input variables.</p>

<h3 id="linear-basis-function-models">Linear basis function models</h3>

<h4 id="ind-basis-dim">Lineear Independence, basis of a vector space</h4>

<p id="lin-basis-func-models">The simplest linear model used for regression tasks is <strong>linear regression</strong>, which is defined as a linear combination of the input variables
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+\ldots+w_Dx_D,\tag{1}\label{1}
\end{equation}
where $\mathbf{x}=(x_1,\ldots,x_D)^\intercal$ is the input variables, while $w_i$’s are the parameters paremeterizing the space of linear function mapping from the input space $\mathcal{X}$ of $\mathbf{x}$ to $\mathcal{Y}$.</p>

<p>We can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\mathbf{x}$, as
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1\phi_1(\mathbf{x})+\ldots+w_{M-1}\phi_{M-1}(\mathbf{x})=w_0+\sum_{i=1}^{M-1}w_i\phi_i(\mathbf{x}),\tag{2}\label{2}
\end{equation}
where $\phi_i(\mathbf{x})$’s are called the <strong>basis functions</strong>; $w_0$ is called a <strong>bias parameter</strong>. By letting $w_0$ be a coeffiecient corresponding to a dummy basis function $\phi_0(\mathbf{x})=1$, \eqref{2} can be written in a more convenient way
\begin{equation}
y(\mathbf{x},\mathbf{w})=\sum_{i=0}^{M-1}w_i\phi_i(\mathbf{x})=\mathbf{w}^\intercal\mathbf{\phi}(\mathbf{x}),\tag{3}\label{3}
\end{equation}
where $\mathbf{w}=(w_0,\ldots,w_{M-1})^\intercal$ and $\mathbf{\phi}=(\phi_0,\ldots,\phi_{M-1})^\intercal$, with $\phi_0(\cdot)=1$.
There are various choices of basis functions</p>
<ul id="number-list">
	<li>
		<b>Polinomial basis</b>
		\begin{equation}
		\phi_i(x)=x^i
		\end{equation}
	</li>
	<li>
		<b>Gaussian basis function</b>.
		\begin{equation}
		\phi_i(x)=\exp\left(-\frac{(x-\mu_i)^2}{2\sigma^2}\right)
		\end{equation}
	</li>
</ul>

<h2 id="references">References</h2>
<p>[1] <span id="bishops-book">Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY.</span></p>

<p>[2] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra</a>.</p>

<p>[3] MIT 18.06. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Linear Algebrea</a>.</p>

<h2 id="footnotes">Footnotes</h2>
:ET