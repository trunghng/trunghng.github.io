I"³<blockquote>
  <p>So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a <strong>parameterized policy</strong>, $\boldsymbol{\theta}$, that can select actions without consulting a value function by considering the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called <strong>policy gradient methods</strong>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#policy-grad-ep">Policy Gradient for Episodic Problems</a>
    <ul>
      <li><a href="#policy-grad-theorem-ep">The Policy Gradient Theorem</a></li>
      <li><a href="#reinforce">REINFORCE</a></li>
      <li><a href="#reinforce-baseline">REINFORCE with Baseline</a></li>
      <li><a href="#actor-critic-methods">Actor-Critic Methods</a></li>
    </ul>
  </li>
  <li><a href="#policy-grad-cont">Policy Gradient for Continuing Problems</a>
    <ul>
      <li><a href="#policy-grad-theorem-cont">The Policy Gradient Theorem</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="policy-grad-ep">Policy Gradient for Episodic Problems</h2>
<p>We begin by considering episodic case, for which we define the performance measure $J(\boldsymbol{\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have:
\begin{equation}
J(\boldsymbol{\theta})\doteq v_{\pi_\boldsymbol{\theta}}(s_0),
\end{equation}
where $v_{\pi_\boldsymbol{\theta}}$ is the true value function for $\pi_\boldsymbol{\theta}$, the policy determined by $\boldsymbol{\theta}$.</p>

<h3 id="policy-grad-theorem-ep">The Policy Gradient Theorem</h3>
<p><strong>Theorem 1</strong><br />
The policy gradient theorem for the episodic case establishes that
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}),\tag{1}\label{1}
\end{equation}
where $\pi$ represents the policy corresponding to parameter vector $\boldsymbol{\theta}$.</p>

<p><strong>Proof</strong><br />
We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written in terms of the action-value function as:
\begin{align}
\nabla_\boldsymbol{\theta}v_\pi(s)&amp;=\nabla_\boldsymbol{\theta}\Big[\sum_a\pi(a|s)q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(s|a)q_\pi(a,s)+\pi(a|s)\nabla_\boldsymbol{\theta}\sum_{sâ€™,r}p(sâ€™,r|s,a)\big(r+v_\pi(sâ€™)\big)\Big] \\ &amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{sâ€™}p(sâ€™|s,a)\nabla_\boldsymbol{\theta}v_\pi(sâ€™)\Big] \\ &amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{sâ€™}p(sâ€™|s,a)\sum_{aâ€™}\big(\nabla_\boldsymbol{\theta}\pi(sâ€™|aâ€™)q_\pi(sâ€™,aâ€™) \\ &amp;\hspace{2cm}+\pi(aâ€™|sâ€™)\sum_{s''}p(s''\vert sâ€™,aâ€™)\nabla_\boldsymbol{\theta}v_\pi(s'')\big)\Big] \\ &amp;=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty}P(s\to x,k,\pi)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a),
\end{align}
After repeated unrolling as in the fifth step, where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$. It is then immediate that:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;=\nabla_\boldsymbol{\theta}v_\pi(s_0) \\ &amp;=\sum_s\Big(\sum_{k=0}^{\infty}P(s_0\to s,k,\pi)\Big)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_s\eta(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_{sâ€™}\eta(sâ€™)\sum_s\frac{\eta(s)}{\sum_{sâ€™}\eta(sâ€™)}\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_{sâ€™}\eta(sâ€™)\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;\propto\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a),
\end{align}
where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode:
\begin{equation}
\eta(s)=h(s)+\sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|s)p(s|\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
where $h(s)$ denotes the probability that an episode begins in each state $s$; $\bar{s}$ denotes the preceding state of $s$. This leads to the result that we have used in the fifth step:
\begin{equation}
\mu(s)=\frac{\eta(s)}{\sum_{sâ€™}\eta(sâ€™)},\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}</p>

<h3 id="reinforce">REINFORCE</h3>
<p>Notice that in <strong>Theorem 1</strong>, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\mu(s)$) under the target policy $\pi$. Therefore, we can rewrite \eqref{1} as:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}) \\ &amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right]\tag{2}\label{2}
\end{align}
Using SGD on maximizing $J(\boldsymbol{\theta})$ gives us the update rule:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\sum_a\hat{q}(S_t,a,\mathbf{w})\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta}),
\end{equation}
where $\hat{q}$ is some learned approximation to $q_\pi$ with $\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called <strong>all-actions</strong> method because its update involves all of the actions.</p>

<p>Continue our derivation in \eqref{2}, we have:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right] \\ &amp;=\mathbb{E}_\pi\left[\sum_a\pi(a|S_t,\boldsymbol{\theta})q_\pi(S_t,a)\frac{\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})}{\pi(a|S_t,\boldsymbol{\theta})}\right] \\ &amp;=\mathbb{E}_\pi\left[q_\pi(S_t,A_t)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right] \\ &amp;=\mathbb{E}_\pi\left[G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right],
\end{align}
where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\sim\pi$; and in the fourth step, we have used the identity
\begin{equation}
\mathbb{E}_\pi\left[G_t|S_t,A_t\right]=q_\pi(S_t,A_t)
\end{equation}</p>

<h3 id="reinforce-baseline">REINFORCE with Baseline</h3>

<h3 id="actor-critic-methods">Actor-Critic Methods</h3>

<h2 id="policy-grad-cont">Policy Gradient with Continuing Problems</h2>

<h3 id="policy-grad-theorem-cont">The Policy Gradient Theorem</h3>
<p><strong>Theorem 2</strong></p>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[2] Deepmind x UCL. <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021">Reinforcement Learning Lecture Series 2021</a>.</p>

<h2 id="footnotes">Footnotes</h2>
:ET