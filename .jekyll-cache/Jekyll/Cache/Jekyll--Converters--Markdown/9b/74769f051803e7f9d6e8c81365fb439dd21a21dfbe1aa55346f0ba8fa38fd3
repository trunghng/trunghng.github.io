I"‹<blockquote>
  <p>So far in this <a href="/tag/my-rl">series</a>, we have gone through the ideas of <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html"><strong>dynamic programming</strong> (DP)</a> and <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html"><strong>Monte Carlo</strong></a>. What will happen if we combine these ideas together? <strong>Temporal-deffirence (TD) learning</strong> is our answer.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#td0">TD(0)</a>
    <ul>
      <li><a href="#td-prediction">TD Prediction</a>
        <ul>
          <li><a href="#adv-over-mc-dp">Adventages over MC &amp; DP</a></li>
          <li><a href="#opt-td0">Optimality of TD(0)</a></li>
        </ul>
      </li>
      <li><a href="#td-control">TD Control</a>
        <ul>
          <li><a href="#sarsa">Sarsa</a></li>
          <li><a href="#q-learning">Q-learining</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#n-step-td">n-step TD</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="td0">TD(0)</h2>
<p>As usual, we approach this new method in the prediction problem.</p>

<h3 id="td-prediction">TD Prediction</h3>
<p>Borrowing the idea of Monte Carlo, TD methods learn from episodes of experience to solve the <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#fn:2">prediction problem</a>. The simplest TD method is <strong>TD(0)</strong> (or <strong>one-step TD</strong>)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right]\tag{1}\label{1},
\end{equation}
where $\alpha&gt;0$ is step size of the update. Here is pseudocode of the TD(0) method</p>
<figure>
	<img src="/assets/images/2022-04-08/td0.png" alt="TD(0)" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
<p><br />
Recall that in <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#mc-prediction">Monte Carlo method</a>, or even in its trivial form, <strong>constant-$\alpha$ MC</strong>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[G_t-V(S_t)\right]\tag{2}\label{2},
\end{equation}
we have to wait until the end of the episode, when the return $G_t$ is determined. However, with TD(0), we can do the update immediately in the next time step $t+1$.</p>

<p>As we can see in \eqref{1} and \eqref{2}, both TD and MC updates look ahead to a sample successor state (or state-action pair), use the value of the successor and the corresponding reward in order to update the value of the current state (or state-action pair). This kind of updates is called <em>sample update</em>, which differs from <em>expected update</em> used by DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.</p>

<p>Other than the sampling of Monte Carlo, TD methods also use the bootstrapping of DP. Because similar to <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-evaluation">DP</a>, TD(0) is also a bootstrapping method, since the target in its update is $R_{t+1}+\gamma V(S_{t+1})$.</p>

<p>The quantity inside bracket in \eqref{1} is called <em>TD error</em>, denoted as $\delta$:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\end{equation}
If the array $V$ does not change during the episode (as in MC), then the MC error can be written as a sum of TD errors
\begin{align}
G_t-V(S_t)&amp;=R_{t+1}+\gamma G_{t+1}-V(S_t)+\gamma V(S_{t+1})-\gamma V(S_{t+1}) \\ &amp;=\delta_t+\gamma\left(G_{t+1}-V(S_{t+1})\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\left(G_{t+2}-V(S_{t+2})\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}\left(G_T-V(S_T)\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0) \\ &amp;=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k
\end{align}</p>

<h4 id="adv-over-mc-dp">Adventages over MC &amp; DP</h4>
<p>With how TD is established, these are some advantages of its over MC and DP:</p>
<ul>
  <li>Only experience is required.</li>
  <li>Can be fully incremental:
    <ul>
      <li>Can make update before knowing the final outcome.</li>
      <li>Requires less memory.</li>
      <li>Requires less peak computation.</li>
    </ul>
  </li>
</ul>

<p>TD(0) does converge to $v_\pi$, in the mean for a sufficient small $\alpha$, and with probability of $1$ if $\alpha$ decreases according to the <em>stochastic approximation condition</em>
\begin{equation}
\sum_{n=1}^{\infty}\alpha_n(a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty,
\end{equation}
where $\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$-th selection of action $a$.</p>

<h4 id="opt-td0">Optimality of TD(0)</h4>
<p>Under batch training, TD(0) converges to the optimal maximum likelihood estimate. The convergence and optimality proofs can be found in this <a href="#td-convergence">paper</a>.</p>
<figure>
	<img src="/assets/images/2022-04-08/random_walk_batch_updating.png" alt="TD(0) vs constant-alpha MC" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Performance of TD(0) and constant-$\alpha$ MC under batch training on the random walk task. The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-6/random-walk.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h3 id="td-control">TD Control</h3>
<p>We begin solving the control problem with an on-policy TD method</p>

<h4 id="sarsa">Sarsa</h4>
<p>Recall that in on-policy methods, we evaluate or improve the policy $\pi$ used to make decision.</p>

<h2 id="n-step-td">n-step TD</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] David Silver. <a href="https://www.davidsilver.uk/teaching/">UCL course on RL</a></p>

<p>[3] <span id="td-convergence">Sutton, R.S. <a href="https://doi.org/10.1007/BF00115009">Learning to predict by the methods of temporal differences</a>. Mach Learn 3, 9â€“44 (1988).</span></p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It is a special case of <a href="#n-step-td">n-step TD</a> and TD($\lambda$).Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET