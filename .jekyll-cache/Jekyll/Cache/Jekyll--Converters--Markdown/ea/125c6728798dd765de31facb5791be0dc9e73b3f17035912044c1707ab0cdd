I"ç,<blockquote>
  <p>In two previous posts, <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a> and <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with <strong>Dynamic Programming</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-dp">What is Dynamic Programming?</a></li>
  <li><a href="#dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</a>
    <ul>
      <li><a href="#policy-evaluation">Policy Evaluation</a></li>
      <li><a href="#policy-improvement">Policy Improvement</a></li>
      <li><a href="#policy-iteration">Policy Iteration</a></li>
      <li><a href="#value-iteration">Value Iteration</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="what-is-dp">What is Dynamic Programming?</h2>
<p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p>

<h2 id="dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</h2>
<p>DP is a very general method for solving problems having two properties:</p>
<ul>
  <li><em>Optimal substructure</em>
    <ul>
      <li>Principle of optimality applies.</li>
      <li>Optimal solution can be decomposed into sub-problems.</li>
    </ul>
  </li>
  <li><em>Overlapping sub-problems</em>
    <ul>
      <li>Sub-problems recur many times.</li>
      <li>Solutions can be cached and reused.</li>
    </ul>
  </li>
</ul>

<p>MDPs satisfy both properties since:</p>
<ul>
  <li>Bellman equation gives recursive decomposition.</li>
  <li>Value function stores and reuses solutions.</li>
</ul>

<h3 id="policy-evaluation">Policy Evaluation</h3>
<p>Recall from the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html/#bellman-equations">Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]\tag{1}\label{1}
\end{equation}
If the environment‚Äôs dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br />
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;=\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_k(s‚Äô)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html/#banach-fixed-pts">Banach‚Äôs fixed points theorem</a> and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <em>Iterative policy evaluation</em>.<br />
We have the backup diagram for this update</p>
<figure>
	<img src="/assets/images/2021-07-25/backup-iterative-policy-evaluation.png" alt="Backup diagram for iterative policy evalution update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Backup diagram for Iterative policy evaluation update</figcaption>
</figure>
<p><br />
When implementing <em>iterative policy evaluation</em>, for all $s\in\mathcal{S}$, we can use:</p>
<ul>
  <li>one array to store the value functions, and update them ‚Äò‚Äòin-place‚Äù (<em>asynchronous DP</em>)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v(s‚Äô)}\right]
\end{equation}</li>
  <li>two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (<em>synchronous DP</em>)
\begin{align}
\color{red}{v_{new}(s)}&amp;\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v_{old}(s‚Äô)}\right]\\ \color{red}{v_{old}}&amp;\leftarrow\color{red}{v_{new}}
\end{align}
Here is the pseudocode of the <em>in-place iterative policy evaluation</em>, given a policy $\pi$, for estimating $V\approx v_\pi$</li>
</ul>
<figure>
	<img src="/assets/images/2021-07-25/iterative-policy-evaluation.png" alt="iterative policy evalution pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: Pseudocode of In-place Iterative Policy Evaluation</figcaption>
</figure>
<h3 id="policy-improvement">Policy Improvement</h3>
<p>The reason why we compute the value function for a given policy $\pi$ is to find better policies. Given the computed value function $v_\pi$ for an deterministic policy $\pi$, we already know how good it is for a state $s$ to choose action $a=\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\neq\pi$, will it be better?<br />
In particular, in state $s$, selecting action $a$ and thereafter following the policy $\pi$, we have:
\begin{align}
q_\pi(s,a)&amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{2}\label{2} \\ &amp;=\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]
\end{align}
<strong>Theorem</strong> (<em>Policy improvement</em>)<br />
Let $\pi,\pi‚Äô$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,
\begin{equation}
q_\pi(s,\pi‚Äô(s))\geq v_\pi(s)\tag{3}\label{3}
\end{equation}
Then $\pi‚Äô\geq\pi$, which means for all $s\in\mathcal{S}$, we have $v_{\pi‚Äô}(s)\geq v_\pi(s)$.</p>

<p><strong>Proof</strong><br />
Deriving \eqref{3} combined with \eqref{2}, we have<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:
\begin{align}
v_\pi(s)&amp;\leq q_\pi(s,\pi‚Äô(s)) \\ &amp;=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi‚Äô(s)\right]\tag{by \eqref{2}} \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right] \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi‚Äô(S_{t+1}))|S_t=s\right]\tag{by \eqref{3}} \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma\mathbb{E}_{\pi‚Äô}\left[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi‚Äô(S_{t+1})\right]|S_t=s\right] \\ &amp;=\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\right] \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|S_t=s\right] \\ &amp;\quad\vdots \\ &amp;\leq\mathbb{E}_{\pi‚Äô}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\dots|S_t=s\right] \\ &amp;=v_{\pi‚Äô}(s)
\end{align}</p>

<p>Consider the new <em>greedy policy</em>, $\pi‚Äô$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\pi$, given by
\begin{align}
\pi‚Äô(s)&amp;\doteq\arg\max_a q_\pi(s,a) \\ &amp;=\arg\max_a\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{4}\label{4} \\ &amp;=\arg\max_a\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]
\end{align}
By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.<br />
Suppose the new greedy policy, $\pi‚Äô$, is as good as, but not better than, $\pi$. Or in other words, $v_\pi=v_{\pi‚Äô}$. And from \eqref{4}, we have for all $s\in\mathcal{S}$,
\begin{align}
v_{\pi‚Äô}(s)&amp;=\max_a\mathbb{E}\left[R_{t+1}+\gamma v_{\pi‚Äô}(S_{t+1})|S_t=s,A_t=a\right] \\ &amp;=\max_a\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_{\pi‚Äô}(s‚Äô)\right]
\end{align}
which is the Bellman optimality equation for action-value function. And therefore, $v_{\pi‚Äô}$ must be $v_*$. Hence, <em>policy improvement</em> must give us a strictly better policy except when the original one is already optimal<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h3 id="policy-iteration">Policy Iteration</h3>
<p>Once we have obtained a better policy, $\pi‚Äô$, by improving a policy $\pi$ using $v_\pi$, we can repeat the same process by computing $v_{\pi‚Äô}$, and improve it to yield an even better $\pi‚Äô‚Äô$. Repeating it again and again, we get an iterative procedure to improve the policy
\begin{equation}
\pi_0\xrightarrow[]{\text{evaluation}}v_{\pi_0}\xrightarrow[]{\text{improvement}}\pi_1\xrightarrow[]{\text{evaluation}}v_{\pi_1}\xrightarrow[]{\text{improvement}}\pi_2\xrightarrow[]{\text{evaluation}}\dots\xrightarrow[]{\text{improvement}}\pi_*\xrightarrow[]{\text{evaluation}}v_*
\end{equation}
Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations.
This algorithm is called <strong>policy iteration</strong>. And here is the pseudocode of the policy iteration.</p>
<figure>
	<img src="/assets/images/2021-07-25/policy-iteration.png" alt="policy iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: Pseudocode of Policy Iteration</figcaption>
</figure>

<h3 id="value-iteration">Value Iteration</h3>
<p>Once we have obtained a better policy, $\pi‚Äô$, by improving a policy $\pi$ using $v_\pi$, we can repeat the same process by computing $v_{\pi‚Äô}$, and improve it to yield an even better $\pi‚Äô‚Äô$. Repeating it again and again, we get an iterative procedure to improve the policy</p>
<figure>
	&lt;img src="/assets/images/2021-07-25/value-iteration.png" alt="Backup diagram of value iteration update" width="360" height="200px"style="display: block; margin-left: auto; margin-right: auto;"/&gt;
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: Backup diagram of Value Iteration update</figcaption>
</figure>

<h2 id="references">References</h2>
<ol>
  <li>Reinforcement Learning: An Introduction - Richard S. Sutton &amp; Andrew G. Barto</li>
  <li><a href="https://www.davidsilver.uk/teaching/">UCL course on RL</a> - David Silver</li>
  <li>Algorithms for Reinforcement Learning - Csaba SzepesvaÃÅri</li>
  <li><a href="http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf">Markov Decision Processes and Dynamic Programming</a> - 
A. Lazaric</li>
  <li></li>
</ol>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">

      <p><a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET