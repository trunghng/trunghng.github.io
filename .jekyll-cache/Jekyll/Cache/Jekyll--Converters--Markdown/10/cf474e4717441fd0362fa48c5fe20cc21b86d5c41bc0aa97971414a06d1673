I"F8<blockquote>
  <p>Dont know yet</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#basics">Mathematical Basics</a>
    <ul>
      <li><a href="#gauss-dist">Gaussian (Normal) Distribution</a>
        <ul>
          <li><a href="#std-norm">Standard Normal</a></li>
        </ul>
      </li>
      <li><a href="#mvn">Multivariate Normal Distribution</a>
        <ul>
          <li><a href="#bvn">Bivariate Normal</a></li>
        </ul>
      </li>
      <li><a href="#kernels">Kernels</a>
        <ul>
          <li><a href="#kernel-func">Kernel functions</a>
            <ul>
              <li><a href="#rbf-kernels">RBF kernels</a></li>
              <li><a href="#mercer-kernels">Mercer (positive definite) kernels</a></li>
              <li><a href="#lin-kernels">Linear kernels</a></li>
              <li><a href="#matern-kernels">Matern kernels</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#gpr">Gaussian Process Regression</a></li>
  <li><a href="#bayes-opt">Bayesian Opitmization</a>
    <ul>
      <li><a href="#surrogate-model">Surrogate Model</a></li>
      <li><a href="#acquisition-func">Acquisition Functions</a></li>
      <li><a href="#opt-alg">Optimization Algorithm</a></li>
      <li><a href="#exp-imp">Expected Improvement</a></li>
      <li><a href="#implementation">Implementation</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p>
<h2 id="basics">Background</h2>
<p>Before diving into details, we need some necessary basic concepts.</p>

<h3 id="gauss-dist">Gaussian (Normal) Distribution</h3>
<p>A random variable $X$ is said to be <strong>Gaussian</strong> or to have the <strong>Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$</p>

<h4 id="std-normal">Standard Normal</h4>
<p>When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution <em>Standard Normal</em>.
\begin{equation}
X\sim\mathcal{N}(0,1)
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2}
\end{equation}
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\,dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt
\end{equation}
Below is some visualizations of Normal distribution.</p>

<figure>
	<img src="/assets/images/2021-11-22/normal.png" alt="normal distribution" width="900" height="380px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found <span><a href="https://github.com/trunghng/bayes-opt/blob/main/gauss-dist.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h3 id="mvn">Multivariate Normal Distribution</h3>
<p>A $k$-dimensional random vector $\mathbf{X}=(X_1,\dots,X_k)^T$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_kX_k
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_k$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})
\end{equation}
where
\begin{equation}
	\mathbf{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^T=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^T
\end{equation}
is the $k$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{k\times k}$ with
\begin{equation}
	\mathbf{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)
\end{equation}
We also have that $\mathbf{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Thus, the PDF of an MVN is defined as
\begin{equation}
f_X(x_1,\ldots,x_k)=\dfrac{1}{(2\pi)^{k/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[\dfrac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right]
\end{equation}
With this idea, <em>Standard Normal</em> distribution in multi-dimensional case can be defined as a Gaussian with mean $\mathbf{\mu}=0$ (here $0$ is an $k$-dimensional vector) and identity covariance matrix $\mathbf{\Sigma}=\mathbf{I}_{k\times k}$.</p>

<h4 id="bvn">Bivariate Normal</h4>
<p>When the number of dimensions in $\mathbf{X}$, $k=2$, this special case of MVN is called the <strong>Bivariate Normal (BVN)</strong>. An example of an BVN is shown as following.</p>

<figure>
	<img src="/assets/images/2021-11-22/bvn.png" alt="monte carlo method" width="750" height="350px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$. The code can be found <span><a href="https://github.com/trunghng/bayes-opt/blob/main/mvn.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h3 id="kernels">Kernels</h3>

<h4 id="kernel-func">Kernel functions</h4>
<p><strong>Kernel function</strong> is a real-valued function of two arguments
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)\in\mathbb{R},
\end{equation}
for $\textbf{x},\textbf{x}’\in\mathcal{X}$, which typically is symmetric (i.e., $\kappa(\textbf{x},\textbf{x}’)=\kappa(\textbf{x}’,\textbf{x})$), and nonnegative ($\kappa(\textbf{x},\textbf{x}’)\geq0$). These are some examples of kernel functions.</p>

<h5 id="rbf-kernels">RBF kernels</h5>
<p>The <strong>squared exponential kernel</strong> (SE kernel) or <strong>Gaussian kernel</strong> is defined by
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{1}{2}(\textbf{x}-\textbf{x}’)^T\mathbf{\Sigma}^{-1}(\textbf{x}-\textbf{x}’)\right)
\end{equation}
If $\mathbf{\Sigma}$ is a diagonal matrix, this can be written as
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{1}{2}\sum_{j=1}^{D}\frac{1}{\sigma_j^2}(x_j-x_j’)^2\right)
\end{equation}
We can interpret the $\sigma_j$ as defining the <strong>characteristic length scale</strong> of dimension $j$. The corresponding dimension of $\sigma_j$ is ignored if $\sigma_j=\infty$. This is known as the <strong>ARD kernel</strong> (Automatic Relevance Determination).<br />
If $\Sigma$ is spherical<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>, we get the isotropic kernel
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{\Vert\textbf{x}-\textbf{x}’\Vert}{2\sigma^2}\right)\tag{1}\label{1}
\end{equation}
Here $\sigma^2$ is known as the <strong>bandwidth</strong>. Since \eqref{1} can be seen as a function of $\Vert\textbf{x}-\textbf{x}’\Vert$, it is an example of a <strong>radial basis function</strong> or <strong>RBF kernel</strong>.</p>

<h5 id="mercer-kernels">Mercer (positive definite) kernels</h5>
<p>If the Gram matrix, defined by
\begin{equation}
\mathbf{K}=\begin{bmatrix}\kappa(\mathbf{x}_1,\mathbf{x}_1)&amp;\ldots&amp;\kappa(\mathbf{x}_1,\mathbf{x}_N) \\&amp;\vdots&amp;\\\kappa(\mathbf{x}_N,\mathbf{x}_1)&amp;\ldots&amp;\kappa(\mathbf{x}_N,\mathbf{x}_N)\end{bmatrix},
\end{equation}
is positive definite for any set $\left\{x_i\right\}_{i=1}^N$, the kernel $\kappa$ is called a <strong>Mercer kernel</strong>, or <strong>positive definite kernel</strong>.</p>

<p>The importance of Mercer kernels is the following reusult, which is known as  <strong>Mercer’s theorem</strong>. If the Gram matrix is positive definite, we can compute an eigenvector decomposition of it as
\begin{equation}
\mathbf{K}=\mathbf{U}^T\mathbf{\Lambda}\mathbf{U},
\end{equation}
where $\mathbf{\Lambda}$ is a diagonal matrix of eigenvalues $\lambda_i&gt;0$. Consider an element of $\mathbf{K}$
\begin{equation}
k_{ij}=\left(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:i}\right)^T\left(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:j}\right)
\end{equation}
If we let $\phi(\textbf{x}_i)=\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:i}$, then we can write
\begin{equation}
k_{ij}=\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)
\end{equation}
Thus we see that the intries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors $\mathbf{U}$. In general, if the kernel is Mercer, then there exists a function $\phi$ mapping $\textbf{x}\in\mathcal{X}$ to $\mathbb{R}^D$ such that
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\phi(\textbf{x})^T\phi(\textbf{x}’),
\end{equation}
where $\phi$ depends on the eigenfunctions of $\kappa$.</p>

<h5 id="lin-kernels">Linear kernels</h5>
<p>Deriving the feature vector implied by a kernel is only possible if the kernel is <em>Mercer</em>. However, deriving a kernel from a feature vector is easy. We have
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\phi(\textbf{x})^T\phi(\textbf{x}’)
\end{equation}
If $\phi(\textbf{x})=\textbf{x}$, we obtain a simple kernel called <strong>linear kernel</strong>
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\textbf{x}^T\textbf{x}’
\end{equation}</p>

<h5 id="matern-kernels">Matern kernels</h5>
<p>Let $\mathcal{X}\subset\mathbb{R}^D,\nu&gt;0,\ell&gt;0$. The <strong>Matern kenel</strong> $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is defined by
\begin{equation}
\kappa(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}r}{\ell}\right)^{\nu}K_{\nu}\left(\frac{\sqrt{2\nu}r}{\ell}\right)\tag{2}\label{2}
\end{equation}
where $\Gamma$ is the <em>gamma function</em>; $r=\Vert\textbf{x}-\textbf{x}’\Vert$ for $x,x’\in\mathcal{X}$ and $K_{\nu}$ is a <em>modified Bessel function</em>.</p>

<p>As $\nu\to\infty$, this approaches the SE kernel.</p>

<p>When $\nu$ is half-integer - i.e., $\nu=p+1/2$, for $p\geq0$, equation \eqref{2} can be reduced to a product of an exponential function and a polynomial of degree $p$, which can be written as
\begin{equation}
\kappa_{\nu=p+1/2}(r)=\exp\left(-\dfrac{\sqrt(2\nu)r}{\ell}\right)\dfrac{\Gamma(p+1)}{\Gamma(2p+1)}\sum_{i=0}^{p}\dfrac{(p+i)!}{i!(p-i)!}\left(\dfrac{\sqrt{8\nu}r}{\ell}\right)^{p-i}
\end{equation}</p>

<p>For an another special case, setting $\nu=1/2$ lets \eqref{2} become
\begin{equation}
\kappa_{\nu=1/2}=\exp\left(\dfrac{-r}{\ell}\right),
\end{equation}
which is known as <strong>Laplace</strong> or <strong>exponential kernel</strong>.</p>

<h2 id="gpr">Gaussian Process Regression</h2>
<p>A <strong>Gaussian process</strong> (GP) is a collection of random variables, any number of which have a joint Gaussian distribution.</p>

<p>A Gaussian process is completely specified by its mean function and covariance function. We define mean function $m(\textbf{x})$ and the covariance function or kernel $\kappa(\textbf{x},\textbf{x}’)$ of a process $f(\textbf{x})$ as
\begin{align}
m(\textbf{x})&amp;=\mathbb{E}\left[f(\textbf{x})\right] \\ \kappa(\textbf{x},\textbf{x}’)&amp;=\mathbb{E}\left[(f(\textbf{x})-m(\textbf{x}))(f(\textbf{x}’)-m(\textbf{x}’))^T\right]
\end{align}
and denote the Gaussian process as
\begin{equation}
f(x)\sim\mathcal{GP}\left(m(\textbf{x}),\kappa(\textbf{x},\textbf{x}’)\right)
\end{equation}
Hence, in this case, $\kappa$ is a Mercer kernel. And for any finite set of point $\textbf{x}_1,\dots,\textbf{x}_k\in\mathbb{R}^d$, the definition of GP define an MVN
\begin{equation}
f(\textbf{X})\sim\mathcal{N}\left(\mathbf{\mu},\textbf{K}\right),
\end{equation}
where $\textbf{X}=\left(\textbf{x}_1\dots\right)$</p>

<h2 id="references">References</h2>
<p>[1] C. E. Rasmussen &amp; C. K. I. Williams. <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>, MIT Press, 2006</p>

<p>[2] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a></p>

<p>[3] Stanford CS229. <a href="http://cs229.stanford.edu">Machine Learning</a></p>

<p>[4] Kevin P. Murphy. <a href="https://probml.github.io/pml-book/book0.html">Machine Learning: A Probabilistic Perspective</a>, MIT Press, 2012</p>

<p>[5] Christopher M. Bishop. <a href="https://www.amazon.com/Pattern-Recognition-Learning-Information-Setatistics/dp/0387310738">Pattern Recognition and Machine Learning</a></p>

<p>[6] Peter I. Frazier. <a href="https://arxiv.org/abs/1807.02811">A Tutorial on Bayesian Optimization</a></p>

<p>[7] Martin Krasser. <a href="https://krasserm.github.io/2018/03/21/bayesian-optimization/">Bayesian Optimization</a></p>

<p>[8] <a href="https://stats.stackexchange.com/users/28666/amoeba">amoeba</a>, <a href="https://stats.stackexchange.com/q/204599">What is an isotropic (spherical) covariance matrix?</a>, StackExchange</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The definition of covariance matrix $\mathbf{\Sigma}$ can be rewritten as
\begin{equation}
\mathbf{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation}
Let $\mathbf{z}\in\mathbb{R}^k$, we have
\begin{equation}
\Var(\mathbf{z}^T\mathbf{X})=\mathbf{z}^T\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^T\mathbf{\Sigma}\mathbf{z}
\end{equation}
And since $\Var(\mathbf{z}^T\mathbf{X})\geq0$, we also have that $\mathbf{z}^T\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\mathbf{\Sigma}$ is a positive semi-definite matrix. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A covariance matrix $\mathbf{C}$ is called <em>isotrophic</em>, or <em>spherical</em> if it is proportionate to the identity matrix
\begin{equation}
\mathbf{C}=\lambda\mathbf{I}
\end{equation} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET