I"º<<blockquote>

</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a>
    <ul>
      <li><a href="#lin-func-approx">Linear function approximation</a></li>
      <li><a href="#nn-func-approx">Neural network function approximation</a></li>
      <li><a href="#exp-replay">Experience replay</a></li>
      <li><a href="#target-net">Target network</a></li>
      <li><a href="#rmsprop">RMSProp</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the post <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_\pi(sâ€™)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V^*(sâ€™)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $sâ€™$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{t+1}(s)=\max_{a}\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_t(sâ€™)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_t\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banachâ€™s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-05-25/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\sum_{aâ€™}\pi(aâ€™\vert sâ€™)Q_\pi(sâ€™,aâ€™)\right] \\ &amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_\pi(sâ€™)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q^*(sâ€™,aâ€™)\right]\label{eq:qvi.2} \\ &amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V^*(sâ€™)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{t+1}(s,a)=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q_t(sâ€™,aâ€™)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-05-25/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{t+1}(s,a)=\mathbb{E}_{sâ€™\sim P(sâ€™\vert s,a)}\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q_t(sâ€™,aâ€™)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(sâ€™\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \eqref{eq:ql.1} can be approximated by sampling, as</p>
<ul id="number-list">
	<li>
		At a state, taking (sampling) action $a$ (e.g. due to an $\varepsilon$-greedy policy), we get the next state:
		\begin{equation}
		s'\sim P(s'\vert s,a)
		\end{equation}
	</li>
	<li>Consider the old estimate $Q_t(s,a)$.</li>
	<li>
		Consider the new sample estimate (target):
		\begin{equation}
		Q_\text{target}=R(s,a,s')+\gamma\max_{a'}Q_t(s',a')
		\end{equation}
	</li>
	<li>
		Append the new estimate into a running average to iteratively update Q-values:
		\begin{align}
		Q_{t+1}(s,a)&amp;=(1-\alpha)Q_t(s,a)+\alpha Q_\text{target} \\ &amp;=(1-\alpha)Q_t(s,a)+\alpha\left[R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\right]
		\end{align}
	</li>
</ul>

<p>This update rule is in form of a <strong>stochastic process</strong>, and thus, is <a href="#q-learning-td-convergence">guaranteed to converge</a> to the optimal $Q^*$, under the <a href="/2022/01/31/td-learning.html#stochastic-approx-condition">stochastic approximation conditions</a> for the learning rate $\alpha$.
\begin{equation}
\sum_{t=1}^{\infty}\alpha_t(s,a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{t=1}^{\infty}\alpha_t^2(s,a)&lt;\infty,\label{eq:ql.2}
\end{equation}
for all $(s,a)\in\mathcal{S}\times\mathcal{A}$.</p>

<p>The method is so called <strong>Q-learning</strong>, with pseudocode given below.</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an <a href="/2022/02/11/func-approx.html">approximated solution</a>.</p>

<p>In particular, we have tried to find an approximated action-value function $Q_\boldsymbol{\theta}(s,a)$, parameterized by a learnable vector $\boldsymbol{\theta}$, of the action-value function $Q(s,a)$, as
\begin{equation}
Q_\boldsymbol{\theta}(s,a)
\end{equation}
Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\boldsymbol{\theta}$ so as to minimize the loss function
\begin{equation}
L(\boldsymbol{\theta})=\mathbb{E}_{s,a\sim\mu(\cdot)}\Big[\big(Q(s,a)-Q_\boldsymbol{\theta}(s,a)\big)^2\Big]
\end{equation}
The resulting SGD update had the form
\begin{align}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t-\frac{1}{2}\alpha\nabla_\boldsymbol{\theta}\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]^2 \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.1}
\end{align}
However, we could not perform the exact update \eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $U_t$, which can be any approximation of $Q(s_t,a_t)$<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[U_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.2}
\end{equation}</p>

<h3 id="lin-func-approx">Linear function approximation</h3>
<p>Recall that, we have applied <a href="/2022/02/11/func-approx.html#lin-func-approx">linear methods</a> as our function approximators:
\begin{equation}
Q_\boldsymbol{\theta}(s,a)=\boldsymbol{\theta}^\text{T}\mathbf{f}(s,a),
\end{equation}
where $\mathbf{f}(s,a)$ represents the <strong>feature vector</strong>, (or <strong>basis functions</strong>) of the state-action pair $(s,a)$.
Linear function approximation allowed us to rewrite \eqref{eq:nql.2} in a simplified form
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[U_t-Q_\boldsymbol{\theta}(s_t,a_t)\big]\mathbf{f}(s_t,a_t)\label{eq:nql.4}
\end{equation}
The corresponding SGD method for Q-learning and Q-learning with linear approximation are respectively given in form of
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{aâ€™}Q_{\boldsymbol{\theta}_t}(s_{t+1},aâ€™)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.3}
\end{equation}
and
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{aâ€™}Q_{\boldsymbol{\theta}_t}(s_{t+1},aâ€™)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\mathbf{f}(s_t,a_t)
\end{equation}
However, in updating $\boldsymbol{\theta}_
{t+1}$, these methods both use the bootstrapping target:
\begin{equation}
R(s_t,a_t,s_{t+1})+\gamma\max_{aâ€™}Q_{\boldsymbol{\theta}_t}(s_{t+1},aâ€™), 
\end{equation}
which depends on the current value $\boldsymbol{\theta}_t$, and thus will be biased. As a consequence, \eqref{eq:nql.3} does not guarantee to converge<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Such methods are known as <strong>semi-gradient</strong> since they take into account the effect of changing the weight vector $\boldsymbol{\theta}_t$ on the estimate, but ignore its effect on the target.</p>

<h3 id="nn-func-approx">Neural network function approximation</h3>
<p>On the other hands, we have already known that a neural network with a particular settings for hidden layers and activation functions can approximate <a href="/2022/09/02/neural-nets.html#unv-approx">any</a> continuous functions on a compact subsets of $\mathbb{R}^n$, so how about using it with the Q-learning algorithm?</p>

<p>Specifically, we will be using neural network with weight $\boldsymbol{\theta}$ as a function approximator for Q-learning update. The network is referred as <strong>Q-network</strong>. The Q-network can be trained by minimizing a sequence of loss function $L_i(\boldsymbol{\theta}_i)$ that changes at each iteration $i$:
\begin{equation}
L_i(\boldsymbol{\theta}_i)=\mathbb{E}_{s,a\sim\rho(\cdot)}\Big[\big(y_i-Q_{\boldsymbol{\theta}_i}(s,a)\big)^2\Big],
\end{equation}
where
\begin{equation}
y_i=\mathbb{E}_{sâ€™\sim\mathcal{E}}\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q_{\boldsymbol{\theta}_{i-1}}(sâ€™,aâ€™)\vert s,a\right]
\end{equation}
is the target in iteration $i$, as the target $U_t$ for iteration $k$ in \eqref{eq:nql.4}; and where $\rho(s,a)$ is referred as the behavior policy.</p>

<h3 id="exp-replay">Experience replay</h3>
<p>Along with Q-network, the authors of deep-Q learning also introduce a mechanism called <strong>experience replay</strong>, which utilizes data efficiency and at the same time reduces the variance of the updates.</p>

<p>In particular, at each time step $t$, the <strong>experience</strong>, $e_t$, defined as
\begin{equation}
e_t=(s_t,a_t,r_t,s_{t+1})
\end{equation}
is added into a set $\mathcal{D}$ of size $N$, which is</p>

<h3 id="target-net">Target network</h3>

<h2 id="references">References</h2>
<p><span id="q-learning-td-convergence">[1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</span></p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[3] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[4] John N Tsitsiklis and Benjamin Van Roy. <a href="">An analysis of temporal-difference learning with function approximation</a>. Automatic Control, IEEE Transactions on, 42(5):674â€“690, 1997.</p>

<p>[5] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[6] Hado van Hasselt. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Double Q-learning</a>. NIPS 2010.</p>

<p>[7] Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</p>

<p>[8] Pieter Abbeel. <a href="https://youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL Series</a>, 2021.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In <strong>Monte Carlo control</strong>, the update target $U_t$ is chosen as the <strong>full return</strong> $G_t$, i.e.
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
and in (episodic on-policy) TD control methods, we use the <strong>TD target</strong> as the choice for $U_t$, i.e. for one-step TD methods such as <strong>one-step Sarsa</strong>, the update rule for $\boldsymbol{\theta}$ is given as
\begin{align*}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+1}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t) \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[R(s_t,a_t,s_{t+1})+\gamma Q_{\boldsymbol{\theta}_t}(s_{t+1},a_{t+1})-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{align*}
and for $n$-step TD method, for instance, <strong>$n$-step Sarsa</strong>, we instead have
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+n}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
where
\begin{equation*}
G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^n Q_{\boldsymbol{\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\hspace{1cm}t+n&lt;T
\end{equation*}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ and where $R_{t+1}\doteq R(s_t,a_t,s_{t+1})$.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The semi-gradient TD methods with linear-approximation are guaranteed to converge to the <strong>TD fixed point</strong> due to the <a href="/2022/02/11/func-approx.html#td-fixed-pt-proof">result</a> we have mentioned.Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET