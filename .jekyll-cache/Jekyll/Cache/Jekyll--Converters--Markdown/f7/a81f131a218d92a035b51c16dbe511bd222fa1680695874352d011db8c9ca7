I"©<blockquote>

  <!-- excerpt-end -->
</blockquote>

<ul>
  <li><a href="#mcts-vanilla">(vanilla) Monte Carlo Tree Search</a></li>
  <li><a href="#uct">Upper Confidence Bound for Trees (UCT)</a></li>
  <li><a href="#example">Example</a></li>
  <li><a href="#alphazero">AlphaZero</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="vanilla-mcts">(vanilla) Monte Carlo Tree Search</h2>

<h2 id="uct">Upper Confidence Bound for Trees (UCT)</h2>

<h2 id="example">Example</h2>

<h2 id="alphazero">AlphaZero</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[2] C. B. Browne et al. <a href="https://ieeexplore.ieee.org/document/6145622">A Survey of Monte Carlo Tree Search Methods</a>, in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012.</p>

<p>[3] Kocsis, L., Szepesv√°ri, C. (2006). <a href="https://doi.org/10.1007/11871842_29">Bandit Based Monte-Carlo Planning</a>. In: F√ºrnkranz, J., Scheffer, T., Spiliopoulou, M. (eds) Machine Learning: ECML 2006. ECML 2006. Lecture Notes in Computer Science, vol 4212. Springer, Berlin, Heidelberg.</p>

<p>[4] David Silver et al. <a href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a></p>

<p>[4] Shangtong Zhang. <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">Reinforcement Learning: An Introduction implementation</a>.</p>

<h2 id="footnotes">Footnotes</h2>
:ET