I"o<blockquote>
  <p>A note on convergence proofs for Q-learning by exploiting the connection with stochastic approximation and the idea of parallel asynchronous.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#q-learning-convergence">The convergence of Q-learning</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>In Q-learning, transition probabilities and costs are unknown but information of them is obtained either by simulation or by experimenting. Q-learning uses simulation or experimental information to estimate the expected cost-to-go. Additionally, the algorithm is recursive and each new piece of information is used for computing an additive correction term to the old estimates. As these correction terms are random, Q-learning therefore has the same general structure as the stochastic approximation algorithms.</p>

<h2 id="preliminaries">Preliminaries</h2>

<h2 id="q-learning-convergence">The convergence of Q-learning</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] T. Jaakkola &amp; M. I. Jordan &amp; S. P. Singh. <a href="doi: 10.1162/neco.1994.6.6.1185">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a> in Neural Computation, vol. 6, no. 6, pp. 1185-1201, Nov. 1994.</p>

<h2 id="footnotes">Footnotes</h2>
:ET