I"¾<!-- excerpt-end -->

<ul>
  <li><a href="#lambda-return">The $\lambda$-return</a></li>
  <li><a href="#td-lambda">TD($\lambda$)</a></li>
  <li><a href="#sarsa-lambda">Sarsa($\lambda$)</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lambda-return">The $\lambda$-return</h2>

<h2 id="td-lambda">TD($\lambda$)</h2>

<h2 id="sarsalambda">Sarsa($\lambda$)</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] Precup, Doina; Sutton, Richard S.; and Singh, Satinder. <a href="https://scholarworks.umass.edu/cs_faculty_pubs/80">Eligibility Traces for Off-Policy Policy Evaluation</a> (2000). ICML â€˜00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.</p>

<h2 id="footnotes">Footnotes</h2>
:ET