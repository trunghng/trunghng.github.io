I"$<blockquote>
  <p>A note on Inverse Reinforcement Learning</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#mdp">Markov decision processes</a></li>
      <li><a href="#irl">Inverse reinforcement learning</a></li>
    </ul>
  </li>
  <li><a href="#max-margin-methods">Max margin methods</a>
    <ul>
      <li><a href="#max-margin-proj">Max-margin &amp; Projection</a>
        <ul>
          <li><a href="#max-margin">Max-margin</a></li>
          <li><a href="#proj">Projection</a></li>
        </ul>
      </li>
      <li><a href="#max-margin-pln">Max margin planning</a></li>
      <li><a href="#learch">LEARCH</a></li>
    </ul>
  </li>
  <li><a href="#max-ent">Max entropy methods</a></li>
  <li><a href="#bayes">Bayesian methods</a>
    <ul>
      <li><a href="#birl">BIRL</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="mdp">Markov decision processes</h3>
<p>We begin by recalling the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html#mdp"><strong>Markov decision processes (MDP)</strong></a>.</p>

<p>A <strong>Markov decision process</strong>, or <strong>MDP</strong> is defined to be a tuple $(\mathcal{S},\mathcal{A},T,\mathcal{R},\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a set of states, represents the <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a set of actions, known as the <strong>action space</strong>.</li>
  <li>$T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>state transition probabilities</strong>, i.e., $T(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$\mathcal{R}$ denotes the <strong>reward function</strong>.</li>
  <li>$\gamma\in[0,1]$ is referred as <strong>discount factor</strong>.</li>
</ul>

<h3 id="irl">Inverse reinforcement learning</h3>
<p>The tuple $(\mathcal{S},\mathcal{A},T,\gamma)$ represents the an $MDP$ without a predefined reward function $\mathcal{R}$</p>

<h2 id="max-margin-methods">Max margin methods</h2>

<h3 id="max-margin-pln">Max margin planning</h3>

<h3 id="max-margin-proj">Max-margin &amp; Projection</h3>

<h4 id="max-margin">Max-margin</h4>

<h4 id="proj">Projection</h4>

<h4 id="learch">LEARCH</h4>

<h2 id="max-ent">Max entropy methods</h2>

<h2 id="references">References</h2>
<p>[1] Pieter Abbeel &amp; Andrew Y. Ng. <a href="https://doi.org/10.1145/1015330.1015430">Apprenticeship Learning via Inverse Reinforcement Learning</a>. ICML ‘04: Proceedings of the twenty-first international conference on Machine learning. July 2004.</p>

<p>[2] Nathan D. Ratliff, J. Andrew Bagnell &amp; Martin A. Zinkevich<a href="https://doi.org/10.1145/1143844.1143936">Maximum margin planning</a>. ICML ‘06: Proceedings of the 23rd international conference on Machine learning. June 2006</p>

<p>[3] Nathan Ratliff, David Silver &amp; J. Andrew (Drew) Bagnell. <a href="https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/">Learning to search: Functional gradient techniques for imitation learning</a>. Autonomous Robots. July 2009.</p>

<h2 id="footnotes">Footnotes</h2>
:ET