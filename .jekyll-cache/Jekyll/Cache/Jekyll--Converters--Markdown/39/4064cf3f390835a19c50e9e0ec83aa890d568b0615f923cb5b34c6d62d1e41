I"P<blockquote>
  <p>Recall that in the previous post, <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html"><strong>Dynamic Programming Algorithms For Solving Markov Decision Processes</strong></a>, we made an assumption about the complete knowledge of the environment. With <strong>Monte Carlo</strong> methods, we only require <em>experience</em> - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#mc-methods">Monte Carlo Methods</a></li>
  <li><a href="#mc-rl">Monte Carlo Methods in Reinforcement Learning</a>
    <ul>
      <li><a href="#mc-prediction">Monte Carlo Prediction</a>
        <ul>
          <li><a href="#first-mc-every-mc">First-visit MC vs. every-visit MC</a></li>
        </ul>
      </li>
      <li><a href="#mc-control">Monte Carlo Control</a>
        <ul>
          <li><a href="#mc-est-action-value">Monte Carlo Estimation of Action Values</a>
            <ul>
              <li><a href="#es">Exploring Starts</a></li>
            </ul>
          </li>
          <li><a href="#mc-policy-iteration">Monte Carlo Policy Iteration</a></li>
        </ul>
      </li>
      <li><a href="#on-policy-mc-control">On-policy Monte Carlo Control</a></li>
      <li><a href="#off-policy-mc-pred">Off-policy Monte Carlo Prediction</a>
        <ul>
          <li><a href="#coverage">Assumption of Coverage</a></li>
          <li><a href="#is">Importance Sampling</a></li>
          <li><a href="#is-off-policy">Off-policy Monte Carlo Prediction via Importance Sampling</a></li>
          <li><a href="#imp-off-policy-is">Incremental Implementation for Off-policy MC Prediction using IS</a>
            <ul>
              <li><a href="#incremental-method">Incremental Method</a></li>
              <li><a href="#applying-off-policy-is">Applying to Off-policy MC Prediction using IS</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#off-policy-mc-control">Off-policy Monte Carlo Control</a>
        <ul>
          <li><a href="#example">Example - Racetrack</a></li>
        </ul>
      </li>
      <li><a href="#discounting-aware-is">Discounting-aware Importance Sampling</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="mc-methods">Monte Carlo Methods<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></h2>
<p><strong>Monte Carlo</strong>, named after a casino in Monaco, simulates complex probabilistic events using simple random events, such as tossing a pair of dice to simulate the casino‚Äôs overall business model.</p>

<figure>
	<img src="/assets/images/2021-08-21/mc-pi.gif" alt="monte carlo method" width="480" height="360px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Using Monte Carlo method to approximate the value of $\pi$</figcaption>
</figure>
<p><br /></p>

<p>Monte Carlo methods have been used in several different tasks:</p>
<ol>
  <li>Simulating a system and its probability distribution $\pi(x)$
\begin{equation}
x\sim\pi(x)
\end{equation}</li>
  <li>Estimating a quantity through Monte Carlo integration
\begin{equation}
c=\mathbb{E}_\pi\left[f(x)\right]=\int\pi(x)f(x)\,dx
\end{equation}</li>
  <li>Optimizing a target function to find its modes (maxima or minima)
\begin{equation}
x^*=\arg\max\pi(x)
\end{equation}</li>
  <li>Learning a parameters from a training set to optimize some loss functions, such as the maximum likelihood estimation from a set of examples $\{x_i,i=1,2,\dots,M\}$
\begin{equation}
\Theta^*=\arg\max\sum_{i=1}^{M}\log p(x_i;\Theta)
\end{equation}</li>
  <li>Visualizing the energy landscape of a target function</li>
</ol>

<h2 id="mc-rl">Monte Carlo Methods in Reinforcement Learning</h2>
<p>Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns. Here, we define Monte Carlo methods only for episodic tasks. Or in other words, they learn from complete episodes of experience.</p>

<h3 id="mc-prediction">Monte Carlo Prediction<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></h3>
<p>Since the value of a state $v_\pi(s)=\mathbb{E}_\pi\left[G_t|S_t=s\right]$ is defined as the expectation of the return when the process is started from the given state $s$, an obvious way of estimating this value from experience is to compute observed mean returns after visits to that state. As more returns are observed, the average should converge to the expected value. This is an instance of the so-called <em>Monte Carlo method</em>.</p>

<p>In particular, suppose we wish to estimate $v_\pi(s)$ given a set of episodes obtained by following $\pi$ and passing through $s$. Each time state $s$ appears in an episode, we call it a <em>visit</em> to $s$. There are two types of Monte Carlo methods:</p>
<ul>
  <li><em>First-visit MC method</em>
    <ul>
      <li>estimates $v_\pi(s)$ as the average of the returns that have followed the <em>first visit</em> to $s$.</li>
      <li>We call the first time $s$ is visited in an episode the <em>first visit</em> to $s$.</li>
    </ul>
  </li>
  <li><em>Every-visit MC method</em>
    <ul>
      <li>estimates $v_\pi(s)$ as the average of the returns that have followed all visits to to $s$.</li>
    </ul>
  </li>
</ul>

<p>The sample mean return for state $s$ is:
\begin{equation}
v_\pi(s)=\dfrac{\sum_{t=1}^{T}ùüô\left(S_t=s\right)G_t}{\sum_{t=1}^{T}ùüô\left(S_t=s\right)},
\end{equation}
where $ùüô(\cdot)$ is an indicator function. In the case of <em>first-visit MC</em>, $ùüô\left(S_t=s\right)$ returns $1$ only in the first time $s$ is encountered in an episode. And for <em>every-visit MC</em>, $ùüô\left(S_t=s\right)$ gives value of $1$ every time $s$ is visited.</p>

<p>Here is pseudocode of the <em>first-visit MC prediction</em>, for estimating $V\approx v_\pi$</p>
<figure>
	<img src="/assets/images/2021-08-21/mc-prediction.png" alt="iterative policy evaluation pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="first-mc-every-mc">First-visit MC vs. every-visit MC</h4>
<p>Both methods converge to $v_\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $\frac{1}{\sqrt{n}}$, where $n$ is the number of returns averaged.</p>

<figure>
	<img src="/assets/images/2021-08-21/first-visit-every-visit.png" alt="first-visit MC vs every-visit MC" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: Summary of Statistical Results comparing first-visit and every-visit MC method</figcaption>
</figure>
<p><br /></p>

<h3 id="mc-control">Monte Carlo Control<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></h3>

<h4 id="mc-est-action-value">Monte Carlo Estimation of Action Values</h4>
<p>When model is not available, it is particular useful to estimate <em>action values</em> rather than <em>state values</em> (which alone are insufficient to determine a policy). We must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$. To achieve this, we first consider the policy evaluation problem for action values.</p>

<p>Similar to when using MC method to estimate $v_\pi(s)$, we can use both first-visit MC and every-visit MC to approximate the value of $q_\pi(s,a)$. The only thing we need to keep in mind is, in this case, we work with visits to a state-action pair rather than to a state. Likewise, we define two types of MC methods for estimating $q_\pi(s,a)$:</p>
<ul>
  <li><em>First-visit MC method</em>
    <ul>
      <li>estimates $q_\pi(s,a)$ as the average of the returns following the first time in each episode that the state $s$ was visited and the action $a$ was selected</li>
    </ul>
  </li>
  <li><em>Every-visit MC method</em>
    <ul>
      <li>estimates $q_\pi(s,a)$ as the average of the returns that have followed all the visits to state-action pair $(s,a)$.</li>
    </ul>
  </li>
</ul>

<h5 id="es">Exploring Starts</h5>
<p>However, here we must exercise <em>exploration</em>. Because many state-action pairs may never be visited, and if $\pi$ is a deterministic policy, then returns of only single one action for each state will be observed. That leads to the consequence that the other actions will not be evaluated since there are no returns to average.</p>

<p>There is one way to achieve this, which is called <em>exploring starts</em> - an assumption that assumes the episodes <em>start in a state-action pair</em>, and that every pair has a <em>nonzero</em> probability of being selected as the start. This assumption assures that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.</p>

<h4 id="mc-policy-iteration">Monte Carlo Policy Iteration</h4>
<p>To learn the optimal policy by MC, we apply the idea of <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#gpi">GPI</a>:
\begin{equation}
\pi_0\overset{\small \text{E}}{\rightarrow}q_{\pi_0}\overset{\small \text{I}}{\rightarrow}\pi_1\overset{\small \text{E}}{\rightarrow}q_{\pi_1}\overset{\small \text{I}}{\rightarrow}\pi_2\overset{\small \text{E}}{\rightarrow}\dots\overset{\small \text{I}}{\rightarrow}\pi_*\overset{\small \text{E}}{\rightarrow}q_*
\end{equation}
In particular,</p>
<ol>
  <li><em>Policy evaluation</em> (denoted as $\overset{\small\text{E}}{\rightarrow}$): estimates action value function $q_\pi(s,a)$ using the episode generated from $s, a$, following by current policy $\pi$
\begin{equation}
q_\pi(s,a)=\dfrac{\sum_{t=1}^{T}ùüô\left(S_t=s,A_t=a\right)G_t}{\sum_{t=1}^{T}ùüô\left(S_t=s,A_t=a\right)}
\end{equation}</li>
  <li><em>Policy improvement</em> (denoted as $\overset{\small\text{I}}{\rightarrow}$): makes the policy <em>greedy</em> with the current value function (action value function in this case)
\begin{equation}
\pi(s)\doteq\arg\max_{a\in\mathcal{A(s)}} q(s,a)
\end{equation}
The policy improvement can be done by constructing each $\pi_{k+1}$ as the greedy policy w.r.t $q_{\pi_k}$ because
\begin{align}
q_{\pi_k}\left(s,\pi_{k+1}(s)\right)&amp;=q_{\pi_k}\left(s,\arg\max_a q_{\pi_k}(s,a)\right) \\ &amp;=\max_a q_{\pi_k}(s,a) \\ &amp;\geq q_{\pi_k}\left(s,\pi_k(s)\right) \\ &amp;\geq v_{\pi_k}(s)
\end{align}
Therefore, by the <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-improvement">policy improvement theorem</a>, we have that $\pi_{k+1}\geq\pi_k$.</li>
</ol>
<figure>
	<img src="/assets/images/2021-08-21/gpi.png" alt="GPI" width="150" height="150px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: MC policy iteration</figcaption>
</figure>
<p><br />
To solve this problem with Monte Carlo policy iteration, in the 1998 version of ‚Äò‚Äò<em>Reinforcement Learning: An Introduction</em>‚Äù, authors of the book introduced <strong>Monte Carlo ES</strong> (MCES), for Monte Carlo with <em>Exploring Starts</em>.</p>

<p>In MCES, value function is approximated by simulated returns and a greedy policy is selected at each iteration. Although MCES does not converge to any sub-optimal policy, the convergence to optimal fixed point is still an open question. For solutions in particular settings, you can check out some results like Tsitsiklis (2002), Chen (2018), Liu (2020).<br />
Down below is pseudocode of the Monte Carlo ES.</p>
<figure>
	<img src="/assets/images/2021-08-21/mces.png" alt="monte carlo es pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="on-policy-mc-control">On-policy Monte Carlo Control<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></h3>
<p>In the previous section, we used the assumption of <a href="#es">exploring starts</a> (ES) to design a Monte Carlo control method called MCES. In this part, without making that impractical assumption, we will be talking about another Monte Carlo control method.</p>

<p>In <em>on-policy control methods</em>, the policy is generally <em>soft</em> (i.e., $\pi(a|s)&gt;0,\forall s\in\mathcal{S},a\in\mathcal{A(s)}$, but gradually shifted closer and closer to a deterministic optimal policy). We can not simply improve the policy by following a greedy policy, since no exploration will take place. Then to get rid of ES, we use the on-policy MC method with $\varepsilon$-<em>greedy</em> policies, e.g, most of the time they choose an action that maximal estimated action value, but with probability of $\varepsilon$ they instead select an action at random. Specifically,</p>
<ul>
  <li>$Pr(\small\textit{non-greedy action})=\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$</li>
  <li>$Pr(\small\textit{greedy action})=1-\varepsilon+\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$</li>
</ul>

<p>The $\varepsilon$-greedy policies are examples of $\varepsilon$-<em>soft</em> policies, defined as ones for which $\pi(a\vert s)\geq\frac{\varepsilon}{\vert\mathcal{A(s)}\vert}$ for all states and actions, for some $\varepsilon&gt;0$. Among $\varepsilon$-soft policies, $\varepsilon$-greedy policies are in some sense those that closest to greedy.</p>

<p>We have that any $\varepsilon$-greedy policy w.r.t $q_\pi$ is an <em>improvement</em> over any $\varepsilon$-soft policy is assured by the <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-improvement">policy improvement theorem</a>.</p>

<p><strong>Proof</strong><br />
Let $\pi‚Äô$ be the $\varepsilon$-greedy. The conditions of the policy improvement theorem apply because for any $s\in\mathcal{S}$, we have:
\begin{align}
q_\pi\left(s,\pi‚Äô(s)\right)&amp;=\sum_a\pi‚Äô(a|s)q_\pi(s,a) \\ &amp;=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\max_a q_\pi(s,a) \\ &amp;\geq\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\sum_a\dfrac{\pi(a|s)-\frac{\varepsilon}{\vert\mathcal{A}(s)\vert}}{1-\varepsilon}q_\pi(s,a) \\ &amp;=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)-\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a) \\ &amp;=v_\pi(s)
\end{align}
(In the third step, we use the fact that the latter $\sum$ is a weighted average over $q_\pi(s,a)$). Thus, by the theorem, $\pi‚Äô\geq\pi$. The equality holds when both $\pi‚Äô$ and $\pi$ are optimal policies among the $\varepsilon$-soft ones.</p>

<p>Pseudocode of the complete algorithm is given below.</p>
<figure>
	<img src="/assets/images/2021-08-21/on-policy-mc-control.png" alt="monte carlo es pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="off-policy-mc-pred">Off-policy Monte Carlo Prediction<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></h3>
<p>When working with control methods, we have to solve a dilemma about <em>exploitation</em> and <em>exploration</em>. In other words, we have to evaluate a policy from episodes generated by following an exploratory policy.</p>

<p>A straightforward way to solve this problem is to use two different policies, one that is learned about and becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy is being learned about is called the <em>target policy</em>, whereas <em>behavior policy</em> is the one which is used to generate behavior.</p>

<p>In this section, we will be considering the off-policy method on prediction task, on which both target (denoted as $\pi$) and behavior (denoted as $b$) policies are fixed and given. Particularly, we wish to estimate $v_\pi$ or $q_\pi$ from episodes retrieved from following another policy $b$, where $\pi\neq b$.</p>

<h4 id="coverage">Assumption of Coverage</h4>
<p>In order to use episodes from $b$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $b$. That means, we assume that $\pi(a|s)&gt;0$ implies $b(s|a)&gt;0$, which leads to a result that $b$ must be stochastic, while $\pi$ may be deterministic since $\pi\neq b$. This is the assumption of <strong>coverage</strong>.</p>

<h4 id="is">Importance Sampling</h4>
<p>Let $X$ be a variable (or set of variables) that takes on values in some space $\textit{Val}(X)$. <strong>Importance sampling</strong> (IS) is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$, typically called the <em>target distribution</em>. We can estimate this expectation by generating samples $x[1],\dots,x[M]$ from $P$, and then estimating
\begin{equation}
\mathbb{E}_P\left[f\right]\approx\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])
\end{equation}
In some cases, it might be impossible or computationally very expensive to generate samples from $P$, we instead prefer to generate samples from a different distribution, $Q$, known as the <em>proposal distribution</em> (or <em>sampling distribution</em>).</p>
<ol>
  <li>
    <p><strong>Unnormalized Importance Sampling</strong><br />
If we generate samples from $Q$ instead of $P$, we cannot simply average the $f$-value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&amp;=\sum_x f(x)P(x) \\ &amp;=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &amp;=\mathbb{E}_{Q(X)}\left[f(X)\dfrac{P(X)}{Q(X)}\right]\tag{1}\label{1}
\end{align}
Based on this observation \eqref{1}, we can use the standard estimator for expectations relative to $Q$. We generate a set of sample $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, and then estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])\dfrac{P(x[m])}{Q(x[m])}\tag{2}\label{2},
\end{equation}
where $\hat{\mathbb{E}}$ denotes empirical expectation. We call this estimator the <strong>unnormalized importance sampling estimator</strong>, this method is also often called <strong>unweighted importance sampling</strong>. The factor $\frac{P(x[m])}{Q(x[m])}$ (denoted as $w(x[m])$) can be viewed as a correction weight to the term $f(x[m])$, which we would have used had $Q$ been our target distribution.</p>
  </li>
  <li>
    <p><strong>Normalized Importance Sampling</strong><br />
In many situations, we have that $P$ is known only up to a normalizing constant $Z$. Particularly, what we have access to is a distribution $\tilde{P}(X)=ZP(X)$.<br />
Thus, rather than to define the weights relative to $P$ as above, we define:
\begin{equation}
w(X)\doteq\dfrac{\tilde{P}(X)}{Q(X)}
\end{equation}
We have that the weight $w(X)$ is a random variable, and has expected value equal to $Z$:
\begin{equation}
\mathbb{E}_{Q(X)}\left[w(X)\right]=\sum_x Q(x)\dfrac{\tilde{P}(x)}{Q(x)}=\sum_x\tilde{P}(x)=Z
\end{equation}
Hence, this quantity is the normalizing constant of the distribution $\tilde{P}$. We can now rewrite \eqref{1} as:
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&amp;=\sum_x P(x)f(x) \\ &amp;=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &amp;=\dfrac{1}{Z}\sum_x Q(x)f(x)\dfrac{\tilde{P}(x)}{Q(x)} \\ &amp;=\dfrac{1}{Z}\mathbb{E}_{Q(X)}\left[f(X)w(X)\right] \\ &amp;=\dfrac{\mathbb{E}_{Q(X)}\left[f(X)w(X)\right]}{\mathbb{E}_{Q(X)}\left[w(X)\right]}\tag{3}\label{3}
\end{align}
We can use an empirical estimator for both the numerator and denominator. Given $M$ samples $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, we can estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{\sum_{m=1}^{M}f(x[m])w(x[m])}{\sum_{m=1}^{M}w(x[m])}\tag{4}\label{4}
\end{equation}
We call this estimator the <strong>normalized importance sampling estimator</strong> (or <strong>weighted importance sampling estimator</strong>).</p>
  </li>
</ol>

<h4 id="is-off-policy">Off-policy Monte Carlo Prediction via Importance Sampling</h4>
<p>We apply IS to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the <em>importance sampling ratio</em> (which we denoted as $w$ as above, but now we change the notation to $\rho$ in order to follows the book).</p>

<p>The probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\dots,S_T$, occurring under any policy $\pi$ given starting state $s$ is:
\begin{align}
Pr(A_t,S_{t+1},\dots,S_T|S_t,A_{t:T-1}\sim\pi)&amp;=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\dots p(S_T|S_{T-1},A_{T-1}) \\ &amp;=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)
\end{align}
Thus, the importance sampling ratio as we defined is:
\begin{equation}
\rho_{t:T-1}\doteq\dfrac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_t,A_t)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_t,A_t)}=\prod_{k=1}^{T-1}\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}
\end{equation}
which depends only on the two policies and the sequence, not on the MDP.</p>

<p>Since $v_b(s)=\mathbb{E}\left[G_t|S_t=s\right]$, then we have
\begin{equation}
\mathbb{E}\left[\rho_{t:T-1}G_t|S_t=s\right]=v_\pi(s)
\end{equation}
To estimate $v_\pi(s)$, we simply scale the returns by the ratios and average the results:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\vert\mathcal{T}(s)\vert},\tag{5}\label{5}
\end{equation}
where $\mathcal{T}(s)$ is the set of all states in which $s$ is visited (only for every-visit). For a first-visit,$\mathcal{T}(s)$ would only include time steps that were first visits to $s$ within their episodes. $T(t)$ denotes the first time of termination following time $t$, and $G_t$ denotes the return after $t$ up through $T(t)$.</p>

<p>When importance sampling is done as simple average in this way, we call it <em>ordinary importance sampling</em> (OIS) (which corresponds to <em>unweighted importance sampling</em> in the previous section).</p>

<p>And the one corresponding to <em>weighted importance sampling</em> (WIS), which uses a weighted average, is defined as:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}},\tag{6}\label{6}
\end{equation}
or zero if the denominator is zero.</p>

<h4 id="imp-off-policy-is">Incremental Implementation for Off-policy MC Prediction using IS</h4>

<h5 id="incremental-method">Incremental Method</h5>
<p><strong>Incremental method</strong> is a way of updating averages with small, constant computation required to process each new reward instead of maintaining a record of all the rewards and then performing this computation whenever the estimated value was needed. It follows the general rule:
\begin{equation}
NewEstimate\leftarrow OldEstimate+StepSize\left[Target-OldEstimate\right]
\end{equation}</p>

<h5 id="applying-off-policy-is">Applying to Off-policy MC Prediction using IS</h5>
<p>In ordinary IS, the returns are scaled by the IS ratio $\rho_{t:T(t)-1}$, then simply averaged, as in \eqref{5}. Thus, it‚Äôs easy to apply incremental method to OIS.</p>

<p>For WIS, as in the equation \eqref{6}, we have to form a weighted average of the returns, and a slightly different incremental incremental algorithm is required.
Suppose we have a sequence of returns $G_1,G_2,\dots,G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$ (e.g., $W_i=\rho_{t_i:T(t_i)}$). We wish to form the estimate
\begin{equation}
V_n\doteq\dfrac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},\hspace{1cm}n\geq2
\end{equation}
and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first $n$ returns. The update rule for $V_n$ is
\begin{equation}
V_{n+1}\doteq V_n+\dfrac{W_n}{C_n}\big[G_n-V_n\big],\hspace{1cm}n\geq1,
\end{equation}
and
\begin{equation}
C_{n+1}\doteq C_n+W_{n+1},
\end{equation}
where $C_0=0$. And here is pseudocode of our algorithm.</p>
<figure>
	<img src="/assets/images/2021-08-21/off-policy-mc-prediction.png" alt="off-policy MC prediction pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="off-policy-mc-control">Off-policy Monte Carlo Control</h3>
<p>Similarly, we develop the algorithm for off-policy MC control, based on GPI and weighted IS, for estimating $\pi_*$ and $q_*$, which is shown below. The target policy $\pi\approx\pi_*$ is the greedy policy w.r.t $Q$, which is an estimate of $q_\pi$. The behavior policy, $b$, can be anything, but in order to assure convergence of $\pi$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be guaranteed by choosing $b$ to be $\varepsilon$-soft. The policy $\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$, which may change between or even within episodes.</p>
<figure>
	<img src="/assets/images/2021-08-21/off-policy-mc-control.png" alt="off-policy MC control pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="example">Example - Racetrack</h4>
<p>(This example is taken from <em>Exercise 5.12</em>, <em>Reinforcement Learning: An Introduction</em> book.)</p>

<p><strong>Problem</strong><br />
Consider driving a race car around a turn like those shown in <strong><em>Figure 4</em></strong>. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car‚Äôs location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).</p>
<figure>
	<img src="/assets/images/2021-08-21/racetrack.png" alt="racetrack" width="200" height="300px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: A turn for the racetrack task</figcaption>
</figure>
<p><br /></p>

<p><strong>Solution code</strong><br />
The source code can be found <a href="https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-5/racetrack.py">here</a>.</p>

<p><button type="button" class="collapsible" id="codeP">Click to show/hide the code</button></p>
<div class="codePanel" id="codePdata">
  <p><br />
We begin by importing some useful packages.</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</code></pre></div>  </div>

  <p>Next, we define our environment</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RaceTrack</span><span class="p">:</span>

	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">NOISE</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">MAX_VELOCITY</span> <span class="o">=</span> <span class="mi">4</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">MIN_VELOCITY</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">starting_line</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">track</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">car_position</span> <span class="o">=</span> <span class="bp">None</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">_load_track</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">_generate_start_state</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">_generate_start_state</span><span class="p">()</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>


	<span class="k">def</span> <span class="nf">get_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">car_position</span><span class="p">.</span><span class="n">copy</span><span class="p">(),</span> <span class="bp">self</span><span class="p">.</span><span class="n">velocity</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>


	<span class="k">def</span> <span class="nf">_generate_start_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">starting_line</span><span class="p">))</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">car_position</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">starting_line</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>


	<span class="k">def</span> <span class="nf">take_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
		<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">is_terminal</span><span class="p">():</span>
			<span class="k">return</span> <span class="mi">0</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">_update_state</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
		<span class="k">return</span> <span class="o">-</span><span class="mi">1</span>


	<span class="k">def</span> <span class="nf">_update_state</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
		<span class="c1"># update velocity
</span>		<span class="c1"># with probability of 0.1, keep the velocity unchanged
</span>		<span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">):</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">action</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">minimum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">velocity</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">MAX_VELOCITY</span><span class="p">)</span>
			<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">velocity</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">MIN_VELOCITY</span><span class="p">)</span>

		<span class="c1"># update car position
</span>		<span class="k">for</span> <span class="n">tstep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">MAX_VELOCITY</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
			<span class="n">t</span> <span class="o">=</span> <span class="n">tstep</span> <span class="o">/</span> <span class="bp">self</span><span class="p">.</span><span class="n">MAX_VELOCITY</span>
			<span class="n">position</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">car_position</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">*</span> <span class="n">t</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>

			<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">position</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">position</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
				<span class="k">return</span>
			<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">position</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">position</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">car_position</span> <span class="o">=</span> <span class="n">position</span>
				<span class="bp">self</span><span class="p">.</span><span class="n">velocity</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
				<span class="k">return</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">car_position</span> <span class="o">=</span> <span class="n">position</span>


	<span class="k">def</span> <span class="nf">_load_track</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
		<span class="n">y_len</span><span class="p">,</span> <span class="n">x_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
		<span class="bp">self</span><span class="p">.</span><span class="n">track</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_len</span><span class="p">):</span>
			<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_len</span><span class="p">):</span>
				<span class="n">pt</span> <span class="o">=</span> <span class="n">grid</span><span class="p">[</span><span class="n">y</span><span class="p">][</span><span class="n">x</span><span class="p">]</span>
				<span class="k">if</span> <span class="n">pt</span> <span class="o">==</span> <span class="s">'W'</span><span class="p">:</span>
					<span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
				<span class="k">elif</span> <span class="n">pt</span> <span class="o">==</span> <span class="s">'o'</span><span class="p">:</span>
					<span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
				<span class="k">elif</span> <span class="n">pt</span> <span class="o">==</span> <span class="s">'-'</span><span class="p">:</span>
					<span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
				<span class="k">else</span><span class="p">:</span>
					<span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">=</span> <span class="mi">2</span>
		<span class="c1"># rotate the track in order to sync the track with actions
</span>		<span class="bp">self</span><span class="p">.</span><span class="n">track</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">fliplr</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">)</span>
		<span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">y_len</span><span class="p">):</span>
			<span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">x_len</span><span class="p">):</span>
				<span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
					<span class="bp">self</span><span class="p">.</span><span class="n">starting_line</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

	<span class="k">def</span> <span class="nf">is_terminal</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
		<span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">track</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">car_position</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">car_position</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">==</span> <span class="mi">2</span>
</code></pre></div>  </div>
  <p>We continue by defining our behavior policy and algorithm.</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">behavior_policy</span><span class="p">(</span><span class="n">track</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
	<span class="n">index</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">track</span><span class="p">.</span><span class="n">actions</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">track</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">index</span><span class="p">])</span>


<span class="k">def</span> <span class="nf">off_policy_MC_control</span><span class="p">(</span><span class="n">episodes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">grid</span><span class="p">):</span>
	<span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
	<span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span> <span class="o">-</span> <span class="mi">40</span>
	<span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
	<span class="n">pi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">int16</span><span class="p">)</span>
	<span class="n">track</span> <span class="o">=</span> <span class="n">RaceTrack</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
	<span class="c1"># for epsilon-soft greedy policy
</span>	<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

	<span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">episodes</span><span class="p">)):</span>
		<span class="c1"># print('episode ', ep+1)
</span>		<span class="n">track</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>
		<span class="n">trajectory</span> <span class="o">=</span> <span class="p">[]</span>
		<span class="k">while</span> <span class="ow">not</span> <span class="n">track</span><span class="p">.</span><span class="n">is_terminal</span><span class="p">():</span>
			<span class="n">state</span> <span class="o">=</span> <span class="n">track</span><span class="p">.</span><span class="n">get_state</span><span class="p">()</span>
			<span class="n">s_x</span><span class="p">,</span> <span class="n">s_y</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
			<span class="n">s_vx</span><span class="p">,</span> <span class="n">s_vy</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
			<span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
				<span class="n">action</span> <span class="o">=</span> <span class="n">pi</span><span class="p">[</span><span class="n">s_x</span><span class="p">,</span> <span class="n">s_y</span><span class="p">,</span> <span class="n">s_vx</span><span class="p">,</span> <span class="n">s_vy</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
			<span class="k">else</span><span class="p">:</span>
				<span class="n">action</span> <span class="o">=</span> <span class="n">behavior_policy</span><span class="p">(</span><span class="n">track</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
			<span class="n">reward</span> <span class="o">=</span> <span class="n">track</span><span class="p">.</span><span class="n">take_action</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
			<span class="n">trajectory</span><span class="p">.</span><span class="n">append</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">])</span>
		<span class="n">G</span> <span class="o">=</span> <span class="mi">0</span>
		<span class="n">W</span> <span class="o">=</span> <span class="mi">1</span>
		<span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
			<span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">trajectory</span><span class="p">.</span><span class="n">pop</span><span class="p">()</span>
			<span class="n">G</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">G</span> <span class="o">+</span> <span class="n">reward</span>
			<span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
			<span class="n">a_x</span><span class="p">,</span> <span class="n">a_y</span> <span class="o">=</span> <span class="n">action</span>
			<span class="n">s_a</span> <span class="o">=</span> <span class="p">(</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span><span class="p">,</span> <span class="n">a_x</span><span class="p">,</span> <span class="n">a_y</span><span class="p">)</span>
			<span class="n">C</span><span class="p">[</span><span class="n">s_a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span>
			<span class="n">Q</span><span class="p">[</span><span class="n">s_a</span><span class="p">]</span> <span class="o">+=</span> <span class="n">W</span><span class="o">/</span><span class="n">C</span><span class="p">[</span><span class="n">s_a</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">G</span><span class="o">-</span><span class="n">Q</span><span class="p">[</span><span class="n">s_a</span><span class="p">])</span>
			<span class="n">q_max</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1e5</span>
			<span class="n">a_max</span> <span class="o">=</span> <span class="bp">None</span>
			<span class="k">for</span> <span class="n">act</span> <span class="ow">in</span> <span class="n">track</span><span class="p">.</span><span class="n">actions</span><span class="p">:</span>
				<span class="n">sa_max</span> <span class="o">=</span> <span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span><span class="p">,</span> <span class="n">act</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">act</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
				<span class="k">if</span> <span class="n">Q</span><span class="p">[</span><span class="n">sa_max</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">q_max</span><span class="p">:</span>
					<span class="n">q_max</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">sa_max</span><span class="p">]</span>
					<span class="n">a_max</span> <span class="o">=</span> <span class="n">act</span>
			<span class="n">pi</span><span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">a_max</span>
			<span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="p">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">pi</span><span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">action</span><span class="p">):</span>
				<span class="k">break</span>
			<span class="n">W</span> <span class="o">*=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">epsilon</span><span class="o">+</span><span class="n">epsilon</span><span class="o">/</span><span class="mi">9</span><span class="p">)</span>
	<span class="k">return</span> <span class="n">pi</span>
</code></pre></div>  </div>
  <p>And wrapping everything up with the main function.</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
	<span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
	<span class="n">episodes</span> <span class="o">=</span> <span class="mi">10000</span>
	<span class="n">grid</span> <span class="o">=</span> <span class="p">[</span><span class="s">'WWWWWWWWWWWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWWooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'WWWoooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'WWWoooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'WWooooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'Woooooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'Woooooooooooooooo+'</span><span class="p">,</span>
          <span class="s">'WooooooooooWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WoooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWooooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWoooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWWooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWWooooooWWWWWWWW'</span><span class="p">,</span>
          <span class="s">'WWWW------WWWWWWWW'</span><span class="p">]</span>
	<span class="n">policy</span> <span class="o">=</span> <span class="n">off_policy_MC_control</span><span class="p">(</span><span class="n">episodes</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
	<span class="n">track_</span> <span class="o">=</span> <span class="n">RaceTrack</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
	<span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid</span><span class="p">)</span>
	<span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">x_len</span><span class="p">,</span> <span class="n">y_len</span><span class="p">))</span>
	<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
		<span class="n">state</span> <span class="o">=</span> <span class="n">track_</span><span class="p">.</span><span class="n">get_state</span><span class="p">()</span>
		<span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span>
		<span class="n">trace</span><span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
		<span class="n">action</span> <span class="o">=</span> <span class="n">policy</span><span class="p">[</span><span class="n">sp_x</span><span class="p">,</span> <span class="n">sp_y</span><span class="p">,</span> <span class="n">sv_x</span><span class="p">,</span> <span class="n">sv_y</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
		<span class="n">reward</span> <span class="o">=</span> <span class="n">track_</span><span class="p">.</span><span class="n">take_action</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
		<span class="k">if</span> <span class="n">track_</span><span class="p">.</span><span class="n">is_terminal</span><span class="p">():</span>
			<span class="k">break</span>
	<span class="n">trace</span> <span class="o">=</span> <span class="p">(</span><span class="n">trace</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
	<span class="n">trace</span> <span class="o">+=</span> <span class="n">track_</span><span class="p">.</span><span class="n">track</span>
	<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">flipud</span><span class="p">(</span><span class="n">trace</span><span class="p">.</span><span class="n">T</span><span class="p">))</span>
	<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'./racetrack_off_policy_control.png'</span><span class="p">)</span>
	<span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>  </div>
</div>
<p>We end up with this result after running the code.</p>
<figure>
	<img src="/assets/images/2021-08-21/racetrack-result.png" alt="racetrack's result" width="450" height="400px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 5</b>: Example - Racetrack's result</figcaption>
</figure>
<p><br /></p>

<h3 id="discounting-aware-importance-sampling">Discounting-aware Importance Sampling</h3>
<p>Recall that in the above <a href="#is-off-policy">section</a>, we defined the estimator for $v_\pi(s)$ as:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\vert\mathcal{T}(s)\vert}
\end{equation}</p>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] Adrian Barbu &amp; Song-Chun Zhu. <a href="https://link.springer.com/book/10.1007/978-981-13-2971-5">Monte Carlo Methods</a></p>

<p>[3] David Silver. <a href="https://www.davidsilver.uk/teaching/">UCL course on RL</a></p>

<p>[4] Csaba SzepesvaÃÅri. <a href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a></p>

<p>[5] Singh, S.P., Sutton, R.S. <a href="https://doi.org/10.1007/BF00114726">Reinforcement learning with replacing eligibility traces</a>. Mach Learn 22, 123‚Äì158 (1996)</p>

<p>[6] John N. Tsitsiklis. <a href="https://www.mit.edu/~jnt/Papers/J089-02-jnt-optimistic.pdf">On the Convergence of Optimistic Policy Iteration</a>. Journal of Machine Learning Research 3 (2002) 59‚Äì72</p>

<p>[7] Yuanlong Chen. <a href="https://arxiv.org/abs/1808.08763">On the convergence of optimistic policy iteration for stochastic shortest path problem</a> (2018)</p>

<p>[8] Jun Liu. <a href="https://arxiv.org/abs/2007.10916">On the Convergence of Reinforcement Learning with Monte Carlo Exploring Starts</a> (2020)</p>

<p>[9] Daphne Koller &amp; Nir Friedman. <a href="https://mitpress.mit.edu/books/probabilistic-graphical-models">Probabilistic Graphical Models: Principles and Techniques</a></p>

<p>[10] A. Rupam Mahmood, Hado P. van Hasselt, Richard S. Sutton. <a href="https://papers.nips.cc/paper/2014/hash/be53ee61104935234b174e62a07e53cf-Abstract.html">Weighted importance sampling for off-policy learning with linear function approximation</a>. Advances in Neural Information Processing Systems 27 (NIPS 2014)</p>

<p>[11]</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>We are gonna talk about Monte Carlo methods in more detail in another post.¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A prediction task in RL is where we are given a policy and our goal is to measure how well it performs.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Along with prediction, a control task in RL is where the policy is not fixed, and our goal is to find the optimal policy.¬†<a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>On-policy is a category of RL algorithms that attempts to evaluate or improve the policy that is used to make decisions.¬†<a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>In contrast to on-policy, off-policy methods evaluate or improve a policy different from that used to generate the data.¬†<a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET