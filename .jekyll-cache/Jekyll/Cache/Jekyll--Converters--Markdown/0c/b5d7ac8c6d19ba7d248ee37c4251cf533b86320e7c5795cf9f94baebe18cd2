I"–<!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the post <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_\pi(sâ€™)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V^*(sâ€™)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $sâ€™$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{k+1}(s)=\max_{a}\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_k(sâ€™)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_k\}$, will eventually converges to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banachâ€™s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_k(s)$ to $V_{k+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-05-25/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\sum_{aâ€™}\pi(aâ€™\vert sâ€™)Q_\pi(sâ€™,aâ€™)\right] \\ &amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V_\pi(sâ€™)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q^*(sâ€™,aâ€™)\right]\label{eq:qvi.2} \\ &amp;=\sum_{sâ€™}P(sâ€™\vert s,a)\big[R(s,a,sâ€™)+\gamma V^*(sâ€™)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{k+1}(s,a)=\sum_{sâ€™}P(sâ€™\vert s,a)\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q_k(sâ€™,aâ€™)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converges to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-05-25/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{k+1}(s,a)=\mathbb{E}_{sâ€™\sim P(sâ€™\vert s,a)}\left[R(s,a,sâ€™)+\gamma\max_{aâ€™}Q_k(sâ€™,aâ€™)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(sâ€™\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, expectation can be approximated by sampling. This mean, when the dynamic of the world does not have the transition model available, we can replace the expectation in \eqref{eq:ql.1} by samples.</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>Recall that</p>

<h2 id="references">References</h2>
<p>[1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[3] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[4] John N Tsitsiklis and Benjamin Van Roy. <a href="">An analysis of temporal-difference learning with function approximation</a>. Automatic Control, IEEE Transactions on, 42(5):674â€“690, 1997.</p>

<p>[5] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[6] Hado van Hasselt. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Double Q-learning</a>. NIPS 2010.</p>

<p>[7] Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</p>

<h2 id="footnotes">Footnotes</h2>
:ET