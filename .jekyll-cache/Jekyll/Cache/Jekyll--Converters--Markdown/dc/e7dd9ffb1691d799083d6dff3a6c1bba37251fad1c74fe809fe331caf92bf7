I"{@<blockquote>
  <p>In the previous post about <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#norms">Norms</a></li>
      <li><a href="#contractions">Contractions</a></li>
      <li><a href="#banach-fixed-pts">Banach’s Fixed-point Theorem</a></li>
      <li><a href="#bellman-operator">Bellman Operator</a></li>
    </ul>
  </li>
  <li><a href="#proof">Proof of the existence</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>Before catching the pokémon, we need to prepare ourselves some pokéball.</p>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="norms">Norms</h3>
<p><strong>Definition</strong> (<em>Norm</em>)<br />
Given a vector space $\mathcal{V}\subseteq\mathbb{R}^d$, a function $f:\mathcal{V}\to\mathbb{R}^+_0$ is a <em>norm</em> if and only if</p>
<ol>
  <li>If $f(v)=0$ for some $v\in\mathcal{V}$, then $v=0$</li>
  <li>For any $\lambda\in\mathbb{R},v\in\mathcal{V},f(\lambda v)=|\lambda|v$</li>
  <li>For any $u,v\in\mathbb{R}, f(u+v)\leq f(u)+f(v)$</li>
</ol>

<p><strong>Examples</strong> (<em>Norm</em>)</p>
<ol>
  <li>$\ell^p$ norms: for $p\geq 1$,
\begin{equation}
\Vert v\Vert_p=\left(\sum_{i=1}^{d}|v_i|^p\right)^{1/p}
\end{equation}</li>
  <li>$\ell^\infty$ norms:
\begin{equation}
\Vert v\Vert_\infty=\max_{1\leq i\leq d}|v_i|
\end{equation}</li>
  <li>$\ell^{\mu,p}$: the weighted variants of these norm are defined as
\begin{equation}
\Vert v\Vert_p=\begin{cases}\left(\sum_{i=1}^{d}\frac{|v_i|^p}{w_i}\right)^{1/p}&amp;\text{if }1\leq p&lt;\infty\\ \max_{1\leq i\leq d}\frac{|v_i|}{w_i}&amp;\text{if }p=\infty\end{cases}
\end{equation}</li>
  <li>$\ell^{2,P}$: the matrix-weighted 2-norm is defined as
\begin{equation}
\Vert v\Vert^2_P=v^\intercal Pv
\end{equation}
Similarly, we can define norms over spaces of functions. For example, if $\mathcal{V}$ is the vector space of functions over domain $\mathcal{X}$ which are <em>uniformly bounded</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, then
\begin{equation}
\Vert f\Vert_\infty=\sup_{x\in\mathcal{X}}\vert f(x)\vert
\end{equation}</li>
</ol>

<p><strong>Definition</strong> (<em>Convergence in norm</em>)<br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a <em>normed vector space</em><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Let $v_n\in\mathcal{V}$ is a sequence of vectors ($n\in\mathbb{N}$). The sequence ($v_n,n\geq 0$) is said to <em>converge to</em> $v\in\mathcal{V}$ in the norm $\Vert\cdot\Vert$, denoted as $v_n\to_{\Vert\cdot\Vert}v$ if
\begin{equation}
\lim_{n\to\infty}\Vert v_n-v\Vert=0,
\end{equation}
<br /></p>

<p><strong>Definition</strong> (<em>Cauchy sequence</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>)<br />
Let ($v_n;n\geq 0$) be a sequence of vectors of a normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$. Then $v_n$ is called a <em>Cauchy sequence</em> if
\begin{equation}
\lim_{n\to\infty}\sup_{m\geq n}\Vert v_n-v_m\Vert=0
\end{equation}
Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Completeness</em>)<br />
A normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ is called <em>complete</em> if every Cauchy sequence in $\mathcal{V}$ is convergent in the norm of the vector space.</p>

<h3 id="contractions">Contractions</h3>
<p><strong>Definition</strong> (<em>Lipschitz function</em>, <em>Contraction</em>) <br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a normed vector space. A mapping $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ is called <em>L-Lipschitz</em> if for any $u,v\in\mathcal{V}$,
\begin{equation}
\Vert\mathcal{T}u-\mathcal{T}v\Vert\leq L\Vert u-v\Vert
\end{equation}
A mapping $\mathcal{T}$ is called a <em>non-expansion</em> if it is <em>Lipschitzian</em> with $L\leq 1$. It is called a <em>contraction</em> if it is <em>Lipschitzian</em> with $L&lt;1$. In this case, $L$ is called the <em>contraction factor of</em> $\mathcal{T}$ and $\mathcal{T}$ is called an <em>L-contraction</em>.</p>

<p><strong>Remark</strong><br />
If $\mathcal{T}$ is <em>Lipschitz</em>, it is also continuous in the sense that if $v_n\to_{\Vert\cdot\Vert}v$, then also $\mathcal{T}v_n\to_{\Vert\cdot\Vert}\mathcal{T}v$. This is because $\Vert\mathcal{T}v_n-\mathcal{T}v\Vert\leq L\Vert v_n-v\Vert\to 0$ as $n\to\infty$.</p>

<h3 id="banach-fixed-pts">Banach’s Fixed-point Theorem</h3>
<p><strong>Definition</strong> (<em>Banach space</em>)<br />
A complete, normed vector space is called a <em>Banach space</em>.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Fixed point</em>)<br />
Let $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be some mapping. The vector $v\in\mathcal{V}$ is called a <em>fixed point of</em> $\mathcal{T}$ if $\mathcal{T}v=v$.<br />
<br /></p>

<p><strong>Theorem</strong> (<em>Banach’s fixed-point</em>)<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>    <br />
Let $\mathcal{V}$ be a Banach space and $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be a $\gamma$-contraction mapping. Then</p>
<ol>
  <li>$\mathcal{T}$ admits a <em>unique fixed point</em> $v$.</li>
  <li>For any $v_0\in\mathcal{V}$, if $v_{n+1}=\mathcal{T}v_n$, then $v_n\to_{\Vert\cdot\Vert}v$ with a <em>geometric convergence rate</em><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:
\begin{equation}
\Vert v_n-v\Vert\leq\gamma^n\Vert v_0-v\Vert
\end{equation}</li>
</ol>

<h3 id="bellman-operator">Bellman Operator</h3>
<p>Previously, we defined Bellman equation for state-value function $v_\pi(s)$ as:
\begin{align}
v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right] \\\text{or}\quad v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_\pi(s’)\right)\tag{1}\label{1}
\end{align}
If we let
\begin{align}
\mathcal{P}^\pi_{ss’}&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}^a_{ss’}; \\\mathcal{R}^\pi_s&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s
\end{align}
then we can rewrite \eqref{1} in another form as
\begin{equation}
v_\pi(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v_\pi(s’)\tag{2}\label{2}
\end{equation}
<br />
<strong>Definition</strong> (<em>Bellman operator</em>)<br />
We define the <em>Bellman operator</em> underlying $\pi,\mathcal{T}:\mathbb{R}^\mathcal{S}\to\mathbb{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^\pi v)(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v(s’)
\end{equation}
<br />
With the help of $\mathcal{T}^\pi$, equation \eqref{2} can be rewrite as:
\begin{equation}
\mathcal{T}^\pi v_\pi=v_\pi\tag{3}\label{3}
\end{equation}
Similarly, we can rewrite the <em>Bellman optimality equation for</em> $v_*$
\begin{align}
v_*(s)&amp;=\max_{a\in\mathcal{A}}\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_*(s’)\right] \\ &amp;=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_*(s’)\right)\tag{4}\label{4}
\end{align}
and thus, we can define the <em>Bellman optimality operator</em> $\mathcal{T}^*:\mathcal{R}^\mathcal{S}\to\mathcal{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^* v)(s)=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v(s’)\right)
\end{equation}
And thus, with the help of $\mathcal{T}^*$, we can rewrite the equation \eqref{4} as:
\begin{equation}
\mathcal{T}^*v_*=v_*\tag{5}\label{5}
\end{equation}
<br />
Now everything is all set, we can move on to the next step.</p>

<h2 id="proof">Proof of the existence</h2>
<p>Let $B(\mathcal{S})$ be the space of <em>uniformly bounded functions</em> with domain $\mathcal{S}$:
\begin{equation}
B(\mathcal{S})=\{v:\mathcal{S}\to\mathbb{R}:\Vert v\Vert_\infty&lt;+\infty\}
\end{equation}
We will view $B(\mathcal{S})$ as a normed vector space with the norm $\Vert\cdot\Vert_\infty$.</p>

<p>It is easily seen that $(B(\mathcal{S}),\Vert\cdot\Vert_\infty)$ is complete: If $(v_n;n\geq0)$ is a Cauchy sequence in it then for any $s\in\mathcal{S}$, $(v_n(s);n\geq0)$ is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of $(v_n(s))$, we can show that $\Vert v_n-v\Vert_\infty\to0$. Vaguely speaking, this holds because $(v_n;n\geq0)$ is a Cauchy sequence in the norm $\Vert\cdot\Vert_\infty$, so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.</p>

<p>Let $\pi$ be some stationary policy. We have that $\mathcal{T}^\pi$ is <em>well-defined</em> since: if $u\in B(\mathcal{S})$, then also $\mathcal{T}^\pi u\in B(S)$.</p>

<p>From equation \eqref{3}, we have that $v_\pi$ is a fixed point to $\mathcal{T}^\pi$.</p>

<p>We also have that $\mathcal{T}^\pi$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$ since for any $u, v\in B(\mathcal{S})$,
\begin{align}
\Vert\mathcal{T}^\pi u-\mathcal{T}^\pi v\Vert_\infty&amp;=\gamma\max_{s\in\mathcal{S}}\left|\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left(u(s’)-v(s’)\right)\right| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\big|u(s’)-v(s’)\big| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\big\Vert u-v\big\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
where the last line follows from $\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}=1$.</p>

<p>It follows that in order to find $v_\pi$, we can construct the sequence $v_0,\mathcal{T}^\pi v_0,(\mathcal{T}^\pi)^2 v_0,\dots$, which, by Banach’s fixed-point theorem will converge to $v_\pi$ at a geometric rate.</p>

<p>From the definition \eqref{5} of $\mathcal{T}^*$, we have that $\mathcal{T}^*$ is well-defined.</p>

<p>Using the fact that $\left|\max_{a\in\mathcal{A}}f(a)-\max_{a\in\mathcal{A}}g(a)\right|\leq\max_{a\in\mathcal{A}}\left|f(a)-g(a)\right|$, similarly, we have:
\begin{align}
\Vert\mathcal{T}^*u-\mathcal{T}^*v\Vert_\infty&amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\Vert u-v\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
which tells us that $\mathcal{T}^*$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$.
<br /></p>

<p><strong>Theorem</strong><br />
Let $v$ be the fixed point of $\mathcal{T}^*$ and assume that there is policy $\pi$ which is greedy w.r.t $v:\mathcal{T}^\pi v=\mathcal{T}^* v$. Then $v=v_*$ and $\pi$ is an optimal policy.<br />
<strong><em>Proof</em></strong><br />
Pick any stationary policy $\pi$. Then $\mathcal{T}^\pi\leq\mathcal{T}^*$ in the sense that for any function $v\in B(\mathcal{S})$, $\mathcal{T}^\pi v\leq\mathcal{T}^* v$ holds ($u\leq v$ means that $u(s)\leq v(s),\forall s\in\mathcal{S}$).<br />
Hence, for all $n\geq0$,
\begin{equation}
v_\pi=\mathcal{T}^\pi v_\pi\leq\mathcal{T}^*v_\pi\leq(\mathcal{T}^*)^2 v_\pi\leq\dots\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
or
\begin{equation}
v_\pi\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
Since $\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\mathcal{T}^*$. Thus, $v_\pi\leq v$. And since $\pi$ was arbitrary, we obtain that $v_*\leq v$.<br />
Pick a policy $\pi$ such that $\mathcal{T}^\pi v=\mathcal{T}^*v$, then $v$ is also a fixed point of $\mathcal{V}^\pi$. Since $v_\pi$ is the unique fixed point of $\mathcal{T}^\pi$, we have that $v=v_\pi$, which shows that $v_*=v$ and that $\pi$ is an optimal policy.</p>

<h2 id="references">References</h2>
<p>[1] Csaba Szepesvári. <a href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a></p>

<p>[2] A. Lazaric. <a href="http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf">Markov Decision Processes and Dynamic Programming</a></p>

<p>[3] <a href="https://ai.stackexchange.com/a/11133">What is the Bellman operator in reinforcement learning?</a></p>

<p>[4] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[5] <a href="https://en.wikipedia.org/wiki/Normed_vector_space">Normed vector space</a></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A function is called <em>uniformly bounded</em> exactly when $\Vert f\Vert_\infty&lt;+\infty$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A <em>normed vector space</em> is a vector space over the real or complex number, on which a norm is defined. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>We are gonna talk further about <em>sequences</em> in another <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#convergent-sequences">post</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><strong><em>Proof</em></strong><br />
Pick any $v_0\in\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\mathcal{T}$. c. Finally, we show that $\mathcal{T}$ has a single fixed point. Assume that $\mathcal{T}$ is a $\gamma$-contraction.<br />
a. To show that $(v_n)$ converges, it suffices  to show that $(v_n)$ is a Cauchy sequence. We have:
\begin{align}
\Vert v_{n+1}-v_n\Vert&amp;=\Vert\mathcal{T}v_{n}-\mathcal{T}v_{n-1}\Vert \\ &amp;\leq\gamma\Vert v_{n}-v_{n-1}\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_1-v_0\Vert
\end{align}
From the properties of norms, we have:
\begin{align}
\Vert v_{n+k}-v_n\Vert&amp;\leq\Vert v_{n+1}-v_n\Vert+\dots+\Vert v_{n+k}-v_{n+k-1}\Vert \\ &amp;\leq\left(\gamma^n+\dots+\gamma^{n+k-1}\right)\Vert v_1-v_0\Vert \\ &amp;=\gamma^n\dfrac{1-\gamma^{k}}{1-\gamma}\Vert v_1-v_0\Vert
\end{align}
and so
\begin{equation}
\lim_{n\to\infty}\sup_{k\geq0}\Vert v_{n+k}-v_n\Vert=0,
\end{equation}
shows us that $(v_n;n\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.<br />
b. Recall that the definition of the sequence $(v_n;n\geq0)$
\begin{equation}
v_{n+1}=\mathcal{T}v_n
\end{equation}
Taking the limes as $n\to\infty$ of both sides, one the one hand, we get that $v_{n+1}\to _{\Vert\cdot\Vert}v$. On the other hand, $\mathcal{T}v_n\to _{\Vert\cdot\Vert}\mathcal{T}v$, since $\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\mathcal{T}v$, which tells us that $v$ is a fixed point of $\mathcal{T}$.<br />
c. Let us assume that $v,v’$ are both fixed points of $\mathcal{T}$. Then,
\begin{align}
\Vert v-v’\Vert&amp;=\Vert\mathcal{T}v-\mathcal{v’}\Vert \\ &amp;\leq\gamma\Vert v-v’\Vert \\ \text{or}\quad(1-\gamma)\Vert v-v’\Vert&amp;\leq0
\end{align}
Thus, we must have that $\Vert v-v’\Vert=0$. Therefore, $v-v’=0$ or $v=v’$.<br />
And finally,
\begin{align}
\Vert v_n-v\Vert&amp;=\Vert\mathcal{T}v_{n-1}-\mathcal{T}v\Vert \\ &amp;\leq\gamma\Vert v_{n-1}-v\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_0-v\Vert
\end{align} <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Also, there’s gonna be a post about <em>rate of convergence</em>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET