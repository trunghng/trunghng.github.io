I"E#<blockquote>
  <p>If we have to describe the defintition of <strong>Markov chain</strong> in one statement, it will be: “It only matters where you are, not where you’ve been”.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#markov-property">Markov Property</a></li>
  <li><a href="#transition-matrix">Transition Matrix</a>
    <ul>
      <li><a href="#nstep-trans-prob">n-step transition probability</a></li>
    </ul>
  </li>
  <li><a href="#marginal-dist-xn">Marginal distribution of $X_n$</a></li>
  <li><a href="#properties">Properties</a></li>
  <li><a href="#stationary-distribution">Stationary distribution</a></li>
  <li><a href="#reversibility">Reversibility</a></li>
  <li><a href="#exp-app">Examples and Applications</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p><strong>Markov chain</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> is a stochastic process in which the random variables follow a special property called <em>Markov</em>.</p>

<h2 id="markov-property">Markov Property</h2>
<p>A sequence of random variables $X_0, X_1, X_2, \dots$ taking values in the <strong>state space</strong> $\mathcal{S}=${$1, 2,\dots, M$} such that for all $n\geq0$,
\begin{equation}
P(X_{n+1}=j|X_n=i)=P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},X_{n-2}=i_{n-2},\dots,X_0=i_0)
\end{equation}
In other words, knowledge of the preceding state is all we need to determine the probability distribution of the current state.</p>

<h2 id="transition-matrix">Transition Matrix</h2>
<p>The quantity $P(X_{n+1}=j|X_n=i)$ is <strong>transition probability</strong> from state $i$ to $j$.</p>

<p>If we denote that $q_{ij}=P(X_{n+1}=j|X_n=i)$ and let $Q$ $M\times M$ matrix, defined as
\begin{equation}
Q=\left[\begin{matrix}q_{11}&amp;\ldots&amp;q_{1M} \\ \vdots&amp;\ddots&amp;\vdots \\ q_{M1}&amp;\ldots&amp;q_{MM}\end{matrix}\right]
\end{equation}
The matrix $Q$ then is referred as the <strong>transition matrix</strong> of the chain.</p>

<p>It is noticeable that each row of $Q$ is a conditional probability mass function (PMF) of $X_{n+1}$ given $X_n$. And hence, sum of its entries is 1.</p>

<h3 id="nstep-trans-prob">$n$-step Transition Probability</h3>
<p>The <strong>$n$-step transition probability</strong> from $i$ to $j$ is the probability of being at $i$ and $n$ steps later being at $j$, and be denoted as $q_{ij}^{(n)}$,
\begin{equation}
q_{ij}^{(n)}=P(X_n=j|X_0=i)
\end{equation}
We have that
\begin{equation}
q_{ij}^{(2)}=\sum_{k}^{}q_{ik}q_{kj}
\end{equation}
since it has to go through an intermediary step $k$ to reach $j$ in 2 steps from $i$. It is easily seen that the right hand side is $Q_{ij}^2$. And by induction, we have that:
\begin{equation}
q_{ij}^{(n)}=Q_{ij}^{n}
\end{equation}
$Q^n$ is also called the <strong>$n$-step transition matrix</strong>.</p>

<h3 id="marginal-dist-xn">Marginal Distribution of $X_n$</h3>
<p>Let $t=(t_1,\dots,t_M)^\text{T}$, where $t_i=P(X_0=i)$. By the <strong>law of total probability</strong> (LOTP), we have that:
\begin{equation}
P(X_n=j)=\sum_{i=1}^{M}P(X_0=i)P(X_n=j|X_0=i)=\sum_{i=1}^{M}t_iq_{ij}^{(n)},
\end{equation}
which implies that the marginal distribution of $X_n$ is given by $tQ^n$.</p>

<h2 id="properties">Properties</h2>
<ul id="roman-list">
	<li>
		State $i$ of a Markov chain is defined as <b>recurrent</b> or <b>transient</b> depending upon whether or not the Markov chain will eventually return to it. Starting with <b>recurrent</b> state $i$, the chain will return to it with the probability of $1$. Otherwise, it is <b>transient</b>.<br />
		<b>Proposition</b>: Number of returns to <b>transient</b> state is distributed by $\text{Geom}(p)$, with $p&gt;0$ is the probability of never returning to $i$.
	</li>
	<li>
		A Markov chain is defined as <b>irreducible</b> if there exists a chain of steps between any $i,j$ that has positive probability. That is for any $i,j$, there is some $n&gt;0,\in\mathbb{N}$ such that $Q^n_{ij}&gt;0$. If not <b>irreducible</b>, the chain is instead referred as <b>reducible</b>.<br />
		<b>Proposition</b>: <b>irreducible</b> implies all states <b>recurrent</b>.
	</li>
	<li>
		A state $i$ has <b>period</b> $k&gt;0$ if
		\begin{equation}
		k=\text{gcd}(n),
		\end{equation}
		where $n$ is possible number of steps it can take to return to $i$ when starting at $i$, or $Q^n_{ii}&gt;0$.<br />
		State $i$ is known as <b>aperiodic</b> if $k_i=1$, and <b>periodic</b> otherwise. The chain itself is called <b>aperiodic</b> if all its states are <b>aperiodic</b>, and <b>periodic</b> otherwise.
	</li>
</ul>

<h2 id="stationary-distribution">Stationary Distribution</h2>
<p>A vector $s=(s_1,\dots,s_M)^\text{T}$ such that $s_i\geq0$ and $\sum_{i}s_i=1$ is a <strong>stationary distribution</strong> for a Markov chain if
\begin{equation}
\sum_{i}s_iq_{ij}=s_j
\end{equation}
for all $j$, or equivalently $sQ=s$.</p>

<p><strong>Theorem</strong> (<em>Existence and uniqueness of stationary distribution</em>)<br />
<em>Any irreducible Markov chain has a unique stationary distribution. In this distribution, every state has positive probability.</em></p>

<p>The theorem is a consequence of a result from <strong>Perron-Frobenius theorem</strong>.</p>

<p><strong>Theorem</strong> (<em>Convergence to stationary distribution</em>)<br />
<em>Let $X_0,X_1,\dots$ be a Markov chain with stationary distribution $s$ and transition matrix $Q$, such that some power $Q^m$ has all entries positive (or in the other words, the chain is irreducible and aperiodic). Then
\begin{equation}
P(X_n=i)\to s_i
\end{equation}
as $n\rightarrow\infty$ (or $Q^n$ converges to a matrix in which each row is $s$)</em>.</p>

<p><strong>Theorem</strong> (<em>Expected time to run</em>)<br />
<em>Let $X_0,X_1,\dots$ be an irreducible Markov chain with stationary distribution $s$. Let $r_i$ be the expected time it takes the chain to return to $i$, given that it starts at $i$. Then</em>
\begin{equation}
s_i=\frac{1}{r_i}
\end{equation}</p>

<h2 id="reversibility">Reversibility</h2>
<p>Let $Q=(q_{ij})$ denote transition matrix of a Markov chain. Suppose there is an $s=(s_1,\dots,s_M)^\text{T}$ with $s_i\geq0,\sum_{i}s_i=1$, such that
\begin{equation}
s_iq_{ij}=s_jq_{ji}
\end{equation}
for all states $i,j$. This equation is called <strong>reversibility</strong> or <strong>detailed balance</strong> condition. And if the condition holds, we say that the chain is <strong>reversible</strong> w.r.t $s$.</p>

<p><strong>Proposition</strong> (<em>Reversible implies stationary</em>)<br />
Suppose that $Q=(q_{ij})$ be the transition matrix of a Markov chain that is reversible w.r.t to an $s=(s_1,\dots,s_M)^\text{T}$ with $s_i\geq0,\sum_{i}s_i=1$. Then $s$ is a stationary distribution of the chain.</p>

<p><strong>Proof</strong><br />
We have that
\begin{equation}
\sum_{j}s_jq_{ji}=\sum_{j}s_iq_{ij}=s_i\sum_{j}q_{ij}=s_i
\end{equation}</p>

<p><strong>Proposition</strong><br />
If each column of $Q$ sum to $1$, then the <strong>Uniform distribution</strong> over all states $(1/M,\dots,1/M)$, is a stationary distribution. (This kind of matrix is called <em>doubly stochastic matrix</em>).</p>

<h2 id="exp-app">Examples and Applications</h2>
<ul>
  <li><a href="https://en.wikipedia.org/wiki/Finite-state_machine"><em>Finite-state machines</em></a>, <a href="https://en.wikipedia.org/wiki/Random_walk"><em>random walks</em></a></li>
  <li>Diced board games such as Ludo, Monopoly,…</li>
  <li><a href="https://en.wikipedia.org/wiki/PageRank"><em>Google PageRank</em></a> - the heart of Google search</li>
  <li>Markov Decision Process (MDP), which is gonna be the content of next <a href="/2021/06/27/mdp-bellman-eqn.html">post</a>.</li>
  <li>And various other applications.</li>
</ul>

<h2 id="references">References</h2>
<p>[1] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a>.</p>

<p>[2] <a href="https://brilliant.org/wiki/markov-chains/">Brillant’s Markov chain</a>.</p>

<p>[3] <a href="https://en.wikipedia.org/wiki/Perron–Frobenius_theorem">Perron-Frobenius theorem</a>.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>This is more like intuitive and less formal definition of Markov chain, we will have a more concrete definition with the help of <em>Measure theory</em> after the post about it. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The Markov chain here is <em>time-homogeneous</em> Markov chain, in which the probability of any state transition is independent of time. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET