I"äπ<blockquote>
  <p>So far in this <a href="/tag/my-rl">series</a>, we have gone through the ideas of <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html"><strong>dynamic programming</strong> (DP)</a> and <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html"><strong>Monte Carlo</strong></a>. What will happen if we combine these ideas together? <strong>Temporal-difference (TD) learning</strong> is our answer.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#td0">TD(0)</a>
    <ul>
      <li><a href="#td-prediction">TD Prediction</a>
        <ul>
          <li><a href="#adv-over-mc-dp">Advantages over MC &amp; DP</a></li>
          <li><a href="#opt-td0">Optimality of TD(0)</a></li>
        </ul>
      </li>
      <li><a href="#td-control">TD Control</a>
        <ul>
          <li><a href="#sarsa">Sarsa</a></li>
          <li><a href="#q-learning">Q-learning</a>
            <ul>
              <li><a href="#eg-cliffwalking">Example: Cliffwalking - Sarsa vs Q-learning</a></li>
            </ul>
          </li>
          <li><a href="#exp-sarsa">Expected Sarsa</a></li>
          <li><a href="#double-q-learning">Double Q-learning</a>
            <ul>
              <li><a href="#max-bias">Maximization Bias</a></li>
              <li><a href="#sol">A Solution</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#n-step-td">$n$-step TD</a>
    <ul>
      <li><a href="#n-step-td-prediction">$n$-step TD Prediction</a>
        <ul>
          <li><a href="#eg-random-walk">Example: Random Walk</a></li>
        </ul>
      </li>
      <li><a href="#n-step-td-control">$n$-step TD Control</a></li>
      <li></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="td0">TD(0)</h2>
<p>As usual, we approach this new method by considering the prediction problem.</p>

<h3 id="td-prediction">TD Prediction</h3>
<p>Borrowing the idea of Monte Carlo, TD methods learn from episodes of experience to solve the <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#fn:2">prediction problem</a>. The simplest TD method is <strong>TD(0)</strong> (or <strong>one-step TD</strong>)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[R_{t+1}+\gamma V(S_{t+1})-V(S_t)\right]\tag{1}\label{1},
\end{equation}
where $\alpha&gt;0$ is step size of the update. Here is pseudocode of the TD(0) method</p>
<figure>
	<img src="/assets/images/2022-04-08/td0.png" alt="TD(0)" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
<p>Recall that in <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#mc-prediction">Monte Carlo method</a>, or even in its trivial form, <strong>constant-$\alpha$ MC</strong>, which has the update form:
\begin{equation}
V(S_t)\leftarrow V(S_t)+\alpha\left[G_t-V(S_t)\right]\tag{2}\label{2},
\end{equation}
we have to wait until the end of the episode, when the return $G_t$ is determined. However, with TD(0), we can do the update immediately in the next time step $t+1$.</p>

<p>As we can see in \eqref{1} and \eqref{2}, both TD and MC updates look ahead to a sample successor state (or state-action pair), use the value of the successor and the corresponding reward in order to update the value of the current state (or state-action pair). This kind of updates is called <em>sample update</em>, which differs from <em>expected update</em> used by DP methods in that they are based on a single sample successor rather than on a complete distribution of all possible successors.</p>

<p>Other than the sampling of Monte Carlo, TD methods also use the bootstrapping of DP. Because similar to <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-evaluation">DP</a>, TD(0) is also a bootstrapping method, since the target in its update is $R_{t+1}+\gamma V(S_{t+1})$.</p>

<p>The quantity inside bracket in \eqref{1} is called <em>TD error</em>, denoted as $\delta$:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma V(S_{t+1})-V(S_t)
\end{equation}
If the array $V$ does not change during the episode (as in MC), then the MC error can be written as a sum of TD errors
\begin{align}
G_t-V(S_t)&amp;=R_{t+1}+\gamma G_{t+1}-V(S_t)+\gamma V(S_{t+1})-\gamma V(S_{t+1}) \\ &amp;=\delta_t+\gamma\left(G_{t+1}-V(S_{t+1})\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\left(G_{t+2}-V(S_{t+2})\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}\left(G_T-V(S_T)\right) \\ &amp;=\delta_t+\gamma\delta_{t+1}+\gamma^2\delta_{t+2}+\dots+\gamma^{T-t-1}\delta_{T-1}+\gamma^{T-t}(0-0) \\ &amp;=\sum_{k=t}^{T-1}\gamma^{k-t}\delta_k
\end{align}</p>

<h4 id="adv-over-mc-dp">Advantages over MC &amp; DP</h4>
<p>With how TD is established, these are some advantages of its over MC and DP:</p>
<ul>
  <li>Only experience is required.</li>
  <li>Can be fully incremental:
    <ul>
      <li>Can make update before knowing the final outcome.</li>
      <li>Requires less memory.</li>
      <li>Requires less peak computation.</li>
    </ul>
  </li>
</ul>

<p>TD(0) does converge to $v_\pi$, in the mean for a sufficient small $\alpha$, and with probability of $1$ if $\alpha$ decreases according to the <em>stochastic approximation condition</em>
\begin{equation}
\sum_{n=1}^{\infty}\alpha_n(a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{n=1}^{\infty}\alpha_n^2(a)&lt;\infty,
\end{equation}
where $\alpha_n(a)$ denote the step-size parameter used to process the reward received after the $n$-th selection of action $a$.</p>

<h4 id="opt-td0">Optimality of TD(0)</h4>
<p>Under batch training, TD(0) converges to the optimal maximum likelihood estimate. The convergence and optimality proofs can be found in this <a href="#td-convergence">paper</a>.</p>
<figure>
	<img src="/assets/images/2022-04-08/random_walk_batch_updating.png" alt="TD(0) vs constant-alpha MC" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Performance of TD(0) and constant-$\alpha$ MC under batch training on the random walk task. The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-6/random-walk.py">here</a></span></figcaption>
</figure>

<h3 id="td-control">TD Control</h3>
<p>We begin solving the control problem with an on-policy TD method. Recall that in on-policy methods, we evaluate or improve the policy $\pi$ used to make decision.</p>

<h4 id="sarsa">Sarsa</h4>
<p>As mentioned in <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#mc-est-action-value">MC methods</a>, when the model is not available, we have to learn an action-value function rather than a state-value function. Or in other words, we need to estimate $q_\pi(s,a)$ for the current policy $\pi$ and $\forall s,a$. Thus, instead of considering transitions from state to state in order to learn the value of states, we now take transitions from state-action pair to state-action pair into account so as to learn the value of state-action pairs.</p>

<p>Similarly, we‚Äôve got the TD update for the action-value function case:
\begin{equation}
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\right]\tag{3}\label{3}
\end{equation}
This update is done after every transition from a non-terminal state $S_t$ to its successor $S_{t+1}$
\begin{equation}
\left(S_t,A_t,R_{t+1},S_{t+1},A_{t+1}\right)
\end{equation}
And if $S_{t+1}$ is terminal (i.e., $S_{t+1}=S_T$), then $Q(S_{t+1},A_{t+1})=0$. The name <strong>Sarsa</strong> of the method is taken based on acronym of the quintuple.</p>

<p>As usual when working on on-policy control problem, we apply the idea of <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#gpi">GPI</a>:
\begin{equation}
\pi_0\overset{\small \text{E}}{\rightarrow}q_{\pi_0}\overset{\small \text{I}}{\rightarrow}\pi_1\overset{\small \text{E}}{\rightarrow}q_{\pi_1}\overset{\small \text{I}}{\rightarrow}\pi_2\overset{\small \text{E}}{\rightarrow}\dots\overset{\small \text{I}}{\rightarrow}\pi_*\overset{\small \text{E}}{\rightarrow}q_*
\end{equation}
However this time, instead, we use it with TD methods. Which is, we continually estimate $q_\pi$ for the behavior policy $\pi$, and at the same time change $\pi$ toward greediness w.r.t $q_\pi$. That gives us the following pseudocode of the Sarsa control algorithm</p>
<figure>
	<img src="/assets/images/2022-04-08/sarsa.png" alt="Sarsa" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="q-learning">Q-learning</h4>
<p>We now turn our move to an off-policy method, which evaluates or improves a policy different from that used to generate the data.<br />
The method we are talking about is called <strong>Q-learning</strong>, which was first introduced by <a href="#q-learning-watkins">Watkins</a>, in which the update on $Q$-value has the form:
\begin{equation}
Q(S_t,A_t)\leftarrow Q(S_t,A_t)+\alpha\left[R_{t+1}+\gamma\max_a Q(S_{t+1},a)-Q(S_t,A_t)\right]\tag{4}\label{4}
\end{equation}
In this case, the learned action-value function, $Q$, directly approximates optimal action-value function $q_*$, independent of the policy being followed. Down below is pseudocode of the Q-learning.</p>
<figure>
	<img src="/assets/images/2022-04-08/q-learning.png" alt="Q-learning" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h5 id="eg-cliffwalking">Example: Cliffwalking - Sarsa vs Q-learning</h5>
<p>(This example is taken from <em>Example 6.6, Reinforcement Learning: An Introduction book</em>.)</p>
<figure>
	<img src="/assets/images/2022-04-08/cliff-walking-eg.png" alt="Cliff Walking example" style="display: block; margin-left: auto; margin-right: auto; width: 500px" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
<p>Say that we have an agent in a gridworld, which is an undiscounted, episodic task described by the above image. Start and goal states are denoted as ‚ÄúS‚Äù and ‚ÄúG‚Äù respectively. Agent can take up, down, left or right action. All the actions lead to a reward of $-1$, except for cliff region, into which stepping gives a reward of $-100$. We will be solving this problem with Q-learning and Sarsa with $\varepsilon$-greedy action selection, for $\varepsilon=0.1$.</p>

<p><strong>Solution code</strong><br />
The source code can be found <a href="https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-6/cliff_walking.py">here</a>.</p>

<p><button type="button" class="collapsible" id="codeP">Click to show the code</button></p>
<div class="codePanel" id="codePdata">
  <p><br />
We begin by importing necessary packages we will be using</p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</code></pre></div>  </div>
  <p>Our first step is to define the environment, gridworld with a cliff, which is constructed by height, width, cliff region, start state, goal state, actions and rewards.</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GridWorld</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">start_state</span><span class="p">,</span> <span class="n">goal_state</span><span class="p">,</span> <span class="n">cliff</span><span class="p">):</span>
        <span class="s">'''
        Initialization function

        Params
        ------
        height: int
            gridworld's height
        width: int
            gridworld's width
        start_state: [int, int]
            gridworld's start state
        goal_state: [int, int]
            gridworld's goal state
        cliff: list&lt;[int, int]&gt;
            gridworld's cliff region
    	'''</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">height</span> <span class="o">=</span> <span class="n">height</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">width</span> <span class="o">=</span> <span class="n">width</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">start_state</span> <span class="o">=</span> <span class="n">start_state</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">goal_state</span> <span class="o">=</span> <span class="n">goal_state</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">cliff</span> <span class="o">=</span> <span class="n">cliff</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s">'cliff'</span><span class="p">:</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="s">'non-cliff'</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span><span class="p">}</span>
</code></pre></div>  </div>
  <p>The gridworld also needs some helper functions. <code class="language-plaintext highlighter-rouge">is_terminal()</code> function checks whether the current state is the goal state; <code class="language-plaintext highlighter-rouge">take_action()</code> takes an state and action as inputs and returns next state and corresponding reward while <code class="language-plaintext highlighter-rouge">get_action_idx()</code> gives us the index of action from action list. Putting all these functions inside <code class="language-plaintext highlighter-rouge">GridWorld</code>‚Äôs body, we have:</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">def</span> <span class="nf">is_terminal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="s">'''
        Whether state @state is the goal state

        Params
        ------
        state: [int, int]
            current state
        '''</span>
        <span class="k">return</span> <span class="n">state</span> <span class="o">==</span> <span class="bp">self</span><span class="p">.</span><span class="n">goal_state</span>


    <span class="k">def</span> <span class="nf">take_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="s">'''
        Take action @action at state @state

        Params
        ------
        state: [int, int]
            current state
        action: (int, int)
            action taken

        Return
        ------
        (next_state, reward): ([int, int], int)
            a tuple of next state and reward
        '''</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">action</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">action</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">[</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="n">next_state</span> <span class="o">=</span> <span class="p">[</span><span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">height</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">min</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>
        <span class="k">if</span> <span class="n">next_state</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">cliff</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">[</span><span class="s">'cliff'</span><span class="p">]</span>
            <span class="n">next_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">start_state</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">[</span><span class="s">'non-cliff'</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span>


    <span class="k">def</span> <span class="nf">get_action_idx</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="s">'''
        Get index of action in action list

        Params
        ------
        action: (int, int)
            action
        '''</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">actions</span><span class="p">.</span><span class="n">index</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
</code></pre></div>  </div>
  <p>Next, we define the $\varepsilon$-greedy function used by our methods in <code class="language-plaintext highlighter-rouge">epsilon_greedy()</code> function.</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="s">'''
    Choose action according to epsilon-greedy policy

    Params:
    -------
    grid_world: GridWorld
    epsilon: float
    Q: np.ndarray
        action-value function
    state: [int, int]
        current state

    Return
    ------
    action: (int, int)
    '''</span>
    <span class="k">if</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">):</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">grid_world</span><span class="p">.</span><span class="n">actions</span><span class="p">))</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_idx</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:]</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="n">action_</span> <span class="k">for</span> <span class="n">action_</span><span class="p">,</span> <span class="n">value_</span> 
            <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="k">if</span> <span class="n">value_</span> <span class="o">==</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">values</span><span class="p">)])</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">action_idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">action</span>
</code></pre></div>  </div>
  <p>It‚Äôs time for our main course, Q-learning and Sarsa.</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">q_learning</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="s">'''
    Q-learning

    Params
    ------
    Q: np.ndarray
        action-value function
    grid_world: GridWorld
    epsilon: float
    alpha: float
        step size
    gamma: float
        discount factor
    '''</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">start_state</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">take_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">get_action_idx</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> \
            <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">:])</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action_idx</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>

    <span class="k">return</span> <span class="n">rewards</span>

<span class="k">def</span> <span class="nf">sarsa</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="s">'''
    Sarsa

    Params
    ------
    Q: np.ndarray
        action-value function
    grid_world: GridWorld
    epsilon: float
    alpha: float
        step size
    gamma: float
        discount factor
    '''</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">start_state</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">while</span> <span class="ow">not</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">is_terminal</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
        <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">take_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">next_action</span> <span class="o">=</span> <span class="n">epsilon_greedy</span><span class="p">(</span><span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
        <span class="n">action_idx</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">get_action_idx</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">next_action_idx</span> <span class="o">=</span> <span class="n">grid_world</span><span class="p">.</span><span class="n">get_action_idx</span><span class="p">(</span><span class="n">next_action</span><span class="p">)</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action_idx</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="n">reward</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">Q</span><span class="p">[</span><span class="n">next_state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> \
            <span class="n">next_state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">next_action_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">action_idx</span><span class="p">])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
        <span class="n">action</span> <span class="o">=</span> <span class="n">next_action</span>

    <span class="k">return</span> <span class="n">rewards</span>
</code></pre></div>  </div>

  <p>And lastly, wrapping everything together in the main function, we have</p>
  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">height</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">width</span> <span class="o">=</span> <span class="mi">13</span>
    <span class="n">start_state</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">goal_state</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">]</span>
    <span class="n">cliff</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">3</span><span class="p">,</span> <span class="n">x</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">12</span><span class="p">)]</span>
    <span class="n">grid_world</span> <span class="o">=</span> <span class="n">GridWorld</span><span class="p">(</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">start_state</span><span class="p">,</span> <span class="n">goal_state</span><span class="p">,</span> <span class="n">cliff</span><span class="p">)</span>
    <span class="n">n_runs</span> <span class="o">=</span> <span class="mi">50</span>
    <span class="n">n_eps</span> <span class="o">=</span> <span class="mi">500</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">height</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">grid_world</span><span class="p">.</span><span class="n">actions</span><span class="p">)))</span>
    <span class="n">rewards_q_learning</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eps</span><span class="p">)</span>
    <span class="n">rewards_sarsa</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_eps</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_runs</span><span class="p">)):</span>
        <span class="n">Q_q_learning</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">Q_sarsa</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">ep</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_eps</span><span class="p">):</span>
            <span class="n">rewards_q_learning</span><span class="p">[</span><span class="n">ep</span><span class="p">]</span> <span class="o">+=</span> <span class="n">q_learning</span><span class="p">(</span><span class="n">Q_q_learning</span><span class="p">,</span> <span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
            <span class="n">rewards_sarsa</span><span class="p">[</span><span class="n">ep</span><span class="p">]</span> <span class="o">+=</span> <span class="n">sarsa</span><span class="p">(</span><span class="n">Q_sarsa</span><span class="p">,</span> <span class="n">grid_world</span><span class="p">,</span> <span class="n">epsilon</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

    <span class="n">rewards_q_learning</span> <span class="o">/=</span> <span class="n">n_runs</span>
    <span class="n">rewards_sarsa</span> <span class="o">/=</span> <span class="n">n_runs</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_q_learning</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Q-Learning'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards_sarsa</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Sarsa'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Episodes'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Sum of rewards during episode'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mi">100</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'./cliff_walking.png'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>  </div>
</div>

<p>This is our result after completing running the code.</p>
<figure>
	<img src="/assets/images/2022-04-08/cliff_walking.png" alt="Q-learning vs Sarsa on Cliff walking" style="display: block; margin-left: auto; margin-right: auto; width: 500px" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="exp-sarsa">Expected Sarsa</h4>
<p>In the update \eqref{4} of Q-learning, rather than taking the maximum over next state-action pairs, if we use the expected value to consider how likely each action is under the current policy. That means, we instead have the following update rule for $Q$-value:
\begin{align}
Q(S_t,A_t)&amp;\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\mathbb{E}_\pi\big[Q(S_{t+1},A_{t+1}\vert S_{t+1})\big]-Q(S_t,A_t)\Big] \\ &amp;\leftarrow Q(S_t,A_t)+\alpha\Big[R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})Q(S_{t+1}|a)-Q(S_t,A_t)\Big]
\end{align}
However, given the next state, $S_{t+1}$, this algorithms move <em>deterministically</em> in the same direction as Sarsa moves in <em>expectation</em>. Thus, this method is also called <strong>Expected Sarsa</strong>.</p>

<p>Expected Sarsa performs better than Sarsa since it eliminates the variance due to the randomization in selecting $A_{t+1}$. Which also means that it takes expected Sarsa more resource than Sarsa.</p>

<h4 id="double-q-learning">Double Q-learning</h4>

<h5 id="max-bias">Maximization Bias</h5>
<p>Consider a set of $M$ random variables $X=\{X_1,\dots,X_M\}$. Say that we are interested in maximizing expected value of the r.v.s in $X$:
\begin{equation}
\max_{i=1,\dots,M}\mathbb{E}(X_i)\tag{5}\label{5}
\end{equation}
This value can be approximated by constructing approximations for $\mathbb{E}(X_i)$ for all $i$. Let
\begin{equation}
S=\bigcup_{i=1}^{M}S_i
\end{equation}
denote a set of samples, where $S_i$ is the subset containing samples for the variables $X_i$, and assume that the samples in $S_i$ are i.i.d. Unbiased estimates for the expected values can be obtained by computing the sample average for each variable:
\begin{equation}
\mathbb{E}(X_i)=\mathbb{E}(\mu_i)\approx\mu_i(S)\doteq\frac{1}{\vert S_i\vert}\sum_{s\in S_i}s,
\end{equation}
where $\mu_i$ is an estimator for variable $X_i$. This approximation is unbiased since every sample $s\in S_i$ is an unbiased estimate for the value of $\mathbb{E}(X_i)$. Thus, \eqref{5} can be approximated by:
\begin{equation}
\max_{i=1,\dots,M}\mathbb{E}(X_i)=\max_{i=1,\dots,M}\mathbb{E}(\mu_i)\approx\max_{i=1,\dots,M}\mu_i(S)\tag{6}\label{6}
\end{equation}
Let $f_i$, $F_i$ denote the PDF and CDF of $X_i$ and $f_i^\mu, F_i^\mu$ denote the PDF and CDF of $\mu_i$ respectively. Hence we have that
\begin{align}
\mathbb{E}(X_i)&amp;=\int_{-\infty}^{\infty}x f_i(x)\,dx;\hspace{0.5cm}F_i(x)=P(X_i\leq x)=\int_{-\infty}^{\infty}f_i(x)\,dx \\ \mathbb{E}(\mu_i)&amp;=\int_{-\infty}^{\infty}x f_i^\mu(x)\,dx;\hspace{0.5cm}F_i^\mu(x)=P(\mu_i\leq x)=\int_{-\infty}^{\infty}f_i^\mu(x)\,dx
\end{align}
With these notations, considering the maximal estimator $\mu_i$, which is distributed by some PDF $f_{\max}^{\mu}$, we have:
\begin{align}
F_{\max}^{\mu}&amp;\doteq P(\max_i \mu_i\leq x) \\ &amp;=P(\mu_1\leq x;\dots;\mu_M\leq x) \\ &amp;=\prod_{i=1}^{M}P(\mu_i\leq x) \\ &amp;=\prod_{i=1}^{M}F_i^\mu(x)
\end{align}
The value $\max_i\mu_i(S)$ is an unbiased estimate of $\mathbb{E}(\max_i\mu_i)$, which is given by
\begin{align}
\mathbb{E}\left(\max_i\mu_i\right) &amp;=\int_{-\infty}^{\infty}x f_{\max}^{\mu}(x)\,dx \\ &amp;=\int_{-\infty}^{\infty}x\frac{d}{dx}\left(\prod_{i=1}^{M}F_i^\mu(x)\right)\,dx \\ &amp;=\sum_{i=1}^M\int_{-\infty}^{\infty}f_i^\mu(x)\prod_{j\neq i}^{M}F_i^\mu(x)\,dx
\end{align}
However, as can be seen in \eqref{5}, the order of expectation and maximization is the other way around. This leads to the result that $\max_i\mu_i(S)$ is a biased estimate of $\max_i\mathbb{E}(X_i)$</p>

<h5 id="sol">A Solution</h5>
<p>The reason why maximization bias happens is we are using the same samples to decide which action is the best (highest reward one) and also to estimate its action-value. To overcome this situation, Hasselt (2010) proposed an alternative method that uses two set of estimators instead, $\mu^A=\{\mu_1^A,\dots,\mu_M^A\}$ and $\mu^B=\{\mu_1^B,\dots,\mu_M^B\}$. The method is thus also called <strong>double estimators</strong>.</p>

<p>Specifically, we use these two sets to learn two independent estimates, called $Q^A$ and $Q^B$, each is an estimate of the true value $q(a)$, for all $a\in\mathcal{A}$.</p>

<figure>
	<img src="/assets/images/2022-04-08/double-q-learning.png" alt="Double Q-learning" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h2 id="n-step-td">$\boldsymbol{n}$-step TD</h2>
<p>From the definition of <em>one-step TD</em>, we can formalize the idea into a more general, <strong>n-step TD</strong>. Once again, first off, we will be considering the prediction problem.</p>

<h3 id="n-step-td-prediction">$\boldsymbol{n}$-step TD Prediction</h3>
<p>Recall that in <em>one-step TD</em>, the update is based on the next reward, bootstrapping<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> from the value of the state at one step later. In particular,
the target of the update is $R_{t+1}+\gamma V_t(S_{t+1})$, which we are going to denote as $G_{t:t+1}$, or <em>one-step return</em>:
\begin{equation}
G_{t:t+1}\doteq R_{t+1}+\gamma V_t(S_{t+1})
\end{equation}
where $V_t:\mathcal{S}\to\mathbb{R}$ is the estimate at time step $t$ of $v_\pi$. Thus, rather than taking into account one step later, in <em>two-step TD</em>, it makes sense to consider the rewards in two steps further, combined with the value function of the state at two step later. In other words, the target of two-step update is the <em>two-step return</em>:
\begin{equation}
G_{t:t+2}\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 V_{t+1}(S_{t+2})
\end{equation}
Similarly, the target of $n$-step update is the <em>$n$-step return</em>:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t&lt;T-n$. If $t+n\geq T$, then all the missing terms are taken as zero, and the <em>n-step return</em> defined to be equal to the full return:
\begin{equation}
G_{t:t+n}=G_t\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots+\gamma^{T-t-1}R_T,\tag{7}\label{7}
\end{equation}
which is the target of the Monte Carlo update.</p>

<p>Hence, the $n-step TD$ method can be defined as:
\begin{equation}
V_{t+n}(S_t)\doteq V_{t+n-1}(S_t)+\alpha\left[G_{t:t+n}-V_{t+n-1}(S_t)\right],
\end{equation}
for $0\leq t&lt;T$, while the values for all other states remain unchanged: $V_{t+n}(s)=V_{t+n-1}(s),\forall s\neq S_t$. Pseudocode of the algorithm is given right below.</p>
<figure>
	<img src="/assets/images/2022-04-08/n-step-td.png" alt="n-step TD" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<p>From \eqref{7} combined with this definition of <em>$n$-step TD</em> method, it is easily seen that by changing the value of $n$ from $1$ to $\infty$, we obtain a corresponding spectrum ranging from <em>one-step TD method</em> to <em>Monte Carlo method</em>.</p>
<figure>
	<img src="/assets/images/2022-04-08/n-step-td-diagram.png" alt="Backup diagram of n-step TD" style="display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The backup diagram of $n$-step TD methods</figcaption>
</figure>

<h4 id="eg-random-walk">Example: Random Walk</h4>
<p>This is our result after completing running the code.</p>
<figure>
	<img src="/assets/images/2022-04-08/random_walk.png" alt="Q-learning vs Sarsa on Cliff walking" style="display: block; margin-left: auto; margin-right: auto; width: 500px" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="boldsymboln-step-td-control">$\boldsymbol{n}$-step TD Control</h3>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] <span id="td-convergence">Sutton, R.S. <a href="https://doi.org/10.1007/BF00115009">Learning to predict by the methods of temporal differences</a>. Mach Learn 3, 9‚Äì44 (1988).</span></p>

<p>[3] <span id="q-learning-watkins">Chris Watkins. <a href="https://www.researchgate.net/publication/33784417_Learning_From_Delayed_Rewards">Learning from Delayed Rewards</a>. PhD Thesis (1989)</span></p>

<p>[4] Hado Hasselt. <a href="https://papers.nips.cc/paper/2010/hash/091d584fced301b442654dd8c23b3fc9-Abstract.html">Double Q-learning</a>. NIPS 2010</p>

<p>[5] Shangtong Zhang. <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">Reinforcement Learning: An Introduction implementation</a></p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It is a special case of <a href="#n-step-td">n-step TD</a> and TD($\lambda$).¬†<a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Bootstrapping is to update estimates  of the value functions of states based on estimates of value functions of other states.¬†<a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET