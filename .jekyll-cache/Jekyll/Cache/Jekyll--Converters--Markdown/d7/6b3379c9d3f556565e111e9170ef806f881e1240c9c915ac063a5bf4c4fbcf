I"´<blockquote>
  <p>In two previous posts, <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a> and <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with <strong>Dynamic Programming</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-dp">What is Dynamic Programming?</a></li>
  <li><a href="#dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</a>
    <ul>
      <li><a href="#policy-evaluation">Policy Evaluation</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h4 id="what-is-dp">What is Dynamic Programming?</h4>
<p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p>

<h4 id="dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</h4>
<p>DP is a very general method for solving problems having two properties:</p>
<ul>
  <li><em>Optimal substructure</em>
    <ul>
      <li>Principle of optimality applies.</li>
      <li>Optimal solution can be decomposed into sub-problems.</li>
    </ul>
  </li>
  <li><em>Overlapping sub-problems</em>
    <ul>
      <li>Sub-problems recur many times.</li>
      <li>Solutions can be cached and reused.</li>
    </ul>
  </li>
</ul>

<p>MDPs satisfy both properties since:</p>
<ul>
  <li>Bellman equation gives recursive decomposition.</li>
  <li>Value function stores and reuses solutions.</li>
</ul>

<h5 id="policy-evaluation">Policy Evaluation</h5>
<p>Recall from the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html/#bellman-equations">Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]\tag{1}\label{1}
\end{equation}
If the environment‚Äôs dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br />
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;=\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_k(s‚Äô)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html/#banach-fixed-pts">Banach‚Äôs fixed points theorem</a> and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <em>Iterative policy evaluation</em>.<br />
We have the backup diagram for this update</p>

<figure>
	<img src="/assets/images/2021-07-25/iterative-policy-evaluation.png" alt="Backup diagram for iterative policy evalution update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;">Figure 1: Backup diagram for Iterative policy evaluation update</figcaption>
</figure>

<p>When implementing <em>iterative policy evaluation</em>, for all $s\in\mathcal{S}$, we can use:</p>
<ul>
  <li>one array to store the value functions, and update them ‚Äò‚Äòin-place‚Äù (<em>asynchronous DP</em>)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v(s‚Äô)}\right]
\end{equation}</li>
  <li>two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (<em>synchronous DP</em>)
\begin{align}
\color{red}{v_{new}(s)}&amp;\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\color{red}{v_{old}(s‚Äô)}\right]\\ \color{red}{v_{old}}&amp;\leftarrow\color{red}{v_{new}}
\end{align}
Here is the pseudocode of the <em>in-place iterative policy evaluation</em>, given a policy $\pi$, for estimating $V\approx v_\pi$</li>
</ul>

<figure class="highlight"><pre><code class="language-javascript" data-lang="javascript"><span class="nx">def</span> <span class="nx">iterative_policy_evaluation</span><span class="p">(</span><span class="nx">policy</span><span class="p">,</span> <span class="nx">threshold</span><span class="p">):</span>
  <span class="nx">V</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span> <span class="o">=</span> <span class="nx">arbitrary_values</span>
  <span class="nx">V</span><span class="p">(</span><span class="nx">terminal_state</span><span class="p">)</span> <span class="o">=</span> <span class="mi">0</span>

  <span class="k">while</span><span class="p">:</span>
    <span class="nx">delta</span> <span class="o">=</span> <span class="mi">0</span>
      <span class="k">for</span> <span class="nx">s</span> <span class="k">in</span> <span class="nx">S</span><span class="p">:</span>
        <span class="nx">v</span> <span class="o">=</span> <span class="nx">V</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span>
        <span class="nx">V</span><span class="p">(</span><span class="nx">s</span><span class="p">)</span> <span class="o">=</span> <span class="nx">sum_over_</span><span class="p">{</span><span class="nx">a</span><span class="p">}[</span><span class="nx">pi</span><span class="p">(</span><span class="nx">a</span><span class="p">,</span><span class="nx">s</span><span class="p">)</span> <span class="o">*</span> <span class="nx">sum_over_</span><span class="p">{</span><span class="nx">s</span><span class="err">\</span><span class="dl">'</span><span class="s1">,r}[p(s</span><span class="dl">'</span><span class="p">,</span><span class="nx">r</span><span class="p">,</span><span class="nx">s</span><span class="p">,</span><span class="nx">a</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="nx">r</span> <span class="o">+</span> <span class="nx">gamma</span> <span class="o">*</span> <span class="nx">V</span><span class="p">(</span><span class="nx">s</span><span class="dl">'</span><span class="s1">))]]
        delta = max(delta, absolute_of_{v - V(s)})
    if delta &lt; 0:
      break</span></code></pre></figure>

:ET