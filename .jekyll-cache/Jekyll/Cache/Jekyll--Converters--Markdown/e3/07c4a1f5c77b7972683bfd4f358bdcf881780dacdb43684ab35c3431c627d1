I"æ<blockquote>
  <p>Beside <a href="/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html#n-step-td">$n$-step TD</a> methods, there is another mechanism called <strong>Eligible traces</strong> that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD method ($\lambda=0$) to Monte Carlo methods ($\lambda=1$).
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#lambda-return">The $\lambda$-return</a></li>
  <li><a href="#td-lambda">TD($\lambda$)</a></li>
  <li><a href="#sarsa-lambda">Sarsa($\lambda$)</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lambda-return">The $\lambda$-return</h2>
<p>Recall that in <a href="/artificial-intelligent/reinforcement-learning/2022/04/08/td-learning.html#n-step-td-prediction">TD-Learning</a> post, we have defined the $n$-step return as
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t\lt T-n$. After the post of <a href="/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html">Function Approximation</a>, for any parameterized function approximator, we can generalize that equation into:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+
\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n}-1),
\end{equation}
for $0\leq t\leq T-n$, where $\hat{v}(s,\mathbf{w})$ is the approximate value of state $s$ given weight vctor $\mathbf{w}$.</p>

<p>We already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate <a href="/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html#stochastic-grad">SGD update</a>, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose
\begin{equation}
\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}
\end{equation}
as the target for our update.</p>

<p>The <strong>TD($\lambda$)</strong> is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\lambda^{n-1}$, for $\lambda\in\left[0,1\right]$, and is normalized by a factor of $1-\lambda$ to guarantee that the weights sum to $1$, as:
\begin{equation}
G_t^\lambda\doteq(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}
\end{equation}
The $G_t^\lambda$ is called <strong>$\lambda$-return</strong> of the update.</p>

<p>This figure below illustrates the backup diagram of TD($\lambda$) algorithm.</p>
<figure>
	<img src="/assets/images/2022-08-08/td-lambda-backup.png" alt="Backup diagram of TD(lambda)" style="display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: The backup diagram of TD($\lambda$)</figcaption>
</figure>

<p>With the definition of $\lambda$-return, we can define the <strong>offline $\lambda$-return</strong> algorithm, which use semi-gradient update and using $\lambda$-return as the target:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{v}(S_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}</p>

<p>[TODO] Add example</p>

<h2 id="td-lambda">TD($\lambda$)</h2>

<h2 id="sarsalambda">Sarsa($\lambda$)</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] Precup, Doina; Sutton, Richard S.; and Singh, Satinder. <a href="https://scholarworks.umass.edu/cs_faculty_pubs/80">Eligibility Traces for Off-Policy Policy Evaluation</a> (2000). ICML â€˜00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.</p>

<p>[3]</p>

<h2 id="footnotes">Footnotes</h2>
:ET