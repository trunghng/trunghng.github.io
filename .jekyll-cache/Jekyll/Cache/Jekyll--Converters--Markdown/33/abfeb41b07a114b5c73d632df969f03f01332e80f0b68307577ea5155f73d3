I"<blockquote>
  <p>So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a <strong>parameterized policy</strong>, $\mathbf{\theta}$, that can select actions without consulting a value function by considering the gradient of some performance measure w.r.t $\mathbf{\theta}$. Such methods are called <strong>policy gradient methods</strong>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#policy-approx">Policy Approximation</a></li>
  <li><a href="#policy-grad-theorem">The Policy Gradient Theorem</a></li>
  <li><a href="#reinforce">REINFORCE</a></li>
  <li><a href="#actor-critic-methods">Actor-Critic Methods</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="policy-approx">Policy Approximation</h2>

<h2 id="policy-grad-theorem">The Policy Gradient Theorem</h2>
<p><strong>Proof</strong><br />
We have that the gradient of the state-value function w.r.t $\mathbf{\theta}$ can be written in terms of the action-value function as:
\begin{align}
\nabla_\mathbf{\theta}v_\pi(s)&amp;=\nabla_\mathbf{\theta}\Big[\sum_a\pi(a|s)q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;=\sum_a\Big[\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\nabla_\mathbf{\theta}q_\pi(s,a)\Big] \\ &amp;=\sum_a\Big[\nabla_\mathbf{\theta}\pi(s|a)q_\pi(a,s)+\pi(a|s)\nabla_\mathbf{\theta}\sum_{s’,r}p(s’,r|s,a)\big(r+v_\pi(s’)\big)\Big] \\ &amp;=\sum_a\Big[\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s’}p(s’|s,a)\nabla_\mathbf{\theta}v_\pi(s’)\Big] \\ &amp;=\sum_a\Big[\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a)+\pi(a|s)\sum_{s’}p(s’|s,a)\sum_{a’}\big(\nabla_\mathbf{\theta}\pi(s’|a’)q_\pi(s’,a’) \\ &amp;\hspace{2cm}+\pi(a’|s’)\sum_{s''}p(s''\vert s’,a’)\nabla_\mathbf{\theta}v_\pi(s'')\big)\Big] \\ &amp;=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty}P(s\to x,k,\pi)\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a),
\end{align}
After repeated unrolling as in the fifth step, where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$. It is then immediate that:
\begin{align}
\nabla_\mathbf{\theta}J(\mathbf{\theta})&amp;=\nabla_\mathbf{\theta}v_\pi(s_0) \\ &amp;=\sum_s\Big(\sum_{k=0}^{\infty}P(s_0\to s,k,\pi)\Big)\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_s\eta(s)\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_{s’}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;=\sum_{s’}\eta(s’)\sum_s\mu(s)\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a) \\ &amp;\propto\sum_s\mu(s)\sum_a\nabla_\mathbf{\theta}\pi(a|s)q_\pi(s,a),
\end{align}
where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode:
\begin{equation}
\eta(s)=h(s)
\end{equation}
where $h(s)$ denotes the probability that an episode begins in each state $s$.</p>

<h2 id="reinforce">REINFORCE</h2>

<h2 id="actor-critic-methods">Actor-Critic Methods</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<h2 id="footnotes">Footnotes</h2>
:ET