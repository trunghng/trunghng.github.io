I"o<blockquote>
  <p>A note on Neural networks.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#ff-func">Feed-forward network functions</a>
    <ul>
      <li><a href="#unv-approx">Universal approximation property</a></li>
      <li><a href="#w-s-sym">Weight-space symmetries</a></li>
    </ul>
  </li>
  <li><a href="#net-training">Network training</a></li>
  <li><a href="#backprop">Backpropagation</a></li>
  <li><a href="#bayes-nn">Bayesian neural networks</a>
    <ul>
      <li><a href="#posterior-param-dist">Posterior parameter distribution</a></li>
    </ul>
  </li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="ff-func">Feed-forward network functions</h2>
<p>Recall that the <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html">linear models</a> used in regression and classification are based on linear combination of fixed nonlinear basis function $\phi_j(\mathbf{x})$ and take the form
\begin{equation}
y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\mathbf{x})\right),\label{1}
\end{equation}
where in the case of regression, $f$ is the function $f(x)=x$, while in the classification case, $f$ takes the form of a nonlinear activation function (e.g., the <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html#logistic-sigmoid-func">sigmoid function</a>).</p>

<p><strong>Neural networks</strong> extend this model \eqref{1} by letting each basis functions $\phi_j(\mathbf{x})$ be a nonlinear function of a linear combination of the inputs, where the coefficients in the combination are the adaptive parameters.</p>

<p>More formally, neural networks is a series of layers, in which each layer represents a functional transformation. Let us consider the first layer by constructing $M$ linear combinations of the input variable $x_1,\ldots,x_D$ in the form
\begin{equation}
a_j=\sum_{i=1}^{D}w_{j\,i}^{(1)}x_i+w_{j\,0}^{(1)},\label{2}
\end{equation}
where</p>
<ul>
  <li>$j=1,\ldots,M$;</li>
  <li>the superscript $(1)$ indicates that we are working with parameters of the first layer;</li>
  <li>$w_{j\,i}^{(1)}$â€™s are called the <strong>weights</strong>;</li>
  <li>$w_{j\,0}^{(1)}$â€™s are known as the <strong>biases</strong>;</li>
  <li>$a_j$â€™s are referred as <strong>activations</strong>.</li>
</ul>

<p>The activations $a_j$â€™s are then transformed using a differentiable, nonlinear <strong>activation function</strong> $h(\cdot)$, which correspond to $f(\cdot)$ in \eqref{1} to give
\begin{equation}
z_j=h(a_j),\label{3}
\end{equation}
where $z_j$ are called the <strong>hidden units</strong>. Repeating the same procedure as \eqref{2}, which was following \eqref{1}, $z_j$â€™s are taken as the inputs of the second layer to give $K$ outputs
\begin{equation}
a_k=\sum_{j=1}^{M}w_{k\,j}^{(2)}z_j+w_{k\,0}^{(2)},\label{4}
\end{equation}
where $k=1,\ldots,K$.</p>

<p>This process will be repeated in $L$ times with $L$ is the number of layers. At the last layer, for instance, the second layer of our example network, the outputs, also called <strong>output unit activations</strong>, $a_k$â€™s are transformed using an appropriate activation function to give a set of network output $y_k$. For example, in multiple binary classification problems, we can choose the logistic sigmoid as our activation function that
\begin{equation}
y_k=\sigma(a_k)\label{5}
\end{equation}
Combining all these steps \eqref{2}, \eqref{3}, \eqref{4} and \eqref{5} together, our neural network with sigmoidal output unit activation functions can be defined as
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=1}^{M}w_{k\,j}^{(2)}h\left(\sum_{i=1}^{D}w_{j\,i}^{(1)}x_i+w_{j\,0}^{(1)}\right)+w_{k\,0}^{(2)}\right),\label{6}
\end{equation}
where all of the weights and biases are comprises together into a parameter vector $\mathbf{w}$. As suggested in <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html#dummy-coeff">linear regression</a>, we can also let the bias $w_{j\,0}^{(1)}$ be coefficient of a dummy input variable $x_0=1$ that makes \eqref{2} can be written as
\begin{equation}
a_j=\sum_{i=0}^{D}w_{j\,i}^{(1)}x_i
\end{equation}
This results that our subsequent layers are also able to be written in a more convenient form, which lets the entire network \eqref{6} take the form
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=0}^{M}w_{k\,j}^{(2)}h\left(\sum_{i=0}^{D}w_{j\,i}^{(1)}x_i\right)\right)
\end{equation}
Our network is also an example of a <strong>multilayer perception</strong>, or <strong>MLP</strong>, which is a combination of <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html#perceptron">perceptron models</a>. The key difference is that while the neural network uses continuous sigmoidal nonlinearities in the hidden units, which is differentiable w.r.t the parameters, the perceptron algorithm uses step-function nonlinearities, which is in contrast non-differentiable.</p>

<p>The network network we have been considering so far is <strong>feed-forward neural network</strong>, whose outputs are deterministic functions of the inputs. Each (hidden or output) unit in such a network computes a function given by
\begin{equation}
z_k=h\left(\sum_{j}w_{k\,j}z_j\right),
\end{equation}
where the sum runs all over units sending connections to unit $k$ (bias included).</p>

<h3 id="unv-approx">Universal approximation property</h3>
<p>Feed-forward networks with <strong>hidden layers</strong> (i.e., the layers in which the training data does not show the desired output, e.g., the first layer of our network, the second layer on the other hands is called the <strong>output layer</strong>) provide <strong>universal approximation</strong> property.</p>

<p>In concrete, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any <strong>squashing</strong> activation function (e.g., the logistic sigmoid function) an approximate any continuous function on a compact subsets of $\mathbf{R}^n$.</p>

<h3 id="w-s-sym">Weight-space symmetries</h3>

<h2 id="net-training">Network training</h2>
<p>Consider given a data set of $\{\mathbf{x}_n,\mathbf{t}_n\}$, for $n=1,\ldots,N$ and we wish to minimize the error function
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w})-\mathbf{t}_n\Vert^2
\end{equation}</p>

<h2 id="backprop">Backpropagation</h2>

<h2 id="bayes-nn">Bayesian neural networks</h2>

<h3 id="posterior-param-dist">Posterior parameter distribution</h3>

<h2 id="preferences">Preferences</h2>
<p>[1] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY.</p>

<p>[2] Ian Goodfellow &amp; Yoshua Bengio &amp; Aaron Courville. <a href="https://www.deeplearningbook.org">Deep Learning</a>. MIT Press (2016).</p>

<h2 id="footnotes">Footnotes</h2>
:ET