I"
<p>You may have known or heard vaguely about a computer program called <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with <strong>Reinforcement Learning</strong>.</p>

<h3 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h3>
<p>Say, there is an unknown <strong>environment</strong> that we’re trying to put an <strong>agent</strong> on. By interacting with the <strong>agent</strong> through taking <strong>actions</strong> that gives rise to <em>rewards</em> continually, the <strong>agent</strong> learns a <strong>policy</strong> that maximize the cumulative <strong>rewards</strong>.<br />
<strong>Reinforcement Learning (RL)</strong>, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy for the <strong>agent</strong> from experimental trials and relative simple feedback received. With the optimal <strong>policy</strong>, the agent is capable to actively adapt to the environment to maximize future rewards.
<img src="/assets/images/robot.png" alt="RL" /></p>

<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p><strong>Markov decision processes (MDPs)</strong> formally describe an environment for <strong>RL</strong>. And almost all <strong>RL</strong> problems can be formalised as <strong>MDPs</strong>.</p>

<p><strong>Definition (MDP)</strong><br />
A <strong>Markov Decision Process</strong> is a tuple $⟨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma⟩$</p>
<ul>
  <li>$\mathcal{S}$ is a set of states called <em>state space</em></li>
  <li>$\mathcal{A}$ is a set of actions called <em>action space</em></li>
  <li>$\mathcal{P}$ is a state transition probability matrix<br />
  \(\mathcal{P}^a_{ss'}=P(S_{t+1}=s'|S_t=s,A_t=a)\)</li>
  <li>$\mathcal{R}$ is a reward function<br />
  \(\mathcal{R}_s^a=\mathbb{E}[R_{t+1}|S_t=s,A_t=a]\)</li>
  <li>$\gamma$ is a discount factor for future reward, $\gamma\in[0, 1]$</li>
</ul>

<p><strong>MDP</strong> is an extension of <a href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">Markov chain</a>. If only one action exists for each state, and all rewards are the same, an <em>MDP</em> reduces to a <em>Markov chain</em>. All states in <strong>MDP</strong> has <em>Markov property</em>, referring to the fact that the current state captures all relevant information from the history.</p>
:ET