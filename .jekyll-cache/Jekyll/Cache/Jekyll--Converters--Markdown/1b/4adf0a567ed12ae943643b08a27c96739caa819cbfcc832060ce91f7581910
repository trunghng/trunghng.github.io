I"þ<p>You may have known or heard vaguely about a computer program called <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a> - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with <strong>Reinforcement Learning</strong>.</p>

<h3 id="what-is-reinforcement-learning">What is Reinforcement Learning?</h3>
<p>Say, there is an unknown <strong>environment</strong> that weâ€™re trying to put an <strong>agent</strong> on. By interacting with the <strong>agent</strong> through taking <strong>actions</strong> that gives rise to <em>rewards</em> continually, the <strong>agent</strong> learns a <strong>policy</strong> that maximize the cumulative <strong>rewards</strong>.<br />
<strong>Reinforcement Learning (RL)</strong>, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy for the <strong>agent</strong> from experimental trials and relative simple feedback received. With the optimal <strong>policy</strong>, the agent is capable to actively adapt to the environment to maximize future rewards.
<img src="/assets/images/robot.png" alt="RL" /></p>

<h3 id="markov-decision-process-mdp">Markov Decision Process (MDP)</h3>
<p><strong>Markov decision processes (MDPs)</strong> formally describe an environment for <strong>RL</strong>. And almost all <strong>RL</strong> problems can be formalised as <strong>MDPs</strong>.</p>

<p><strong>Definition (MDP)</strong><br />
A <strong>Markov Decision Process</strong> is a tuple $âŸ¨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gammaâŸ©$</p>
<ul>
  <li>$\mathcal{S}$ is a set of states called <em>state space</em></li>
  <li>$\mathcal{A}$ is a set of actions called <em>action space</em></li>
  <li>$\mathcal{P}$ is a state transition probability matrix<br />
  \(\mathcal{P}^a_{ss'}=P(S_{t+1}=s'|S_t=s,A_t=a)\)</li>
  <li>$\mathcal{R}$ is a reward function<br />
  \(\mathcal{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]\)</li>
  <li>$\gamma$ is a discount factor for future reward, $\gamma\in[0, 1]$</li>
</ul>

<p><strong>MDP</strong> is an extension of <a href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">Markov chain</a>. If only one action exists for each state, and all rewards are the same, an <em>MDP</em> reduces to a <em>Markov chain</em>. All states in <strong>MDP</strong> has <em>Markov property</em>, referring to the fact that the current state captures all relevant information from the history
\begin{equation}
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\dots,S_t)
\end{equation}</p>

<h4 id="return">Return</h4>
<p>In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the <em>expected return</em>.</p>

<p><strong>Definition</strong> (<em>Return</em>)<br />
The <em>return</em> $G_t$ is the total discounted reward from t
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called <em>discount rate</em> (or <em>discount factor</em>).</p>

<p>The <em>discount rate</em> $\gamma$ determines the present value of future rewards: a reward received
k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.</p>

<h4 id="policy">Policy</h4>
<p><em>Policy</em>, which is denoted as $\pi$, is the behaviour function of the agent. $\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either <em>deterministic</em> or <em>stochastic</em></p>
<ul>
  <li>Deterministic policy:	$\quad\pi(s)=a$</li>
  <li>Stochastic policy: $\quad\pi(a|s)=P(A_t=a|S_t=s)$</li>
</ul>

<h4 id="value-function">Value Function</h4>
<p><em>Value function</em> measures <em>how good</em> the a particular state is (or <em>how good</em> it is to perform a given action in a given state).</p>

<p><strong>Definition</strong> (<em>state-value function</em>)<br />
The <em>state-value function</em> of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=E_\pi[G_t|S_t=s]
\end{equation}</p>

<p><strong>Definition</strong> (<em>action-value function</em>)<br />
Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]
\end{equation}</p>

<p>Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}</p>

<h4 id="optimal-policy-and-optimal-value-function">Optimal Policy and Optimal Value Function</h4>
<p>For finite MDPs (finte state and action space), we can precisely define an <em>optimal policy</em>. <em>Value functions</em> define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\piâ€™$ if its expected return is greater than or equal to that of $\piâ€™$ for all states. In other words,
\begin{equation}
pi\geq\piâ€™\iff v_\pi(s)\geq v_{\piâ€™} \forall s\in\mathcal{S}
\end{equation}</p>

<p><strong>Theorem</strong> (<em>Optimal policy</em>)<br />
For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal to all other policies,
\begin{equation}
\pi_*\geq\pi,\forall\pi
\end{equation}</p>

<p>The proof of the above theorem is gonna be provided in another post since we need some additional tools to do that.</p>

<p>There may be more than one <em>optimal policy</em>, they share the same <em>state-value function</em>, called <em>optimal state-value function</em> though.
\begin{equation}
v_*(s)=\max_{\pi}v_\pi(s)
\end{equation}
<em>Optimal policies</em> also share the same <em>action-value function</em>, call <em>optimal action-value function</em>
\begin{equation}
q_*(s,a)=\max_{\pi}q_\pi(s,a)
\end{equation}</p>

<h3 id="bellman-equations">Bellman Equations</h3>
<p>A fundamental property of <em>value functions</em> used throughout RL is that they satisfy recursive relationships
\begin{align}
v_\pi(s)&amp;=E_\pi[G_t|S_t=s] \\&amp;=E_\pi[R_t+\gamma G_{t+1}|S_t=s] \\&amp;=\sum_{sâ€™,r,gâ€™,a}p(sâ€™,r,gâ€™,a|s)(r+\gamma gâ€™) \\&amp;=\sum_{a}p(a|s)\sum_{sâ€™,r,gâ€™}p(sâ€™,r,gâ€™|a,s)(r+\gamma gâ€™) \\&amp;=\sum_{a}\pi(a|s)\sum_{sâ€™,r,gâ€™}p(sâ€™,r|a,s)p(gâ€™|sâ€™,r,a,s)(r+\gamma gâ€™) \\&amp;=\sum_{a}\pi(a|s)\sum_{sâ€™,r}p(sâ€™,r|a,s)\sum_{gâ€™}p(gâ€™|sâ€™)(r+\gamma gâ€™) \\&amp;=\sum_{a}\pi(a|s)\sum_{sâ€™,r}p(sâ€™,r|a,s)\left[r+\gamma\sum_{gâ€™}p(gâ€™|sâ€™)gâ€™\right] \\&amp;=\sum_{a}\pi(a|s)\sum_{sâ€™,r}p(sâ€™,r|a,s)\left[r+\gamma v_\pi(sâ€™)\right],
\end{align}
where $p(sâ€™,r|s,a)=P(S_{t+1}=sâ€™,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the <em>Bellman equation for</em> $v_\pi(s)$. It expresses a relationship between the value state $s$ and the values of its successor states $sâ€™$.</p>

<p>Similarly, we define the <em>Bellman equation for</em> $q_\pi(s,a)$
\begin{align}
q_\pi(s,a)&amp;=E_\pi[G_t|S_t=s,A_t=a] \\&amp;=E_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;=\sum_{sâ€™,r}p(sâ€™,r|s,a)\left[r+\gamma\sum_{aâ€™}\pi(aâ€™|sâ€™)q_\pi(sâ€™,aâ€™)\right]
\end{align}</p>

<h4 id="bellman-optimality-equations">Bellman Optimality Equations</h4>
:ET