I"À
<blockquote>
  <p>A note on Neural networks.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#ff-func">Feed-forward network functions</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="ff-func">Feed-forward network functions</h2>
<p>Recall that the <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html">linear models</a> used in regression and classification are based on linear combination of fixed nonlinear basis function $\phi_j(\mathbf{x})$ and take the form
\begin{equation}
y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\mathbf{x})\right),\label{1}
\end{equation}
where in the case of regression, $f$ is the function $f(x)=x$, while in the classification case, $f$ takes the form of a nonlinear activation function (e.g., the <a href="/artificial-intelligent/machine-learning/2022/08/13/linear-models.html#logistic-sigmoid-func">sigmoid function</a>).</p>

<p><strong>Neural networks</strong> extend this model \eqref{1} by letting each basis functions $\phi_j(\mathbf{x})$ be a nonlinear function of a linear combination of the inputs, where the coefficients in the combination are the adaptive parameters.</p>

<p>More formally, neural networks is a series of layers, in which each layer represents a functional transformation. Let us consider the first layer by constructing $M$ linear combinations of the input variable $x_1,\ldots,x_D$ in the form
\begin{equation}
a_j=\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)},\label{2}
\end{equation}
where</p>
<ul>
  <li>$j=1,\ldots,M$;</li>
  <li>the superscript $(1)$ indicates that we are working with parameters of the first layer;</li>
  <li>$w_{ji}^{(1)}$â€™s are called the <strong>weights</strong>;</li>
  <li>$w_{j0}^{(1)}$â€™s are known as the <strong>biases</strong>;</li>
  <li>$a_j$â€™s are referred as <strong>activations</strong>.</li>
</ul>

<p>The activations $a_j$â€™s are then transformed using a differentiable, nonlinear <strong>activation function</strong> $h(\cdot)$, which correspond to $f(\cdot)$ in \eqref{1} to give
\begin{equation}
z_j=h(a_j),
\end{equation}
where $z_j$ are called the <strong>hidden units</strong>. Repeating the same procedure as \eqref{2}, which was following \eqref{1}, $z_j$â€™s are taken as the inputs of the second layer to give $K$ outputs
\begin{equation}
a_k=\sum_{j=1}^{M}w_{kj}^{2}z_j+w_{k0}^{2},
\end{equation}
where $k=1,\ldots,K$.</p>

<p>This process will repeat in $L$ times with $L$ is</p>

<h2 id="preferences">Preferences</h2>
<p>[1] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY.</p>

<h2 id="footnotes">Footnotes</h2>
:ET