I"¡<blockquote>
  <p>In two previous posts, <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a> and <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with <strong>Dynamic Programming</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-dp">What is Dynamic Programming?</a></li>
  <li><a href="#dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</a>
    <ul>
      <li><a href="#policy-evaluation">Policy Evaluation</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h4 id="what-is-dp">What is Dynamic Programming?</h4>
<p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p>

<h4 id="dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</h4>
<p>DP is a very general method for solving problems having two properties:</p>
<ul>
  <li><em>Optimal substructure</em>
    <ul>
      <li>Principle of optimality applies.</li>
      <li>Optimal solution can be decomposed into sub-problems.</li>
    </ul>
  </li>
  <li><em>Overlapping sub-problems</em>
    <ul>
      <li>Sub-problems recur many times.</li>
      <li>Solutions can be cached and reused.</li>
    </ul>
  </li>
</ul>

<p>MDPs satisfy both properties since:</p>
<ul>
  <li>Bellman equation gives recursive decomposition.</li>
  <li>Value function stores and reuses solutions.</li>
</ul>

<h5 id="policy-evaluation">Policy Evaluation</h5>
<p>Recall from the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html/#bellman-equations">Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_\pi(s‚Äô)\right]\tag{1}\label{1}
\end{equation}
If the environment‚Äôs dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br />
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;=\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+\gamma v_k(s‚Äô)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html/#banach-fixed-pts">Banach‚Äôs fixed points theorem</a> and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <em>Iterative policy evaluation</em>.<br />
We have the backup diagram for this update</p>

<figure>
	<img src="/assets/images/2021-07-25/iterative-policy-evaluation.png" alt="Backup diagram for iterative policy evalution update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;">Figure 1: Backup diagram for Iterative policy evaluation update</figcaption>
</figure>

<p>When implementing <em>iterative policy evaluation</em>, for all $s\in\mathcal{S}$, we can use one array to store the value functions, and update them ‚Äò‚Äòin-place‚Äù (asynchronous DP)
\begin{equation}
v(s)\leftarrow\sum_a\pi(a|s)\sum_{s‚Äô,r}p(s‚Äô,r|s,a)\left[r+v(s‚Äô)\right]
\end{equation}</p>

:ET