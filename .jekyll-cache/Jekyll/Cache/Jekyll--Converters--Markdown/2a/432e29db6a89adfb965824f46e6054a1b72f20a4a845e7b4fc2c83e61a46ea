I"<!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the post <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{s’}P(s’\vert s,a)\left(R(s,a,s’)+\gamma V_\pi(s’)\right)
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\left(R(s,a,s’)+\gamma V^*(s’)\right),\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s’$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{k+1}(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\left(R(s,a,s’)+\gamma V_k(s’)\right)\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_k\}$, will eventually converges to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banach’s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_{k}(s)$ to $V_{k+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-05-25/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma\sum_{a’}\pi(a’\vert s’)Q_\pi(s’,a’)\big] \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma\max_{a’}Q^*(s’,a’)\big]\label{eq:qvi.2} \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected cumulative reward when starting at state $s$, taking action $a$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{k+1}(s,a)=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma\max_{a’}Q_k(s’,a’)\big]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converges to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-05-25/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{k+1}(s,a)=\mathbb{E}\big[R(s,a,s’)+\gamma\max_{a’}Q_k()\big]
\end{equaiton}</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>Recall that</p>

<h2 id="references">References</h2>
<p>[1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[3] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[4] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[5] Hado van Hasselt. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Double Q-learning</a>. NIPS 2010.</p>

<p>[6] Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</p>

<h2 id="footnotes">Footnotes</h2>
:ET