I"<blockquote>
  <p>A note on Inverse Reinforcement Learning</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#mdp">Markov decision processes</a></li>
      <li><a href="#irl">Inverse reinforcement learning</a></li>
    </ul>
  </li>
  <li><a href="#max-margin-methods">Max margin methods</a>
    <ul>
      <li><a href="#max-margin-proj">Max-margin &amp; Projection</a>
        <ul>
          <li><a href="#max-margin">Max-margin</a></li>
          <li><a href="#proj">Projection</a></li>
        </ul>
      </li>
      <li><a href="#max-margin-pln">Max margin planning</a></li>
      <li><a href="#learch">LEARCH</a></li>
    </ul>
  </li>
  <li><a href="#max-ent">Max entropy methods</a></li>
  <li><a href="#bayes">Bayesian methods</a>
    <ul>
      <li><a href="#birl">BIRL</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="mdp">Markov decision processes</h3>
<p>We begin by recalling the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html#mdp"><strong>Markov decision processes (MDP)</strong></a>.</p>

<p>A <strong>Markov decision process</strong>, or <strong>MDP</strong> is defined to be a tuple $(\mathcal{S},\mathcal{A},T,r,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a set of states, represents the <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a set of actions, known as the <strong>action space</strong>.</li>
  <li>$T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>state transition probabilities</strong>, i.e., $T(sâ€™,s,a)=p(sâ€™\vert s,a)$ denotes the probability of transitioning to state $sâ€™$ when taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ denotes the <strong>reward function</strong>.</li>
  <li>$\gamma\in[0,1]$ is referred as <strong>discount factor</strong>.</li>
</ul>

<p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$.</p>

<p>For a policy $\pi$, the <strong>state value function</strong>, denoted as $v_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$. Specifically, $v_\pi(s)$ is defined as the expected sum of discounted rewards when starting in $s$ and following $\pi$.</p>

<p>The expected value of state value function presents the value of the policy it is following. In particular, the value of a policy $\pi$ can be given by
\begin{equation}
v_\pi(S_0)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r(S_t,\pi(S_t))\big\vert S_0\right]\label{eq:mdp.1}
\end{equation}
Analogously, the <strong>state-action value function</strong>, denoted by $q_\pi$, of a state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$ specifies how good it is to take action $a$ from state $s$. Similar to $v_\pi$, $q_\pi$ is defined as the expected sum of discounted rewards when starting from $s$, taking action $a$ and thereby following policy $\pi$.</p>

<h3 id="irl">Inverse reinforcement learning</h3>
<p>In <strong>reverse reinforcment learning</strong>, we are instead working on an interface of tuple $(\mathcal{S},\mathcal{A},T,\gamma)$, which represents an $MDP$ without a predefined reward function $r$, or MDP\R for short. Rather than an explicit reward function given, we are provided samples $\{\tau_i\}$ sampled from an optimal policy $\pi^*(\tau)$.</p>

<p>In this problem, our goal is to learn a reward function $r_\boldsymbol{\psi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ parameterized by the weight vector $\boldsymbol{\psi}$, and then use it to learn the optimal policy $\pi^*$. Specifically, for a linear reward function, we have
\begin{equation}
r_\boldsymbol{\psi}(s,a)=\sum_{i}\psi_i f_i(s,a)=\boldsymbol{\psi}^\text{T}\mathbf{f}(s,a),
\end{equation}
where the weight vector is defined as 
\begin{equation}
\boldsymbol{\psi}=(\psi_1,\ldots,\psi_k)^\text{T},
\end{equation}
and where $\mathbf{f}(s,a)$ is called the <strong>feature vector</strong> of the state-action pair $(s,a)$, defined by
\begin{equation}
\mathbf{f}(s,a)=\big(f_1(s,a),\ldots,f_k(s,a)\big)^\text{T}
\end{equation}
By \eqref{eq:mdp.1}, the value of a policy $\pi^{r_\boldsymbol{\psi}}$ corresponding to the reward function $r_\boldsymbol{\psi}$ thus can be written as
\begin{equation}
v_{\pi^{r_\boldsymbol{\psi}}}()
\end{equation}
The <strong>feature expectation</strong> or also called the expected discounted accumulated feature value, denoted as $\boldsymbol{\mu}(\pi)$, of a policy $\pi$ is defined as
\begin{equation}
\boldsymbol{\mu}(\pi)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t\boldsymbol{\phi}\big(S_t,\pi(A_t)\big)\right]
\end{equation}
Hence, we have that
\begin{equation}
v_\pi(s_0)=\mathbf{w}^\text{T}\boldsymbol{\mu}(\pi)
\end{equation}</p>

<h2 id="max-margin-methods">Max margin methods</h2>
<p>The idea of <strong>max margin methods</strong> is to learn a reward function $\tilde{R}_E$ that explains the demonstrated policy $\pi_E$ better than alternative policies by a margin.</p>

<h3 id="max-margin-proj">Max-margin &amp; Projection</h3>
<p>In these algorithms setting, we are given the expertâ€™s feature expectation $\boldsymbol{\mu}_E$ and our work is to find a policy $\tilde{\pi}$ such that
\begin{equation}
\left\Vert\boldsymbol{\tilde{\pi}}-\boldsymbol{\mu}_E\right\Vert_2\leq\varepsilon,
\end{equation}
where $\varepsilon&gt;0$.</p>

<p>For such $\tilde{\pi}$, we have
\begin{equation}
\left\vert\mathbb{E}_{\pi_E}\left[\sum_{t=0}^{\infty}\right]\right\vert
\end{equation}</p>

<h4 id="max-margin">Max-margin</h4>

<h4 id="proj">Projection</h4>

<h3 id="max-margin-pln">Max margin planning</h3>

<h3 id="learch">LEARCH</h3>

<h2 id="max-ent">Max entropy methods</h2>

<h2 id="references">References</h2>
<p>[1] Pieter Abbeel &amp; Andrew Y. Ng. <a href="https://doi.org/10.1145/1015330.1015430">Apprenticeship Learning via Inverse Reinforcement Learning</a>. ICML â€˜04: Proceedings of the twenty-first international conference on Machine learning. July 2004.</p>

<p>[2] Nathan D. Ratliff, J. Andrew Bagnell &amp; Martin A. Zinkevich. <a href="https://doi.org/10.1145/1143844.1143936">Maximum margin planning</a>. ICML â€˜06: Proceedings of the 23rd international conference on Machine learning. June 2006</p>

<p>[3] Nathan Ratliff, David Silver &amp; J. Andrew (Drew) Bagnell. <a href="https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/">Learning to search: Functional gradient techniques for imitation learning</a>. Autonomous Robots. July 2009.</p>

<h2 id="footnotes">Footnotes</h2>
:ET