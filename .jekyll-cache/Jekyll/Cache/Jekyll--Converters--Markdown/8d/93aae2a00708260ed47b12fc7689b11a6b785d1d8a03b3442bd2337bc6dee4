I"´<blockquote>
  <p>Recall that when using <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html">dynamic programming (DP) method</a> in solving reinforcement learning problems, we required the availability of a model of the enviroment. Whereas with <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html">Monte Carlo methods</a> and <a href="/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html">temporal-difference learning</a>, the models are unnecessary. Such methods with requirement of a model like the case of DP is called <strong>model-based</strong>, while methods without using a model is called <strong>model-free</strong>. 
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#models-planning">Models &amp; Planning</a></li>
  <li><a href="#prior-sweep">Prioritized Sweeping</a></li>
  <li><a href="#trajectory-sampling">Trajectory Sampling</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="models-planning">Model &amp; Planning</h2>
<p>A <strong>model</strong> of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the enviroment to its chosen actions.</p>

<h2 id="prior-sweep">Prioritized Sweeping</h2>

<h2 id="trajectory-sampling">Trajectory Sampling</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[2] David Silver &amp; Richard S. Sutton &amp; Martin MÃ¼ller. <a href="https://doi.org/10.1145/1390156.1390278">Sample-Based Learning and Search with Permanent and Transient Memories</a>. ICML â€˜08: Proceedings of the 25th international conference on Machine learning. July 2008.</p>

<p>[3]</p>

<h2 id="footnotes">Footnotes</h2>
:ET