I"Ý<blockquote>
  <p>A note on CMA - Evolution Strategy
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#bsc-eqn">Basic equation</a></li>
  <li><a href="#upd-mean">Updating the mean</a></li>
  <li><a href="#adp-cov">Adapting the covariance matrix</a>
    <ul>
      <li><a href="#est-scratch">Estimating from scratch</a></li>
      <li><a href="#rank-mu-update">Rank-$\boldsymbol{\mu}$-update</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="bsc-eqn">Basic equation</h2>
<p>In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\ldots$
\begin{equation}
\mathbf{x}_k^{(t+1)}\sim\boldsymbol{\mu}^{(t)}+\sigma^{(t)}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})\sim\mathcal{N}\left(\boldsymbol{\mu}^{(t)},{\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}\right),\hspace{1cm}k=1,\ldots,\lambda\label{eq:be.1}
\end{equation}
where</p>
<ul>
  <li>$\mathbf{x}_k^{(t+1)}\in\mathbb{R}^n$: the $k$-th sample at generation $t+1$.</li>
  <li>$\boldsymbol{\mu}^{(t)}\in\mathbb{R}^n$: mean of the search distribution at generation $t$.</li>
  <li>$\sigma^{(t)}\in\mathbb{R}$: step-size at generation $t$.</li>
  <li>$\boldsymbol{\Sigma}^{(t)}$: covariance matrix at generation $t$.</li>
  <li>${\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$.</li>
  <li>$\lambda\geq 2$: sample size.</li>
</ul>

<h2 id="update-mean">Updating the mean</h2>
<p>The mean $\boldsymbol{\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\lambda_\boldsymbol{\mu}$ selected points from the sample $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$:
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\sum_{i=1}^{\lambda_\boldsymbol{\mu}}w_i\mathbf{x}_{i:\lambda}^{(t+1)},\label{eq:um.1}
\end{equation}
where</p>
<ul>
  <li>$\sum_{i=1}^{\lambda_\boldsymbol{\mu}}w_i=1$ with $w_1\geq w_2\geq\ldots\geq w_{\lambda_\boldsymbol{\mu}}&gt;0$.</li>
  <li>$\lambda_\boldsymbol{\mu}\leq\lambda$: number of selected points.</li>
  <li>$\mathbf{x}_{i:\lambda}^{(t+1)}$: $i$-th best sample out of $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$ from \eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have
\begin{equation}
f(\mathbf{x}_{1:\lambda}^{(t+1)})\geq f(\mathbf{x}_{2:\lambda}^{(t+1)})\geq\ldots\geq f(\mathbf{x}_{\lambda:\lambda}^{(t+1)})
\end{equation}
We can rewrite \eqref{eq:um.1} as an update rule for the mean $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\boldsymbol{\mu}^{(t)}+\alpha_\boldsymbol{\mu}\sum_{i=1}^{\lambda_\boldsymbol{\mu}}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right),
\end{equation}
where $\alpha_\boldsymbol{\mu}\leq 1$ is the learning rate, which is usually set to $1$.</li>
</ul>

<h2 id="adp-cov">Adapting the covariance matrix</h2>
<p>Rather than using the empirical covariance matrix as an estimator for $\boldsymbol{\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation
\begin{equation}
\boldsymbol{\Sigma}_\lambda^{(t+1)}=\frac{1}{\lambda{\sigma^{(t)}}^2}\sum_{i=1}^{\lambda}\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T}\label{eq:ac.1}
\end{equation}
Notice that in the above estimation \eqref{eq:ac.1}, we have used all of the $\lambda$ samples. We thus can estimate a better covariance by select some of the best individual out of $\lambda$ samples, which is analogous how we update the mean $\boldsymbol{\mu}$. In particular, we instead consider the estimation
\begin{equation}
\boldsymbol{\Sigma}_{\lambda_\boldsymbol{\Sigma}}^{(t+1)}
\end{equation}</p>

<h3 id="est-scratch">Estimating from scratch</h3>

<h3 id="rank-mu-update">Rank-$\boldsymbol{\mu}$-update</h3>

<h2 id="references">References</h2>
<p>[1] Nikolaus Hansen. <a href="#https://arxiv.org/abs/1604.00772">The CMA Evolution Strategy: A Tutorial</a>.</p>

<h2 id="footnotes">Footnotes</h2>
:ET