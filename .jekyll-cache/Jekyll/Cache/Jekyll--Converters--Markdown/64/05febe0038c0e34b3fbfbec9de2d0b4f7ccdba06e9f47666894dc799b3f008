I"<blockquote>
  <p>Recall that when using <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html">dynamic programming (DP) method</a> in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html">Monte Carlo methods</a> and <a href="/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html">temporal-difference learning</a>, the models are unnecessary. Such methods with requirement of a model like the case of DP is called <strong>model-based</strong>, while methods without using a model is called <strong>model-free</strong>. Model-based methods primarily rely on <strong>planning</strong>; and model-free methods, on the other hand, primarily rely on <strong>learning</strong>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#models-planning">Models &amp; Planning</a>
    <ul>
      <li><a href="#models">Models</a></li>
      <li><a href="#planning">Planning</a></li>
    </ul>
  </li>
  <li><a href="#prior-sweep">Prioritized Sweeping</a></li>
  <li><a href="#trajectory-sampling">Trajectory Sampling</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="models-planning">Models &amp; Planning</h2>

<h3 id="models">Models</h3>
<p>A <strong>model</strong> of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.</p>

<p>When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.</p>
<ul>
  <li>If a model produces a description of all possibilities and their probabilities, we call it <strong>distribution model</strong>. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.</li>
  <li>On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it <strong>sample model</strong>.</li>
</ul>

<p>Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to <strong>simulate</strong> the environment in order to produce <strong>simulated experience</strong>.</p>

<h3 id="planning">Planning</h3>
<p><strong>Planning</strong> in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:</p>
<ul>
  <li><strong>State-space planning</strong> is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:
    <ul>
      <li>Involving computing value functions as a key intermediate step toward improving the policy.</li>
      <li>Computing value functions by updates or backup applied to simulated experience.
  \begin{equation}
  \text{model}\xrightarrow[]{}\text{simulated experience}\xrightarrow[]{\text{backups}}\text{backups}\xrightarrow[]{}\text{policy}
  \end{equation}</li>
    </ul>
  </li>
  <li><strong>Plan-space planning</strong> is a search through the space of plans.</li>
</ul>

<h2 id="prior-sweep">Prioritized Sweeping</h2>

<h2 id="trajectory-sampling">Trajectory Sampling</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[2] David Silver &amp; Richard S. Sutton &amp; Martin Müller. <a href="https://doi.org/10.1145/1390156.1390278">Sample-Based Learning and Search with Permanent and Transient Memories</a>. ICML ‘08: Proceedings of the 25th international conference on Machine learning. July 2008.</p>

<p>[3]</p>

<h2 id="footnotes">Footnotes</h2>
:ET