I"qT<blockquote>

  <!-- excerpt-end -->
  <ul>
    <li><a href="#on-policy-methods">On-policy Methods</a>
      <ul>
        <li><a href="#value-func-approx">Value-function Approximation</a></li>
        <li><a href="#pred-obj">The Prediction Objective</a></li>
        <li><a href="#grad-algs">Gradient-based algorithms</a>
          <ul>
            <li><a href="#stochastic-grad">Stochastic-gradient</a></li>
            <li><a href="#on-policy-semi-grad">Semi-gradient</a></li>
          </ul>
        </li>
        <li><a href="#lin-func-approx">Linear Function Approximation</a>
          <ul>
            <li><a href="#lin-methods">Linear Methods</a></li>
            <li><a href="#feature-cons">Feature Construction</a>
              <ul>
                <li><a href="#polynomial">Polynomial Basis</a></li>
                <li><a href="#fourier">Fourier Basis</a>
                  <ul>
                    <li><a href="#uni-fourier-series">The Univariate Fourier Series</a></li>
                    <li><a href="#even-odd-non-periodic-func">Even, Odd and Non-Periodic Functions</a></li>
                    <li><a href="#mult-fourier-series">The Multivariate Fourier Series</a></li>
                  </ul>
                </li>
                <li><a href="#coarse-coding">Coarse Coding</a></li>
                <li><a href="#tile-coding">Tile Coding</a></li>
                <li><a href="#rbf">Radial Basis Functions</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#lstd">Least-Squares TD</a></li>
        <li><a href="#ep-semi-grad-control">Episodic Semi-gradient Control</a></li>
        <li><a href="#semi-grad-n-step-sarsa">Semi-gradient n-step Sarsa</a></li>
      </ul>
    </li>
    <li><a href="#off-policy-methods">Off-policy Methods</a>
      <ul>
        <li><a href="#off-policy-semi-grad">Semi-gradient</a></li>
        <li><a href="#grad-td">Gradient-TD</a></li>
        <li><a href="#em-td">Emphatic-TD</a></li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="on-policy-methods">On-policy Methods</h2>
<p>So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.</p>

<h3 id="value-func-approx">Value-function Approximation</h3>
<p>All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value” (or <em>update target</em>) for that state
\begin{equation}
s\mapsto u,
\end{equation}
where $s$ is the state updated and $u$ is the update target that $s$’s estimated value is shifted toward.</p>

<p>For example,</p>
<ul>
  <li>the MC update for value prediction is: $S_t\mapsto G_t$.</li>
  <li>the TD(0) update for value prediction is: $S_t\mapsto R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)$.</li>
  <li>the $n$-step TD update is: $S_t\mapsto G_{t:t+n}$.</li>
  <li>and in the DP, policy-evaluation update, $s\mapsto\mathbb{E}\big[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\vert S_t=s\big]$, an arbitrary $s$ is updated.</li>
</ul>

<p>Each update $s\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process <strong>function approximation</strong>.</p>

<h3 id="pred-obj">The Prediction Objective</h3>
<p>In constrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is imposible to find the exact value function of all states. And moreover, an update at one state also affects many others.</p>

<p>Hence, it is necessary to specify a state distribution $\mu(s)\geq0,\sum_s\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_\pi(s)$) in each state $s$. Weighting this over the state space $\mathcal{S}$ by $\mu$, we obtain a natural objective function, called the <em>Mean Squared Value Error</em>, denoted as $\overline{\text{VE}}$:
\begin{equation}
\overline{\text{VE}}(\mathbf{w})\doteq\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
The distribution $\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divived by total amount of visits). Under on-policy training this is called the <em>on-policy distribution</em>.</p>

<ul>
  <li>In continuing tasks, the on-policy distribution is the stationary distribution under $\pi$.</li>
  <li>In episodic tasks, the on-policy distribution depends on how the initial states are chosen.
    <ul>
      <li>Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode
  \begin{equation}
  \eta(s)=h(s)+\sum_\bar{s}\eta(\bar{s})\sum_a\pi(a\vert\bar{s})p(s\vert\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}
  This system of equation can be solved for the expected number of visits $\eta(s)$. The on-policy distribution is then
  \begin{equation}
  \mu(s)=\frac{\eta(s)}{\sum_{s’}\eta(s’)},\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}</li>
    </ul>
  </li>
</ul>

<h3 id="grad-algs">Gradient-based algorithms</h3>
<p>To solve the least squares problem, we are going to use a popular method, named <strong>Gradient descent</strong>.</p>

<p>Say, consider a differentiable function $J(\mathbf{w})$ of parameter vector $\mathbf{w}$.</p>

<p>The gradient of $J(\mathbf{w})$ w.r.t $\mathbf{w}$ is defined to be
\begin{equation}
\nabla_{\mathbf{w}}J(\mathbf{w})=\left(\begin{smallmatrix}\dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_1} \\ \vdots \\ \dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_d}\end{smallmatrix}\right)
\end{equation}
The idea of Gradient descent is to minimize the objective function $J(\mathbf{w})$, we repeatly move $\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\nabla_\mathbf{w}J(\mathbf{w})$.</p>

<p>Thus, we have the update rule of Gradient descent:
\begin{equation}
\mathbf{w}\leftarrow\mathbf{w}-\dfrac{1}{2}\alpha\nabla_\mathbf{w}J(\mathbf{w}),
\end{equation}
where $\alpha$ is a positive step-size parameter.</p>

<h4 id="stochastic-grad">Stochastic-gradient</h4>
<p>Apply gradient descent to our problem, which is we have to find the minimization of
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
Since $\mu(s)$ is the state distribution over state space $\mathcal{S}$, we can rewrite $\overline{\text{VE}}$ as
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\mathbb{E}_{s\sim\mu}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
By the update we have defined earlier, in each step, we need to decrease $\mathbf{w}$ by an amount of
\begin{equation}
\Delta\mathbf{w}=-\dfrac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{VE}}(\mathbf{w})=\alpha\mathbb{E}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})
\end{equation}
Using <strong>Stochastic Gradient descent (SGD)</strong>, and since the Monte Carlo target $G_t$ by definition is an unbiased estimate of $v_\pi(S_t)$ , we sample the gradient:
\begin{equation}
\Delta\mathbf{w}=\alpha(G_t-\hat{v}(S_t,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w})
\end{equation}
which gives us pseudocode of the algorithm:</p>
<figure>
	<img src="/assets/images/2022-07-10/sgd_mc.png" alt="SGD Monte Carlo" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="on-policy-semi-grad">Semi-gradient</h4>
<p>If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\sum_{a,s’,r}\pi(a\vert S_t)p(s’,r\vert S_t,a)\left[r+\gamma\hat{v}(s’,\mathbf{w}_t)\right]$, which all depend on the current value of the weight vector $\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.</p>

<p>Such methods are called <strong>semi-gradient</strong> since they include only a part of the gradient.</p>
<figure>
	<img src="/assets/images/2022-07-10/semi_gd.png" alt="Semi-gradient TD(0)" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="lin-func-approx">Linear Function Approximation</h3>
<p>One of the most crucial special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot,\mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$.</p>

<p>Corresponding to every state $s$, there is a real-valued vector $\mathbf{x}(s)\doteq\left(x_1(s),x_2(s),\dots,x_d(s)\right)^\intercal$, with the same number of components with $\mathbf{w}$.</p>

<h4 id="lin-methods">Linear Methods</h4>
<p>Linear methods approximate value function by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:
\begin{equation}
\hat{v}(s,\mathbf{w})\doteq\mathbf{w}^\intercal\mathbf{x}(s)=\sum_{i=1}^{d}w_ix_i(s)\tag{1}\label{1}
\end{equation}
The vector $\mathbf{x}(s)$ is called a <em>feature vector</em> representing state $s$, i.e., $x_i:\mathcal{S}\to\mathbb{R}$.</p>

<p>For linear methods, features are <em>basis functions</em> because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.</p>

<p>From \eqref{1}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})=\mathbf{x}(s)
\end{equation}
Thus, with linear approximation, the SGD update can be rewrite as
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t-\hat{v}(S_t,\mathbf{w}_t)\right]\mathbf{x}(S_t)
\end{equation}</p>

<p>In the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.</p>
<ul>
  <li>The gradient MC algorithm in the previous section converges to the global optimum of the $\overline{\text{VE}}$ under linear function approximation if $\alpha$ is reduced over time according to the usual conditions.</li>
  <li>The semi-gradient TD algorithm also converges under linear approximation.
    <ul>
      <li>Recall that, at each time $t$, the semi-gradient TD update is
  \begin{align}
  \mathbf{w}_{t+1}&amp;\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;=\mathbf{w}_t+\alpha\left(R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\intercal\mathbf{w}_t\right),
  \end{align}
  where $\mathbf{x}_t=\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\mathbf{w}_t$, the expected next weight vector can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\mathbf{w}_t+\alpha\left(\mathbf{b}-\mathbf{A}\mathbf{w}_t\right),\tag{2}\label{2}
  \end{equation}
  where
  \begin{align}
  \mathbf{b}&amp;\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]\in\mathbb{R}^d, \\ \mathbf{A}&amp;\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\right]\in\mathbb{R}^d\times\mathbb{R}^d\tag{3}\label{3}
  \end{align}
  From \eqref{2}, it is easily seen that if the system converges, it must converges to the weight vector $\mathbf{w}_{\text{TD}}$ at which
  \begin{align}
  \mathbf{b}-\mathbf{A}\mathbf{w}_{\text{TD}}&amp;=\mathbf{0} \\ \mathbf{w}_{\text{TD}}&amp;=\mathbf{A}^{-1}\mathbf{b}
  \end{align}
  This quantity, $\mathbf{w}_{\text{TD}}$, is called the <em>TD fixed point</em>. And in fact, linear semi-gradient TD(0) converges to this point.</li>
      <li><strong>Proof</strong>:<br />
  We have \eqref{2} can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\left(\mathbf{I}-\alpha\mathbf{A}\right)\mathbf{w}_t+\alpha\mathbf{b}
  \end{equation}
  The idea of the proof is prove that the matrix $\mathbf{A}$ in \eqref{3} is a positive definite matrix<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, since $\mathbf{w}_t$ will be reduced toward zero whenever $\mathbf{A}$ is positive definite.<br />
  For linear TD(0), in the continuing case with $\gamma&lt;1$, the matrix $\mathbf{A}$ can be written as
  \begin{align}
  \mathbf{A}&amp;=\sum_s\mu(s)\sum_a\pi(a\vert s)\sum_{r,s’}p(r,s’\vert s,a)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;=\sum_s\mu(s)\sum_{s’}p(s’\vert s)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;=\sum_s\mu(s)\mathbf{x}(s)\Big(\mathbf{x}(s)-\gamma\sum_{s’}p(s’\vert s)\mathbf{x}(s’)\Big)^\intercal \\ &amp;=\mathbf{X}^\intercal\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})\mathbf{X},\tag{4}\label{4}
  \end{align}
  where
        <ul>
          <li>$\mu(s)$ is the stationary distribution under $\pi$;</li>
          <li>$p(s’\vert s)$ is the probability transition from $s$ to $s’$ under policy $\pi$;</li>
          <li>$\mathbf{P}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of these probabilities;</li>
          <li>$\mathbf{D}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on its diagonal;</li>
          <li>$\mathbf{X}$ is the $\vert\mathcal{S}\vert\times d$ matrix with $\mathbf{x}(s)$ as its row.</li>
        </ul>

        <p>Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ in \eqref{4}.</p>

        <p>To continue proving the posititive definiteness of $\mathbf{A}$, we use two lemmas:</p>
        <ul>
          <li><strong>Lemma 1</strong>: <em>A square matrix $\mathbf{A}$ is positive definite if $\mathbf{A}+\mathbf{A}^\intercal$</em> is positive definite.</li>
          <li><strong>Lemma 2</strong>: <em>If $\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\mathbf{A}$ is positive definite</em>.</li>
        </ul>

        <p>With these lemmas, plus since $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\mathbf{P}$ is a stochastic matrix and $\gamma&lt;1$. Thus the problem remains to show that the column sums are nonnegative.</p>

        <p>Let $\mathbf{1}$ denote the column vector with all components equal to $1$ and $\boldsymbol{\mu}(s)$ denote the vectorized version of $\mu(s)$: i.e., $\boldsymbol{\mu}\in\mathbb{R}^{\vert\mathcal{S}\vert}$. Thus, $\boldsymbol{\mu}=\mathbf{P}^\intercal\boldsymbol{\mu}$ since $\mu(s)$ is the stationary distribution. We have:
  \begin{align}
  \mathbf{1}^\intercal\mathbf{D}\left(\mathbf{I}-\gamma\mathbf{P}\right)&amp;=\boldsymbol{\mu}^\intercal\left(\mathbf{I}-\gamma\mathbf{P}\right) \\ &amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal\mathbf{P} \\ &amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal \\ &amp;=\left(1-\gamma\right)\boldsymbol{\mu}^\intercal,
  \end{align}
  which implies that the column sums of $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ are positive.</p>
      </li>
      <li>At the TD fixed point, it has also been proven (in the continuing case) that $\overline{\text{VE}}$ is within a bounded expansion of the lowest possible error
  \begin{equation}
  \overline{\text{VE}}(\mathbf{w}_{\text{TD}})\leq\dfrac{1}{1-\gamma}\min_{\mathbf{w}}\overline{\text{VE}}(\mathbf{w})
  \end{equation}</li>
    </ul>
  </li>
</ul>

<h4 id="feature-cons">Feature Construction</h4>
<p>There are various ways to define features. The simpliest way is to use each variable directly as a basis function along with a constant function. However, most interesting value functions are too complex to be represented in this way. This scheme therefore was generalized into polynomial</p>

<h5 id="polynomial">Polynomial Basis</h5>
<p>Suppose each</p>

<h5 id="fourier">Fourier Basis</h5>

<h6 id="uni-fourier-series">The Univariate Fourier Series</h6>
<p><strong>Fourier series</strong> is applied widely in Maths to approximate a periodic function<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. For example:</p>

<figure>
	<img src="/assets/images/2022-07-10/fourier_series.gif" alt="Fourier series visualization" width="480" height="360px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\theta)=\frac{4\sin\theta}{\pi},f_2(\theta)=\frac{4\sin3\theta}{3\pi},f_3(\theta)=\frac{4\sin5\theta}{5\pi}$ and $f_4(\theta)=\frac{4\sin7\theta}{7\pi}$. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/fourier-series/fourier_series.py">here</a></span></figcaption>
</figure>
<p><br />
In particular, the $n$-degree Fourier expansion of $f$ is
\begin{equation}
\bar{f}(x)=\dfrac{a_0}{2}+\sum_{k=1}^{n}\left[a_k\cos\left(k\frac{2\pi}{\tau}x\right)+b_k\left(k\frac{2\pi}{\tau}x\right)\right],
\end{equation}
where
\begin{align}
a_k&amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\cos\left(\frac{2\pi kx}{\tau}\right)\,dx, \\ b_k&amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi kx}{\tau}\right)\,dx
\end{align}
In the RL setting, $f$ is unknown so we cannot compute $a_0,\dots,a_n$ and $b_1,\dots,b_n$, but we can instead treat them as parameters in a linear funciton approximation scheme, with
\begin{equation}
\phi_i(x)=\begin{cases}1 &amp;\text{if }i=0 \\ \cos\left(\frac{(i+1)\pi x}{\tau}\right) &amp;\text{if }i&gt;0,i\text{ odd} \\ \sin\left(\frac{i\pi x}{\tau}\right) &amp;\text{if }i&gt;0,i\text{ even}\end{cases}
\end{equation}
Thus, a full $n$-th order Fourier approximation to a one-dimensional value function results in a linear function approximation with $2n+1$ terms.</p>

<h6 id="even-odd-non-periodic-func">Even, Odd and Non-Periodic Functions</h6>
<p>If $f$ is known to be even (i.e., $f(x)=f(-x)$), then $\forall i&gt;0$, we have:
\begin{align}
b_i&amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx \\ &amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx\right] \\ &amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi ix}{\tau}-2\pi i\right)\,dx\right] \\ &amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi i(x-\tau)}{\tau}\right)\,dx\right] \\ &amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{-\tau/2}^{0}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx\right] \\ &amp;=0,
\end{align}
so the $\sin$ terms can be dropped, which reduces the terms required for an $n$-th order Fourier approximation to $n+1$.</p>

<h6 id="mult-fourier-series">The Multivariate Fourier Series</h6>

<h5 id="coarse-coding">Coarse Coding</h5>

<h5 id="tile-coding">Tile Coding</h5>

<h5 id="rbf">Radial Basis Functions</h5>

<h3 id="lstd">Least-Squares TD</h3>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] Deepmind x UCL. <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021">Reinforcement Learning Lecture Series 2021</a></p>

<p>[3] Sutton, R. S. (1988). <a href="doi:10.1007/bf00115009">Learning to predict by the methods of temporal differences</a>. Machine Learning, 3(1), 9–44.</p>

<p>[4] Konidaris, G. &amp; Osentoski, S. &amp; Thomas, P.. <a href="#https://dl.acm.org/doi/10.5555/2900423.2900483">Value Function Approximation in Reinforcement Learning Using the Fourier Basis</a>. AAAI Conference on Artificial Intelligence, North America, aug. 2011.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A $n\times n$ matrix $A$ is called <em>positive definite</em> if and only if for any non-zero vector $\mathbf{x}\in\mathbb{R}^n$, we always have
\begin{equation}
\mathbf{x}^\intercal\mathbf{A}\mathbf{x}&gt;0
\end{equation} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A function $f$ is periodic with period $\tau$ if
\begin{equation}
f(x+\tau)=f(x),\forall x
\end{equation} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET