I"ÿE<blockquote>
  <p>Recall that in the previous post, <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">Infinite Series of Constants</a>, we mentioned a type of series called <strong>power series</strong> a lot. In the content of this post, we will be diving deeper into details of that series.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#power-series">Power Series</a></li>
  <li><a href="#int-conv">The Interval of Convergence</a>
    <ul>
      <li><a href="#eg1">Example</a></li>
    </ul>
  </li>
  <li><a href="#dif-int-power-series">Differentiation and Integration of Power Series</a>
    <ul>
      <li><a href="#dif-power-series">Differentiation of Power Series</a></li>
      <li><a href="#int-power-series">Integration of Power Series</a></li>
      <li><a href="#eg2">Example</a></li>
    </ul>
  </li>
  <li><a href="#taylor-series-formula">Taylor Series, Taylorâ€™s Formula</a>
    <ul>
      <li><a href="#taylor-series">Taylor Series</a></li>
      <li><a href="#taylors-formula">Taylorâ€™s Formula</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="power-series">Power Series</h2>
<p>A <strong>power series</strong> is a series of the form
\begin{equation}
\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots,
\end{equation}
where the coefficient $a_n$ are constants and $x$ is a variable.</p>

<h2 id="int-conv">The Interval of Convergence</h2>
<p>Similar to what we have done in the post of <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">infinite series of constants</a>, we begin studying properties of power series by considering their convergence behavior.</p>

<p><strong>Lemma 1</strong><br />
<em>If a power series $\sum a_nx^n$ converges at $x_1$, with $x_1\neq 0$, then it converges <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-conv">absolutely</a> at all $x$ with $\vert x\vert&lt;\vert x_1\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\vert x\vert&gt;\vert x_1\vert$.</em></p>

<p><strong>Proof</strong><br />
By the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#nth-term-test">$n$-th term test</a>, we have that if $\sum a_nx^n$ converges, then $a_nx^n\to 0$. In particular, if $n$ is sufficiently large, then $\vert a_n{x_1}^n\vert&lt;1$, and therefore
\begin{equation}
\vert a_nx^n\vert=\vert a_n{x_1}^n\vert\left\vert\dfrac{x}{x_1}\right\vert^n&lt;r^n,\tag{1}\label{1}
\end{equation}
where $r=\vert\frac{x}{x_1}\vert$. Suppose that $\vert x\vert&lt;\vert x_1\vert$, we have
\begin{equation}
r=\left\vert\dfrac{x}{x_1}\right\vert&lt;1,
\end{equation}
which leads to the result that geometric series $\sum r^n$ converges (with the sum $\frac{1}{1-r}$). And hence, from \eqref{1} and by the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, the series $\sum\vert a_nx^n\vert$ also converges.</p>

<p>Moreover, if $\sum a_n{x_1}^n$ diverges, then $\sum\vert a_n{x_1}^n\vert$ also diverges. By the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, for any $x$ such that $\vert x\vert&gt;\vert x_1\vert$, we also have that $\sum\vert a_nx^n\vert$ diverges. This leads to the divergence of $\sum a_nx^n$, because if the series $\sum a_nx^n$ converges, so does $\sum\vert a_nx^n\vert$, which contradicts to our result.</p>

<p>These are some main facts about the convergence behavior of an arbitrary power series and some properties of its:</p>
<ul>
  <li>Given a power series $\sum a_nx^n$, precisely one of the following is true:
    <ul>
      <li>The series converges only for $x=0$.</li>
      <li>The series is absolutely convergent for all $x$.</li>
      <li>There exists a positive real number $R$ such that the series is absolutely convergent for $\vert x\vert&lt;R$ and divergent for $\vert x\vert&gt;R$.</li>
    </ul>
  </li>
  <li>The positive real number $R$ is called <strong>radius of convergence</strong> of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$.</li>
  <li>The set of all $x$â€™s for which a power series converges is called its <strong>interval of convergence</strong>.</li>
  <li>When the series converges only for $x=0$, we define $R=0$; and we define $R=\infty$ when the series converges for all $x$.</li>
  <li>Every power series $\sum a_nx^n$ has a radius of convergence $R$, where $0\leq R\leq\infty$, with the property that the series converges absolutely if $\vert x\vert&lt;R$ and diverges if $\vert x\vert&gt;R$.</li>
</ul>

<h3 id="eg1">Example</h3>
<p>Find the interval of convergence of the series
\begin{equation}
\sum_{n=0}^{\infty}\dfrac{x^n}{n+1}=1+\dfrac{x}{2}+\dfrac{x^2}{3}+\ldots
\end{equation}</p>

<p><strong>Solution</strong><br />
In order to find the interval of convergence of a series, we begin by identifying its radius of convergence.</p>

<p>Consider a power series $\sum a_nx^n$. Suppose that this limit exists, and has $\infty$ as an allowed value, we have
\begin{equation}
\lim_{n\to\infty}\dfrac{\vert a_{n+1}x^{n+1}\vert}{a_nx^n}=\lim_{n\to\infty}\left\vert\dfrac{a_{n+1}}{a_n}\right\vert.\vert x\vert=\dfrac{\vert x\vert}{\lim_{n\to\infty}\left\vert\frac{a_n}{a_{n+1}}\right\vert}=L
\end{equation}
By the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#ratio-test">ratio test</a>, we have $\sum a_nx^n$ converges absolutely if $L&lt;1$ and diverges in case of $L&gt;1$. Or in other words, the series converges absolutely if
\begin{equation}
\vert x\vert&lt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert,
\end{equation}
or diverges if
\begin{equation}
\vert x\vert&gt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}
From the definition of radius of convergence, we can choose the radius of converge of $\sum a_nx^n$ as
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}</p>

<p>Back to our problem, for the series $\sum\frac{x^n}{n+1}$, we have its radius of convergence is
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert=\lim_{n\to\infty}\dfrac{\frac{1}{n+1}}{\frac{1}{n+2}}=\lim_{n\to\infty}\dfrac{n+2}{n+1}=1
\end{equation}
At $x=1$, the series becomes the <em>harmonic series</em> $1+\frac{1}{2}+\frac{1}{3}+\ldots$, which diverges; and at $x=-1$, it is the <em>alternating harmonic series</em> $1-\frac{1}{2}+\frac{1}{3}-\ldots$, which converges. Hence, the interval of convergence of the series is $[-1,1)$.</p>

<h2 id="dif-int-power-series">Differentiation and Integration of Power Series</h2>

<p>It is easily seen that the sum of the series $\sum_{n=0}^{\infty}a_nx^n$  is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots\tag{2}\label{2}
\end{equation}
This relation between the series and the function is also expressed by saying that $\sum a_nx^n$ is a <strong>power series expansion</strong> of $f(x)$.</p>

<p>These are some crucial facts about that relation.</p>
<ul>
  <li>(i) The function $f(x)$ defined by \eqref{2} is continuous on the open interval $(-R,R)$.</li>
  <li>(ii) The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula
\begin{equation}
fâ€™(x)=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots\tag{3}\label{3}
\end{equation}</li>
  <li>(iii) If $x$ is any point in $(-R,R)$, then
\begin{equation}
\int_{0}^{x}f(t)\,dt=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{4}\label{4}
\end{equation}</li>
</ul>

<p><strong>Remark</strong><br />
We have that series \eqref{3} and \eqref{4} converge on the interval $(-R,R)$.</p>

<p><strong>Proof</strong></p>
<ol>
  <li>We begin by proving the convergence on $(-R,R)$ of \eqref{3}.<br />
Let $x$ be a point in the interval $(-R,R)$ and choose $\epsilon&gt;0$ so that $\vert x\vert+\epsilon&lt;R$. Since $\vert x\vert+\epsilon$ is in the interval, $\sum\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert$ converges.<br />
We continue by proving the inequality
\begin{equation}
\vert nx^{n-1}\vert\leq\left(\vert x\vert+\epsilon\right)^n\hspace{1cm}\forall n\geq n_0,
\end{equation}
where $\epsilon&gt;0$, $n_0$ is a positive integer.<br />
We have
\begin{align}
\lim_{n\to\infty}n^{1/n}&amp;=\lim_{n\to\infty} \\ &amp;=\lim_{n\to\infty}\exp\left(\frac{\ln n}{n}\right) \\ &amp;=\exp\left(\lim_{n\to\infty}\frac{\ln n}{n}\right) \\ &amp;={\rm e}^0=1,
\end{align}
where in the fourth step, we use the <em>Lâ€™Hospital theorem</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Therefore, as $n\to\infty$
\begin{equation}
n^{1/n}\vert x\vert^{1-1/n}\to\vert x\vert
\end{equation}
Then for all sufficiently large $n$â€™s
\begin{align}
n^{1/n}\vert x\vert^{1-1/n}&amp;\leq\vert x\vert+\epsilon \\ \vert nx^{n-1}\vert&amp;\leq\left(\vert x\vert+\epsilon\right)^n
\end{align}
This implies that
\begin{equation}
\vert na_nx^{n-1}\vert\leq\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert
\end{equation}
By the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, we have that the series $\sum\vert na_nx^{n-1}\vert$ converges, and so does $\sum na_nx^{n-1}$.</li>
  <li>Since $\sum\vert a_nx^n\vert$ converges and
\begin{equation}
\left\vert\dfrac{a_nx^n}{n+1}\right\vert\leq\vert a_nx^n\vert,
\end{equation}
the <a href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a> implies that $\sum\left\vert\frac{a_nx^n}{n+1}\right\vert$ converges, and therefore
\begin{equation}
x\sum\frac{a_nx^n}{n+1}=\sum\frac{1}{n+1}a_nx^{n+1}
\end{equation}
also converges.</li>
</ol>

<h3 id="dif-power-series">Differentiation of Power Series</h3>

<p>If we instead apply (ii) to the function $fâ€™(x)$ in \eqref{3}, then it follows that $fâ€™(x)$ is also differentiable. Doing the exact same process to $fâ€™'(x)$, we also have that $fâ€™'(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:</p>

<p><em>In the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term</em>.
\begin{equation}
\dfrac{d}{dx}\left(\sum a_nx^n\right)=\sum\dfrac{d}{dx}(a_nx^n)
\end{equation}</p>

<h3 id="int-power-series">Integration of Power Series</h3>

<p>Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \eqref{4} as
\begin{equation}
\int\left(\sum a_nx^n\right)\,dx=\sum\left(\int a_nx^n\,dx\right)
\end{equation}</p>

<h3 id="eg2">Example</h3>

<p>Find a power series expansion of ${\rm e}^x$.</p>

<p><strong>Solution</strong><br />
Since ${\rm e}^x$ is the only function that equals its own derivatives<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We therefore want each term to be the derivative of the one that follows it.</p>

<p>Starting with $1$ as the constant term, the next should be $x$, then $\frac{1}{2}x^2$, then $\frac{1}{2.3}x^3$, and so on. This produces the series
\begin{equation}
1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots,\tag{5}\label{5}
\end{equation}
which converges for all $x$ because
\begin{equation}
R=\lim_{n\to\infty}\dfrac{\frac{1}{n!}}{\frac{1}{(n+1)!}}=\lim_{n\to\infty}(n+1)=\infty
\end{equation}
We have constructed the series \eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$,
\begin{equation}
{\rm e}^x=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots
\end{equation}</p>

<h2 id="taylor-series-formula">Taylor Series, Taylorâ€™s Formula</h2>

<h3 id="taylor-series">Taylor Series</h3>
<p>Assume that $f(x)$ is the sum of a power series with positive radius of convergence
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots,\hspace{1cm}R&gt;0\tag{6}\label{6}
\end{equation}
By the results obtained from previous section, differentiating \eqref{6} term by term we have
\begin{align}
f^{(1)}(x)&amp;=a_1+2a_2x+3a_3x^2+\ldots \\ f^{(2)}(x)&amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\ldots \\ f^{(3)}(x)&amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\ldots
\end{align}
and in general,
\begin{equation}
f^{(n)}(x)=n!a_n+A(x),\tag{7}\label{7}
\end{equation}
where $A(x)$ contains $x$ as a factor.</p>

<p>Since these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \eqref{7} we obtain
\begin{equation}
f^{(n)}(0)=n!a_n
\end{equation}
so
\begin{equation}
a_n=\dfrac{f^{(n)}(0)}{n!}
\end{equation}
Putting this result in \eqref{6}, our series becomes
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\ldots\tag{8}\label{8}
\end{equation}
This power series is called <strong>Taylor series</strong> of $f(x)$ [at $x=0$], which is named after the person who introduced it, Brook Taylor.</p>

<p>If we use the convention that $0!=1$, then \eqref{8} can be written as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(0)}{n!}x^n
\end{equation}
The numbers $a_n=\frac{f^{(n)}(0)}{n!}$ are called the <strong>Taylor coefficients</strong> of $f(x)$.</p>

<p><strong>Remark</strong><br />
Given a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?<br />
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\ldots
\end{equation}
Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is
\begin{align}
f(x)&amp;=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\ldots
\end{align}</p>

<h3 id="taylors-formula">Taylorâ€™s Formula</h3>
<p>If we break off the Taylor series on the right side of \eqref{8} at the term containing $x^n$ and define the <em>remainder</em> $R_n(x)$ by the equation
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\tag{9}\label{9}
\end{equation}
then the Taylor series on the right side of \eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when
\begin{equation}
\lim_{n\to\infty}R_n(x)=0
\end{equation}
Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing
\begin{equation}
R_n(x)=S_n(x)x^{n+1}
\end{equation}
for $x\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for</p>

<h2 id="references">References</h2>
<p>[1] George F.Simmons. <a href="https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424">Calculus With Analytic Geometry - 2nd Edition</a></p>

<p>[2] Marian M. <a href="https://www.springer.com/gp/book/9780387789323">A Concrete Approach to Classical Analysis</a></p>

<p>[3] MIT 18.01. <a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/">Single Variable Calculus</a></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><strong>Theorem</strong> (<em>Lâ€™Hospital</em>)<br />
<em>Assume $f$ and $g$ are real and differentiable on $]a,b[$ and $gâ€™(x)\neq 0$ for all $x\in]a,b[$, where $-\infty\leq a&lt;b\leq\infty$. Suppose as $x\to a$,
\begin{equation}
\dfrac{fâ€™(x)}{gâ€™(x)}\to A\,(\in[-\infty,\infty])
\end{equation}
If as $x\to a$, $f(x)\to 0$ and $g(x)\to 0$ or if $g(x)\to+\infty$ as $x\to a$, then
\begin{equation}
\dfrac{f(x)}{g(x)}\to A
\end{equation}
as $x\to a$.</em>Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><strong>Proof</strong><br />
Consider the function $f(x)=a^x$.<br />
Using the definition of the derivative, we have
\begin{align}
\dfrac{d}{dx}f(x)&amp;=\lim_{h\to0}\dfrac{f(x+h)-f(x)}{h} \\ &amp;=\lim_{h\to0}\dfrac{a^{x+h}-a^x}{h} \\ &amp;=a^x\lim_{h\to0}\dfrac{a^h-1}{h}
\end{align}
Therefore,
\begin{equation}
\lim_{h\to0}\dfrac{a^h-1}{h}=1
\end{equation}
then, let $n=\frac{1}{h}$, we have
\begin{equation}
a=\lim_{h\to0}\left(1+\dfrac{1}{h}\right)^{1/h}=\lim_{n\to\infty}\left(1+\dfrac{1}{n}\right)^n={\rm e}
\end{equation}
Thus, $f(x)=a^x={\rm e}^x$. Every function $y=c{\rm e}^x$ also satisfies the differential equation $\frac{dy}{dx}=y$, because
\begin{equation}
\dfrac{dy}{dx}=\dfrac{d}{dx}c{\rm e}^x=c\dfrac{d}{dx}{\rm e}^x=c{\rm e}^x=y
\end{equation}<br />
The rest of our proof is to prove that these are only functions that are unchanged by differentiation.<br />
To prove this, suppose $f(x)$ is any function with that property. By the quotient rule,
\begin{equation}
\dfrac{d}{dx}\dfrac{f(x)}{e^x}=\dfrac{fâ€™(x)e^x-e^xf(x)}{e^{2x}}=\dfrac{e^xf(x)-e^xf(x)}{e^{2x}}=0
\end{equation}
which implies that
\begin{equation}
\dfrac{f(x)}{e^x}=c,
\end{equation}
for some constant $c$, and so $f(x)=ce^x$.Â <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET