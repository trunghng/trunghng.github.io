I"‹'<blockquote>
  <p>Recall that when using <a href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html">dynamic programming (DP) method</a> in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with <a href="/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html">Monte Carlo methods</a> and <a href="/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html">temporal-difference learning</a>, the models are unnecessary. Such methods with requirement of a model like the case of DP is called <strong>model-based</strong>, while methods without using a model is called <strong>model-free</strong>. Model-based methods primarily rely on <strong>planning</strong>; and model-free methods, on the other hand, primarily rely on <strong>learning</strong>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#models-planning">Models &amp; Planning</a>
    <ul>
      <li><a href="#models">Models</a></li>
      <li><a href="#planning">Planning</a></li>
    </ul>
  </li>
  <li><a href="#dyna">Dyna</a>
    <ul>
      <li><a href="#dyna-q">Dyna-Q</a>
        <ul>
          <li><a href="#dyna-q-eg">Example</a></li>
        </ul>
      </li>
      <li><a href="#dyna-q-plus">Dyna-Q+</a></li>
      <li><a href="#dyna-2">Dyna-2</a></li>
    </ul>
  </li>
  <li><a href="#prior-sweep">Prioritized Sweeping</a></li>
  <li><a href="#trajectory-sampling">Trajectory Sampling</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="models-planning">Models &amp; Planning</h2>

<h3 id="models">Models</h3>
<p>A <strong>model</strong> of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.</p>

<p>When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.</p>
<ul>
  <li>If a model produces a description of all possibilities and their probabilities, we call it <strong>distribution model</strong>. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.</li>
  <li>On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it <strong>sample model</strong>.</li>
</ul>

<p>Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to <strong>simulate</strong> the environment in order to produce <strong>simulated experience</strong>.</p>

<h3 id="planning">Planning</h3>
<p><strong>Planning</strong> in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:</p>
<ul>
  <li><strong>State-space planning</strong> is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:
    <ul>
      <li>Involving computing value functions as a key intermediate step toward improving the policy.</li>
      <li>Computing value functions by updates or backup applied to simulated experience.
  \begin{equation}
  \text{model}\xrightarrow[]{\hspace{1.5cm}}\text{simulated experience}\xrightarrow[]{\hspace{0.3cm}\text{backups}\hspace{0.3cm}}\text{backups}\xrightarrow[]{\hspace{1.5cm}}\text{policy}
  \end{equation}</li>
    </ul>
  </li>
  <li><strong>Plan-space planning</strong> is a search through the space of plans.
    <ul>
      <li>Plan-space planning methods consist of <strong>evolutionary methods</strong> and <strong>partial-order planning</strong>, in which the ordering of steps is not completely determined at all states of planning.</li>
    </ul>
  </li>
</ul>

<p>Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.</p>

<p>For instance, following is pseudocode of a planning method, called <strong>random-sample one-step tabular Q-planning</strong>, based on <a href="/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#q-learning">one-step tabular Q-learning</a>, and on random samples from a sample model.</p>
<figure>
	<img src="/assets/images/2022-05-19/rand-samp-one-step-q-planning.png" alt="Random-sample one-step Q-planning" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="dyna">Dyna</h2>
<p>Within a planning agent, experience plays at least two roles:</p>
<ul>
  <li><strong>model learning</strong>: improving the model;</li>
  <li><strong>direct reinforcement learning (RL)</strong>: improving the value function and policy</li>
</ul>

<p>The figure below illustrates the possible relationships between experience, model, value functions and policy.</p>
<figure>
    <img src="/assets/images/2022-05-19/exp-model-value-policy.png" alt="Exp, model, values and policy relationships" style="display: block; margin-left: auto; margin-right: auto; width: 300px; height: 250px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: The possible relationships between experience, model, values and policy (taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>

<p>Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called <strong>indirect RL</strong>), which involved in planning.</p>
<ul>
  <li>direct RL: simpler, not affected by bad models;</li>
  <li>indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions.</li>
</ul>

<h3 id="dyna-q">Dyna-Q</h3>
<p><strong>Dyna-Q</strong> is the method having all of the processes shown in the diagram in <strong><em>Figure 1</em></strong> - planning, acting, model-learning and direct RL - all occurring continually:</p>
<ul>
  <li>the <em>planning</em> method is the random-sample one-step tabular Q-planning in the previous section;</li>
  <li>the <em>direct RL</em> method is the one-step tabular Q-learning;</li>
  <li>the <em>model-learning</em> method is also table-based and assumes the environment is deterministic.</li>
</ul>

<p>After each transition $S_t,A_t\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.</p>

<p>During planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.</p>

<p>Following is the general architecture of Dyna methods, of which Dyna-Q is an instance.</p>
<figure>
    <img src="/assets/images/2022-05-19/dyna-arch.png" alt="Dyna architecture" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 320px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The general Dyna Architecture (taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>

<p>In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.</p>

<p>Pseudocode of Dyna-Q method is shown below.</p>
<figure>
	<img src="/assets/images/2022-05-19/tabular-dyna-q.png" alt="Tabular Dyna-Q" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="dyna-q-eg">Example</h4>
<p>(This example is taken from <a href="#rl-book">RL book</a> - example 8.1.)
Consider a gridworld with some obstacles, called ‚Äúmaze‚Äù in this example, shown in the figure below.</p>
<figure>
	<img src="/assets/images/2022-05-19/dyna-maze.png" alt="Dyna maze" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 200px" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
<p>As usual, four action, $\text{up}, \text{down}, \text{right}$ and $\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Each transition to a non-goal state will give a reward of zero, while moving to the goal state will reward $+1$.</p>

<h3 id="dyna-q-plus">Dyna-Q+</h3>

<h3 id="dyna-2">Dyna-2</h3>

<h2 id="prior-sweep">Prioritized Sweeping</h2>

<h2 id="trajectory-sampling">Trajectory Sampling</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] <span id="rl-book">Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></span>.</p>

<p>[2] David Silver &amp; Richard S. Sutton &amp; Martin M√ºller. <a href="https://doi.org/10.1145/1390156.1390278">Sample-Based Learning and Search with Permanent and Transient Memories</a>. ICML ‚Äò08: Proceedings of the 25th international conference on Machine learning. July 2008.</p>

<p>[3]</p>

<h2 id="footnotes">Footnotes</h2>
:ET