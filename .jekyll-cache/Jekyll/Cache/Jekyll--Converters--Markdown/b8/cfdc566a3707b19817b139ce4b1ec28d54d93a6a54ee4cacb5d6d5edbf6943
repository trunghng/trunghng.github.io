I"è<blockquote>
  <p>Since its first appearance in 2016, <strong>AlphaGo</strong> had continued. <strong>Monte Carlo Tree Search (MCTS)</strong> is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#background">Background</a>
    <ul>
      <li><a href="#bandit-based-method">Bandit-based Methods</a></li>
    </ul>
  </li>
  <li><a href="#mcts">Monte Carlo Tree Search</a>
    <ul>
      <li><a href="#algorithm">Algorithm</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="background">Background</h2>

<h3 id="bandit-based-method">Bandit-based Methods<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></h3>

<h2 id="references">References</h2>
<p>[1] C. B. Browne et al., <a href="https://ieeexplore.ieee.org/document/6145622">A Survey of Monte Carlo Tree Search Methods</a>, in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012, doi: 10.1109/TCIAIG.2012.2186810.</p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[3]</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>For more detailed about Multi Armed Bandit problem, we will save it for another post since this one is gonna be a long post.Â <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET