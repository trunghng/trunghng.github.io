I"x<blockquote>
  <p>A note on Inverse Reinforcement Learning</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#mdp">Markov decision processes</a></li>
      <li><a href="#irl">Inverse reinforcement learning</a></li>
    </ul>
  </li>
  <li><a href="#max-margin-methods">Max margin methods</a>
    <ul>
      <li><a href="#max-margin-proj">Max-margin &amp; Projection</a>
        <ul>
          <li><a href="#max-margin">Max-margin</a></li>
          <li><a href="#proj">Projection</a></li>
        </ul>
      </li>
      <li><a href="#max-margin-pln">Max margin planning</a></li>
      <li><a href="#learch">LEARCH</a></li>
    </ul>
  </li>
  <li><a href="#max-ent">Max entropy methods</a></li>
  <li><a href="#bayes">Bayesian methods</a>
    <ul>
      <li><a href="#birl">BIRL</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="mdp">Markov decision processes</h3>
<p>We begin by recalling the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html#mdp"><strong>Markov decision processes (MDP)</strong></a>.</p>

<p>A <strong>Markov decision process</strong>, or <strong>MDP</strong> is defined to be a tuple $(\mathcal{S},\mathcal{A},T,R,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a set of states, represents the <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a set of actions, known as the <strong>action space</strong>.</li>
  <li>$T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>state transition probabilities</strong>, i.e., $T(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$R:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ denotes the <strong>reward function</strong>.</li>
  <li>$\gamma\in[0,1]$ is referred as <strong>discount factor</strong>.</li>
</ul>

<p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$.</p>

<p>Under a particular policy $\pi$, the <strong>state value function</strong>, denoted as $v_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$. Specifically, $v_\pi(s)$ is defined as the expected sum of discounted rewards when starting in $s$ and following $\pi$.</p>

<p>For an initial state $s_0$, its value can be computed as
\begin{equation}
v_\pi(s_0)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t R(S_t,\pi(S_t))\big\vert S_0=s_0\right]\label{eq:mdp.1}
\end{equation}
Analogously, the <strong>state-action value function</strong>, denoted by $q_\pi$, of a state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$ specifies how good it is to take action $a$ from state $s$. Similar to $v_\pi$, $q_\pi$ is defined as the expected sum of discounted rewards when starting from $s$, taking action $a$ and thereby following policy $\pi$.</p>

<h3 id="irl">Inverse reinforcement learning</h3>
<p>The tuple $(\mathcal{S},\mathcal{A},T,\gamma)$ represents an $MDP$ without a predefined reward function $R$, or MDP\R for short. Suppose that we are given either a set of demonstrated trajectories, denoted as $\mathcal{D}$, or an <strong>expert</strong> (policy) $\pi_E$, which is good at our task. Without an explicit reward function, with <strong>inverse reinforcement learning</strong>, or <strong>IRL</strong> methods, we instead try to find a reward function $\hat{R}_E$ that is good at explaining $\pi_E$ or $\mathcal{D}$.</p>

<p>For each state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$, we assume there is a corresponding feature vector $\boldsymbol{\phi}(s,a)\in\mathbb{R}^k$ where
\begin{equation}
\boldsymbol{\phi}(s,a)=\big(\phi_1(s,a),\ldots,\phi_k(s,a)\big)^\text{T},
\end{equation}
where $\phi_i:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ are feature functions such that the reward function can be expressed as a linear weighted sum of these features. In particular, for $w_i\in\mathbb{R}$ we can represent the reward function $R$ as
\begin{align}
R(s,a)&amp;=w_1\phi_1(s,a)+\ldots+w_k\phi_k(s,a) \\ &amp;=\mathbf{w}^\text{T}\boldsymbol{\phi}(s,a)
\end{align}
Thus, we can rewrite \eqref{eq:mdp.1} as
\begin{align}
v_\pi(s_0)&amp;=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t R(S_t,\pi(S_t))\big\vert S_0=s_0\right] \<br />
\end{align}
The <strong>feature expectation</strong> or also called the expected discounted accumulated feature value, denoted as $\mu(\pi)$, of a policy $\pi$ is defined as
\begin{equation}
\mu(\pi)=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t\phi(S_t,A_t)\right]
\end{equation}</p>

<h2 id="max-margin-methods">Max margin methods</h2>
<p>The idea of <strong>max margin methods</strong> is to learn a reward function $\hat{R}_E$ that explains the demonstrated policy $\pi_E$ better than alternative policies by a margin.</p>

<h3 id="max-margin-proj">Max-margin &amp; Projection</h3>
<p>In these algorithms setting,</p>

<h4 id="max-margin">Max-margin</h4>

<h4 id="proj">Projection</h4>

<h3 id="max-margin-pln">Max margin planning</h3>

<h3 id="learch">LEARCH</h3>

<h2 id="max-ent">Max entropy methods</h2>

<h2 id="references">References</h2>
<p>[1] Pieter Abbeel &amp; Andrew Y. Ng. <a href="https://doi.org/10.1145/1015330.1015430">Apprenticeship Learning via Inverse Reinforcement Learning</a>. ICML ‘04: Proceedings of the twenty-first international conference on Machine learning. July 2004.</p>

<p>[2] Nathan D. Ratliff, J. Andrew Bagnell &amp; Martin A. Zinkevich. <a href="https://doi.org/10.1145/1143844.1143936">Maximum margin planning</a>. ICML ‘06: Proceedings of the 23rd international conference on Machine learning. June 2006</p>

<p>[3] Nathan Ratliff, David Silver &amp; J. Andrew (Drew) Bagnell. <a href="https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/">Learning to search: Functional gradient techniques for imitation learning</a>. Autonomous Robots. July 2009.</p>

<h2 id="footnotes">Footnotes</h2>
:ET