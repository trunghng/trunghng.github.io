<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>model-based on Littleroot</title>
    <link>https://trunghng.github.io/tags/model-based/</link>
    <description>Recent content in model-based on Littleroot</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 22 Sep 2024 17:54:43 +0700</lastBuildDate><atom:link href="https://trunghng.github.io/tags/model-based/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Model-based RL with latent variable models</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</link>
      <pubDate>Sun, 22 Sep 2024 17:54:43 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/mbrl-lvm/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Model-based RL methods that learn latent-variable models instead of trying to predict dynamics models in the observed space. The learned world model then can be used in planning effectively rather than being less efficiently, for instance in visual-based tasks, generating images for future time steps and feed them back into the model to predict the next ones, which requires more computation.&lt;/p&gt;
&lt;/blockquote&gt;</description>
    </item>
    
    <item>
      <title>MuZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/muzero/</link>
      <pubDate>Tue, 02 Jan 2024 11:52:40 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/muzero/</guid>
      <description>&lt;h2 id=&#34;muzero&#34;&gt;MuZero&lt;/h2&gt;
&lt;p&gt;Predictions are made at each time step $t$, for each of $k=0,\ldots,K$ steps, by a model $\mu_\theta$, parameterized by $\theta$, conditioned on past observations $o_1,\ldots,o_t$ and on future actions $a_{t+1},\ldots,a_{t+k}$ for $k&amp;gt;0$.&lt;br&gt;
The model $\mu_\theta$ predicts three future quantities that are directly relevant for planning:&lt;/p&gt;
&lt;ul class=&#39;number-list&#39;&gt;
	&lt;li&gt;
		the policy $p_t^k\approx\pi(a_{t+k+1}\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k})$;
	&lt;/li&gt;
	&lt;li&gt;
		the value function $v_t^k\approx\mathbb{E}\big[u_{t+k+1}+\gamma u_{t+k+2}+\ldots\vert o_1,\ldots,o_t,a_{t+1},\ldots,a_{t+k}\big]$;
	&lt;/li&gt;
	&lt;li&gt;
		the immediate reward $r_t^k\approx u_{t+k}$,
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;where $u$ is the true, observed reward, $\pi$ is the policy used to select real actions and $\gamma$ is the discount function of the environment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>AlphaGo, AlphaGo Zero, AlphaZero</title>
      <link>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</link>
      <pubDate>Tue, 17 Oct 2023 10:23:22 +0700</pubDate>
      
      <guid>https://trunghng.github.io/posts/reinforcement-learning/alphazero/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Model-based RL methods that use Monte Carlo Tree Search for planning and ultilize self-play mechanism for training.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;!-- for Milu --&gt;</description>
    </item>
    
  </channel>
</rss>
