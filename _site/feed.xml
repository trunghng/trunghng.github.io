<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-09-12T15:04:07+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">The Convergence of Q-learning</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence.html" rel="alternate" type="text/html" title="The Convergence of Q-learning" /><published>2022-08-21T07:00:00+07:00</published><updated>2022-08-21T07:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence.html">&lt;blockquote&gt;
  &lt;p&gt;A note on convergence proofs for Q-learning by exploiting the connection with stochastic approximation and the idea of parallel asynchronous.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminaries&quot;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#q-learning-convergence&quot;&gt;The convergence of Q-learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preferences&quot;&gt;Preferences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In Q-learning, transition probabilities and costs are unknown but information of them is obtained either by simulation or by experimenting. Q-learning uses simulation or experimental information to estimate the expected cost-to-go. Additionally, the algorithm is recursive and each new piece of information is used for computing an additive correction term to the old estimates. As these correction terms are random, Q-learning therefore has the same general structure as the stochastic approximation algorithms.&lt;/p&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h2 id=&quot;q-learning-convergence&quot;&gt;The convergence of Q-learning&lt;/h2&gt;

&lt;h2 id=&quot;preferences&quot;&gt;Preferences&lt;/h2&gt;
&lt;p&gt;[1] T. Jaakkola &amp;amp; M. I. Jordan &amp;amp; S. P. Singh. &lt;a href=&quot;doi: 10.1162/neco.1994.6.6.1185&quot;&gt;On the Convergence of Stochastic Iterative Dynamic Programming Algorithms&lt;/a&gt; in Neural Computation, vol. 6, no. 6, pp. 1185-1201, Nov. 1994.&lt;/p&gt;

&lt;p&gt;[2] Dvoretzky A. &lt;a href=&quot;https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Stochastic-Approximation/bsmsp/1200501645?tab=ArticleFirstPage&quot;&gt;On stochastic approximation&lt;/a&gt;. Berkeley Symposium on Mathematical Statistics and Probability, 1956: 39-55 (1956).&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="q-learning" /><category term="dynamic-programming" /><summary type="html">A note on convergence proofs for Q-learning by exploiting the connection with stochastic approximation and the idea of parallel asynchronous.</summary></entry><entry><title type="html">Neural Network from scratch</title><link href="http://localhost:4000/artificial-intelligent/machine-learning/deep-learning/2022/08/13/nn-scratch.html" rel="alternate" type="text/html" title="Neural Network from scratch" /><published>2022-08-13T13:00:00+07:00</published><updated>2022-08-13T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/machine-learning/deep-learning/2022/08/13/nn-scratch</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/machine-learning/deep-learning/2022/08/13/nn-scratch.html">&lt;blockquote&gt;

  &lt;!-- excerpt-end --&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;#fnn&quot;&gt;Feedforward neural networks&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&quot;#xor&quot;&gt;The XOR function&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;fnn&quot;&gt;Feedforward neural networks&lt;/h2&gt;

&lt;h3 id=&quot;xor&quot;&gt;The XOR function&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;XOR function&lt;/strong&gt; (or &lt;strong&gt;exclusive or function&lt;/strong&gt;), denoted as $\oplus:\{0,1\}\times\{0,1\}\to\{0,1\}$, is defined as:
\begin{align}
\oplus(0,0)&amp;amp;=0, \\ \oplus(0,1)&amp;amp;=1, \\ \oplus(1,0)&amp;amp;=1, \\ \oplus(1,1)&amp;amp;=0,
\end{align}
or by words, $f(x_1,x_2)=1$ only if exactly one of the two binary inputs having the value of $1$, otherwise it returns the value of $0$.&lt;/p&gt;

&lt;p&gt;Suppose given a set of four points $\mathbb{X}=\left\{(0,0),(0,1),(1,0),(1,1)\right\}$ and their projected value of them on $\oplus$ space, $\hat{f}(\mathbf{x}),\mathbf{x}\in\mathbb{X}$, we will learn an approximator of $\oplus$, denoted as $f$, by a feedforward network.&lt;/p&gt;

&lt;p&gt;Consider this as a regression problem, we will be using the MSE as our loss function. Or in particular,
\begin{equation}
J(\mathbf{w},b)=\frac{1}{4}\sum_{\mathbf{x}\in\mathbb{X}}\left(\hat{f}(\mathbf{x})-f(\mathbf{x};\mathbf{w},b)\right)^2\tag{1}\label{1}
\end{equation}
Let us assume that we can learn a linear model $f$, that means
\begin{equation}
f(\mathbf{x};\mathbf{w},b)=\mathbf{x}^\intercal\mathbf{w}+b,
\end{equation}
which lets equation \eqref{1} be written as
\begin{equation}
J(\mathbf{w},b)=\frac{1}{4}\sum_{\mathbf{x}\in\mathbb{X}}\left(\hat{f}(\mathbf{x})-\left(\mathbf{x}^\intercal\mathbf{w}+b\right)\right)^2
\end{equation}
Taking the derivatives of $J$ w.r.t $\mathbf{w}$ and $b$, we have
\begin{align}
\nabla_\mathbf{w}J(\mathbf{w},b)&amp;amp;\propto\sum_{\mathbf{x}\in\mathbb{X}}\left(\hat{f}(x)-\left(\mathbf{x}^\intercal\mathbf{w}+b\right)\right)\mathbf{x}, \\ \nabla_b J(\mathbf{w},b)&amp;amp;\propto\sum_{\mathbf{x}\in\mathbb{X}}\left(\hat{f}(x)-\left(\mathbf{x}^\intercal\mathbf{w}+b\right)\right)
\end{align}
Letting these gradients be zero gives us $\mathbf{w}=\mathbf{0}$ and $b=\frac{1}{2}$. With this solution, our model simply returns $\frac{1}{2}$ for any given input. This means that we can not find a linear function that describes exactly how the XOR works.&lt;/p&gt;

&lt;p&gt;One possible solution to this problem is that instead of taking $\mathbb{X}$ as the domain of our linear model, we will choose a space $\mathbb{X}’$ on which we can successfully apply a linear model. On other words, we select a vector-valued function $f^{(1)}:\mathbb{X}\to\mathbb{X}’$ such that
\begin{equation}
\oplus(\mathbf{x})=f^{(2)}(f^{(1}(\mathbf{x})),
\end{equation}
where $f^{(2)}$ is a linear function.&lt;/p&gt;

&lt;p&gt;Clearly we can not pick $f^{(1)}$ as a linear function, or specifically a linear transform because the composition $f^{(2)}\circ f^{(1)}$ of two linear functions $f^{(2)}$ and $f^{(1)}$ is still a linear function. In particular, assume that $f^{(1)}(\mathbf{x})=\mathbf{W}^\intercal\mathbf{x}+\mathbf{c}$, then
\begin{align}
f^{(2)}(f^{(1)}(\mathbf{x}))&amp;amp;=\left(\mathbf{W}^\intercal\mathbf{x}+\mathbf{c}\right)^\intercal\mathbf{w}+b \\ &amp;amp;=\left(\mathbf{w}^\intercal\mathbf{W}^\intercal\right)\mathbf{x}+\left(\mathbf{w}^\intercal\mathbf{c}+b\right)
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] joelgrus &lt;a href=&quot;https://github.com/joelgrus/joelnet&quot;&gt;JoelNet&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Ian Goodfellow &amp;amp; Yoshua Bengio &amp;amp; Aaron Courville. &lt;a href=&quot;https://www.deeplearningbook.org&quot;&gt;Deep Learning&lt;/a&gt;. MIT Press (2016).&lt;/p&gt;

&lt;p&gt;[3] Adrew Ng. &lt;a href=&quot;https://coursera.com&quot;&gt;Deep Learning Specialization&lt;/a&gt;. Coursera.&lt;/p&gt;

&lt;p&gt;[4] Pytorch Documentation &lt;a href=&quot;https://pytorch.org/docs/stable/index.html&quot;&gt;Pytorch Docs&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="machine-learning" /><category term="deep-learning" /><category term="artificial-intelligent" /><category term="machine-learning" /><category term="deep-learning" /><category term="neural-network" /><summary type="html"></summary></entry><entry><title type="html">Measure theory - II</title><link href="http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2.html" rel="alternate" type="text/html" title="Measure theory - II" /><published>2022-07-03T13:00:00+07:00</published><updated>2022-07-03T13:00:00+07:00</updated><id>http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2</id><content type="html" xml:base="http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2.html">&lt;blockquote&gt;
  &lt;p&gt;A note on measure theory (continued from &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory.html&quot;&gt;part I&lt;/a&gt;): materials were mostly taken from &lt;a href=&quot;/mathematics/measure-theory/2022/07/03/measure-theory-p2.html#taos-book&quot;&gt;Tao’s book&lt;/a&gt;, except for some notations needed from &lt;a href=&quot;/mathematics/measure-theory/2022/07/03/measure-theory-p2.html#steins-book&quot;&gt;Stein’s book&lt;/a&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lebesgue-measure&quot;&gt;Lebesgue measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lebesgue-outer-measure-properties&quot;&gt;Properties of Lebesgue outer measure&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lebesgue-measurability&quot;&gt;Lebesgue measurability&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#criteria-measurability&quot;&gt;Criteria for measurability&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#non-measurable-sets&quot;&gt;Non-measurable sets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lebesgue-int&quot;&gt;Lebesgue integral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lebesgue-measure&quot;&gt;Lebesgue measure&lt;/h2&gt;
&lt;p&gt;Recall that the Jordan outer measure of a set $E\subset\mathbb{R}^d$ has been defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
From the finite additivity and subadditivity of elementary measure, we can also write the Jordan outer measure as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B_1\cup\dots\cup B_k\supset E;B_1,\dots,B_k\text{ boxes}}\vert B_1\vert+\dots+\vert B_k\vert,
\end{equation}
which means the Jordan outer measure is the infimal cost required to cover $E$ by a finite union of boxes. By replacing the finite union of boxes by a countable union of boxes, we obtain the &lt;strong&gt;Lebesgue outer measure&lt;/strong&gt; $m^{*}(E)$ of $E$:
\begin{equation}
m^{*}(E)\doteq\inf_{\bigcup_{n=1}^{\infty}B_n\supset E;B_1,B_2,\dots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
which is be seen as the infimal cost required to cover $E$ by a countable union of boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;. we always have $m^*(E)\leq m^{*,(J)}(E)$.&lt;/p&gt;

&lt;p&gt;A set $E\subset\mathbb{R}^d$ is said to be &lt;strong&gt;Lebesgue measurable&lt;/strong&gt; if, for every $\varepsilon&amp;gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $E$ such that $m^{*}(U\backslash E)\leq\varepsilon$. If $E$ is Lebesgue measurable, we refer to
\begin{equation}
m(E)\doteq m^{*}(E)
\end{equation}
as the &lt;strong&gt;Lebesgue measure&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;h3 id=&quot;lebesgue-outer-measure-properties&quot;&gt;Properties of Lebesgue outer measure&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;. (&lt;strong&gt;The outer measure axioms&lt;/strong&gt;)&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;&lt;b&gt;Empty set&lt;/b&gt;. $m^*(\emptyset)=0$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Monotonicity&lt;/b&gt;. If $E\subset F\subset\mathbb{R}^d$, then $m^*(E)\leq m^*(F)$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Countable subadditivity&lt;/b&gt;. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of sets, then $m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{\infty}m^*(E_n)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;This follows from the definition of Lebesgue outer measure.&lt;/li&gt;
	&lt;li&gt;
		Since $E\subset F\subset\mathbb{R}^d$, then any set containing $F$ also includes $E$, but not every set having $E$ contains $F$. That means
		\begin{equation}
		\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\supset\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		Thus,
		\begin{equation}
		\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\leq\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		or
		\begin{equation}
		m^*(E)&amp;lt; m^*(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		By the definition of Lebesgue outer measure, for any positive integer $i$, we have
		\begin{equation}
		m^*(E_i)=\inf_{\bigcup_{n=1}^{\infty}B_n\supset E_i;B_1,B_2,\ldots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert
		\end{equation}
		Thus, by definition of infimum and by axiom of countable choice (&lt;b&gt;Axiom 8&lt;/b&gt;), for each $E_i$ in the sequence $(E_n)_{n\in\mathbb{N}}$, there exists a family of boxes $B_{i,1},B_{i,2},\ldots$ in the doubly sequence $(B_{i,j})_{(i,j)\in\mathbb{N}^2}$ covering $E_i$ such that
		\begin{equation}
		\sum_{j=1}^{\infty}\vert B_{i,j}\vert\lt m^*(E_i)+\frac{\varepsilon}{i},
		\end{equation}
		for any $\varepsilon&amp;gt;0$, and for $i=1,2,\ldots$. Plus, we also have
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}
		\end{equation}
		Moreover, by the Tonelli&apos;s theorem for series (&lt;b&gt;Theorem 6&lt;/b&gt;), we have
		\begin{equation}
		\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}=\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}
		\end{equation}
		Therefore once again, by definition of outer measure and definition of infimum, we obtain
		\begin{align}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;amp;=\inf_{\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert\leq\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert \\\\ &amp;amp;\lt\sum_{i=1}^{\infty}m^*(E_i)+\frac{\varepsilon}{2^i}=\sum_{i=1}^{\infty}m^*(E_i)+\varepsilon
		\end{align}
		And since $\varepsilon&amp;gt;0$ was arbitrary, we can conclude that
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{i=n}^{\infty}m^*(E_n)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Corollary 11&lt;/strong&gt;&lt;br /&gt;
Combining empty set with countable subadditivity axiom gives us the finite subadditivity property
\begin{equation}
m^{*}\left(E_1\cup\ldots\cup E_k\right)\leq m^{*}(E_1)+\ldots+m^{*}(E_k),\hspace{1cm}\forall k\geq 0
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 12&lt;/strong&gt;. (&lt;strong&gt;Finite additivity for separated sets&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $E,F\subset\mathbb{R}^d$ be such that $\text{dist}(E,F)&amp;gt;0$, where
\begin{equation}
\text{dist}(E,F)\doteq\inf\left\{\vert x-y\vert:x\in E,y\in F\right\}
\end{equation}
is the distance between $E$ and $F$. Then $m^*(E\cup F)=m^*(E)+m^*(F)$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From subadditivity property, we have $m^*(E\cup F)\leq m^*(E)+m^*(F)$. Then it suffices to prove the inverse, that
\begin{equation}
m^*(E\cup F)\geq m^*(E)+m^*(F)
\end{equation}
Let $\varepsilon&amp;gt;0$. By definition of Lebesgue outer measure, we can cover $E\cup F$ by a countable family $B_1,B_2,\ldots$ of boxes such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E\cup F)+\varepsilon
\end{equation}
Suppose it was the case that each box intersected at most one of $E$ and $F$. Then we could divide this family into two subfamilies $B_1’,B_2’,\ldots$ and $B_1&apos;&apos;,B_2&apos;&apos;,B_3&apos;&apos;,\ldots$, the first of which covered $E$, while the second of which covered $F$. From definition of Lebesgue outer measure, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}
and
\begin{equation}
m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n&apos;&apos;\vert
\end{equation}
Summing up these two equation, we obtain
\begin{equation}
m^*(E)+m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
and thus
\begin{equation}
m^*(E)+m^*(F)\leq m^*(E\cup F)+\varepsilon
\end{equation}
Since $\varepsilon$ was arbitrary, this gives $m^*(E)+m^*(F)\leq m^*(E\cup F)$ as required.&lt;/p&gt;

&lt;p&gt;Now we consider the case that some of the boxes $B_n$ intersect both $E$ and $F$.&lt;/p&gt;

&lt;p&gt;Since given any $r&amp;gt;0$, we can always partition a box $B_n$ into a finite number of smaller boxes, each of which has diameter&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; at most $r$, with the total volume of these sub-boxes equal to the volume of the original box $B_n$. Therefore, given any $r&amp;gt;0$, we may assume without loss of generality that the boxes $B_1,B_2,\ldots$ covering $E\cup F$ have diameter at most $r$. Or in particular, we may assume that all such boxes have diameter strictly less than $\text{dist}(E,f)$.&lt;/p&gt;

&lt;p&gt;Once we do this, then it is no longer possible for any box to intersect both $E$ and $F$, which allows the previous argument be applicable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
Let $E,F\subset\mathbb{R}^d$ be disjoint closed sets, with at least one of $E,F$ being compact. Then $\text{dist}(E,F)&amp;gt;0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 13&lt;/strong&gt;. (&lt;strong&gt;Outer measure of elementary sets&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $E$ be an elementary set. Then the Lebesgue outer measure of $E$ is equal to the elementary measure of $E$:&lt;/em&gt;
\begin{equation}
m^*(E)=m(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since
\begin{equation}
m^*(E)\leq m^{*,(J)}(E)=m(E),
\end{equation}
then it suffices to show that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
We first consider the case that $E$ is closed. Since $E$ is elementary, $E$ is also bounded, which implies that $E$ is compact.&lt;/p&gt;

&lt;p&gt;Let $\varepsilon&amp;gt;0$ be arbitrary, then we can find a countable family $B_1,B_2,\ldots$ of boxes that cover $E$
\begin{equation}
E\subset\bigcup_{n=1}^{\infty}B_n,
\end{equation}
such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We have that for each box $B_n$, we can find an open box $B_n’$ containing $B_n$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n}
\end{equation}
The $B_n’$ still cover $E$ and we have
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq\sum_{n=1}^{\infty}\left(\vert B_n\vert+\frac{\varepsilon}{2^n}\right)=\left(\sum_{n=1}^{\infty}\vert B_n\vert\right)+\varepsilon\leq m^*(E)+2\varepsilon\tag{11}\label{11}
\end{equation}
As the $B_n’$ are open, apply the &lt;strong&gt;Heine-Borel theorem&lt;/strong&gt; (&lt;strong&gt;Theorem 5&lt;/strong&gt;), we obtain
\begin{equation}
E\subset\bigcup_{n=1}^{N}B_n’,
\end{equation}
for some finite $N$. Thus, using the finite subadditivity property of elementary measure, combined with the result \eqref{11}, we obtain
\begin{equation}
m(E)\leq\sum_{n=1}^{N}m(B_n’)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&amp;gt;0$ was arbitrary, we can conclude that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
Now we turn to considering the case that $E$ is not closed. Then we can write $E$ as the finite union of disjoint boxes
\begin{equation}
E=Q_1\cup\ldots\cup Q_k,
\end{equation}
which need not be closed.&lt;/p&gt;

&lt;p&gt;Analogy to before, we have that for every $\varepsilon&amp;gt;0$ and every $1\leq j\leq k$, we can find a closed sub-box $Q_j’$ of $Q_j$ such that
\begin{equation}
\vert Q_j’\vert\geq\vert Q_j\vert-\frac{\varepsilon}{k}
\end{equation}
Then $E$ now contains the finite union of $Q_1’\cup\ldots\cup Q_k’$ disjoint closed boxes, which is a closed elementary set. By the finite additivity property of elementary measure, the monotonicity property of Lebesgue measure, combined with the result we have proved in the first case, we have
\begin{align}
m^*(E)&amp;amp;\geq m^*(Q_1’\cup\ldots\cup Q_k’) \\ &amp;amp;=m(Q_1’\cup\ldots\cup Q_k’) \\ &amp;amp;=m(Q_1’)+\ldots+m(Q_k’) \\ &amp;amp;\geq m(Q_1)+\ldots+m(Q_k)-\varepsilon \\ &amp;amp;= m(E)-\varepsilon,
\end{align}
for every $\varepsilon&amp;gt;0$. And since $\varepsilon&amp;gt;0$ was arbitrary, our claim has been proved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 14&lt;/strong&gt;&lt;br /&gt;
From the lemma above and the monotonicity property, 
for every $E\in\mathbb{R}^d$, we have
\begin{equation}
m_{*,(J)}(E)\leq m^{*}(E)\leq m^{*,(J)}(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Not every bounded open set or compact set (bounded closed) is Jordan measurable.&lt;/li&gt;
  &lt;li&gt;Two boxes are &lt;strong&gt;almost disjoint&lt;/strong&gt; if their interiors are disjoint. E.g., $[0,1]$ and $[1,2]$ are almost disjoint. If a box has the same elementary as its interior, we see that the finite additivity property
\begin{equation}
m(B_1\cup\ldots\cup B_n)=\vert B_1\vert+\ldots+\vert B_n\vert\tag{12}\label{12}
\end{equation}
also holds for almost disjoint boxes $B_1,\ldots,B_n$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Lemma 15&lt;/strong&gt;. (&lt;strong&gt;Outer measure of countable unions of almost disjoint boxes&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $E=\bigcup_{n=1}^{\infty}B_n$ be a countable union of almost disjoint boxes $B_1,B_2,\ldots$. Then&lt;/em&gt;
\begin{equation}
m^*(E)=\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
Thus, for example, $\mathbb{R}^d$ has an infinite outer measure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From countable subadditivity property of Lebesgue measure and &lt;strong&gt;Lemma 13&lt;/strong&gt;, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}m^*(B_n)=\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
so it suffices to show that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)
\end{equation}
Since for each integer $N$, $E$ contains the elementary set $B_1\cup\ldots\cup B_N$, then by monotonicity property and &lt;strong&gt;Lemma 13&lt;/strong&gt;
\begin{align}
m^*(E)&amp;amp;\geq m^*(B_1\cup\ldots\cup B_N)=m(B_1\cup\ldots\cup B_N)
\end{align}
And thus by \eqref{12}, we have
\begin{equation}
\sum_{n=1}^{N}\vert B_n\vert\leq m^*(E)
\end{equation}
Letting $N\to\infty$ we obtain the claim.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 16&lt;/strong&gt;&lt;br /&gt;
If $E=\bigcup_{n=1}^{\infty}B_n=\bigcup_{n=1}^{\infty}B_n’$ can be decomposed in two different ways as the countable union of almost disjoint boxes, then
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert=\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 17&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an open set. Then $E$ can be expressed as the countable union of almost disjoint boxes (and, in fact, as the countable union of almost disjoint closed cubes)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We begin by defining a &lt;strong&gt;closed dyadic cube&lt;/strong&gt; to be a cube $Q$ of the form
\begin{equation}
Q=\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right]\times\ldots\times\left[\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d;n\geq 0$.&lt;/p&gt;

&lt;p&gt;We have that such closed dyadic cubes of a fixed sidelength $2^{-n}$ are almost disjoint and cover all of $\mathbb{R}^d$. And also, each dyadic cube of sidelength $2^{-n}$ is contained in exactly one “parent” of sidelength $2^{-n+1}$ (which, conversely, has $2^d$ “children” of sidelength $2^{-n}$), giving the dyadic cubes a structure analogous to that of a binary tree.&lt;/p&gt;

&lt;p&gt;As a consequence of these facts, we also obtain the &lt;strong&gt;dyadic nesting property&lt;/strong&gt;: given any two closed dyadic cubes (not necessarily same sidelength), then either they are almost disjoint, or one of them is contained in the other.&lt;/p&gt;

&lt;p&gt;If $E$ is open, and $x\in E$, then by definition there is an open ball centered at $x$ that is contained in $E$. Also, it is easily seen that there is also a closed dyadic cube containing $x$ that is contained in $E$. Hence, if we let $\mathcal{Q}$ be the collection of all the dyadic cubes $Q$ that are contained in $E$, we see that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}}Q
\end{equation}
Let $\mathcal{Q}^*$ denote cubes in $\mathcal{Q}$ such that they are not contained in any other cube in $\mathcal{Q}$. From the nesting property, we see that every cube in $\mathcal{Q}$ is contained in exactly one maximal cube in $\mathcal{Q}^*$, and that any two such maximal cubes in $\mathcal{Q}^*$ are almost disjoint. Thus, we have that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}^*}Q,
\end{equation}
which is union of almost disjoint cubes. As $\mathcal{Q}^*$ is at most countable, the claim follows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 18&lt;/strong&gt;&lt;br /&gt;
The Lebesgue outer measure of any open set is equal to the Jordan inner measure of that set, or of the total volume of any partitioning of that set into almost disjoint boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 19&lt;/strong&gt;. (&lt;strong&gt;Outer regularity&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an arbitrary set. Then we have&lt;/em&gt;
\begin{equation}
m^*(E)=\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From monotonicity property, we have
\begin{equation}
m^*(E)\leq\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}
Then, it suffices to show that
\begin{equation}
m^*(E)\geq\inf_{E\subset U,U\text{ open}}m^*(U),
\end{equation}
which is obvious in the case that $m^*(E)$ is infinite. Thus, we now assume that $m^*(E)$ is finite.&lt;/p&gt;

&lt;p&gt;Let $\varepsilon&amp;gt;0$. By the definition of Lebesgue outer measure, there exists a countable family $B_1,B_2,\ldots$ of boxes covering $E$ such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We can enlarge each of these boxes $B_n$ to an open box $B_n’$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n},
\end{equation}
for any $\varepsilon&amp;gt;0$. Then the set $\bigcup_{n=1}^{\infty}B_n’$, being a union of open sets, is itself open, and contains $E$, and
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq m^*(E)+\varepsilon+\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=m^*(E)+2\varepsilon
\end{equation}
By countable subadditivity property, it implies that
\begin{equation}
m^*\left(\bigcup_{n=1}^{\infty}B_n’\right)\leq m^*(E)+2\varepsilon
\end{equation}
and thus
\begin{equation}
\inf_{E\subset U,U\text{ open}}m^*(U)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&amp;gt;0$ was arbitrary, the claim follows.&lt;/p&gt;

&lt;h3 id=&quot;lebesgue-measurability&quot;&gt;Lebesgue measurability&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Lemma 20&lt;/strong&gt;. (&lt;strong&gt;Existence of Lebesgue measurable sets&lt;/strong&gt;)&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;Every open set is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;Every closed set is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;Every set of Lebesgue outer measure zero is measurable. (Such sets are called &lt;b&gt;null sets&lt;/b&gt;.)&lt;/li&gt;
	&lt;li&gt;The empty set $\emptyset$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;If $E\subset\mathbb{R}^d$ is Lebesgue measurable, then so its complement $\mathbb{R}^d\backslash E$.&lt;/li&gt;
	&lt;li&gt;If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the union $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the intersection $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;This follows from definition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p id=&quot;criteria-measurability&quot;&gt;&lt;strong&gt;Remark&lt;/strong&gt;. (&lt;strong&gt;Criteria for measurability&lt;/strong&gt;)&lt;/p&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$. The following are equivalent&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;$E$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Outer approximation by open&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, $E$ can be contained in an open set $U$ with $m^*(U\backslash E)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost open&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find an open set $U$ such that $m^*(U\Delta E)\leq\varepsilon$. ($E$ differs from an open set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Inner approximation by closed&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a closed set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost closed&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a closed set $F$ such that $m^*(F\Delta E)\leq\varepsilon$. ($E$ differs from a closed set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost measurable&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a Lebesgue measurable set $E_\varepsilon$ such that $m^*(E_\varepsilon\Delta E)\leq\varepsilon$. ($E$ differs from a measurable set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 21&lt;/strong&gt;. (&lt;strong&gt;The measure axioms&lt;/strong&gt;)&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;&lt;b&gt;Empty set&lt;/b&gt;. $m(\emptyset)=0$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Countable additivity&lt;/b&gt;. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then&lt;/li&gt;
	\begin{equation}
	m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
	\end{equation}
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		We have that empty set $\emptyset$ is Lebesgue measurable since for every $\varepsilon&amp;gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $\emptyset$ such that $m^*(U\backslash\emptyset)\leq\varepsilon$. Thus,
		\begin{equation}
		m(\emptyset)=m^*(\emptyset)=0
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		We begin by considering the case that $E_n$ are all compact sets.
		&lt;br /&gt;
		By repeated use of &lt;b&gt;Lemma 12&lt;/b&gt; and &lt;b&gt;Example ?&lt;/b&gt;, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Thus, using monotonicity property, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Let $N\to\infty$, we obtain
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
		On the other hand, by countable subadditivity, we also have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Therefore, we can conclude that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Next, we consider the case that $E_n$ are bounded but not necessarily compact.
		&lt;br /&gt;
		Let $\varepsilon&amp;gt;0$. By criteria for measurability, we know that each $E_n$ is the union of a compact set $K_n$ and a set of outer measure at most $\varepsilon/2^n$. Thus
		\begin{equation}
		m(E_n)\leq m(K_n)+\frac{\varepsilon}{2^n}
		\end{equation}
		And hence
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq\left(\sum_{n=1}^{\infty}m(K_n)\right)+\varepsilon
		\end{equation}
		From the first case, we know that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)=\sum_{n=1}^{\infty}m(K_n)
		\end{equation}
		while from monotonicity property of Lebesgue measure
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Putting these results together we obtain
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)+\varepsilon,
		\end{equation}
		for every $\varepsilon&amp;gt;0$. And since $\varepsilon$ was arbitrary, we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		while from countable subadditivity property we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\geq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Therefore, the claim follows.
		&lt;br /&gt;
		Finally, we consider the case that $E_n$ are not bounded or closed with the idea of decomposing each $E_n$ as a countable disjoint union of bounded Lebesgue measurable sets.
		&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; (&lt;strong&gt;Monotone convergence theorem for measurable sets&lt;/strong&gt;)&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Upward monotone convergence&lt;/b&gt;. Let $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ be a countable non-decreasing sequence of Lebesgue measurable sets. Then
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Downward monotone convergence&lt;/b&gt;. Let $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ be a countable non-increasing sequence of Lebesgue measurable sets. If at least one of the $m(E_n)$ is finite, then
		\begin{equation}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
We say that a sequence $E_n$ of sets in $\mathbb{R}^d$ &lt;strong&gt;converges pointwise&lt;/strong&gt; to another set $E$ in $\mathbb{R}^d$ if the indicator function $1_{E_n}$ converges pointwise to $1_E$.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;If the $E_n$ are all Lebesgue measurable, and converge pointwise to $E$, then $E_n$ is Lebesgue measurable also.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Dominated convergence theorem&lt;/b&gt;. Suppose that the $E_n$ are all contained in another Lebesgue measurable set $F$ of finite measure. Then $m(E_n)$ converges to $m(E)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;non-measurable-sets&quot;&gt;Non-measurable sets&lt;/h3&gt;

&lt;h2 id=&quot;lebesgue-integral&quot;&gt;Lebesgue integral&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;taos-book&quot;&gt;Terence Tao. &lt;a href=&quot;https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/&quot;&gt;An introduction to measure theory&lt;/a&gt;. Graduate Studies in Mathematics, vol. 126.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;span id=&quot;steins-book&quot;&gt;Elias M. Stein &amp;amp; Rami Shakarchi. &lt;a href=&quot;#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf&quot;&gt;Real Analysis: Measure Theory, Integration, and Hilbert Spaces&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The &lt;strong&gt;diameter&lt;/strong&gt; of a set $B$ is defined as
\begin{equation}
\text{dia}(B)\doteq\sup\{\vert x-y\vert:x,y\in B\}
\end{equation} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="random-stuffs" /><summary type="html">A note on measure theory (continued from part I): materials were mostly taken from Tao’s book, except for some notations needed from Stein’s book.</summary></entry><entry><title type="html">Measure theory</title><link href="http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory.html" rel="alternate" type="text/html" title="Measure theory" /><published>2022-06-16T13:00:00+07:00</published><updated>2022-06-16T13:00:00+07:00</updated><id>http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory</id><content type="html" xml:base="http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory.html">&lt;blockquote&gt;
  &lt;p&gt;A note on measure theory: materials were mostly taken from &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory.html#taos-book&quot;&gt;Tao’s book&lt;/a&gt;, except for some notations needed from &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory.html#steins-book&quot;&gt;Stein’s book&lt;/a&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminaries&quot;&gt;Preliminaries&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pts-sets&quot;&gt;Points, sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#open-closed-compact-sets&quot;&gt;Open, closed, compact sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rects-cubes&quot;&gt;Rectangles, cubes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cantor-set&quot;&gt;The Cantor set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#others&quot;&gt;Others&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#elementary-measure&quot;&gt;Elementary measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#intervals-boxes-elementary-sets&quot;&gt;Intervals, boxes, elementary sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#measure-elementary-set&quot;&gt;Measure of an elementary set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#elementary-measure-properties&quot;&gt;Properties of elementary measure&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#jordan-measure&quot;&gt;Jordan measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#jordan-measurability-characterisation&quot;&gt;Characterisation of Jordan measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#jordan-measurability-properties&quot;&gt;Properties of Jordan measurability&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#connect-riemann-int&quot;&gt;Connection with the Riemann integral&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-integrability&quot;&gt;Riemann integrability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pc-func&quot;&gt;Piecewise constant functions&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#pc-int-properties&quot;&gt;Basic properties of piecewise constant integral&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#darboux-int&quot;&gt;Darboux integral&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#equiv-riemann-darboux-int&quot;&gt;Equivalence of Riemann integral and Darboux integral&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-int-properties&quot;&gt;Basic properties of the Riemann integral&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-int-area-interpret&quot;&gt;Area interpretation of the Riemann integral&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;pts-sets&quot;&gt;Points, sets&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;point&lt;/strong&gt; $x\in\mathbb{R}^d$ consists of a $d$-tuple of real numbers
\begin{equation}
x=\left(x_1,x_2,\dots,x_d\right),\hspace{1cm}x_i\in\mathbb{R}, i=1,\dots,d
\end{equation}
Addition between points and multiplication of a point by a real scalar is elementwise.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;norm&lt;/strong&gt; of $x$ is denoted by $\vert x\vert$ and is defined to be the standard &lt;strong&gt;Euclidean norm&lt;/strong&gt; given by
\begin{equation}
\vert x\vert=\left(x_1^2+\dots+x_d^2\right)^{1/2}
\end{equation}
We can then calculate the &lt;strong&gt;distance&lt;/strong&gt; between two points $x$ and $y$, which is
\begin{equation}
\text{dist}(x,y)=\vert x-y\vert
\end{equation}
The &lt;strong&gt;complement&lt;/strong&gt; of a set $E$ in $\mathbb{R}^d$ is denoted as $E^c$, and defined by
\begin{equation}
E^c=\{x\in\mathbb{R}^d:x\notin E\}
\end{equation}
If $E$ and $F$ are two subsets of $\mathbb{R}^d$, we denote the complement of $F$ in $E$ by
\begin{equation}
E-F=\{x\in\mathbb{R}^d:x\in E;\,x\notin F\}
\end{equation}
The &lt;strong&gt;distance&lt;/strong&gt; between two sets $E$ and $F$ is defined by
\begin{equation}
\text{dist}(E,F)=\inf_{x\in E,\,y\in F}\vert x-y\vert
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;open-closed-compact-sets&quot;&gt;Open, closed and compact sets&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;open ball&lt;/strong&gt; in $\mathbb{R}^d$ centered at $x$ and of radius $r$ is defined by
\begin{equation}
B_r(x)=\{y\in\mathbb{R}^d:\vert y-x\vert&amp;lt; r\}
\end{equation}
A subset $E\subset\mathbb{R}^d$ is &lt;strong&gt;open&lt;/strong&gt; if for every $x\in E$ there exists $r&amp;gt;0$ with $B_r(x)\subset E$. And a set is &lt;strong&gt;closed&lt;/strong&gt; if its complement is open.&lt;br /&gt;
Any (not necessarily countable) union of open sets is open, while in general, the intersection of only finitely many open sets is open. A similar statement holds for the class of closed sets, if we interchange the roles of unions and intersections.&lt;/p&gt;

&lt;p&gt;A set $E$ is &lt;strong&gt;bounded&lt;/strong&gt; if it is contained in some ball of finite radius. A set is &lt;strong&gt;compact&lt;/strong&gt; if it is bounded and is also closed. Compact sets enjoy the &lt;strong&gt;Heine-Borel&lt;/strong&gt; covering property:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;. (&lt;strong&gt;Heine-Borel theorem&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Assume $E$ is compact, $E\subset\bigcup_\alpha\mathcal{O}_\alpha$, and each $\mathcal{O}_\alpha$ is open. Then there are finitely many of the open sets $\mathcal{O}_{\alpha_1},\mathcal{O}_{\alpha_2},\dots,\mathcal{O}_{\alpha_N}$, such that $E\subset\bigcup_{j=1}^{N}\mathcal{O}_{\alpha_j}$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In words, &lt;em&gt;any&lt;/em&gt; covering of a compact set by a collection of open sets contains a &lt;em&gt;finite&lt;/em&gt; subcovering.&lt;/p&gt;

&lt;p&gt;A point $x\in\mathbb{R}^d$ is a &lt;strong&gt;limit point&lt;/strong&gt; of the set $E$ if for every $r&amp;gt;0$, the ball $B_r(x)$ contains points of $E$. This means that there are points in $E$ which are arbitrarily close to $x$. An &lt;strong&gt;isolated point&lt;/strong&gt; of $E$ is a point $x\in E$ such that there exists an $r&amp;gt;0$ where $B_r(x)\cap E=\{x\}$.&lt;/p&gt;

&lt;p&gt;A point $x\in E$ is an &lt;strong&gt;interior point&lt;/strong&gt; of $E$ if there exists $r&amp;gt;0$ such that $B_r(x)\subset E$. The set of all interior points of $E$ is called the &lt;strong&gt;interior&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;closure&lt;/strong&gt; of $E$, denoted as $\bar{E}$, consists the union of $E$ and all its limit points. The &lt;strong&gt;boundary&lt;/strong&gt; of $E$, denoted as $\partial E$, is the set of points which are in the closure of $E$ but not in the interior of $E$.&lt;/p&gt;

&lt;p&gt;A closed set $E$ is &lt;strong&gt;perfect&lt;/strong&gt; if $E$ does not have any isolated point.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The closure of a set is a closed set.&lt;/li&gt;
  &lt;li&gt;Every point in $E$ is a limit point of $E$.&lt;/li&gt;
  &lt;li&gt;A set is closed iff it contains all its limit points.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rects-cubes&quot;&gt;Rectangles, cubes&lt;/h3&gt;
&lt;p&gt;A (closed) &lt;strong&gt;rectangle&lt;/strong&gt; $R$ in $\mathbb{R}^d$ is given by the product of $d$ one-dimensional closed and bounded intervals
\begin{equation}
R\doteq[a_1,b_1]\times[a_2,b_2]\times\ldots\times[a_d,b_d],
\end{equation}
where $a_j\leq b_j$, for $j=1,\ldots,d$, are real numbers. In other words, we have
\begin{equation}
R=\left\{\left(x_1,\ldots,x_d\right)\in\mathbb{R}^d:a_j\leq x_j\leq b_j,\forall j=1,\ldots,d\right\}
\end{equation}
With this definition, a rectangle is closed and has sides parallel to the coordinate axis. In $\mathbb{R}$, the rectangles are the closed and bounded intervals; they becomes the usual rectangles as we usually see in $\mathbb{R}^2$; while in $\mathbb{R}^3$, they are the closed parallelepipeds.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-06-16/rectangles.png&quot; alt=&quot;Rectangles in R^d&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Rectangles in $\mathbb{R}^d,d=1,2,3$&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The lengths of the sides of the rectangle $R$ in $\mathbb{R}^d$ are $b_1-a_1,\ldots,b_d-a_d$. The &lt;strong&gt;volume&lt;/strong&gt; of its, denoted as $\vert R\vert$, is defined as
\begin{equation}
\vert R\vert\doteq(b_1-a_1)\dots(b_d-a_d)
\end{equation}
An open rectangle is the product of open intervals, and the interior of the rectangle $R$ is then
\begin{equation}
(a_1,b_1)\times\ldots\times(a_d,b_d)
\end{equation}
A &lt;strong&gt;cube&lt;/strong&gt; is a rectangle for which $b_1-a_1=\ldots=b_d-a_d$.&lt;/p&gt;

&lt;p&gt;A union of rectangles is said to be &lt;strong&gt;almost disjoint&lt;/strong&gt; if the interiors of the rectangles are disjoint.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a rectangle is the almost disjoint union of finitely many other rectangles, say $R=\bigcup_{k=1}^{N}R_k$, then&lt;/em&gt;
\begin{equation}
\vert R\vert=\sum_{k=1}^{N}\vert R_k\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $R,R_1,\ldots,R_N$ are rectangles, and $R\subset\bigcup_{k=1}^{N}R_k$, then&lt;/em&gt;
\begin{equation}
\vert R\vert\leq\sum_{k=1}^{N}\vert R_k\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 4&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Every open $\mathcal{O}\subset\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 5&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Every open $\mathcal{O}\subset\mathbb{R}^d,d\geq 1$, can be written as countable union of almost disjoint closed cubes&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cantor-set&quot;&gt;The Cantor set&lt;/h3&gt;
&lt;p&gt;Let $C_0=[0,1]$ denote the closed unit interval and let $C_1$ represent the set obtained from deleting the middle third open interval from $[0,1]$, as
\begin{equation}
C_1=[0,1/3]\cup[2/3,1]
\end{equation}
We repeat this procedure of deleting the middle third open interval for each subinterval of $C_1$. In the second stage we obtain
\begin{equation}
C_1=[0,1/9]\cup[2/9,1/3]\cup[2/3,7/9]\cup[8/9,1]
\end{equation}
We continue to repeat this process for each subinterval of $C_2$, and so on. The result of this process is a sequence $(C_k)_{k=0,1,\ldots}$ of compact sets with
\begin{equation}
C_0\supset C_1\supset C_2\supset\ldots\supset C_k\supset C_{k+1}\supset\ldots
\end{equation}
The &lt;strong&gt;Cantor set&lt;/strong&gt; $\mathcal{C}$ is defined as the intersection of all $C_k$’s
\begin{equation}
\mathcal{C}=\bigcap_{k=0}^{\infty}C_k
\end{equation}
The set $\mathcal{C}$ is not empty, since all end-points of the intervals in $C_k$ (all $k$) belong to $\mathcal{C}$.&lt;/p&gt;

&lt;h3 id=&quot;others&quot;&gt;Others&lt;/h3&gt;
&lt;p&gt;Given any sequence $x_1,x_2,\ldots\in[0,+\infty]$. We can always form the sum
\begin{equation}
\sum_{n=1}^{n}x_n\in[0,+\infty]
\end{equation}
as the limit of the partial sums $\sum_{n=1}^{N}x_n$, which may be either finite or infinite. An equivalence definition of this infinite sum is as the supremum of all finite subsums:
\begin{equation}
\sum_{n=1}^{\infty}x_n=\sup_{F\subset\mathbb{N},F\text{ finite}}\sum_{n\in F}x_n
\end{equation}
From this equation, given any collection $(x_\alpha)_{\alpha\in A}$ of numbers $x_\alpha\in[0,+\infty]$ indexed by an arbitrary set $A$, we can define the sum $\sum_{\alpha\in A}x_\alpha$ as
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sup_{F\subset A,F\text{ finite}}\sum_{\alpha\in F}x_\alpha
\end{equation}
Or moreover, given any bijection $\phi:B\to A$, we has the change of variables formula
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sum_{\beta\in B}x_{\phi(\beta)}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt;. (&lt;strong&gt;Tonelli’s theorem for series&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $(x_{n,m})_{n,m\in\mathbb{N}}$ be a doubly infinite sequence of extended nonnegative reals $x_{n,m}\in[0,+\infty]$. Then&lt;/em&gt;
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}x_{n,m}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We will prove the equality between the first and second expression, the proof for the equality between the first and the third one is similar.&lt;/p&gt;

&lt;p&gt;We begin by showing that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Let $F\subset\mathbb{N}^2$ be any finite set. Then $F\subset\{1,\ldots,N\}\times\{1,\ldots,N\}$ for some finite $N$. Since $x_{n,m}$ are nonnegative, we have
\begin{align}
\sum_{(n,m)\in F}x_{n,m}&amp;amp;\leq\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,\ldots,N\}}x_{n,m} \\ &amp;amp;=\sum_{n=1}^{N}\sum_{m=1}^{N}x_{n,m} \\ &amp;amp;\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{align}
for any finite subset $F$ of $\mathbb{R}^2$. Then by \eqref{1}, we have
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sup_{F\subset\mathbb{N}^2,F\text{ finite}}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
The problem now remains to prove that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{equation}
which will be proved if we can show that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Fix $N$, we have since each $\sum_{m=1}^{\infty}$ is the limit of $\sum_{m=1}^{M}x_{n,m}$, LHS is the limit of $\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}$ as $M\to\infty$. Thus, it suffices to show that for each finite $M$
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}=\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,ldots,M\}}x_{n,m}
\end{equation}
which is true for all finite $M,N$. And it concludes our proof.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiom 7&lt;/strong&gt;. (&lt;strong&gt;Axiom of choice&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $(E_\alpha)_{\alpha\in A}$ be a family of non-empty set $E_\alpha$, indexed by an index set $A$. Then we can find a family $(x_\alpha)_{\alpha\in A}$ of elements $x_\alpha$ of $E_\alpha$, indexed by the same set $A$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 8&lt;/strong&gt;. (&lt;strong&gt;Axiom of countable choice&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $E_1,E_2,\ldots$ be a sequence of non-empty sets. Then we can find a sequence $x_1,x_2,\ldots$ such that $x_n\in E_n,\forall n=1,2,\ldots$.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;elementary-measure&quot;&gt;Elementary measure&lt;/h2&gt;

&lt;h3 id=&quot;intervals-boxes-elementary-sets&quot;&gt;Intervals, boxes, elementary sets&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;interval&lt;/strong&gt; is a subset of $\mathbb{R}$ having one of the forms
\begin{align}
[a,b]&amp;amp;\doteq\{x\in\mathbb{R}:a\leq x\leq b\}, \\ [a,b)&amp;amp;\doteq\{x\in\mathbb{R}:a\leq x\lt b\}, \\ (a,b]&amp;amp;\doteq\{x\in\mathbb{R}:a\lt x\leq b\}, \\ (a,b)&amp;amp;\doteq\{x\in\mathbb{R}:a\lt x\lt b\},
\end{align}
where $a\leq b$ are real numbers.&lt;br /&gt;
The &lt;strong&gt;length&lt;/strong&gt; of an interval $I=[a,b],[a,b),(a,b],(a,b)$ is denoted as $\vert I\vert$ and is defined by
\begin{equation}
\vert I\vert\doteq b-a
\end{equation}
A &lt;strong&gt;box&lt;/strong&gt; in $\mathbb{R}^d$ is a Cartesian product $B\doteq I_1\times\ldots\times I_d$ of $d$ intervals $I_1,\ldots,I_d$ (not necessarily the same length). The &lt;strong&gt;volume&lt;/strong&gt; $\vert B\vert$ of such a box $B$ is defined as
\begin{equation}
\vert B\vert\doteq \vert I_1\vert\times\ldots\times\vert I_d\vert
\end{equation}
An &lt;strong&gt;elementary set&lt;/strong&gt; is any subset of $\mathbb{R}^d$ which is the union of a finite number of boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt; (&lt;strong&gt;Boolean closure&lt;/strong&gt;)&lt;br /&gt;
If $E,F\subset\mathbb{R}^d$ are elementary sets, then&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the union $E\cup F$,&lt;/li&gt;
  &lt;li&gt;the intersection $E\cap F$,&lt;/li&gt;
  &lt;li&gt;the set theoretic difference $E\backslash F\doteq\{x\in E:x\notin F\}$,&lt;/li&gt;
  &lt;li&gt;the symmetric difference $E\Delta F\doteq(E\backslash F)\cup(F\backslash E)$ 
are also elementary,&lt;/li&gt;
  &lt;li&gt;if $x\in\mathbb{R}^d$, then the translate $E+x\doteq\{y+x:y\in E\}$ is also an elementary set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
With their definitions as elementary sets, we can assume that
\begin{align}
E&amp;amp;=B_1\cup\ldots\cup B_k, \\ F&amp;amp;=B_1’\cup\ldots\cup B_{k’}’,
\end{align}
where each $B_i$ and $B_i’$ is a $d$-dimensional box. By set theory, we have that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The union of $E$ and $F$ can be written as
\begin{equation}
E\cup F=B_1\cup\ldots\cup B_k\cup B_1’\cup\ldots\cup B_{k’}’,
\end{equation}
which is an elementary set.&lt;/li&gt;
  &lt;li&gt;The intersection of $E$ and $F$ can be written as
\begin{align}
E\cap F&amp;amp;=\left(B_1\cup\ldots\cup B_k\right)\cup\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\cap B_j’\right),
\end{align}
which is also an elementary set.&lt;/li&gt;
  &lt;li&gt;The set theoretic difference of $E$ and $F$ can be written as
\begin{align}
E\backslash F&amp;amp;=\left(B_1\cup\ldots\cup B_k\right)\backslash\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right),
\end{align}
which is, once again, an elementary set.&lt;/li&gt;
  &lt;li&gt;With this display, the symmetric difference of $E$ and $F$ can be written as
\begin{align}
E\Delta F&amp;amp;=\left(E\backslash F\right)\cup\left(F\backslash E\right) \\ &amp;amp;=\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right)\Bigg]\cup\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_j’\backslash B_i\right)\Bigg],
\end{align}
which satisfies conditions of an elementary set.&lt;/li&gt;
  &lt;li&gt;Since $B_i$’s are $d$-dimensional boxes, we can express them as
\begin{equation}
B_i=I_{i,1}\times\ldots I_{i,d},
\end{equation}
where each $I_{i,j}$ is an interval in $\mathbb{R}^d$. Without loss of generality, we assume that they are all closed. In particular, for $j=1,\ldots,d$
\begin{equation}
I_{i,j}=(a_{i,j},b_{i,j})
\end{equation}
Thus, for any $x\in\mathbb{R}^d$, we have that
\begin{align}
E+x&amp;amp;=\left\{y+x:y\in E\right\} \\ &amp;amp;=\Big\{y+x:y\in B_1\cup\ldots\cup B_k\Big\} \\ &amp;amp;=\Big\{y+x:y\in\bigcup_{i=1}^{k}B_i\Big\} \\ &amp;amp;=\left\{y+x:y\in\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j},b_{i,j})\right\} \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j}+x,b_{i,j}+x),
\end{align}
which is an elementary set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;measure-elementary-set&quot;&gt;Measure of an elementary set&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 9&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an elementary set&lt;/em&gt;.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;$E$ &lt;i&gt;can be expressed as the finite union of disjoint boxes.&lt;/i&gt;&lt;/li&gt;
	&lt;li&gt;If $E$ is partitioned as the finite union $B_1\cup\ldots\cup B_k$ of disjoint boxes, then the quantity $m(E)\doteq\vert B_1\vert+\ldots+\vert B_k\vert$ is independent of the partition. In other words, given any other partition $B_1&apos;\cup\ldots\cup B_{k&apos;}&apos;$ of $E$, we have&lt;/li&gt;
	\begin{equation}
	\vert B_1\vert+\ldots+\vert B_k\vert=\vert B_1&apos;\vert+\ldots+\vert B_{k&apos;}&apos;\vert
	\end{equation}
&lt;/ul&gt;

&lt;p&gt;We refer to $m(E)$ as the &lt;strong&gt;elementary measure&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;Consider the one-dimensional case, with these $k$ intervals, we can put their $2k$ endpoints into an increasing-order list (discarding repetitions). By looking at the open intervals between these end points, together with the endpoints themselves (viewed as intervals of length zero), we see that there exists a finite collection of disjoint intervals $J_1,\dots,J_{k&apos;}$, such that each of the $I_1,\dots,I_k$ are union of some collection of the $J_1,\dots,J_{k&apos;}$. And since each interval is a one-dimensional box, our statement has been proved with $d=1$.&lt;br /&gt;
	In order to prove the multi-dimensional case, we begin by expressing $E$ as
	\begin{equation}
	E=\bigcap_{i=1}^{k}B_i,
	\end{equation}
	where each box $B_i=I_{i,1}\times\dots\times I_{i,d}$. For each $j=1,\dots,d$, since we has proved the one-dimensional case, we can express $I_{1,j},\dots I_{k,j}$ as the union of subcollections of collections $J_{1,j},\dots,J_{k&apos;,j}$ of disjoint intervals. Taking Cartesian product, we can express the $B_1,\dots,B_k$ as finite unions of box $J_{i_1,1}\times\dots\times J_{i_d,d}$, where $1\leq i_j\leq k_j&apos;$ for all $1\leq j\leq d$. Moreover such boxes are disjoint, which proved our argument.&lt;/li&gt;
	&lt;li&gt; We have that the length for an interval $I$ can be computed as
	\begin{equation}
	\vert I\vert=\lim_{N\to\infty}\frac{1}{N}\#\left(I\cap\frac{1}{N}\mathbb{Z}\right),
	\end{equation}
	where $\#A$ represents the cardinality of a finite set $A$ and 
	\begin{equation}
	\frac{1}{N}\mathbb{Z}\doteq\left\{\frac{x}{N}:x\in\mathbb{Z}\right\}
	\end{equation}
	Thus, volume of the box, say $B$, established from $d$ intervals $I_1,\dots,I_d$ by taking Cartesian product of them can be written as
	\begin{equation}
	\vert B\vert=\lim_{N\to\infty}\frac{1}{N^d}\#\left(B\cap\frac{1}{N}\mathbb{Z}^d\right)
	\end{equation}
	Therefore, with $k$ disjoint boxes $B_1,\dots,B_k$, we have that
	\begin{align}
	\vert B_1\vert+\dots+\vert B_k\vert&amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k}B_i\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cap\frac{1}{N}\mathbb{Z}^d\right) \\\\ &amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k&apos;}B_i&apos;\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;amp;=\vert B_1&apos;\vert+\dots+\vert B_{k&apos;}&apos;\vert
	\end{align}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elementary-measure-properties&quot;&gt;Properties of elementary measure&lt;/h3&gt;
&lt;p&gt;From the definition of elementary measure, it is easily seen that, for any elementary sets $E$ and $F$ (not necessarily disjoint),&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		$m(E)$ is a nonnegative real number (&lt;b&gt;non-negativity&lt;/b&gt;), and has &lt;b&gt;finite additivity property&lt;/b&gt;:
		\begin{equation}
		m(E\cup F)=m(E)+m(F)
		\end{equation}
		And by induction, it also implies that
		\begin{equation}
		m(E_1\cup\dots\cup E_k)=m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,E_k$ are disjoint elementary sets.
	&lt;/li&gt;
	&lt;li&gt;
		$m(\emptyset)=0$.
	&lt;/li&gt;
	&lt;li&gt;
		$m(B)=\vert B\vert$ for all box $B$.
	&lt;/li&gt;
	&lt;li&gt;
		From non-negativity, finite additivity and &lt;b&gt;Example 1&lt;/b&gt;, we conclude the &lt;b&gt;monotonicity&lt;/b&gt; property, i.e., $E\subset F$ implies that
		\begin{equation}
		m(E)\leq m(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		From the above and finite additivity, we also obtain the &lt;b&gt;finite subadditivity&lt;/b&gt; property
		\begin{equation}
		m(E\cup F)\leq m(E)+m(F)
		\end{equation}
		And by induction, we then have
		\begin{equation}
		m(E_1\cup\dots\cup E_k)\leq m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,
		E_k$ are elementary sets (not necessarily disjoint).
	&lt;/li&gt;
	&lt;li&gt;
		We also have the &lt;b&gt;translation invariance&lt;/b&gt; property
		\begin{equation}
		m(E+x)=m(E),\hspace{1cm}\forall x\in\mathbb{R}^d
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;. (&lt;strong&gt;Uniqueness of elementary measure&lt;/strong&gt;)&lt;br /&gt;
Let $d\geq 1$ and let $m’:\mathcal{E}(\mathbb{R}^d)\to\mathbb{R}^+$ be a map from the collection $\mathcal{E}(\mathbb{R}^d)$ of elementary subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity, and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all elementary sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Set $c\doteq m’([0,1)^d)$, we then have that $c\in\mathbb{R}^+$ by the non-negativity property. Using the translation invariance property, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)=\dots=m’\left(\left[\frac{n-1}{n},1\right)^d\right)
\end{equation}
On other hand, using the finite additivity property, for any positive integer $n$, we obtain that
\begin{align}
m’([0,1)^d)&amp;amp;=m’\left(\left[0,\frac{1}{n}\right)^d\cup\left[\frac{1}{n},\frac{2}{n}\right)^d\cup\dots\cup\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;amp;=m’\left(\left[0,\frac{1}{n}\right)^d\right)+m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)+\dots+m’\left(\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;amp;=n m’\left(\left[0,\frac{1}{n}\right)^d\right)
\end{align}
Thus,
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{c}{n},\hspace{1cm}\forall n\in\mathbb{Z}^+
\end{equation}
Moreover, since $m\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{1}{n}$, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=cm\left(\left[0,\frac{1}{n}\right)^d\right)
\end{equation}
It then follows by induction that
\begin{equation}
m’(E)=cm(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 3&lt;/strong&gt;&lt;br /&gt;
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be elementary sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also elementary, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Without loss of generality, assume that $d_1\leq d_2$. With their definitions as elementary sets, we can assume that
\begin{align}
E_1&amp;amp;=B_1\cup\dots\cup B_{k_1}, \\ E_2&amp;amp;=B_1’\cup\dots\cup B_{k_2}’,
\end{align}
where each $B_i$ is a $d_1$-dimensional box while each $B_i’$ is a $d_2$-dimensional box. And using &lt;strong&gt;Lemma 5&lt;/strong&gt;, without loss of generality, we can assume that $B_i$ are disjoint boxes and $B_i’$ are also disjoint, which implies that
\begin{align}
m^{d_1}(E_1)&amp;amp;=m^{d_1}(B_1)+\dots+m^{d_1}(B_{k_1}),\tag{1}\label{1} \\ m^{d_2}(E_2)&amp;amp;=m^{d_2}(B_1’)+\dots+m^{d_2}(B_{k_2}’)\tag{2}\label{2}
\end{align}
By set theory, we have that
\begin{align}
E_1\times E_2&amp;amp;=\Big(B_1\cup\dots\cup B_{k_1}\Big)\times\Big(B_1’\cup\dots\cup B_{k_2}’\Big) \\ &amp;amp;=\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right),\tag{3}\label{3}
\end{align}
which is an elementary set.&lt;/p&gt;

&lt;p&gt;Since $B_1,\dots,B_{k_1}$ are disjoint and $B_1’,\dots,B_{k_2}’$ are disjoint, the Cartesian products $B_i\times B_j’$ for $i=1,\dots,k_1$ and $j=1,\dots,k_2$ are also disjoint. From \eqref{3} and using the finite additivity property, we have that
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;amp;=m^{d_1+d_2}\Bigg(\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right)\Bigg) \\ &amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right)\tag{4}\label{4}
\end{align}
On the one hand, using the definition of boxes, and without loss of generality we can express, for each $i=1,\dots,k_1$, that:
\begin{equation}
B_i=(a_{i,1},b_{i,1})\times\dots\times(a_{i,d_1},b_{i,d_1}),
\end{equation}
where $a_{i,j},b_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_1$. Hence,
\begin{equation}
m^{d_1}(B_i)=\prod_{j=1}^{d_1}(b_{i,j}-a_{i,j}),\hspace{1cm}i=1,\dots,k_1\tag{5}\label{5}
\end{equation}
Similarly, we also have that
\begin{equation}
m^{d_2}(B_i’)=\prod_{j=1}^{d_2}(d_{i,j}-c_{i,j}),\hspace{1cm}i=1,\dots,k_2\tag{6}\label{6}
\end{equation}
where $c_{i,j},d_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_2$.&lt;/p&gt;

&lt;p&gt;Moreover, on the other hand, we also have that the $(d_1+d_2)$-dimensional box $B_i\times B_j’$ can be expressed as
\begin{equation}
B_i\times B_j’=(e_1,f_1)\times\dots\times(e_{d_1+d_2},f_{d_1+d_2}),\tag{7}\label{7}
\end{equation}
where $e_k=a_{i,k};f_k=b_{i,k}$ for all $k=1,\dots,d_1$ and $e_k=c_{j,k-d_1};f_k=d_{j,k-d_1}$ for all $k=d_1+1,\dots,d_2$.&lt;/p&gt;

&lt;p&gt;From \eqref{5}, \eqref{6} and \eqref{7}, for any $i=1,\dots,k_1$ and for any $j=1,\dots,k_2$, we have
\begin{align}
m^{d_1+d_2}(B_i\times B_j’)&amp;amp;=\prod_{k=1}^{d_1+d_2}(f_k-e_k) \\ &amp;amp;=\Bigg(\prod_{k=1}^{d_1}(b_{i,k}-a_{i,k})\Bigg)\Bigg(\prod_{k=1}^{d_2}(d_{j,k}-c_{j,k})\Bigg) \\ &amp;amp;=m^{d_1}(B_i)\times m^{d_2}(B_j’)
\end{align}
With this result, combined with \eqref{1} and \eqref{2}, equation \eqref{4} can be written as
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right) \\ &amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1}(B_i)\times m^{d_2}(B_j’) \\ &amp;amp;=m^{d_1}(E_1)\times m^{d_2}(E_2),
\end{align}
which concludes our proof.&lt;/p&gt;

&lt;h2 id=&quot;jordan-measure&quot;&gt;Jordan measure&lt;/h2&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be a bounded set.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;Jordan inner measure&lt;/strong&gt; $m_{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m_{*,(J)}(E)\doteq\sup_{A\subset E,A\text{ elementary}}m(A)
\end{equation}&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;Jordan outer measure&lt;/strong&gt; $m^{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E,B\text{ elementary}}m(B)
\end{equation}&lt;/li&gt;
  &lt;li&gt;If $m_{*,(J)}(E)=m^{*,(J)}(E)$, then we say that $E$ is &lt;strong&gt;Jordan measurable&lt;/strong&gt;, and call
\begin{equation}
m(E)\doteq m_{*,(J)}(E)=m^{*,(J)}(E)
\end{equation}
the &lt;strong&gt;Jordan measure&lt;/strong&gt; of $E$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;jordan-measurability-characterisation&quot;&gt;Characterisation of Jordan measurability&lt;/h3&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be bounded. These following statements are equivalence&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;$E$ is Jordan measurable.&lt;/li&gt;
	&lt;li&gt;For every $\varepsilon&amp;gt;0$, there exists elementary sets $A\subset E\subset B$ such that $m(B\backslash A)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;For every $\varepsilon&amp;gt;0$, there exists an elementary set $A$ such that $m^{*,(J)}(A\Delta E)\leq\varepsilon$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
In order to prove these three statements are equivalence, we will be proving that (1) implies (2); (2) implies (3); and that (2) implies (1).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) implies (2).&lt;br /&gt;
Since $E$ is Jordan measurable, we have that
\begin{equation}
m(E)=\sup_{A\subset E;A\text{ elementary}}m(A)=\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
By the definition of supremum, there exists an elementary set $A\subset E$ such that for any $\varepsilon&amp;gt;0$ 
\begin{equation}
m(A)\geq m(E)-\frac{\varepsilon}{2}\tag{8}\label{8}
\end{equation}
In addition, by the definition of infimum, there also exists an elementary set $B\supset E$ such that for any $\varepsilon&amp;gt;0$
\begin{equation}
m(B)\leq m(E)+\frac{\varepsilon}{2}\tag{9}\label{9}
\end{equation}
From \eqref{8} and \eqref{9}, we have that for any $\varepsilon&amp;gt;0$
\begin{equation}
m(B\backslash A)=m(B)-m(A)\leq\varepsilon
\end{equation}&lt;/li&gt;
  &lt;li&gt;(2) implies (3).&lt;br /&gt;
With (2) satisfied, we have that we can find elementary sets $A\subset E\subset B$ such that
\begin{equation}
m(B\backslash A)\leq\varepsilon,\hspace{1cm}\forall\varepsilon&amp;gt;0
\end{equation}
Since $A\subset E\subset B$ and by the definition of symmetric difference, we have
\begin{equation}
A\Delta E=(A\backslash E)\cup(E\backslash A)=(E\backslash A)\subset(B\backslash A)
\end{equation}
Hence
\begin{equation}
m^{*,(J)}(A\Delta E)\leq m(B\backslash A)\leq\varepsilon
\end{equation}&lt;/li&gt;
  &lt;li&gt;(2) implies (1).&lt;br /&gt;
Let $(A_n)_{n\in\mathbb{N}}$ and $(B_n)_{n\in\mathbb{N}}$ be sequences of elementary sets such that $A_n\subset E\subset B_n$ for all $n\in\mathbb{N}$. Statement (2) says that for all $\varepsilon&amp;gt;0$, there exists $i,j\in\mathbb{N}$ such that
\begin{equation}
m(B_j\backslash A_i)\leq\varepsilon
\end{equation}
or
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon\tag{10}\label{10}
\end{equation}
Let $A_\text{sup}$ and $B_\text{inf}$ be two sets in the two sequences above with
\begin{align}
m(A_\text{sup})&amp;amp;=\sup_{n\in\mathbb{N}}m(A_n), \\ m(B_\text{inf})&amp;amp;=\inf_{n\in\mathbb{N}}m(B_n),
\end{align}
which means
\begin{align}
m_{*,(J)}(E)&amp;amp;=m(A_\text{sup}) \\ m^{*,(J)}(E)&amp;amp;=m(B_\text{inf})
\end{align}
Using the monotonicity property of elementary measure, we have that
\begin{equation}
m(A_\text{sup})\leq m(B_\text{inf})
\end{equation}
Assume that $m(B_\text{inf})&amp;gt;m(A_\text{sup})$, and consider an $\varepsilon&amp;gt;0$ such that $\varepsilon&amp;lt; m(B_\text{inf})-m(A_\text{sup})$. We can continue to derive \eqref{10} as
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon&amp;lt; m(A_i)+m(B_\text{inf})-m(A_\text{sup})&amp;lt; m(B_\text{inf}),
\end{equation}
which is false with the definition of $B_\text{inf}$. Therefore, our assumption is also false, which means
\begin{equation}
m(A_\text{sup})=m(B_\text{inf})
\end{equation}
or
\begin{equation}
m_{*,(J)}(E)=m^{*,(J)}(E),
\end{equation}
or in other words, $E$ is Jordan measurable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Corollary 10&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Every elementary set $E$ is Jordan measurable.&lt;/li&gt;
  &lt;li&gt;On elementary sets, Jordan measure is elementary measure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jordan measurability also inherits many of the properties of elementary measure.&lt;/p&gt;

&lt;h3 id=&quot;jordan-measurability-properties&quot;&gt;Properties of Jordan measurability&lt;/h3&gt;
&lt;p&gt;Let $E,F\in\mathbb{R}^d$ be Jordan measurable sets. Then&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Boolean closure&lt;/b&gt;. $E\cup F,E\cap F,E\backslash F,E\Delta F$ are also Jordan measurable sets.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Non-negativity&lt;/b&gt;. $m(E)\geq 0$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite additivity&lt;/b&gt;. If $E,F$ are disjoint, then $m(E\cup F)=m(E)+m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $E\subset F$, then $m(E)\leq m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite subadditivity&lt;/b&gt;. $m(E\cup F)\leq m(E)+m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Translation invariance&lt;/b&gt;. For any $x\in\mathbb{R}^d$, $E+x$ is Jordan measurable, and $m(E+x)=m(E)$.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol id=&quot;number-list&quot;&gt;
	&lt;li&gt;&lt;b&gt;Boolean closure&lt;/b&gt;.
		&lt;ol&gt;
			&lt;li&gt;
				By characterisation of Jordan measurability, we can find elementary sets $A_1\subset E\subset B_1$ and $A_2\subset F\subset B_2$ such that for any $\varepsilon&amp;gt;0$
				\begin{align}
				m(B_1\backslash A_1)&amp;amp;\leq\frac{\varepsilon}{2}, \\ m(B_2\backslash A_2)&amp;amp;\leq\frac{\varepsilon}{2}
				\end{align}
				Thus, we have that
				\begin{equation}
				\left(A_1\cap A_2\right)\subset\left(E\cap F\right)\subset\left(B_1\cap B_2\right)
				\end{equation}
				and
				\begin{equation}
				\left(A_1\cup A_2\right)\subset\left(E\cup F\right)\subset\left(B_1\cup B_2\right)
				\end{equation}
				Moreover, for any $\varepsilon&amp;gt;0$, we have that
				\begin{align}
				m\big((B_1\cup B_2)\backslash(A_1\cup A_2)\big)&amp;amp;=m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)+m(B_2\backslash B_1)-m(A_1\cup A_2) \\ &amp;amp;\leq m(B_1)+m(B_2\backslash A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)-m(A_1)+m(B_2\backslash A_1)+m(A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)-m(A_1)+m(B_2\cup A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1\backslash A_1)+m\big((B_2\cup A_1)\backslash(A_1\cup A_2)\big) \\ &amp;amp;=m(B_1\backslash A_1)+m(B_2\backslash A_2) \\ &amp;amp;\leq\varepsilon/2+\varepsilon/2 \\ &amp;amp;=\varepsilon,
				\end{align}
				which implies that $E\cup F$ is Jordan measurable.
			&lt;/li&gt;
			&lt;li&gt;
				From the result above, and by monotonicity, finite additivity, finite subadditivity properties of elementary measure, for any $\varepsilon&amp;gt;0$, we also have that
				\begin{align}
				m\big((B_1\cap B_2)\backslash(A_1\cap A_2)\big)&amp;amp;=m(B_1\cap B_2)-m(A_1\cap A_2) \\ &amp;amp;=m\Big(\big(B_1\cup B_2\big)\backslash\big((B_1\backslash B_2)\cup(B_2\backslash B_1)\big)\Big) \\ &amp;amp;\hspace{1cm}-m\Big(\big(A_1\cup A_2\big)\backslash\big((A_1\backslash A_2)\cup(A_2\backslash A_1)\big)\Big) \\ &amp;amp;=m(B_1\cup B_2)-m(B_1\backslash B_2)-m(B_2\backslash B_1) \\ &amp;amp;\hspace{1cm}-m(A_1\cup A_2)+m(A_1\backslash A_2)+m(A_2\backslash A_1) \\ &amp;amp;=m(B_1\cup B_2)-m(A_1\cup A_2)+m(A_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;amp;\hspace{1cm}+m(A_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2)+m(B_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;amp;\hspace{1cm}+m(B_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;amp;\leq\varepsilon,
				\end{align}
				which also implies that $E\cap F$ is Jordan measurable.
			&lt;/li&gt;
		&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Non-negativity&lt;/b&gt;. This follows directly from definition.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite additivity&lt;/b&gt;.
	&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Example 4&lt;/strong&gt;. (&lt;strong&gt;Uniqueness of Jordan measure&lt;/strong&gt;)&lt;br /&gt;
Let $d\geq 1$ and let $m’:\mathcal{J}(\mathbb{R}^d)\to\mathbb{R}^+$  be a map from the collection of Jordan measurable subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all Jordan measurable sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Follow the same steps as the solution for &lt;strong&gt;Example 2&lt;/strong&gt;, the argument above can easily be proved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 5&lt;/strong&gt;&lt;br /&gt;
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be Jordan measurable sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also Jordan measurable, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;. (&lt;strong&gt;Carathéodory type property&lt;/strong&gt;)&lt;br /&gt;
Let $E\subset\mathbb{R}^d$ be a bounded set, and $F\subset\mathbb{R}^d$ be an elementary set. Then we have that
\begin{equation}
m^{*,(J)}(E)=m^{*,(J)}(E\cap F)+m^{*,(J)}(E\backslash F)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;connect-riemann-int&quot;&gt;Connection with the Riemann integral&lt;/h2&gt;

&lt;h3 id=&quot;riemann-integrability&quot;&gt;Riemann integrability&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval of positive length, and $f:[a,b]\to\mathbb{R}$ be a function. A &lt;strong&gt;tagged partition&lt;/strong&gt;
\begin{equation}
\mathcal{P}=\left(\left(x_0,x_1,\dots,x_n\right),\left(x_1^{*},\dots,x_n^{*}\right)\right)
\end{equation}
of $[a,b]$ is a finite sequence of real numbers $a=x_0&amp;lt; x_1&amp;lt;\dots&amp;lt; x_n=b$, together with additional numbers $x_{i-1}\leq x_i^{*}\leq x_i$ for each $i=1,\dots,n$. Let $\delta x_i\doteq x_i-x_{i-1}$, the quantity
\begin{equation}
\Delta(\mathcal{P})\doteq\sup_{1\leq i\leq n}\delta x_i
\end{equation}
is called the &lt;strong&gt;norm&lt;/strong&gt; of the tagged partition. The &lt;strong&gt;Riemann sum&lt;/strong&gt; $\mathcal{R}(f,\mathcal{P})$ of $f$ w.r.t the tagged partition $\mathcal{P}$ is defined as
\begin{equation}
\mathcal{R}(f,\mathcal{P})\doteq\sum_{i=1}^{n}f(x_i^{*})\delta x_i
\end{equation}
we say that $f$ is &lt;strong&gt;Riemann integrable&lt;/strong&gt; on $[a,b]$ if there exists a real number, denoted as $\int_{a}^{b}f(x)\,dx$ and referred to as the &lt;strong&gt;Riemann integral&lt;/strong&gt; on $[a,b]$, for which we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=\lim_{\Delta\mathcal{P}\to 0}\mathcal{R}(f,\mathcal{P}),
\end{equation}
by which we mean that for every $\varepsilon&amp;gt;0$ there exists $\delta&amp;gt;0$ such that
\begin{equation}
\left\vert\mathcal{R}(f,\mathcal{P})-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon,
\end{equation}
for every tagged partition $\mathcal{P}$ with $\Delta(\mathcal{P})\leq\delta$.&lt;/p&gt;

&lt;h3 id=&quot;pc-func&quot;&gt;Piecewise constant functions&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval. a &lt;strong&gt;piecewise constant function&lt;/strong&gt; $f:[a,b]\to\mathbb{R}$ is a function for which there exists a partition of $[a,b]$ into infinitely many intervals $I_1,\dots,I_n$ such that $f$ is equal to a constant $c_i$ on each of the intervals $I_i$. Then, the expression
\begin{equation}
\sum_{i=1}^{n}c_i\vert I_i\vert
\end{equation}
is independent of the choice of partition used to demonstrate the piecewise constant nature of $f$. We denote this quantity as $\text{p.c.}\int_{a}^{b}f(x)\,dx$, and refer it to as &lt;strong&gt;piecewise constant integral&lt;/strong&gt; of $f$ on $[a,b]$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider two partitions of the interval $[a,b]$ into finitely many intervals $(I_i)_{i=1,\ldots,n}=I_1,\ldots,I_n$ and $(J_i)_{i=1,\ldots,m}=J_1,\ldots,J_m$ such that:
\begin{align}
f(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ f(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in J_i
\end{align}
Thus, we have that:
\begin{equation}
c_i=d_j,\hspace{1cm}\forall x\in\left(I_i\cap J_j\right)
\end{equation}
With this result, we have:
\begin{align}
\sum_{i=1}^{n}c_i\vert I_i\vert&amp;amp;=\sum_{i=1}^{n}c_i\left\vert\bigcup_{j=1}^{m}\left(I_i\cap J_j\right)\right\vert \\ &amp;amp;=\sum_{i=1}^{n}\sum_{j=1}^{m}c_i\left\vert I_i\cap J_j\right\vert \\ &amp;amp;=\sum_{j=1}^{m}\sum_{i=1}^{n}d_j\left\vert I_i\cap J_j\right\vert \\ &amp;amp;=\sum_{j=1}^{m}d_j\left\vert\bigcup_{i=1}^{n}\left(J_j\cap I_i\right)\right\vert \\ &amp;amp;=\sum_{j=1}^{m}d_j\vert J_j\vert,
\end{align}
which claims the independence of the choices of partition of $f$.&lt;/p&gt;

&lt;h4 id=&quot;pc-int-properties&quot;&gt;Basic properties of piecewise constant integral&lt;/h4&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be piecewise constant functions. Then&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are piecewise constant functions, with
		\begin{align}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx&amp;amp;=c\text{p.c.}\int_{a}^{b}f(x)\,dx \\\\ \text{p.c.}\int_{a}^{b}\left(f(x)+g(x)\right)\,dx&amp;amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $f\leq g$ pointwise, i.e., $f(x)\leq g(x),\forall x\in[a,b]$, then
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx\leq\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;. If $E$ is an elementary subset of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ (defined by setting $1_E(x)\doteq 1$ if $x\in E$ and 0 otherwise) is piecewise constant, and
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;&lt;br /&gt;
		For any $c\in\mathbb{R}$, we have:
		\begin{equation}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx=\sum_{i=1}^{n}cc_i\vert I_i\vert=c\sum_{i=1}^{n}c_i\vert I_i\vert=c\text{p.c.}\int_{a}^{b}f(x)\,dx
		\end{equation}
		From the partitioning independence of piecewise constant functions, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{equation}
		f(x)=c_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		and
		\begin{equation}
		g(x)=d_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		Thus, we have
		\begin{align}
		\text{p.c.}\int_{a}^{b}f(x)+g(x)\,dx&amp;amp;=\sum_{i=1}^{n}\left(c_i+d_i\right)\vert I_i\vert \\ &amp;amp;=\sum_{i=1}^{n}c_i\vert I_i\vert+\sum_{i=1}^{n}d_i\vert I_i\vert \\ &amp;amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;&lt;br /&gt;
		Analogy to the above proof, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{align}
		f(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ g(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in I_i,
		\end{align}
		Since $f\leq g$ pointwise, in any interval $I_i$, we also have that $c_i=f(x)\leq g(x)=d_i$. Therefore,
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx=\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert=\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;&lt;br /&gt;
		Since $E\subset[a,b]\subset\mathbb{R}$ is an elementary set, we can represent the elementary measure $m(E)$ of set $E$ as
		\begin{equation}
		m(E)=\sum_{i=1}^{n}\vert I_i\vert
		\end{equation}
		Therefore, for any $x\in I_i$ for $i=1,\ldots n$, we have that $1_E(x)=1$; and for any $x\in[b-a]\backslash E=\bigcup_{j=1}^{m}J_j$, we get that $1_E(x)=0$, which lets $1_E$ satisfy the condition of a piecewise constant function.&lt;br /&gt;
		Moreover, we have that
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=\sum_{i=1}^{n}1\vert I_i\vert+\sum_{j=1}^{m}0\vert J_j\vert=\sum_{i=1}^{n}\vert I_i\vert=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;darboux-int&quot;&gt;Darboux integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an integral, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. The &lt;strong&gt;lower Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$, denoted as $\underline{\int_{a}^{b}}f(x)\,dx$, is defined as
\begin{equation}
\underline{\int_a^b}f(x)\,dx\doteq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx,
\end{equation}
where $g$ ranges over all piecewise constant functions that are pointwise bounded above by $f$ (the hypothesis that $f$ is bounded ensures that the supremum is over a non-empty set).&lt;/p&gt;

&lt;p&gt;Similarly, we can define the &lt;strong&gt;upper Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$, denoted as $\overline{\int_a^b}f(x)\,dx$, as
\begin{equation}
\overline{\int_a^b}f(x)\,dx\doteq\inf_{h\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx
\end{equation}
It is easily seen that $\underline{\int_a^b}f(x)\,dx\leq\overline{\int_a^b}f(x)\,dx$. The equality holds when $f$ is &lt;strong&gt;Darboux integrable&lt;/strong&gt;, and we refer to this quantity as &lt;strong&gt;Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$.&lt;/p&gt;

&lt;p&gt;Note that the upper and lower Darboux integrals are related by
\begin{equation}
\overline{\int_a^b}-f(x)\,dx=-\underline{\int_a^b}f(x)\,dx
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;equiv-riemann-darboux-int&quot;&gt;Equivalence of Riemann integral and Darboux integral&lt;/h4&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff it is Darboux integrable, in which case the Riemann integrals and Darboux integrals are the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given $f$ is Darboux integrable on $[a,b]$, we have that the upper and lower Darboux integrals are equal, and equal to the Darboux integral of $f$ on $[a,b]$ which we denote as $\text{d.}\int_{a}^{b}f(x)\,dx\in\mathbb{R}$.
\begin{equation}
\underline{\int_a^b}f(x)\,dx=\overline{\int_a^b}f(x)\,dx=\text{d.}\int_{a}^{b}f(x)\,dx
\end{equation}
By definition of the lower Darboux integral, there exists a piecewise constant function $g(x)$ bounded above by $f$ (i.e., $g\leq f$ piecewise), such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;gt;\underline{\int_{a}^{b}}f(x)\,dx-\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon\tag{11}\label{11}
\end{equation}
Likewise, by definition of the upper Darboux integral, there exists a piecewise constant function $h(x)$ bounded below by $f$ (i.e., $h\geq f$ piecewise), such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}h(x)\,dx&amp;lt;\overline{\int_{a}^{b}}f(x)\,dx+\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon\tag{12}\label{12}
\end{equation}
From the independence of choice of partition of piecewise constant functions $g$ and $h$, there exists a partition $I_1,\ldots,I_n$ such that
\begin{align}
g(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ h(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in I_i
\end{align}
and
\begin{align}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;amp;=\sum_{i=1}^{n}c_i\vert I_i\vert,\tag{13}\label{13} \\ \text{p.c.}\int_{a}^{b}h(x)\,dx&amp;amp;=\sum_{i=1}^{n}d_i\vert I_i\vert,\tag{14}\label{14}
\end{align}
then it follows immediately that $c_i\leq d_i$. And since $g\leq f\leq h$ piecewise, in any interval $I_i$, we can find a $x_i^*$ such that $c_i\leq f(x_i^*)\leq d_i$. Additionally, combining with \eqref{11}, \eqref{12}, \eqref{13} and \eqref{14}, we have that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon&amp;lt;\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert&amp;lt;\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon
\end{equation}
Therefore, for any $\varepsilon&amp;gt;0$, we have
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\text{d.}\int_{a}^{b}f(x)\,dx\right\vert&amp;lt;\varepsilon,
\end{equation}
which claims that $f$ is Riemann integrable on $[a,b]$ with $\text{d.}\int_{a}^{b}f(x)\,dx$ is the Riemann integral of $f$.&lt;/li&gt;
  &lt;li&gt;Given $f$ is Riemann integrable on $[a,b]$, for any $\varepsilon&amp;gt;0$, there exists a partition $\mathcal{P}=((x_0,x_1,\ldots,x_n),(x_1^*,\ldots,x_n^*))$ of $[a,b]$ with $0=x_0&amp;lt; x_1&amp;lt;\ldots&amp;lt; x_n=b$ and $x_{i-1}\leq x_i^*\leq x_i$ such that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\delta x_i-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
Any continuous function $f:[a,b]\to\mathbb{R}$ is Riemann integrable. More generally, any bounded, piecewise continuous function&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; $f:[a,b]\to\mathbb{R}$ is Riemann integrable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;riemann-int-properties&quot;&gt;Basic properties of Riemann integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be Riemann integrable. We then have that&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are Riemann integrable, with
		\begin{align}
		\int_{a}^{b}cf(x)\,dx&amp;amp;=c\int_{a}^{b}f(x)\,dx \\\\ \int_{a}^{b}\big(f(x)+g(x)\big)\,dx&amp;amp;=\int_{a}^{b}f(x)\,dx+\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $f\leq g$ pointwise, then
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;. If $E$ is a Jordan measurable of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ is Riemann integrable, and
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These properties uniquely define the Riemann integral, in the sense that the function $f\mapsto\int_{a}^{b}f(x)\,dx$ is the only map from the space of Riemann integrable functions on $[a,b]$ to $\mathbb{R}$ which obeys all of these above properties.&lt;/p&gt;

&lt;h3 id=&quot;riemann-int-area-interpret&quot;&gt;Area interpretation of the Riemann integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff the sets $E_+\doteq\{(x,t):x\in[a,b];0\leq t\leq f(x)\}$ and $E_-\doteq\{(x,t):x\in[a,b];f(x)\leq t\leq 0\}$ are both Jordan measurable in $R^2$, in which case we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=m^2(E_+)-m^(E_-),
\end{equation}
where $m^2$ denotes two-dimensional Jordan measure.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;taos-book&quot;&gt;Terence Tao. &lt;a href=&quot;https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/&quot;&gt;An introduction to measure theory&lt;/a&gt;. Graduate Studies in Mathematics, vol. 126.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;span id=&quot;steins-book&quot;&gt;Elias M. Stein &amp;amp; Rami Shakarchi. &lt;a href=&quot;#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf&quot;&gt;Real Analysis: Measure Theory, Integration, and Hilbert Spaces&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function $f:[a,b]\to\mathbb{R}$ is &lt;strong&gt;piecewise continuous&lt;/strong&gt; if we can partition $[a,b]$ into finitely many intervals, such that $f$ is continuous on each interval. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="random-stuffs" /><summary type="html">A note on measure theory: materials were mostly taken from Tao’s book, except for some notations needed from Stein’s book.</summary></entry><entry><title type="html">Monte Carlo Tree Search</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html" rel="alternate" type="text/html" title="Monte Carlo Tree Search" /><published>2022-05-25T13:00:00+07:00</published><updated>2022-05-25T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html">&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Monte Carlo Tree Search (MCTS)&lt;/strong&gt; is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#mcts-vanilla&quot;&gt;(Vanilla) Monte Carlo Tree Search&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uct&quot;&gt;Upper Confidence Bound for Trees (UCT)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#alphazero&quot;&gt;AlphaZero&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vanilla-mcts&quot;&gt;(Vanilla) Monte Carlo Tree Search&lt;/h2&gt;

&lt;h2 id=&quot;uct&quot;&gt;Upper Confidence Bound for Trees (UCT)&lt;/h2&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;h2 id=&quot;alphazero&quot;&gt;AlphaZero&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] C. B. Browne et al. &lt;a href=&quot;https://ieeexplore.ieee.org/document/6145622&quot;&gt;A Survey of Monte Carlo Tree Search Methods&lt;/a&gt;, in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012.&lt;/p&gt;

&lt;p&gt;[3] Kocsis, L. &amp;amp; Szepesvári, C. (2006). &lt;a href=&quot;https://doi.org/10.1007/11871842_29&quot;&gt;Bandit Based Monte-Carlo Planning&lt;/a&gt;. In: Fürnkranz, J., Scheffer, T., Spiliopoulou, M. (eds) Machine Learning: ECML 2006. ECML 2006. Lecture Notes in Computer Science, vol 4212. Springer, Berlin, Heidelberg.&lt;/p&gt;

&lt;p&gt;[4] David Silver &amp;amp; Julian Schrittwieser &amp;amp; Karen Simonyan et al. &lt;a href=&quot;https://doi.org/10.1038/nature24270&quot;&gt;Mastering the game of Go without human knowledge&lt;/a&gt;. Nature 550, 354–359 (2017).&lt;/p&gt;

&lt;p&gt;[5] David Silver &amp;amp; Thomas Hubert &amp;amp; Julian Schrittwieser et al. &lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot;&gt;Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm&lt;/a&gt;. arXiv.&lt;/p&gt;

&lt;p&gt;[6] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="monte-carlo" /><category term="mcts" /><category term="uct" /><category term="planning" /><summary type="html">Monte Carlo Tree Search (MCTS) is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.</summary></entry><entry><title type="html">Planning &amp;amp; Learning</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning.html" rel="alternate" type="text/html" title="Planning &amp;amp; Learning" /><published>2022-05-19T14:09:00+07:00</published><updated>2022-05-19T14:09:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning.html">&lt;blockquote&gt;
  &lt;p&gt;Recall that when using &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html&quot;&gt;dynamic programming (DP) method&lt;/a&gt; in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html&quot;&gt;Monte Carlo methods&lt;/a&gt; and &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html&quot;&gt;temporal-difference learning&lt;/a&gt;, the models are unnecessary. Such methods with requirement of a model like the case of DP is called &lt;strong&gt;model-based&lt;/strong&gt;, while methods without using a model is called &lt;strong&gt;model-free&lt;/strong&gt;. Model-based methods primarily rely on &lt;strong&gt;planning&lt;/strong&gt;; and model-free methods, on the other hand, primarily rely on &lt;strong&gt;learning&lt;/strong&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#models-planning&quot;&gt;Models &amp;amp; Planning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#models&quot;&gt;Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#planning&quot;&gt;Planning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dyna&quot;&gt;Dyna&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dyna-q&quot;&gt;Dyna-Q&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dyna-q-eg&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dyna-q-plus&quot;&gt;Dyna-Q+&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prioritized-sweeping&quot;&gt;Prioritized Sweeping&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#small-backups&quot;&gt;Small backups&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#heuristic-search&quot;&gt;Heuristic Search&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preferences&quot;&gt;Preferences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models-planning&quot;&gt;Models &amp;amp; Planning&lt;/h2&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.&lt;/p&gt;

&lt;p&gt;When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a model produces a description of all possibilities and their probabilities, we call it &lt;strong&gt;distribution model&lt;/strong&gt;. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.&lt;/li&gt;
  &lt;li&gt;On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it &lt;strong&gt;sample model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to &lt;strong&gt;simulate&lt;/strong&gt; the environment in order to produce &lt;strong&gt;simulated experience&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;planning&quot;&gt;Planning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Planning&lt;/strong&gt; in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;State-space planning&lt;/strong&gt; is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:
    &lt;ul&gt;
      &lt;li&gt;Involving computing value functions as a key intermediate step toward improving the policy.&lt;/li&gt;
      &lt;li&gt;Computing value functions by updates or backup applied to simulated experience.
  \begin{equation}
  \text{model}\xrightarrow[]{\hspace{1.5cm}}\text{simulated experience}\xrightarrow[]{\hspace{0.3cm}\text{backups}\hspace{0.3cm}}\text{backups}\xrightarrow[]{\hspace{1.5cm}}\text{policy}
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plan-space planning&lt;/strong&gt; is a search through the space of plans.
    &lt;ul&gt;
      &lt;li&gt;Plan-space planning methods consist of &lt;strong&gt;evolutionary methods&lt;/strong&gt; and &lt;strong&gt;partial-order planning&lt;/strong&gt;, in which the ordering of steps is not completely determined at all states of planning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.&lt;/p&gt;

&lt;p&gt;For instance, following is pseudocode of a planning method, called &lt;strong&gt;random-sample one-step tabular Q-planning&lt;/strong&gt;, based on &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#q-learning&quot;&gt;one-step tabular Q-learning&lt;/a&gt;, and on random samples from a sample model.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/rand-samp-one-step-q-planning.png&quot; alt=&quot;Random-sample one-step Q-planning&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;dyna&quot;&gt;Dyna&lt;/h2&gt;
&lt;p&gt;Within a planning agent, experience plays at least two roles:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;model learning&lt;/strong&gt;: improving the model;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;direct reinforcement learning (RL)&lt;/strong&gt;: improving the value function and policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The figure below illustrates the possible relationships between experience, model, value functions and policy.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/exp-model-value-policy.png&quot; alt=&quot;Exp, model, values and policy relationships&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 300px; height: 250px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: The possible relationships between experience, model, values and policy&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called &lt;strong&gt;indirect RL&lt;/strong&gt;), which involved in planning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;direct RL: simpler, not affected by bad models;&lt;/li&gt;
  &lt;li&gt;indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dyna-q&quot;&gt;Dyna-Q&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; is the method having all of the processes shown in the diagram in &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt; - planning, acting, model-learning and direct RL - all occurring continually:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the &lt;em&gt;planning&lt;/em&gt; method is the random-sample one-step tabular Q-planning in the previous section;&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;direct RL&lt;/em&gt; method is the one-step tabular Q-learning;&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;model-learning&lt;/em&gt; method is also table-based and assumes the environment is deterministic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After each transition $S_t,A_t\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.&lt;/p&gt;

&lt;p&gt;During planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.&lt;/p&gt;

&lt;p&gt;Following is the general architecture of Dyna methods, of which Dyna-Q is an instance.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-arch.png&quot; alt=&quot;Dyna architecture&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 400px; height: 320px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: The general Dyna Architecture&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.&lt;/p&gt;

&lt;p&gt;Pseudocode of Dyna-Q method is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/tabular-dyna-q.png&quot; alt=&quot;Tabular Dyna-Q&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;dyna-q-eg&quot;&gt;Example&lt;/h4&gt;
&lt;p&gt;(This example is taken from &lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt; - example 8.1.)&lt;/p&gt;

&lt;p&gt;Consider a gridworld with some obstacles, called “maze” in this example, shown in the figure below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/dyna-maze.png&quot; alt=&quot;Dyna maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 400px; height: 200px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: The maze with some obstacles&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As usual, four action, $\text{up}, \text{down}, \text{right}$ and $\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state.&lt;/p&gt;

&lt;p&gt;The task is discounted, episodic with $\gamma=0.95$.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-maze-dyna-q.png&quot; alt=&quot;Dyna maze solved with Dyna-Q&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: Using Dyna-Q with different setting of number of planning steps on the maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-8/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;dyna-q-plus&quot;&gt;Dyna-Q+&lt;/h3&gt;
&lt;p&gt;Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/blocking-maze.png&quot; alt=&quot;Blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: The maze before and after change&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps.&lt;/p&gt;

&lt;p&gt;In this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an &lt;strong&gt;exploration bonus&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment.&lt;/li&gt;
  &lt;li&gt;An special &lt;strong&gt;bonus reward&lt;/strong&gt; is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting:
\begin{equation}
r+\kappa\sqrt{\tau},
\end{equation}
for a small (time weight) $\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\tau$ time steps.&lt;/li&gt;
  &lt;li&gt;The agent actually plans how to visit long unvisited state-action pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/blocking-maze-dyna-q-qplus.png&quot; alt=&quot;Dyna-Q, Dyna-Q+ on blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Average performance of Dyna-Q and Dyna-Q+ on blocking maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-8/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/shortcut-maze.png&quot; alt=&quot;shortcut maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: The maze before and after change&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/shortcut-maze-dyna-q-qplus.png&quot; alt=&quot;Dyna-Q, Dyna-Q+ on blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 8&lt;/b&gt;: Average performance of Dyna-Q and Dyna-Q+ on shortcut maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-8/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).&lt;/p&gt;

&lt;p&gt;The reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.&lt;/p&gt;

&lt;h2 id=&quot;prioritized-sweeping&quot;&gt;Prioritized Sweeping&lt;/h2&gt;
&lt;p&gt;Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.&lt;/p&gt;

&lt;p&gt;Pseudocode of prioritized sweeping is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/prioritized-sweeping.png&quot; alt=&quot;Prioritized sweeping&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-maze-prioritized-sweeping.png&quot; alt=&quot;Prioritized sweeping on dyna maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 9&lt;/b&gt;: Using prioritized sweeping on mazes.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-8/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/h2&gt;

&lt;h2 id=&quot;heuristic-search&quot;&gt;Heuristic Search&lt;/h2&gt;

&lt;h2 id=&quot;preferences&quot;&gt;Preferences&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;rl-book&quot;&gt;Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;[2] Richard S. Sutton. &lt;a href=&quot;https://doi.org/10.1016/B978-1-55860-141-3.50030-4&quot;&gt;Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming&lt;/a&gt;. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.&lt;/p&gt;

&lt;p&gt;[3] Harm van Seijen &amp;amp; Richard S. Sutton. &lt;a href=&quot;https://proceedings.mlr.press/v28/vanseijen13.pdf&quot;&gt;Efficient planning in MDPs by small backups&lt;/a&gt;. Proceedings
of the 30th International Conference on Machine Learning (ICML 2013).&lt;/p&gt;

&lt;p&gt;[3] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="planning" /><category term="learning" /><category term="dyna" /><category term="q-learning" /><category term="mcts" /><category term="my-rl" /><summary type="html">Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.</summary></entry><entry><title type="html">Policy Gradient Methods</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient.html" rel="alternate" type="text/html" title="Policy Gradient Methods" /><published>2022-05-04T14:00:00+07:00</published><updated>2022-05-04T14:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient.html">&lt;blockquote&gt;
  &lt;p&gt;So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a &lt;strong&gt;parameterized policy&lt;/strong&gt;, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called &lt;strong&gt;policy gradient methods&lt;/strong&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-grad-ep&quot;&gt;Policy Gradient for Episodic Problems&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-grad-theorem-ep&quot;&gt;The Policy Gradient Theorem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforce&quot;&gt;REINFORCE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforce-baseline&quot;&gt;REINFORCE with Baseline&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#actor-critic-methods&quot;&gt;Actor-Critic Methods&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-grad-cont&quot;&gt;Policy Gradient for Continuing Problems&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-grad-theorem-cont&quot;&gt;The Policy Gradient Theorem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-prm-cont-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-grad-ep&quot;&gt;Policy Gradient for Episodic Problems&lt;/h2&gt;
&lt;p&gt;We begin by considering episodic case, for which we define the performance measure $J(\boldsymbol{\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have:
\begin{equation}
J(\boldsymbol{\theta})\doteq v_{\pi_\boldsymbol{\theta}}(s_0),
\end{equation}
where $v_{\pi_\boldsymbol{\theta}}$ is the true value function for $\pi_\boldsymbol{\theta}$, the policy determined by $\boldsymbol{\theta}$.&lt;/p&gt;

&lt;h3 id=&quot;policy-grad-theorem-ep&quot;&gt;The Policy Gradient Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
The policy gradient theorem for the episodic case establishes that
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}),\tag{1}\label{1}
\end{equation}
where $\pi$ represents the policy corresponding to parameter vector $\boldsymbol{\theta}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written in terms of the action-value function, for any $s\in\mathcal{S}$, as:
\begin{align}
\nabla_\boldsymbol{\theta}v_\pi(s)&amp;amp;=\nabla_\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(s|a)q_\pi(a,s)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s’,r}p(s’,r|s,a)\big(r+v_\pi(s’)\big)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\sum_{a’}\big(\nabla_\boldsymbol{\theta}\pi(s’|a’,\boldsymbol{\theta})q_\pi(s’,a’) \\ &amp;amp;\hspace{2cm}+\pi(a’|s’,\boldsymbol{\theta})\sum_{s&apos;&apos;}p(s&apos;&apos;\vert s’,a’)\nabla_\boldsymbol{\theta}v_\pi(s&apos;&apos;)\big)\Big] \\ &amp;amp;=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty}P(s\to x,k,\pi)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
After repeated unrolling as in the fifth step, where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$. It is then immediate that:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}v_\pi(s_0) \\ &amp;amp;=\sum_s\Big(\sum_{k=0}^{\infty}P(s_0\to s,k,\pi)\Big)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_s\eta(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_{s’}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_{s’}\eta(s’)\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\propto\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode:
\begin{equation}
\eta(s)=h(s)+\sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|s,\boldsymbol{\theta})p(s|\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
where $h(s)$ denotes the probability that an episode begins in each state $s$; $\bar{s}$ denotes a preceding state of $s$. This leads to the result that we have used in the fifth step:
\begin{equation}
\mu(s)=\frac{\eta(s)}{\sum_{s’}\eta(s’)},\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;
&lt;p&gt;Notice that in &lt;strong&gt;Theorem 1&lt;/strong&gt;, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\mu(s)$) under the target policy $\pi$. Therefore, we can rewrite \eqref{1} as:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}) \\ &amp;amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right]\tag{2}\label{2}
\end{align}
Using SGD on maximizing $J(\boldsymbol{\theta})$ gives us the update rule:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\sum_a\hat{q}(S_t,a,\mathbf{w})\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta}),
\end{equation}
where $\hat{q}$ is some learned approximation to $q_\pi$ with $\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called &lt;strong&gt;all-actions&lt;/strong&gt; method because its update involves all of the actions.&lt;/p&gt;

&lt;p&gt;Continue our derivation in \eqref{2}, we have:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right] \\ &amp;amp;=\mathbb{E}_\pi\left[\sum_a\pi(a|S_t,\boldsymbol{\theta})q_\pi(S_t,a)\frac{\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})}{\pi(a|S_t,\boldsymbol{\theta})}\right] \\ &amp;amp;=\mathbb{E}_\pi\left[q_\pi(S_t,A_t)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right] \\ &amp;amp;=\mathbb{E}_\pi\left[G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right],
\end{align}
where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\sim\pi$; and in the fourth step, we have used the identity
\begin{equation}
\mathbb{E}_\pi\left[G_t|S_t,A_t\right]=q_\pi(S_t,A_t)
\end{equation}
With this gradient, we have the SGD update for time step $t$, called the &lt;strong&gt;REINFORCE&lt;/strong&gt; update, is then:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{3}\label{3}
\end{equation}
Pseudocode of the algorithm is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/reinforce.png&quot; alt=&quot;REINFORCE&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The vector
\begin{equation}
\frac{\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})}{\pi(a|s,\boldsymbol{\theta})}=\nabla_\boldsymbol{\theta}\ln\pi(a|s,\boldsymbol{\theta})
\end{equation}
in \eqref{3} is called the &lt;strong&gt;eligibility vector&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Consider using &lt;strong&gt;soft-max in action preferences&lt;/strong&gt; with linear action preferences, which means that:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\dfrac{\exp\Big[h(s,a,\boldsymbol{\theta})\Big]}{\sum_b\exp\Big[h(s,b,\boldsymbol{\theta})\Big]},
\end{equation}
where the preferences $h(s,a,\boldsymbol{\theta})$ is defined as:
\begin{equation}
h(s,a,\boldsymbol{\theta})=\boldsymbol{\theta}^\intercal\mathbf{x}(s,a)
\end{equation}
Using the chain rule we can rewrite the eligibility vector as:
\begin{align}
\nabla_\boldsymbol{\theta}\ln\pi(a|s,\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}\ln{\frac{\exp\Big[\boldsymbol{\theta}^\intercal\mathbf{x}(s,a)\Big]}{\sum_b\exp\Big[\boldsymbol{\theta}^\intercal\mathbf{x}(s,b)\Big]}} \\ &amp;amp;=\nabla_\boldsymbol{\theta}\Big(\boldsymbol{\theta}^\intercal\mathbf{x}(s,a)\Big)-\nabla_\boldsymbol{\theta}\ln\sum_b\exp\Big[\boldsymbol{\theta}^\intercal\mathbf{x}(s,b)\Big] \\ &amp;amp;=\mathbf{x}(s,a)-\dfrac{\sum_b\exp\Big[\boldsymbol{\theta}^\intercal\mathbf{x}(s,b)\Big]\mathbf{x}(s,b)}{\sum_{b’}\exp\Big[\boldsymbol{\theta}^\intercal\mathbf{x}(s,b’)\Big]} \\ &amp;amp;=\mathbf{x}(s,a)-\sum_b\pi(b|s,\boldsymbol{\theta})\mathbf{x}(s,b)
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;reinforce-baseline&quot;&gt;REINFORCE with Baseline&lt;/h3&gt;
&lt;p&gt;The policy gradient theorem \eqref{1} can be generalized to include a comparison of the action value to an arbitrary &lt;em&gt;baseline&lt;/em&gt; $b(s)$:
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a\Big(q_\pi(s,a)-b(s)\Big)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})\tag{4}\label{4}
\end{equation}
The baseline can be any function, even a r.v, as long as it is independent with $a$. The equation is valid because:
\begin{align}
\sum_a b(s)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})&amp;amp;=b(s)\nabla_\boldsymbol{\theta}\sum_a\pi(a|s,\boldsymbol{\theta}) \\ &amp;amp;=b(s)\nabla_\boldsymbol{\theta}1=0
\end{align}
Using the derivation steps analogous to REINFORCE, we end up with another version of REINFORCE that includes a general baseline:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\Big(G_t-b(s)\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{5}\label{5}
\end{equation}
One natural baseline choice is the estimate of the state value, $\hat{v}(S_t,\mathbf{w})$, with $\mathbf{w}\in\mathbb{R}^d$ is the weight vector of its. Using this baseline, we have pseudocode of the generalization with baseline of REINFORCE algorithm \eqref{5} given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/reinforce-baseline.png&quot; alt=&quot;REINFORCE with Baseline&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;actor-critic-methods&quot;&gt;Actor-Critic Methods&lt;/h3&gt;
&lt;p&gt;In Reinforcement Learning, methods that learn both policy and value function at the same time are called &lt;strong&gt;actor-critic methods&lt;/strong&gt;, in which &lt;strong&gt;actor&lt;/strong&gt; refers to the learned policy and &lt;strong&gt;critic&lt;/strong&gt; is a reference to the learned value function. Although the REINFORCE with Baseline method in the previous section learns both policy and value function, but it is not an actor-critic method. Because its state-value function is used as a baseline, not as a critic, which is used for bootstrapping.&lt;/p&gt;

&lt;p&gt;We begin by considering one-step actor-critic methods. One-step actor-critic methods replace the full return, $G_t$, of REINFORCE \eqref{5} with the one-step return, $G_{t:t+1}$:
\begin{align}
\boldsymbol{\theta}_{t+1}&amp;amp;\doteq\boldsymbol{\theta}_t+\alpha\Big(G_{t:t+1}-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{6}\label{6} \\ &amp;amp;=\boldsymbol{\theta}_t+\alpha\Big(R_{t+1}+\hat{v}(S_{t+1},\mathbf{w})-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})} \\ &amp;amp;=\boldsymbol{\theta}_t+\alpha\delta_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}
\end{align}
The natural state-value function learning method to pair with this is semi-gradient TD(0), which produces the pseudocode given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/one-step-actor-critic.png&quot; alt=&quot;One-step Actor-Critic&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To generalize the one-step methods to the forward view of $n$-step methods and then to $\lambda$-return, in \eqref{6}, we simply replace the one-step return, $G_{t+1}$, by the $n$-step return, $G_{t:t+n}$, and the $\lambda$-return, $G_t^\lambda$, respectively.&lt;/p&gt;

&lt;p&gt;In order to obtain the backward view of the $\lambda$-return algorithm, we use separately eligible traces for the actor and critic, as in the pseudocode given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/actor-critic-eligible-traces.png&quot; alt=&quot;Actor-Critic with Eligible Traces&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;policy-grad-cont&quot;&gt;Policy Gradient with Continuing Problems&lt;/h2&gt;
&lt;p&gt;In the continuing tasks, we define the performance measure in terms of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#avg-reward&quot;&gt;average-reward&lt;/a&gt;, as:
\begin{align}
J(\boldsymbol{\theta})\doteq r(\pi)&amp;amp;\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}\Big[R_t\big|S_0,A_{0:1}\sim\pi\Big] \\ &amp;amp;=\lim_{t\to\infty}\mathbb{E}\Big[R_t|S_0,A_{0:1}\sim\pi\Big] \\ &amp;amp;=\sum_s\mu(s)\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)r,\tag{7}\label{7}
\end{align}
where $\mu$ is the steady-state distribution under $\pi$, $\mu(s)\doteq\lim_{t\to\infty}P(S_t=s|A_{0:t}\sim\pi)$ which is assumed to exist and to be independent of $S_0$; and we also have that:
\begin{equation}
\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s’|s,a)=\mu(s’),\hspace{1cm}\forall s’\in\mathcal{S}
\end{equation}
Recall that in continuing tasks with average-reward setting, we use the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#differential-return&quot;&gt;differential return&lt;/a&gt;, which is defined in terms of differences between rewards and the average reward:
\begin{equation}
G_t\doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\dots\tag{8}\label{8}
\end{equation}
And thus, we also use the differential version of value functions, which are defined as usual except that they use the differential return \eqref{8}:
\begin{align}
v_\pi(s)&amp;amp;\doteq\mathbb{E}_\pi\left[G_t|S_t=s\right] \\ q_\pi(s,a)&amp;amp;\doteq\mathbb{E}_\pi\left[G_t|S_t=s,A_t=s\right]
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;policy-grad-theorem-cont&quot;&gt;The Policy Gradient Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;&lt;br /&gt;
The policy gradient theorem for continuing case with average-reward states that
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written, for any $s\in\mathcal{S}$, as:
\begin{align}
\nabla_\boldsymbol{\theta}v_\pi(s)&amp;amp;=\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s’,r}p(s’,r|s,a)\big(r-r(\boldsymbol{\theta})+v_\pi(s’)\big)\Big] \\ &amp;amp;=\sum_a\Bigg[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\Big[-\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta})+\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]\Bigg]
\end{align}
Thus, the gradient of the performance measure w.r.t $\boldsymbol{\theta}$ is:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta}) \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\Bigg(\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s)\Bigg) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\sum_{s’}\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\sum_{s’}\mu(s’)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;policy-prm-cont-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/h2&gt;
&lt;p&gt;For tasks having continuous action space with an infinite number of actions, instead of computing learned probabilities for each action, we can learn statistics of the probability distribution.&lt;/p&gt;

&lt;p&gt;In particular, to produce a policy parameterization, the policy can be defined as the &lt;a href=&quot;/mathematics/probability-statistics/2021/11/22/normal-dist.html&quot;&gt;Normal distribution&lt;/a&gt; over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state, as given:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\frac{1}{\sigma(s,\boldsymbol{\theta})\sqrt{2\pi}}\exp\left(-\frac{(a-\mu(s,\boldsymbol{\theta}))^2}{2\sigma(s,\boldsymbol{\theta})^2}\right),
\end{equation}
where $\mu:\mathcal{S}\times\mathbb{R}^{d’}\to\mathbb{R}$ and $\sigma:\mathcal{S}\times\mathbb{R}^{d’}\to\mathbb{R}^+$ are two parameterized function approximators.&lt;/p&gt;

&lt;p&gt;We continue by dividing the policy’s parameter vector, $\boldsymbol{\theta}=[\boldsymbol{\theta}_\mu, \boldsymbol{\theta}_\sigma]^\intercal$, into two parts: one part, $\boldsymbol{\theta}_\mu$, is used for the approximation of the mean and the other, $\boldsymbol{\theta}_\sigma$, is used for the approximation of the standard deviation.&lt;/p&gt;

&lt;p&gt;The mean, $\mu$, can be approximated as a linear function, while the standard deviation, $\sigma$, must always be positive, which should be approximated as the exponential of a linear function, as:
\begin{align}
\mu(s,\boldsymbol{\theta})&amp;amp;\doteq\boldsymbol{\theta}_\mu^\intercal\mathbf{x}_\mu(s) \\ \sigma(s,\boldsymbol{\theta})&amp;amp;\doteq\exp\Big(\boldsymbol{\theta}_\sigma^\intercal\mathbf{x}_\sigma(s)\Big),
\end{align}
where $\mathbf{x}_\mu(s)$ and $\mathbf{x}_\sigma(s)$ are state feature vectors corresponding to each approximator.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Deepmind x UCL. &lt;a href=&quot;https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021&quot;&gt;Reinforcement Learning Lecture Series 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Richard S. Sutton &amp;amp; David McAllester &amp;amp; Satinder Singh &amp;amp; Yishay Mansour. &lt;a href=&quot;https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html&quot;&gt;Policy Gradient Methods for Reinforcement Learning with Function Approximation&lt;/a&gt;. NIPS 1999.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="policy-gradient" /><category term="actor-critic" /><category term="function-approximation" /><category term="my-rl" /><summary type="html">So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.</summary></entry><entry><title type="html">Search strategies in Artificial Intelligent</title><link href="http://localhost:4000/artificial-intelligent/2022/04/04/search-strategies.html" rel="alternate" type="text/html" title="Search strategies in Artificial Intelligent" /><published>2022-04-04T13:00:00+07:00</published><updated>2022-04-04T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/2022/04/04/search-strategies</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/2022/04/04/search-strategies.html">&lt;blockquote&gt;

  &lt;!-- excerpt-end --&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Stuart Russell &amp;amp; Peter Norvig. [Artificial Intelligence: A Modern Approach, 4th edition] (http://aima.cs.berkeley.edu).&lt;/p&gt;

&lt;p&gt;[2] Berkeley. &lt;a href=&quot;https://inst.eecs.berkeley.edu/~cs188/sp22/&quot;&gt;CS188&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="artificial-intelligent" /><category term="search-strategy" /><summary type="html"></summary></entry><entry><title type="html">Eligible Traces</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces.html" rel="alternate" type="text/html" title="Eligible Traces" /><published>2022-03-13T14:11:00+07:00</published><updated>2022-03-13T14:11:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces.html">&lt;blockquote&gt;
  &lt;p&gt;Beside &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#n-step-td&quot;&gt;$n$-step TD&lt;/a&gt; methods, there is another mechanism called &lt;strong&gt;Eligible traces&lt;/strong&gt; that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-return&quot;&gt;The λ-return&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#off-lambda-return&quot;&gt;Offline λ-return&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#td-lambda&quot;&gt;TD(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#truncated-td&quot;&gt;Truncated TD Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#onl-lambda-return&quot;&gt;Online λ-return&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#true-onl-td-lambda&quot;&gt;True Online TD(λ)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#equivalence-bw-forward-backward&quot;&gt;Equivalence between forward and backward views&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dutch-traces-mc&quot;&gt;Dutch Traces in Monte Carlo&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sarsa-lambda&quot;&gt;Sarsa(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-gamma&quot;&gt;Variable λ and \(\gamma\)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#off-policy-traces-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tree-backup-lambda&quot;&gt;Tree-Backup(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-off-policy-methods-traces&quot;&gt;Other Off-policy Methods with Traces&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gtd-lambda&quot;&gt;GTD(λ)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gq-lambda&quot;&gt;GQ(λ)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#greedy-gq-lambda&quot;&gt;Greedy-GQ(λ)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#htd-lambda&quot;&gt;HTD(\(\lambda\))&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#em-td-lambda&quot;&gt;Emphatic TD(λ)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#etd-stability&quot;&gt;Stability&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lambda-return&quot;&gt;The $\lambda$-return&lt;/h2&gt;
&lt;p&gt;Recall that in &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-td-prediction&quot;&gt;TD-Learning&lt;/a&gt; post, we have defined the $n$-step return as
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t\lt T-n$. After the post of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html&quot;&gt;Function Approximation&lt;/a&gt;, for any parameterized function approximator, we can generalize that equation into:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+
\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\leq T-n
\end{equation}
where $\hat{v}(s,\mathbf{w})$ is the approximate value of state $s$ given weight vector $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;We already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#stochastic-grad&quot;&gt;SGD update&lt;/a&gt;, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose
\begin{equation}
\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}
\end{equation}
as the target for our update.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;TD($\lambda$)&lt;/strong&gt; is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\lambda^{n-1}$, for $\lambda\in\left[0,1\right]$, and is normalized by a factor of $1-\lambda$ to guarantee that the weights sum to $1$, as:
\begin{equation}
G_t^\lambda\doteq(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}
\end{equation}
The $G_t^\lambda$ is called &lt;strong&gt;$\lambda$-return&lt;/strong&gt; of the update.&lt;/p&gt;

&lt;p&gt;This figure below illustrates the backup diagram of TD($\lambda$) algorithm.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/td-lambda-backup.png&quot; alt=&quot;Backup diagram of TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: The backup diagram of TD($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;off-lambda-return&quot;&gt;Offline $\lambda$-return&lt;/h3&gt;
&lt;p&gt;With the definition of $\lambda$-return, we can define the &lt;strong&gt;offline $\lambda$-return&lt;/strong&gt; algorithm, which use semi-gradient update and using $\lambda$-return as the target:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{v}(S_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}&lt;/p&gt;

&lt;p&gt;A result when applying offline $\lambda$-return on the random walk problem is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/offline-lambda-return.png&quot; alt=&quot;Offline lambda-return on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Using offline $\lambda$-return on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;td-lambda&quot;&gt;TD($\lambda$)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TD($\lambda$)&lt;/strong&gt; improves over the offline $\lambda$-return algorithm since:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It updates the weight vector $\mathbf{w}$ on every step of an episode rather than only at the end, which leads to a time improvement.&lt;/li&gt;
  &lt;li&gt;Its computations are equally distributed in time rather than all at the end of the episode.&lt;/li&gt;
  &lt;li&gt;It can be applied to continuing problems rather than just to episodic ones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With function approximation, the eligible trace is a vector $\mathbf{z}_t\in\mathbb{R}^d$ with the same number of components as the weight vector $\mathbf{w}_t$. Whereas $\mathbf{w}_t$ is long-term memory, $\mathbf{z}_t$ on the other hand is a short-term memory, typically lasting less time than the length of an episode.&lt;/p&gt;

&lt;p&gt;In TD($\lambda$), starting at the initial value of zero at the beginning of the episode, on each time step, the eligible trace vector $\mathbf{z}_t$ is incremented by the value gradient, and then fades away by $\gamma\lambda$:
\begin{align}
\mathbf{z}_{-1}&amp;amp;\doteq\mathbf{0} \\ \mathbf{z}_t&amp;amp;\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T\tag{1}\label{1}
\end{align}
where $\gamma$ is the discount factor; $\lambda$ is also called &lt;strong&gt;trace-decay parameter&lt;/strong&gt;. On the other hand, the weight vector $\mathbf{w}_t$ is updated on each step proportional to the scalar &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#td_error&quot;&gt;TD errors&lt;/a&gt; and the eligible trace vector $\mathbf{z}_t$:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,\tag{2}\label{2}
\end{equation}
where the TD error is defined as
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}&lt;/p&gt;

&lt;p&gt;Pseudocode of &lt;strong&gt;semi-gradient TD($\lambda$)&lt;/strong&gt; is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/semi-grad-td-lambda.png&quot; alt=&quot;Semi-gradient TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Linear TD($\lambda$) has been proved to converge in the on-policy case if the step size parameter, $\alpha$, is reduced over time according to the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#stochastic-approx-condition&quot;&gt;usual conditions&lt;/a&gt;. And also in the continuing discounted case, for any $\lambda$, $\overline{\text{VE}}$ is proven to be within a bounded expansion of the lowest possible error:
\begin{equation}
\overline{\text{VE}}(\mathbf{w}_\infty)\leq\dfrac{1-\gamma\lambda}{1-\gamma}\min_\mathbf{w}\overline{\text{VE}}(\mathbf{w})
\end{equation}&lt;/p&gt;

&lt;p&gt;The figure below illustrates the result for using TD($\lambda$) on the usual random walk task.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/td-lambda.png&quot; alt=&quot;TD(lambda) on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Using TD($\lambda$) on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;truncated-td&quot;&gt;Truncated TD Methods&lt;/h2&gt;
&lt;p&gt;Since in the offline $\lambda$-return, the target $\lambda$-return is not known until the end of episode. And moreover, in the continuing case, since the $n$-step returns depend on arbitrary large $n$, it maybe never known.
However, the dependence becomes weaker for longer-delayed rewards, falling by $\gamma\lambda$ for each step of delay.&lt;/p&gt;

&lt;p&gt;A natural approximation is to truncate the sequence after some number of steps. In general, we define the &lt;strong&gt;truncated $\lambda$-return&lt;/strong&gt; for time $t$, given data only up to some later horizon, $h$, as:
\begin{equation}
G_{t:h}^\lambda\doteq(1-\lambda)\sum_{n=1}^{h-t-1}\lambda^{n-1}G_{t:t+n}+\lambda^{h-t-1}G_{t:h},\hspace{1cm}0\leq t\lt h\leq T
\end{equation}
With this definition of the return, and based on the function approximation version of the $n$-step TD we have defined &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#semi-grad-n-step-td-update&quot;&gt;before&lt;/a&gt;, we have the &lt;strong&gt;TTD($\lambda$)&lt;/strong&gt; is defined as:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}^\lambda-\hat{v}(S_t,\mathbf{w}_{t+n-1})\right]\nabla_\mathbf{w}\hat{w}(S_t,\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\lt T
\end{equation}
We have the $k$-step $\lambda$-return can be written as:
\begin{align}
G_{t:t+k}^\lambda&amp;amp;=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}G_{t:t+n}+\lambda^{k-1}G_{t:t+k} \\ &amp;amp;=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right] \\ &amp;amp;\hspace{1cm}+\lambda^{k-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{k-1}R_{t+k}+\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})\right] \\ &amp;amp;=R_{t+1}+\gamma\lambda R_{t+2}+\dots+\gamma^{k-1}\lambda^{k-1}R_{t+k} \\ &amp;amp;\hspace{1cm}+(1-\lambda)\left[\sum_{n=1}^{k-1}\lambda^{n-1}\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right]+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1}) \\ &amp;amp;=\hat{v}(S_t,\mathbf{w}_{t-1})+\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1})\right] \\ &amp;amp;\hspace{1cm}+\left[\lambda\gamma R_{t+2}+\lambda\gamma^2\hat{v}(S_{t+2},\mathbf{w}_{t+1})-\lambda\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\right]+\dots \\ &amp;amp;\hspace{1cm}+\left[\lambda^{k-1}\gamma^{k-1}R_{t+k}+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})-\lambda^{k-1}\gamma^{k-1}\hat{v}(S_{t+k-1},\mathbf{w}_{t+k-2})\right] \\ &amp;amp;=\hat{v}(S_t,\mathbf{w}_{t-1})+\sum_{i=t}^{t+k-1}(\gamma\lambda)^{i-t}\delta_i’,\tag{3}\label{3}
\end{align}
with
\begin{equation}
\delta_t’\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1}),
\end{equation}
where in the third step of the derivation, we use the identity
\begin{equation}
(1-\lambda)(1+\lambda+\dots+\lambda^{k-2})=1-\lambda^{k-1}
\end{equation}
From \eqref{3}, we can see that the $k$-step $\lambda$-return can be written as sums of TD errors if the value function is held constant, which allows us to implement the TTD($\lambda$) algorithm efficiently.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/ttd-lambda-backup.png&quot; alt=&quot;Backup diagram of truncated TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: The backup diagram of truncated TD($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;onl-lambda-return&quot;&gt;Online $\lambda$-return&lt;/h2&gt;
&lt;p&gt;The idea of &lt;strong&gt;online $\lambda$-return&lt;/strong&gt; involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{w}_t^h$ denote the weights used to generate the value at time $t$ in the sequence up to horizon $h$. The first weight vector $\mathbf{w}_0^h$ in each sequence is the one that inherited from the previous episode (thus they are the same for all $h$), and the last weight vector $\mathbf{w}_h^h$ in each sequence defines the weight-vector sequence of the algorithm. At the final horizon $h=T$, we obtain the final weight $\mathbf{w}_T^T$  which will be passed on to form the initial weights of the next episode.&lt;/p&gt;

&lt;p&gt;In particular, we can define the first three sequences as:
\begin{align}
h=1:\hspace{1cm}&amp;amp;\mathbf{w}_1^1\doteq\mathbf{w}_0^1+\alpha\left[G_{0:1}^\lambda-\hat{v}(S_0,\mathbf{w}_0^1)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^1), \\ \\ h=2:\hspace{1cm}&amp;amp;\mathbf{w}_1^2\doteq\mathbf{w}_0^2+\alpha\left[G_{0:2}^\lambda-\hat{v}(S_0,\mathbf{w}_0^2)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^2), \\ &amp;amp;\mathbf{w}_2^2\doteq\mathbf{w}_1^2+\alpha\left[G_{1:2}^\lambda-\hat{v}(S_t,\mathbf{w}_1^2)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^2), \\ \\ h=3:\hspace{1cm}&amp;amp;\mathbf{w}_1^3\doteq\mathbf{w}_0^3+\alpha\left[G_{0:3}^\lambda-\hat{v}(S_0,\mathbf{w}_0^3)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^3), \\ &amp;amp;\mathbf{w}_2^3\doteq\mathbf{w}_1^3+\alpha\left[G_{1:3}^\lambda-\hat{v}(S_1,\mathbf{w}_1^3)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^3), \\ &amp;amp;\mathbf{w}_3^3\doteq\mathbf{w}_2^3+\alpha\left[G_{2:3}^\lambda-\hat{v}(S_2,\mathbf{w}_2^3)\right]\nabla_\mathbf{w}\hat{v}(S_2,\mathbf{w}_2^3)
\end{align}
The general form for the update of the &lt;strong&gt;online $\lambda$-return&lt;/strong&gt; is
\begin{equation}
\mathbf{w}_{t+1}^h\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h),\hspace{1cm}0\leq t\lt h\leq T,\tag{4}\label{4}
\end{equation}
with $\mathbf{w}_t\doteq\mathbf{w}_t^t$, and $\mathbf{w}_0^h$ is the same for all $h$, we denote this vector as $\mathbf{w}_{init}$.&lt;/p&gt;

&lt;p&gt;The online $\lambda$-return algorithm is fully online, determining a new weight vector $\mathbf{w}_t$ at each time step $t$ during an episode, using only information available at time $t$. Whereas the offline version passes through all the steps at the time of termination but does not make any updates during the episode.&lt;/p&gt;

&lt;h2 id=&quot;true-onl-td-lambda&quot;&gt;True Online TD($\lambda$)&lt;/h2&gt;
&lt;p&gt;In the online $\lambda$-return, at each time step a sequence of updates is performed. The length of this sequence, and hence the computation per time step, increase over time.&lt;/p&gt;

&lt;p&gt;However, it is possible to compute the weight vector resulting from time step $t+1$, $\mathbf{w}_{t+1}$, directly from the weight vector resulting from the sequence at time step $t$, $\mathbf{w}_t$.&lt;/p&gt;

&lt;p&gt;Consider using linear approximation for our task, which gives us 
\begin{align}
\hat{v}(S_t,\mathbf{w}_t)&amp;amp;=\mathbf{w}_t^\intercal\mathbf{x}_t; \\ \nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)&amp;amp;=\mathbf{x}_t,
\end{align}
where $\mathbf{x}_t=\mathbf{x}(S_t)$ as usual.&lt;/p&gt;

&lt;p&gt;We begin by rewriting \eqref{4}, as
\begin{align}
\mathbf{w}_{t+1}^h&amp;amp;\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h) \\ &amp;amp;=\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\left(\mathbf{w}_t^h\right)^\intercal\mathbf{x}_t\right]\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\mathbf{w}_t^h+\alpha\mathbf{x}_t G_{t:h}^\lambda,
\end{align}
where $\mathbf{I}$ is the identity matrix. With this equation, consider $\mathbf{w}_t^h$ in the cases of $t=1$ and $t=2$, we have:
\begin{align}
\mathbf{w}_1^h&amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\intercal\right)\mathbf{w}_0^h+\alpha\mathbf{x}_0 G_{0:h}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\intercal\right)\mathbf{w}_{init}+\alpha\mathbf{x}_0 G_{0:h}^\lambda, \\ \mathbf{w}_2^h&amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\intercal\right)\mathbf{w}_1^h+\alpha\mathbf{x}_1 G_{1:h}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\intercal\right)\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\intercal\right)\mathbf{w}_{init}+\alpha\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\intercal\right)\mathbf{x}_0 G_{0:h}^\lambda+\alpha\mathbf{x}_1 G_{1:h}^\lambda
\end{align}
In general, for $t\leq h$, we can write:
\begin{equation}
\mathbf{w}_t^h=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:h}^\lambda,
\end{equation}
where $\mathbf{A}_i^j$ is defined as:
\begin{equation}
\mathbf{A}_i^j\doteq\left(\mathbf{I}-\alpha\mathbf{x}_j\mathbf{x}_j^\intercal\right)\left(\mathbf{I}-\alpha\mathbf{x}_{j-1}\mathbf{x}_{j-1}^\intercal\right)\dots\left(\mathbf{I}-\alpha\mathbf{x}_i\mathbf{x}_i^\intercal\right),\hspace{1cm}j\geq i,
\end{equation}
with $\mathbf{A}_{j+1}^j\doteq\mathbf{I}$. Hence, we can express $\mathbf{w}_t$ as:
\begin{equation}
\mathbf{w}_t=\mathbf{w}_t^t=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:t}^\lambda\tag{5}\label{5}
\end{equation}
Using \eqref{3}, we have:
\begin{align}
G_{i:t+1}^\lambda-G_{i:t}^\lambda&amp;amp;=\mathbf{w}_i^\intercal\mathbf{x}_i+\sum_{j=1}^{t}(\gamma\lambda)^{j-i}\delta_j’-\left(\mathbf{w}_i^\intercal\mathbf{x}_i+\sum_{j=1}^{t-1}(\gamma\lambda)^{j-i}\delta_j’\right) \\ &amp;amp;=(\gamma\lambda)^{t-i}\delta_t’\tag{6}\label{6}
\end{align}
with the TD error, $\delta_t’$ is defined as earlier:
\begin{equation}
\delta_t’\doteq R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\tag{7}\label{7}
\end{equation}
Using \eqref{5}, \eqref{6} and \eqref{7}, we have:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda \\ &amp;amp;=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\mathbf{A}_0^t\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t}^\lambda+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\left(\mathbf{A}_0^{t-1}\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{t:t+1}^\lambda\right) \\ &amp;amp;\hspace{1cm}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t\mathbf{x}_t\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’ \\ &amp;amp;\hspace{1cm}+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t+\mathbf{w}_{t-1}^\intercal\mathbf{x}_t-\mathbf{w}_t^\intercal\mathbf{x}_t\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\delta_t’-\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’-\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t’-\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\left(\delta_t+\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)-\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t+\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\left(\mathbf{z}_t-\mathbf{x}_t\right),\tag{8}\label{8}
\end{align}
where in the eleventh step, we define $\mathbf{z}_t$ as:
\begin{equation}
\mathbf{z}_t\doteq\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i},
\end{equation}
and in the twelfth step, we also define $\delta_t$ as:
\begin{align}
\delta_t&amp;amp;\doteq\delta_t’-\mathbf{w}_t^\intercal\mathbf{x}_t+\mathbf{w}_{t-1}^\intercal\mathbf{x}_t \\ &amp;amp;=R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t,
\end{align}
which is the same as the TD error of TD($\lambda$) we have defined earlier.&lt;/p&gt;

&lt;p&gt;We then need to derive an update rule to recursively compute $\mathbf{z}_t$ from $\mathbf{z}_{t-1}$, as:
\begin{align}
\mathbf{z}_t&amp;amp;=\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i} \\ &amp;amp;=\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}+\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\gamma\lambda\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i(\gamma\lambda)^{t-i-1}+\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\intercal\right)\gamma\lambda\mathbf{z}_{t-1}+\mathbf{x}_t \\ &amp;amp;=\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\intercal\mathbf{x}_t\right)\right)\mathbf{x}_t\tag{9}\label{9}
\end{align}
Equations \eqref{8} and \eqref{9} form the update of the &lt;strong&gt;true online TD($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t+\alpha\left(\mathbf{w}_t^\intercal\mathbf{x}_t-\mathbf{w}_{t-1}^\intercal\mathbf{x}_t\right)\left(\mathbf{z}t_t-\mathbf{x}_t\right),
\end{equation}
where
\begin{align}
\mathbf{z}_t&amp;amp;\doteq\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\intercal\mathbf{x}_t\right)\right)\mathbf{x}_t,\tag{10}\label{10} \\ \delta_t&amp;amp;\doteq R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t
\end{align}
Pseudocode of the algorithm is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-onl-td-lambda.png&quot; alt=&quot;True Online TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As other methods above, below is an illustration of using true online TD($\lambda$) on the random walk problem.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-online-td-lambda.png&quot; alt=&quot;True online TD(lambda) on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: Using True online TD($\lambda$) on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The eligible trace \eqref{10} is called &lt;strong&gt;dutch trace&lt;/strong&gt; to distinguish it from the trace \eqref{1} of TD($\lambda$), which is called &lt;strong&gt;accumulating trace&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There is another kind of trace called &lt;strong&gt;replacing trace&lt;/strong&gt;, defined for the tabular case or for binary feature vectors
\begin{equation}
z_{i,t}\doteq\begin{cases}1 &amp;amp;\text{if }x_{i,t}=1 \\ \gamma\lambda z_{i,t-1} &amp;amp;\text{if }x_{i,t}=0\end{cases}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;equivalence-bw-forward-backward&quot;&gt;Equivalence between forward and backward views&lt;/h3&gt;
&lt;p&gt;In this section, we will show that there is an interchange between forward and backward view.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Consider any forward view that updates towards some interim targets $Y_k^t$ with
\begin{equation}
\mathbf{w}_{k+1}^t=\mathbf{w}_k+\eta_k\left(Y_k^t-\mathbf{x}_k^\intercal\mathbf{w}_k^t\right)\mathbf{x}_k+\mathbf{u}_k,\hspace{1cm}0\leq k\lt t,
\end{equation}
where $\mathbf{w}_0^t=\mathbf{w}_0$ for some initial $\mathbf{w}_0$; $\mathbf{u}_k\in\mathbb{R}^d$ is any vector that does not depend on $t$. Assume that the temporal differences $Y_k^{t+1}-Y_k^t$ for different $k$ are related through
\begin{equation}
Y_k^{t+1}-Y_k^t=c_k(Y_{k+1}^{t+1}-Y_{k+1}^t),\hspace{1cm}\forall k\lt t\tag{11}\label{11} 
\end{equation}
where $c_k$ is a scalar that does not depend on $t$. Then the final weights $\mathbf{w}_t^t$ at each time step $t$ are equal to the weight $\mathbf{w}_t$ as defined by $\mathbf{z}_0=\eta_0\mathbf{x}_0$ and the backward view
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t+(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\intercal\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t, \\ \mathbf{z}_t&amp;amp;=c_{t-1}\mathbf{z}_{t-1}+\eta_t\left(1-c_{t-1}\mathbf{x}_t^\intercal\mathbf{z}_{t-1}\right)\mathbf{x}_t,\hspace{1cm}t\gt 0
\end{align}&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $\mathbf{F}_t\doteq\mathbf{I}-\eta_t\mathbf{x}_t\mathbf{x}_t^\intercal$ be the &lt;em&gt;fading matrix&lt;/em&gt; such that $\mathbf{w}_{t+1}=\mathbf{F}_k\mathbf{w}_k^t+\eta_k Y_k^t\mathbf{x}_k$. For each step $t$, we have:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&amp;amp;=\mathbf{F}_t\mathbf{w}_t^{t+1}-\mathbf{w}_t^t+\eta_t Y_t^{t+1}\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t+(\mathbf{F}_t-\mathbf{I})\mathbf{w}_t^t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t-\eta_t\mathbf{x}_t\mathbf{x}_t^\intercal\mathbf{w}_t^t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\intercal\mathbf{w}_t^t)\mathbf{x}_t+\mathbf{u}_t\tag{12}\label{12}
\end{align}
We also have that:
\begin{align}
\mathbf{w}_t^{t+1}-\mathbf{w}_t^t&amp;amp;=\mathbf{F}_{t-1}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &amp;amp;=\mathbf{F}_{t-1}\mathbf{F}_{t-2}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{n-2}(Y_{t-2}^{t+1}-Y_{t-2}^t)\mathbf{F}_{t-1}\mathbf{x}_{t-2} \\ &amp;amp;\hspace{1cm}+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=\mathbf{F}_{t-1}\dots\mathbf{F}_0(\mathbf{w}_0^{t+1}-\mathbf{w}_0^t)+\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}c_k(Y_{k+1}^{t+1}-Y_{k+1}^t)\mathbf{x}_k \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=c_{t-1}\underbrace{\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k}_{\doteq\mathbf{z}_{t-1}}(Y_t^{t+1}-Y_t^t) \\ &amp;amp;=c_{t-1}\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t),\tag{13}\label{13}
\end{align}
where in the fifth step, we use the assumption \eqref{11}; the vector $\mathbf{z}_t$ defined in the sixth step can be computed recursively in terms of $\mathbf{z}_{t-1}$:
\begin{align}
\mathbf{z}_t&amp;amp;=\sum_{k=0}^{t}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{F}_t\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{F}_1\mathbf{z}_{t-1}+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{z}_{t-1}+\eta_t(1-c_{t-1}\mathbf{x}_t^\intercal\mathbf{z}_{t-1})\mathbf{x}_t
\end{align}
Plug \eqref{13} back into \eqref{12} we obtain:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&amp;amp;=c_{t-1}\mathbf{F}_t\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\intercal\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=(\mathbf{z}_t-\eta_t\mathbf{x}_t)(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\intercal\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\intercal\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t
\end{align}
Since $\mathbf{w}_{0,t}\doteq\mathbf{w}_0$, the desired result follows through induction.&lt;/p&gt;

&lt;h3 id=&quot;dutch-traces-mc&quot;&gt;Dutch Traces In Monte Carlo&lt;/h3&gt;

&lt;h2 id=&quot;sarsa-lambda&quot;&gt;Sarsa($\lambda$)&lt;/h2&gt;
&lt;p&gt;To apply the use off eligible traces on control problems, we begin by defining the $n$-step return, which is the same as what we have defined &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#n-step-return&quot;&gt;before&lt;/a&gt;:
\begin{equation}
G_{t:t+n}\doteq\ R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}t+n\lt T\tag{14}\label{14}
\end{equation}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$. With this definition of the return, the action-value form of offline $\lambda$-return can be defined as:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{q}(S_t,A_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}
where $G_t^\lambda\doteq G_{t:\infty}^\lambda$.&lt;/p&gt;

&lt;p&gt;The TD method for action values, known as &lt;strong&gt;Sarsa($\lambda$)&lt;/strong&gt;, approximates this forward view and has the same update rule as TD($\lambda$):
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,
\end{equation}
except that the TD error, $\delta_t$, is defined in terms of action-value function:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t),
\end{equation}
and so it is with eligible trace vector:
\begin{align}
\mathbf{z}_{-1}&amp;amp;\doteq\mathbf{0}, \\ \mathbf{z}&amp;amp;_t\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T
\end{align}&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/sarsa-lambda-backup.png&quot; alt=&quot;Backup diagram of Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 390px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: The backup diagram of Sarsa($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Pseudocode of the Sarsa($\lambda$) is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/sarsa-lambda.png&quot; alt=&quot;Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There is also an action-value version of the online $\lambda$-return algorithm, and its efficient implementation as true online TD($\lambda$), called &lt;strong&gt;True online Sarsa($\lambda$)&lt;/strong&gt;, which can be achieved by using $n$-step return \eqref{14} instead (which also leads to the change of $\mathbf{x}_t=\mathbf{x}(S_t)$ to $\mathbf{x}_t=\mathbf{x}(S_t,A_t)$).&lt;/p&gt;

&lt;p&gt;Pseudocode of the true online Sarsa($\lambda$) is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-online-sarsa-lambda.png&quot; alt=&quot;True online Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;lambda-gamma&quot;&gt;Variable $\lambda$ and $\gamma$&lt;/h2&gt;
&lt;p&gt;We can generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action. In other words, each time step $t$, we will have a different $\lambda$ and $\gamma$, denoted as $\lambda_t$ and $\gamma_t$.&lt;/p&gt;

&lt;p&gt;In particular, say $\lambda:\mathcal{S}\times\mathcal{A}\to[0,1]$ such that $\lambda_t\doteq\lambda(S_t,A_t)$ and similarly, $\gamma:\mathcal{S}\to[0,1]$ such that $\gamma_t\doteq\gamma(S_t)$.&lt;/p&gt;

&lt;p&gt;With this definition of $\gamma$, the return can be rewritten generally as:
\begin{align}
G_t&amp;amp;\doteq R_{t+1}+\gamma_{t+1}G_{t+1} \\ &amp;amp;=R_{t+1}+\gamma_{t+1}R_{t+2}+\gamma_{t+1}\gamma_{t+2}R_{t+3}+\dots \\ &amp;amp;=\sum_{k=t}^{\infty}\left(\prod_{i=t+1}^{k}\gamma_i\right)R_{k+1},
\end{align}
where we require that $\prod_{k=t}^{\infty}\gamma_k=0$ with probability $1$ for all $t$ to assure the sums are finite.&lt;/p&gt;

&lt;p&gt;The generalization of $\lambda$ also lets us rewrite the state-based $\lambda$-return as:
\begin{equation}
G_t^{\lambda s}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\Big),\tag{15}\label{15}
\end{equation}
where $G_t^{\lambda s}$ denotes that this $\lambda$
-return is bootstrapped from state values, and hence the $G_t^{\lambda a}$ denotes the $\lambda$-return that bootstraps from action values. The Sarsa form of action-based $\lambda$-return is defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),
\end{equation}
and the Expected Sarsa form of its can be defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),\tag{16}\label{16}
\end{equation}
where the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#expected-approximate-value&quot;&gt;expected approximate value&lt;/a&gt; is generalized to function approximation as:
\begin{equation}
\bar{V}_t\doteq\sum_a\pi(a|s)\hat{q}(s,a,\mathbf{w}_t)\tag{17}\label{17}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;off-policy-traces-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/h2&gt;
&lt;p&gt;We can also apply the use of importance sampling with eligible traces.&lt;/p&gt;

&lt;p&gt;We begin with the new definition of $\lambda$-return, which is achieved by generalizing the $\lambda$-return \eqref{15} with the idea of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-return-control-variate-state-value&quot;&gt;control variates on $n$-step off-policy return&lt;/a&gt;:
\begin{equation}
G_t^{\lambda s}\doteq\rho_t\Big(R_{t+1}+\gamma_{t+1}\big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\big)\Big)+(1-\rho_t)\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
Much like the other returns, the truncated version of this return can be approximated simply in terms of sums of state-based TD errors:
\begin{equation}
G_t^{\lambda s}\approx\hat{v}(S_t,\mathbf{w}_t)+\rho_t\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
where the state-based TD error, $\delta_t^s$, is defined as:
\begin{equation}
\delta_t^s\doteq R_{t+1}+\gamma_{t+1}\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),\tag{18}\label{18}
\end{equation}
with the approximation becoming exact if the approximate value function does not change.&lt;/p&gt;

&lt;p&gt;With this approximation, we have that:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t+\alpha\left(G_t^{\lambda s}-\hat{v}(S_t,\mathbf{w}_t)\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t) \\ &amp;amp;\approx\mathbf{w}_t+\alpha\rho_t\left(\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)
\end{align}
This is one time step of a forward view. And in fact, the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time. Since the sum of the forward-view update over time is:
\begin{align}
\sum_{t=1}^{\infty}(\mathbf{w}_{t+1}-\mathbf{w}_t)&amp;amp;\approx\sum_{t=1}^{\infty}\sum_{k=t}^{\infty}\alpha\rho_t\delta_k^s\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{k=1}^{\infty}\sum_{t=1}^{k}\alpha\rho_t\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{k=1}^{\infty}\alpha\delta_k^s\sum_{t=1}^{k}\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,\tag{19}\label{19}
\end{align}
where in the second step, we use the summation rule: $\sum_{t=x}^{y}\sum_{k=t}^{y}=\sum_{k=x}^{y}\sum_{t=x}^{k}$.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{z}_k$ is defined as:
\begin{align}
\mathbf{z}_k &amp;amp;=\sum_{t=1}^{k}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t, \mathbf{w}_t\right)\prod_{i=t+1}^{k} \gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &amp;amp;=\gamma_k\lambda_k\rho_k\underbrace{\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k-1}\gamma_i\lambda_i\rho_i}_{\mathbf{z}_{k-1}}+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &amp;amp;=\rho_k\big(\gamma_k\lambda_k\mathbf{z}_{k-1}+\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right)\big)
\end{align}
Then we can rewrite \eqref{19} as:
\begin{equation}
\sum_{t=1}^{\infty}\left(\mathbf{w}_{t+1}-\mathbf{w}_t\right)\approx\sum_{k=1}^{\infty}\alpha\delta_k^s\mathbf{z}_k,
\end{equation}
which is sum of the backward-view update over time, with the eligible trace vector is defined as:
\begin{equation}
\mathbf{z}_t\doteq\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\big)\tag{20}\label{20}
\end{equation}
Using this eligible trace with the parameter update rule \eqref{2} of TD($\lambda$), we obtain a general TD($\lambda$) algorithm that can be applied to either on-policy or off-policy data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the on-policy case, the algorithm is exactly TD($\lambda$) because $\rho_t=1$ for all $t$ and \eqref{20} becomes the accumulating trace \eqref{1} with extending to variable $\lambda$ and $\gamma$.&lt;/li&gt;
  &lt;li&gt;In the off-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For action-value function, we generalize the definition of the $\lambda$-return \eqref{16} of Expected Sarsa with the idea of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-return-control-variate-action-value&quot;&gt;control variate&lt;/a&gt;:
\begin{align}
G_t^{\lambda a}&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\big[\rho_{t+1}G_{t+1}^{\lambda a}+\bar{V}_t(S_{t+1}) \\ &amp;amp;\hspace{2cm}-\rho_{t+1}\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\big]\Big) \\ &amp;amp;=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\rho_{t+1}\left[G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right]\Big),
\end{align}
where the expected approximate value $\bar{V}_t(S_{t+1})$ is as given by \eqref{17}.&lt;/p&gt;

&lt;p&gt;Similar to the others, this $\lambda$-return can also be written approximately as the sum of TD errors
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
with the action-based TD error is defined in terms of the expected approximate value:
\begin{equation}
\delta_t^a=R_{t+1}+\gamma_{t+1}\bar{V}_t(S_{t+1})-\hat{q}(S_t,A_t,\mathbf{w}_t)\tag{21}\label{21}
\end{equation}
Like the state value function case, this approximation also becomes exact if the approximate value function does not change.&lt;/p&gt;

&lt;p&gt;Similar to the state case \eqref{20}, we can also define the eligible trace for action values:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace with the parameter update rule \eqref{2} of TD($\lambda$) and the expectation-based TD error \eqref{21}, we end up with an Expected Sarsa($\lambda$) algorithm that can applied to either on-policy or off-policy data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the on-policy case with constant $\lambda$ and $\gamma$, this becomes the Sarsa($\lambda$) algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tree-backup-lambda&quot;&gt;Tree-Backup($\lambda$)&lt;/h2&gt;
&lt;p&gt;Recall that in the post of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html&quot;&gt;TD-Learning&lt;/a&gt;, we have mentioned that there is an off-policy method without importance sampling called &lt;strong&gt;tree-backup&lt;/strong&gt;. Can we extend the idea of tree-backup to an eligible trace version? Yes, we can.&lt;/p&gt;

&lt;p&gt;As usual, we begin with establishing the $\lambda$-return by generalizing the $\lambda$-return of Expected Sarsa \eqref{16} with the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-tree-backup-return&quot;&gt;$n$-step Tree-backup return&lt;/a&gt;:
\begin{align}
G_t^{\lambda a}&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\Bigg((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\Big[\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t) \\ &amp;amp;\hspace{2cm}+\pi(A_{t+1}|S_{t+1})G_{t+1}^{\lambda a}\Big]\Bigg) \\ &amp;amp;=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\pi(A_{t+1}|S_{t+1})\left(G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right)\Big)
\end{align}
This return, as usual, can also be written approximately (ignoring changes in the approximate value function) as sum of TD errors:
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\pi(A_i|S_i),
\end{equation}
with the TD error is defined as given by \eqref{21}.&lt;/p&gt;

&lt;p&gt;Similar to how we derive the eligible trace \eqref{20}, we can define a new eligible trace in terms of target-policy probabilities of the selected actions:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\pi(A_t|S_t)\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace vector with the parameter update rule \eqref{2} of TD($\lambda$), we end up with the &lt;strong&gt;Tree-Backup($\lambda$)&lt;/strong&gt; or &lt;strong&gt;TB($\lambda$)&lt;/strong&gt;.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/tree-backup-lambda-backup.png&quot; alt=&quot;Backup diagram of Tree Backup(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 390px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: The backup diagram of Tree Backup($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;other-off-policy-methods-traces&quot;&gt;Other Off-policy Methods with Traces&lt;/h2&gt;

&lt;h3 id=&quot;gtd-lambda&quot;&gt;GTD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GTD($\lambda$)&lt;/strong&gt; is the extended version of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#tdc&quot;&gt;&lt;strong&gt;TDC&lt;/strong&gt;&lt;/a&gt;, a state-value Gradient-TD method, with eligible traces.&lt;/p&gt;

&lt;p&gt;In this algorithm, we will define a new off-policy, $\lambda$-return, not like usual but as a function:
\begin{equation}
G_t^{\lambda}(v)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})v(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda}(v)\Big]\tag{22}\label{22}
\end{equation}
where $v(s)$ denotes the value at state $s$, and $\lambda\in[0,1]$ is the trace-decay parameter.&lt;/p&gt;

&lt;p&gt;Let $T_\pi^\lambda$ denote the $\lambda$-weighted Bellman operator for policy $\pi$ such that:
\begin{align}
v_\pi(s)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(v_\pi)\big|S_t=s,\pi\Big] \\ &amp;amp;\doteq (T_\pi^\lambda v_\pi)(s)
\end{align}&lt;/p&gt;

&lt;p&gt;Consider using linear function approximation, or in particular, we are trying to approximate $v(s)$ by $v_\mathbf{w}(s)=\mathbf{w}^\intercal\mathbf{x}(s)$. Our objective is to find the fixed point which satisfies:
\begin{equation}
v_\mathbf{w}=\Pi T_\pi^\lambda v_\mathbf{w},\tag{23}\label{23}
\end{equation}
where $\Pi v$ is a projection of $v$ into the space of representable functions $\{v_\mathbf{w}|\mathbf{w}\in\mathbb{R}^d\}$.
Let $\mu$ be the steady-state distribution of states under the behavior policy $b$. Then, the projection can be defined as:
\begin{equation}
\Pi v\doteq v_{\mathbf{w}},
\end{equation}
where
\begin{equation}
\mathbf{w}={\arg\min}_{\mathbf{w}\in\mathbb{R}^d}\left\Vert v-v_\mathbf{w}\right\Vert_\mu^2,
\end{equation}
In a linear case, in which $v_\mathbf{w}=\mathbf{X}\mathbf{w}$, the projection operator is linear and independent of $\mathbf{w}$:
\begin{equation}
\Pi=\mathbf{X}(\mathbf{X}^\intercal\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^\intercal\mathbf{D},
\end{equation}
where $\mathbf{D}$ denotes $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix whose diagonal elements are $\mu(s)$, and $\mathbf{X}$ denotes the $\vert\mathcal{S}\vert\times d$ matrix whose rows are the feature vectors $\mathbf{x}(s)^\intercal$, one for each state $s$.&lt;/p&gt;

&lt;p&gt;With linear function approximation, we can rewrite the $\lambda$-return \eqref{22} as:
\begin{equation}
G_t^{\lambda}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda}(\mathbf{w})\Big]\tag{24}\label{24}
\end{equation}
Let
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t,
\end{equation}
and
\begin{equation}
\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\doteq\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\mathbf{x}(s), 
\end{equation}
where $\mathcal{P}_\mu^\pi$ is an operator.&lt;/p&gt;

&lt;p&gt;The fixed point in \eqref{23} can be found by minimizing the Mean Square Projected Bellman Error (MSPBE):
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big\Vert v_\mathbf{w}-\Pi T_\pi^\lambda v_\mathbf{w}\Big\Vert_\mu^2 \\ &amp;amp;=\Big\Vert\Pi(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w})\Big\Vert_\mu^2 \\ &amp;amp;=\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big)^\intercal\mathbf{D}\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big) \\ &amp;amp;=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\intercal\Pi^\intercal\mathbf{D}\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &amp;amp;=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\intercal\mathbf{D}^\intercal\mathbf{X}\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{D}\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &amp;amp;=\Big(\mathbf{X}^\intercal\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-\mathbf{w}\right)\Big)^\intercal\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\intercal\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)\tag{25}\label{25}
\end{align}&lt;/p&gt;

&lt;p&gt;From the definition of $T_\pi^\lambda$ and $\delta_t^\lambda$, we have:
\begin{align}
(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{v})(s)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t\big|S_t=s,\pi\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\tag{26}\label{26}
\end{align}
Therefore,
\begin{align}
\mathbf{X}^\intercal\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)&amp;amp;=\sum_s\mu(s)\Big[\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)(s)\Big]\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})|S_t=s,\pi\Big]\mathbf{x}(s) \\ &amp;amp;=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\tag{27}\label{27}
\end{align}
Moreover, we also have:
\begin{equation}
\mathbf{X}^\intercal\mathbf{D}\mathbf{X}=\sum_s\mu(s)\mathbf{x}(s)\mathbf{x}(s)^\intercal=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]\tag{28}\label{28}
\end{equation}
Substitute \eqref{26}, \eqref{27} and \eqref{28} back to the \eqref{25}, we have:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)\tag{29}\label{29}
\end{equation}
In the objective function \eqref{29}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy problem, as usual, we use importance sampling.&lt;/p&gt;

&lt;p&gt;We then instead use an importance-sampling version of $\lambda$-return \eqref{24}:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})=\rho_t\left(R_{t+1}+\gamma_{t+1}\left[(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\right]\right),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
This also leads to an another version of $\delta_t^\lambda$, defined as:
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t
\end{equation}
With this definition of the $\lambda$-return, we have:
\begin{align}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]&amp;amp;=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}\big)+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}\big)\big|S_t=s\Big] \\ &amp;amp;\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}\big|S_t=s,\pi\Big] \\ &amp;amp;\hspace{2cm}+\sum_{a,s’}p(s’|s,a)b(a|s)\frac{\pi(a|s)}{b(a|s)}\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}\big|S_t=s,\pi\Big] \\ &amp;amp;\hspace{2cm}+\sum_{a,s’}p(s’|s,a)\pi(a|s)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1} \\ &amp;amp;\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big]\big|S_t=s,\pi\Big],
\end{align}
which, as it continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]=\mathbb{E}\Big[G_t^{\lambda}(\mathbf{w})\big|S_t=s,\pi\Big]
\end{equation}
And eventually, we get:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t
\end{equation}
because the state distribution is based on behavior state-distribution $\mu$.&lt;/p&gt;

&lt;p&gt;With this result, our objective function \eqref{29} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &amp;amp;=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\tag{30}\label{30}
\end{align}
From the definition of $\delta_t^{\lambda\rho}$, we have:
\begin{align}
\delta_t^{\lambda\rho}(\mathbf{w})&amp;amp;=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\big[(1-\lambda_{t+1})\mathbf{w}^\intercal\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big]\Big)-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t+\mathbf{w}^\intercal\mathbf{x}_t\Big) \\ &amp;amp;\hspace{2cm}-\rho_t\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\mathbf{x}_{t+1}+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t\Big)+\rho_t\mathbf{w}^\intercal\mathbf{x}_t-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_{t+1}\Big) \\ &amp;amp;=\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\intercal\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w}),
\end{align}
where the TD error, $\delta_t(\mathbf{w})$, is defined as usual:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t
\end{equation}
Also, we have that:
\begin{align}
\mathbb{E}\Big[(1-\rho_t)\mathbf{w}^\intercal\mathbf{x}_t\mathbf{x}_t\Big]&amp;amp;=\sum_{s,a}\mu(s)b(a|s)\left(1-\frac{\pi(a|s)}{b(a|s)}\right)\mathbf{w}^\intercal\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)\left(\sum_a b(a|s)-\sum_a\pi(a|s)\right)\mathbf{w}^\intercal\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)(1-1)\mathbf{w}^\intercal\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=0
\end{align}
With these results, we have:
\begin{align}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+(\rho_t-1)\mathbf{w}^\intercal\mathbf{x}_t\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t\Big]+0+\mathbb{E}_{\pi b}\Big[\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-1}\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\rho_t\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-2}+\dots\big)\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t=\rho_t(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1})
\end{equation}
Plugging this result back to \eqref{30} lets our objective function become:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\tag{31}\label{31}
\end{equation}
Similar to TDC, we also use gradient descent in order to find the minimum value of $\overline{\text{PBE}}(\mathbf{w})$. The gradient of our objective function w.r.t the weight vector $\mathbf{w}$ is:
\begin{align}
\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})&amp;amp;=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &amp;amp;=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\big(\gamma_{t+1}\mathbf{x}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal-\mathbf{x}_t\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal-\mathbf{x}_t\rho_t\big(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1}\big)^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal-\big(\mathbf{x}_t\rho_t\mathbf{x}_t^\intercal+\mathbf{x}_t\rho_t\gamma_t\lambda_t\mathbf{z}_{t-1}^\intercal\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal-\big(\mathbf{x}_t\mathbf{x}_t^\intercal+\mathbf{x}_{t+1}\gamma_{t+1}\lambda_{t+1}\mathbf{z}_t^\intercal\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal-\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\intercal\Big]\mathbf{v}(\mathbf{w}),\tag{32}\label{32}
\end{align}
where in the seventh step, we have used shifting indices trick and the identities:
\begin{align}
\mathbb{E}\Big[\mathbf{x}_t\rho_t\mathbf{x}_t^\intercal\Big]&amp;amp;=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big], \\ \mathbb{E}\Big[\mathbf{x}_{t+1}\rho_t\gamma_t\lambda_t\mathbf{z}_t^\intercal\Big]&amp;amp;=\mathbb{E}\Big[\mathbf{x}_{t+1}\gamma_t\lambda_t\mathbf{z}_t^\intercal\Big]
\end{align}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from \eqref{32} and following TDC derivation steps we obtain the &lt;strong&gt;GTD($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\intercal\mathbf{v}_t)\mathbf{x}_{t+1},
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the TD error $\delta_t^s$ is defined, as usual, as state-based TD error \eqref{18};&lt;/li&gt;
  &lt;li&gt;the eligible trace vector $\mathbf{z}_t$ is defined as given in \eqref{20} for state value;&lt;/li&gt;
  &lt;li&gt;and $\mathbf{v}_t$ is a vector of the same dimension as $\mathbf{w}$, initialized to $\mathbf{v}_0=\mathbf{0}$ with $\beta&amp;gt;0$ is a step-size parameter:
\begin{align}
\delta_t^s&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\rho_t(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t), \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta(\mathbf{v}_t^\intercal\mathbf{x}_t)\mathbf{x}_t
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gq-lambda&quot;&gt;GQ($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GQ($\lambda$)&lt;/strong&gt; is another eligible trace version of a Gradient-TD method but with action values. Its goal is to learn a parameter $\mathbf{w}_t$ such that $\hat{q}(s,a,\mathbf{w}_t)\doteq\mathbf{w}_t^\intercal\mathbf{x}(s,a)\approx q_\pi(s,a)$ from data given by following a behavior policy $b$.&lt;/p&gt;

&lt;p&gt;Similar to the state-values case of GTD($\lambda$), we begin with the definition of $\lambda$-return (function):
\begin{equation}
G_t^\lambda(q)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})q(S_{t+1},A_{t+1})+\lambda_{t+1}G_{t+1}^\lambda(q)\Big],\tag{33}\label{33}
\end{equation}
where $q(s,a)$ denotes the value of taking action $a$ at state $s$ and $\lambda\in[0,1]$ is the trace decay parameter.&lt;/p&gt;

&lt;p&gt;Let $T_\pi^\lambda$ denote the $\lambda$-weighted state-action version of the affine $\vert\mathcal{S}\times\mathcal{A}\vert\times\vert\mathcal{S}\times\mathcal{A}\vert$ Bellman operator for the target policy $\pi$ such that:
\begin{align}
q_\pi(s,a)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(q_\pi)\big|S_t=s,A_t=a,\pi\Big] \\ &amp;amp;\doteq(T_\pi^\lambda q_\pi)(s,a)
\end{align}
Analogous to the state value functions, with linear function approximation (i.e., we are trying to estimate $q(s,a)$ by $q_\mathbf{w}(s,a)=\mathbf{w}^\intercal\mathbf{x}(s,a)$), our objective is to find the fixed point $q_\mathbf{w}$ such that:
\begin{equation}
q_\mathbf{w}=\Pi T_\pi^\lambda q_\mathbf{w},
\end{equation}
where $\Pi$ is the projection operator defined as above. This point also can be found by minimizing the MSPBE objective function:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\left\Vert q_\mathbf{w}-\Pi T_\pi^\lambda q_\mathbf{w}\right\Vert_\mu^2 \\ &amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big),\tag{34}\label{34}
\end{align}
where the second step is acquired from the result \eqref{29}, and where the TD error $\delta_t^\lambda$ is defined as the above section:
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t
\end{equation}
where $G_t^\lambda$ as given in \eqref{24}.&lt;/p&gt;

&lt;p&gt;In the objective function \eqref{34}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy issue, as usual, we use importance sampling.&lt;/p&gt;

&lt;p&gt;We start with the definition of the $\lambda$-return \eqref{33}, which is a noisy estimate of the future return by following policy $\pi$. In order to have a noisy estimate for the return of target policy $\pi$ while following behavior policy $b$, we define another $\lambda$-return (function), based on importance sampling:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big],\tag{35}\label{35}
\end{equation}
where $\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$:
\begin{equation}
\bar{\mathbf{x}}_t\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a),
\end{equation}
where $\rho_t$ is the single-step importance sampling ratio, and $G_t^{\lambda\rho}(\mathbf{w})$ is a noisy guess of future rewards of target policy $\pi$, if the agent follows policy $\pi$ from time $t$.&lt;br /&gt;
Let
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t\tag{36}\label{36}
\end{equation}
With the definition of the $\lambda$-return \eqref{35}, we have that:
\begin{align}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=s\Big]&amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1} \\ &amp;amp;\hspace{1cm}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\big|S_t=s,A_t=a\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big] \\ &amp;amp;\hspace{1cm}+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big] \\ &amp;amp;\hspace{1cm}+\sum_{s’}p(s’|s,a)\sum_{a’}b(a’|s’)\frac{\pi(a’|s’)}{b(a’|s’)}\gamma_{t+1}\lambda_{t+1} \\ &amp;amp;\hspace{1cm}\times\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big] \\ &amp;amp;\hspace{1cm}+\sum_{s’,a’}p(s’|s,a)\pi(a’|s’)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big] \\ &amp;amp;=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1} \\ &amp;amp;\hspace{1cm}+\gamma_{t+1}\lambda_{t_1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big]\big|S_t=s,A_t=a,\pi\Big],
\end{align}
which, as continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big]=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})\big|S_t=s,A_t=a,\pi\Big]
\end{equation}
And eventually, it yields:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t,
\end{equation}
because the state-action distribution is based on the behavior state-action pair distribution, $\mu$.&lt;/p&gt;

&lt;p&gt;Hence, the objective function \eqref{34} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &amp;amp;=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\tag{37}\label{37}
\end{align}
From the definition of the importance-sampling based TD error\eqref{36}, we have:
\begin{align}
\delta_t^{\lambda\rho}(\mathbf{w})&amp;amp;=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big]-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}\Big]+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_t \\ &amp;amp;=\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t\Big)-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w}) \\ &amp;amp;=\delta_t(\mathbf{w})-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w}) \\ &amp;amp;\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_{t+1}\Big) \\ &amp;amp;=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\intercal\mathbf{x}_{t+1}\Big)+\gamma_{t+1}\lambda_{t+1}\Big(\rho_{t+1}\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}\Big) \\ &amp;amp;=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big),
\end{align}
where in the fifth step, we define:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t\tag{38}\label{38}
\end{equation}
Note that the last part of the above equation has expected value of vector zero under the behavior policy $b$ because:
\begin{align}
\mathbb{E}\Big[\rho_t\mathbf{x}_t\big|S_t\Big]&amp;amp;=\sum_a b(a|S_t)\frac{\pi(a|S_t)}{b(a|S_t)}\mathbf{x}(S_t,a) \\ &amp;amp;=\sum_a\pi(a|S_t)\mathbf{x}(S_t,a) \\ &amp;amp;=\bar{\mathbf{x}}_t
\end{align}
With the result obtained above, we have:
\begin{align}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&amp;amp;=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\mathbf{x}_t\Big] \\ &amp;amp;\hspace{2cm}+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]+0 \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}_b\Big[\gamma_t\lambda_t\rho_t\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w}) \\ &amp;amp;\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\intercal\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;\hspace{2cm}+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big]+0 \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}\big)\Big]+\mathbb{E}\Big[\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=\mathbb{E}_b\Big[\delta_t(\mathbf{w})\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}+\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}+\dots\Big)\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t\doteq\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\tag{39}\label{39}
\end{equation}
Plugging this result back to our objective function \eqref{37} gives us:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
Following the derivation of GTD($\lambda$), we have:
\begin{align}
-\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})&amp;amp;=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\intercal\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &amp;amp;=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\big(\gamma_{t+1}\bar{\mathbf{x}}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\mathbf{x}_t\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\mathbf{x}_t\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\Big)^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\Big(\mathbf{x}_t\mathbf{x}_t^\intercal+\gamma_t\lambda_t\rho_t\mathbf{x}_t\mathbf{z}_{t-1}^\intercal\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\Big(\mathbf{x}_t\mathbf{x}_t^\intercal+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\Big(\mathbf{x}_t\mathbf{x}_t^\intercal+\gamma_{t+1}\lambda_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal-\mathbf{x}_t\mathbf{x}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal\Big]\mathbf{v}(\mathbf{w}),
\end{align}
where in the eighth step, we have used the identity:
\begin{equation}
\mathbb{E}\Big[\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\intercal\Big]=\mathbb{E}\Big[\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\intercal\Big],
\end{equation}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\intercal\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from the above gradient-descent direction and weight-duplication trick, we obtain the &lt;strong&gt;GQ($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\intercal\mathbf{v}_t)\bar{\mathbf{x}}_{t+1},
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$;&lt;/li&gt;
  &lt;li&gt;$\delta_t^a$ is the expectation form of the TD error, defined as \eqref{38};&lt;/li&gt;
  &lt;li&gt;the eligible trace vector $\mathbf{z}_t$ is defined as \eqref{39} for action value;&lt;/li&gt;
  &lt;li&gt;and $\mathbf{v}_t$ is defined as in GTD($\lambda$):
\begin{align}
\bar{\mathbf{x}}_t&amp;amp;\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a), \\ \delta_t^a&amp;amp;\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\intercal\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\mathbf{x}_t, \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^a\mathbf{z}_t-\beta(\mathbf{v}_t^\intercal\mathbf{x}_t)\mathbf{x}_t
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;greedy-gq-lambda&quot;&gt;Greedy-GQ($\lambda$)&lt;/h4&gt;
&lt;p&gt;If the target policy is $\varepsilon$-greedy, or otherwise biased towards the greedy policy for $\hat{q}$, then GQ($\lambda$) can be used as a control algorithm, called &lt;strong&gt;Greedy-GQ($\lambda$)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the case of $\lambda=0$, called GQ(0), Greedy-GQ($\lambda$) is defined by:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{x}_t+\alpha\gamma_{t+1}(\mathbf{z}_t^\intercal\mathbf{x}_t)\mathbf{x}(S_{t+1},a_{t+1}^{*}),
\end{equation}
where the eligible trace $\mathbf{z}_t$, TD error $\delta_t^a$ and $a_{t+1}^{*}$ are defined as:
\begin{align}
\mathbf{z}_t&amp;amp;\doteq\mathbf{z}_t+\beta\delta_t^a\mathbf{x}_t-\beta(\mathbf{z}_t^\intercal\mathbf{x}_t)\mathbf{x}_t, \\ \delta_t^a&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\max_a\Big(\mathbf{w}_t^\intercal\mathbf{x}(S_{t+1},a)\Big)-\mathbf{w}_t^\intercal\mathbf{x}_t, \\ a_{t+1}^{*}&amp;amp;\doteq\arg\max_a\Big(\mathbf{w}_t^\intercal\mathbf{x}(S_{t+1},a)\Big),
\end{align}
where $\beta&amp;gt;0$ is a step-size parameter.&lt;/p&gt;

&lt;h3 id=&quot;htd-lambda&quot;&gt;HTD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;HTD($\lambda$)&lt;/strong&gt; is a hybrid state-value algorithm combining aspects of GTD($\lambda$) and TD($\lambda$).&lt;/p&gt;

&lt;p&gt;HTD($\lambda$) has the following update:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t+\alpha\left(\left(\mathbf{z}_t-\mathbf{z}_t^b\right)^\intercal\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta\left({\mathbf{z}_t^b}^\intercal\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{z}_t&amp;amp;\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t\right), \\ \mathbf{z}_t^b&amp;amp;\doteq\gamma_t\lambda_t\mathbf{z}_{t-1}^b+\mathbf{x}_t,
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;em-td-lambda&quot;&gt;Emphatic TD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Emphatic TD($\lambda$) (ETD($\lambda$))&lt;/strong&gt; is the extension of the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#em-td&quot;&gt;one-step Emphatic-TD algorithm&lt;/a&gt; to eligible traces.&lt;/p&gt;

&lt;p&gt;Emphatic TD($\lambda$) or ETD($\lambda$) is defined by:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t, \\ \delta_t&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\right), \\ M_t&amp;amp;\doteq\gamma_t i(S_t)+(1-\lambda_t)F_t, \\ F_t&amp;amp;\doteq\rho_{t-1}\gamma_t F_{t-1}+i(S_t),
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$M_t\geq 0$ is the general form of &lt;strong&gt;emphasis&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;$i:\mathcal{S}\to[0,\infty)$ is the &lt;strong&gt;interest function&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;$F_t\geq 0$ is the &lt;strong&gt;followon trace&lt;/strong&gt;, with $F_0\doteq i(S_0)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;etd-stability&quot;&gt;Stability&lt;/h4&gt;
&lt;p&gt;Consider any stochastic algorithm of the form,
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t),
\end{equation}
where $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector that varies over time. Let
\begin{align}
\mathbf{A}&amp;amp;\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right], \\ \mathbf{b}&amp;amp;\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{b}_t\right]
\end{align}
We define the stochastic update to be &lt;strong&gt;stable&lt;/strong&gt; if and only if the corresponding deterministic algorithm,
\begin{equation}
\bar{\mathbf{w}}_{t+1}\doteq\bar{\mathbf{w}}_t+\alpha\left(\mathbf{b}-\mathbf{A}\bar{\mathbf{w}}_t\right),
\end{equation}
is convergent to a unique fixed point independent of the initial $\bar{\mathbf{w}}_0$. This will occur iff $\mathbf{A}$ has a full set of eigenvalues having positive real parts, which can be proved if $\mathbf{A}$ is positive definite.&lt;/p&gt;

&lt;p&gt;With this definition of stability, in order to exam the stability of ETD($\lambda$), we begin by considering the SGD update for the weight vector $\mathbf{w}$ at time step $t$.
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t\right)\mathbf{z}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\left(\mathbf{z}_t R_{t+1}-\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\intercal\mathbf{w}_t\right)\tag{40}\label{40}
\end{align}
Let $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector such that:
\begin{align}
\mathbf{A}_t&amp;amp;\doteq\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\intercal, \\ \mathbf{b}_t&amp;amp;\doteq\mathbf{z}_t R_{t+1}
\end{align}
The stochastic update \eqref{40} is then can be written as:
\begin{align}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t\right)
\end{align}
From the definition of $\mathbf{A}$, we have:
\begin{align}
\mathbf{A}&amp;amp;=\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right] \\ &amp;amp;=\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\intercal\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\intercal\big|S_t=s\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big)\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\intercal\big|S_t=s\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]\mathbb{E}_b\Big[\rho_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\intercal\big|S_t=s\Big] \\ &amp;amp;=\sum_s\underbrace{\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]}_{\mathbf{z}(s)}\mathbb{E}_b\Big[\rho_k\big(\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big)^\intercal\big|S_k=s\Big] \\ &amp;amp;=\sum_s\mathbf{z}(s)\mathbb{E}_\pi\Big[\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big|S_k=s\Big] \\ &amp;amp;=\sum_s\mathbf{z}(s)\Big(\mathbf{x}_t-\sum_{s’}\left[\mathbf{P}_\pi\right]_{ss’}\gamma(s’)\mathbf{x}(s’)\Big)^\intercal \\ &amp;amp;=\mathbf{Z}\left(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\right)\mathbf{X},\tag{41}\label{41}
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;in the fifth step, given $S_t=s$, $\mathbf{z}_{t-1}$ and $M_t$ are independent of $\rho_t(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1})^\intercal$;&lt;/li&gt;
  &lt;li&gt;$\mathbf{P}_\pi$ represents the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of transition probabilities:
\begin{equation}
\left[\mathbf{P}_\pi\right]_{ij}\doteq\sum_a\pi(a|i)p(j|i,a),
\end{equation}
where $p(j|i,a)\doteq P(S_{t+1}=j|S_i=s,A_i=a)$.&lt;/li&gt;
  &lt;li&gt;$\mathbf{Z}$ is a $\vert\mathcal{S}\vert\times d$ matrix, whose rows are $\mathbf{z}(s)$’s (i.e., $\mathbf{Z}^\intercal\doteq\left[\mathbf{z}(s_1),\dots,\mathbf{z}(s_{\vert\mathcal{S}\vert})\right]$), with $\mathbf{z}(s)\in\mathbb{R}^d$ is a vector defined by&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:
\begin{align}
\mathbf{z}(s)&amp;amp;\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big] \\ &amp;amp;=\underbrace{\mu_(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big]}_{m(s)}\mathbf{x}_t+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_t=s\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}p(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s) \\ &amp;amp;\hspace{2cm}\times\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)} \\ &amp;amp;\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s},\bar{a}}\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})} \\\ &amp;amp;\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_{t-1}\lambda_{t-1}\mathbf{z}_{t-2}+M_{t-1}\mathbf{x}_{t-1}\big|S_t=s\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\Big(\sum_{\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\Big)\mathbf{z}(\bar{s}) \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\left[\mathbf{P}_\pi\right]_{\bar{s}s}\mathbf{z}(\bar{s})\tag{42}\label{42}
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We now introduce three $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrices:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbf{M}$, which has the $m(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big\vert S_t=s\Big]$ on its diagonal;&lt;/li&gt;
  &lt;li&gt;$\mathbf{\Gamma}$, which has the $\gamma(s)$ on its diagonal;&lt;/li&gt;
  &lt;li&gt;$\mathbf{\Lambda}$, which has the $\lambda(s)$ on its diagonal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these matrices, we can rewrite \eqref{42} in matrix form, as:
\begin{align}
\mathbf{Z}^\intercal&amp;amp;=\mathbf{X}^\intercal\mathbf{M}+\mathbf{Z}^\intercal\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda} \\ \Rightarrow\mathbf{Z}^\intercal&amp;amp;=\mathbf{X}^\intercal\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}
\end{align}
Substitute this equation back to \eqref{41}, we obtain:
\begin{equation}
\mathbf{A}=\mathbf{X}^\intercal\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}\tag{43}\label{43}
\end{equation}
Doing similar steps, we can also obtain the ETD($\lambda$)’s $\mathbf{b}$ vector:
\begin{equation}
\mathbf{b}=\mathbf{Z}\mathbf{r}_\pi=\mathbf{X}^\intercal\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{r}_\pi,
\end{equation}
where $\mathbf{r}_\pi\in\mathbb{R}^{\vert\mathcal{S}\vert}$ is the vector of expected immediate rewards from each state under $\pi$.&lt;/p&gt;

&lt;p&gt;Since the positive definiteness of $\mathbf{A}$ implies the stability of the algorithm, from \eqref{43}, it is sufficient to prove the positive definiteness of the &lt;strong&gt;key matrix&lt;/strong&gt; $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})$ because this matrix can be written in the form of:
\begin{equation}
\mathbf{X}^\intercal\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}=\sum_{i=1}^{\vert\mathcal{S}\vert}\mathbf{x}_i^\intercal\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{x}_i
\end{equation}
To prove this definiteness, we begin by writing the last part of the key matrix in form of the identity matrix minus a probability matrix.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{P}_\pi^\lambda$ be the matrix with this probability as its $\{ij\}$-component. This matrix can be written as:
\begin{align}
\mathbf{P}_\pi^\lambda&amp;amp;=\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma})^2(\mathbf{I}-\mathbf{\Gamma}) \\ &amp;amp;=\left(\sum_{k=0}^{\infty}(\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^k\right)\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{I}+\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &amp;amp;=\mathbf{I}-(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}),
\end{align}
or
\begin{equation}
\mathbf{I}-\mathbf{P}_\pi^\lambda=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})
\end{equation}
Then our key matrix now can be written as:
\begin{equation}
\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})=\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)
\end{equation}
In order to prove the positive definiteness of $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$, analogous to the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#td-fixed-pt-proof&quot;&gt;proof&lt;/a&gt; of the convergence to TD fixed point of semi-gradient TD, we use two lemmas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;: &lt;em&gt;Any matrix $\mathbf{A}$ is positive definite iff the symmetric matrix $\mathbf{S}=\mathbf{A}+\mathbf{A}^\intercal$ is positive definite&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: &lt;em&gt;Any symmetric real matrix $\mathbf{S}$ is positive definite if all of its diagonal entries are positive and greater than the sum of the corresponding off-diagonal entries&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since $\mathbf{M}$ is a diagonal matrix whose diagonal is a distribution and $\mathbf{P}_\pi^\lambda$ is a probability matrix, we have that the matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ has a diagonal of non-negative entries, and non-positive off-diagonal entries, and its row sums also are non-negative. Hence, our problem remains to show that the column sums of the key matrix are positive.&lt;/p&gt;

&lt;p&gt;To show this we need to analyze the matrix $\mathbf{M}$, and to do that we first analyze the vector $\mathbf{f}\in\mathbb{R}^{\vert\mathcal{S}\vert}$, which having $f(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\left[F_t|S_t=s\right]$ as its components. We have:
\begin{align}
f(s)&amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[F_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[i(S_t)+\rho_{t-1}\gamma_t F_{t-1}\big|S_t=s\Big] \\ &amp;amp;=\mu(s)i(s)+\mu(s)\gamma(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}P(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s)\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}]\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\mu(s)\gamma(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)}\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\gamma(s)\sum_{\bar{s},\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\mu(\bar{s})\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\gamma(s)\sum_s\left[\mathbf{P}_\pi\right]_{\bar{s}s}f(\bar{s})\tag{44}\label{44}
\end{align}
Let $\mathbf{i}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be the vector having components $[\mathbf{i}]_s\doteq\mu(s)i(s)$. Equation \eqref{44} allows  us to write $\mathbf{f}$ in matrix-vector form, as:
\begin{align}
\mathbf{f}&amp;amp;=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\mathbf{f} \\ &amp;amp;=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\mathbf{i}+(\mathbf{\Gamma}\mathbf{P}_\pi^\intercal)^2\mathbf{i}+\dots \\ &amp;amp;=\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\right)^{-1}
\end{align}
Back to the definition of $m(s)$, we have:
\begin{align}
m(s)&amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\lambda_t i(S_t)+(1-\lambda_t)F_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lambda(s)i(s)+(1-\lambda(s))f(s)
\end{align}
Continuing as usual, we rewrite this equation in matrix-vector form by letting $\mathbf{m}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be a vector having $m(s)$ as its components:
\begin{align}
\mathbf{m}&amp;amp;=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})\mathbf{f} \\ &amp;amp;=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\intercal)^{-1}\mathbf{i} \\ &amp;amp;=\Big[\mathbf{\Lambda}(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\intercal)+(\mathbf{I}-\mathbf{\Lambda})\Big]\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\right)\mathbf{i} \\ &amp;amp;=\Big(\mathbf{I}-\mathbf{\Lambda}\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\Big)\Big(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\intercal\Big)^{-1}\mathbf{i} \\ &amp;amp;=\Big(\mathbf{I}-{\mathbf{P}_\pi^\lambda}^\intercal\Big)^{-1}\mathbf{i}
\end{align}
Let $\mathbf{1}$ denote the column vector with all components equal to $1$. And using the result above, we have the vector of column sums of the key matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ is:
\begin{align}
\mathbf{1}^\intercal{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)&amp;amp;=\mathbf{m}^\intercal(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &amp;amp;=\mathbf{i}^\intercal(\mathbf{I}-\mathbf{P}_\pi^\lambda)^{-1}(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &amp;amp;=\mathbf{i}^\intercal
\end{align}
Instead of having domain of $[0,\infty)$, if we further assume that $i(s)&amp;gt;0,\,\forall s\in\mathcal{S}$, then it implies immediately that the column sums are all positive, the key matrix is positive definite, so is the matrix $\mathbf{A}$, and the ETD($\lambda$) and its expected update are stable.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Doina Precup &amp;amp; Richard S. Sutton &amp;amp; Satinder Singh. &lt;a href=&quot;https://scholarworks.umass.edu/cs_faculty_pubs/80&quot;&gt;Eligibility Traces for Off-Policy Policy Evaluation&lt;/a&gt; (2000). ICML ‘00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.&lt;/p&gt;

&lt;p&gt;[3] Deepmind x UCL. &lt;a href=&quot;https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021&quot;&gt;Reinforcement Learning Lecture Series 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Harm van Seijen &amp;amp; A. Rupam Mahmood &amp;amp; Patrick M. Pilarski &amp;amp; Marlos C. Machado &amp;amp; Richard S. Sutton. &lt;a href=&quot;http://jmlr.org/papers/v17/15-599.html&quot;&gt;True Online Temporal-Difference Learning&lt;/a&gt;. Journal of Machine Learning Research. 17(145):1−40, 2016.&lt;/p&gt;

&lt;p&gt;[5] Hado Van Hasselt &amp;amp; A. Rupam Mahmood &amp;amp; Richard S. Sutton. &lt;a href=&quot;https://www.researchgate.net/publication/263653431_Off-policy_TDl_with_a_true_online_equivalence&quot;&gt;Off-policy TD(λ) with a true online equivalence&lt;/a&gt;. Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014.&lt;/p&gt;

&lt;p&gt;[6] Hamid Reza Maei. &lt;a href=&quot;https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a/view/373459a7-72d1-4de2-bcd5-5f51e2f745e9/Hamid_Maei_PhDThesis.pdf&quot;&gt;Gradient Temporal-Difference Learning Algorithms&lt;/a&gt;. PhD Thesis, University of Alberta, 2011.&lt;/p&gt;

&lt;p&gt;[7] Hamid Reza Maei &amp;amp; Richard S. Sutton &lt;a href=&quot;http://dx.doi.org/10.2991/agi.2010.22&quot;&gt;GQ($\lambda$): A general gradient algorithm for temporal-difference prediction learning with eligibility traces&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[8] Richard S. Sutton &amp;amp; A. Rupam Mahmood &amp;amp; Martha White. &lt;a href=&quot;https://arxiv.org/abs/1503.04269&quot;&gt;An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[9] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;$\mathbf{z}_t$ is a vector random variable, one per time step, while $\mathbf{z}(s)$ is a vector expectation, one per state. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="td-learning" /><category term="eligible-traces" /><category term="function-approximation" /><category term="importance-sampling" /><category term="my-rl" /><summary type="html">Beside $n$-step TD methods, there is another mechanism called Eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.</summary></entry><entry><title type="html">Function Approximation</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html" rel="alternate" type="text/html" title="Function Approximation" /><published>2022-02-11T15:26:00+07:00</published><updated>2022-02-11T15:26:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html">&lt;blockquote&gt;
  &lt;p&gt;Reinforcement Learning in continuous state space requires function approximation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#on-policy-methods&quot;&gt;On-policy Methods&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#value-func-approx&quot;&gt;Value-function Approximation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pred-obj&quot;&gt;The Prediction Objective&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#grad-algs&quot;&gt;Gradient-based algorithms&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#stochastic-grad&quot;&gt;Stochastic-gradient&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#on-policy-semi-grad&quot;&gt;Semi-gradient&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lin-func-approx&quot;&gt;Linear Function Approximation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#lin-methods&quot;&gt;Linear Methods&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#feature-cons&quot;&gt;Feature Construction&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#polynomial&quot;&gt;Polynomial Basis&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#fourier&quot;&gt;Fourier Basis&lt;/a&gt;
                &lt;ul&gt;
                  &lt;li&gt;&lt;a href=&quot;#uni-fourier-series&quot;&gt;The Univariate Fourier Series&lt;/a&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;#even-odd-non-periodic-func&quot;&gt;Even, Odd and Non-Periodic Functions&lt;/a&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;#mult-fourier-series&quot;&gt;The Multivariate Fourier Series&lt;/a&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#coarse-coding&quot;&gt;Coarse Coding&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#tile-coding&quot;&gt;Tile Coding&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#rbf&quot;&gt;Radial Basis Functions&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lstd&quot;&gt;Least-Squares TD&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ep-semi-grad-sarsa&quot;&gt;Episodic Semi-gradient Sarsa&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ep-semi-grad-n-step-sarsa&quot;&gt;Episodic Semi-gradient n-step Sarsa&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#avg-reward&quot;&gt;Average Reward&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dif-semi-grad-sarsa&quot;&gt;Differential Semi-gradient Sarsa&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#dif-semi-grad-n-step-sarsa&quot;&gt;Differential Semi-gradient n-step Sarsa&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#off-policy-methods&quot;&gt;Off-policy Methods&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#off-policy-semi-grad&quot;&gt;Semi-gradient&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#residual-bellman-update&quot;&gt;Residual Bellman Update&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#grad-td&quot;&gt;Gradient-TD&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#em-td&quot;&gt;Emphatic-TD&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;on-policy-methods&quot;&gt;On-policy Methods&lt;/h2&gt;
&lt;p&gt;So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.&lt;/p&gt;

&lt;h3 id=&quot;value-func-approx&quot;&gt;Value-function Approximation&lt;/h3&gt;
&lt;p&gt;All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value” (or &lt;em&gt;update target&lt;/em&gt;) for that state
\begin{equation}
s\mapsto u,
\end{equation}
where $s$ is the state updated and $u$ is the update target that $s$’s estimated value is shifted toward.&lt;/p&gt;

&lt;p&gt;For example,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the MC update for value prediction is: $S_t\mapsto G_t$.&lt;/li&gt;
  &lt;li&gt;the TD(0) update for value prediction is: $S_t\mapsto R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)$.&lt;/li&gt;
  &lt;li&gt;the $n$-step TD update is: $S_t\mapsto G_{t:t+n}$.&lt;/li&gt;
  &lt;li&gt;and in the DP, policy-evaluation update, $s\mapsto\mathbb{E}\big[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\vert S_t=s\big]$, an arbitrary $s$ is updated.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Each update $s\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process &lt;strong&gt;function approximation&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;pred-obj&quot;&gt;The Prediction Objective&lt;/h3&gt;
&lt;p&gt;In contrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is impossible to find the exact value function of all states. And moreover, an update at one state also affects many others.&lt;/p&gt;

&lt;p&gt;Hence, it is necessary to specify a state distribution $\mu(s)\geq0,\sum_s\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_\pi(s)$) in each state $s$. Weighting this over the state space $\mathcal{S}$ by $\mu$, we obtain a natural objective function, called the &lt;em&gt;Mean Squared Value Error&lt;/em&gt;, denoted as $\overline{\text{VE}}$:
\begin{equation}
\overline{\text{VE}}(\mathbf{w})\doteq\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
The distribution $\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divided by total amount of visits). Under on-policy training this is called the &lt;em&gt;on-policy distribution&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In continuing tasks, the on-policy distribution is the stationary distribution under $\pi$.&lt;/li&gt;
  &lt;li&gt;In episodic tasks, the on-policy distribution depends on how the initial states are chosen.
    &lt;ul&gt;
      &lt;li&gt;Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode
  \begin{equation}
  \eta(s)=h(s)+\sum_\bar{s}\eta(\bar{s})\sum_a\pi(a\vert\bar{s})p(s\vert\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}
  This system of equation can be solved for the expected number of visits $\eta(s)$. The on-policy distribution is then
  \begin{equation}
  \mu(s)=\frac{\eta(s)}{\sum_{s’}\eta(s’)},\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;grad-algs&quot;&gt;Gradient-based algorithms&lt;/h3&gt;
&lt;p&gt;To solve the least squares problem, we are going to use a popular method, named &lt;strong&gt;Gradient descent&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Say, consider a differentiable function $J(\mathbf{w})$ of parameter vector $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;The gradient of $J(\mathbf{w})$ w.r.t $\mathbf{w}$ is defined to be
\begin{equation}
\nabla_{\mathbf{w}}J(\mathbf{w})=\left(\begin{smallmatrix}\dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_1} \\ \vdots \\ \dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_d}\end{smallmatrix}\right)
\end{equation}
The idea of Gradient descent is to minimize the objective function $J(\mathbf{w})$, we repeatedly move $\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\nabla_\mathbf{w}J(\mathbf{w})$.&lt;/p&gt;

&lt;p&gt;Thus, we have the update rule of Gradient descent:
\begin{equation}
\mathbf{w}:=\mathbf{w}-\dfrac{1}{2}\alpha\nabla_\mathbf{w}J(\mathbf{w}),
\end{equation}
where $\alpha$ is a positive step-size parameter.&lt;/p&gt;

&lt;h4 id=&quot;stochastic-grad&quot;&gt;Stochastic-gradient&lt;/h4&gt;
&lt;p&gt;Apply gradient descent to our problem, which is we have to find the minimization of
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
Since $\mu(s)$ is the state distribution over state space $\mathcal{S}$, we can rewrite $\overline{\text{VE}}$ as
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\mathbb{E}_{s\sim\mu}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
By the update we have defined earlier, in each step, we need to decrease $\mathbf{w}$ by an amount of
\begin{equation}
\Delta\mathbf{w}=-\dfrac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{VE}}(\mathbf{w})=\alpha\mathbb{E}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})
\end{equation}
Assume that, on each step, we observe a new example $S_t\mapsto v_\pi(S_t)$ consisting of a state $S_t$ and its true value under the policy $\pi$.&lt;/p&gt;

&lt;p&gt;Using &lt;strong&gt;Stochastic Gradient descent (SGD)&lt;/strong&gt;, we adjust the weight vector after each example by a small amount in the direction that would most reduce the error on that example:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t-\frac{1}{2}\alpha\nabla_\mathbf{w}\big[v_\pi(S_t)-\hat{v}(S_t,\mathbf{w}_t)\big]^2 \\ &amp;amp;=\mathbf{w}_t+\alpha\big[v_\pi(S_t)-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\tag{1}\label{1}
\end{align}
When the target output, here denoted as $U_t\in\mathbb{R}$, of the $t$-th training example, $S_t\mapsto U_t$, is not the true value, $v_\pi(S_t)$, but some approximation to it, we cannot perform the exact update \eqref{1} since $v_\pi(S_t)$ is unknown, but we can approximate it by substituting $U_t$ in place of $v_\pi(S_t)$. This yield the following general SGD method for state-value prediction:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[U_t-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\tag{2}\label{2}
\end{equation}
If $U_t$ is an &lt;em&gt;unbiased estimate&lt;/em&gt; of $v_\pi(S_t)$, i.e., $\mathbb{E}\left[U_t\vert S_t=s\right]=v_\pi(S_t)$, for each $t$, then $\mathbf{w}_t$ is guaranteed to converge to a local optimum under the usual stochastic conditions for decreasing $\alpha$.&lt;/p&gt;

&lt;p&gt;In particular, since the true value of a state is the expected value of the return following it, the Monte Carlo target $U_t\doteq G_t$, we have that the SGD version of Monte Carlo state-value prediction,
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[G_t-\hat{v}(S_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
is guaranteed to converge to a local optimal point.&lt;/p&gt;

&lt;p&gt;We have the pseudocode of the algorithm:&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/sgd-mc.png&quot; alt=&quot;SGD Monte Carlo&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;on-policy-semi-grad&quot;&gt;Semi-gradient&lt;/h4&gt;
&lt;p&gt;If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\sum_{a,s’,r}\pi(a\vert S_t)p(s’,r\vert S_t,a)\left[r+\gamma\hat{v}(s’,\mathbf{w}_t)\right]$, which all depend on the current value of the weight vector $\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.&lt;/p&gt;

&lt;p&gt;Such methods are called &lt;strong&gt;semi-gradient&lt;/strong&gt; since they include only a part of the gradient.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/semi-grad-td.png&quot; alt=&quot;Semi-gradient TD(0)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;lin-func-approx&quot;&gt;Linear Function Approximation&lt;/h3&gt;
&lt;p&gt;One of the most crucial special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot,\mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;Corresponding to every state $s$, there is a real-valued vector $\mathbf{x}(s)\doteq\left(x_1(s),x_2(s),\dots,x_d(s)\right)^\intercal$, with the same number of components with $\mathbf{w}$.&lt;/p&gt;

&lt;h4 id=&quot;lin-methods&quot;&gt;Linear Methods&lt;/h4&gt;
&lt;p&gt;Linear methods approximate value function by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:
\begin{equation}
\hat{v}(s,\mathbf{w})\doteq\mathbf{w}^\intercal\mathbf{x}(s)=\sum_{i=1}^{d}w_ix_i(s)\tag{3}\label{3}
\end{equation}
The vector $\mathbf{x}(s)$ is called a &lt;em&gt;feature vector&lt;/em&gt; representing state $s$, i.e., $x_i:\mathcal{S}\to\mathbb{R}$.&lt;/p&gt;

&lt;p&gt;For linear methods, features are &lt;em&gt;basis functions&lt;/em&gt; because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.&lt;/p&gt;

&lt;p&gt;From \eqref{3}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})=\mathbf{x}(s)
\end{equation}
Thus, with linear approximation, the SGD update can be rewrite as
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t-\hat{v}(S_t,\mathbf{w}_t)\right]\mathbf{x}(S_t)
\end{equation}&lt;/p&gt;

&lt;p&gt;In the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The gradient MC algorithm in the previous section converges to the global optimum of the $\overline{\text{VE}}$ under linear function approximation if $\alpha$ is reduced over time according to the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#stochastic-approx-condition&quot;&gt;usual conditions&lt;/a&gt;. In particular, it converges to the fixed point, called $\mathbf{w}_{\text{MC}}$, with:
\begin{align}
\nabla_{\mathbf{w}_{\text{MC}}}\mathbb{E}\left[\big(G_t-v_{\mathbf{w}_{\text{MC}}}(S_t)\big)^2\right]&amp;amp;=0 \\ \mathbb{E}\Big[\big(G_t-v_{\mathbf{w}_{\text{MC}}}(S_t)\big)\mathbf{x}_t\Big]&amp;amp;=0 \\ \mathbb{E}\Big[(G_t-\mathbf{x}_t^\intercal\mathbf{w}_{\text{MC}})\mathbf{x}_t\Big]&amp;amp;=0 \\ \mathbb{E}\left[G_t\mathbf{x}_t-\mathbf{x}_t\mathbf{x}_t^\intercal\mathbf{w}_{\text{MC}}\right]&amp;amp;=0 \\ \mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]\mathbf{w}_\text{MC}&amp;amp;=\mathbb{E}\left[G_t\mathbf{x}_t\right] \\ \mathbf{w}_\text{MC}&amp;amp;=\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[G_t\mathbf{x}_t\right]
\end{align}&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The semi-gradient TD algorithm also converges under linear approximation.&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Recall that, at each time $t$, the semi-gradient TD update is
  \begin{align}
  \mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\left(R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\intercal\mathbf{w}_t\right),
  \end{align}
  where $\mathbf{x}_t=\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\mathbf{w}_t$, the expected next weight vector can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\mathbf{w}_t+\alpha\left(\mathbf{b}-\mathbf{A}\mathbf{w}_t\right),\tag{4}\label{4}
  \end{equation}
  where
  \begin{align}
  \mathbf{b}&amp;amp;\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]\in\mathbb{R}^d, \\ \mathbf{A}&amp;amp;\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\right]\in\mathbb{R}^d\times\mathbb{R}^d\tag{5}\label{5}
  \end{align}
  From \eqref{4}, it is easily seen that if the system converges, it must converges to the weight vector $\mathbf{w}_{\text{TD}}$ at which
  \begin{align}
  \mathbf{b}-\mathbf{A}\mathbf{w}_{\text{TD}}&amp;amp;=\mathbf{0} \\ \mathbf{w}_{\text{TD}}&amp;amp;=\mathbf{A}^{-1}\mathbf{b}
  \end{align}
  This quantity, $\mathbf{w}_{\text{TD}}$, is called the &lt;strong&gt;TD fixed point&lt;/strong&gt;. And in fact, linear semi-gradient TD(0) converges to this point.&lt;/li&gt;
      &lt;li&gt;&lt;span id=&quot;td-fixed-pt-proof&quot;&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/span&gt;:&lt;br /&gt;
  We have \eqref{4} can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\left(\mathbf{I}-\alpha\mathbf{A}\right)\mathbf{w}_t+\alpha\mathbf{b}
  \end{equation}
  The idea of the proof is prove that the matrix $\mathbf{A}$ in \eqref{5} is a positive definite matrix&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, since $\mathbf{w}_t$ will be reduced toward zero whenever $\mathbf{A}$ is positive definite.&lt;br /&gt;
  For linear TD(0), in the continuing case with $\gamma&amp;lt;1$, the matrix $\mathbf{A}$ can be written as
  \begin{align}
  \mathbf{A}&amp;amp;=\sum_s\mu(s)\sum_a\pi(a\vert s)\sum_{r,s’}p(r,s’\vert s,a)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;amp;=\sum_s\mu(s)\sum_{s’}p(s’\vert s)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;amp;=\sum_s\mu(s)\mathbf{x}(s)\Big(\mathbf{x}(s)-\gamma\sum_{s’}p(s’\vert s)\mathbf{x}(s’)\Big)^\intercal \\ &amp;amp;=\mathbf{X}^\intercal\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})\mathbf{X},\tag{6}\label{6}
  \end{align}
  where
        &lt;ul&gt;
          &lt;li&gt;$\mu(s)$ is the stationary distribution under $\pi$;&lt;/li&gt;
          &lt;li&gt;$p(s’\vert s)$ is the probability transition from $s$ to $s’$ under policy $\pi$;&lt;/li&gt;
          &lt;li&gt;$\mathbf{P}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of these probabilities;&lt;/li&gt;
          &lt;li&gt;$\mathbf{D}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on its diagonal;&lt;/li&gt;
          &lt;li&gt;$\mathbf{X}$ is the $\vert\mathcal{S}\vert\times d$ matrix with $\mathbf{x}(s)$ as its row.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ in \eqref{6}.&lt;/p&gt;

        &lt;p&gt;To continue proving the positive definiteness of $\mathbf{A}$, we use two lemmas:&lt;/p&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;: &lt;em&gt;A square matrix $\mathbf{A}$ is positive definite if the symmetric matrix $\mathbf{S}=\mathbf{A}+\mathbf{A}^\intercal$ is positive definite&lt;/em&gt;.&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: &lt;em&gt;If $\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\mathbf{A}$ is positive definite&lt;/em&gt;.&lt;/li&gt;
        &lt;/ul&gt;

        &lt;p&gt;With these lemmas, plus since $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\mathbf{P}$ is a stochastic matrix and $\gamma&amp;lt;1$. Thus the problem remains to show that the column sums are nonnegative.&lt;/p&gt;

        &lt;p&gt;Let $\mathbf{1}$ denote the column vector with all components equal to $1$ and $\boldsymbol{\mu}(s)$ denote the vectorized version of $\mu(s)$: i.e., $\boldsymbol{\mu}\in\mathbb{R}^{\vert\mathcal{S}\vert}$. Thus, $\boldsymbol{\mu}=\mathbf{P}^\intercal\boldsymbol{\mu}$ since $\mu(s)$ is the stationary distribution. We have:
  \begin{align}
  \mathbf{1}^\intercal\mathbf{D}\left(\mathbf{I}-\gamma\mathbf{P}\right)&amp;amp;=\boldsymbol{\mu}^\intercal\left(\mathbf{I}-\gamma\mathbf{P}\right) \\ &amp;amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal\mathbf{P} \\ &amp;amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal \\ &amp;amp;=\left(1-\gamma\right)\boldsymbol{\mu}^\intercal,
  \end{align}
  which implies that the column sums of $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ are positive.&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;At the TD fixed point, it has also been proven (in the continuing case) that $\overline{\text{VE}}$ is within a bounded expansion of the lowest possible error, while the Monte Carlo solutions minimize the value error $\overline{\text{VE}}$:
  \begin{equation}
  \overline{\text{VE}}(\mathbf{w}_{\text{TD}})\leq\dfrac{1}{1-\gamma}\overline{\text{VE}}(\mathbf{w}_{\text{MC}})=\dfrac{1}{1-\gamma}\min_{\mathbf{w}}\overline{\text{VE}}(\mathbf{w})
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Based on the tabular &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-td-update&quot;&gt;$n$-step TD&lt;/a&gt; we have defined before, applying the semi-gradient method, we have the function approximation version of its, called &lt;span id=&quot;semi-grad-n-step-td-update&quot;&gt;&lt;strong&gt;semi-gradient $\boldsymbol{n}$-step TD&lt;/strong&gt;&lt;/span&gt;, can be defined as:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}-\hat{v}(S_t,\mathbf{w}_{t+n-1})\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\lt T
\end{equation}
where the $n$-step return is generalized from the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-return&quot;&gt;tabular version&lt;/a&gt;:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}0\geq t\geq T-n
\end{equation}
We therefore have the pseudocode of the semi-gradient $n$-step TD algorithm.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/semi-grad-n-step-td.png&quot; alt=&quot;Semi-gradient n-step TD&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;feature-cons&quot;&gt;Feature Construction&lt;/h4&gt;
&lt;p&gt;There are various ways to define features. The simplest way is to use each variable directly as a basis function along with a constant function, i.e., setting:
\begin{equation}
x_0(s)=1;\hspace{1cm}x_i(s)=s_i,0\leq i\leq d
\end{equation}
However, most interesting value functions are too complex to be represented in this way. This scheme therefore was generalized into the polynomial basis.&lt;/p&gt;

&lt;h5 id=&quot;polynomial&quot;&gt;Polynomial Basis&lt;/h5&gt;
&lt;p&gt;Suppose each state $s$ corresponds to $d$ numbers, $s_1,s_2\dots,s_d$, with each $s_i\in\mathbb{R}$. For this $d$-dimensional state space, each order-$n$ polynomial basis feature $x_i$ can be written as
\begin{equation}
x_i(s)=\prod_{j=1}^{d}s_j^{c_{i,j}},
\end{equation}
where each $c_{i,j}\in\{0,1,\dots,n\}$ for an integer $n\geq 0$. These features make up the order-$n$ polynomial basis for dimension $d$, which contains $(n+1)^d$ different features.&lt;/p&gt;

&lt;h5 id=&quot;fourier&quot;&gt;Fourier Basis&lt;/h5&gt;

&lt;h6 id=&quot;uni-fourier-series&quot;&gt;The Univariate Fourier Series&lt;/h6&gt;
&lt;p&gt;&lt;strong&gt;Fourier series&lt;/strong&gt; is applied widely in Mathematics to approximate a periodic function&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. For example:&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/fourier_series.gif&quot; alt=&quot;Fourier series visualization&quot; width=&quot;480&quot; height=&quot;360px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\theta)=\frac{4\sin\theta}{\pi},f_2(\theta)=\frac{4\sin 3\theta}{3\pi},f_3(\theta)=\frac{4\sin 5\theta}{5\pi}$ and $f_4(\theta)=\frac{4\sin 7\theta}{7\pi}$. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/maths-visualization/blob/main/fourier-series/fourier_series.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
In particular, the $n$-degree Fourier expansion of $f$ with period $\tau$ is
\begin{equation}
\bar{f}(x)=\dfrac{a_0}{2}+\sum_{k=1}^{n}\left[a_k\cos\left(k\frac{2\pi}{\tau}x\right)+b_k\left(k\frac{2\pi}{\tau}x\right)\right],
\end{equation}
where
\begin{align}
a_k&amp;amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\cos\left(\frac{2\pi kx}{\tau}\right)\,dx, \\ b_k&amp;amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi kx}{\tau}\right)\,dx
\end{align}
In the RL setting, $f$ is unknown so we cannot compute $a_0,\dots,a_n$ and $b_1,\dots,b_n$, but we can instead treat them as parameters in a linear function approximation scheme, with
\begin{equation}
\phi_i(x)=\begin{cases}1 &amp;amp;\text{if }i=0 \\ \cos\left(\frac{(i+1)\pi x}{\tau}\right) &amp;amp;\text{if }i&amp;gt;0,i\text{ odd} \\ \sin\left(\frac{i\pi x}{\tau}\right) &amp;amp;\text{if }i&amp;gt;0,i\text{ even}\end{cases}
\end{equation}
Thus, a full $n$-th order Fourier approximation to a one-dimensional value function results in a linear function approximation with $2n+1$ terms.&lt;/p&gt;

&lt;h6 id=&quot;even-odd-non-periodic-func&quot;&gt;Even, Odd and Non-Periodic Functions&lt;/h6&gt;
&lt;p&gt;If $f$ is known to be &lt;em&gt;even&lt;/em&gt; (i.e., $f(x)=f(-x)$), then $\forall i&amp;gt;0$, we have:
\begin{align}
b_i&amp;amp;=\frac{2}{\tau}\int_{0}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx \\ &amp;amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx\right] \\ &amp;amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi ix}{\tau}-2\pi i\right)\,dx\right] \\ &amp;amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{\tau/2}^{\tau}f(x-\tau)\sin\left(\frac{2\pi i(x-\tau)}{\tau}\right)\,dx\right] \\ &amp;amp;=\frac{2}{\tau}\left[\int_{0}^{\tau/2}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx+\int_{-\tau/2}^{0}f(x)\sin\left(\frac{2\pi ix}{\tau}\right)\,dx\right] \\ &amp;amp;=0,
\end{align}
so the $\sin$ terms can be dropped, which reduces the terms required for an $n$-th order Fourier approximation to $n+1$.&lt;/p&gt;

&lt;p&gt;Similarly, if $f$ is known to be &lt;em&gt;odd&lt;/em&gt; (i.e., $f(x)=-f(-x)$), then $\forall i&amp;gt;0, a_i=0$, so we can omit the $\cos$ terms.&lt;/p&gt;

&lt;p&gt;However, in general, value functions are not even, odd, or periodic (or known to be in advance). In such cases, if $f$ is defined over a bounded interval with length, let us assume, $\tau$, or without loss of generality, $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, but only project the input variable to $\left[0,\frac{\tau}{2}\right]$. This results in a function periodic on $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, but unconstrained on $\left(0,\frac{\tau}{2}\right]$. We are now free to choose whether or not the function is even or odd over $\left[-\frac{\tau}{2},\frac{\tau}{2}\right]$, and can drop half of the terms in the approximation.&lt;/p&gt;

&lt;p&gt;In general, we expect it will be better to use the “half-even” approximation and drop the $\sin$ terms because this causes only a slight discontinuity at the origin. Thus, we can define the univariate $n$-th order Fourier basis as:
\begin{equation}
x_i(s)=\cos(i\pi s),
\end{equation}
for $i=0,\dots,n$.&lt;/p&gt;

&lt;h6 id=&quot;mult-fourier-series&quot;&gt;The Multivariate Fourier Series&lt;/h6&gt;
&lt;p&gt;The $n$-order Fourier expansion of the multivariate function $F$ with period $\tau$ in $d$ dimensions is
\begin{equation}
\overline{F}(\mathbf{x})=\sum_\mathbf{c}\left[a_\mathbf{c}\cos\left(\frac{2\pi}{\tau}\mathbf{c}\cdot\mathbf{x}\right)+b_\mathbf{c}\sin\left(\frac{2\pi}{\tau}\mathbf{c}\cdot\mathbf{x}\right)\right],
\end{equation}
where $\mathbf{c}=(c_1,\dots,c_d)^\intercal,c_i\in\left[0,\dots,n\right],1\leq i\leq d$.&lt;/p&gt;

&lt;p&gt;This results in $2(n+1)^d$ basis functions for an $n$-th order full Fourier approximation to a value function in $d$ dimensions, which can be reduced to $(n+1)^d$ if we drop either the $sin$ or $cos$ terms for each variable as described above. Thus, we can define the $n$-th order Fourier basis in the multi-dimensional case as:&lt;/p&gt;

&lt;p&gt;Suppose each state $s$ corresponds to a vector of $d$ numbers, $\mathbf{s}=(s_1,\dots,s_d)^\intercal$, with each $s_i\in[0,1]$. The $i$-th feature in the order-$n$ Fourier cosine basis can then be written as:
\begin{equation}
x_i(s)=\cos\left(\pi\mathbf{s}^\intercal\mathbf{c}^i\right),
\end{equation}
where $\mathbf{c}=(c_1^i,\dots,c_d^i)^\intercal$, with $c_j^i\in\{0,\dots,n\}$ for $j=1,\dots,d$ and $i=0,\dots,(n+1)^d$.&lt;/p&gt;

&lt;p&gt;This defines a feature for each of the $(n+1)^d$ possible integer vector $\mathbf{c}^i$. The inner product $\mathbf{s}^\intercal\mathbf{c}^i$ has the effect of assigning an integer in $\{0,\dots,n\}$ to each dimension of $\mathbf{s}$. As in the one-dimensional case, this integer determines the feature’s frequency along that dimension. The feature thus can be shifted and scaled to suit the bounded state space of a particular application.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/gradient_mc_bases.png&quot; alt=&quot;Fourier basis vs polynomial basis&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Fourier basis vs Polynomial basis on the 1000-state random walk&lt;br /&gt;&lt;span&gt;(&lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt; - Example 9.2).&lt;/span&gt;&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-9/random_walk.py&quot;&gt;chere&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;coarse-coding&quot;&gt;Coarse Coding&lt;/h5&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/square_wave_function.png&quot; alt=&quot;Square wave function approximated using Coarse Coding&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Using linear function approximation based on coarse coding to learn a one-dimensional square-wave function &lt;br /&gt;&lt;span&gt;(&lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt; - Example 9.3).&lt;/span&gt;&lt;br /&gt; The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-9/square_wave.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;tile-coding&quot;&gt;Tile Coding&lt;/h5&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/gradient_mc_tile_coding.png&quot; alt=&quot;Gradient MC with tile coding&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: Gradient Monte Carlo with single tiling and with multiple tilings on the 1000-state random walk&lt;br /&gt;&lt;span&gt;(&lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt; - Example 9.2).&lt;/span&gt;&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-9/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h5 id=&quot;rbf&quot;&gt;Radial Basis Functions&lt;/h5&gt;
&lt;p&gt;Another common scheme is &lt;strong&gt;Radial Basis Functions (RBFs)&lt;/strong&gt;. RBFs are the natural generalization of coarse coding to continuous valued features. Rather than each feature taking either $0$ or $1$, it can be anything within $[0,1]$, reflecting various degrees to which the feature is present.&lt;/p&gt;

&lt;p&gt;A typical RBF feature, $x_i$, has a Gaussian response $x_i(s)$ dependent only on the distance between the state, $s$, and the feature’s prototypical or center state, $c_i$, and relative to the feature’s width, $\sigma_i$:
\begin{equation}
x_i(s)\doteq\exp\left(\frac{\Vert s-c_i\Vert^2}{2\sigma_i^2}\right)
\end{equation}
The figures below shows a one-dimensional example with a Euclidean distance metric.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/1-d-rbf.png&quot; alt=&quot;one-dimensional RBFs&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 300px; height: 100px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: One-dimensional RBFs&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;lstd&quot;&gt;Least-Squares TD&lt;/h3&gt;
&lt;p&gt;Recall when using TD(0) with linear function approximation, $v_\mathbf{w}(s)=\mathbf{w}^\intercal\mathbf{x}(s)$, we need to find a point $\mathbf{w}$ such that
\begin{equation}
\mathbb{E}\Big[\big(R_{t+1}+\gamma v_\mathbf{w}(S_{t+1})-v_{\mathbf{w}}(S_t)\big)\mathbf{x}_t\Big]=\mathbf{0}\tag{7}\label{7}
\end{equation}
or
\begin{equation}
\mathbb{E}\Big[R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\intercal\mathbf{w}_t\Big]=\mathbf{0}
\end{equation}
We found out that the solution is:
\begin{equation}
\mathbf{w}_{\text{TD}}=\mathbf{A}^{-1}\mathbf{b},
\end{equation}
where
\begin{align}
\mathbf{A}&amp;amp;\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\right], \\ \mathbf{b}&amp;amp;\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]
\end{align}
Instead of computing these expectations over all possible states and all possible transitions that could happen, we now only care about the things that did happen. In particular, we now consider the empirical loss of \eqref{7}, as:
\begin{equation}
\frac{1}{t}\sum_{k=0}^{t-1}\big(R_{k+1}+\gamma v_\mathbf{w}(S_{k+1})-v_{\mathbf{w}}(S_k)\big)\mathbf{x}_i=\mathbf{0}\tag{8}\label{8}
\end{equation}
By the law of large numbers&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, when $t\to\infty$, \eqref{8} converges to its expectation, which is \eqref{7}. Hence, we now just have to compute the estimate of $\mathbf{w}_{\text{TD}}$, called $\mathbf{w}_{\text{LSTD}}$ (as LSTD stands for &lt;strong&gt;Least-Squares TD&lt;/strong&gt;), which is defined as:
\begin{equation}
\mathbf{w}_{\text{LSTD}}\doteq\left(\sum_{k=0}^{t-1}\mathbf{x}_i\left(\mathbf{x}_k-\gamma\mathbf{x}_{k+1}\right)^\intercal\right)^{-1}\left(\sum_{k=1}^{t-1}R_{k+1}\mathbf{x}_k\right)\tag{9}\label{9}
\end{equation}
In other words, our work is to compute estimates $\widehat{\mathbf{A}}_t$ and $\widehat{\mathbf{b}}_t$ of $\mathbf{A}$ and $\mathbf{b}$:
\begin{align}
\widehat{\mathbf{A}}_t&amp;amp;\doteq\sum_{k=0}^{t-1}\mathbf{x}_k\left(\mathbf{x}_k-\gamma\mathbf{x}_{k+1}\right)^\intercal+\varepsilon\mathbf{I};\tag{10}\label{10} \\ \widehat{\mathbf{b}}_t&amp;amp;\doteq\sum_{k=0}^{t-1}R_{k+1}\mathbf{x}_k,\tag{11}\label{11}
\end{align}
where $\mathbf{I}$ is the identity matrix, and $\varepsilon\mathbf{I}$, for some small $\varepsilon&amp;gt;0$, ensures that $\widehat{\mathbf{A}}_t$ is always invertible. Thus, \eqref{9} can be rewritten as:
\begin{equation}
\mathbf{w}_{\text{LSTD}}\doteq\widehat{\mathbf{A}}_t^{-1}\widehat{\mathbf{b}}_t
\end{equation}
The two approximations in \eqref{10} and \eqref{11} could be implemented incrementally using the same &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html#incremental-method&quot;&gt;technique&lt;/a&gt; we used to apply earlier so that they can be done in constant time per step. Even so, the update for $\widehat{\mathbf{A}}_t$ would have the computational complexity of $O(d^2)$, and so is its memory required to hold the $\widehat{\mathbf{A}}_t$ matrix.&lt;/p&gt;

&lt;p&gt;This leads to a problem that our next step, which is the computation of the inverse $\widehat{\mathbf{A}}_t^{-1}$ of $\widehat{\mathbf{A}}_t$, is going to be $O(d^3)$. Fortunately, with the so-called &lt;strong&gt;Sherman-Morrison formula&lt;/strong&gt;, an inverse of our special form matrix - a sum of outer products - can also be updated incrementally with only $O(d^2)$ computations, as
\begin{align}
\widehat{\mathbf{A}}_t^{-1}&amp;amp;=\left(\widehat{\mathbf{A}}_t+\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\right)^{-1} \\ &amp;amp;=\widehat{\mathbf{A}}_{t-1}^{-1}-\frac{\widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\widehat{\mathbf{A}}_{t-1}^{-1}}{1+\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\widehat{\mathbf{A}}_{t-1}^{-1}\mathbf{x}_t},
\end{align}
for $t&amp;gt;0$, with $\mathbf{\widehat{A}}_0\doteq\varepsilon\mathbf{I}$.&lt;/p&gt;

&lt;p&gt;For the estimate $\widehat{\mathbf{b}}_t$ of $\mathbf{b}$, it can be updated using naive approach:
\begin{equation}
\widehat{\mathbf{b}}_{t+1}=\widehat{\mathbf{b}}_t+R_{t+1}\mathbf{x}_t
\end{equation}
The pseudocode for LSTD is given below&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/lstd.png&quot; alt=&quot;LSTD&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;ep-semi-grad-sarsa&quot;&gt;Episodic Semi-gradient Sarsa&lt;/h3&gt;
&lt;p&gt;We now consider the control problem, with parametric approximation of the action-value function $\hat{q}(s,a,\mathbf{w})\approx q_*(s,a)$, where $\mathbf{w}\in\mathbb{R}^d$ is a finite-dimensional weight vector.&lt;/p&gt;

&lt;p&gt;Similar to the prediction problem, we can apply semi-gradient methods in solving the control problem. The difference is rather than considering training examples of the form $S_t\mapsto U_t$, we now consider examples of the form $S_t,A_t\mapsto U_t$.&lt;/p&gt;

&lt;p&gt;From \eqref{2}, we can derive the general SGD update for action-value prediction as 
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[U_t-\hat{q}(S_t,A_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\tag{12}\label{12}
\end{equation}
The update for the one-step Sarsa method therefore would be
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\big[R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\tag{13}\label{13}
\end{equation}
We call this method &lt;strong&gt;episodic semi-gradient one-step Sarsa&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;To form the control method, we need to couple the action-value&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/ep-semi-grad-sarsa.png&quot; alt=&quot;Episodic Semi-gradient Sarsa&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The following figure illustrates the cost-to-go function $\max_a\hat{q}(s,a,\mathbf{w})$ learned during one run of the semi-gradient Sarsa on Mountain Car task.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/mountain-car-ep-semi-grad-sarsa.png&quot; alt=&quot;Semi-gradient Sarsa on Mountain Car task&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Cost-to-go learned during one run of Semi-gradient Sarsa on Mountain Car problem&lt;br /&gt;&lt;span&gt;(&lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt; - Example 10.1).&lt;/span&gt;&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-10/mountain_car.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;ep-semi-grad-n-step-sarsa&quot;&gt;Episodic Semi-gradient $\boldsymbol{n}$-step Sarsa&lt;/h3&gt;
&lt;p&gt;Similar to how we defined the one-step Sarsa version of semi-gradient, we can replace the update target in \eqref{12} by an &lt;span id=&quot;n-step-return&quot;&gt;$n$-step return&lt;/span&gt;,
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\tag{14}\label{14}
\end{equation}
for $t+n\lt T$, with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$, as usual, to obtain the &lt;strong&gt;semi-gradient $n$-step Sarsa&lt;/strong&gt; update:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
for $0\leq t\lt T$. The pseudocode is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/ep-semi-grad-n-step-sarsa.png&quot; alt=&quot;Episodic Semi-gradient n-step Sarsa&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The figure below shows how the $n$-step ($8$-step in particular) tends to learn faster than the one-step algorithm.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/mountain-car-ep-semi-grad-n-step-sarsa.png&quot; alt=&quot;one-step vs 8-step Semi-gradient Sarsa on Mountain Car task&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: Performance of one-step vs 8-step Semi-gradient Sarsa on Mountain Car task&lt;br /&gt;&lt;span&gt;(&lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;).&lt;/span&gt;&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-10/mountain_car.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;avg-reward&quot;&gt;Average Reward&lt;/h3&gt;
&lt;p&gt;We now consider a new setting for continuing tasks - alongside the episodic and discounted settings - &lt;strong&gt;average reward&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the average-reward setting, the quality of a policy $\pi$ is defined as the average rate of reward, or simply &lt;strong&gt;average reward&lt;/strong&gt;, while following that policy, which we denote as $r(\pi)$:
\begin{align}
r(\pi)&amp;amp;\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}\Big[R_t\vert S_0,A_{0:t-1}\sim\pi\Big] \\ &amp;amp;=\lim_{t\to\infty}\mathbb{E}\Big[R_t\vert S_0,A_{0:t-1}\sim\pi\Big] \\ &amp;amp;=\sum_s\mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s’,r}p(s’,r\vert s,a)r,
\end{align}
where:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the expectations are conditioned on the initial state $S_0$, and on the subsequent action $A_0,A_1,\dots,A_{t-1}$, being taken according to $\pi$;&lt;/li&gt;
  &lt;li&gt;$\mu_\pi$ is the steady-state distribution,
\begin{equation}
\mu_\pi\doteq\lim_{t\to\infty}P\left(S_t=s\vert A_{0:t-1}\sim\pi\right),
\end{equation}
which is assumed to exist for any $\pi$ and to be independent of $S_0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The steady state distribution is the special distribution under which, if we select actions according to $\pi$, we remain in the same distribution. That is, for which
\begin{equation}
\sum_s\mu_\pi(x)\sum_a\pi(a\vert s)p(s’\vert s,a)=\mu_\pi(s’)
\end{equation}
In the average-reward setting, returns are defined in terms of differences between rewards and the average reward:&lt;/p&gt;
&lt;tag id=&quot;differential-return&quot;&gt;\begin{equation}
G_t\doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\dots\tag{15}\label{15}
\end{equation}&lt;/tag&gt;
&lt;p&gt;This is known as the &lt;strong&gt;differential return&lt;/strong&gt;, and the corresponding value functions are known as &lt;strong&gt;differential value functions&lt;/strong&gt;, $v_\pi(s)$ and $q_\pi(s,a)$, which are defined in the same way as we have done before:
\begin{align}
v_\pi(s)&amp;amp;\doteq\mathbb{E}\big[G_t\vert S_t=s\big]; \\ q_\pi(s,a)&amp;amp;\doteq\mathbb{E}\big[G_t\vert S_t=s,A_t=a\big],
\end{align}
and similarly for $v_{*}$ and $q_{*}$. Likewise, differential value functions also have Bellman equations, with some modifications by replacing all discounted factor $\gamma$ and replacing all rewards, $r$, by the difference between the reward and the true average reward, $r-r(\pi)$, as:
\begin{align}
&amp;amp;v_\pi(s)=\sum_a\pi(a|s)\sum_{r,s’}p(r,s’|s,a)\left[r-r(\pi)+v_\pi(s’)\right], \\ &amp;amp;q_\pi(s,a)=\sum_{r,s’}p(s’,r|s,a)\left[r-r(\pi)+\sum_{a’}\pi(a’|s’)q_\pi(s’,a’)\right], \\ &amp;amp;v_{*}(s)=\max_a\sum_{r,s’}p(s’,r|s,a)\left[r-\max_\pi r(\pi)+v_{*}(s’)\right], \\ &amp;amp;q_{*}(s,a)=\sum_{r,s’}p(s’,r|s,a)\left[r-\max_\pi r(\pi)+\max_{a’}q_{*}(s’,a’)\right]
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;dif-semi-grad-sarsa&quot;&gt;Differential Semi-gradient Sarsa&lt;/h4&gt;
&lt;p&gt;There is also a differential form of the two &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#td_error&quot;&gt;TD errors&lt;/a&gt;:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}_{t+1}+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
and
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}_{t+1}+\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t),\tag{16}\label{16}
\end{equation}
where $\bar{R}_t$ is an estimate at time $t$ of the average reward $r(\pi)$.&lt;/p&gt;

&lt;p&gt;With these alternative definitions, most of our algorithms and many theoretical results carry through to the average-reward setting without change.&lt;/p&gt;

&lt;p&gt;For example, the average reward version of semi-gradient Sarsa is defined just as in \eqref{13} except with the differential version of the TD error \eqref{16}:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)\tag{17}\label{17}
\end{equation}
The pseudocode of the algorithm is then given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/dif-semi-grad-sarsa.png&quot; alt=&quot;Differential Semi-gradient Sarsa&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;dif-semi-grad-n-step-sarsa&quot;&gt;Differential Semi-gradient $\boldsymbol{n}$-step Sarsa&lt;/h4&gt;
&lt;p&gt;To derive the $n$-step version of \eqref{17}, we use the same update rule, except with an $n$-step version of the TD error.&lt;/p&gt;

&lt;p&gt;First, we need to define the $n$-step differential return, with function approximation, by combining the idea of \eqref{14} and \eqref{15} together, as:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}-\bar{R}_{t+1}+R_{t+2}-\bar{R}_{t+2}+\dots+R_{t+n}-\bar{R}_{t+n}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),
\end{equation}
where $\bar{R}$ is an estimate of $r(\pi),n\geq 1$, $t+n\lt T$; $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ as usual. The $n$-step TD error is then
\begin{equation}
\delta_t\doteq G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w})
\end{equation}
The pseudocode of the algorithm is then given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-02-11/dif-semi-grad-n-step-sarsa.png&quot; alt=&quot;Differential Semi-gradient n-step Sarsa&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;off-policy-methods&quot;&gt;Off-policy Methods&lt;/h2&gt;
&lt;p&gt;We now consider off-policy methods with function approximation.&lt;/p&gt;

&lt;h3 id=&quot;off-policy-semi-grad&quot;&gt;Semi-gradient&lt;/h3&gt;
&lt;p&gt;To derive the semi-gradient form of off-policy tabular methods we have known, we simply replace the update to an array ($V$ or $Q$) to an update to a weight vector $\mathbf{w}$, using the approximate value function $\hat{v}$ or $\hat{q}$ and its gradient.&lt;/p&gt;

&lt;p&gt;Recall that in off-policy learning we seek to learn a value function for a &lt;em&gt;target policy&lt;/em&gt; $\pi$, given data due to a different &lt;em&gt;behavior policy&lt;/em&gt; $b$.&lt;/p&gt;

&lt;p&gt;Many of these algorithms use the per-step importance sampling ratio:
\begin{equation}
\rho_t\doteq\rho_{t:t}=\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}&lt;/p&gt;

&lt;p&gt;In particular, for state-value functions, the one-step algorithm is &lt;strong&gt;semi-gradient off-policy TD(0)&lt;/strong&gt; has the update rule:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\rho_t\delta_t\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\tag{18}\label{18}
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if the problem is episodic and discounted, we have:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}&lt;/li&gt;
  &lt;li&gt;if the problem is continuing and undiscounted using average reward, we have:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}+\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For action values, the one-step algorithm is &lt;strong&gt;semi-gradient Expected Sarsa&lt;/strong&gt;, which has the update rule:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}),
\end{equation}
with&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;episodic tasks:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}&lt;/li&gt;
  &lt;li&gt;continuing tasks:
\begin{equation}
\delta_t\doteq R_{t+1}-\bar{R}+\sum_a\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With multi-step algorithms, we begin with &lt;strong&gt;semi-gradient $\boldsymbol{n}$-step Expected Sarsa&lt;/strong&gt;, which has the update rule:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\rho_{t+1}\dots\rho_{t+n-1}\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
where $\rho_k=1$ for $k\geq T$ and $G_{t:n}\doteq G_t$ if $t+n\geq T$, and with&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;episodic tasks:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1})
\end{equation}&lt;/li&gt;
  &lt;li&gt;continuing tasks:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}-\bar{R}_t+\dots+R_{t+n}-\bar{R}_{t+n-1}+\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For the semi-gradient version of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-tree-backup&quot;&gt;$n$-step tree-backup&lt;/a&gt;, called &lt;strong&gt;semi-gradient $\boldsymbol{n}$-step tree-backup&lt;/strong&gt;, the update rule is:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\big[G_{t:t+n}-\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1})\big]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_{t+n-1}),
\end{equation}
where
\begin{equation}
G_{t:t+n}\doteq\hat{q}(S_t,A_t,\mathbf{w}_{t-1})+\sum_{k=t}^{t+n-1}\delta_k\prod_{i=t+1}^{k}\gamma\pi(A_i|S_i),
\end{equation}
with $\delta_t$ is defined similar to the case of &lt;strong&gt;semi-gradient Expected Sarsa&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;residual-bellman-update&quot;&gt;Residual Bellman Update&lt;/h3&gt;

&lt;h3 id=&quot;grad-td&quot;&gt;Gradient-TD&lt;/h3&gt;
&lt;p&gt;In this section, we will be considering SGD methods for minimizing the $\overline{\text{PBE}}$.&lt;/p&gt;

&lt;p&gt;Rewrite the objective $\overline{\text{PBE}}$ in matrix terms, we have:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\left\Vert\Pi\bar{\delta}_\mathbf{w}\right\Vert_{\mu}^{2} \\ &amp;amp;=\left(\Pi\bar{\delta}_\mathbf{w}\right)^\intercal\mathbf{D}\Pi\bar{\delta}_\mathbf{w} \\ &amp;amp;=\bar{\delta}_\mathbf{w}^\intercal\Pi^\intercal\mathbf{D}\Pi\bar{\delta}_\mathbf{w} \\ &amp;amp;=\bar{\delta}_\mathbf{w}^\intercal\mathbf{D}\mathbf{X}\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w} \\ &amp;amp;=\left(\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w}\right)^\intercal\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\left(\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w}\right),
\end{align}
where in the fourth step, we use the property of projection operation&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; and the identity
\begin{equation}
\Pi^\intercal\mathbf{D}\Pi=\mathbf{D}\mathbf{X}\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\intercal\mathbf{D}
\end{equation}
Thus, the gradient w.r.t weight vector $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})=2\nabla_\mathbf{w}\left[\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w}\right]^\intercal\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\left(\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w}\right)\tag{19}\label{19}
\end{equation}&lt;/p&gt;

&lt;p&gt;To turn this into an SGD method, we have to sample something on every time step that has this gradient as its expected value. Let $\mu$ be the distribution of states visited under the behavior policy. The last factor of \eqref{19} can be written as:
\begin{equation}
\mathbf{X}^\intercal\mathbf{D}\bar{\delta}_\mathbf{w}=\sum_s\mu(s)\mathbf{x}(s)\bar{\delta}_\mathbf{w}=\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right],
\end{equation}
which is the expectation of the semi-gradient TD(0) update \eqref{18}. The first factor of \eqref{19}, which is the transpose of the gradient of this update, then can also be written as:
\begin{align}
\nabla_\mathbf{w}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]^\intercal&amp;amp;=\mathbb{E}\left[\rho_t\nabla_\mathbf{w}\delta_t^\intercal\mathbf{x}_t^\intercal\right] \\ &amp;amp;=\mathbb{E}\left[\rho_t\nabla_\mathbf{w}\left(R_{t+1}+\gamma\mathbf{w}^\intercal\mathbf{x}_{t+1}-\mathbf{w}^\intercal\mathbf{x}_t\right)^\intercal\mathbf{x}_t^\intercal\right] \\ &amp;amp;=\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\intercal\right]
\end{align}
And the middle factor, without the inverse operation, can also be written as:
\begin{equation}
\mathbf{X}^\intercal\mathbf{D}\mathbf{X}=\sum_a\mu(s)\mathbf{x}_s\mathbf{x}_s^\intercal=\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]
\end{equation}
Substituting these expectations back to \eqref{19}, we obtain:
\begin{equation}
\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})=2\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\intercal\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\tag{20}\label{20}
\end{equation}&lt;/p&gt;

&lt;p&gt;Here, we use the &lt;strong&gt;Gradient-TD&lt;/strong&gt; to estimate and store the product of the second two factors in \eqref{20}, denoted as $\mathbf{v}$:
\begin{equation}
\mathbf{v}\approx\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right],\tag{21}\label{21}
\end{equation}
which is the solution of the linear least-squares problem that tries to approximate $\rho_t\delta_t$ from the features. The SGD for incrementally finding the vector $\mathbf{v}$ that minimizes the expected squared error $\left(\mathbf{v}^\intercal\mathbf{x}_t\right)^2$ is known as the &lt;strong&gt;Least Mean Square (LMS)&lt;/strong&gt; rule (here augmented with an IS ratio):
\begin{equation}
\mathbf{v}_{t+1}\doteq\mathbf{v}_t+\beta\rho_t\left(\delta_t-\mathbf{v}^\intercal\mathbf{x}_t\right)\mathbf{x}_t,
\end{equation}
where $\beta&amp;gt;0$ is a step-size parameter.&lt;/p&gt;

&lt;p&gt;With a given stored estimate $\mathbf{v}_t$ approximating \eqref{21}, we can apply SGD update to the parameter vector $\mathbf{w}_t$:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t-\frac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w}_t) \\ &amp;amp;=\mathbf{w}_t-\frac{1}{2}\alpha2\mathbb{E}\left[\rho_t\left(\gamma\mathbf{x}_{t+1}-\mathbf{x}_t\right)\mathbf{x}_t^\intercal\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\intercal\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\tag{22}\label{22} \\ &amp;amp;\approx\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\intercal\right]\mathbf{v}_t \\ &amp;amp;\approx\mathbf{w}_t+\alpha\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t\mathbf{v}_t
\end{align}
This algorithm is called &lt;strong&gt;GTD2&lt;/strong&gt;. From \eqref{22}, we can also continue to derive as:
&lt;span id=&quot;tdc&quot;&gt;\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t+\alpha\mathbb{E}\left[\rho_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)\mathbf{x}_t^\intercal\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &amp;amp;=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\rho_t\mathbf{x}_t\mathbf{x}_t^\intercal\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\intercal\right]\right)\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &amp;amp;=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\intercal\right]\right)\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right] \\ &amp;amp;=\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\rho_t\delta_t\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\intercal\right]\mathbb{E}\left[\mathbf{x}_t\mathbf{x}_t^\intercal\right]^{-1}\mathbb{E}\left[\rho_t\delta_t\mathbf{x}_t\right]\right) \\ &amp;amp;\approx\mathbf{w}_t+\alpha\left(\mathbb{E}\left[\mathbf{x}_t\rho_t\delta_t\right]-\gamma\mathbb{E}\left[\rho_t\mathbf{x}_{t+1}\mathbf{x}_t^\intercal\right]\right)\mathbf{v}_t \\ &amp;amp;\approx\mathbf{w}_t+\alpha\rho_t\left(\delta_t\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\mathbf{x}_t^\intercal\mathbf{v}_t\right)
\end{align}&lt;/span&gt;
This algorithm is known as &lt;strong&gt;TD(0) with gradient correction (TDC)&lt;/strong&gt;, or as &lt;strong&gt;GTD(0)&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;em-td&quot;&gt;Emphatic-TD&lt;/h3&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;rl-book&quot;&gt;Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;[2] Deepmind x UCL. &lt;a href=&quot;https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021&quot;&gt;Reinforcement Learning Lecture Series 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Sutton, R. S. (1988). &lt;a href=&quot;doi:10.1007/bf00115009&quot;&gt;Learning to predict by the methods of temporal differences&lt;/a&gt;. Machine Learning, 3(1), 9–44.&lt;/p&gt;

&lt;p&gt;[4] Konidaris, G. &amp;amp; Osentoski, S. &amp;amp; Thomas, P.. &lt;a href=&quot;https://dl.acm.org/doi/10.5555/2900423.2900483&quot;&gt;Value Function Approximation in Reinforcement Learning Using the Fourier Basis&lt;/a&gt;. AAAI Conference on Artificial Intelligence, North America, aug. 2011.&lt;/p&gt;

&lt;p&gt;[5] Joseph K. Blitzstein &amp;amp; Jessica Hwang. &lt;a href=&quot;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573&quot;&gt;Introduction to Probability&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[6] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A $n\times n$ matrix $A$ is called &lt;em&gt;positive definite&lt;/em&gt; if and only if for any non-zero vector $\mathbf{x}\in\mathbb{R}^n$, we always have
\begin{equation}
\mathbf{x}^\intercal\mathbf{A}\mathbf{x}&amp;gt;0
\end{equation} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function $f$ is periodic with period $\tau$ if
\begin{equation}
f(x+\tau)=f(x),\forall x
\end{equation} &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Consider i.i.d r.v.s $X_1,X_2,\dots$ with finite mean $\mu$ and finite variance $\sigma^2$. For all positive integer $n$, let: 
\begin{equation}
\overline{X}_n\doteq\frac{X_1+\dots+X_n}{n}
\end{equation}
be the &lt;em&gt;sample mean&lt;/em&gt; of $X_1$ through $X_n$.&lt;/p&gt;

      &lt;p&gt;As $n\to\infty$, the sample mean $\overline{X}_n$ converges to the true mean $\mu$, with probability $1$. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For a linear function approximator, the projection is linear, which implies that it can be represented as an $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix:
\begin{equation}
\Pi\doteq\mathbf{X}\left(\mathbf{X}^\intercal\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\intercal\mathbf{D},
\end{equation}&lt;/p&gt;

      &lt;p&gt;where $\mathbf{D}$ denotes the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on the diagonal, and $\mathbf{X}$ denotes the $\vert\mathcal{S}\vert\times d$ matrix whose rows are the feature vectors $\mathbf{x}(s)^\intercal$, one for each state $s$. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="function-approximation" /><category term="td-learning" /><category term="importance-sampling" /><category term="my-rl" /><summary type="html">Reinforcement Learning in continuous state space requires function approximation.</summary></entry></feed>