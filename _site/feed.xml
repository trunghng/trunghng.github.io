<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-12T18:59:43+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">CMA Evolution Strategy</title><link href="http://localhost:4000/artificial-intelligent/machine-learning/2022/09/14/cma-es.html" rel="alternate" type="text/html" title="CMA Evolution Strategy" /><published>2022-09-14T13:00:00+07:00</published><updated>2022-09-14T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/machine-learning/2022/09/14/cma-es</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/machine-learning/2022/09/14/cma-es.html">&lt;blockquote&gt;
  &lt;p&gt;A note on CMA - Evolution Strategy
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminaries&quot;&gt;Preliminaries&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bsc-eqn&quot;&gt;Basic equation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#upd-mean&quot;&gt;Updating the mean&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#adp-cov&quot;&gt;Adapting the covariance matrix&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#est-scratch&quot;&gt;Estimating from scratch&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rank-lambda-mu-update&quot;&gt;Rank-$\gamma$ update&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rank-one-update&quot;&gt;Rank-one update&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#final-update&quot;&gt;Final update&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ctrl-sigma&quot;&gt;Controlling the step-size&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#test-on-rast&quot;&gt;Testing on Rastrigin function&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;condition number&lt;/strong&gt; of a matrix $\mathbf{A}$ is defined by
\begin{equation}
\kappa(\mathbf{A})\doteq\Vert\mathbf{A}\Vert\Vert\mathbf{A}^{-1}\Vert,
\end{equation}
where $\Vert\mathbf{A}\Vert=\sup_{\Vert\mathbf{x}\Vert=1}\Vert\mathbf{Ax}\Vert$.&lt;/p&gt;

&lt;p&gt;For $\mathbf{A}$ that is non-singular, $\kappa(\mathbf{A})=\infty$.&lt;/p&gt;

&lt;p&gt;For $\mathbf{A}$ which is positive definite, we thus have $\Vert\mathbf{A}\Vert=\lambda_\text{max}$ where $\lambda_\text{max}$ denotes the largest eigenvalue of $\mathbf{A}$, correspondingly $\lambda_\text{min}$ denotes the smallest eigenvalue of $\mathbf{A}$. The condition number of $\mathbf{A}$ therefore can be written as
\begin{equation}
\kappa(\mathbf{A})=\frac{\lambda_\text{max}}{\lambda_\text{min}}\geq 1,
\end{equation}
since corresponding to each eigenvalue $\lambda$ of $\mathbf{A}$, the inverse matrix $\mathbf{A}^{-1}$ takes $1/\lambda$ as its eigenvalue.&lt;/p&gt;

&lt;h2 id=&quot;bsc-eqn&quot;&gt;Basic equation&lt;/h2&gt;
&lt;p&gt;In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\ldots$
\begin{equation}
\mathbf{x}_k^{(t+1)}\sim\boldsymbol{\mu}^{(t)}+\sigma^{(t)}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})\sim\mathcal{N}\left(\boldsymbol{\mu}^{(t)},{\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}\right),\hspace{1cm}k=1,\ldots,\lambda\label{eq:be.1}
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbf{x}_k^{(t+1)}\in\mathbb{R}^n$: the $k$-th sample at generation $t+1$.&lt;/li&gt;
  &lt;li&gt;$\boldsymbol{\mu}^{(t)}\in\mathbb{R}^n$: mean of the search distribution at generation $t$.&lt;/li&gt;
  &lt;li&gt;$\sigma^{(t)}\in\mathbb{R}$: step-size at generation $t$.&lt;/li&gt;
  &lt;li&gt;$\boldsymbol{\Sigma}^{(t)}$: covariance matrix at generation $t$.&lt;/li&gt;
  &lt;li&gt;${\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$.&lt;/li&gt;
  &lt;li&gt;$\lambda\geq 2$: sample size.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;update-mean&quot;&gt;Updating the mean&lt;/h2&gt;
&lt;p&gt;The mean $\boldsymbol{\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\gamma$ selected points from the sample $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$:
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)},\label{eq:um.1}
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\sum_{i=1}^{\gamma}w_i=1$ with $w_1\geq w_2\geq\ldots\geq w_{\gamma}&amp;gt;0$.&lt;/li&gt;
  &lt;li&gt;$\gamma\leq\lambda$: number of selected points.&lt;/li&gt;
  &lt;li&gt;$\mathbf{x}_{i:\lambda}^{(t+1)}$: $i$-th best sample out of $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$ from \eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have
\begin{equation}
f(\mathbf{x}_{1:\lambda}^{(t+1)})\geq f(\mathbf{x}_{2:\lambda}^{(t+1)})\geq\ldots\geq f(\mathbf{x}_{\lambda:\lambda}^{(t+1)})
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We can rewrite \eqref{eq:um.1} as an update rule for the mean $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\boldsymbol{\mu}^{(t)}+\alpha_\boldsymbol{\mu}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right),
\end{equation}
where $\alpha_\boldsymbol{\mu}\leq 1$ is the learning rate, which is usually set to $1$.&lt;/p&gt;

&lt;p&gt;When choosing the weight values $w_i$ and population size $\gamma$ for recombination, we take into account the &lt;strong&gt;variance effective selection mass&lt;/strong&gt;, denoted as $\gamma_\text{eff}$, given by
\begin{equation}
\gamma_\text{eff}\doteq\left(\frac{\Vert\mathbf{w}\Vert_1}{\Vert\mathbf{w}\Vert_2}\right)=\frac{\Vert\mathbf{w}\Vert_1^2}{\Vert\mathbf{w}\Vert_2^2}=\frac{1}{\sum_{i=1}^{\gamma}w_i^2}
\end{equation}
where $\mathbf{w}$ is defined as the weight vector
\begin{equation}
\mathbf{w}=(w_1,\ldots,w_\gamma)^\text{T}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;adp-cov&quot;&gt;Adapting the covariance matrix&lt;/h2&gt;
&lt;p&gt;The covariance matrix can be estimated from scratch using the population of the current generation or can be estimated with covariance matrix from previous generations.&lt;/p&gt;

&lt;h3 id=&quot;est-scratch&quot;&gt;Estimating from scratch&lt;/h3&gt;
&lt;p&gt;Rather than using the empirical covariance matrix as an estimator for $\boldsymbol{\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation
\begin{equation}
\boldsymbol{\Sigma}_\lambda^{(t+1)}=\frac{1}{\lambda{\sigma^{(t)}}^2}\sum_{i=1}^{\lambda}\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T}\label{eq:es.1}
\end{equation}
Notice that in the above estimation \eqref{eq:es.1}, we have used all of the $\lambda$ samples. We thus can estimate a better covariance matrix by select some of the best individual out of $\lambda$ samples, which is analogous to how we update the mean $\boldsymbol{\mu}$.&lt;/p&gt;

&lt;p&gt;In particular, we instead consider the estimation
\begin{equation}
\boldsymbol{\Sigma}_{\gamma}^{(t+1)}=\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T},\label{eq:es.2}
\end{equation}
where $\gamma\leq\lambda$ is the number of selected points; the weights $w_i$ and selected points $\mathbf{x}_{i:\lambda}^{(t+1)}$ are defined as given in the update for $\boldsymbol{\mu}$.&lt;/p&gt;

&lt;h3 id=&quot;rank-lambda-mu-update&quot;&gt;Rank-$\gamma$ update&lt;/h3&gt;
&lt;p&gt;In order to ensure that \eqref{eq:es.2} is a reliable estimator, the selected population must be large enough. However, to get a fast search, the population size $\lambda$ must be small, which lets the selected sample size consequently small also. Thus, we can not get a reliable estimator for a good covariance matrix from \eqref{eq:es.2}. However, we can use the history as a helping hand.&lt;/p&gt;

&lt;p&gt;In particular, if we have experienced a sufficient number of generations, the mean of the $\boldsymbol{\Sigma}_\gamma$ from all previous generations
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\frac{1}{t+1}\sum_{i=0}^{t}\boldsymbol{\Sigma}_\gamma^{(i+1)}\label{eq:rlmu.1}
\end{equation}
would be a reliable estimator.&lt;/p&gt;

&lt;p&gt;In addition, it is reasonable that the recent generations will have more affection to the current generation than the distant ones. Hence, rather than assigning estimated covariance matrices $\boldsymbol{\Sigma}_\gamma$ from preceding generations the same weight as in \eqref{eq:rlmu.1}, it would be a better choice to give the more recent generations the higher weight.&lt;/p&gt;

&lt;p&gt;Specifically, starting with an initial $\boldsymbol{\Sigma}^{(0)}=\mathbf{I}$, we consider the update, called &lt;strong&gt;rank-$\gamma$ update&lt;/strong&gt;, for the covariance matrix at generation $t+1$ using &lt;strong&gt;exponential smoothing&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\boldsymbol{\Sigma}_\gamma^{(t+1)} \\ &amp;amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T} \\ &amp;amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},\label{eq:rlmu.2}
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\alpha_\gamma\leq 1$: learning rate.&lt;/li&gt;
  &lt;li&gt;$w_1,\ldots,w_\gamma$ and $\mathbf{x}_{1:\lambda}^{(t+1)},\ldots,\mathbf{x}_{\lambda:\lambda}^{(g+1)}$ are defined as usual.&lt;/li&gt;
  &lt;li&gt;$\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The update \eqref{eq:rlmu.2} can be generalized to $\lambda$ weights values which neither necessarily sum to $1$, nor be non-negative anymore, as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;amp;=\left(1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T}\label{eq:rlmu.3} \\ &amp;amp;={\boldsymbol{\Sigma}^{(t)}}^{1/2}\left[\mathbf{I}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\left(\mathbf{z}_{i:\lambda}^{(t+1)}{\mathbf{z}_{i:\lambda}^{(t+1)}}^\text{T}-\mathbf{I}\right)\right]{\boldsymbol{\Sigma}^{(t)}}^{1/2},
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$w_1\geq\ldots\geq w_\gamma&amp;gt;0\geq w_{\gamma+1}\geq\ldots\geq w_\lambda\in\mathbb{R}$, and usually $\sum_{i=1}^{\gamma}w_i=1$ and $\sum_{i=1}^{\lambda}w_i\approx 0$.&lt;/li&gt;
  &lt;li&gt;$\mathbf{z}_{i:\lambda}^{(t+1)}={\boldsymbol{\Sigma}^{(t)}}^{1/2}\mathbf{y}_{i:\lambda}^{(t+1)}$ is the mutation vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rank-one-update&quot;&gt;Rank-one update&lt;/h3&gt;
&lt;p&gt;We first consider a method that produces an $n$-dimensional normal distribution with zero mean. Specifically, let $\mathbf{y}_1,\ldots,\mathbf{y}_{t_0}\in\mathbb{R}^n$, for $t_0\geq n$ be vectors span $\mathbb{R}^n$. We thus have that
\begin{align}
\mathcal{N}(0,1)\mathbf{y}_1+\ldots+\mathcal{N}(0,1)\mathbf{y}_{t_0}&amp;amp;\sim\mathcal{N}(\mathbf{0},\mathbf{y}_1\mathbf{y}_1^\text{T})+\ldots+\mathcal{N}(\mathbf{0},\mathbf{y}_{t_0}\mathbf{y}_{t_0}^\text{T}) \\ &amp;amp;\sim\mathcal{N}\left(\mathbf{0},\sum_{i=1}^{t_0}\mathbf{y}_i\mathbf{y}_i^\text{T}\right)
\end{align}
The covariance matrix $\mathbf{y}_i\mathbf{y}_i^\text{T}$ has rank one, with only one eigenvalue $\Vert\mathbf{y}_i\Vert^2$ and a corresponding eigenvector within the form $\alpha\mathbf{y}_i$ for $\alpha\in\mathbb{R}$. Using the above equation, we can generate any MVN distribution.&lt;/p&gt;

&lt;p&gt;Consider the update \eqref{eq:rlmu.3} with $\gamma=1$ and let $\mathbf{y}_{t+1}=\left(\mathbf{x}_{1:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$, the &lt;strong&gt;rank-one update&lt;/strong&gt; for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ is given by
\begin{equation}
\boldsymbol{\Sigma}^{t+1}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{y}_{t+1}\mathbf{y}_{t+1}^\text{T}
\end{equation}
The latter summand in the RHS has rank one and adds the maximum likelihood term for $\mathbf{y}_{t+1}$ into the covariance matrix $\boldsymbol{\Sigma}^{(t)}$, which makes the probability of generating $\mathbf{y}_{t+1}$ in the generation $t+1$ increase.&lt;/p&gt;

&lt;p&gt;We continue by noticing that to update the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$, in \eqref{eq:rlmu.3}, we have used the selected steps
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}=\frac{\mathbf{x}_{i:\lambda}^{(g+1)}-\boldsymbol{\mu}^{(g)}}{\sigma^{(g)}}
\end{equation}
However, since
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}{\mathbf{y}_{i:\lambda}^{(g+1)}}^\text{T}=-\mathbf{y}_{i:\lambda}^{(g+1)}\left(-\mathbf{y}_{i:\lambda}^{(g+1)}\right)^\text{T},
\end{equation}
which means the sign information is lost when computing the covariance matrix. To track the sign information to the update rule of $\boldsymbol{\Sigma}^{(t+1)}$, we use &lt;strong&gt;evolution path&lt;/strong&gt;, which defined as a sequence of successive steps over number of generations.&lt;/p&gt;

&lt;p&gt;In particular, analogy to \eqref{eq:rlmu.3}, we use exponential smoothing to establish the evolution path, $\mathbf{p}_c\in\mathbb{R}^n$, which starting with an initial value $\mathbf{p}_c^{(0)}=\mathbf{0}$ and being updated with
\begin{align}
\mathbf{p}_c^{(t+1)}&amp;amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{(1-(1-\alpha_c)^2)\mu_\text{eff}}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\sum_{i=1}^{\gamma}\frac{w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)}{\sigma^{(t)}} \\ &amp;amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbf{p}_c^{(t)}\in\mathbb{R}^n$ is the evolution path at generation $t$.&lt;/li&gt;
  &lt;li&gt;$\alpha_c\leq 1$ is the learning rate.&lt;/li&gt;
  &lt;li&gt;$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_c^{(t+1)}$ such that
\begin{equation}
\mathbf{p}_c^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),
\end{equation}
since by $\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$ we have that
\begin{equation}
\mathbf{p}_c^{(t)}\sim\mathbf{y}_{i:\lambda}^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),\hspace{1cm}\forall i=1,\ldots,\gamma
\end{equation}
which by $\gamma_\text{eff}=\left(\sum_{i=1}^{\gamma}w_i^2\right)^{-1}$ implies that
\begin{equation}
\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}\sim\frac{1}{\sqrt{\gamma_\text{eff}}}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The &lt;strong&gt;rank-one update&lt;/strong&gt; for the covariance matrix $\boldsymbol{\Sigma}^{(t)}$ via the evolution path $\mathbf{p}_c^{(t+1)}$ then given as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T},\label{eq:rou.1}
\end{equation}
An empirical validated choice for the learning rate $\alpha_1$ is $\alpha_1\approx 2/n^2$.&lt;/p&gt;

&lt;h3 id=&quot;final-update&quot;&gt;Final update&lt;/h3&gt;
&lt;p&gt;Combining rank-$\gamma$ update \eqref{eq:rlmu.3} and rank-one update \eqref{eq:rou.1} together, we obtain the final update for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\left(1-\alpha_1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\alpha_1\approx 2/n^2$.&lt;/li&gt;
  &lt;li&gt;$\alpha_\gamma\approx\min(\gamma_\text{eff}/n^2,1-\alpha_1)$.&lt;/li&gt;
  &lt;li&gt;$\mathbf{y}_{i:\lambda}^{(t+1)}=\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$.&lt;/li&gt;
  &lt;li&gt;$\sum_{i=1}^{\lambda}w_i\approx-\alpha_1/\alpha_\gamma$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ctrl-sigma&quot;&gt;Controlling the step-size&lt;/h2&gt;
&lt;p&gt;To control the step-size $\sigma^{(t)}$, similar to how we cumulatively update the covariance matrix by rank-one covariance matrices, we also use an evolution path, which is defined as sum of successive steps $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$.&lt;/p&gt;

&lt;p&gt;However, in this step-size adaption, we utilize a conjugate evolution path $\mathbf{p}_\sigma$, which begins with an initial value $\mathbf{p}_\sigma^{(0)}=\mathbf{0}$ and is repeatedly updated by
\begin{align}
\mathbf{p}_\sigma^{(t+1)}&amp;amp;=(1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{(1-(1-\alpha_\sigma)^2)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\frac{\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}} \\ &amp;amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbf{p}_\sigma^{(t)}\in\mathbb{R}^n$ is the conjugate evolution path at generation $t$.&lt;/li&gt;
  &lt;li&gt;$\alpha_\sigma&amp;lt;1$ is the learning rate.&lt;/li&gt;
  &lt;li&gt;$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_\sigma^{(t+1)}$, which analogously to the covariance matrix adaption, lets
\begin{equation}
\mathbf{p}_\sigma^{(t+1)}\sim\mathcal{N}(\mathbf{0},\mathbf{I})
\end{equation}&lt;/li&gt;
  &lt;li&gt;The covariance matrix ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$ is defined as
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\doteq\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T},\label{eq:cs.1}
\end{equation}
where
\begin{equation}
\hspace{-0.8cm}\boldsymbol{\Sigma}^{(t)}=\mathbf{Q}^{(t)}\boldsymbol{\Lambda}^{(t)}{\mathbf{Q}^{(t)}}^\text{T}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{q}_1^{(t)}&amp;amp;\ldots&amp;amp;\mathbf{q}_n^{(t)} \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]\left[\begin{matrix}\lambda_1^{(t)}&amp;amp;&amp;amp; \\ &amp;amp;\ddots&amp;amp; \\ &amp;amp;&amp;amp; \lambda_n^{(t)}\end{matrix}\right]\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{q}_1^{(t)}&amp;amp;\ldots&amp;amp;\mathbf{q}_n^{(t)} \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]^\text{T}
\end{equation}
is an eigendecomposition of the positive definite covariance matrix $\boldsymbol{\Sigma}^{(t)}$, where $\mathbf{Q}^{(t)}\in\mathbb{R}^{n\times n}$ is the orthonormal matrix whose columns are unit eigenvectors $\mathbf{q}_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$ and $\boldsymbol{\Lambda}^{(t)}\in\mathbb{R}^{n\times n}$ is the diagonal matrix whose diagonal entries are eigenvalues $\lambda_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$.&lt;br /&gt;
Moreover, for each eigenvalue, eigenvector pair $(\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ of $\boldsymbol{\Sigma}^{(t)}$ we have
\begin{equation}
\lambda_i^{(t)}{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}={\boldsymbol{\Sigma}^{(t)}}^{-1}\boldsymbol{\Sigma}^{(t)}\mathbf{q}_i^{(t)}=\mathbf{q}_i^{(t)},
\end{equation}
or
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}=\frac{1}{\lambda_i^{(t)}}\mathbf{q}_i^{(t)},
\end{equation}
or in other words, $(1/\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ is an eigenvalue, eigenvector pair of ${\boldsymbol{\Sigma}^{(t)}}^{-1}$. Therefore, the inverse of $\boldsymbol{\Sigma}^{(t)}$, which is also positive definite can be written by
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}=\mathbf{Q}^{(t)}\left[\begin{matrix}1/\lambda_1^{(t)}&amp;amp;&amp;amp; \\ &amp;amp;\ddots&amp;amp; \\ &amp;amp;&amp;amp; 1/\lambda_n^{(t)}\end{matrix}\right]{\mathbf{Q}^{(t)}}^\text{T}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1}{\mathbf{Q}^{(t)}}^\text{T},
\end{equation}
which allows us to obtain the representation \eqref{eq:cs.1} of ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The transformation ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T}$ re-scales length of the step $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$ without changing its direction. In more specific:&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		${\mathbf{Q}^{(t)}}^\text{T}$ transform the original space into the coordinate space with columns of $\mathbf{Q}^{(t)}$, which is also the eigenvectors of $\boldsymbol{\Sigma}^{(t)}$ or the principle axes of $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})$, as its principle axes.
	&lt;/li&gt;
	&lt;li&gt;
		${\boldsymbol{\Lambda}^{(t)}}^{-1/2}$ re-scales the principle axes to have the same length.
	&lt;/li&gt;
	&lt;li&gt;
		$\mathbf{Q}^{(t)}$ transforms the coordinate system back to the original space.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;test-on-rast&quot;&gt;Testing on Rastrigin function&lt;/h2&gt;
&lt;p&gt;Let us get our hands dirty by giving the CMA-ES algorithm a try on the &lt;a href=&quot;https://en.wikipedia.org/wiki/Rastrigin_function&quot;&gt;&lt;strong&gt;Rastrigin function&lt;/strong&gt;&lt;/a&gt;, which is given by
\begin{equation}
f(\mathbf{x})=10 n+\sum_{i=1}^{n}x_i^2-10\cos\left(2\pi x_i\right)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Nikolaus Hansen. &lt;a href=&quot;#https://arxiv.org/abs/1604.00772&quot;&gt;The CMA Evolution Strategy: A Tutorial&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The simplest form of &lt;strong&gt;exponential smoothing&lt;/strong&gt; is given by the formula
\begin{align*}
s_0&amp;amp;=x_0 \\ s_t&amp;amp;=\alpha x_t+(1-\alpha)s_{t-1},\hspace{1cm}t&amp;gt;0
\end{align*}
where $0&amp;lt;\alpha&amp;lt;1$ is referred as the &lt;strong&gt;smoothing factor&lt;/strong&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="machine-learning" /><category term="artificial-intelligent" /><category term="evolution-strategy" /><summary type="html">A note on CMA - Evolution Strategy</summary></entry><entry><title type="html">Neural networks</title><link href="http://localhost:4000/artificial-intelligent/machine-learning/2022/09/02/neural-nets.html" rel="alternate" type="text/html" title="Neural networks" /><published>2022-09-02T13:00:00+07:00</published><updated>2022-09-02T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/machine-learning/2022/09/02/neural-nets</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/machine-learning/2022/09/02/neural-nets.html">&lt;blockquote&gt;
  &lt;p&gt;A note on Neural networks.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#ff-func&quot;&gt;Feed-forward network functions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#unv-approx&quot;&gt;Universal approximation property&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#w-s-sym&quot;&gt;Weight-space symmetries&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#net-training&quot;&gt;Network training&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#output-prob-itp&quot;&gt;Network outputs probabilistic interpretation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#univ-output&quot;&gt;Univariate regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mult-output&quot;&gt;Multivariate regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bi-clf&quot;&gt;Binary classification&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mult-clf&quot;&gt;Multi-class classification&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#param-opt&quot;&gt;Parameter optimization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backprop&quot;&gt;Backpropagation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#erf-drv&quot;&gt;Error-function derivatives&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#jacobian-mtx&quot;&gt;Jacobian matrix&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#hessian-mtx&quot;&gt;Hessian matrix&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-nn&quot;&gt;Bayesian neural networks&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#posterior-param-dist&quot;&gt;Posterior parameter distribution&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preferences&quot;&gt;Preferences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ff-func&quot;&gt;Feed-forward network functions&lt;/h2&gt;
&lt;p&gt;Recall that the &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html&quot;&gt;linear models&lt;/a&gt; used in regression and classification are based on linear combination of fixed nonlinear basis function $\phi_j(\mathbf{x})$ and take the form
\begin{equation}
y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\mathbf{x})\right),\label{1}
\end{equation}
where in the case of regression, $f$ is the function $f(x)=x$, while in the classification case, $f$ takes the form of a nonlinear activation function (e.g., the &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#logistic-sigmoid-func&quot;&gt;sigmoid function&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Neural networks&lt;/strong&gt; extend this model \eqref{1} by letting each basis functions $\phi_j(\mathbf{x})$ be a nonlinear function of a linear combination of the inputs, where the coefficients in the combination are the adaptive parameters.&lt;/p&gt;

&lt;p&gt;More formally, neural networks is a series of layers, in which each layer represents a functional transformation. Let us consider the first layer by constructing $M$ linear combinations of the input variable $x_1,\ldots,x_D$ in the form
\begin{equation}
a_j=\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)},\label{2}
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$j=1,\ldots,M$;&lt;/li&gt;
  &lt;li&gt;the superscript $^{(1)}$ indicates that we are working with parameters of the first layer;&lt;/li&gt;
  &lt;li&gt;$w_{ji}^{(1)}$’s are called the &lt;strong&gt;weights&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;$w_{j0}^{(1)}$’s are known as the &lt;strong&gt;biases&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;$a_j$’s are referred as &lt;strong&gt;activations&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The activations $a_j$’s are then transformed using a differentiable, nonlinear &lt;strong&gt;activation function&lt;/strong&gt; $h(\cdot)$, which correspond to $f(\cdot)$ in \eqref{1} to give
\begin{equation}
z_j=h(a_j),\label{3}
\end{equation}
where $z_j$ are called the &lt;strong&gt;hidden units&lt;/strong&gt;. Repeating the same procedure as \eqref{2}, which was following \eqref{1}, $z_j$’s are taken as the inputs of the second layer to give $K$ outputs
\begin{equation}
a_k=\sum_{j=1}^{M}w_{kj}^{(2)}z_j+w_{k0}^{(2)},\label{4}
\end{equation}
where $k=1,\ldots,K$.&lt;/p&gt;

&lt;p&gt;This process will be repeated in $L$ times with $L$ is the number of layers. At the last layer, for instance, the second layer of our example network, the outputs, also called &lt;strong&gt;output unit activations&lt;/strong&gt;, $a_k$’s are transformed using an appropriate activation function to give a set of network output $y_k$. For example, in multiple binary classification problems, we can choose the logistic sigmoid as our activation function that
\begin{equation}
y_k=\sigma(a_k)\label{5}
\end{equation}
Combining all these steps \eqref{2}, \eqref{3}, \eqref{4} and \eqref{5} together, our neural network with sigmoidal output unit activation functions can be defined as
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right),\label{6}
\end{equation}
where all of the weights and biases are comprises together into a parameter vector $\mathbf{w}$. As suggested in &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#dummy-coeff&quot;&gt;linear regression&lt;/a&gt;, we can also let the bias $w_{j0}^{(1)}$ be coefficient of a dummy input variable $x_0=1$ that makes \eqref{2} can be written as
\begin{equation}
a_j=\sum_{i=0}^{D}w_{ji}^{(1)}x_i
\end{equation}
This results that our subsequent layers are also able to be written in a more convenient form, which lets the entire network \eqref{6} take the form
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=0}^{M}w_{kj}^{(2)}h\left(\sum_{i=0}^{D}w_{ji}^{(1)}x_i\right)\right)
\end{equation}
Our network is also an example of a &lt;strong&gt;multilayer perception&lt;/strong&gt;, or &lt;strong&gt;MLP&lt;/strong&gt;, which is a combination of &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#perceptron&quot;&gt;perceptron models&lt;/a&gt;. The key difference is that while the neural network uses continuous sigmoidal nonlinearities in the hidden units, which is differentiable w.r.t the parameters, the perceptron algorithm uses step-function nonlinearities, which is in contrast non-differentiable.&lt;/p&gt;

&lt;p&gt;The network network we have been considering so far is &lt;strong&gt;feed-forward neural network&lt;/strong&gt;, whose outputs are deterministic functions of the inputs. Each (hidden or output) unit in such a network computes a function given by
\begin{equation}
z_k=h\left(\sum_{j}w_{kj}z_j\right),
\end{equation}
where the sum runs all over units sending connections to unit $k$ (bias included).&lt;/p&gt;

&lt;h3 id=&quot;unv-approx&quot;&gt;Universal approximation property&lt;/h3&gt;
&lt;p&gt;Feed-forward networks with &lt;strong&gt;hidden layers&lt;/strong&gt; (i.e., the layers in which the training data does not show the desired output, e.g., the first layer of our network, the second layer on the other hands is called the &lt;strong&gt;output layer&lt;/strong&gt;) provide &lt;strong&gt;universal approximation&lt;/strong&gt; property.&lt;/p&gt;

&lt;p&gt;In concrete, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any &lt;strong&gt;squashing&lt;/strong&gt; activation function (e.g., the logistic sigmoid function) an approximate any continuous function on a compact subsets of $\mathbb{R}^n$.&lt;/p&gt;

&lt;h3 id=&quot;w-s-sym&quot;&gt;Weight-space symmetries&lt;/h3&gt;

&lt;h2 id=&quot;net-training&quot;&gt;Network training&lt;/h2&gt;

&lt;h3 id=&quot;output-prob-itp&quot;&gt;Network outputs probabilistic interpretation&lt;/h3&gt;

&lt;h4 id=&quot;univ-output&quot;&gt;Univariate regression&lt;/h4&gt;
&lt;p&gt;Consider the &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#least-squares-reg&quot;&gt;regression problem&lt;/a&gt; in which the target variable $t$ has Gaussian distribution with an $\mathbf{x}$ dependent mean
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=\mathcal{N}(t\vert y(\mathbf{x},\mathbf{w}),\beta^{-1}),
\end{equation}
For the conditional distribution above, it is sufficient to take the output unit activation function to be the function $h(x)=x$, because such a network can approximate any continuous function from $\mathbf{x}$ to $y$.&lt;/p&gt;

&lt;p&gt;Given the data set $(\mathbf{X},\mathbf{t})=\{\mathbf{x}_n,t_n\}$, where $\mathbf{x}_n$’s are i.i.d for $n=1,\ldots,N$, and where
\begin{align}
\mathbf{X}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{x}_1&amp;amp;\ldots&amp;amp;\mathbf{x}_N \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right],\hspace{1cm}\mathbf{t}=\left[\begin{matrix}t_1 \\ \vdots \\ t_N\end{matrix}\right]
\end{align}
The likelihood function therefore can be given by
\begin{align}
p(t\vert\mathbf{X},\mathbf{w},\beta)&amp;amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w},\beta) \\ &amp;amp;=\prod_{n=1}^{N}\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1})
\end{align}
With a minor change as usual that taking negative natural logarithm of both sides gives us
\begin{align}
-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w},\beta)&amp;amp;=-\sum_{n=1}^{N}\log\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1}) \\ &amp;amp;=\frac{\beta}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2-\frac{N}{2}\log\beta+\frac{N}{2}\log 2\pi
\end{align}
Therefore, maximizing the likelihood function $p(\mathbf{t}\vert\mathbf{X},\mathbf{x},\beta)$ is equivalent to minimizing the sum-of-squares error function given as
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2,
\end{equation}
This also means the value of $\mathbf{w}$ that minimizes $E(\mathbf{w})$ will be $\mathbf{w}_\text{ML}$, which implies that the corresponding solution for $\beta$ will be given by
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{N}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w}_\text{ML})-t_n\big)^2
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;mult-output&quot;&gt;Multivariate regression&lt;/h4&gt;
&lt;p&gt;Similarly, we consider the multiple target variables case, in which the conditional distribution of the target therefore takes the form
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}\vert\mathbf{y}(\mathbf{x},\mathbf{w}),\beta^{-1}\mathbf{I})
\end{equation}
Repeating the same procedure as the univariate case, maximizing likelihood function is also equivalent to minimizing the sum-of-squares error function given by
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w})-\mathbf{t}_n\big\Vert^2,
\end{equation}
which gives us the solution for the noise precision $\beta$ in the multivariate case as
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{NK}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w}_\text{ML})-\mathbf{t}_n\big\Vert^2,
\end{equation}
where $K$ is the number of target variables.&lt;/p&gt;

&lt;h4 id=&quot;bi-clf&quot;&gt;Binary classification&lt;/h4&gt;
&lt;p&gt;Consider the problem of binary classification which outputs $t=1$ to denote class $\mathcal{C}_1$ and otherwise to denote class $\mathcal{C}_2$.&lt;/p&gt;

&lt;p&gt;In particular, we consider a network having a single output whose activation function is a logistic sigmoid
\begin{equation}
y=\sigma(a)\doteq\frac{1}{1+\exp(-a)},
\end{equation}
which follows immediately that $0\leq y(\mathbf{x},\mathbf{w})\leq 1$.&lt;/p&gt;

&lt;p&gt;This suggests us interpreting $y(\mathbf{x},\mathbf{w})$ as the conditional probability for class $\mathcal{C}_1$, $p(\mathcal{C}_1\vert\mathbf{x})$, and hence the corresponding conditional probability for class $\mathcal{C}_2$ will be $p(\mathcal{C}_2\vert\mathbf{x})=1-y(\mathbf{x},\mathbf{w})$. Or in other words, the conditional distribution $p(t\vert\mathbf{x},\mathbf{w})$ of targets $t$ given inputs $\mathbf{x}$ is then a Bernoulli distribution of the form
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=y(\mathbf{x},\mathbf{w})^t\big(1-y(\mathbf{x},\mathbf{w})\big)^{1-t}
\end{equation}
If we consider a training set of $N$ independent observations as in the two regression tasks above, the likelihood function of our classification task will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;amp;=\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n}
\end{align}
Taking the negative natural logarithm of the likelihood as above gives us the cross-entropy error function
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;amp;=-\log\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n} \\ &amp;amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n),
\end{align}
where $y_n=y(\mathbf{x}_n,\mathbf{w})$.&lt;/p&gt;

&lt;p&gt;Moreover, consider the partial derivative of this error function w.r.t the activation $a_i$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_i}&amp;amp;=\frac{\partial}{\partial a_i}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;amp;=-\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_i}-\frac{1-t_i}{1-y_i}\frac{\partial(1-y_i)}{\partial a_i} \\ &amp;amp;=\frac{\partial y_i}{\partial a_i}\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;amp;=y_i(1-y_i)\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;amp;=y_i-t_i,\label{eq:bin-clf-drv-error-a}
\end{align}
where in the forth step, we have use the identity of the &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#sigmoid-derivative&quot;&gt;derivative of sigmoid function&lt;/a&gt; that
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;mult-clf&quot;&gt;Multi-class classification&lt;/h4&gt;
&lt;p&gt;For the multi-class classification that assigns input variables to $K$ separated classes, we can use the network with $K$ outputs each of which has a logistic sigmoid activation function. Each output $t_k\in\{0,1\}$ for $k=1,\ldots,K$ indicates whether the input will be assigned to class $\mathcal{C}_k$&lt;/p&gt;

&lt;p&gt;We first consider the case that the class labels are independent given the input vector, which means the conditional distributions for class $C_k$’s will be $K$ i.i.d Bernoulli distributions, in which the conditional probability for class $\mathcal{C}_k$ will take the form
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w})=y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{equation}
Therefore, the joint distribution of them, the conditional distribution of the target variables will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})&amp;amp;=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w}) \\ &amp;amp;=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{align}
Let $\mathbf{T}$ denote the combination of all the targets $\mathbf{t}_n$, i.e.,
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right],
\end{equation}
the likelihood function therefore takes the form of
\begin{align}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_k}\label{eq:mult-clf-llh}
\end{align}
Analogy to the binary case, taking the negative natural logarithm of the likelihood \eqref{eq:mult-clf-llh} gives us the corresponding cross-entropy error function for the multi-class case, given as
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_{nk}} \\ &amp;amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}),\label{eq:mult-clf-error}
\end{align}
where $y_{nk}$ is short for $y_k(\mathbf{x}_n,\mathbf{w})$.&lt;/p&gt;

&lt;p&gt;Similar to the binary case, consider the partial derivative of the error function \eqref{eq:mult-clf-error} w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}) \\ &amp;amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)\frac{\partial y_{ij}}{\partial a_{ij}} \\ &amp;amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)y_{ij}(1-y_{ij}) \\ &amp;amp;=y_{ij}-t_{ij}\label{eq:mult-drv-error-a}
\end{align}
which takes the same form as \eqref{eq:bin-clf-drv-error-a}&lt;/p&gt;

&lt;p&gt;On the other hands, if each input is assigned only to one of $K$ classes (mutually exclusive), the conditional distributions for class $C_k$ will be instead given as
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x})=p(t_k=1\vert\mathbf{x})=y_k(\mathbf{x},\mathbf{w}),
\end{equation}
and thus the conditional distribution of the targets is
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})=\prod_{k=1}^{K}p(t_k=1\vert\mathbf{x})^{t_k}=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}
\end{equation}
The likelihood is therefore given as
\begin{equation}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w})=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}},
\end{equation}
which gives us the following cross-entropy error function by taking the negative natural logarithm
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_{nk}} \\ &amp;amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_k(\mathbf{x}_n,\mathbf{w})\label{eq:mult-me-clf-error}
\end{align}
As discussed in &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#softmax-reg&quot;&gt;Softmax regression&lt;/a&gt;, we see that the output unit activation function is given by the softmax function
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\frac{\exp\big[a_k(\mathbf{x},\mathbf{w})\big]}{\sum_{j=1}^{K}\exp\big[a_j(\mathbf{x},\mathbf{w})\big]}
\end{equation}
Taking the derivative of the error function \eqref{eq:mult-me-clf-error}  w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk} \\ &amp;amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}\frac{\partial y_{ik}}{\partial a_{ij}} \\ &amp;amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}y_{ik}(1\{j=k\}-y_{ij})\label{53} \\ &amp;amp;=y_{ij}\sum_{k=1}^{K}t_{ik}-\sum_{k=1}^{K}t_{ik}1\{j=k\} \\ &amp;amp;=y_{ij}-t_{ij}
\end{align}
where we have used the identity of the &lt;a href=&quot;/artificial-intelligent/machine-learning/2022/08/13/glm.html#softmax-derivative&quot;&gt;derivative of the softmax function&lt;/a&gt; in the forth step to obtain \eqref{53}.&lt;/p&gt;

&lt;h3 id=&quot;param-opt&quot;&gt;Parameter optimization&lt;/h3&gt;
&lt;p&gt;In training neural network to find a value of $\mathbf{w}$ to minimize the error function $E(\mathbf{w})$, we usually start with some initial value $\mathbf{w}_0$ and iteratively update the weight vector $\mathbf{w}$, in which the weight at time step $\tau+1$ is given as
\begin{equation}
\mathbf{w}^{(t+1)}=\mathbf{w}^{(\tau)}+\Delta\mathbf{w}^{(\tau)},
\end{equation}
where $\Delta\mathbf{w}^{(\tau)}$ is some update rule.&lt;/p&gt;

&lt;p&gt;At each time step $\tau$, there are two distinct stages:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Stage 1 refers to evaluating the derivatives of the error function w.r.t the weights, which can be accomplished efficiently using &lt;strong&gt;backpropagation&lt;/strong&gt; that will be discussed in the next section.&lt;/li&gt;
  &lt;li&gt;Stage 2 relates to using those computed derivatives to calculate the adjustments to be made to the weights $\mathbf{w}$. &lt;strong&gt;Gradient descent&lt;/strong&gt;, for instance, is the simplest approach in which each time step the weights take a small step in the direction of the negative gradient, as
\begin{equation}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta\nabla_\mathbf{w}E(\mathbf{w}^{(\tau)}),
\end{equation}
where $\eta&amp;gt;0$ is called the &lt;strong&gt;learning rate&lt;/strong&gt; of the update.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;backprop&quot;&gt;Backpropagation&lt;/h3&gt;
&lt;p&gt;In this section, we will consider the use of &lt;strong&gt;backpropagation&lt;/strong&gt; technique to evaluate the first and second derivatives of error-functions w.r.t the weights and also the derivatives of the network outputs w.r.t the inputs.&lt;/p&gt;

&lt;h4 id=&quot;erf-drv&quot;&gt;Error-function derivatives&lt;/h4&gt;
&lt;p&gt;We first consider the case of evaluating the first order derivative of the error function w.r.t to the weight parameter $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;Consider a simple linear model where the outputs $y_k$’s are linear combinations of the input variable $x_i$’s
\begin{equation}
y_k=\sum_{i}w_{ki}x_i,
\end{equation}
together with the error function, in which the error function for the $n$ data point is defined as
\begin{equation}
E_n(\mathbf{w})=\frac{1}{2}\sum_{k}(y_{nk}-t_{nk})^2,
\end{equation}
where $y_{nk}=y_k(\mathbf{x}_n,\mathbf{w})$.&lt;/p&gt;

&lt;p&gt;The gradient of this error function w.r.t to a weight $w_{ji}$ then can be computed by
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}}=(y_{nj}-t_{nj})x_{ni}
\end{equation}
In a general feed-forward network, each unit is a weighted sum of its inputs
\begin{equation}
a_j=\sum_{i}w_{ji}z_i
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;jacobian-mtx&quot;&gt;Jacobian matrix&lt;/h4&gt;

&lt;h4 id=&quot;hessian-mtx&quot;&gt;Hessian matrix&lt;/h4&gt;

&lt;h2 id=&quot;bayes-nn&quot;&gt;Bayesian neural networks&lt;/h2&gt;

&lt;h3 id=&quot;posterior-param-dist&quot;&gt;Posterior parameter distribution&lt;/h3&gt;

&lt;h2 id=&quot;preferences&quot;&gt;Preferences&lt;/h2&gt;
&lt;p&gt;[1] Christopher M. Bishop. &lt;a href=&quot;https://link.springer.com/book/9780387310732&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;. Springer New York, NY.&lt;/p&gt;

&lt;p&gt;[2] Ian Goodfellow &amp;amp; Yoshua Bengio &amp;amp; Aaron Courville. &lt;a href=&quot;https://www.deeplearningbook.org&quot;&gt;Deep Learning&lt;/a&gt;. MIT Press (2016).&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="machine-learning" /><category term="artificial-intelligent" /><category term="machine-learning" /><category term="neural-network" /><summary type="html">A note on Neural networks.</summary></entry><entry><title type="html">Measure theory - III: the Lebesgue integral</title><link href="http://localhost:4000/mathematics/measure-theory/2022/08/21/measure-theory-p3.html" rel="alternate" type="text/html" title="Measure theory - III: the Lebesgue integral" /><published>2022-08-21T13:00:00+07:00</published><updated>2022-08-21T13:00:00+07:00</updated><id>http://localhost:4000/mathematics/measure-theory/2022/08/21/measure-theory-p3</id><content type="html" xml:base="http://localhost:4000/mathematics/measure-theory/2022/08/21/measure-theory-p3.html">&lt;blockquote&gt;
  &lt;p&gt;Part III of the measure theory series. Materials are mostly taken from &lt;a href=&quot;/mathematics/measure-theory/2022/08/21/measure-theory-p3.html#taos-book&quot;&gt;Tao’s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&quot;/mathematics/measure-theory/2022/08/21/measure-theory-p3.html#steins-book&quot;&gt;Stein’s book&lt;/a&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#int-simp-funcs&quot;&gt;Integration of simple functions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#simp-func&quot;&gt;Simple function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#int-unsgn-simp-func&quot;&gt;Integral of unsigned simple functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#well-dfn-simp-int&quot;&gt;Well-definedness of simple integral&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#alm-evwhr-spt&quot;&gt;Almost everywhere and support&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bsc-prop-simp-unsgn-int&quot;&gt;Basic properties of the simple unsigned integral&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abs-cvg-simp-int&quot;&gt;Absolutely convergence simple integral&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bsc-prop-cmplx-simp-int&quot;&gt;Basic properties of the complex-valued simple integral&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#msr-funcs&quot;&gt;Measurable functions&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#unsgn-msr-funcs&quot;&gt;Unsigned measurable functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#equiv-ntn-msrb&quot;&gt;Equivalent notions of measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cmplx-msrb&quot;&gt;Complex measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#equiv-ntn-cmplx-msrb&quot;&gt;Equivalent notions of complex measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eg-msr-func&quot;&gt;Examples of measurable function&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unsgn-lebesgue-int&quot;&gt;Unsigned Lebesgue integrals&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lwr-unsgn-lebesgue-int&quot;&gt;Lower unsigned Lebesgue integral&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abs-intb&quot;&gt;Absolute integrability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#littlewoods-prncpl&quot;&gt;Littlewood’s three principles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;int-simp-funcs&quot;&gt;Integration of simple functions&lt;/h2&gt;
&lt;p&gt;Analogy to how the &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#riemann-integrability&quot;&gt;&lt;strong&gt;Riemann integral&lt;/strong&gt;&lt;/a&gt; was established by using the integral for &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#pc-func&quot;&gt;&lt;strong&gt;piecewise constant functions&lt;/strong&gt;&lt;/a&gt;, the &lt;strong&gt;Lebesgue integral&lt;/strong&gt; is set up using the integral for &lt;strong&gt;simple functions&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;simp-func&quot;&gt;Simple function&lt;/h3&gt;
&lt;p&gt;A (complex-valued) &lt;strong&gt;simple function&lt;/strong&gt; $f:\mathbb{R}^d\to\mathbb{C}$ is a finite linear combination
\begin{equation}
f=c_1 1_{E_1}+\ldots+c_k 1_{E_k},\label{eq:sf.1}
\end{equation}
of indicator functions $1_{E_i}$ of Lebesgue measurable sets $E_i\subset\mathbb{R}^d$ for $i=1,\ldots,k$, for natural number $k\geq 0$ and where $c_1,\ldots,c_k\in\mathbb{C}$ are complex numbers.&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;unsigned simple function&lt;/strong&gt; $f:\mathbb{R}^d\to[0,+\infty]$ is given as \eqref{eq:sf.1} but with the $c_i$ taking values in $[0,+\infty]$ rather than $\mathbb{C}$.&lt;/p&gt;

&lt;h3 id=&quot;int-unsgn-simp-func&quot;&gt;Integral of a unsigned simple function&lt;/h3&gt;
&lt;p&gt;If $f=c_1 1_{E_1}+\ldots+c_k 1_{E_k}$ is an unsigned simple function, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq c_1 m(E_1)+\ldots+c_k m(E_k),
\end{equation}
which means $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\in[0,+\infty]$.&lt;/p&gt;

&lt;h3 id=&quot;well-dfn-simp-int&quot;&gt;Well-definedness of simple integral&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $k,k’\geq 0$ be natural, $c_1,\ldots,c_k,c_1’,\dots,c_{k’}’\in[0,+\infty]$ and $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’\subset\mathbb{R}^d$ be Lebesgue measurable sets such that the identity
\begin{equation}
c_1 1_{E_1}+\ldots+c_k 1_{E_k}=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’}\label{eq:lemma1.1}
\end{equation}
holds identically on $\mathbb{R}^d$. Then we have&lt;/em&gt;
\begin{equation}
c_1 m(E_1)+\ldots+c_k m(E_k)=c_1’ m(E_1’)+\ldots+c_{k’}’ m(E_{k’}’)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
The $k+k’$ sets $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ partition $\mathbb{R}^d$ into $2^{k+k’}$ disjoint sets, each of which is an intersection of some of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ and their complements&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Removing any sets that are empty, we end up with a partition of $R^d$ of $m$ non-empty disjoint sets $A_1,\ldots,A_m$ for some $0\leq m\leq 2^{k+k’}$. It easily seen that $A_1,\ldots,A_m$ are then Lebesgue measurable due to the Lebesgue measurability of $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$.&lt;/p&gt;

&lt;p&gt;With this set up, each of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ are unions of some of the $A_1,\ldots,A_m$. Or in other words, we have
\begin{equation}
E_1=\bigcup_{j\in J_i}A_j,
\end{equation}
and
\begin{equation}
E_{i’}’=\bigcup_{j’\in J_{i’}’}A_j’,
\end{equation}
for all $i=1,\ldots,k$ and $i’=1,\ldots,k’$, and some subsets $J_i,J_{i’}’\subset\{1,\ldots,m\}$. By finite additivity property of Lebesgue measure, we therefore have
\begin{equation}
m(E_i)=\sum_{j\in J_i}m(A_j)
\end{equation}
and
\begin{equation}
m(E_{i’}’)=\sum_{j\in J_{i’}’}m(A_j)
\end{equation}
Hence, the problem remains to show that
\begin{equation}
\sum_{i=1}^{k}c_i\sum_{j\in J_i}m(A_j)=\sum_{i’=1}^{k’}c_{i’}’\sum_{j\in J_{i’}’}m(A_j)\label{eq:lemma1.2}
\end{equation}
Fix $1\leq j\leq m$, we have that at each point $x$ in the non-empty set $A_j$, $1_{E_i}(x)$ is equal to $1_{J_i}(j)$, and similarly $1_{E_{i’}’}(x)$ is equal to $1_{J_{i’}’}(j)$. Then from \eqref{eq:lemma1.1} we have
\begin{equation}
\sum_{i=1}^{k}c_i 1_{J_i}(j)=\sum_{i’=1}^{k’}c_{i’}’1_{J_{i’}’}(j)
\end{equation}
Multiplying both sides by $m(A_j)$ and then summing over all $j=1,\ldots,m$, we obtain \eqref{eq:lemma1.2}&lt;/p&gt;

&lt;h3 id=&quot;alm-evwhr-spt&quot;&gt;Almost everywhere and support&lt;/h3&gt;
&lt;p&gt;A property $P(x)$ of a point $x\in\mathbb{R}^d$ is said to hold &lt;strong&gt;(Lebesgue) almost everywhere&lt;/strong&gt; in $\mathbb{R}^d$ or for &lt;strong&gt;(Lebesgue) almost every point&lt;/strong&gt; $x\in\mathbb{R}^d$, if the set of $x\in\mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure of zero (i.e. $P$ is true outside of a null set).&lt;/p&gt;

&lt;p&gt;Two functions $f,g:\mathbb{R}^d\to Z$ into an arbitrary range $Z$ are referred to &lt;strong&gt;agree almost everywhere&lt;/strong&gt; if we have $f(x)=g(x)$ almost every $x\in\mathbb{R}^d$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;support&lt;/strong&gt; of a function $f:\mathbb{R}^d\to\mathbb{C}$ or $f:\mathbb{R}^d\to[0,+\infty]$ is defined to be the set $\{x\in\mathbb{R}^d:f(x)\neq 0\}$ where $f$ is non-zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 2&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If $P(x)$ holds for almost every $x$, and $P(x)$ implies $Q(x)$, then $Q(x)$ holds for almost every $x$.&lt;/li&gt;
  &lt;li&gt;If $P_1(x),P_2(x),\ldots$ are an at most countable family of properties, each of which individually holds for almost every $x$, then they will simultaneously holds for almost every $x$, since the countable union of null sets is still a null set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;bsc-prop-simp-unsgn-int&quot;&gt;Basic properties of the simple unsigned integral&lt;/h3&gt;
&lt;p&gt;Let $f,g:\mathbb{R}^d\to[0,+\infty]$ be simple unsigned functions.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Unsigned linearity&lt;/b&gt;. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in[0,+\infty]$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finiteness&lt;/b&gt;. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&amp;lt;\infty$ iff $f$ is finite almost everywhere, and its support has finite measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Vanishing&lt;/b&gt;. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=0$ iff $f$ is zero almost everywhere.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Equivalence&lt;/b&gt;. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $f(x)\leq g(x)$ for almost every $x\in\mathbb{R}^d$, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\leq\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Compatibility with Lebesgue measure&lt;/b&gt;. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $f,g:\mathbb{R}^d\to[0,+\infty]$ are simple unsigned functions, we can assume that
\begin{align}
f&amp;amp;=c_1 1_{E_1}+\ldots+c_k 1_{E_k}, \\ g&amp;amp;=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’},
\end{align}
where $c_1,\ldots,c_k,c_1’,\ldots,c_{k’}’\in[0,+\infty]$.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Unsigned linearity&lt;/b&gt;&lt;br /&gt;
		We have
		\begin{align}
		\hspace{-1cm}\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx&amp;amp;=c_1 m(E_1)+\ldots+c_k m(E_k)+c_1&apos; m(E_1&apos;)+\ldots+c_{k&apos;}&apos; m(E_{k&apos;}&apos;) \\ &amp;amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For any $c\in[0,+\infty]$, we have
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;amp;=c\left(c_1 m(E_1)+\ldots+c_k m(E_k)\right) \\ &amp;amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finiteness&lt;/b&gt;&lt;br /&gt;
		Given $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&amp;lt;\infty$, then for every $i=1,\ldots,k$ we have that
		\begin{equation}
		c_i m(E_i)&amp;lt;\infty\label{eq:bpsui.1}
		\end{equation}
		Suppose that $f$ is not finite almost everywhere, which means that there exists $1\leq i\leq k$ such that $E_i$ is a non-null set and $c_i=\infty$, or
		\begin{equation}
		c_i m(E_i)=\infty,
		\end{equation}
		which is in contrast with \eqref{eq:bpsui.1}.&lt;br /&gt;
		Suppose that the support of $f$ has infinite measure, or in other word
		\begin{equation}
		c_i\neq 0,\hspace{1cm}i=1,\ldots,k\label{eq:bpsui.2}
		\end{equation}
		and
		\begin{equation}
		m\left(\bigcup_{n=1}^{k}E_n\right)=\infty,
		\end{equation}
		Since any $k$ subsets $E_1,\ldots,E_k$ of $\mathbb{R}^d$ partition $\mathbb{R}^d$ into $2^k$ disjoint sets, say $F_1,\ldots,F_{2^k}$. Hence, by finite additivity property of Lebesgue measure, we have
		\begin{equation}
		\sum_{n=1}^{2^k}m(F_n)=\infty,
		\end{equation}
		which implies that there exists $1\leq n\leq 2^k$ such that $m(F_n)=\infty$. And therefore, for a particular $1\leq i\leq k$ such that $F_n\subset E_i$, by monotonicity property of Lebesgue measure
		\begin{equation}
		m(E_i)\geq m(F_n)=\infty
		\end{equation}
		Thus, combining with \eqref{eq:bpsui.2} gives us
		\begin{equation}
		c_i m(E_1)=\infty,
		\end{equation}
		which again contradicts to \eqref{eq:bpsui.1}.&lt;br /&gt;
		Given $f$ is finite almost everywhere and its support has finite measure, suppose that its integral is infinite, or
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=\infty,
		\end{equation}
		which implies that there exists $1\leq i\leq k$ such that either&lt;br /&gt;
		(1) $c_i=\infty$ and $E_i$ is a non-null set, or&lt;br /&gt;
		(2) $c_i\neq 0$ and $m(E)=\infty$.&lt;br /&gt;
		If (1) happens, we then have that
		\begin{equation}
		f\geq c_i 1_{E_i}=\infty,
		\end{equation}
		which contradicts to our hypothesis.&lt;br /&gt;
		If (2) happens, by monotonicity of Lebesgue measure, the support of $f$ then has infinite measure, which also contradicts to our hypothesis.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Vanishing&lt;/b&gt;&lt;br /&gt;
		Given $\text{Simp}\int_{\mathbf{R^d}}f(x)\,dx=0$, we then have
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=0,
		\end{equation}
		which implies that for every $1\leq i\leq k$, we have that $c_i=0$ or $E_i$ is a null set.
		Therefore, $f$ is zero almost everywhere because in this case $f$ takes the value of non-zero iff $x$ is in a particular null set $E_j$.&lt;br /&gt;
		Given $f$ is zero almost everywhere, for every $i=1,\ldots,k$, we have that either&lt;br /&gt;
		(1) $c_i=0$, or&lt;br /&gt;
		(2) $c_i\neq 0$ and $x\notin E_i$ with $E_i$ is a null set, or&lt;br /&gt;
		(3) $c_i=0$ and and $x\notin E_i$ with $E_i$ is a null set.&lt;br /&gt;
		Therefore, the integral of $f$
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=c_1 m(E_1)+\ldots+c_k m(E_k)=0
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Equivalence&lt;/b&gt;&lt;br /&gt;
		Given $f$ and $g$ agree almost everywhere, we have that at any point $x\in\mathbb{R}^d$ such that $f(x)=g(x)$, by &lt;b&gt;lemma 1&lt;/b&gt;, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		For more convenient, let $K=\{E_i\cap E_{i&apos;}&apos;:1\leq i\leq k,1\leq i&apos;\leq k&apos;\}$. The set $K$ then has cardinality of $kk&apos;$. Thus, without loss of generality, we can denote $K$ as
		\begin{equation}
		K=\{K_1,\ldots,K_{kk&apos;}\}
		\end{equation}
		With this definition of $K$, the functions $f$ and $g$ can be rewritten by
		\begin{equation}
		f=a_1 1_{K_1}+\ldots+a_{kk&apos;}1_{K_{kk&apos;}}\label{eq:bpsui.3}
		\end{equation}
		and
		\begin{equation}
		g=b_1 1_{K_1}+\ldots+b_{kk&apos;}1_{K_{kk&apos;}}\label{eq:bpsui.4}
		\end{equation}
		On the other hand, the set in which $f(x)\neq g(x)$ is a null set. Thus by \eqref{eq:bpsui.3} and \eqref{eq:bpsui.4}, we have $x\in A$, where some $A\subset K$ is a null set, and for each $i$ such that $K_i\subset A$ (thus is also a null set, or $m(K_i)=0$), $a_i\neq b_i$, otherwise if $K_i\notin A$, $a_i=b_i$. Therefore, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		which proves our claim.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;&lt;br /&gt;
		Using the same procedure as the proof for equivalence, our claim can be proved.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Compatibility with Lebesgue measure&lt;/b&gt;&lt;br /&gt;
		This follows directly from definition
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;abs-cvg-simp-int&quot;&gt;Absolutely convergence simple integral&lt;/h3&gt;
&lt;p&gt;A complex valued simple function $f:\mathbb{R}^d\to\mathbb{C}$ is known as &lt;strong&gt;absolutely integrable&lt;/strong&gt; if
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}\vert f(x)\vert\,dx&amp;lt;\infty
\end{equation}
If $f$ is absolutely integrable, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined for real signed $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}f_+(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}f_-(x)\,dx,
\end{equation}
where
\begin{align}
f_+(x)&amp;amp;\doteq\max\left(f(x),0\right), \\ f_-(x)&amp;amp;\doteq\max\left(-f(x),0\right),
\end{align}
and for complex-valued $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}\text{Re}\,f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}\,f(x)\,dx
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;bsc-prop-cmplx-simp-int&quot;&gt;Basic properties of the complex-valued simple integral&lt;/h3&gt;
&lt;p&gt;Let $f,g:\mathbb{R}^d\to\mathbb{C}$ be absolutely integrable simple functions&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;*-linearity&lt;/b&gt;. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in\mathbb{C}$. Also we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Equivalence&lt;/b&gt;. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Compatibility with Lebesgue measure&lt;/b&gt;. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We first consider the case of real-valued $f$ and $g$.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;*-linearity&lt;/b&gt;&lt;br /&gt;
		Using the identity
		\begin{equation}
		f+g=(f+g)_{+}-(f+g)_{-}=(f_{+}-f_{-})+(g_{+}-g_{-})
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Equivalence&lt;/b&gt;&lt;br /&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Compatibility with Lebesgue measure&lt;/b&gt;&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For complex-valued $f$ and $g$ we have:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;*-linearity&lt;/b&gt;&lt;br /&gt;
		By definition of complex-valued simple integral and by linearity of simple unsigned integral we have
		\begin{align}
		&amp;amp;\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx\nonumber \\ &amp;amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}(f(x)+g(x))\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}(f(x)+g(x))\,dx \\ &amp;amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}\text{Re}g(x)\,dx\nonumber \\ &amp;amp;\hspace{2cm}+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}g(x)\,dx \\ &amp;amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For the complex conjugate $\overline{f}$, we have its integral can be written as
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx&amp;amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx-\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;amp;=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{align}
		Also, for any $c\in\mathbb{C}$, using linearity of simple unsigned integrals once again gives us
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;amp;=\text{Simp}\int_{\mathbb{R}^d}c\,\text{Re}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}c\,\text{Im}f(x)\,dx \\ &amp;amp;=c\,\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+c i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Equivalence&lt;/b&gt;&lt;br /&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Compatibility with Lebesgue measure&lt;/b&gt;&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;msr-funcs&quot;&gt;Measurable functions&lt;/h2&gt;
&lt;p&gt;Just as how the piecewise constant integral can be extended to the Riemann integral, the unsigned simple integral can be extended to the unsigned Lebesgue integral, by expanding the class of unsigned simple functions to the broader class of &lt;strong&gt;unsigned Lebesgue measurable functions&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;unsgn-msr-funcs&quot;&gt;Unsigned measurable functions&lt;/h3&gt;
&lt;p&gt;An unsigned function $f:\mathbb{R}^d\to[0,+\infty]$ is &lt;strong&gt;unsigned Lebesgue measurable&lt;/strong&gt;, or &lt;strong&gt;measurable&lt;/strong&gt;, if it is the pointwise limit of unsigned simple functions, i.e. if there exists a sequence $f_1,f_2,\ldots:\mathbb{R}\to[0,+\infty]$ of unsigned simple functions such that $f_n(x)\to f(x)$ for every $x\in\mathbb{R}^d$.&lt;/p&gt;

&lt;h3 id=&quot;equiv-ntn-msrb&quot;&gt;Equivalent notions of measurability&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;&lt;br /&gt;
Let $f:\mathbb{R}\to[0,+\infty]$ be an unsigned function. The following are then equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		$f$ is unsigned Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		$f$ is the pointwise limit of unsigned simple functions $f_n$ (hence $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for all $x\in\mathbb{R}^d$).
	&lt;/li&gt;
	&lt;li&gt;
		$f$ is the pointwise almost everywhere limit of unsigned simple function $f_n$ (thus $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for almost every $x\in\mathbb{R}^d$).
	&lt;/li&gt;
	&lt;li&gt;
		$f(x)=\sup_n f_n(x)$, where $0\leq f_1\leq f_2\leq\ldots$ is an increasing sequence of unsigned simple functions, each of which are bounded with finite measure support.
	&lt;/li&gt;
	&lt;li&gt;
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&amp;gt;\lambda\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\geq\lambda\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&amp;lt;\lambda\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\leq\lambda\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every interval $I\subset[0,+\infty)$, the set $f^{-1}(I)\doteq\{x\in\mathbb{R}^d:f(x)\in I\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every (relatively) open set $U\subset[0,+\infty)$, the set $f^{-1}(U)\doteq\{x\in\mathbb{R}^d:f(x)\in U\}$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every (relatively) closed set $K\subset[0,+\infty)$, the set $f^{-1}(K)\doteq\{x\in\mathbb{R}^d:f(x)\in K\}$ is Lebesgue measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;eg-msr-func&quot;&gt;Examples of measurable function&lt;/h3&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		Every continuous function $f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	&lt;/li&gt;
	&lt;li&gt;
		Every unsigned simple function is measurable.
	&lt;/li&gt;
	&lt;li&gt;
		The supremum, infimum, limit superior, or limit inferior of unsigned measurable functions is unsigned measurable.
	&lt;/li&gt;
	&lt;li&gt;
		An unsigned function that is equal almost everywhere to an unsigned measurable function, is also measurable.
	&lt;/li&gt;
	&lt;li&gt;
		If a sequence $f_n$ of unsigned measurable functions converges pointwise almost everywhere to an unsigned limit $f$, then $f$ is also measurable.
	&lt;/li&gt;
	&lt;li&gt;
		If $f:\mathbb{R}^d\to[0,+\infty]$ is measurable and $\phi:[0,+\infty]\to[0,+\infty]$ is continuous, then $\phi\circ f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	&lt;/li&gt;
	&lt;li&gt;
		If $f,g$ are unsigned measurable functions, then $f+g$ and $fg$ are measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;cmplx-msrb&quot;&gt;Complex measurability&lt;/h3&gt;
&lt;p&gt;An almost everywhere defined complex-valued function $f:\mathbb{R}^d\to\mathbb{C}$ is &lt;strong&gt;Lebesgue measurable&lt;/strong&gt;, or &lt;strong&gt;measurable&lt;/strong&gt;, if it is the pointwise almost everywhere limit of complex-valued simple functions.&lt;/p&gt;

&lt;h3 id=&quot;equiv-ntn-cmplx-msrb&quot;&gt;Equivalent notions of complex measurability&lt;/h3&gt;
&lt;p&gt;Let $f:\mathbb{R}^d\to\mathbb{C}$ be an almost everywhere defined complex-valued function. The following are then equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		$f$ is measurable.
	&lt;/li&gt;
	&lt;li&gt;
		$f$ is the pointwise almost everywhere limit of complex-valued simple functions.
	&lt;/li&gt;
	&lt;li&gt;
		The (magnitudes of the) positive and negative parts of $\text{Re}(f)$ and $\text{Im}(f)$ are unsigned measurable functions.
	&lt;/li&gt;
	&lt;li&gt;
		$f^{-1}(U)$ is Lebesgue measurable for every open set $U\subset\mathbb{C}$.
	&lt;/li&gt;
	&lt;li&gt;
		$f^{-1}(K)$ is Lebesgue measurable for every closed set $K\subset\mathbb{C}$.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;unsgn-lebesgue-int&quot;&gt;Unsigned Lebesgue integrals&lt;/h2&gt;

&lt;h3 id=&quot;lwr-unsgn-lebesgue-int&quot;&gt;Lower unsigned Lebesgue integral&lt;/h3&gt;
&lt;p&gt;Let $f:\mathbb{R}^d\to[0,+\infty]$ be an unsigned functions (not necessarily measurable). We define the &lt;strong&gt;lower unsigned Lebesgue integral&lt;/strong&gt;, denoted as $\underline{\int_{\mathbb{R}^d}}f(x)\,dx$, to be the quantity
\begin{equation}
\underline{\int_\mathbb{R}^d}f(x)\,dx\doteq\sup_{0\leq g\leq f;g\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx,
\end{equation}
where $g$ ranges over all unsigned simple functions $g:\mathbb{R}^d\to[0,+\infty]$ that are pointwise bounded by $f$.&lt;/p&gt;

&lt;p&gt;We can also define the &lt;strong&gt;upper unsigned Lebesgue integral&lt;/strong&gt; as
\begin{equation}
\overline{\int_\mathbb{R}^d}f(x)\,dx\doteq\inf_{h\geq f;h\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}h(x)\,dx
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;abs-intb&quot;&gt;Absolute integrability&lt;/h2&gt;

&lt;h2 id=&quot;littlewoods-prncpl&quot;&gt;Littlewood’s three principles&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;taos-book&quot;&gt;Terence Tao. &lt;a href=&quot;https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/&quot;&gt;An introduction to measure theory&lt;/a&gt;. Graduate Studies in Mathematics, vol. 126.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;span id=&quot;steins-book&quot;&gt;Elias M. Stein &amp;amp; Rami Shakarchi. &lt;a href=&quot;#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf&quot;&gt;Real Analysis: Measure Theory, Integration, and Hilbert Spaces&lt;/a&gt;.&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It should be simpler to consider the case of $k=2$, in particular with two sets $E_1,E_2\subset\mathbb{R}^d$. These two sets partition $\mathbb{R}^d$ into four disjoint sets: $E_1\cap E_2,E_1\cap E_2^c,E_1^c\cap E_2,E_1^c\cap E_2^c$. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-integral" /><summary type="html">Part III of the measure theory series. Materials are mostly taken from Tao’s book, except for some needed notations extracted from Stein’s book.</summary></entry><entry><title type="html">Generalized Linear Models</title><link href="http://localhost:4000/artificial-intelligent/machine-learning/2022/08/13/glm.html" rel="alternate" type="text/html" title="Generalized Linear Models" /><published>2022-08-13T13:00:00+07:00</published><updated>2022-08-13T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/machine-learning/2022/08/13/glm</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/machine-learning/2022/08/13/glm.html">&lt;blockquote&gt;
  &lt;p&gt;Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminaries&quot;&gt;Preliminaries&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ind-basis&quot;&gt;Independence, basis in vector space&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#lin-ind&quot;&gt;Linear independence&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#basis&quot;&gt;Basis of a vector space&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lagrange-mult&quot;&gt;Lagrange Multipliers&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lin-models-reg&quot;&gt;Linear models for Regression&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lin-basis-func-models&quot;&gt;Linear basis function models&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#least-squares-reg&quot;&gt;Least squares&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#geo-least-squares&quot;&gt;Geometrical interpretation of least squares&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#lms&quot;&gt;The LMS algorithm&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#reg-least-squares&quot;&gt;Regularized least squares&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mult-outputs&quot;&gt;Multiple outputs&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes-lin-reg&quot;&gt;Bayesian Linear Regression&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#param-dist&quot;&gt;Parameter distribution&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#pred-dist-reg&quot;&gt;Predictive distribution&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lin-models-reg&quot;&gt;Linear models for Classification&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#disc-funcs&quot;&gt;Discriminant functions&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#least-squares-clf&quot;&gt;Least squares&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#fisher-ld&quot;&gt;Fisher’s linear discriminant&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#fisher-ld-bin-clf&quot;&gt;Binary classification&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#fisher-ld-clf&quot;&gt;Multi-class classification&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#perceptron&quot;&gt;The perceptron algorithm&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#prob-gen-models&quot;&gt;Probabilistic Generative Models&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#gauss-gen-models&quot;&gt;Gaussian Generative Models&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#max-likelihood-sols&quot;&gt;Maximum likelihood solutions&lt;/a&gt;
                &lt;ul&gt;
                  &lt;li&gt;&lt;a href=&quot;#ggm-bin-clf&quot;&gt;Binary classification&lt;/a&gt;&lt;/li&gt;
                  &lt;li&gt;&lt;a href=&quot;#ggm-clf&quot;&gt;Multi-class classification&lt;/a&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#prob-disc-models&quot;&gt;Probabilistic Discriminative Models&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#log-reg&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#softmax-reg&quot;&gt;Softmax Regression&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#newtons-method&quot;&gt;Newton’s method&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#nm-lin-reg&quot;&gt;Linear Regression&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#nm-log-reg&quot;&gt;Logistic Regression&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#nm-softmax-reg&quot;&gt;Softmax Regression&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bayes-log-reg&quot;&gt;Bayesian Logistic Regression&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#laplace-approx&quot;&gt;The Laplace approximation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#approx-posterior&quot;&gt;Approximation of the posterior&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#pred-dist-clf&quot;&gt;Predictive distribution&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#glm&quot;&gt;Generalized linear models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;ind-basis&quot;&gt;Independence, basis in vector space&lt;/h3&gt;

&lt;h4 id=&quot;lin-ind&quot;&gt;Linear independence&lt;/h4&gt;
&lt;p&gt;The sequence of vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ is said to be &lt;strong&gt;linearly independent&lt;/strong&gt; (or &lt;strong&gt;independent&lt;/strong&gt;) if
\begin{equation}
c_1\mathbf{x}_1+\ldots+c_n\mathbf{x}_n=\mathbf{0}
\end{equation}
only when $c_1,\ldots,c_n$ are all zero.&lt;/p&gt;

&lt;p&gt;Considering those $n$ vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ as $n$ columns of a matrix $\mathbf{A}$
\begin{equation}
\mathbf{A}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{x}_1 &amp;amp; \ldots &amp;amp; \mathbf{x}_n \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]
\end{equation}
we have that the columns of $\mathbf{A}$ are independent when
\begin{equation}
\mathbf{A}\mathbf{x}=\mathbf{0}\hspace{0.5cm}\Leftrightarrow\hspace{0.5cm}\mathbf{x}=\mathbf{0},
\end{equation}
or in other words, the rank of $\mathbf{A}$ is equal to the number of columns of $\mathbf{A}$.&lt;/p&gt;

&lt;h4 id=&quot;basis&quot;&gt;Basis of a vector space&lt;/h4&gt;
&lt;p&gt;We say that vectors $\mathbf{v}_1,\ldots,\mathbf{v}_k$ span a space $S$ when the space consists of all combinations of those vectors. Or in other words, any vector $\mathbf{u}\in S$ can be displayed as linear combination of $\mathbf{v}_i$.&lt;br /&gt;
In this case, $S$ is the smallest space containing those vectors.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;basis&lt;/strong&gt; for a vector space $S$ is a sequence of vectors $\mathbf{v}_1,\ldots,\mathbf{v}_d$ having two properties:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;$\mathbf{v}_1,\ldots,\mathbf{v}_d$ are independent&lt;/li&gt;
	&lt;li&gt;$\mathbf{v}_1,\ldots,\mathbf{v}_d$ span $S$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In $S$, every basis for that space has the same number of vectors, which is the dimension of $S$. Therefore, there are exactly $n$ vectors in every basis for $\mathbb{R}^n$.&lt;/p&gt;

&lt;p&gt;With that definition of a basis $\mathbf{v}_1,\dots,\mathbf{v}_d$ of $S$, for each vector $\mathbf{u}\in S$, there exists only one sequence $c_1,\ldots,c_d$ such that
\begin{equation}
\mathbf{u}=c_1\mathbf{v}_1+\ldots+c_d\mathbf{v}_d
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;lagrange-mult&quot;&gt;Lagrange Multipliers&lt;/h3&gt;
&lt;p&gt;Consider the problem of finding the maximum (or minimum) of $w=f(x_1,x_2,x_3)$ subject to a constraint relating $x_1,x_2$ and $x_3$
\begin{equation}
g(x_1,x_2,x_3)=0
\end{equation}
Apart from solving $x_3$ in terms of $x_1$ and $x_2$ in the constraint and substituting into the original function, which now becomes an unconstrained, here we can also solve this problem as a constrained one.&lt;/p&gt;

&lt;p&gt;The idea is we are using the observation that the gradient vector $\nabla f(\mathbf{x})$ and $\nabla g(\mathbf{x})$ are parallel, because:&lt;/p&gt;

&lt;p&gt;Suppose $f(\mathbf{x})$ has a local maximum at $\mathbf{x}^*$ on the constraint surface $g(\mathbf{x})=0$.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{r}(t)=\langle x_1(t),x_2(t),x_3(t)\rangle$ be a parameterized curve on the constraint surface such that and $\mathbf{r}(t)$ has
\begin{equation}
(x_1(0),x_2(0),x_3(0))^\text{T}=\mathbf{x}
\end{equation}
And also, let $h(t)=f(x_1(t),x_2(t),x_3(t))$, then it implies that $h$ has a maximum at $t=0$, which lets
\begin{equation}
h’(0)=0
\end{equation}
Taking the derivative of $h$ w.r.t, we obtain
\begin{equation}
h’(t)=\nabla f(\mathbf{x})\big\vert_{\mathbf{r}(t)}\mathbf{r}’(t)
\end{equation}
Therefore,
\begin{equation}
\nabla f(\mathbf{x})\big\vert_{\mathbf{x}^*}\mathbf{r}’(0)=0,
\end{equation}
which implies that $\nabla f(\mathbf{x})$ is perpendicular to any curve in the constraint space that goes through $\mathbf{x}^*$. And since $\nabla g(\mathbf{x})$ perpendicular to the constraint surface $g(x)=0$, then $\nabla g(\mathbf{x})$ is also perpendicular to those curves. This implies that $\nabla f(\mathbf{x})$ is parallel to $\nabla g(\mathbf{x})$.&lt;/p&gt;

&lt;p&gt;With this property, we can write $\nabla f(\mathbf{x})$ in terms of $\nabla g(\mathbf{x})$, as
\begin{equation}
\nabla f(\mathbf{x})=\lambda\nabla g(\mathbf{x}),
\end{equation}
where $\lambda\neq 0$ is a constant called &lt;strong&gt;Lagrange multiplier&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;With this definition of Lagrange multiplier, we continue to define the &lt;strong&gt;Lagrangian&lt;/strong&gt; function, given as
\begin{equation}
\mathcal{L}(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda g(\mathbf{x})
\end{equation}
Then letting the partial derivative of Lagrangian w.r.t $\lambda$ give us the constraint
\begin{equation}
0=\frac{\partial \mathcal{L}(\mathbf{x},\lambda)}{\partial\lambda}=g(\mathbf{x})
\end{equation}
With Lagrangian, in order to find the maximum of $f(\mathbf{x})$ that satisfies $g(\mathbf{x})=0$, we set the partial derivatives of the Lagrangian $\mathcal{L}$ w.r.t $x_i$ (which are components of $\mathbf{x}$) and $\lambda$ and solve for $x_i$’s, $\lambda$.&lt;/p&gt;

&lt;h2 id=&quot;lin-models-reg&quot;&gt;Linear models for Regression&lt;/h2&gt;
&lt;p&gt;Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\mathbf{x}$ of input variables.&lt;/p&gt;

&lt;h3 id=&quot;lin-basis-func-models&quot;&gt;Linear basis function models&lt;/h3&gt;
&lt;p&gt;The simplest linear model used for regression tasks is &lt;strong&gt;linear regression&lt;/strong&gt;, which is defined as a linear combination of the input variables
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+\ldots+w_Dx_D,
\end{equation}
where $\mathbf{x}=(x_1,\ldots,x_D)^\text{T}$ is the input variables, while $w_i$’s are the parameters parameterizing the space of linear function mapping from the input space $\mathcal{X}$ of $\mathbf{x}$ to $\mathcal{Y}$.&lt;/p&gt;

&lt;p&gt;With the idea of spanning a space by its basis vectors, we can generalize it to establishing a function space by linear combinations of simpler basis functions. Or in other words, we can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\mathbf{x}$, as
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1\phi_1(\mathbf{x})+\ldots+w_{M-1}\phi_{M-1}(\mathbf{x})=w_0+\sum_{i=1}^{M-1}w_i\phi_i(\mathbf{x}),\label{eq:lbfm.1}
\end{equation}
where $\phi_i(\mathbf{x})$’s are called the &lt;strong&gt;basis functions&lt;/strong&gt;; $w_0$ is called a &lt;strong&gt;bias parameter&lt;/strong&gt;. By letting &lt;span id=&quot;dummy-coeff&quot;&gt;$w_0$&lt;/span&gt; be a coefficient corresponding to a dummy basis function $\phi_0(\mathbf{x})=1$, \eqref{eq:lbfm.1} can be written in a more convenient way
\begin{equation}
y(\mathbf{x},\mathbf{w})=\sum_{i=0}^{M-1}w_i\phi_i(\mathbf{x})=\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\label{eq:lbfm.2}
\end{equation}
where $\mathbf{w}=(w_0,\ldots,w_{M-1})^\text{T}$ and $\boldsymbol{\phi}=(\phi_0,\ldots,\phi_{M-1})^\text{T}$, with $\phi_0(\cdot)=1$.&lt;/p&gt;

&lt;p&gt;There are various choices of basis functions:&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Polynomial basis&lt;/b&gt;. Each basis function $\phi_i$ is a powers of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=x^i
		\end{equation}
		An example of polynomial basis functions is illustrated as below
		&lt;figure&gt;
			&lt;img src=&quot;/assets/images/2022-08-13/polynomial-basis.png&quot; alt=&quot;polynomial basis&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
			&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Example of polynomial basis functions. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
		&lt;/figure&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Gaussian basis function&lt;/b&gt;. Each basis function $\phi_i$ is a Gaussian function of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=\exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right)
		\end{equation}
		An example of Gaussian basis functions is illustrated as below
		&lt;figure&gt;
			&lt;img src=&quot;/assets/images/2022-08-13/gaussian-basis.png&quot; alt=&quot;Gaussian basis&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
			&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Example of Gaussian basis functions. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
		&lt;/figure&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Sigmoidal basis function&lt;/b&gt;. Each basis function $\phi_i$ is defined as
		\begin{equation}
		\phi_i(x)=\sigma\left(\frac{x-\mu_i}{\sigma_i}\right),
		\end{equation}
		where $\sigma(\cdot)$ is the logistic sigmoid function
		\begin{equation}
		\sigma(x)=\frac{1}{1+\exp(-x)}
		\end{equation}
		An example of sigmoidal basis functions is illustrated as below
		&lt;figure&gt;
			&lt;img src=&quot;/assets/images/2022-08-13/sigmoidal-basis.png&quot; alt=&quot;sigmoidal basis&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
			&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Example of sigmoidal basis functions. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
		&lt;/figure&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;least-squares-reg&quot;&gt;Least squares&lt;/h4&gt;
&lt;p&gt;Assume that the target variable $t$ and the inputs $\mathbf{x}$ is related via the equation
\begin{equation}
t=y(\mathbf{x},\mathbf{w})+\epsilon,
\end{equation}
where $\epsilon$ is an error term that captures random noise such that $\epsilon\sim\mathcal{N}(0,\sigma^2)$, which means the density of $\epsilon$ can be written as
\begin{equation}
p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\epsilon^2}{2\sigma^2}\right),
\end{equation}
which implies that
\begin{equation}
p(t|\mathbf{x};\mathbf{w},\beta)=\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t-y(\mathbf{x},\mathbf{w}))^2\beta}{2}\right),\label{eq:lsr.1}
\end{equation}
where $\beta=1/\sigma^2$ is the precision of $\epsilon$, or
\begin{equation}
t|\mathbf{x};\mathbf{w},\beta\sim\mathcal{N}(y(\mathbf{x},\mathbf{w}),\beta^{-1})
\end{equation}
Consider a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$ with corresponding target values $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$ and assume that these data points are drawn independently from the distribution above, we obtain the batch version of \eqref{eq:lsr.1}, called the &lt;strong&gt;likelihood function&lt;/strong&gt;, given as
\begin{align}
L(\mathbf{w},\beta)=p(\mathbf{t}|\mathbf{X};\mathbf{w},\beta)&amp;amp;=\prod_{i=1}^{N}p(t_i|\mathbf{x}_i;\mathbf{w},\beta) \\ &amp;amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\label{eq:lsr.2}
\end{align}
By maximum likelihood, we will be looking for values of $\mathbf{w}$ and $\beta$ that maximize the likelihood. We do this by considering maximizing a simpler likelihood, called &lt;strong&gt;log likelihood&lt;/strong&gt;, denoted as $\ell(\mathbf{w},\beta)$, defined as
\begin{align}
\ell(\mathbf{w},\beta)=\log{L(\mathbf{w},\beta)}&amp;amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right) \\ &amp;amp;=\sum_{i=1}^{N}\log\left[\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\right] \\ &amp;amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\sum_{i=1}^{N}\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2} \\ &amp;amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\label{eq:lsr.3},
\end{align}
where $E_D(\mathbf{w})$ is the sum-of-squares error function, defined as
\begin{equation}
E_D(\mathbf{w})\doteq\frac{1}{2}\sum_{i=1}^{N}\left(t_i-y(\mathbf{x}_i,\mathbf{w})\right)^2\label{eq:lsr.4}
\end{equation}
Consider the gradient of \eqref{eq:lsr.3} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}\ell(\mathbf{w},\beta)&amp;amp;=\nabla_\mathbf{w}\left[\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\right] \\ &amp;amp;\propto\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\big(t_i-y(\mathbf{x}_i,\mathbf{w})\big)^2 \\ &amp;amp;=\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\left(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}\big(\mathbf{x}_i\right)\big)^2 \\ &amp;amp;=\sum_{i=1}^{N}(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i))\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}
\end{align}
By gradient descent, letting this gradient to zero gives us
\begin{equation}
\sum_{i=1}^{N}t_i\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}-\mathbf{w}^\text{T}\sum_{i=1}^{N}\boldsymbol{\phi}(\mathbf{x}_i)\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}=0,
\end{equation}
which implies that
\begin{equation}
\mathbf{w}_\text{ML}=\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},\label{eq:lsr.5}
\end{equation}
which is known as the &lt;strong&gt;normal equations&lt;/strong&gt; for the least squares problem. In \eqref{eq:lsr.5}, $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M}$ is called the &lt;strong&gt;design matrix&lt;/strong&gt;, whose elements are given by $\boldsymbol{\Phi}_{ij}=\phi_j(\mathbf{x}_i)$
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;amp;\ldots&amp;amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;amp;\ldots&amp;amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
and the quantity
\begin{equation}
\boldsymbol{\Phi}^\dagger\doteq\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}
\end{equation}
is called the &lt;strong&gt;Moore-Penrose pseudoinverse&lt;/strong&gt; of the matrix $\boldsymbol{\Phi}$.&lt;/p&gt;

&lt;p&gt;On the other hand, consider the gradient of \eqref{eq:lsr.3} w.r.t $\beta$ and set it equal to zero, we obtain
\begin{equation}
\beta=\frac{N}{\sum_{i=1}^{N}\big(t_i-\mathbf{w}_\text{ML}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;geo-least-squares&quot;&gt;Geometrical interpretation of least squares&lt;/h4&gt;
&lt;p&gt;As mentioned before, we have applied the idea of spanning a vector space by its basis vectors when constructing basis functions.&lt;/p&gt;

&lt;p&gt;In particular, consider an $N$-dimensional space whose axes are given by $t_i$, which implies that
\begin{equation}
\mathbf{t}=(t_1,\ldots,t_N)^\text{T}
\end{equation}
is a vector contained in the space.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-08-13/geo-least-squares.png&quot; alt=&quot;geometry of least squares&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 400px; height: 300px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: Geometrical interpretation of the least-squares solution. The figure is taken from &lt;span&gt;&lt;a href=&quot;#bishops-book&quot;&gt;Bishop’s book&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Each basis function $\phi_j(\mathbf{x}_i)$, evaluated at the $N$ data points, then can also be presented as a vector in the same space, denoted by $\boldsymbol{\varphi}_j$, as illustrated in &lt;strong&gt;Figure 4&lt;/strong&gt; above. Therefore, the design matrix $\boldsymbol{\Phi}$ can be represented as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \boldsymbol{\varphi}_{0}&amp;amp;\ldots&amp;amp;\boldsymbol{\varphi}_{M-1} \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]
\end{equation}
When the number $M$ of basis functions is smaller than the number $N$ of data points, the $M$ vectors $\phi_j(\mathbf{x}_i)$ will span a linear subspace $\mathcal{S}$ of $M$ dimensions.&lt;/p&gt;

&lt;p&gt;We define $\mathbf{y}$ to be an $N$-dimensional vector whose the $i$-th element is given by $y(\mathbf{x}_i,\mathbf{w})$
\begin{equation}
\mathbf{y}=\big(y(\mathbf{x}_1,\mathbf{w}),\ldots,y(\mathbf{x}_N,\mathbf{w})\big)^\text{T}
\end{equation}
Since $\mathbf{y}$ is a linear combination of $\boldsymbol{\varphi}_i$, then $\mathbf{y}\in\mathcal{S}$.
Then the sum-of-squares error \eqref{eq:lsr.4} is exactly (with a factor of $1/2$) the squared Euclidean distance between $\mathbf{y}$ and $\mathbf{t}$. Therefore, the least square solution to $\mathbf{w}$ is the one that makes $\mathbf{y}$ closest to $\mathbf{t}$.&lt;/p&gt;

&lt;p&gt;This solution corresponds to the orthogonal projection of $t$ onto the subspace $S$ spanned by $\boldsymbol{\varphi}_i$, because we have that
\begin{align}
\mathbf{y}^\text{T}(\mathbf{t}-\mathbf{y})&amp;amp;=\left(\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right) \\ &amp;amp;=\left(\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right) \\ &amp;amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t} \\ &amp;amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ &amp;amp;=0,
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;lms&quot;&gt;The LMS algorithm&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;least-means-squares&lt;/strong&gt;, or &lt;strong&gt;LMS&lt;/strong&gt; algorithm for the sum-of-squares error \eqref{eq:lsr.4}, which start with some initial vector $\mathbf{w}_0$ of $\mathbf{w}$, and repeatedly perform the update
\begin{equation}
\mathbf{w}_{t+1}=\mathbf{w}_t+\eta(t_n-\mathbf{w}_t^\text{T}\boldsymbol{\phi}_n)\boldsymbol{\phi}_n,
\end{equation}
where $\boldsymbol{\phi}_n$ denotes $\boldsymbol{\phi}(\mathbf{x}_n)$, and $\eta$ is called the &lt;strong&gt;learning rate&lt;/strong&gt; which controls the update amount.&lt;/p&gt;

&lt;h4 id=&quot;reg-least-squares&quot;&gt;Regularized least squares&lt;/h4&gt;
&lt;p&gt;To control over-fitting, in the error function \eqref{eq:lsr.4}, we add an regularization term, which makes the total error function to be minimized take the form
\begin{equation}
E_D(\mathbf{w})+\lambda E_W(\mathbf{w}),\label{eq:rls.1}
\end{equation}
where $\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\mathbf{w})$ and the regularization term $E_W(\mathbf{w})$. One simple possible form of regularizer is given as
\begin{equation}
E_W(\mathbf{w})=\frac{1}{2}\mathbf{w}^\text{T}\mathbf{w}
\end{equation}
The total error function \eqref{eq:rls.1} then can be written as
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\mathbf{w}^\text{T}\mathbf{w}\label{eq:rls.2}
\end{equation}
Setting the gradient of this error to zero and solving for $\mathbf{w}$, we have the solution
\begin{equation}
\mathbf{w}= (\lambda\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}\label{eq:rls.3}
\end{equation}
This particular choice of regularizer is called &lt;strong&gt;weight decay&lt;/strong&gt; because it encourages weight values to decay towards zero in sequential learning.&lt;/p&gt;

&lt;p&gt;Another choice of regularizer which is more general lets the regularized error have the form
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\sum_{j=1}^{M}\vert w_j\vert^q,
\end{equation}
where $q=2$ corresponds to the regularizer \eqref{eq:rls.2}.&lt;/p&gt;

&lt;h4 id=&quot;mult-outputs&quot;&gt;Multiple outputs&lt;/h4&gt;
&lt;p&gt;When the target of our model is instead in multiple-dimensional form, denoted as $\mathbf{t}$, we can generalize our model to be
\begin{equation}
\mathbf{y}(\mathbf{x},\mathbf{w})=\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),
\end{equation}
where $\mathbf{y}\in\mathbb{R}^K, \mathbf{W}\in\mathbb{R}^{M\times K}$ is the matrix of parameters, $\boldsymbol{\phi}\in\mathbb{R}^M$ with $\phi_i(\mathbf{x})$ as the $i$-th element, and with $\phi_0(\mathbf{x})=1$.&lt;/p&gt;

&lt;p&gt;With this generalization, \eqref{eq:lsr.1} can be also be rewritten as
\begin{equation}
p(\mathbf{t}|\mathbf{x};\mathbf{W},\beta)=\sqrt{\frac{\beta}{2\pi\vert\mathbf{I}\vert}}\exp\left[-\frac{1}{2}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)^\text{T}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)\beta\mathbf{I}^{-1}\right],
\end{equation}
or in other words
\begin{equation}
\mathbf{t}|\mathbf{x};\mathbf{W},\beta\sim\mathcal{N}(\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1}\mathbf{I})
\end{equation}
With a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$, our target values can also be vectorized into $\mathbf{T}\in\mathbb{R}^{N\times K}$ given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.1cm}\mathbf{t}_1^\text{T}\hspace{0.1cm}- \\ \vdots \\ -\hspace{0.1cm}\mathbf{t}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right],
\end{equation}
and likewise with the input matrix $\mathbf{X}$ vectorized from input vectors $\mathbf{x}_1,\ldots,\mathbf{x}_N$. With these definitions, the multi-dimensional likelihood can be defined as
\begin{align}
L(\mathbf{W},\beta)&amp;amp;=p(\mathbf{T}|\mathbf{X};\mathbf{W},\beta) \\ &amp;amp;=\prod_{i=1}^{N}p(\mathbf{t}_i|\mathbf{x}_i;\mathbf{W},\beta) \\ &amp;amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right]
\end{align}
And thus the log likelihood now becomes
\begin{align}
\ell(\mathbf{W},\beta)&amp;amp;=\log L(\mathbf{W},\beta) \\ &amp;amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;amp;=\sum_{i=1}^{N}\log\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;amp;=\frac{N}{2}\log\frac{\beta}{2\pi}-\frac{\beta}{2}\sum_{i=1}^{N}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)
\end{align}
Taking the gradient of the log likelihood w.r.t $\mathbf{W}$, setting it to zero and solving for $\mathbf{W}$ gives us
\begin{equation}
\mathbf{W}_\text{ML}=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{T}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;bayes-lin-reg&quot;&gt;Bayesian linear regression&lt;/h3&gt;

&lt;h4 id=&quot;param-dist&quot;&gt;Parameter distribution&lt;/h4&gt;
&lt;p&gt;Consider the noise precision parameter $\beta$ as a constant. From the equation \eqref{eq:lsr.2}, we see that the likelihood function $L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})$ takes the form of an exponential of a quadratic form in $\mathbf{w}$. Thus, if we choose the prior $p(\mathbf{w})$ as a Gaussian, the corresponding posterior will also become a Gaussian due to being computed as a product of two exponentials of quadratic forms of $\mathbf{w}$. This makes the prior be a conjugate distribution for the likelihood function, and hence be given by
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ is the mean vector and $\mathbf{S}_0$ is the covariance matrix.&lt;/p&gt;

&lt;p&gt;By the &lt;a href=&quot;/mathematics/probability-statistics/2021/11/22/normal-dist.html#marg-cond-gaussian&quot;&gt;result&lt;/a&gt;, we have that the corresponding posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, which is a conditional Gaussian distribution, is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N),\label{eq:pd.1}
\end{equation}
where the mean $\mathbf{m}_N$ and the precision matrix $\mathbf{S}_N^{-1}$ are defined as
\begin{align}
\mathbf{m}_N&amp;amp;=\mathbf{S}_N(\mathbf{S}_0^{-1}\mathbf{m}_0+\beta\boldsymbol{\Phi}^\text{T}\mathbf{t}), \\ \mathbf{S}_N^{-1}&amp;amp;=\mathbf{S}_0^{-1}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Therefore, by MAP, we have
\begin{align}
\mathbf{w}_\text{MAP}&amp;amp;=\underset{\mathbf{w}}{\text{argmax}}\,\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\Big] \\ &amp;amp;=\underset{\mathbf{w}}{\text{argmin}}\,(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)
\end{align}
By this &lt;a href=&quot;/mathematics/probability-statistics/2021/11/22/normal-dist.html#precision-eigenvalue&quot;&gt;property&lt;/a&gt; of the covariance matrix, we have that the precision matrix $\mathbf{S}_N^{-1}$ and the covariance matrix $\mathbf{S}_N$ have the same set of eigenvalues, which are non-negative due to the fact that $\mathbf{S}_N$ is positive semi-definite. This also means that $\mathbf{S}_N$ is positive semi-definite, and thus
\begin{equation}
(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\geq0
\end{equation}
Therefore, the maximum posterior weight vector is also the mean vector
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N\label{eq:pd.2}
\end{equation}
Consider an infinite broad prior $\mathbf{S}_0=\alpha^{-1}\mathbf{I}$ with $\alpha\to 0$, in this case the mean $\mathbf{m}_N$ reduces to the maximum likelihood value $\mathbf{w}_\text{ML}$ given by \eqref{eq:lsr.5}. And if $N=0$, then the posterior distribution reverts to the prior.&lt;/p&gt;

&lt;p&gt;Furthermore, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$, the posterior given in \eqref{eq:pd.1} can be regarded as the prior distribution for that data point. If the model is given as \eqref{eq:lsr.1}, the likelihood function of the newly added data point is then given in form
\begin{equation}
p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})=\left(\frac{\beta}{2\pi}\right)^{1/2}\exp\left(-\frac{(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})\beta}{2}\right),
\end{equation}
where $\boldsymbol{\phi}_{N+1}=\boldsymbol{\phi}(\mathbf{x}_{N+1})$.
Therefore, the posterior distribution of the data point $(\mathbf{x}_{N+1},t_{N+1})$ can be computed as
\begin{align}
&amp;amp;\hspace{0.7cm}p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t}) \\ &amp;amp;\propto p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})p(\mathbf{w}\vert\mathbf{t}) \\ &amp;amp;=\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)-\frac{1}{2}(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})^2\beta\Big] \\ &amp;amp;=\exp\Big[-\frac{1}{2}\big(\mathbf{w}^\text{T}\mathbf{S}_N^{-1}\mathbf{w}+\beta\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{w}\big)+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big] \\ &amp;amp;=\exp\Big[-\frac{1}{2}\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)\mathbf{w}+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big],
\end{align}
where $c$ is a constant w.r.t $\mathbf{w}$, i.e., $c$ is independent of $\mathbf{w}$, which claims that the posterior distribution is also a Gaussian, given by
\begin{equation}
p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1})\label{eq:pd.3}
\end{equation}
where the precision matrix $\mathbf{S}_{N+1}$ is defined as
\begin{equation}
\mathbf{S}_{N+1}^{-1}\doteq\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T},
\end{equation}
and the mean $\mathbf{m}_{N+1}$ is given by
\begin{equation}
\mathbf{m}_{N+1}\doteq\mathbf{S}_{N+1}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)
\end{equation}
Consider the prior as a Gaussian, defined by
\begin{equation}
p(\mathbf{w}\vert\alpha)=\mathcal{N}(\mathbf{w}\vert\mathbf{0},\alpha^{-1}\mathbf{I}),
\end{equation}
Therefore, the corresponding posterior over $\mathbf{w}$, $p(\mathbf{w}\vert\mathbf{t})$, will be given as \eqref{eq:pd.1} with
\begin{align}
\mathbf{m}_N&amp;amp;=\beta\mathbf{S}_N\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ \mathbf{S}_N^{-1}&amp;amp;=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Taking the natural logarithm of the posterior distribution gives us the sum of the log likelihood and the log of the prior, as a function of $\mathbf{w}$, given by
\begin{equation}
\log p(\mathbf{w}\vert\mathbf{t})=-\frac{\beta}{2}\sum_{n=1}^{N}\big(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_n)\big)^2-\frac{\alpha}{2}\mathbf{w}^\text{T}\mathbf{w}+c
\end{equation}
Therefore, maximizing this posterior is equivalent to minimizing the sum of the sum-of-squares error function with addition of a quadratic regularization term, which is exactly the equation \eqref{eq:rls.2} with $\lambda=\alpha/\beta$.&lt;/p&gt;

&lt;p&gt;In addition, by $\eqref{eq:pd.2}$, we have that
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N=\beta\left(\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}=\left(\frac{\alpha}{\beta}\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{equation}
which for setting $\lambda=\alpha/\beta$ gives us exactly the solution \eqref{eq:rls.3} for the regularized least squares \eqref{eq:rls.2}.&lt;/p&gt;

&lt;h4 id=&quot;pred-dist-reg&quot;&gt;Predictive distribution&lt;/h4&gt;
&lt;p&gt;The &lt;strong&gt;predictive distribution&lt;/strong&gt; that gives us the information to make predictions $t$ for new values $\mathbf{x}$ is defined as
\begin{equation}
p(t\vert\mathbf{x},\mathbf{t},\alpha,\beta)=\int p(t\vert\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)\,d\mathbf{w}\label{eq:pdr.1}
\end{equation}
in which $\mathbf{t}$ is the vector of target values from the training set.&lt;/p&gt;

&lt;p&gt;The conditional distribution $p(t\vert\mathbf{x},\mathbf{w},\beta)$ of the target variable is given by \eqref{eq:lsr.1}, and the posterior weight distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$ is given by \eqref{eq:pd.1}. Thus, as a &lt;a href=&quot;/mathematics/probability-statistics/2021/11/22/normal-dist.html#marg-cond-gaussian&quot;&gt;marginal Gaussian distribution&lt;/a&gt;, the distribution \eqref{eq:pdr.1} can be rewritten as
\begin{align}
p(t\vert\mathbf{x},\mathbf{t},\mathbf{w},\beta)&amp;amp;=\int\mathcal{N}(t\vert\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1})\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N)\,d\mathbf{w} \\ &amp;amp;=\mathcal{N}(t\vert\mathbf{m}_N^\text{T}\boldsymbol{\phi}(\mathbf{x}),\sigma_N^2(\mathbf{x})),
\end{align}
where the variance $\sigma_N^2(\mathbf{x})$ of the predictive distribution is defined as
\begin{equation}
\sigma_N^2(\mathbf{x})\doteq\beta^{-1}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.2}
\end{equation}
The first term in \eqref{eq:pdr.2} represents the noise on the data, while the second term reflects the uncertainty associated with the parameters $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;It is worth noting that as additional data points are observed, the posterior distribution becomes narrower. In particular, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$. Therefore, as given by the result \eqref{eq:pd.3}, its posterior distribution is
\begin{equation}
p(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1}),
\end{equation}
where
\begin{align}
\mathbf{m}_{N+1}&amp;amp;=\mathbf{S}_{N+1}(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}), \\ \mathbf{S}_{N+1}^{-1}&amp;amp;=\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}
\end{align}
Therefore, the variance of the corresponding predictive distribution for the newly added data point is then given as
\begin{equation}
\sigma_{N+1}^2(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_{N+1}\boldsymbol{\phi}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\big(\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)^{-1}\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.3}
\end{equation}
Using the matrix identity
\begin{equation}
(\mathbf{M}+\mathbf{v}\mathbf{v}^\text{T})^{-1}=\mathbf{M}^{-1}-\frac{(\mathbf{M}^{-1}\mathbf{v})(\mathbf{v}^\text{T}\mathbf{M}^{-1})}{1+\mathbf{v}^\text{T}\mathbf{M}^{-1}\mathbf{v}},
\end{equation}
in the equation \eqref{eq:pdr.3} gives us
\begin{align}
\sigma_{N+1}^2(\mathbf{x})&amp;amp;=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\left(\mathbf{S}_N-\frac{\beta\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\right)\boldsymbol{\phi}(\mathbf{x}) \\ &amp;amp;=\sigma_N^2(\mathbf{x})-\beta\frac{\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\leq\sigma_N^2(\mathbf{x}),
\end{align}
since
\begin{equation}
\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})=\left\Vert\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\right\Vert_2^2\geq0,
\end{equation}
and since
\begin{equation}
1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}&amp;gt;0
\end{equation}
due to $\mathbf{S}_N$ is the covariance matrix of the posterior distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$, which implies that it is positive semi-definite.&lt;/p&gt;

&lt;p&gt;In other words, as $N\to\infty$, the second term in \eqref{eq:pdr.2} goes to zero, and the variance of the predictive distribution solely depends on $\beta$.&lt;/p&gt;

&lt;h2 id=&quot;linear-models-for-classification&quot;&gt;Linear models for Classification&lt;/h2&gt;
&lt;p&gt;In Machine Learning literature, &lt;strong&gt;classification&lt;/strong&gt; refers the to task of taking an input vector $\mathbf{x}$ and assigning it to one of $K$ classes $\mathcal{C}_k$ for $k=1,\ldots,K$. Usually, each input will be assigned only to a single class. In this case, the input space is divided by the &lt;strong&gt;decision boundaries&lt;/strong&gt; (or &lt;strong&gt;decision surfaces&lt;/strong&gt;) into &lt;strong&gt;decision regions&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Taking an input space of $D$ dimensions, linear models are defined to be linear functions of the input vector $x$, and thus are a $(D-1)$-dimensional hyperplane.&lt;/p&gt;

&lt;h3 id=&quot;disc-funcs&quot;&gt;Discriminant functions&lt;/h3&gt;
&lt;p&gt;A discriminant is a function that takes an input vector $x$ and assigns it to one of $K$ class, denoted as $\mathcal{C}_k$&lt;/p&gt;

&lt;p&gt;The simplest discriminant function is a linear function of the input vector
\begin{equation}
y(\mathbf{x})=\mathbf{w}^\text{T}\mathbf{x}+w_0,
\end{equation}
where $\mathbf{w}$ is called the &lt;strong&gt;weight vector&lt;/strong&gt;, and $w_0$ is the &lt;strong&gt;bias&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the case of binary classification, an input $\mathbf{x}$ is assigned to class $\mathcal{C}_1$ if $y(\mathbf{x})\geq 0$ and otherwise $y(\mathbf{x})\lt 0$, it belongs to class $\mathcal{C}_2$, thus the decision boundary is defined by
\begin{equation}
y(\mathbf{x})=0,
\end{equation}
which corresponds to a $(D-1)$-dimensional hyperplane with an $D$-dimensional input space.&lt;/p&gt;

&lt;p&gt;Consider $\mathbf{x}_A$ and $\mathbf{x}_B$ lying on the hyperplane, thus $y(\mathbf{x}_A)=y(\mathbf{x}_B)=0$, which gives us that
\begin{equation}
0=y(\mathbf{x}_A)-y(\mathbf{x}_B)=\mathbf{w}^\text{T}\mathbf{x}_A-\mathbf{w}^\text{T}\mathbf{x}_B=\mathbf{w}^\text{T}(\mathbf{x}_A-\mathbf{x}_B)
\end{equation}
This claims that $\mathbf{w}$ is perpendicular to any vector within the decision boundary, and thus $\mathbf{w}$ is a normal vector of the decision boundary itself.&lt;/p&gt;

&lt;p&gt;Hence, projecting a point $\mathbf{x}_0$ into the hyperplane, we have that the distant of $\mathbf{x}_0$ to the hyperplane is given by
\begin{equation}
\text{dist}(\mathbf{x}_0,y(\mathbf{x}))=\frac{y(\mathbf{x}_0)}{\Vert\mathbf{w}\Vert},
\end{equation}
which implies that
\begin{equation}
\text{dist}(\mathbf{0},y(\mathbf{x}))=\frac{w_0}{\Vert\mathbf{w}\Vert}
\end{equation}
To generalize the binary classification problem into multiple-class ones, we consider a $K$-class discriminant comprising $K$ linear functions of the form
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}
\end{equation}
Then for each input $\mathbf{x}$, it will be assigned to class $\mathcal{C}_k$ if $y_k(\mathbf{x})&amp;gt;y_i(\mathbf{x}),\forall i\neq k$, or in other words $\mathbf{x}$ is assigned to a class $C_k$ that
\begin{equation}
k=\underset{i=1,\ldots,K}{\text{argmax}}\,y_i(\mathbf{x})
\end{equation}
The boundary between two class $\mathcal{C}_i$ and $\mathcal{C}_j$ is therefore given by
\begin{equation}
y_i(\mathbf{x})=y_j(\mathbf{x}),
\end{equation}
or
\begin{equation}
(\mathbf{w}_i-\mathbf{w}_j)^\text{T}\mathbf{x}+w_{i,0}-w_{j,0}=0,
\end{equation}
which is an $(D-1)$-dimensional hyperplane.&lt;/p&gt;

&lt;h4 id=&quot;least-squares-clf&quot;&gt;Least squares&lt;/h4&gt;
&lt;p&gt;Recall that in the regression task, we used least squares to find the models in form of linear functions of the parameters. We can also apply least squares approach to classification problems.&lt;/p&gt;

&lt;p&gt;To begin, we have that for $k=1,\ldots,K$, each class $\mathcal{C}_k$ is represented the model
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}\label{eq:lsc.1}
\end{equation}
By giving the bias parameter $w_{k,0}$ a dummy input variable $x_0=0$, we can rewrite \eqref{eq:lsc.1} in a more convenient form
\begin{equation}
y_k(\mathbf{x})=\widetilde{\mathbf{w}}_k^\text{T}\widetilde{\mathbf{x}},
\end{equation}
where
\begin{equation}
\widetilde{\mathbf{w}}_k=\left(w_{k,0},\mathbf{w}_k^\text{T}\right)^\text{T};\hspace{1cm}\widetilde{\mathbf{x}}=\left(1,\mathbf{x}^\text{T}\right)^\text{T}
\end{equation}
Thus, we can vectorize the $K$ linear models into
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}},\label{eq:lsc.2}
\end{equation}
where $\widetilde{\mathbf{W}}$ is the parameter matrix whose $k$-th column is the $(D+1)$-dimensional vector $\widetilde{\mathbf{w}}_k$
\begin{equation}
\widetilde{\mathbf{W}}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \widetilde{\mathbf{w}}_1&amp;amp;\ldots&amp;amp;\widetilde{\mathbf{w}}_K \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]
\end{equation}
Consider a training set $\{\mathbf{x}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$, analogy to the parameter matrix $\widetilde{\mathbf{W}}$, we can vectorize those input variables and target values into
\begin{equation}
\widetilde{\mathbf{X}}=\left[\begin{matrix}-\hspace{0.15cm}\widetilde{\mathbf{x}}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\widetilde{\mathbf{x}}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
and
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
With these definition, the sum-of-squares error function then can be written as
\begin{equation}
E_D(\widetilde{\mathbf{W}})=\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big]
\end{equation}
Taking the derivative of $E_D(\widetilde{\mathbf{W}})$ w.r.t $\widetilde{\mathbf{W}}$, we obtain
\begin{align}
\nabla_\widetilde{\mathbf{W}}E_D(\widetilde{\mathbf{W}})&amp;amp;=\nabla_\widetilde{\mathbf{W}}\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big] \\ &amp;amp;=\frac{1}{2}\nabla_\widetilde{\mathbf{W}}\text{Tr}\Big[\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\mathbf{T}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}+\mathbf{T}^\text{T}\mathbf{T}\Big] \\ &amp;amp;=\frac{1}{2}\Big[2\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}\Big] \\ &amp;amp;=\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}
\end{align}
Setting this derivative equal to zero, we obtain the least squares solution for $\widetilde{\mathbf{W}}$ as
\begin{equation}
\widetilde{\mathbf{W}}=(\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}})^{-1}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}=\widetilde{\mathbf{X}}^\dagger\mathbf{T}
\end{equation}
Therefore, the discriminant function \eqref{eq:lsc.2} can be rewritten as
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}}=\mathbf{T}^\text{T}\big(\widetilde{\mathbf{X}}^\dagger\big)^\text{T}\widetilde{\mathbf{x}}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;fisher-lin-disc&quot;&gt;Fisher’s linear discriminant&lt;/h4&gt;
&lt;p&gt;One way to view a linear classification model is in terms of dimensional reduction. In particular, given an $D$-dimensional input $\mathbf{x}$, we project it down to one dimension using
\begin{equation}
y=\mathbf{w}^\text{T}\mathbf{x}
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;fisher-ld-bin-clf&quot;&gt;Binary classification&lt;/h5&gt;
&lt;p&gt;Consider a binary classification in which there are $N_1$ points of class $\mathcal{C}_1$ and $N_2$ points of class $\mathcal{C}_2$, thus the mean vectors of those two classes are given by
\begin{align}
\mathbf{m}_1&amp;amp;=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}\mathbf{x}_n, \\ \mathbf{m}_2&amp;amp;=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}\mathbf{x}_n
\end{align}
The simplest measure of the separation of the classes, when projected onto $\mathbf{w}$, is the separation of the projected class means, which suggests us choosing $\mathbf{w}$ in order to maximize
\begin{equation}
m_2-m_1=\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1),
\end{equation}
where for $k=1,\ldots,K$
\begin{equation}
m_k=\mathbf{w}^\text{T}\mathbf{m}_k
\end{equation}
is the mean of the projected data from class $\mathcal{C}_k$.&lt;/p&gt;

&lt;p&gt;To make the computation simpler, we normalize the projection simply by making a constraint of $\mathbf{w}$ being a unit vector, which means
\begin{equation}
\big\Vert\mathbf{w}\big\Vert_2=\sum_{i}w_i=1
\end{equation}
Therefore, by Lagrange multiplier, in order to maximize $m_2-m_1$, we have that
\begin{equation}
\mathbf{w}\propto(\mathbf{m}_2-\mathbf{m}_1)
\end{equation}
To solve this problem, we use the Fisher’s LD approach to minimize the class overlap by maximizing the ratio of the &lt;strong&gt;between-class variance&lt;/strong&gt; to the &lt;strong&gt;within-class variance&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The within-class variance of projected data from class $\mathbf{w}_k$ is defined as
\begin{equation}
s_k^2\doteq\sum_{n\in\mathcal{C}_k}(y_n-m_k)^2,
\end{equation}
where $y_n=\mathbf{w}^\text{T}\mathbf{x}_n$ is the projected of $\mathbf{x}_n$. Thus the total within-class variance for the whole data set is $s_1^2+s_2^2$.&lt;/p&gt;

&lt;p&gt;The between-class variance is simply defined to be the squared of the difference of means, given as
\begin{equation}
(m_2-m_1)^2
\end{equation}
Hence, the ratio of the between-class variance to the within-class variance, called the &lt;strong&gt;Fisher criterion&lt;/strong&gt;, can be defined as
\begin{align}
J(\mathbf{w})&amp;amp;=\frac{(m_2-m_1)^2}{s_1^2+s_2^2} \\ &amp;amp;=\frac{\big\Vert\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1)\big\Vert_2^2}{\sum_{n\in\mathcal{C}_1}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_1)\big\Vert_2^2+\sum_{n\in\mathcal{C}_2}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_2)\big\Vert_2^2} \\ &amp;amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}},\label{eq:flbc.1}
\end{align}
where
\begin{equation}
\mathbf{S}_\text{B}\doteq(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T},
\end{equation}
is called the &lt;strong&gt;between-class covariance matrix&lt;/strong&gt; and
\begin{equation}
\mathbf{S}_\text{W}\doteq\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\mathbf{m}_1)(\mathbf{x}_n-\mathbf{m}_1)^\text{T}+\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\mathbf{m}_2)(\mathbf{x}_n-\mathbf{m}_2)^\text{T},
\end{equation}
is called the &lt;strong&gt;total within-class covariance matrix&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As usual, taking the gradient of \eqref{eq:flbc.1} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}J(\mathbf{w})&amp;amp;=\nabla_\mathbf{w}\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}} \\ &amp;amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}(\mathbf{S}_\text{B}+\mathbf{S}_\text{B}^\text{T})\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}(\mathbf{S}_\text{W}+\mathbf{S}_\text{W}^\text{T})\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;amp;\propto\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{align}
Setting the gradient equal to zero and solving for $\mathbf{w}$, we obtain that $\mathbf{w}$ satisfies
\begin{equation}
\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}=\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{equation}
Since $\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}$ and $\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}$ are two scalars, we then have
\begin{equation}
\mathbf{S}_\text{W}\mathbf{w}\propto\mathbf{S}_\text{B}\mathbf{w}
\end{equation}
Multiply both side by $\mathbf{S}_\text{W}^{-1}$, we obtain
\begin{align}
\mathbf{w}&amp;amp;\propto\mathbf{S}_\text{W}^{-1}\mathbf{S}_\text{B}\mathbf{w} \\ &amp;amp;=\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w} \\ &amp;amp;\propto\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1),\label{eq:flbc.2}
\end{align}
since $(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w}$ is a scalar.&lt;/p&gt;

&lt;p&gt;If the within-class covariance matrix $\mathbf{S}_\text{W}$ is isotropic&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, we then have
\begin{equation}
\mathbf{w}\propto\mathbf{m}_2-\mathbf{m}_1
\end{equation}
The result \eqref{eq:flbc.2} is called &lt;strong&gt;Fisher’s linear discriminant&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;With this $\mathbf{w}$, we can project our data down into one dimension and from projected data, we construct a discriminant by selecting a threshold $y_0$ such that $\mathbf{x}$ belongs to class $\mathcal{C}_1$ if $y(\mathbf{x})\gg y_0$ and otherwise it belongs to $\mathcal{C}_1$.&lt;/p&gt;

&lt;h5 id=&quot;fisher-ld-clf&quot;&gt;Multi-class classification&lt;/h5&gt;
&lt;p&gt;To generalize the Fisher discriminant to the case of $K&amp;gt;2$, we first assume that $D&amp;gt;K$ and consider the $D’&amp;gt;1$ linear features
\begin{equation}
y=\mathbf{w}_k^\text{T}\mathbf{x},
\end{equation}
where $k=1,\ldots,D’$. Thus, as usual we can vectorize these feature values as
\begin{equation}
\mathbf{y}=\mathbf{W}^\text{T}\mathbf{x},\label{eq:flc.1}
\end{equation}
where
\begin{equation}
\mathbf{y}=(y_1,\ldots,y_k)^\text{T},\hspace{2cm}\mathbf{W}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{w}_1&amp;amp;\ldots&amp;amp;\mathbf{w}_{D’} \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]
\end{equation}
The mean vector for each class is unchanged, which is given as
\begin{equation}
\mathbf{m}_k=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n,
\end{equation}
where $N_k$ is the number of points in class $\mathcal{C}_k$  for $k=1,\ldots,K$.&lt;/p&gt;

&lt;p&gt;The within-class variance covariance matrix $\mathbf{S}_\text{W}$ now can be simply generalized as
\begin{equation}
\mathbf{S}_\text{W}=\sum_{k=1}^{K}\mathbf{S}_k,\label{eq:flc.2}
\end{equation}
where
\begin{equation}
\mathbf{S}_k=\sum_{n\in\mathcal{C}_k}(\mathbf{x}_n-\mathbf{m}_k)(\mathbf{x}_n-\mathbf{m}_k)^\text{T}
\end{equation}
To find the generalization of the between-class covariance matrix $\mathbf{S}_\text{B}$, we first consider the total covariance matrix
\begin{equation}
\mathbf{S}_T=\sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{m})(\mathbf{x}_n-\mathbf{m})^\text{T},
\end{equation}
where
\begin{equation}
\mathbf{m}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n=\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k
\end{equation}
is the mean of the whole data set, where $N=\sum_{k=1}^{K}N_k$ is the number of the data points. The total covariance matrix can be decomposed into the sum of the within-class covariance matrix $\mathbf{S}_\text{W}$, as given in \eqref{eq:flc.2} with a matrix $\mathbf{S}_\text{B}$, defined as a measure of the between-class covariance
\begin{equation}
\mathbf{S}_\text{T}=\mathbf{S}_\text{W}+\mathbf{S}_\text{B},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{B}=\sum_{k=1}^{K}N_k(\mathbf{m}_k-\mathbf{m})(\mathbf{m}_k-\mathbf{m})^\text{T}
\end{equation}
Using \eqref{eq:flc.1}, we project the whole data set into the $D’$-dimensional space of $\mathbf{y}$, the corresponding within-class covariance matrix of the transformed data are given as
\begin{align}
\mathbf{s}_\text{W}&amp;amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)^\text{T} \\ &amp;amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}(\mathbf{y}_n-\boldsymbol{\mu}_k)(\mathbf{y}_n-\boldsymbol{\mu}_k)^\text{T} \\ &amp;amp;=\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T}
\end{align}
and also the transformed between-class covariance matrix
\begin{align}
\mathbf{s}_\text{B}&amp;amp;=\sum_{k=1}^{K}N_k(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})^\text{T} \\ &amp;amp;=\sum_{k=1}^{K}(\boldsymbol{\mu}_k-\boldsymbol{\mu})(\boldsymbol{\mu}_k-\boldsymbol{\mu})^\text{T} \\ &amp;amp;=\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T},
\end{align}
where
\begin{align}
\boldsymbol{\mu}_k&amp;amp;=\mathbf{W}^\text{T}\mathbf{m}_k=\mathbf{W}^\text{T}\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{y}_n, \\ \boldsymbol{\mu}&amp;amp;=\mathbf{W}^\text{T}\mathbf{m}=\mathbf{W}^\text{T}\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{\mu}_k
\end{align}
Analogy to the case of binary classification with Fisher’s criterion \eqref{eq:flbc.1}, we need a new measure that is large when the between-class covariance is large and when the within-class covariance is small. A simple choice of criterion is given as
\begin{equation}
J(\mathbf{W})=\text{Tr}\left(\mathbf{s}_\text{W}^{-1}\mathbf{s}_\text{B}\right)
\end{equation}
or
\begin{equation}
J(\mathbf{w})=\text{Tr}\big[(\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T})^{-1}(\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T})\big]
\end{equation}
for which the linear basis model follow the same rule as the above&lt;/p&gt;

&lt;h4 id=&quot;perceptron&quot;&gt;The perceptron algorithm&lt;/h4&gt;
&lt;p&gt;Another example of linear discriminant model is the perceptron algorithm&lt;/p&gt;

&lt;h3 id=&quot;prob-gen-models&quot;&gt;Probabilistic Generative Models&lt;/h3&gt;
&lt;p&gt;When solving the classification problems, we divide the strategy into two stage&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Inference stage&lt;/b&gt;. In this stage we use training data to learn a model for $p(\mathcal{C}_k|\mathbf{x})$ 
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Decision stage&lt;/b&gt;. In this stage we use those posterior probabilities to make optimal class assignments.
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can solve both inference and decision problems at the same time by learning a function, which is the discriminant function, maps inputs $\mathbf{x}$ directly into decisions.&lt;/p&gt;

&lt;p&gt;When using the generative approach to solve the problem of classification, we first model the class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ and the class priors $p(\mathcal{C}_k)$ then apply Bayes’ theorem to compute the posterior probabilities $p(\mathcal{C}_k\vert\mathbf{x})$.&lt;/p&gt;

&lt;p&gt;Consider the binary case, in which specifically the posterior probability for class $\mathcal{C}_1$ can be computed as
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)+p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)} \\ &amp;amp;=\frac{1}{1+\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}} \\ &amp;amp;=\frac{1}{1+\exp(-a)}=\sigma(a)\label{eq:pgm.1}
\end{align}
where
\begin{equation}
a=\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}
\end{equation}
and where $\sigma(\cdot)$ is the &lt;span id=&quot;logistic-sigmoid-func&quot;&gt;logistic sigmoid function&lt;/span&gt;, defined before as
\begin{equation}
\sigma(a)\doteq\frac{1}{1+\exp(-a)}
\end{equation}
For the case of multi-class, $K&amp;gt;2$, the posterior probability for class $\mathcal{C}_k$ can be generalized as
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{i=1}^{K}p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)}=\dfrac{\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big)\Big]}{\sum_{i=1}^{K}\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)\big)\Big]} \\ &amp;amp;=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)}=\sigma(\mathbf{a})_k\label{eq:pgm.2}
\end{align}
where
\begin{align}
a_k&amp;amp;=\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big), \\ \mathbf{a}&amp;amp;=(a_1,\ldots,a_K)^\text{T},
\end{align}
and the function $\sigma:\mathbb{R}^K\to(0,1)^K$, known as the &lt;strong&gt;normalized exponential&lt;/strong&gt; or &lt;strong&gt;softmax function&lt;/strong&gt; - a generalization of sigmoid into multi-dimensional, in which the $k$-th element is defined as
\begin{equation}
\sigma(\mathbf{a})_k\doteq\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
for $k=1,\ldots,K$ and $\mathbf{a}=(a_1,\ldots,a_K)^\text{T}$.&lt;/p&gt;

&lt;h4 id=&quot;gauss-gen-models&quot;&gt;Gaussian Generative models&lt;/h4&gt;
&lt;p&gt;If the class-conditional probabilities are Gaussian, or specifically Multivariate Normal and share the same covariance matrix $\boldsymbol{\Sigma}$, then for $k=1,\ldots,K$,
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Thus, the density for class $\mathcal{C}_k$ is defined as
\begin{equation}
p(\mathbf{x}\vert\mathcal{C}_k)=\frac{1}{(2\pi)^{D/2}\big\vert\boldsymbol{\Sigma}\big\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)\right]
\end{equation}
In the binary case, in which the densities above become Bivariate Normal, by \eqref{eq:pgm.1} we have that
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;amp;=\sigma\left(\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}\right) \\ &amp;amp;=\sigma\left(\log\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_2)\Big]p(\mathcal{C}_2)}{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_1)\Big]p(\mathcal{C}_1)}\right) \\ &amp;amp;=\sigma\Bigg(-\frac{1}{2}\Big[-\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2-\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\nonumber \\ &amp;amp;\hspace{2cm}+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\Big]-\log\frac{p(\mathcal{C}_2)}{p(\mathcal{C}_1)}\Bigg) \\ &amp;amp;=\sigma\left(\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2\right)^\text{T}\mathbf{x}-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\label{eq:ggm.1}
\end{align}
Let
\begin{align}
\mathbf{w}&amp;amp;=\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2), \\ w_0&amp;amp;=-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)},
\end{align}
we have \eqref{eq:ggm.1} can be rewritten in more convenient form as
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
From the derivation, we see that by making an assumption of having the same covariance matrix $\boldsymbol{\Sigma}$ across the densities helped us remove out the quadratic terms of $\mathbf{x}$, which leads us to ending up with a logistic sigmoid of a linear function of $\mathbf{x}$.&lt;/p&gt;

&lt;p&gt;For the multi-dimensional case, $K&amp;gt;2$, by \eqref{eq:pgm.2}, we have that the density for class $\mathcal{C}_k$ is
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;amp;=\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)+\log p(\mathcal{C}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_i)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_i)+\log p(\mathcal{C}_i)\Big]} \\ &amp;amp;=\frac{\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_k+\log p(\mathbf{w}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_i-\frac{1}{2}\boldsymbol{\mu}_i^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_i+\log p(\mathbf{w}_i)\Big]}
\end{align}
Or in other words, we can simplify each element of $\mathbf{a}$ into a linear function as
\begin{equation}
a_k\doteq a_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0},
\end{equation}
where
\begin{align}
\mathbf{w}_k&amp;amp;=\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k, \\ w_{k,0}&amp;amp;=-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k+\log p(\mathcal{C}_k)
\end{align}
The simplification we can make also come from the assumption of sharing the same covariance matrix between densities, which is analogous to the binary case that cancelled out the quadratic terms.&lt;/p&gt;

&lt;h5 id=&quot;max-likelihood-sols&quot;&gt;Maximum likelihood solutions&lt;/h5&gt;
&lt;p&gt;Once we have specified a parametric functional form of $p(\mathbf{x}\vert\mathcal{C}_k)$, using maximum likelihood, we can solve for the values of the parameters and also the prior probabilities $p(\mathcal{C}_k)$.&lt;/p&gt;

&lt;h6 id=&quot;ggm-bin-clf&quot;&gt;Binary classification&lt;/h6&gt;
&lt;p&gt;In particular, first off for the binary case, in which each class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ is a Bivariate Normal, with a shared covariance matrix, as
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Consider the data set $\{\mathbf{x}_n,t_n\}$ for $n=1,\ldots,N$, i.e., $t_n=1$ denotes class $\mathcal{C}_1$ and $t_n=0$ denotes class $\mathcal{C}_2$. Let the class prior probability $p(\mathcal{C}_1)=\pi$, thus $p(\mathcal{C}_2)=1-\pi$. Or
\begin{align}
p(\mathbf{x}_n,\mathcal{C}_1)&amp;amp;=p(\mathcal{C}_1)p(\mathbf{x}_n\vert\mathcal{C}_1)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}), \\ p(\mathbf{x}_n,\mathcal{C}_2)&amp;amp;=p(\mathcal{C}_2)p(\mathbf{x}_n\vert\mathcal{C}_2)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})
\end{align}
We also have that
\begin{equation}
p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n}
\end{equation}
Therefore, the likelihood can be defined as
\begin{align}
L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})&amp;amp;=p(\mathbf{t}\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\prod_{n=1}^{N}p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\prod_{n=1}^{N}p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n} \\ &amp;amp;=\prod_{n=1}^{N}\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]^{t_n}\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]^{1-t_n},
\end{align}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$. As usual, we continue to consider the log likelihood $\ell(\cdot)$
\begin{align}
&amp;amp;\hspace{0.7cm}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\log L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]\label{eq:gbc.1}
\end{align}
Taking the gradient of the log likelihood w.r.t $\pi$ we have
\begin{align}
&amp;amp;\hspace{0.7cm}\nabla_\pi\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\pi+(1-t_n)\log(1-\pi) \\ &amp;amp;=\sum_{n=1}^{N}\left[\frac{t_n}{\pi}-\frac{1-t_n}{1-\pi}\right]
\end{align}
Setting the derivative to zero and solve for $\pi$ as usual, we have
\begin{equation}
\sum_{n=1}^{N}t_n-\pi=0
\end{equation}
Thus, we obtain the solution
\begin{equation}
\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2},
\end{equation}
where $N_1,N_2$ denote the total number of data points in class $\mathcal{C}_1$ and $\mathcal{C}_2$ respectively.&lt;/p&gt;

&lt;p&gt;On the other hand, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\mu}_1$, we have
\begin{align}
&amp;amp;\hspace{0.7cm}\nabla_{\boldsymbol{\mu}_1}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right] \\ &amp;amp;\propto\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\big(-\boldsymbol{x}_n^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}_n+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\big) \\ &amp;amp;=\sum_{n=1}^{N}t_n\Big[\big(\boldsymbol{\Sigma}^{-1}+(\boldsymbol{\Sigma}^{-1})^\text{T}\big)\big(\boldsymbol{\mu}_1-\mathbf{x}_n\big)\Big] \\ &amp;amp;\propto\sum_{n=1}^{N}t_n(\boldsymbol{\mu}_1-\mathbf{x}_n)
\end{align}
Setting the above gradient to zero and solve for $\boldsymbol{\mu}_1$, we obtain the solution
\begin{equation}
\boldsymbol{\mu}_1=\frac{1}{N_1}\sum_{n=1}^{N}t_n\mathbf{x}_n,
\end{equation}
which is simply the mean of all input vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_1$.&lt;/p&gt;

&lt;p&gt;Similarly, with the same procedure, we have that the maximum likelihood solution for $\boldsymbol{\mu}_2$ is the mean of all inputs vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_2$, as
\begin{equation}
\boldsymbol{\mu}_2=\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\mathbf{x}_n
\end{equation}
Lastly, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\Sigma}$, we have
\begin{align}
&amp;amp;\hspace{0.7cm}\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})+(1-t_n)\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right]\nonumber \\ &amp;amp;\hspace{2cm}+(1-t_n)\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2)\right] \\ &amp;amp;\propto\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\log\big\vert\boldsymbol{\Sigma}\big\vert+t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\nonumber \\ &amp;amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2) \\ &amp;amp;=N\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\nonumber \\ &amp;amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gbc.2}
\end{align}
The first term of the gradient can be computed as
\begin{align}
\frac{\partial\log\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\frac{\partial\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\text{adj}(\boldsymbol{\Sigma})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ij},
\end{align}
since $\boldsymbol{\Sigma}$ is symmetric and so is its inverse. This implies that
\begin{equation}
\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert=\boldsymbol{\Sigma}^{-1}\label{eq:gbc.3}
\end{equation}
Let $\mathbf{S}$ be a matrix defined as
\begin{equation}
\mathbf{S}=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}
\end{equation}
Therefore, the derivative \eqref{eq:gbc.2} can be rewritten as
\begin{equation}
\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=N\boldsymbol{\Sigma}^{-1}-N\boldsymbol{\Sigma}^{-1}\mathbf{S}\boldsymbol{\Sigma}^{-1}
\end{equation}
Setting this gradient to zero and solve for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{S},
\end{equation}
where $\mathbf{S}$ can be continued to derive as
\begin{align}
\mathbf{S}&amp;amp;=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T} \\ &amp;amp;=\frac{N_1}{N}\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+\frac{N_2}{N}\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T},
\end{align}
which is the weighted average of the covariance matrices corresponded to each of the two classes $\mathcal{C}_1,\mathcal{C}_2$.&lt;/p&gt;

&lt;h6 id=&quot;ggm-clf&quot;&gt;Multi-class classification&lt;/h6&gt;
&lt;p&gt;To generalize the Gaussian generative binary classification, we consider a model for $K&amp;gt;2$ classes defined by prior class probabilities $p(\mathcal{C}_k)=\pi_k$ and Multivariate Normal class-conditional densities with shared covariance matrix, given as
\begin{equation}
p({\boldsymbol{\phi}}\vert\mathcal{C}_k)=\mathcal{N}(\boldsymbol{\phi}\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}),
\end{equation}
where $\boldsymbol{\phi}$ is the input feature vector.&lt;/p&gt;

&lt;p&gt;Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Therefore, we have that
\begin{equation}
p(\boldsymbol{\phi}_n,\mathcal{C}_k)=p(\mathcal{C}_k)p(\boldsymbol{\phi}_n\vert\mathcal{C}_k)=\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Analogy to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma})=\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k}
\end{equation}
To simplify the notation, we let $\mathbf{w}$ denote
\begin{equation}
\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma}
\end{equation}
And let $\mathbf{T}$ be a matrix that associates those targets $\mathbf{t}_n$’s together, given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Thus, the likelihood is given as
\begin{align}
L(\mathbf{w})&amp;amp;=p(\mathbf{T}\vert\mathbf{w}) \\ &amp;amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}) \\ &amp;amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k} \\ &amp;amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k}
\end{align}
And thus, the log likelihood $\ell(\cdot)$ can be computed as
\begin{align}
\ell(\mathbf{w})&amp;amp;=\log L(\mathbf{w}) \\ &amp;amp;=\log\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k} \\ &amp;amp;=\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]\label{eq:gc.1}
\end{align}
As usual, we continue by using maximum likelihood, which begins by taking gradient of the log likelihood w.r.t to the parameters. However, when maximizing the likelihood w.r.t $\pi_k$, we have to compute subject to a constraint that
\begin{equation}
\sum_{k=1}^{K}\pi_k=1
\end{equation}
Therefore, using a Lagrange multiplier $\lambda$, we instead maximize the Lagrangian w.r.t $\pi_k$, which is
\begin{equation}
\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda)=\ell(\mathbf{w})+\lambda\left(\sum_{k=1}^{K}\pi_k-1\right)
\end{equation}
Differentiating $\mathcal{L}$ w.r.t $\pi_k$, we have
\begin{align}
&amp;amp;\hspace{0.7cm}\nabla_{\pi_k}\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda) \\ &amp;amp;=\nabla_{\pi_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big]+\nabla_{\pi_k}\lambda\left(\sum_{i=1}^{K}\pi_i-1\right) \\ &amp;amp;=\lambda+\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\pi_k}\log\pi_k \\ &amp;amp;=\lambda+\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\pi_k}
\end{align}
Setting the derivative equal to zero and solve for $\pi_k$, we have
\begin{equation}
\pi_k=-\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\lambda}=\frac{N_k}{\lambda},
\end{equation}
where $N_k$ denotes the number of data points in class $\mathcal{C}_k$. Moreover, since $\sum_{k=1}^{K}\pi_k=1$, we have
\begin{equation}
1=-\sum_{k=1}^{K}\frac{N_k}{\lambda}=\frac{-N}{\lambda},
\end{equation}
which implies that
\begin{equation}
\lambda=-N
\end{equation}
Hence, the maximum likelihood solution for $\pi_k$ is
\begin{equation}
\pi_k=-\frac{N_k}{\lambda}=\frac{N_k}{N}
\end{equation}
We continue by taking the gradient of the log likelihood \eqref{eq:gc.1} w.r.t $\boldsymbol{\mu}_k$, as
\begin{align}
\nabla_{\boldsymbol{\mu}_k}\ell(\mathbf{w})&amp;amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big] \\ &amp;amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;amp;=-\frac{1}{2}\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\boldsymbol{\mu}_k}\Big[\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-2\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big] \\ &amp;amp;=\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big]
\end{align}
Setting the above gradient equal to zero and solve for $\boldsymbol{\mu}_k$ we obtain the solution
\begin{equation}
\boldsymbol{\mu}_k=\frac{1}{\sum_{n=1}^{N}(\mathbf{t}_n)_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n,
\end{equation}
which is the mean of feature vectors assigned to class $\mathcal{C}_k$.&lt;/p&gt;

&lt;p&gt;Finally, consider the gradient of \eqref{eq:gc.1} w.r.t $\boldsymbol{\Sigma}$, combined with the result \eqref{eq:gbc.3} we have
\begin{align}
\nabla_\boldsymbol{\Sigma}\ell(\mathbf{w})&amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma})\Big] \\ &amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma}) \\ &amp;amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;amp;=-\frac{N}{2}\boldsymbol{\Sigma}^{-1}+\frac{1}{2}\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1} \\ &amp;amp;\propto N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gc.2}
\end{align}
Let $\mathbf{S}_k$ be the covariance of the data associated with class $\mathcal{C}_k$, defined as
\begin{equation}
\mathbf{S}_k=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}
\end{equation}
Therefore, letting the derivative \eqref{eq:gc.2} equal to zero, we have
\begin{equation}
N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{k=1}^{K}N_k\mathbf{S}_k\Big]\boldsymbol{\Sigma}^{-1}=0
\end{equation}
Solving this equation for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\sum_{k=1}^{K}\frac{N_k}{N}\mathbf{S}_k
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;prob-disc-models&quot;&gt;Probabilistic Discriminative Models&lt;/h3&gt;

&lt;h4 id=&quot;log-reg&quot;&gt;Logistic Regression&lt;/h4&gt;
&lt;p&gt;Recall that in the previous section of generative approach, in particular for the binary case we knew that the posterior probability for class $\mathcal{C}_1$ can be defined as the logistic sigmoid of a linear function of the input vector $\mathbf{x}$
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
In general, the posterior probabilities can be written as the logistic sigmoid of a linear function of instead feature vector $\boldsymbol{\phi}$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma\big(\mathbf{w}^\text{T}\boldsymbol{\phi}+w_0\big)
\end{equation}
This model is called &lt;strong&gt;logistic regression&lt;/strong&gt;, although it is applied for classification tasks.
Consider a data set $\{\boldsymbol{\phi}_n,t_n\}$, where $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\mathbf{x}_n)$ and $t_n\in\{0,1\}$, with $n=1,\ldots,N$. Therefore,
\begin{equation}
p(t_n\vert\mathbf{w})=y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $y_n=p(\mathcal{C}_1\vert\boldsymbol{\phi}_n)$.&lt;/p&gt;

&lt;p&gt;Comprise $t_n$’s into $\mathbf{t}\doteq(t_1,\ldots,t_N)^\text{T}$, then we have that the likelihood function can be defined as
\begin{equation}
L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}\tag{35}\label{eq:lr.1}
\end{equation}
Taking the negative logarithm of the likelihood gives us the &lt;strong&gt;cross-entropy&lt;/strong&gt; error function, as
\begin{align}
E(\mathbf{w})=-\log L(\mathbf{w})&amp;amp;=-\log\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \\ &amp;amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n)\label{eq:lr.2}
\end{align}
Differentiating the error function $E(\mathbf{w})$ w.r.t $\mathbf{w}$ we have that
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;amp;=\nabla_\mathbf{w}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;amp;=\sum_{n=1}^{N}\frac{(1-t_n)\nabla_\mathbf{w}y_n}{1-y_n}-\frac{t_n\nabla_\mathbf{w}y_n}{y_n} \\ &amp;amp;=\sum_{n=1}^{N}\frac{(1-t_n)y_n(1-y_n)\boldsymbol{\phi}_n}{1-y_n}-\frac{t_n y_n(1-y_n)\boldsymbol{\phi}_n}{y_n} \\ &amp;amp;=\sum_{n=1}^{N}(1-t_n)y_n\boldsymbol{\phi}_n-t_n(1-y_n)\boldsymbol{\phi}_n \\ &amp;amp;=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n,\label{eq:lr.3}
\end{align}
where in the third step, we have used the identity of the &lt;span id=&quot;sigmoid-derivative&quot;&gt;derivative of the logistic sigmoid function&lt;/span&gt;
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}
and the chain rule to compute the gradient of $y_n$ w.r.t $\mathbf{w}$ as
\begin{align}
\nabla_\mathbf{w}y_n&amp;amp;=\nabla_\mathbf{w}\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;amp;=\frac{d\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}{d(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}\nabla_\mathbf{w}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;amp;=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big(1-\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big)\boldsymbol{\phi}_n \\ &amp;amp;=y_n(1-y_n)\boldsymbol{\phi}_n
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;softmax-reg&quot;&gt;Softmax Regression&lt;/h4&gt;
&lt;p&gt;Analogy to the generalization of the binary case into logistic regression, for the multi-class case, the posterior probability for class $\mathcal{C}_k$ can be written as the softmax function of a linear function of feature vectors $\boldsymbol{\phi}$ as
\begin{equation}
p(\mathcal{C}_k\vert\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
where $a_k$’s is called the &lt;strong&gt;activations&lt;/strong&gt;, defined as
\begin{equation}
a_k=\mathbf{w}_k^\text{T}\boldsymbol{\phi}
\end{equation}
Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Similar to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K)=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\boldsymbol{\phi}_n)^{(\mathbf{t}_n)_k}=\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k},
\end{equation}
where $(y_{n})_k=y_k(\boldsymbol{\phi}_n)$.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{T}$ be a $N\times K$ matrix comprising $\mathbf{t}_n$’s together as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the likelihood function can be written by
\begin{align}
L(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;amp;=p(\mathbf{T}\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k}
\end{align}
We also obtain the cross-entropy error function by taking the negative logarithm of the likelihood, as
\begin{align}
E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;amp;=-\log L(\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k} \\ &amp;amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log(y_{n})_k\label{eq:sr.1}
\end{align}
As usual, taking the gradient of the error function $E(\mathbf{w}_1,\ldots,\mathbf{w}_K)$ w.r.t $\mathbf{w}_k$ we have
\begin{align}
\nabla_{\mathbf{w}_k}E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;amp;=\nabla_{\mathbf{w}_k}-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\log(y_{n})_i \\ &amp;amp;=-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\frac{(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n}{(y_n)_i} \\ &amp;amp;=\sum_{n=1}^{N}\Big[(y_n)_k\sum_{i=1}^{K}(\mathbf{t}_n)_i-\sum_{i=1}^{K}(\mathbf{t}_n)_i 1\{i=k\}\Big]\boldsymbol{\phi}_n \\ &amp;amp;=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n\label{eq:sr.2}
\end{align}
where in the second step, we have used the &lt;span id=&quot;softmax-derivative&quot;&gt;identity&lt;/span&gt;
\begin{align}
\frac{\partial y_k}{\partial a_j}&amp;amp;=\frac{\big(\partial\exp(a_k)/\partial\exp(a_j)\big)\sum_{i=1}^{K}\exp(a_i)-\exp(a_j)\exp(a_k)}{\big(\sum_{i=1}^{K}\exp(a_i)\big)^2} \\ &amp;amp;=\frac{\exp(a_k)1\{k=j\}}{\sum_{i=1}^{K}\exp(a_i)}-y_k y_j \\ &amp;amp;=y_k(1\{k=j\}-y_j)
\end{align}
where $1\{k=j\}$ is the indicator function, which returns $1$ if $k=j$ and returns $0$ otherwise. Hence, by chain rule, we obtain the gradient of $(y_n)_i$ w.r.t $\mathbf{w}_k$ given by
\begin{align}
\nabla_{\mathbf{w}_k}(y_n)_i&amp;amp;=\frac{\partial(y_n)_i}{\partial a_k}\frac{\partial a_k(\mathbf{w}_k,\boldsymbol{\phi}_n)}{\partial\mathbf{w}_k} \\ &amp;amp;=(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;newtons-method&quot;&gt;Newton’s method&lt;/h4&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-08-13/newtons-method.gif&quot; alt=&quot;Newton&apos;s method)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: Illustration of the Newton&apos;s method. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/classification/newtons-method.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;\begin{equation}
\mathbf{w}^{(\text{new})}=\mathbf{w}^{(\text{old})}-\mathbf{H}^{-1}\nabla_\mathbf{w}E(\mathbf{w})
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;nm-lin-reg&quot;&gt;Linear Regression&lt;/h5&gt;
&lt;p&gt;Consider applying the Newton’s method to the sum-of-squares error function \eqref{eq:lsr.4} for the linear regression model \eqref{eq:lbfm.2}. The gradient and Hessian of this error function are
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}\left(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}_n)\right)^2 \\ &amp;amp;=\sum_{n=1}^{N}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
and
\begin{equation}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})=\nabla_\mathbf{w}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big)=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi},
\end{equation}
where $\boldsymbol{\Phi}$, as defined before, is the $N\times M$ design matrix
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;amp;\ldots&amp;amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;amp;\ddots&amp;amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;amp;\ldots&amp;amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
Hence, we have that the Newton’s update of the model is given by
\begin{align}
\mathbf{w}^{(\text{new})}&amp;amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big) \\ &amp;amp;=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
which is exactly the standard least-squares solution.&lt;/p&gt;

&lt;h5 id=&quot;nm-log-reg&quot;&gt;Logistic Regression&lt;/h5&gt;
&lt;p&gt;Consider using the Newton’s method to the logistic regression model with the cross-entropy error function \eqref{eq:lr.2}. By the result \eqref{eq:lr.3}, we have the gradient and Hessian of this error function are given as
\begin{equation}
\nabla_\mathbf{w}E(\mathbf{w})=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t})
\end{equation}
and
\begin{align}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})&amp;amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n \\ &amp;amp;=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\label{eq:nlr.1} \\ &amp;amp;=\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi},
\end{align}
where $\mathbf{R}$ is the $N\times N$ diagonal matrix with diagonal elements
\begin{equation}
\mathbf{R}_{n n}=y_n(1-y_n)
\end{equation}
It is noticeable that hessian matrix $\mathbf{H}$ is positive definite because for any vector $\mathbf{v}$
\begin{equation}
\mathbf{v}^\text{T}\mathbf{H}\mathbf{v}=\mathbf{v}^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{v}&amp;gt;0,
\end{equation}
since $\mathbf{R}$ is positive definite due to $y_n\in(0,1)$ letting all the diagonal elements of $\mathbf{R}$ are positive. This positive definiteness claims that the cross-entropy error function is a concave function of $\mathbf{w}$ and thus has a unique minimum.&lt;/p&gt;

&lt;p&gt;Back to our main attention, the Newton’s update of the model then takes the form
\begin{align}
\mathbf{w}^{(\text{new})}&amp;amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t}) \\ &amp;amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\Big[\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}(\mathbf{y}-\mathbf{t})\Big] \\ &amp;amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
where $\mathbf{z}$ is an $N$-dimensional vector given by
\begin{equation}
\mathbf{z}=\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\mathbf{R}^{-1}(\mathbf{y}-\mathbf{t})
\end{equation}
This algorithm is known as &lt;strong&gt;iterative reweighted least squares&lt;/strong&gt;, or &lt;strong&gt;IRLS&lt;/strong&gt;.&lt;/p&gt;

&lt;h5 id=&quot;nm-softmax-reg&quot;&gt;Softmax Regression&lt;/h5&gt;
&lt;p&gt;Consider applying the Newton’s method to the cross-entropy error function \eqref{eq:sr.1} for the softmax regression model.&lt;/p&gt;

&lt;p&gt;First, let $\mathbf{W}$ be the $M\times K$ matrix that comprises $\mathbf{w}_1,\ldots,\mathbf{w}_K$ together, as
\begin{equation}
\mathbf{W}=\left[\begin{matrix}\vert&amp;amp;&amp;amp;\vert \\ \mathbf{w}_1&amp;amp;\ldots&amp;amp;\mathbf{w}_K \\ \vert&amp;amp;&amp;amp;\vert\end{matrix}\right]
\end{equation}
By the result \eqref{eq:sr.2}, we have that the $k$-th column of the gradient of this error function is given by
\begin{equation}
\nabla_{\mathbf{w}_k}E(\mathbf{W})=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}_k-\mathbf{T}_k),
\end{equation}
where $\boldsymbol{\Phi}$ be the $N\times M$ design matrix, given as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}_1^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right]
\end{equation}
and where $\mathbf{Y}_k,\mathbf{T}_k$ are the $k$th columns of the $N\times K$ matrices
\begin{equation}
\mathbf{Y}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{y}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{y}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right];\hspace{2cm}\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the gradient of the error function w.r.t $\mathbf{W}$ can be written as
\begin{equation}
\nabla_\mathbf{W}E(\mathbf{W})=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}-\mathbf{T})
\end{equation}
Now we consider the hessian matrix $\mathbf{H}$ of the error function, whose block $(k,j)$ is given by
\begin{align}
\mathbf{H}_{k j}&amp;amp;=\nabla_{\mathbf{w}_j}\nabla_{\mathbf{w}_k} E(\mathbf{W}) \\ &amp;amp;=\nabla_{\mathbf{w}_j}\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n \\ &amp;amp;=\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}
\end{align}
Analogous to the binary case, the hessian $\mathbf{H}$ for the multi-class logistic regression model is positive semi-definite. To prove it, since $\mathbf{H}$ is an $MK\times MK$ matrix, consider an $MK$-dimensional vector $\mathbf{u}$. Thus, $\mathbf{u}$ can be represented as
\begin{equation}
\mathbf{u}=\left[\begin{matrix}\mathbf{u}_1^\text{T}&amp;amp;\ldots&amp;amp;\mathbf{u}_K^\text{T}\end{matrix}\right]^\text{T},
\end{equation}
where each $\mathbf{u}_k$ is a vector of length $M$, for $k=1,\ldots,K$. Therefore, we have
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\mathbf{H}_{k j}\mathbf{u}_j \\ &amp;amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j \\ &amp;amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}\sum_{j=1}^{K}(y_n)_k(y_n)_j\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j\right] \\ &amp;amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right]\label{eq:nsr.1}
\end{align}
Consider $f:\mathbb{R}^M\to\mathbb{R}$, defined as
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}
\end{equation}
Thus, it follows immediately from the definition of $f$ that $f$ is convex since
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}=\Vert\mathbf{x}^\text{T}\boldsymbol{\phi}_n\Vert_2^2\geq 0
\end{equation}
Let us apply &lt;strong&gt;Jensen’s inequality&lt;/strong&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; for $f$ with observing that $\sum_{k=1}^{K}(y_n)_k=\sum_{j=1}^{K}(y_n)_j=1$, then \eqref{eq:nsr.1} can be continued to derive as
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right] \\ &amp;amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k f\left(\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;amp;\geq\sum_{n=1}^{N}\left[f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;amp;=0,
\end{align}
which claims the positive semi-definiteness of $\mathbf{H}$. Therefore, the error function $E(\mathbf{w})$ is concave and thus has a unique minimum.&lt;/p&gt;

&lt;h3 id=&quot;bayes-log-reg&quot;&gt;Bayesian Logistic Regression&lt;/h3&gt;
&lt;p&gt;When using Bayesian approach for logistic regression model, unlike the case of linear regression \eqref{eq:pd.1}, the posterior distribution now is no longer Gaussian. This makes the evaluation of posterior be intractable when integrating over the parameter $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;Therefore, it is necessary to use some approximation methods.&lt;/p&gt;

&lt;h4 id=&quot;laplace-approx&quot;&gt;The Laplace approximation&lt;/h4&gt;
&lt;p&gt;The goal of &lt;strong&gt;Laplace approximation&lt;/strong&gt; is to fit a Gaussian distribution to a probability density defined over a set of continuous variables&lt;/p&gt;

&lt;p&gt;We begin by consider applying Laplace method to one-dimensional variables $z$ with the density function $p(z)$ is defined as
\begin{equation}
p(z)=\frac{1}{Z}f(z),
\end{equation}
where $Z=\int f(z)\,dz$ is the normalization coefficient, and is unknown.&lt;/p&gt;

&lt;p&gt;The idea behind Laplace method is to place a Gaussian $q(z)$ on a mode of the distribution $p(z)$. A mode $z_0$ of $p(z)$ is where the distribution reaches its global maximum, which also means the derivative of $p(z)$ at $z_0$ is zero
\begin{equation}
\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}=0
\end{equation}
Therefore, the Taylor expansion of $\log f(z)$ about $z=z_0$ can be written by
\begin{align}
\log f(z)&amp;amp;\simeq\log f(z_0)+\log f(z)\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}(z-z_0)+\frac{1}{2}\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}(z-z_0)^2 \\ &amp;amp;=\log f(z_0)-\frac{A}{2}(z-z_0)^2,
\end{align}
where
\begin{equation}
A=-\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}
\end{equation}
Thus, taking the exponential gives us
\begin{equation}
f(z)\simeq f(z_0)\exp\left(-\frac{A}{2}(z-z_0)^2\right),
\end{equation}
which is in a form of an unnormalized Gaussian distribution. Hence, we can obtain a Gaussian approximation $q(z)$ of $p(z)$ by adding a normalization parameter to form a Normal distribution, as
\begin{equation}
q(z)=\left(\frac{A}{2\pi}\right)^{1/2}\exp\left(-\frac{A}{2}(z-z_0)^2\right)=\mathcal{N}(\mathbf{z}\vert z_0,A^{-1})
\end{equation}
We can extend the Laplace approximation into multi-dimensional variable $\mathbf{z}$, which is finding an Gaussian approximation of distribution
\begin{equation}
p(\mathbf{z})=\frac{1}{Z}f(\mathbf{z}),
\end{equation}
where $z$ is a vector of length $M\geq 2$.&lt;/p&gt;

&lt;p&gt;Analogy to the univariate case, the first step is to consider the Taylor expansion of $\log f(\mathbf{z})$ about its stationary point $\mathbf{z}_0$, which means $\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}=0$. We have
\begin{align}
\log f(\mathbf{z})&amp;amp;\simeq f(\mathbf{z}_0)+\log f(\mathbf{z})\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}+\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}(\mathbf{z}-\mathbf{z}_0) \\ &amp;amp;=\log f(\mathbf{z}_0)-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0),
\end{align}
where
\begin{equation}
\mathbf{A}=-\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(z)\vert_{\mathbf{z}=\mathbf{z}_0}
\end{equation}
Taking the exponentials of both sides lets us obtain
\begin{equation}
f(\mathbf{z})\simeq f(\mathbf{z}_0)\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right),
\end{equation}
which is in form of an unnormalized multivariate Gaussian. Adding a normalization parameter gives us the Gaussian approximation $q(\mathbf{z})$ of $p(\mathbf{z})$
\begin{equation}
q(\mathbf{z})=\frac{\vert\mathbf{A}\vert^{1/2}}{(2\pi)^{M/2}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right)=\mathcal{N}(\mathbf{z}\vert\mathbf{z}_0,\mathbf{A}^{-1})
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;approx-posterior&quot;&gt;Approximation of the posterior&lt;/h4&gt;
&lt;p&gt;Consider the prior to be a Gaussian, which is
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ and $\mathbf{S}_0$ are known. Along with this is the likelihood function, which is defined by \eqref{eq:lr.1}, as
\begin{equation}
p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$, and $y_n=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n)$. Therefore, by Bayes’ theorem, the posterior is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})\propto p(\mathbf{w})p(\mathbf{t}\vert\mathbf{w}),
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{w}\vert\mathbf{t})&amp;amp;=-\frac{1}{2}(\mathbf{w}-\mathbf{m}_0)^\text{T}\mathbf{S}_0^{-1}(\mathbf{w}-\mathbf{m}_0)\nonumber \\ &amp;amp;\hspace{2cm}+\sum_{n=1}^{N}\big[t_n\log y_n+(1-t_n)\log(1-y_n)\big]+c,
\end{align}
where $c$ is independent of $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;By Laplace approximation, to find a Gaussian approximation of the posterior, the first step is looking for the point which maximizes the posterior, which is the $\mathbf{w}_\text{MAP}$. This point also defines the mean of the approximation. The corresponding covariance matrix $\mathbf{S}_N$ of the Gaussian is given by
\begin{align}
\mathbf{S}_N&amp;amp;=-\nabla_\mathbf{w}\nabla_\mathbf{w}\log p(\mathbf{w}\vert\mathbf{t}) \\ &amp;amp;=\mathbf{S}_0^{-1}+\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T},
\end{align}
where the second step is obtained by using the result \eqref{eq:nlr.1}. Therefore, the Gaussian approximation $q(\mathbf{w})$ for the posterior distribution is given by
\begin{equation}
q(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{w}_\text{MAP},\mathbf{S}_N)\label{eq:ap.1}
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;pred-dist-clf&quot;&gt;Predictive distribution&lt;/h4&gt;
&lt;p&gt;With the Gaussian approximation \eqref{eq:ap.1}, the predict distribution for class $\mathcal{C}_1$, given a new feature vector $\boldsymbol{\phi}(\mathbf{x})$, is then given by marginalizing w.r.t the posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})=\int p(\mathcal{C}_1\vert\boldsymbol{\phi}\mathbf{w})p(\mathbf{w}\vert\mathbf{t})\,d\mathbf{w}\simeq\int\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi})q(\mathbf{w})\,d\mathbf{w}
\end{equation}
And thus, the predictive distribution for class $\mathcal{C}_2$ is given by
\begin{equation}
p(\mathcal{C}_2\vert\boldsymbol{\phi},\mathbf{t})=1-p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;glm&quot;&gt;Generalized linear models&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;bishops-book&quot;&gt;Christopher M. Bishop. &lt;a href=&quot;https://link.springer.com/book/9780387310732&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;. Springer New York, NY.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] Gilbert Strang. &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] MIT 18.06. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&quot;&gt;Linear Algebra&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] MIT 18.02. &lt;a href=&quot;https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/&quot;&gt;Multivariable Calculus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://stats.stackexchange.com/users/28666/amoeba&quot;&gt;amoeba&lt;/a&gt;. &lt;a href=&quot;https://stats.stackexchange.com/q/204599&quot;&gt;What is an isotropic (spherical) covariance matrix?&lt;/a&gt;. Cross Validated.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A covariance matrix $\mathbf{C}$ is &lt;strong&gt;isotropic&lt;/strong&gt; (or &lt;strong&gt;spherical&lt;/strong&gt;) if it is proportional to the identity matrix $\mathbf{I}$
\begin{equation*}
\mathbf{C}=\lambda\mathbf{I},
\end{equation*}
where $\lambda\in\mathbb{R}$ is a constant. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;For positive numbers $p_1,\ldots,p_n$ such that $\sum_{i=1}^{n}p_i=1$ and $f$ is a continuous function, if $f$ is &lt;strong&gt;convex&lt;/strong&gt;, then
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\leq\sum_{i=1}^{n}p_if(x_i),
\end{equation*}
and if $f$ is &lt;strong&gt;concave&lt;/strong&gt;, we instead have
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\geq\sum_{i=1}^{n}p_if(x_i),
\end{equation*} &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="machine-learning" /><category term="artificial-intelligent" /><category term="machine-learning" /><category term="linear-regression" /><category term="logistic-regression" /><category term="linear-discriminant-analysis" /><summary type="html">Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.</summary></entry><entry><title type="html">Measure theory - II: Lebesgue measure</title><link href="http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2.html" rel="alternate" type="text/html" title="Measure theory - II: Lebesgue measure" /><published>2022-07-03T13:00:00+07:00</published><updated>2022-07-03T13:00:00+07:00</updated><id>http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2</id><content type="html" xml:base="http://localhost:4000/mathematics/measure-theory/2022/07/03/measure-theory-p2.html">&lt;blockquote&gt;
  &lt;p&gt;Part II of the measure theory series. Materials are mostly taken from &lt;a href=&quot;/mathematics/measure-theory/2022/07/03/measure-theory-p2.html#taos-book&quot;&gt;Tao’s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&quot;/mathematics/measure-theory/2022/07/03/measure-theory-p2.html#steins-book&quot;&gt;Stein’s book&lt;/a&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lebesgue-measure&quot;&gt;Lebesgue measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lebesgue-outer-measure-properties&quot;&gt;Properties of Lebesgue outer measure&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#fnt-add-spt-sets&quot;&gt;Finite additivity for separated sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#outer-measure-elem-sets&quot;&gt;Outer measure of elementary sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#fnt-add-alm-dsjnt-boxes&quot;&gt;Finite additivity for almost disjoint boxes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#outer-msr-cntbl-uni-alm-dsjnt-boxes&quot;&gt;Outer measure of countable unions of almost disjoint boxes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#open-sets-cntbl-uni-alm-dsjnt-boxes&quot;&gt;Open sets as countable unions of almost disjoint boxes&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#outer-msr-open-sets&quot;&gt;Outer measure of open sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#outer-msr-arb-sets&quot;&gt;Outer measure of arbitrary sets - Outer regularity&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lebesgue-measurability&quot;&gt;Lebesgue measurability&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#exist-lebesgue-msr-sets&quot;&gt;Existence of Lebesgue measurable sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#crt-msrb&quot;&gt;Criteria for measurability&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#msr-axiom&quot;&gt;The measure axioms&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mnt-cvg-theorem-msr-sets&quot;&gt;Monotone convergence theorem for measurable sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#dmnt-cvg-theorem-msr-sets&quot;&gt;Dominated convergence theorem for measurable sets&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#inn-rglr&quot;&gt;Inner regularity&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#crt-fnt-msr&quot;&gt;Criteria for finite measure&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#caratheodory-crt&quot;&gt;Carathéodory criterion, one direction&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#inn-msr&quot;&gt;Inner measure&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#trans-inv&quot;&gt;Translation invariance&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#change-vars&quot;&gt;Change of variables&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#uniq-lebesgue-msr&quot;&gt;Uniqueness of Lebesgue measure&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#non-measurable-sets&quot;&gt;Non-measurable sets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lebesgue-measure&quot;&gt;Lebesgue measure&lt;/h2&gt;
&lt;p&gt;Recall that the Jordan outer measure of a set $E\subset\mathbb{R}^d$ has been defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
From the finite additivity and subadditivity of elementary measure, we can also write the Jordan outer measure as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B_1\cup\dots\cup B_k\supset E;B_1,\dots,B_k\text{ boxes}}\vert B_1\vert+\dots+\vert B_k\vert,
\end{equation}
which means the Jordan outer measure is the infimal cost required to cover $E$ by a finite union of boxes. By replacing the finite union of boxes by a countable union of boxes, we obtain the &lt;strong&gt;Lebesgue outer measure&lt;/strong&gt; $m^{*}(E)$ of $E$:
\begin{equation}
m^{*}(E)\doteq\inf_{\bigcup_{n=1}^{\infty}B_n\supset E;B_1,B_2,\dots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
which is be seen as the infimal cost required to cover $E$ by a countable union of boxes.&lt;/p&gt;

&lt;p&gt;A set $E\subset\mathbb{R}^d$ is said to be &lt;strong&gt;Lebesgue measurable&lt;/strong&gt; if, for every $\varepsilon&amp;gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $E$ such that $m^{*}(U\backslash E)\leq\varepsilon$. If $E$ is Lebesgue measurable, we refer to
\begin{equation}
m(E)\doteq m^{*}(E)
\end{equation}
as the &lt;strong&gt;Lebesgue measure&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;h3 id=&quot;lebesgue-outer-measure-properties&quot;&gt;Properties of Lebesgue outer measure&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Remark 1&lt;/strong&gt;. (&lt;strong&gt;The outer measure axioms&lt;/strong&gt;)&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;&lt;b&gt;Empty set&lt;/b&gt;. $m^*(\emptyset)=0$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Monotonicity&lt;/b&gt;. If $E\subset F\subset\mathbb{R}^d$, then $m^*(E)\leq m^*(F)$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Countable subadditivity&lt;/b&gt;. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of sets, then $m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{\infty}m^*(E_n)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;This follows from the definition of Lebesgue outer measure.&lt;/li&gt;
	&lt;li&gt;
		Since $E\subset F\subset\mathbb{R}^d$, then any set containing $F$ also includes $E$, but not every set having $E$ contains $F$. That means
		\begin{equation}
		\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\supset\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		Thus,
		\begin{equation}
		\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\leq\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		or
		\begin{equation}
		m^*(E)&amp;lt; m^*(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		By the definition of Lebesgue outer measure, for any positive integer $i$, we have
		\begin{equation}
		m^*(E_i)=\inf_{\bigcup_{n=1}^{\infty}B_n\supset E_i;B_1,B_2,\ldots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert
		\end{equation}
		Thus, by definition of infimum and by &lt;span&gt;&lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#countable-choice-axiom&quot;&gt;axiom of countable choice&lt;/a&gt;&lt;/span&gt;, for each $E_i$ in the sequence $(E_n)_{n\in\mathbb{N}}$, there exists a family of boxes $B_{i,1},B_{i,2},\ldots$ in the doubly sequence $(B_{i,j})_{(i,j)\in\mathbb{N}^2}$ covering $E_i$ such that
		\begin{equation}
		\sum_{j=1}^{\infty}\vert B_{i,j}\vert\lt m^*(E_i)+\frac{\varepsilon}{i},
		\end{equation}
		for any $\varepsilon&amp;gt;0$, and for $i=1,2,\ldots$. Plus, we also have
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}
		\end{equation}
		Moreover, by the &lt;span&gt;&lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#tonelli-theorem&quot;&gt;Tonelli’s theorem for series&lt;/a&gt;&lt;/span&gt;, we have
		\begin{equation}
		\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}=\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}
		\end{equation}
		Therefore once again, by definition of outer measure and definition of infimum, we obtain
		\begin{align}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;amp;=\inf_{\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert\leq\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert \\\\ &amp;amp;\lt\sum_{i=1}^{\infty}m^*(E_i)+\frac{\varepsilon}{2^i}=\sum_{i=1}^{\infty}m^*(E_i)+\varepsilon
		\end{align}
		And since $\varepsilon&amp;gt;0$ was arbitrary, we can conclude that
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{i=n}^{\infty}m^*(E_n)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Corollary 2&lt;/strong&gt;&lt;br /&gt;
Combining empty set with countable subadditivity axiom gives us the &lt;strong&gt;finite subadditivity&lt;/strong&gt; property
\begin{equation}
m^{*}\left(E_1\cup\ldots\cup E_k\right)\leq m^{*}(E_1)+\ldots+m^{*}(E_k),\hspace{1cm}\forall k\geq 0
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;fnt-add-spt-sets&quot;&gt;Finite additivity for separated sets&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;  &lt;br /&gt;
&lt;em&gt;Let $E,F\subset\mathbb{R}^d$ be such that $\text{dist}(E,F)&amp;gt;0$, where
\begin{equation}
\text{dist}(E,F)\doteq\inf\left\{\vert x-y\vert:x\in E,y\in F\right\}
\end{equation}
is the distance between $E$ and $F$. Then $m^*(E\cup F)=m^*(E)+m^*(F)$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From subadditivity property, we have $m^*(E\cup F)\leq m^*(E)+m^*(F)$. Then it suffices to prove the inverse, that
\begin{equation}
m^*(E\cup F)\geq m^*(E)+m^*(F)
\end{equation}
Let $\varepsilon&amp;gt;0$. By definition of Lebesgue outer measure, we can cover $E\cup F$ by a countable family $B_1,B_2,\ldots$ of boxes such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E\cup F)+\varepsilon
\end{equation}
Suppose it was the case that each box intersected at most one of $E$ and $F$. Then we could divide this family into two subfamilies $B_1’,B_2’,\ldots$ and $B_1&apos;&apos;,B_2&apos;&apos;,B_3&apos;&apos;,\ldots$, the first of which covered $E$, while the second of which covered $F$. From definition of Lebesgue outer measure, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}
and
\begin{equation}
m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n&apos;&apos;\vert
\end{equation}
Summing up these two equation, we obtain
\begin{equation}
m^*(E)+m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
and thus
\begin{equation}
m^*(E)+m^*(F)\leq m^*(E\cup F)+\varepsilon
\end{equation}
Since $\varepsilon$ was arbitrary, this gives $m^*(E)+m^*(F)\leq m^*(E\cup F)$ as required.&lt;/p&gt;

&lt;p&gt;Now we consider the case that some of the boxes $B_n$ intersect both $E$ and $F$.&lt;/p&gt;

&lt;p&gt;Since given any $r&amp;gt;0$, we can always partition a box $B_n$ into a finite number of smaller boxes, each of which has diameter&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; at most $r$, with the total volume of these sub-boxes equal to the volume of the original box $B_n$. Therefore, given any $r&amp;gt;0$, we may assume without loss of generality that the boxes $B_1,B_2,\ldots$ covering $E\cup F$ have diameter at most $r$. Or in particular, we may assume that all such boxes have diameter strictly less than $\text{dist}(E,f)$.&lt;/p&gt;

&lt;p&gt;Once we do this, then it is no longer possible for any box to intersect both $E$ and $F$, which allows the previous argument be applicable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 1&lt;/strong&gt;&lt;br /&gt;
Let $E,F\subset\mathbb{R}^d$ be disjoint closed sets, with at least one of $E,F$ being compact. Then $\text{dist}(E,F)&amp;gt;0$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;outer-measure-elem-sets&quot;&gt;Outer measure of elementary sets&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 4&lt;/strong&gt;  &lt;br /&gt;
&lt;em&gt;Let $E$ be an elementary set. Then the Lebesgue outer measure of $E$ is equal to the elementary measure of $E$:&lt;/em&gt;
\begin{equation}
m^*(E)=m(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since
\begin{equation}
m^*(E)\leq m^{*,(J)}(E)=m(E),
\end{equation}
then it suffices to show that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
We first consider the case that $E$ is closed. Since $E$ is elementary, $E$ is also bounded, which implies that $E$ is compact.&lt;/p&gt;

&lt;p&gt;Let $\varepsilon&amp;gt;0$ be arbitrary, then we can find a countable family $B_1,B_2,\ldots$ of boxes that cover $E$
\begin{equation}
E\subset\bigcup_{n=1}^{\infty}B_n,
\end{equation}
such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We have that for each box $B_n$, we can find an open box $B_n’$ containing $B_n$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n}
\end{equation}
The $B_n’$ still cover $E$ and we have
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq\sum_{n=1}^{\infty}\left(\vert B_n\vert+\frac{\varepsilon}{2^n}\right)=\left(\sum_{n=1}^{\infty}\vert B_n\vert\right)+\varepsilon\leq m^*(E)+2\varepsilon\label{eq:lemma5.1}
\end{equation}
As the $B_n’$ are open, apply the &lt;span&gt;&lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#heine-borel-theorem&quot;&gt;&lt;strong&gt;Heine-Borel theorem&lt;/strong&gt;&lt;/a&gt;, we obtain
\begin{equation}
E\subset\bigcup_{n=1}^{N}B_n’,
\end{equation}
for some finite $N$. Thus, using the finite subadditivity property of elementary measure, combined with the result \eqref{eq:lemma5.1}, we obtain
\begin{equation}
m(E)\leq\sum_{n=1}^{N}m(B_n’)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&amp;gt;0$ was arbitrary, we can conclude that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
Now we turn to considering the case that $E$ is not closed. Then we can write $E$ as the finite union of disjoint boxes
\begin{equation}
E=Q_1\cup\ldots\cup Q_k,
\end{equation}
which need not be closed.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Analogy to before, we have that for every $\varepsilon&amp;gt;0$ and every $1\leq j\leq k$, we can find a closed sub-box $Q_j’$ of $Q_j$ such that
\begin{equation}
\vert Q_j’\vert\geq\vert Q_j\vert-\frac{\varepsilon}{k}
\end{equation}
Then $E$ now contains the finite union of $Q_1’\cup\ldots\cup Q_k’$ disjoint closed boxes, which is a closed elementary set. By the finite additivity property of elementary measure, the monotonicity property of Lebesgue measure, combined with the result we have proved in the first case, we have
\begin{align}
m^*(E)&amp;amp;\geq m^*(Q_1’\cup\ldots\cup Q_k’) \\ &amp;amp;=m(Q_1’\cup\ldots\cup Q_k’) \\ &amp;amp;=m(Q_1’)+\ldots+m(Q_k’) \\ &amp;amp;\geq m(Q_1)+\ldots+m(Q_k)-\varepsilon \\ &amp;amp;= m(E)-\varepsilon,
\end{align}
for every $\varepsilon&amp;gt;0$. And since $\varepsilon&amp;gt;0$ was arbitrary, our claim has been proved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 6&lt;/strong&gt;&lt;br /&gt;
From the lemma above and the monotonicity property, 
for every $E\in\mathbb{R}^d$, we have
\begin{equation}
m_{*,(J)}(E)\leq m^{*}(E)\leq m^{*,(J)}(E)\label{eq:cor6.1}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 7&lt;/strong&gt;&lt;br /&gt;
Not every bounded open set or compact set (bounded closed) is Jordan measurable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider the countable set $\mathbf{Q}\cap[0,1]$, which we enumerate as $\{q_1,q_2,\ldots\}$. Let $\varepsilon&amp;gt;0$ be a small number, and consider that
\begin{equation}
U\doteq\bigcup_{n=1}^{\infty}(q_n-\varepsilon/2^n,q_n+\varepsilon/2^n),
\end{equation}
which is a union of open sets and thus is open. On the other hand, by countable subadditivity property of Lebesgue outer measure, we have
\begin{align}
m^{*}(U)&amp;amp;=m^{*}\left(\sum_{n=1}^{\infty}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right)\right) \\ &amp;amp;\leq\sum_{n=1}^{\infty}m^{*}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right) \\ &amp;amp;=\sum_{n=1}^{\infty}\frac{2\varepsilon}{2^n}=2\varepsilon
\end{align}
As $U$ dense in $[0,1]$ (i.e.,$\overline{U}$ contains $[0,1]$), we have
\begin{equation}
m^{*}(U)=m^{*,(J)}(\overline{U})\geq m^{*,(J)}([0,1])=1
\end{equation}
Then for $\varepsilon\lt 1$, we have that
\begin{equation}
m^{*}(U)\lt 1\leq m^{*,(J)}(U)
\end{equation}
Combining with \eqref{eq:cor6.1}, we obtain that the bounded open set $U$ is not Jordan measurable.&lt;/p&gt;

&lt;h4 id=&quot;fnt-add-alm-dsjnt-boxes&quot;&gt;Finite additivity for almost disjoint boxes&lt;/h4&gt;
&lt;p&gt;Two boxes are &lt;strong&gt;almost disjoint&lt;/strong&gt; if their interiors are disjoint, e.g., $[0,1]$ and $[1,2]$ are almost disjoint. If a box has the same elementary as its interior, we see that the finite additivity property
\begin{equation}
m(B_1\cup\ldots\cup B_n)=\vert B_1\vert+\ldots+\vert B_n\vert\label{eq:faadb.1}
\end{equation}
also holds for almost disjoint boxes $B_1,\ldots,B_n$.&lt;/p&gt;

&lt;h4 id=&quot;outer-msr-cntbl-uni-alm-dsjnt-boxes&quot;&gt;Outer measure of countable unions of almost disjoint boxes&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 8&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $E=\bigcup_{n=1}^{\infty}B_n$ be a countable union of almost disjoint boxes $B_1,B_2,\ldots$. Then&lt;/em&gt;
\begin{equation}
m^*(E)=\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
Thus, for example, $\mathbb{R}^d$ has an infinite outer measure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From countable subadditivity property of Lebesgue measure and &lt;strong&gt;Lemma 5&lt;/strong&gt;, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}m^*(B_n)=\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
so it suffices to show that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)
\end{equation}
Since for each integer $N$, $E$ contains the elementary set $B_1\cup\ldots\cup B_N$, then by monotonicity property and &lt;strong&gt;Lemma 5&lt;/strong&gt;
\begin{align}
m^*(E)&amp;amp;\geq m^*(B_1\cup\ldots\cup B_N)=m(B_1\cup\ldots\cup B_N)
\end{align}
And thus by \eqref{eq:faadb.1}, we have
\begin{equation}
\sum_{n=1}^{N}\vert B_n\vert\leq m^*(E)
\end{equation}
Letting $N\to\infty$ we obtain the claim.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Corollary 9&lt;/strong&gt;&lt;br /&gt;
If $E=\bigcup_{n=1}^{\infty}B_n=\bigcup_{n=1}^{\infty}B_n’$ can be decomposed in two different ways as the countable union of almost disjoint boxes, then
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert=\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2&lt;/strong&gt;&lt;br /&gt;
If a set $E\subset\mathbb{R}^{d}$ is expressible as the countable union of almost disjoint boxes, then
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
For $B_n$’s are disjoint boxes, we begin by express $E$ as 
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n\label{eq:eg2.1}
\end{equation}
Hence, by &lt;strong&gt;Lemma 8&lt;/strong&gt;, we have
\begin{equation}
m^{*}(E)=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.2}
\end{equation}
Moreover, \eqref{eq:eg2.1} can be continued to derive as
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n=\left(\bigcup_{n=1}^{N}B_n\right)\cup\left(\bigcup_{n=N+1}^{\infty}B_n\right)=\left(\bigcup_{n=1}^{N}B_n\right)\cup B,
\end{equation}
where we have defined $B=\bigcup_{n=N+1}^{\infty}B_n$. And thus, we also have that $B_1,\ldots,B_N,B$ are almost disjoint boxes, which claims that $E$ is an elementary set. Therefore, $E$ is also Jordan measurable. Using finite additivity property of Jordan measurability yields
\begin{equation}
m_{*,(J)}(E)=m(E)=\left(\sum_{n=1}^{N}\vert B_n\vert\right)+\vert B\vert=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.3}
\end{equation}
Combining \eqref{eq:eg2.2} and \eqref{eq:eg2.3} together gives us
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;open-sets-cntbl-uni-alm-dsjnt-boxes&quot;&gt;Open sets as countable unions of almost disjoint boxes&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 10&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an open set. Then $E$ can be expressed as the countable union of almost disjoint boxes (and, in fact, as the countable union of almost disjoint closed cubes)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We begin by defining a &lt;strong&gt;closed dyadic cube&lt;/strong&gt; to be a cube $Q$ of the form
\begin{equation}
Q=\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right]\times\ldots\times\left[\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d;n\geq 0$.&lt;/p&gt;

&lt;p&gt;We have that such closed dyadic cubes of a fixed sidelength $2^{-n}$ are almost disjoint and cover all of $\mathbb{R}^d$. And also, each dyadic cube of sidelength $2^{-n}$ is contained in exactly one “parent” of sidelength $2^{-n+1}$ (which, conversely, has $2^d$ “children” of sidelength $2^{-n}$), giving the dyadic cubes a structure analogous to that of a binary tree.&lt;/p&gt;

&lt;p&gt;As a consequence of these facts, we also obtain the &lt;strong&gt;dyadic nesting property&lt;/strong&gt;: given any two closed dyadic cubes (not necessarily same sidelength), then either they are almost disjoint, or one of them is contained in the other.&lt;/p&gt;

&lt;p&gt;If $E$ is open, and $x\in E$, then by definition there is an open ball centered at $x$ that is contained in $E$. Also, it is easily seen that there is also a closed dyadic cube containing $x$ that is contained in $E$. Hence, if we let $\mathcal{Q}$ be the collection of all the dyadic cubes $Q$ that are contained in $E$, we see that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}}Q
\end{equation}
Let $\mathcal{Q}^*$ denote cubes in $\mathcal{Q}$ such that they are not contained in any other cube in $\mathcal{Q}$. From the nesting property, we see that every cube in $\mathcal{Q}$ is contained in exactly one maximal cube in $\mathcal{Q}^*$, and that any two such maximal cubes in $\mathcal{Q}^*$ are almost disjoint. Thus, we have that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}^*}Q,
\end{equation}
which is union of almost disjoint cubes. As $\mathcal{Q}^*$ is at most countable, the claim follows.&lt;/p&gt;

&lt;h4 id=&quot;outer-msr-open-sets&quot;&gt;Outer measure of open sets&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Corollary 11&lt;/strong&gt;&lt;br /&gt;
The Lebesgue outer measure of any open set is equal to the Jordan inner measure of that set, or of the total volume of any partitioning of that set into almost disjoint boxes.&lt;/p&gt;

&lt;h4 id=&quot;outer-msr-arb-sets&quot;&gt;Outer measure of arbitrary sets - Outer regularity&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 12&lt;/strong&gt;.&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an arbitrary set. Then we have&lt;/em&gt;
\begin{equation}
m^*(E)=\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From monotonicity property, we have
\begin{equation}
m^*(E)\leq\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}
Then, it suffices to show that
\begin{equation}
m^*(E)\geq\inf_{E\subset U,U\text{ open}}m^*(U),
\end{equation}
which is obvious in the case that $m^*(E)$ is infinite. Thus, we now assume that $m^*(E)$ is finite.&lt;/p&gt;

&lt;p&gt;Let $\varepsilon&amp;gt;0$. By the definition of Lebesgue outer measure, there exists a countable family $B_1,B_2,\ldots$ of boxes covering $E$ such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We can enlarge each of these boxes $B_n$ to an open box $B_n’$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n},
\end{equation}
for any $\varepsilon&amp;gt;0$. Then the set $\bigcup_{n=1}^{\infty}B_n’$, being a union of open sets, is itself open, and contains $E$, and
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq m^*(E)+\varepsilon+\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=m^*(E)+2\varepsilon
\end{equation}
By countable subadditivity property, it implies that
\begin{equation}
m^*\left(\bigcup_{n=1}^{\infty}B_n’\right)\leq m^*(E)+2\varepsilon
\end{equation}
and thus
\begin{equation}
\inf_{E\subset U,U\text{ open}}m^*(U)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&amp;gt;0$ was arbitrary, the claim follows.&lt;/p&gt;

&lt;h3 id=&quot;lebesgue-measurability&quot;&gt;Lebesgue measurability&lt;/h3&gt;

&lt;h4 id=&quot;exist-lebesgue-msr-sets&quot;&gt;Existence of Lebesgue measurable sets&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 13&lt;/strong&gt;.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;Every open set is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;Every closed set is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;Every set of Lebesgue outer measure zero is measurable. (Such sets are called &lt;b&gt;null sets&lt;/b&gt;.)&lt;/li&gt;
	&lt;li&gt;The empty set $\emptyset$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;If $E\subset\mathbb{R}^d$ is Lebesgue measurable, then so its complement $\mathbb{R}^d\backslash E$.&lt;/li&gt;
	&lt;li&gt;If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the union $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the intersection $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;This follows from definition.&lt;/li&gt;
	&lt;li&gt;
		We have that every closed set is a the countable union of closed and bounded set, so by (vi), if suffices to verify the claim when $E$ is bounded and closed.&lt;br /&gt;
		Let $U\supset E$ be an open set, we thus have that $U\backslash E$ is also open due to the compactness of $E$. By &lt;b&gt;lemma 10&lt;/b&gt;, we can represent the open set $U\backslash E$ as a countable union of almost disjoint boxes as
		\begin{equation}
		U\backslash E=\bigcup_{n=1}^{\infty}B_n
		\end{equation}
		The problem remains to prove that for any $\varepsilon&amp;gt;0$
		\begin{equation}
		\sum_{n=1}^{\infty}\vert B_n\vert&amp;lt;\varepsilon
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;This follows from definition.&lt;/li&gt;
	&lt;li&gt;This follows from definition.&lt;/li&gt;
	&lt;li&gt;
		Given $E$ is Lebesgue measurable, for each positive integer $n$, we can find an open set $U_n$ containing $E$ such that
		\begin{equation}
		m^*(U_n\backslash E)\leq\frac{1}{n}
		\end{equation}
		Let $F_n=U_n^c=\mathbb{R}^d\backslash U_n$. Thus, we have $F_n\subset\mathbb{R}^d\backslash E$ and
		\begin{equation}
		m^*\big((\mathbb{R}^d\backslash E)\backslash F_n\big)=m^*\big((\mathbb{R}^d\backslash E)\backslash(\mathbb{R}^d\backslash U_n)\big)=m^*(U_n\backslash E)\leq\frac{1}{n}\label{eq:lemma13.1}
		\end{equation}
		In addition, since $F_n\subset\mathbb{R}^d\backslash E$, the countable union of them, denoted as $F$, is also a subset of $\mathbb{R}^d\backslash E$
		\begin{equation}
		F=\bigcup_{n=1}^{\infty}F_n\subset\mathbb{R}^d\backslash E
		\end{equation}
		Moreover, from \eqref{eq:lemma13.2}, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{N}F_n\right)=m^*\left(\bigcap_{n=1}^{N}(\mathbb{R}^d\backslash E)\backslash F_n\right)\leq\frac{1}{N}
		\end{equation}
		Let $N$ approaches $\infty$, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{\infty}F_n\right)\leq 0
		\end{equation}
		By non-negativity property, we then have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=0,
		\end{equation}
		Hence, $\mathbb{R}^d\backslash E$ is a union of $F$ with a set of Lebesgue outer measure of zero. The set $F$, in the other hand, is a countable union of closed set $F_n$&apos;s (since each $U_n$ is an open set). Therefore, by (ii), (iii) and (vi), we have that $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For each Lebesgue measurable set $E_n$, for any $\varepsilon&amp;gt;0$ and for $U_n$ is an open set containing $E_n$ we have 
		\begin{equation}
		m^{*}(U_n\backslash E_n)\leq\frac{\varepsilon}{2^n}\label{eq:lemma13.2}
		\end{equation}
		Moreover, since $E_n\subset U_n$, then
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{n=1}^{\infty}U_n,
		\end{equation}
		which is also an open set. Therefore, from \eqref{eq:lemma13.2} and by countable subadditivity, we have
		\begin{equation}
		m^*\left(\left(\bigcup_{n=1}^{\infty}U_n\right)\backslash\left(\bigcup_{n=1}^{\infty}E_n\right)\right)\leq\sum_{n=1}^{\infty}m^*(U_n\backslash E_n)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon,
		\end{equation}
		which proves that $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		Given $E_1,E_2,E_3,\ldots\subset\mathbb{R}^d$ are Lebesgue measurable, by (v), the complement of them,
		\begin{equation}
		E_1^c,E_2^c,E_3^c,\ldots\subset\mathbb{R}^d,
		\end{equation}
		are also Lebesgue measurable. By &lt;b&gt;De Morgan&apos;s laws&lt;/b&gt;, we have
		\begin{equation}
		\left(\bigcap_{n=1}^{\infty}E_n\right)^c=\bigcup_{n=1}^{\infty}E_n^c,
		\end{equation}
		which is Lebesgue measurable by (vi). Thus, $\left(\bigcap_{n=1}^{\infty}E_n\right)^c$ is also Lebesgue measurable. This means, using (v) once again, we obtain that $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;crt-msrb&quot;&gt;Criteria for measurability&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$, then the following are equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;$E$ is Lebesgue measurable.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Outer approximation by open&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, $E$ can be contained in an open set $U$ with $m^*(U\backslash E)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost open&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find an open set $U$ such that $m^*(U\Delta E)\leq\varepsilon$. ($E$ differs from an open set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Inner approximation by closed&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a closed set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost closed&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a closed set $F$ such that $m^*(F\Delta E)\leq\varepsilon$. ($E$ differs from a closed set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Almost measurable&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a Lebesgue measurable set $E_\varepsilon$ such that $m^*(E_\varepsilon\Delta E)\leq\varepsilon$. ($E$ differs from a measurable set by a set of outer measure at most $\varepsilon$.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (ii)&lt;br /&gt;
		This follows from definition
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (iii)&lt;br /&gt;
		Given $E$ is Lebesgue measurable, for any $\varepsilon&amp;gt;0$, we can find an open set $U$ containing $E$ such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		And since $E\subset U$, we have that
		\begin{equation}
		m^(E\backslash U)=m^*(\emptyset)=0,
		\end{equation}
		which implies that for any $\varepsilon&amp;gt;0$
		\begin{equation}
		m^*(U\Delta E)=m^*(U\backslash E)+m^*(E\backslash U)\leq\varepsilon
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (iv)&lt;br /&gt;
		By the claim (v) in &lt;b&gt;lemma 13&lt;/b&gt;, given Lebesgue measurable set $E\subset\mathbb{R}^d$, we have that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable. Therefore, there exists an open set $U$ containing $\mathbb{R}^d\backslash E$ such that for any $\varepsilon&amp;gt;0$ we have
		\begin{equation}
		m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.1}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}\backslash U$, thus $F$ is a closed set contained in $E$. Moreover, from \eqref{eq:cm.1} we also have for any $\varepsilon&amp;gt;0$
		\begin{equation}
		m^*(E\backslash F)=m^*\left(E\backslash(\mathbb{R}^d\backslash U)\right)=m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (v)&lt;br /&gt;
		Given Lebesgue measurable set $E\subset\mathbb{R}^d$, using the claim (v) in &lt;b&gt;lemma 13&lt;/b&gt; gives us that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.&lt;br /&gt;
		From claim (iii), for any $\varepsilon&amp;gt;0$, we can find an open set $U$ such that
		\begin{equation}
		m^*\left(U\Delta(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.2}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}^d\backslash$. We then have that $F$ is a closed set. In addition, $U\Delta(\mathbb{R}^d\backslash E)$ can be rewritten by
		\begin{align}
		U\Delta(\mathbb{R}^d\backslash E)&amp;amp;=\left(U\backslash(\mathbb{R}^d\backslash E)\right)\cup\left((\mathbb{R}^d\backslash E)\backslash U\right) \\ &amp;amp;=\left(E\backslash(\mathbb{R}^d\backslash U)\right)\cup\left((\mathbb{R}^d\backslash U)\backslash E\right) \\ &amp;amp;=(\mathbb{R}^d\backslash U)\backslash E \\ &amp;amp;=F\Delta E,
		\end{align}
		which lets \eqref{eq:cm.2} can be written as, for any $\varepsilon&amp;gt;0$
		\begin{equation}
		m^*(F\Delta E)\leq\varepsilon
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (vi)&lt;br /&gt;
		Given $E$ is Lebesgue measurable, by claim (v), for any $\varepsilon&amp;gt;0$ we can find a closed set $E_\varepsilon$ such that
		\begin{equation}
		m^*(E_\varepsilon\Delta E)\leq\varepsilon
		\end{equation}
		While by property (ii) of &lt;b&gt;lemma 13&lt;/b&gt;, we have that $E_\varepsilon$ is Lebesgue measurable, which proves our claim.
	&lt;/li&gt;
	&lt;li&gt;
		(vi) $\Rightarrow$ (i)&lt;br /&gt;
		Given (vi), for any $\varepsilon&amp;gt;0$, we can find a Lebesgue measurable set $E_\varepsilon^{(n)}$ such that
		\begin{equation}
		m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\frac{\varepsilon}{2^n}
		\end{equation}
		Therefore, by countable subadditivity property of Lebesgue outer measurability
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark 14&lt;/strong&gt;  &lt;br /&gt;
Every Jordan measurable set is Lebesgue measurable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
This follows directly from &lt;strong&gt;corollary 6&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 15&lt;/strong&gt;  &lt;br /&gt;
The &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#cantor-set&quot;&gt;&lt;strong&gt;Cantor set&lt;/strong&gt;&lt;/a&gt; is compact, uncountable, and a null set.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Since $\mathcal{C}\subseteq[0,1]$ is closed and bounded, by the &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#heine-borel-theorem&quot;&gt;Heine-Borel theorem&lt;/a&gt;, $\mathcal{C}$ is then compact.&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;msr-axiom&quot;&gt;The measure axioms&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Lemma 16&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;&lt;b&gt;Empty set&lt;/b&gt;. $m(\emptyset)=0$.&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Countable additivity&lt;/b&gt;. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then&lt;/li&gt;
	\begin{equation}
	m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
	\end{equation}
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Empty set&lt;/b&gt;&lt;br /&gt;
		We have that empty set $\emptyset$ is Lebesgue measurable since for every $\varepsilon&amp;gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $\emptyset$ such that $m^*(U\backslash\emptyset)\leq\varepsilon$. Thus,
		\begin{equation}
		m(\emptyset)=m^*(\emptyset)=0
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Countable additivity&lt;/b&gt;&lt;br /&gt;
		We begin by considering the case that $E_n$ are all compact sets.
		&lt;br /&gt;
		By repeated use of &lt;b&gt;Lemma 12&lt;/b&gt; and &lt;b&gt;Example ?&lt;/b&gt;, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Thus, using monotonicity property, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Let $N\to\infty$, we obtain
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
		On the other hand, by countable subadditivity, we also have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Therefore, we can conclude that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Next, we consider the case that $E_n$ are bounded but not necessarily compact.
		&lt;br /&gt;
		Let $\varepsilon&amp;gt;0$. By criteria for measurability, we know that each $E_n$ is the union of a compact set $K_n$ and a set of outer measure at most $\varepsilon/2^n$. Thus
		\begin{equation}
		m(E_n)\leq m(K_n)+\frac{\varepsilon}{2^n}
		\end{equation}
		And hence
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq\left(\sum_{n=1}^{\infty}m(K_n)\right)+\varepsilon
		\end{equation}
		From the first case, we know that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)=\sum_{n=1}^{\infty}m(K_n)
		\end{equation}
		while from monotonicity property of Lebesgue measure
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Putting these results together we obtain
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)+\varepsilon,
		\end{equation}
		for every $\varepsilon&amp;gt;0$. And since $\varepsilon$ was arbitrary, we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		while from countable subadditivity property we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\geq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Therefore, the claim follows.
		&lt;br /&gt;
		Finally, we consider the case that $E_n$ are not bounded or closed with the idea of decomposing each $E_n$ as a countable disjoint union of bounded Lebesgue measurable sets.
		&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark 17&lt;/strong&gt;&lt;br /&gt;
The countable additivity also implies the &lt;strong&gt;finite additivity&lt;/strong&gt; property of Lebesgue  measure
\begin{equation}
m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n),
\end{equation}
where $E_1,\ldots,E_N$ are Lebesgue measurable.&lt;/p&gt;

&lt;h4 id=&quot;mnt-cvg-theorem-msr-sets&quot;&gt;Monotone convergence theorem for measurable sets&lt;/h4&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Upward monotone convergence&lt;/b&gt;. Let $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ be a countable non-decreasing sequence of Lebesgue measurable sets. Then
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Downward monotone convergence&lt;/b&gt;. Let $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ be a countable non-increasing sequence of Lebesgue measurable sets. If at least one of the $m(E_n)$ is finite, then
		\begin{equation}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		The hypothesis that at least one of the $m(E_n)$ is finite in the downward monotone convergence theorem cannot be dropped.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Upward monotone convergence&lt;/b&gt;&lt;br /&gt;
		Since $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ is a countable non-decreasing sequence of Lebesgue measurable sets, by countable additivity, we have
		\begin{align}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash\bigcup_{n&apos;=1}^{n-1}E_{n&apos;}\right) \\ &amp;amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash E_{n-1}\right) \\ &amp;amp;=\left(\sum_{n=2}^{\infty}m(E_n)-m(E_{n-1})\right)+m(E_1) \\ &amp;amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Downward monotone convergence&lt;/b&gt;&lt;br /&gt;
		Since $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ is a countable non-increasing sequence of Lebesgue measurable sets, the sequence of their complement $E_1^c\subset E_2^c\subset\ldots\subset\mathbb{R}^d$ is therefore a countable non-decreasing sequence of Lebesgue measurable sets. Using the claim (i) and by De Morgan&apos;s laws, we have
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;amp;=m(\mathbb{R}^d)-m\left(\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;amp;=m(\mathbb{R}^d)-\lim_{n\to\infty}m(E_n^c) \\ &amp;amp;=m(\mathbb{R}^d)-m(\mathbb{R}^d)+\lim_{n\to\infty}m(E_n) \\ &amp;amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		Consider sequence $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ of non-increasing Lebesgue measurable sets where each $E_n$ is given by
		\begin{equation}
		E_n\doteq[n,+\infty)
		\end{equation}
		Therefore, by De Morgan&apos;s laws, the Lebesgue measure of their countable intersection is
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}(-\infty,n)\right) \\ &amp;amp;=m(\mathbb{R}^d\backslash\mathbb{R}^d) \\ &amp;amp;=m(\emptyset)=0,
		\end{align}
		while for every $n$, we have
		\begin{equation}
		m(E_n)=m\left([n,+\infty)\right)=\infty
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;dmnt-cvg-theorem-msr-sets&quot;&gt;Dominated convergence theorem for measurable sets&lt;/h4&gt;
&lt;p&gt;We say that a sequence $E_n$ of sets in $\mathbb{R}^d$ &lt;strong&gt;converges pointwise&lt;/strong&gt; to another set $E$ in $\mathbb{R}^d$ if the indicator function $1_{E_n}$ converges pointwise to $1_E$.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		If the $E_n$ are all Lebesgue measurable, and converge pointwise to $E$, then $E$ is Lebesgue measurable also.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Dominated convergence theorem&lt;/b&gt;. Suppose that the $E_n$ are all contained in another Lebesgue measurable set $F$ of finite measure. Then $m(E_n)$ converges to $m(E)$.
	&lt;/li&gt;
	&lt;li&gt;
		The dominated convergence theorem fails if the $E_n$&apos;s are not contained in a set of finite measure, even if we assume that the $m(E_n)$ are all uniformly bounded.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		We have
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark 18&lt;/strong&gt;&lt;br /&gt;
Let $E\subset\mathbb{R}^d$, then $E$ is contained in a Lebesgue measurable set of measure exactly equal to $m^*(E)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;inn-rglr&quot;&gt;Inner regularity&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be Lebesgue measurable. Then
\begin{equation}
m(E)=\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
By monotonic we have that
\begin{equation}
m(E)\geq\sup_{K\subset E,K\text{ compact}}m(K),
\end{equation}
thus it suffices to show that
\begin{equation}
m(E)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Consider the case that $E$ is bounded. By the &lt;strong&gt;criteria for Lebesgue measurability&lt;/strong&gt;, we have that for any $\varepsilon&amp;gt;0$, there exist a bounded and closed, and thus compact by the Heine-Borel theorem, set $K’$ contained in $E$ such that
\begin{equation}
m(E\backslash K’)\leq\varepsilon
\end{equation}
Moreover, by claim (ii) of &lt;strong&gt;lemma 13&lt;/strong&gt;, we have that $K’$ is Lebesgue measurable. Using finite additivity property of Lebesgue measure gives us
\begin{equation}
\varepsilon\geq m(E\backslash K’)=m(E)-m(K’),
\end{equation}
which means
\begin{equation}
m(E)\leq m(K’)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Now consider {the case that $E$ is an unbounded set. Let $(K_r)_{r=1,2,\ldots}$ be the sequence sets in which each $K_r$ is defined as
\begin{equation}
K_r\doteq E\cap B_r(\mathbf{0}),\label{eq:ir.1}
\end{equation}
where $B_r(\mathbf{0})$ is a closed ball centered at $\mathbf{0}\in\mathbb{R}^d$ with radius $r$
\begin{equation}
B_r(\mathbf{0})=\{\mathbf{x}:\vert\mathbf{x}\vert\leq r\}
\end{equation}
which means $K_1\subset K_2\subset\ldots\subset E$ is an increasing sequence of compact set (since \eqref{eq:ir.1} also implies that $K_r\subset B_r(\mathbf{0})$, and hence bounded and closed, then using the Heine-Borel theorem to obtain the compactness of $K_r$). By the &lt;strong&gt;monotone convergence theorem&lt;/strong&gt;, we have
\begin{equation}
m\left(\bigcup_{r=1}^{\infty}K_r\right)=\lim_{r\to\infty}m(K_r)
\end{equation}
On the other hand, the countable union of $K_r$ can be written as
\begin{equation}
\bigcup_{r=1}^{\infty}K_r=\bigcup_{r=1}^{\infty}E\cap B_r(\mathbf{0})=E\cap\bigcup_{r=1}^{\infty}B_r(\mathbf{0})=E\cap\mathbb{R}^d=E,
\end{equation}
which therefore gives us
\begin{equation}
m(E)=\lim_{r\to\infty}m(K_r)\label{eq:ir.2}
\end{equation}
Moreover, by monotonicity property $m(E)\geq m(K_r),\forall r$. Hence, \eqref{eq:ir.2} implies that for any $\varepsilon&amp;gt;0$, there exists $r’$ such that for all $r\geq r’$
\begin{equation}
\varepsilon&amp;gt;\vert m(E)-m(K_{r’})\vert=m(E)-m(K_{r’})
\end{equation}
This means that
\begin{equation}
m(A)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Our claim then follows.&lt;/p&gt;

&lt;h4 id=&quot;crt-fnt-msr&quot;&gt;Criteria for finite measure&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$, then the following are equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		$E$ is Lebesgue measurable with finite measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Outer approximation by open&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can contain $E$ in an open set $U$ of finite measure with $m^*(U\backslash E)\leq\varepsilon$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost open bounded&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, there exists a bounded open set $U$ such that $m^*(E\Delta U)\leq\varepsilon$. (In other words, $E$ differs from a bounded set by a set of arbitrarily small Lebesgue outer measure.)
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Inner approximation by compact&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, we can find a compact set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost compact&lt;/b&gt;. $E$ differs from a compact set by a set of arbitrarily small Lebesgue outer measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost bounded measurable&lt;/b&gt;. $E$ differs from a bounded Lebesgue measurable set by a set of arbitrarily small Lebesgue outer measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost finite measure&lt;/b&gt;. $E$ differs from a Lebesgue measurable set with finite measure by a set of arbitrarily small Lebesgue outer measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost elementary&lt;/b&gt;. $E$ differs from an elementary set by a set of arbitrarily small Lebesgue outer measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Almost dyadically elementary&lt;/b&gt;. For every $\varepsilon&amp;gt;0$, there exists an integer $n$ and a finite union $F$ of closed dyadic cubes of sidelength $2^{-n}$ such that $m^*(E\Delta F)\leq\varepsilon$.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (ii)&lt;br /&gt;
		Given $E$ is Lebesgue measurable with finite measure, by definition, for any $\varepsilon&amp;gt;0$, there exists an open set $U$ o such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		Then, by finite subadditivity property of Lebesgue outer measure
		\begin{equation}
		m^*(U)\leq m^*(E)+\varepsilon,
		\end{equation}
		which implies that $m^*(U)$ finite due to finiteness of $m^*(E)$ and $\varepsilon$, and hence $U$ has finite measure since $m(U)\leq m^*(U)$.
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (iii)&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;caratheodory-crt&quot;&gt;Carathéodory criterion, one direction&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$, the following are then equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		$E$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		For every elementary set $A$
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		For every box $B$, we have
		\begin{equation}
		\vert B\vert=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (ii)&lt;br /&gt;
		We begin with an observation that, by finite additivity property of Lebesgue measure
		\begin{equation}
		m(A)=m(A\cap E)+m(A\backslash E)\leq m^*(A\cap E)+m^*(A\backslash E)\label{eq:cc.1}
		\end{equation}
		Given $A$ is elementary, by &lt;span&gt;&lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#measure-elementary-set&quot;&gt;&lt;b&gt;lemma 10&lt;/b&gt;&lt;/a&gt;&lt;/span&gt;, we can express $A$ as a finite union of disjoint boxes
		\begin{equation}
		A=\bigcup_{n=1}^{N}B_n
		\end{equation}
		Continuing using finite subadditivity of Lebesgue outer measure and finite additivity of Lebesgue measure, \eqref{eq:cc.1} then can be continued to derive as
		\begin{align}
		m(A)&amp;amp;\leq m^*(A\cap E)+m^*(A\backslash E) \\ &amp;amp;=m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\cap E\right)+m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\backslash E\right) \\ &amp;amp;=m^*\left(\bigcup_{n=1}^{N}B_n\cap E\right)+m^*\left(\bigcup_{n=1}^{N}B_n\backslash E\right) \\ &amp;amp;\leq\sum_{n=1}^{N}m^*(B_n\cap E)+m^*(B_n\backslash E) \\ &amp;amp;=\sum_{n=1}^{N}m^*(B_n)=\sum_{n=1}^{N}m(B_n)=m\left(\bigcup_{n=1}^{N}B_n\right)=m(A),
		\end{align}
		which implies that
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		(i) $\Rightarrow$ (iii)&lt;br /&gt;
		Since every box $B$ is Lebesgue measurable, then given $E$ is also Lebesgue measurable, by &lt;b&gt;lemma 13&lt;/b&gt;, their difference and intersection are also Lebesgue measurable, which means by additivity property of Lebesgue measure we have
		\begin{equation}
		\vert B\vert=m(B)=m(B\cap E)+m(B\backslash E)=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		(ii) $\Rightarrow$ (i)&lt;br /&gt;
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;inn-msr&quot;&gt;Inner measure&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be a bounded set. The &lt;strong&gt;Lebesgue inner measure&lt;/strong&gt; $m_*(E)$ of $E$ is defined by
\begin{equation}
m_*(E)\doteq m(A)-m^*(A\backslash E),
\end{equation}
for any elementary set $A$ containing $E$. Then&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		If $A,A&apos;$ are two elementary sets containing $E$, then
		\begin{equation}
		m(A)-m^*(A\backslash E)=m(A&apos;)-m^*(A&apos;\backslash E)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		We have that $m_*(E)\leq m^*(E)$, and that equality holds iff $E$ is Lebesgue measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 3&lt;/strong&gt;&lt;br /&gt;
Let $E\subset \mathbb{R}^d$, and define a $G_\delta$ &lt;em&gt;set&lt;/em&gt; to be a countable intersection $\bigcap_{n=1}^{\infty}U_n$ of open sets, and define an $F_\delta$ &lt;em&gt;set&lt;/em&gt; to be a countable union $\bigcup_{n=1}^{\infty}F_n$ of closed sets. The following are then equivalent:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		$E$ is Lebesgue measurable.
	&lt;/li&gt;
	&lt;li&gt;
		$E$ is a $G_\delta$ set with a null set removed.
	&lt;/li&gt;
	&lt;li&gt;
		$E$ is the union of an $F_\delta$ set and a null set.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;trans-inv&quot;&gt;Translation invariance&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, then $E+x$ is also Lebesgue measurable for any $x\in\mathbb{R}^d$, and $m(E+x)=m(E)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;change-vars&quot;&gt;Change of variables&lt;/h4&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, and $T:\mathbb{R}^d\to\mathbb{R}^d$ be a linear transformation, then $T(E)$ is Lebesgue measurable, and $m(T(E))=\vert\text{det}(T)\vert m(E)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;&lt;br /&gt;
If $T:\mathbb{R}^d\to\mathbb{R}^{d’}$ is a linear map to a space $\mathbb{R}^{d’}$ of strictly smaller dimension than $\mathbb{R}^d$, then $T(E)$ need not be Lebesgue measurable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 19&lt;/strong&gt;&lt;br /&gt;
Let $d,d’\geq 1$ be natural numbers&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		If $E\subset\mathbb{R}^d$ and $F\subset\mathbb{R}^{d&apos;}$, then
		\begin{equation}
		(m^{d+d&apos;})^*(E\times F)\leq(m^d)^*(E)(m^{d&apos;})^*(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		Let $E\subset\mathbb{R}^d,F\subset\mathbb{R}^{d&apos;}$ be Lebesgue measurable sets. Then $E\times F\subset\mathbb{R}^{d+d&apos;}$ is Lebesgue measurable, with \begin{equation}
		m^{d+d&apos;}(E\times F)=m^d(E).m^{d&apos;}(F)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;uniq-lebesgue-msr&quot;&gt;Uniqueness of Lebesgue measure&lt;/h4&gt;
&lt;p&gt;Lebesgue measure $E\mapsto m(E)$ is the only map from Lebesgue measurable sets to $[0,+\infty]$ that obeys the following axioms:&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Empty set&lt;/b&gt;. $m(\emptyset)=0$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Countable additivity&lt;/b&gt;. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then 
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Translation invariance&lt;/b&gt;. If $E$ is Lebesgue measurable and $x\in\mathbb{R}^d$, then $m(E+x)=m(E)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Normalisation&lt;/b&gt;. $m([0,1]^d)=1$.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;non-measurable-sets&quot;&gt;Non-measurable sets&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Remark 20&lt;/strong&gt;&lt;br /&gt;
There exists a subset $E\subset[0,1]$ which is not Lebesgue measurable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 21&lt;/strong&gt; (Outer measure is not finitely additive)&lt;br /&gt;
There exists disjoint bounded subsets $E,F\subset\mathbb{R}$ such that
\begin{equation}
m^*(E\cap F)\neq m^*(E)+m^*(F)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 22&lt;/strong&gt;&lt;br /&gt;
Let $\pi:\mathbb{R}^2\to\mathbb{R}$ be the coordinate projection $\pi(x,y)\doteq x$. Then there exists a measurable $E\subset\mathbb{R}^2$ such that $\pi(E)$ is not measurable.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;taos-book&quot;&gt;Terence Tao. &lt;a href=&quot;https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/&quot;&gt;An introduction to measure theory&lt;/a&gt;. Graduate Studies in Mathematics, vol. 126.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;span id=&quot;steins-book&quot;&gt;Elias M. Stein &amp;amp; Rami Shakarchi. &lt;a href=&quot;#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf&quot;&gt;Real Analysis: Measure Theory, Integration, and Hilbert Spaces&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The &lt;strong&gt;diameter&lt;/strong&gt; of a set $B$ is defined as
\begin{equation*}
\text{dia}(B)\doteq\sup\{\vert x-y\vert:x,y\in B\}
\end{equation*} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-measure" /><summary type="html">Part II of the measure theory series. Materials are mostly taken from Tao’s book, except for some needed notations extracted from Stein’s book.</summary></entry><entry><title type="html">Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral</title><link href="http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory-p1.html" rel="alternate" type="text/html" title="Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral" /><published>2022-06-16T13:00:00+07:00</published><updated>2022-06-16T13:00:00+07:00</updated><id>http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory-p1</id><content type="html" xml:base="http://localhost:4000/mathematics/measure-theory/2022/06/16/measure-theory-p1.html">&lt;blockquote&gt;
  &lt;p&gt;Part I of the measure theory series. Materials are mostly taken from &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#taos-book&quot;&gt;Tao’s book&lt;/a&gt;, except for some needed notations extracted from &lt;a href=&quot;/mathematics/measure-theory/2022/06/16/measure-theory-p1.html#steins-book&quot;&gt;Stein’s book&lt;/a&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#preliminaries&quot;&gt;Preliminaries&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#pts-sets&quot;&gt;Points, sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#open-closed-compact-sets&quot;&gt;Open, closed, compact sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rects-cubes&quot;&gt;Rectangles, cubes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cantor-set&quot;&gt;The Cantor set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#others&quot;&gt;Others&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#elementary-measure&quot;&gt;Elementary measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#intervals-boxes-elementary-sets&quot;&gt;Intervals, boxes, elementary sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#measure-elementary-set&quot;&gt;Measure of an elementary set&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#elementary-measure-properties&quot;&gt;Properties of elementary measure&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#uniqueness-elementary-measure&quot;&gt;Uniqueness of elementary measure&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#jordan-measure&quot;&gt;Jordan measure&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#jordan-measurability-characterisation&quot;&gt;Characterisation of Jordan measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#jordan-measurability-properties&quot;&gt;Properties of Jordan measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#jordan-null-sets&quot;&gt;Jordan null sets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#metric-formula-jordan-measurability&quot;&gt;Metric entropy formulation of Jordan measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#uniqueness-jordan-measure&quot;&gt;Uniqueness of Jordan measure&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#topo-jordan-measurability&quot;&gt;Topological of Jordan measurability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#caratheodory-type-property&quot;&gt;Carathéodory type property&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#connect-riemann-int&quot;&gt;Connection with the Riemann integral&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-integrability&quot;&gt;Riemann integrability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pc-func&quot;&gt;Piecewise constant functions&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#pc-int-properties&quot;&gt;Basic properties of piecewise constant integral&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#darboux-int&quot;&gt;Darboux integral&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#equiv-riemann-darboux-int&quot;&gt;Equivalence of Riemann integral and Darboux integral&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-int-properties&quot;&gt;Basic properties of the Riemann integral&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#riemann-int-area-interpret&quot;&gt;Area interpretation of the Riemann integral&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;preliminaries&quot;&gt;Preliminaries&lt;/h2&gt;

&lt;h3 id=&quot;pts-sets&quot;&gt;Points, sets&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;point&lt;/strong&gt; $x\in\mathbb{R}^d$ consists of a $d$-tuple of real numbers
\begin{equation}
x=\left(x_1,x_2,\dots,x_d\right),\hspace{1cm}x_i\in\mathbb{R}, i=1,\dots,d
\end{equation}
Addition between points and multiplication of a point by a real scalar is elementwise.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;norm&lt;/strong&gt; of $x$ is denoted by $\vert x\vert$ and is defined to be the standard &lt;strong&gt;Euclidean norm&lt;/strong&gt; given by
\begin{equation}
\vert x\vert=\left(x_1^2+\dots+x_d^2\right)^{1/2}
\end{equation}
We can then calculate the &lt;strong&gt;distance&lt;/strong&gt; between two points $x$ and $y$, which is
\begin{equation}
\text{dist}(x,y)=\vert x-y\vert
\end{equation}
The &lt;strong&gt;complement&lt;/strong&gt; of a set $E$ in $\mathbb{R}^d$ is denoted as $E^c$, and defined by
\begin{equation}
E^c=\{x\in\mathbb{R}^d:x\notin E\}
\end{equation}
If $E$ and $F$ are two subsets of $\mathbb{R}^d$, we denote the complement of $F$ in $E$ by
\begin{equation}
E-F=\{x\in\mathbb{R}^d:x\in E;\,x\notin F\}
\end{equation}
The &lt;strong&gt;distance&lt;/strong&gt; between two sets $E$ and $F$ is defined by
\begin{equation}
\text{dist}(E,F)=\inf_{x\in E,\,y\in F}\vert x-y\vert
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;open-closed-compact-sets&quot;&gt;Open, closed and compact sets&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;open ball&lt;/strong&gt; in $\mathbb{R}^d$ centered at $x$ and of radius $r$ is defined by
\begin{equation}
B_r(x)=\{y\in\mathbb{R}^d:\vert y-x\vert&amp;lt; r\}
\end{equation}
A subset $E\subset\mathbb{R}^d$ is &lt;strong&gt;open&lt;/strong&gt; if for every $x\in E$ there exists $r&amp;gt;0$ with $B_r(x)\subset E$. And a set is &lt;strong&gt;closed&lt;/strong&gt; if its complement is open.&lt;br /&gt;
Any (not necessarily countable) union of open sets is open, while in general, the intersection of only finitely many open sets is open. A similar statement holds for the class of closed sets, if we interchange the roles of unions and intersections.&lt;/p&gt;

&lt;p&gt;A set $E$ is &lt;strong&gt;bounded&lt;/strong&gt; if it is contained in some ball of finite radius. A set is &lt;strong&gt;compact&lt;/strong&gt; if it is bounded and is also closed. Compact sets enjoy the &lt;strong&gt;Heine-Borel&lt;/strong&gt; covering property:&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;heine-borel-theorem&quot;&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;. (&lt;strong&gt;Heine-Borel theorem&lt;/strong&gt;)&lt;/span&gt;&lt;br /&gt;
&lt;em&gt;Assume $E$ is compact, $E\subset\bigcup_\alpha\mathcal{O}_\alpha$, and each $\mathcal{O}_\alpha$ is open. Then there are finitely many of the open sets $\mathcal{O}_{\alpha_1},\mathcal{O}_{\alpha_2},\dots,\mathcal{O}_{\alpha_N}$, such that $E\subset\bigcup_{j=1}^{N}\mathcal{O}_{\alpha_j}$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In words, &lt;em&gt;any&lt;/em&gt; covering of a compact set by a collection of open sets contains a &lt;em&gt;finite&lt;/em&gt; subcovering.&lt;/p&gt;

&lt;p&gt;A point $x\in\mathbb{R}^d$ is a &lt;strong&gt;limit point&lt;/strong&gt; of the set $E$ if for every $r&amp;gt;0$, the ball $B_r(x)$ contains points of $E$. This means that there are points in $E$ which are arbitrarily close to $x$. An &lt;strong&gt;isolated point&lt;/strong&gt; of $E$ is a point $x\in E$ such that there exists an $r&amp;gt;0$ where $B_r(x)\cap E=\{x\}$.&lt;/p&gt;

&lt;p&gt;A point $x\in E$ is an &lt;strong&gt;interior point&lt;/strong&gt; of $E$ if there exists $r&amp;gt;0$ such that $B_r(x)\subset E$. The set of all interior points of $E$ is called the &lt;strong&gt;interior&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;closure&lt;/strong&gt; of $E$, denoted as $\bar{E}$, consists the union of $E$ and all its limit points. The &lt;strong&gt;boundary&lt;/strong&gt; of $E$, denoted as $\partial E$, is the set of points which are in the closure of $E$ but not in the interior of $E$.&lt;/p&gt;

&lt;p&gt;A closed set $E$ is &lt;strong&gt;perfect&lt;/strong&gt; if $E$ does not have any isolated point.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;boundary&lt;/strong&gt; of $E$, denoted by $\partial E$, is the set of points in the closure of $E$ not belonging to the interior of $E$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The closure of a set is a closed set.&lt;/li&gt;
  &lt;li&gt;Every point in $E$ is a limit point of $E$.&lt;/li&gt;
  &lt;li&gt;A set is closed iff it contains all its limit points.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rects-cubes&quot;&gt;Rectangles, cubes&lt;/h3&gt;
&lt;p&gt;A (closed) &lt;strong&gt;rectangle&lt;/strong&gt; $R$ in $\mathbb{R}^d$ is given by the product of $d$ one-dimensional closed and bounded intervals
\begin{equation}
R\doteq[a_1,b_1]\times[a_2,b_2]\times\ldots\times[a_d,b_d],
\end{equation}
where $a_j\leq b_j$, for $j=1,\ldots,d$, are real numbers. In other words, we have
\begin{equation}
R=\left\{\left(x_1,\ldots,x_d\right)\in\mathbb{R}^d:a_j\leq x_j\leq b_j,\forall j=1,\ldots,d\right\}
\end{equation}
With this definition, a rectangle is closed and has sides parallel to the coordinate axis. In $\mathbb{R}$, the rectangles are the closed and bounded intervals; they becomes the usual rectangles as we usually see in $\mathbb{R}^2$; while in $\mathbb{R}^3$, they are the closed parallelepipeds.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-06-16/rectangles.png&quot; alt=&quot;Rectangles in R^d&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Rectangles in $\mathbb{R}^d,d=1,2,3$&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The lengths of the sides of the rectangle $R$ in $\mathbb{R}^d$ are $b_1-a_1,\ldots,b_d-a_d$. The &lt;strong&gt;volume&lt;/strong&gt; of its, denoted as $\vert R\vert$, is defined as
\begin{equation}
\vert R\vert\doteq(b_1-a_1)\dots(b_d-a_d)
\end{equation}
An open rectangle is the product of open intervals, and the interior of the rectangle $R$ is then
\begin{equation}
(a_1,b_1)\times\ldots\times(a_d,b_d)
\end{equation}
A &lt;strong&gt;cube&lt;/strong&gt; is a rectangle for which $b_1-a_1=\ldots=b_d-a_d$.&lt;/p&gt;

&lt;p&gt;A union of rectangles is said to be &lt;strong&gt;almost disjoint&lt;/strong&gt; if the interiors of the rectangles are disjoint.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a rectangle is the almost disjoint union of finitely many other rectangles, say $R=\bigcup_{k=1}^{N}R_k$, then&lt;/em&gt;
\begin{equation}
\vert R\vert=\sum_{k=1}^{N}\vert R_k\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 3&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $R,R_1,\ldots,R_N$ are rectangles, and $R\subset\bigcup_{k=1}^{N}R_k$, then&lt;/em&gt;
\begin{equation}
\vert R\vert\leq\sum_{k=1}^{N}\vert R_k\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 4&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Every open $\mathcal{O}\subset\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 5&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Every open $\mathcal{O}\subset\mathbb{R}^d,d\geq 1$, can be written as countable union of almost disjoint closed cubes&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cantor-set&quot;&gt;The Cantor set&lt;/h3&gt;
&lt;p&gt;Let $C_0=[0,1]$ denote the closed unit interval and let $C_1$ represent the set obtained from deleting the middle third open interval from $[0,1]$, as
\begin{equation}
C_1=[0,1/3]\cup[2/3,1]
\end{equation}
We repeat this procedure of deleting the middle third open interval for each subinterval of $C_1$. In the second stage we obtain
\begin{equation}
C_1=[0,1/9]\cup[2/9,1/3]\cup[2/3,7/9]\cup[8/9,1]
\end{equation}
We continue to repeat this process for each subinterval of $C_2$, and so on. The result of this process is a sequence $(C_k)_{k=0,1,\ldots}$ of compact sets with
\begin{equation}
C_0\supset C_1\supset C_2\supset\ldots\supset C_k\supset C_{k+1}\supset\ldots
\end{equation}
The &lt;strong&gt;Cantor set&lt;/strong&gt; $\mathcal{C}$ is defined as the intersection of all $C_k$’s
\begin{equation}
\mathcal{C}=\bigcap_{k=0}^{\infty}C_k
\end{equation}
The set $\mathcal{C}$ is not empty, since all end-points of the intervals in $C_k$ (all $k$) belong to $\mathcal{C}$.&lt;/p&gt;

&lt;h3 id=&quot;others&quot;&gt;Others&lt;/h3&gt;
&lt;p&gt;Given any sequence $x_1,x_2,\ldots\in[0,+\infty]$. We can always form the sum
\begin{equation}
\sum_{n=1}^{n}x_n\in[0,+\infty]
\end{equation}
as the limit of the partial sums $\sum_{n=1}^{N}x_n$, which may be either finite or infinite. An equivalence definition of this infinite sum is as the supremum of all finite subsums:
\begin{equation}
\sum_{n=1}^{\infty}x_n=\sup_{F\subset\mathbb{N},F\text{ finite}}\sum_{n\in F}x_n
\end{equation}
From this equation, given any collection $(x_\alpha)_{\alpha\in A}$ of numbers $x_\alpha\in[0,+\infty]$ indexed by an arbitrary set $A$, we can define the sum $\sum_{\alpha\in A}x_\alpha$ as
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sup_{F\subset A,F\text{ finite}}\sum_{\alpha\in F}x_\alpha\label{eq:others.1}
\end{equation}
Or moreover, given any bijection $\phi:B\to A$, we has the change of variables formula
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sum_{\beta\in B}x_{\phi(\beta)}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;tonelli-theorem&quot;&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt;. (&lt;strong&gt;Tonelli’s theorem for series&lt;/strong&gt;)&lt;/span&gt;&lt;br /&gt;
&lt;em&gt;Let $(x_{n,m})_{n,m\in\mathbb{N}}$ be a doubly infinite sequence of extended nonnegative reals $x_{n,m}\in[0,+\infty]$. Then&lt;/em&gt;
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}x_{n,m}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We will prove the equality between the first and second expression, the proof for the equality between the first and the third one is similar.&lt;/p&gt;

&lt;p&gt;We begin by showing that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Let $F\subset\mathbb{N}^2$ be any finite set. Then $F\subset\{1,\ldots,N\}\times\{1,\ldots,N\}$ for some finite $N$. Since $x_{n,m}$ are nonnegative, we have
\begin{align}
\sum_{(n,m)\in F}x_{n,m}&amp;amp;\leq\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,\ldots,N\}}x_{n,m} \\ &amp;amp;=\sum_{n=1}^{N}\sum_{m=1}^{N}x_{n,m} \\ &amp;amp;\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{align}
for any finite subset $F$ of $\mathbb{R}^2$. Then by \eqref{eq:others.1}, we have
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sup_{F\subset\mathbb{N}^2,F\text{ finite}}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
The problem now remains to prove that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{equation}
which will be proved if we can show that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Fix $N$, we have since each $\sum_{m=1}^{\infty}$ is the limit of $\sum_{m=1}^{M}x_{n,m}$, LHS is the limit of $\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}$ as $M\to\infty$. Thus, it suffices to show that for each finite $M$
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}=\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,ldots,M\}}x_{n,m}
\end{equation}
which is true for all finite $M,N$. And it concludes our proof.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Axiom 7&lt;/strong&gt;. (&lt;strong&gt;Axiom of choice&lt;/strong&gt;)&lt;br /&gt;
&lt;em&gt;Let $(E_\alpha)_{\alpha\in A}$ be a family of non-empty set $E_\alpha$, indexed by an index set $A$. Then we can find a family $(x_\alpha)_{\alpha\in A}$ of elements $x_\alpha$ of $E_\alpha$, indexed by the same set $A$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;span id=&quot;countable-choice-axiom&quot;&gt;&lt;strong&gt;Corollary 8&lt;/strong&gt;. (&lt;strong&gt;Axiom of countable choice&lt;/strong&gt;)&lt;/span&gt;&lt;br /&gt;
&lt;em&gt;Let $E_1,E_2,\ldots$ be a sequence of non-empty sets. Then we can find a sequence $x_1,x_2,\ldots$ such that $x_n\in E_n,\forall n=1,2,\ldots$.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;elementary-measure&quot;&gt;Elementary measure&lt;/h2&gt;

&lt;h3 id=&quot;intervals-boxes-elementary-sets&quot;&gt;Intervals, boxes, elementary sets&lt;/h3&gt;
&lt;p&gt;An &lt;strong&gt;interval&lt;/strong&gt; is a subset of $\mathbb{R}$ having one of the forms
\begin{align}
[a,b]&amp;amp;\doteq\{x\in\mathbb{R}:a\leq x\leq b\}, \\ [a,b)&amp;amp;\doteq\{x\in\mathbb{R}:a\leq x\lt b\}, \\ (a,b]&amp;amp;\doteq\{x\in\mathbb{R}:a\lt x\leq b\}, \\ (a,b)&amp;amp;\doteq\{x\in\mathbb{R}:a\lt x\lt b\},
\end{align}
where $a\leq b$ are real numbers.&lt;br /&gt;
The &lt;strong&gt;length&lt;/strong&gt; of an interval $I=[a,b],[a,b),(a,b],(a,b)$ is denoted as $\vert I\vert$ and is defined by
\begin{equation}
\vert I\vert\doteq b-a
\end{equation}
A &lt;strong&gt;box&lt;/strong&gt; in $\mathbb{R}^d$ is a Cartesian product $B\doteq I_1\times\ldots\times I_d$ of $d$ intervals $I_1,\ldots,I_d$ (not necessarily the same length). The &lt;strong&gt;volume&lt;/strong&gt; $\vert B\vert$ of such a box $B$ is defined as
\begin{equation}
\vert B\vert\doteq \vert I_1\vert\times\ldots\times\vert I_d\vert
\end{equation}
An &lt;strong&gt;elementary set&lt;/strong&gt; is any subset of $\mathbb{R}^d$ which is the union of a finite number of boxes.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 9&lt;/strong&gt; (&lt;strong&gt;Boolean closure&lt;/strong&gt;)&lt;br /&gt;
If $E,F\subset\mathbb{R}^d$ are elementary sets, then&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the union $E\cup F$,&lt;/li&gt;
  &lt;li&gt;the intersection $E\cap F$,&lt;/li&gt;
  &lt;li&gt;the set theoretic difference $E\backslash F\doteq\{x\in E:x\notin F\}$,&lt;/li&gt;
  &lt;li&gt;the symmetric difference $E\Delta F\doteq(E\backslash F)\cup(F\backslash E)$ 
are also elementary,&lt;/li&gt;
  &lt;li&gt;if $x\in\mathbb{R}^d$, then the translate $E+x\doteq\{y+x:y\in E\}$ is also an elementary set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
With their definitions as elementary sets, we can assume that
\begin{align}
E&amp;amp;=B_1\cup\ldots\cup B_k, \\ F&amp;amp;=B_1’\cup\ldots\cup B_{k’}’,
\end{align}
where each $B_i$ and $B_i’$ is a $d$-dimensional box. By set theory, we have that&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The union of $E$ and $F$ can be written as
\begin{equation}
E\cup F=B_1\cup\ldots\cup B_k\cup B_1’\cup\ldots\cup B_{k’}’,
\end{equation}
which is an elementary set.&lt;/li&gt;
  &lt;li&gt;The intersection of $E$ and $F$ can be written as
\begin{align}
E\cap F&amp;amp;=\left(B_1\cup\ldots\cup B_k\right)\cup\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\cap B_j’\right),
\end{align}
which is also an elementary set.&lt;/li&gt;
  &lt;li&gt;The set theoretic difference of $E$ and $F$ can be written as
\begin{align}
E\backslash F&amp;amp;=\left(B_1\cup\ldots\cup B_k\right)\backslash\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right),
\end{align}
which is, once again, an elementary set.&lt;/li&gt;
  &lt;li&gt;With this display, the symmetric difference of $E$ and $F$ can be written as
\begin{align}
E\Delta F&amp;amp;=\left(E\backslash F\right)\cup\left(F\backslash E\right) \\ &amp;amp;=\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right)\Bigg]\cup\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_j’\backslash B_i\right)\Bigg],
\end{align}
which satisfies conditions of an elementary set.&lt;/li&gt;
  &lt;li&gt;Since $B_i$’s are $d$-dimensional boxes, we can express them as
\begin{equation}
B_i=I_{i,1}\times\ldots I_{i,d},
\end{equation}
where each $I_{i,j}$ is an interval in $\mathbb{R}^d$. Without loss of generality, we assume that they are all closed. In particular, for $j=1,\ldots,d$
\begin{equation}
I_{i,j}=(a_{i,j},b_{i,j})
\end{equation}
Thus, for any $x\in\mathbb{R}^d$, we have that
\begin{align}
E+x&amp;amp;=\left\{y+x:y\in E\right\} \\ &amp;amp;=\Big\{y+x:y\in B_1\cup\ldots\cup B_k\Big\} \\ &amp;amp;=\Big\{y+x:y\in\bigcup_{i=1}^{k}B_i\Big\} \\ &amp;amp;=\left\{y+x:y\in\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j},b_{i,j})\right\} \\ &amp;amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j}+x,b_{i,j}+x),
\end{align}
which is an elementary set.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;measure-elementary-set&quot;&gt;Measure of an elementary set&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Lemma 10&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Let $E\subset\mathbb{R}^d$ be an elementary set&lt;/em&gt;.&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot; style=&quot;font-style: italic;&quot;&gt;
	&lt;li&gt;$E$ &lt;i&gt;can be expressed as the finite union of disjoint boxes.&lt;/i&gt;&lt;/li&gt;
	&lt;li&gt;If $E$ is partitioned as the finite union $B_1\cup\ldots\cup B_k$ of disjoint boxes, then the quantity $m(E)\doteq\vert B_1\vert+\ldots+\vert B_k\vert$ is independent of the partition. In other words, given any other partition $B_1&apos;\cup\ldots\cup B_{k&apos;}&apos;$ of $E$, we have&lt;/li&gt;
	\begin{equation}
	\vert B_1\vert+\ldots+\vert B_k\vert=\vert B_1&apos;\vert+\ldots+\vert B_{k&apos;}&apos;\vert
	\end{equation}
&lt;/ul&gt;

&lt;p&gt;We refer to $m(E)$ as the &lt;strong&gt;elementary measure&lt;/strong&gt; of $E$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;roman-list&quot;&gt;
	&lt;li&gt;Consider the one-dimensional case, with these $k$ intervals, we can put their $2k$ endpoints into an increasing-order list (discarding repetitions). By looking at the open intervals between these end points, together with the endpoints themselves (viewed as intervals of length zero), we see that there exists a finite collection of disjoint intervals $J_1,\dots,J_{k&apos;}$, such that each of the $I_1,\dots,I_k$ are union of some collection of the $J_1,\dots,J_{k&apos;}$. And since each interval is a one-dimensional box, our statement has been proved with $d=1$.&lt;br /&gt;
	In order to prove the multi-dimensional case, we begin by expressing $E$ as
	\begin{equation}
	E=\bigcap_{i=1}^{k}B_i,
	\end{equation}
	where each box $B_i=I_{i,1}\times\dots\times I_{i,d}$. For each $j=1,\dots,d$, since we has proved the one-dimensional case, we can express $I_{1,j},\dots I_{k,j}$ as the union of subcollections of collections $J_{1,j},\dots,J_{k&apos;,j}$ of disjoint intervals. Taking Cartesian product, we can express the $B_1,\dots,B_k$ as finite unions of box $J_{i_1,1}\times\dots\times J_{i_d,d}$, where $1\leq i_j\leq k_j&apos;$ for all $1\leq j\leq d$. Moreover such boxes are disjoint, which proved our argument.&lt;/li&gt;
	&lt;li&gt; We have that the length for an interval $I$ can be computed as
	\begin{equation}
	\vert I\vert=\lim_{N\to\infty}\frac{1}{N}\#\left(I\cap\frac{1}{N}\mathbb{Z}\right),
	\end{equation}
	where $\#A$ represents the cardinality of a finite set $A$ and 
	\begin{equation}
	\frac{1}{N}\mathbb{Z}\doteq\left\{\frac{x}{N}:x\in\mathbb{Z}\right\}
	\end{equation}
	Thus, volume of the box, say $B$, established from $d$ intervals $I_1,\dots,I_d$ by taking Cartesian product of them can be written as
	\begin{equation}
	\vert B\vert=\lim_{N\to\infty}\frac{1}{N^d}\#\left(B\cap\frac{1}{N}\mathbb{Z}^d\right)
	\end{equation}
	Therefore, with $k$ disjoint boxes $B_1,\dots,B_k$, we have that
	\begin{align}
	\vert B_1\vert+\dots+\vert B_k\vert&amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k}B_i\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cap\frac{1}{N}\mathbb{Z}^d\right) \\\\ &amp;amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k&apos;}B_i&apos;\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;amp;=\vert B_1&apos;\vert+\dots+\vert B_{k&apos;}&apos;\vert
	\end{align}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;elementary-measure-properties&quot;&gt;Properties of elementary measure&lt;/h3&gt;
&lt;p&gt;From the definition of elementary measure, it is easily seen that, for any elementary sets $E$ and $F$ (not necessarily disjoint),&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		$m(E)$ is a nonnegative real number (&lt;b&gt;non-negativity&lt;/b&gt;), and has &lt;b&gt;finite additivity property&lt;/b&gt;:
		\begin{equation}
		m(E\cup F)=m(E)+m(F)
		\end{equation}
		And by induction, it also implies that
		\begin{equation}
		m(E_1\cup\dots\cup E_k)=m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,E_k$ are disjoint elementary sets.
	&lt;/li&gt;
	&lt;li&gt;
		$m(\emptyset)=0$.
	&lt;/li&gt;
	&lt;li&gt;
		$m(B)=\vert B\vert$ for all box $B$.
	&lt;/li&gt;
	&lt;li&gt;
		From non-negativity, finite additivity and &lt;b&gt;Remark 9&lt;/b&gt;, we conclude the &lt;b&gt;monotonicity&lt;/b&gt; property, i.e., $E\subset F$ implies that
		\begin{equation}
		m(E)\leq m(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		From the above and finite additivity, we also obtain the &lt;b&gt;finite subadditivity&lt;/b&gt; property
		\begin{equation}
		m(E\cup F)\leq m(E)+m(F)
		\end{equation}
		And by induction, we then have
		\begin{equation}
		m(E_1\cup\dots\cup E_k)\leq m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,
		E_k$ are elementary sets (not necessarily disjoint).
	&lt;/li&gt;
	&lt;li&gt;
		We also have the &lt;b&gt;translation invariance&lt;/b&gt; property
		\begin{equation}
		m(E+x)=m(E),\hspace{1cm}\forall x\in\mathbb{R}^d
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;uniqueness-elementary-measure&quot;&gt;Uniqueness of elementary measure&lt;/h3&gt;
&lt;p&gt;Let $d\geq 1$ and let $m’:\mathcal{E}(\mathbb{R}^d)\to\mathbb{R}^+$ be a map from the collection $\mathcal{E}(\mathbb{R}^d)$ of elementary subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity, and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all elementary sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Set $c\doteq m’([0,1)^d)$, we then have that $c\in\mathbb{R}^+$ by the non-negativity property. Using the translation invariance property, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)=\dots=m’\left(\left[\frac{n-1}{n},1\right)^d\right)
\end{equation}
On other hand, using the finite additivity property, for any positive integer $n$, we obtain that
\begin{align}
m’([0,1)^d)&amp;amp;=m’\left(\left[0,\frac{1}{n}\right)^d\cup\left[\frac{1}{n},\frac{2}{n}\right)^d\cup\dots\cup\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;amp;=m’\left(\left[0,\frac{1}{n}\right)^d\right)+m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)+\dots+m’\left(\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;amp;=n m’\left(\left[0,\frac{1}{n}\right)^d\right)
\end{align}
Thus,
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{c}{n},\hspace{1cm}\forall n\in\mathbb{Z}^+
\end{equation}
Moreover, since $m\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{1}{n}$, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=cm\left(\left[0,\frac{1}{n}\right)^d\right)
\end{equation}
It then follows by induction that
\begin{equation}
m’(E)=cm(E)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 11&lt;/strong&gt;&lt;br /&gt;
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be elementary sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also elementary, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Without loss of generality, assume that $d_1\leq d_2$. With their definitions as elementary sets, we can assume that
\begin{align}
E_1&amp;amp;=B_1\cup\dots\cup B_{k_1}, \\ E_2&amp;amp;=B_1’\cup\dots\cup B_{k_2}’,
\end{align}
where each $B_i$ is a $d_1$-dimensional box while each $B_i’$ is a $d_2$-dimensional box. And using &lt;strong&gt;Lemma 5&lt;/strong&gt;, without loss of generality, we can assume that $B_i$ are disjoint boxes and $B_i’$ are also disjoint, which implies that
\begin{align}
m^{d_1}(E_1)&amp;amp;=m^{d_1}(B_1)+\dots+m^{d_1}(B_{k_1}),\label{eq:remark11.1} \\ m^{d_2}(E_2)&amp;amp;=m^{d_2}(B_1’)+\dots+m^{d_2}(B_{k_2}’)\label{eq:remark11.2}
\end{align}
By set theory, we have that
\begin{align}
E_1\times E_2&amp;amp;=\Big(B_1\cup\dots\cup B_{k_1}\Big)\times\Big(B_1’\cup\dots\cup B_{k_2}’\Big) \\ &amp;amp;=\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right),\label{eq:remark11.3}
\end{align}
which is an elementary set.&lt;/p&gt;

&lt;p&gt;Since $B_1,\dots,B_{k_1}$ are disjoint and $B_1’,\dots,B_{k_2}’$ are disjoint, the Cartesian products $B_i\times B_j’$ for $i=1,\dots,k_1$ and $j=1,\dots,k_2$ are also disjoint. From \eqref{eq:remark11.3} and using the finite additivity property, we have that
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;amp;=m^{d_1+d_2}\Bigg(\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right)\Bigg) \\ &amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right)\label{eq:remark11.4}
\end{align}
On the one hand, using the definition of boxes, and without loss of generality we can express, for each $i=1,\dots,k_1$, that:
\begin{equation}
B_i=(a_{i,1},b_{i,1})\times\dots\times(a_{i,d_1},b_{i,d_1}),
\end{equation}
where $a_{i,j},b_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_1$. Hence,
\begin{equation}
m^{d_1}(B_i)=\prod_{j=1}^{d_1}(b_{i,j}-a_{i,j}),\hspace{1cm}i=1,\dots,k_1\label{eq:remark11.5}
\end{equation}
Similarly, we also have that
\begin{equation}
m^{d_2}(B_i’)=\prod_{j=1}^{d_2}(d_{i,j}-c_{i,j}),\hspace{1cm}i=1,\dots,k_2\label{eq:remark11.6}
\end{equation}
where $c_{i,j},d_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_2$.&lt;/p&gt;

&lt;p&gt;Moreover, on the other hand, we also have that the $(d_1+d_2)$-dimensional box $B_i\times B_j’$ can be expressed as
\begin{equation}
B_i\times B_j’=(e_1,f_1)\times\dots\times(e_{d_1+d_2},f_{d_1+d_2}),\label{eq:remark11.7}
\end{equation}
where $e_k=a_{i,k};f_k=b_{i,k}$ for all $k=1,\dots,d_1$ and $e_k=c_{j,k-d_1};f_k=d_{j,k-d_1}$ for all $k=d_1+1,\dots,d_2$.&lt;/p&gt;

&lt;p&gt;From \eqref{eq:remark11.5}, \eqref{eq:remark11.6} and \eqref{eq:remark11.5}, for any $i=1,\dots,k_1$ and for any $j=1,\dots,k_2$, we have
\begin{align}
m^{d_1+d_2}(B_i\times B_j’)&amp;amp;=\prod_{k=1}^{d_1+d_2}(f_k-e_k) \\ &amp;amp;=\Bigg(\prod_{k=1}^{d_1}(b_{i,k}-a_{i,k})\Bigg)\Bigg(\prod_{k=1}^{d_2}(d_{j,k}-c_{j,k})\Bigg) \\ &amp;amp;=m^{d_1}(B_i)\times m^{d_2}(B_j’)
\end{align}
With this result, combined with \eqref{eq:remark11.1} and \eqref{eq:remark11.2}, equation \eqref{eq:remark11.4} can be written as
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right) \\ &amp;amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1}(B_i)\times m^{d_2}(B_j’) \\ &amp;amp;=m^{d_1}(E_1)\times m^{d_2}(E_2),
\end{align}
which concludes our proof.&lt;/p&gt;

&lt;h2 id=&quot;jordan-measure&quot;&gt;Jordan measure&lt;/h2&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be a bounded set.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;strong&gt;Jordan inner measure&lt;/strong&gt; $m_{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m_{*,(J)}(E)\doteq\sup_{A\subset E,A\text{ elementary}}m(A)
\end{equation}&lt;/li&gt;
  &lt;li&gt;The &lt;strong&gt;Jordan outer measure&lt;/strong&gt; $m^{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E,B\text{ elementary}}m(B)
\end{equation}&lt;/li&gt;
  &lt;li&gt;If $m_{*,(J)}(E)=m^{*,(J)}(E)$, then we say that $E$ is &lt;strong&gt;Jordan measurable&lt;/strong&gt;, and call
\begin{equation}
m(E)\doteq m_{*,(J)}(E)=m^{*,(J)}(E)
\end{equation}
the &lt;strong&gt;Jordan measure&lt;/strong&gt; of $E$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;jordan-measurability-characterisation&quot;&gt;Characterisation of Jordan measurability&lt;/h3&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be bounded. These following statements are equivalence&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;$E$ is Jordan measurable.&lt;/li&gt;
	&lt;li&gt;For every $\varepsilon&amp;gt;0$, there exists elementary sets $A\subset E\subset B$ such that $m(B\backslash A)\leq\varepsilon$.&lt;/li&gt;
	&lt;li&gt;For every $\varepsilon&amp;gt;0$, there exists an elementary set $A$ such that $m^{*,(J)}(A\Delta E)\leq\varepsilon$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
In order to prove these three statements are equivalence, we will be proving that (1) implies (2); (2) implies (3); and that (2) implies (1).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(1) implies (2).&lt;br /&gt;
Since $E$ is Jordan measurable, we have that
\begin{equation}
m(E)=\sup_{A\subset E;A\text{ elementary}}m(A)=\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
By the definition of supremum, there exists an elementary set $A\subset E$ such that for any $\varepsilon&amp;gt;0$ 
\begin{equation}
m(A)\geq m(E)-\frac{\varepsilon}{2}\label{eq:jmc.1}
\end{equation}
In addition, by the definition of infimum, there also exists an elementary set $B\supset E$ such that for any $\varepsilon&amp;gt;0$
\begin{equation}
m(B)\leq m(E)+\frac{\varepsilon}{2}\label{eq:jmc.2}
\end{equation}
From \eqref{eq:jmc.1} and \eqref{eq:jmc.2}, we have that for any $\varepsilon&amp;gt;0$
\begin{equation}
m(B\backslash A)=m(B)-m(A)\leq\varepsilon
\end{equation}&lt;/li&gt;
  &lt;li&gt;(2) implies (3).&lt;br /&gt;
With (2) satisfied, we have that we can find elementary sets $A\subset E\subset B$ such that
\begin{equation}
m(B\backslash A)\leq\varepsilon,\hspace{1cm}\forall\varepsilon&amp;gt;0
\end{equation}
Since $A\subset E\subset B$ and by the definition of symmetric difference, we have
\begin{equation}
A\Delta E=(A\backslash E)\cup(E\backslash A)=(E\backslash A)\subset(B\backslash A)
\end{equation}
Hence
\begin{equation}
m^{*,(J)}(A\Delta E)\leq m(B\backslash A)\leq\varepsilon
\end{equation}&lt;/li&gt;
  &lt;li&gt;(2) implies (1).&lt;br /&gt;
Let $(A_n)_{n\in\mathbb{N}}$ and $(B_n)_{n\in\mathbb{N}}$ be sequences of elementary sets such that $A_n\subset E\subset B_n$ for all $n\in\mathbb{N}$. Statement (2) says that for all $\varepsilon&amp;gt;0$, there exists $i,j\in\mathbb{N}$ such that
\begin{equation}
m(B_j\backslash A_i)\leq\varepsilon
\end{equation}
or
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon\label{eq:jmc.3}
\end{equation}
Let $A_\text{sup}$ and $B_\text{inf}$ be two sets in the two sequences above with
\begin{align}
m(A_\text{sup})&amp;amp;=\sup_{n\in\mathbb{N}}m(A_n), \\ m(B_\text{inf})&amp;amp;=\inf_{n\in\mathbb{N}}m(B_n),
\end{align}
which means
\begin{align}
m_{*,(J)}(E)&amp;amp;=m(A_\text{sup}) \\ m^{*,(J)}(E)&amp;amp;=m(B_\text{inf})
\end{align}
Using the monotonicity property of elementary measure, we have that
\begin{equation}
m(A_\text{sup})\leq m(B_\text{inf})
\end{equation}
Assume that $m(B_\text{inf})&amp;gt;m(A_\text{sup})$, and consider an $\varepsilon&amp;gt;0$ such that $\varepsilon&amp;lt; m(B_\text{inf})-m(A_\text{sup})$. We can continue to derive \eqref{eq:jmc.3} as
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon&amp;lt; m(A_i)+m(B_\text{inf})-m(A_\text{sup})&amp;lt; m(B_\text{inf}),
\end{equation}
which is false with the definition of $B_\text{inf}$. Therefore, our assumption is also false, which means
\begin{equation}
m(A_\text{sup})=m(B_\text{inf})
\end{equation}
or
\begin{equation}
m_{*,(J)}(E)=m^{*,(J)}(E),
\end{equation}
or in other words, $E$ is Jordan measurable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Corollary 12&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Every elementary set $E$ is Jordan measurable.&lt;/li&gt;
  &lt;li&gt;On elementary sets, Jordan measure is elementary measure.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Jordan measurability also inherits many of the properties of elementary measure.&lt;/p&gt;

&lt;h3 id=&quot;jordan-measurability-properties&quot;&gt;Properties of Jordan measurability&lt;/h3&gt;
&lt;p&gt;Let $E,F\in\mathbb{R}^d$ be Jordan measurable sets. Then&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Boolean closure&lt;/b&gt;. $E\cup F,E\cap F,E\backslash F,E\Delta F$ are also Jordan measurable sets.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Non-negativity&lt;/b&gt;. $m(E)\geq 0$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite additivity&lt;/b&gt;. If $E,F$ are disjoint, then $m(E\cup F)=m(E)+m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $E\subset F$, then $m(E)\leq m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite subadditivity&lt;/b&gt;. $m(E\cup F)\leq m(E)+m(F)$.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Translation invariance&lt;/b&gt;. For any $x\in\mathbb{R}^d$, $E+x$ is Jordan measurable, and $m(E+x)=m(E)$.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Boolean closure&lt;/b&gt;.
		&lt;ul&gt;
			&lt;li&gt;
				By characterisation of Jordan measurability, we can find elementary sets $A_1\subset E\subset B_1$ and $A_2\subset F\subset B_2$ such that for any $\varepsilon&amp;gt;0$
				\begin{align}
				m(B_1\backslash A_1)&amp;amp;\leq\frac{\varepsilon}{2}, \\ m(B_2\backslash A_2)&amp;amp;\leq\frac{\varepsilon}{2}
				\end{align}
				Thus, we have that
				\begin{equation}
				\left(A_1\cap A_2\right)\subset\left(E\cap F\right)\subset\left(B_1\cap B_2\right)
				\end{equation}
				and
				\begin{equation}
				\left(A_1\cup A_2\right)\subset\left(E\cup F\right)\subset\left(B_1\cup B_2\right)
				\end{equation}
				Moreover, for any $\varepsilon&amp;gt;0$, we have that
				\begin{align*}
				m\big((B_1\cup B_2)\backslash(A_1\cup A_2)\big)&amp;amp;=m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)+m(B_2\backslash B_1)-m(A_1\cup A_2) \\ &amp;amp;\leq m(B_1)+m(B_2\backslash A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)-m(A_1)+m(B_2\backslash A_1)+m(A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1)-m(A_1)+m(B_2\cup A_1)-m(A_1\cup A_2) \\ &amp;amp;=m(B_1\backslash A_1)+m\big((B_2\cup A_1)\backslash(A_1\cup A_2)\big) \\ &amp;amp;=m(B_1\backslash A_1)+m(B_2\backslash A_2) \\ &amp;amp;\leq\varepsilon/2+\varepsilon/2 \\ &amp;amp;=\varepsilon,
				\end{align*}
				which implies that $E\cup F$ is Jordan measurable.
			&lt;/li&gt;
			&lt;li&gt;
				From the result above, and by monotonicity, finite additivity, finite subadditivity properties of elementary measure, for any $\varepsilon&amp;gt;0$, we also have that
				\begin{align*}
				m\big((B_1\cap B_2)\backslash(A_1\cap A_2)\big)&amp;amp;=m(B_1\cap B_2)-m(A_1\cap A_2) \\ &amp;amp;=m\Big(\big(B_1\cup B_2\big)\backslash\big((B_1\backslash B_2)\cup(B_2\backslash B_1)\big)\Big) \\ &amp;amp;\hspace{1cm}-m\Big(\big(A_1\cup A_2\big)\backslash\big((A_1\backslash A_2)\cup(A_2\backslash A_1)\big)\Big) \\ &amp;amp;=m(B_1\cup B_2)-m(B_1\backslash B_2)-m(B_2\backslash B_1) \\ &amp;amp;\hspace{1cm}-m(A_1\cup A_2)+m(A_1\backslash A_2)+m(A_2\backslash A_1) \\ &amp;amp;=m(B_1\cup B_2)-m(A_1\cup A_2)+m(A_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;amp;\hspace{1cm}+m(A_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2)+m(B_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;amp;\hspace{1cm}+m(B_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;amp;\leq\varepsilon,
				\end{align*}
				which also implies that $E\cap F$ is Jordan measurable.
			&lt;/li&gt;
			&lt;li&gt;&lt;/li&gt;
		&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Non-negativity&lt;/b&gt;.&lt;br /&gt;
		Given $E$ being Jordan measurable set, we have
		\begin{equation}
		m(E)=\sup_{A\subset E,A\text{ elementary}}m(A)\geq m(\emptyset)=0,
		\end{equation}
		by the monotonicity property of elementary measure.
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite additivity&lt;/b&gt;.&lt;br /&gt;
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite additivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset F,A_2\text{ elementary}}m(A_2) \\ &amp;amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1)+m(A_2) \\ &amp;amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2)=m(E\cup F)
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;.&lt;br /&gt;
		Given $E\subset F$ are Jordan measurable sets, the we have
		\begin{equation}
		m(E)\leq\sup_{A\subset F,A\text{ elementary}}m(A)=m(F)
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Finite subadditivity&lt;/b&gt;.&lt;br /&gt;
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite subadditivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset E,A_2\text{ elementary}}m(A_2) \\ &amp;amp;\geq\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2) \\ &amp;amp;=m(E\cup F)=m(E\cup F)
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Translation invariance&lt;/b&gt;.&lt;br /&gt;
		By the translation invariance property of elementary measure, for any $x\in\mathbb{R}^d$, the Jordan inner measure of $E+x$ can be written as
		\begin{align}
		m_{*,(J)}(E+x)&amp;amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A) \\ &amp;amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A-x) \\ &amp;amp;=\sup_{A-x\subset E,A-x\text{ elementary}}m(A-x)=m(E)
		\end{align}
		Similarly, we also have the Jordan outer measure of $E+x$ is also equal to the Jordan measure of $E$
		\begin{equation}
		m^{*,(J)}(E+x)=m(E)
		\end{equation}
		Hence,
		\begin{equation}
		m_{*,(J)}(E+x)=m^{*,(J)}(E+x)=m(E),
		\end{equation}
		or in other words, $E+x$ is Jordan measurable with $m(E+x)=m(E)$.
	&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Remark 13&lt;/strong&gt; (Regions under graphs are Jordan measurable)&lt;br /&gt;
Let $B$ be a closed box in $\mathbb{R}^d$, and let $f:B\to\mathbb{R}$ be a continuous function. Then&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		The graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ is Jordan measurable in $\mathbb{R}^{d+1}$ with Jordan measure zero.
	&lt;/li&gt;
	&lt;li&gt;
		The set $\{(x,t):x\in B;0\leq t\leq f(x)\}\subset\mathbb{R}^{d+1}$ is Jordan measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		For any closed box $C\in\mathbb{R}^d$, we have $\{(x,f(x)):x\in C\}\subset\mathbb{R}^{d+1}$ with $f:C\to\mathbb{R}$ is a compact set. And when $f$ continuous in a compact set we also have $f$ is &lt;span&gt;uniformly continuous&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/span&gt;, which means for any $\varepsilon&amp;gt;0$, there exists $\delta$ such that for every $x,y\in C$
		\begin{equation}
		\vert f(x)-f(y)\vert&amp;lt;\varepsilon,
		\end{equation}
		with $\vert x-y\vert&amp;lt;\delta$. Therefore, we can divide $C$ into finitely many almost disjoint boxes $C_1,\ldots,C_n$ such that $\vert x_i-y_i\vert&amp;lt;\delta$ for every $x_i,y_i\in C_i$ and for any $\varepsilon&amp;gt;0$
		\begin{equation}
		\vert f(x_i)-f(y_i)\vert&amp;lt;\varepsilon
		\end{equation}
		Moreover, for each such box $C_i$ with center of the box $x_i$ we also have
		\begin{equation}
		\left\{(x,f(x)):x\in C_i\right\}\subset C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		Therefore
		\begin{equation}
		\left\{(x,f(x)):x\in C\right\}=\bigcup_{i=1}^{n}\left\{(x,f(x)):x\in C_i\right\}\subset\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		With this result, and by the monotonicity, finite additivity of elementary measure, we have the Jordan outer measure of the graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ can be written as
		\begin{align}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)&amp;amp;=\inf_{C\supset B,C\text{ closed box}}m\left(\left\{(x,f(x)):x\in C\right\}\right) \\ &amp;amp;\leq m^{d+1}\left(\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;amp;=\sum_{i=1}^{n}m^d(C_i)\times m^1\left(\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;amp;=2n\varepsilon m^d(C)&amp;lt;2n\varepsilon\delta
		\end{align}
		And since $\varepsilon&amp;gt;0$ arbitrarily, we finally obtain
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)=0
		\end{equation}
		Plus that, since
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq m_{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq 0,
		\end{equation}
		we have that
		\begin{equation}
		m^{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=m_{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=0,
		\end{equation}
		or in other words, the graph $\left(\{(x,f(x)):x\in B\}\right)$ is Jordan measurable on $\mathbb{R}^{d+1}$ with Jordan measure zero.
	&lt;/li&gt;
	&lt;li&gt;
		Let $E=\big\{(x,t):x\in B;0\leq t\leq f(x)\big\}$ and let $I$, $O$ be sets defined as for an arbitrary $\varepsilon&amp;gt;0$
		\begin{align}
		I&amp;amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)-\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)-\frac{\varepsilon}{2}\right], \\ O&amp;amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)+\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]
		\end{align}
		Therefore, it follows immediately that $I\subset E\subset O$ and moreover
		\begin{align}
		m^{d+1}(O\backslash I)&amp;amp;=m^{d+1}\left(B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash B\times\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;amp;=m^d(B)\times m^1\left(\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;amp;=m^d(B)\times\varepsilon
		\end{align}
		And since $\varepsilon&amp;gt;0$ arbitrarily, we can claim that $E$ is Jordan measurable.
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark 14&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;
		All open and closed Euclidean balls, $B(x,r)\doteq\{y\in\mathbb{R}^d:\vert y-x\vert&amp;lt; r\}$ and $\overline{B(x,r)}\doteq\{y\in\mathbb{R}^d:\vert y-x\vert\leq r\}$, in $\mathbb{R}^d$ are Jordan measurable, with Jordan measure $c_dr^d$ for some constant $c_d$ depending only on $d$.
	&lt;/li&gt;
	&lt;li&gt;
		Establish the crude bounds
		\begin{equation}
		\left(\frac{2}{\sqrt{d}}\leq c_d\leq 2^d\right)
		\end{equation} 
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;jordan-null-sets&quot;&gt;Jordan null sets&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;Jordan null set&lt;/strong&gt; is a Jordan measurable set of Jordan measure zero. We have that any subset of a Jordan null set is also a Jordan null set.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $E\subset F$ where F is a Jordan null set. Also let $A\subset E$, it follows that $A\subset F$, and hence
\begin{equation}
m(A)\leq m_{*,(J)}(F)=0
\end{equation}
Since $m(E)=0$, we can choose a set $B\supset F$ such that $m(B)\leq\varepsilon$ for $\varepsilon&amp;gt;0$ arbitrarily. Thus, $E\subset B$ and moreover
\begin{equation}
m(B\backslash A)\leq\varepsilon,
\end{equation}
which claims that $E$ is Jordan measurable with measurable of zero since $m(E)\leq m(F)=0$. Or in other words, $E$ is also a Jordan null set.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 15&lt;/strong&gt;&lt;br /&gt;
For any Jordan measurable set $E\subset\mathbb{R}^d$, its Jordan measure can be written as
\begin{equation}
m(E)\doteq\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cup\frac{1}{N}\mathbb{Z}^d\right)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;metric-formula-jordan-measurability&quot;&gt;Metric entropy formulation of Jordan measurability&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;dyadic cube&lt;/strong&gt; is defined to be a half-open box of the form
\begin{equation}
\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right)\times\ldots\times\left(\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d$. Let $E\subset\mathbb{R}^d$ be a bounded set. For each integer $n$, let $\mathcal{E}_*(E,2^{-n})$ denote the number of dyadic cubes of sidelength $2^{-n}$ that are contained in $E$, and let $\mathcal{E}^*(E,2^{-n})$ be the number of dyadic cubes of sidelength $2^{-n}$ that intersect $E$. Then $E$ is Jordan measurable iff
\begin{equation}
\lim_{n\to\infty}2^{-dn}(\mathcal{E}^*(E,2^{-n}))-\mathcal{E}_*(E, 2^{-n})=0,
\end{equation}
in which case we have
\begin{equation}
m(E)=\lim_{n\to\infty}2^{-dn}\mathcal{E}_*(E,2^{-n})=\lim_{n\to\infty}2^{-dn}\mathcal{E}^*(E,2^{-n})
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;uniqueness-jordan-measure&quot;&gt;Uniqueness of Jordan measure&lt;/h3&gt;
&lt;p&gt;Let $d\geq 1$ and let $m’:\mathcal{J}(\mathbb{R}^d)\to\mathbb{R}^+$  be a map from the collection of Jordan measurable subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all Jordan measurable sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Follow the same steps as the proof of the uniqueness of elementary measure, the argument above can easily be proved.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark 16&lt;/strong&gt;&lt;br /&gt;
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be Jordan measurable sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also Jordan measurable, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $A_1\subset E_1$ such that $A_1$ is elementary and
\begin{equation}
m^{d_1}(A_1)=\sup_{A\subset E_1,A\text{ elementary}}m(A)=m_{*,(J)}(E_1)=m^{d_1}(E_1)
\end{equation}
Let $B_1\supset E_1$ such that $B_1$ is elementary and
\begin{equation}
m^{d_1}(B_1)=\inf_{B\supset E_1,B\text{ elementary}}m(B)=m^{*,(J)}(E_1)=m^{d_1}(E_1),
\end{equation}
which implies that
\begin{equation}
m^{d_1}(A_1)=m^{d_1}(B_1)=m^{d_1}(E_1)
\end{equation}
Analogously, we define $A_2\subset E_2\subset B_2$ such that
\begin{align}
m^{d_2}(A_2)&amp;amp;=\sup_{A\subset E_2,A\text{ elementary}}m(A)=m_{*,(J)}(E_2)=m^{d_2}(E_2) \\ m^{d_2}(B_2)&amp;amp;=\inf_{B\supset E_2,B\text{ elementary}}m(B)=m^{*,(J)}(E_2)=m^{d_1}(E_2)
\end{align}
And thus, we also have
\begin{equation}
m^{d_2}(A_2)=m^{d_2}(B_2)=m^{d_2}(E_2)
\end{equation}
On the one hand, with these definitions, we have
\begin{equation}
m^{d_1+d_2}(A_1\times A_2)=\sup_{A\subset E_1\times E_2,A\text{ elementary}}=m_{*,(J)}(E_1\times E_2)\label{eq:remark15.1}
\end{equation}
and
\begin{equation}
m^{d_1\times d_2}(B_1\times B_2)=\sup_{B\supset E_1\times E_2,A\text{ elementary}}=m^{*,(J)}(E_1\times E_2)\label{eq:remark15.2}
\end{equation}
On the other hands, By &lt;strong&gt;remark 11&lt;/strong&gt;, we have that $A_1\times A_2$ and $B_1\times B_2$ are also elementary sets and
\begin{align}
m^{d_1}(A_1)\times m^{d_2}(A_2)&amp;amp;=m^{d_1+d_2}(A_1\times A_2)\label{eq:remark15.3} \\ m^{d_1}(B_1)\times m^{d_2}(B_2)&amp;amp;=m^{d_1+d_2}(B_1\times B_2)\label{eq:remark15.4}
\end{align}
From \eqref{eq:remark15.1}, \eqref{eq:remark15.2}, \eqref{eq:remark15.3} and \eqref{eq:remark15.4}, we can claim that $E_1\times E_2$ is Jordan measurable and
\begin{equation}
m^{d_1}(E_1)\times m^{d_2}(E_2)=m^{d_1+d_2}(E_1\times E_2)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;topo-jordan-measurability&quot;&gt;Topological of Jordan measurability&lt;/h3&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be a bounded set&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;$E$ and the closure $\bar{E}$ of $E$ have the same Jordan outer measure.&lt;/li&gt;
	&lt;li&gt;$E$ and the interior $E^\circ$ of $E$ have the same Jordan inner measure.&lt;/li&gt;
	&lt;li&gt;$E$ is Jordan measurable iff the &lt;b&gt;topological boundary&lt;/b&gt; $\partial E$ of $E$ has Jordan outer measure zero.&lt;/li&gt;
	&lt;li&gt;The &lt;b&gt;bullet-riddled square&lt;/b&gt; $[0,1]^2\backslash\mathbf{Q}^2$, and set of bullets $[0,1]^2\cup Q^2$, both have Jordan inner measure zero and Jordan outer measure one. In particular, both sets are not Jordan measurable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		Since $E\subset\overline{E}$, it is easily seen that
		\begin{equation}
		m^{*,(J)}(E)\leq m^{*,(J)}(\overline{E})
		\end{equation}
		Thus, the problem remains to prove that
		\begin{equation}
		m^{*,(J)}(E)\geq m^{*,(J)(\overline{E})}
		\end{equation}
		Let $B_1,\ldots,B_N$ be $N$ disjoint boxes such that
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;caratheodory-type-property&quot;&gt;Carathéodory type property&lt;/h3&gt;
&lt;p&gt;Let $E\subset\mathbb{R}^d$ be a bounded set, and $F\subset\mathbb{R}^d$ be an elementary set. Then we have that
\begin{equation}
m^{*,(J)}(E)=m^{*,(J)}(E\cap F)+m^{*,(J)}(E\backslash F)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;connect-riemann-int&quot;&gt;Connection with the Riemann integral&lt;/h2&gt;
&lt;p&gt;We then consider the relationship between Jordan measure and the &lt;strong&gt;Rieman integral&lt;/strong&gt;, or the equivalent &lt;strong&gt;Darboux integral&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;riemann-integrability&quot;&gt;Riemann integrability&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval of positive length, and $f:[a,b]\to\mathbb{R}$ be a function. A &lt;strong&gt;tagged partition&lt;/strong&gt;
\begin{equation}
\mathcal{P}=\left(\left(x_0,x_1,\dots,x_n\right),\left(x_1^{*},\dots,x_n^{*}\right)\right)
\end{equation}
of $[a,b]$ is a finite sequence of real numbers $a=x_0&amp;lt; x_1&amp;lt;\dots&amp;lt; x_n=b$, together with additional numbers $x_{i-1}\leq x_i^{*}\leq x_i$ for each $i=1,\dots,n$. Let $\delta x_i\doteq x_i-x_{i-1}$, the quantity
\begin{equation}
\Delta(\mathcal{P})\doteq\sup_{1\leq i\leq n}\delta x_i
\end{equation}
is called the &lt;strong&gt;norm&lt;/strong&gt; of the tagged partition. The &lt;strong&gt;Riemann sum&lt;/strong&gt; $\mathcal{R}(f,\mathcal{P})$ of $f$ w.r.t the tagged partition $\mathcal{P}$ is defined as
\begin{equation}
\mathcal{R}(f,\mathcal{P})\doteq\sum_{i=1}^{n}f(x_i^{*})\delta x_i
\end{equation}
we say that $f$ is &lt;strong&gt;Riemann integrable&lt;/strong&gt; on $[a,b]$ if there exists a real number, denoted as $\int_{a}^{b}f(x)\,dx$ and referred to as the &lt;strong&gt;Riemann integral&lt;/strong&gt; on $[a,b]$, for which we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=\lim_{\Delta\mathcal{P}\to 0}\mathcal{R}(f,\mathcal{P}),
\end{equation}
by which we mean that for every $\varepsilon&amp;gt;0$ there exists $\delta&amp;gt;0$ such that
\begin{equation}
\left\vert\mathcal{R}(f,\mathcal{P})-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon,
\end{equation}
for every tagged partition $\mathcal{P}$ with $\Delta(\mathcal{P})\leq\delta$.&lt;/p&gt;

&lt;h3 id=&quot;pc-func&quot;&gt;Piecewise constant functions&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval. a &lt;strong&gt;piecewise constant function&lt;/strong&gt; $f:[a,b]\to\mathbb{R}$ is a function for which there exists a partition of $[a,b]$ into infinitely many intervals $I_1,\dots,I_n$ such that $f$ is equal to a constant $c_i$ on each of the intervals $I_i$. Then, the expression
\begin{equation}
\sum_{i=1}^{n}c_i\vert I_i\vert
\end{equation}
is independent of the choice of partition used to demonstrate the piecewise constant nature of $f$. We denote this quantity as $\text{p.c.}\int_{a}^{b}f(x)\,dx$, and refer it to as &lt;strong&gt;piecewise constant integral&lt;/strong&gt; of $f$ on $[a,b]$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider two partitions of the interval $[a,b]$ into finitely many intervals $(I_i)_{i=1,\ldots,n}=I_1,\ldots,I_n$ and $(J_i)_{i=1,\ldots,m}=J_1,\ldots,J_m$ such that:
\begin{align}
f(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ f(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in J_i
\end{align}
Thus, we have that:
\begin{equation}
c_i=d_j,\hspace{1cm}\forall x\in\left(I_i\cap J_j\right)
\end{equation}
With this result, we have:
\begin{align}
\sum_{i=1}^{n}c_i\vert I_i\vert&amp;amp;=\sum_{i=1}^{n}c_i\left\vert\bigcup_{j=1}^{m}\left(I_i\cap J_j\right)\right\vert \\ &amp;amp;=\sum_{i=1}^{n}\sum_{j=1}^{m}c_i\left\vert I_i\cap J_j\right\vert \\ &amp;amp;=\sum_{j=1}^{m}\sum_{i=1}^{n}d_j\left\vert I_i\cap J_j\right\vert \\ &amp;amp;=\sum_{j=1}^{m}d_j\left\vert\bigcup_{i=1}^{n}\left(J_j\cap I_i\right)\right\vert \\ &amp;amp;=\sum_{j=1}^{m}d_j\vert J_j\vert,
\end{align}
which claims the independence of the choices of partition of $f$.&lt;/p&gt;

&lt;h4 id=&quot;pc-int-properties&quot;&gt;Basic properties of piecewise constant integral&lt;/h4&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be piecewise constant functions. Then&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are piecewise constant functions, with
		\begin{align}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx&amp;amp;=c\text{p.c.}\int_{a}^{b}f(x)\,dx \\\\ \text{p.c.}\int_{a}^{b}\left(f(x)+g(x)\right)\,dx&amp;amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $f\leq g$ pointwise, i.e., $f(x)\leq g(x),\forall x\in[a,b]$, then
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx\leq\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;. If $E$ is an elementary subset of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ (defined by setting $1_E(x)\doteq 1$ if $x\in E$ and 0 otherwise) is piecewise constant, and
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;&lt;br /&gt;
		For any $c\in\mathbb{R}$, we have:
		\begin{equation}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx=\sum_{i=1}^{n}cc_i\vert I_i\vert=c\sum_{i=1}^{n}c_i\vert I_i\vert=c\text{p.c.}\int_{a}^{b}f(x)\,dx
		\end{equation}
		From the partitioning independence of piecewise constant functions, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{equation}
		f(x)=c_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		and
		\begin{equation}
		g(x)=d_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		Thus, we have
		\begin{align}
		\text{p.c.}\int_{a}^{b}f(x)+g(x)\,dx&amp;amp;=\sum_{i=1}^{n}\left(c_i+d_i\right)\vert I_i\vert \\ &amp;amp;=\sum_{i=1}^{n}c_i\vert I_i\vert+\sum_{i=1}^{n}d_i\vert I_i\vert \\ &amp;amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;&lt;br /&gt;
		Analogy to the above proof, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{align}
		f(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ g(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in I_i,
		\end{align}
		Since $f\leq g$ pointwise, in any interval $I_i$, we also have that $c_i=f(x)\leq g(x)=d_i$. Therefore,
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx=\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert=\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;&lt;br /&gt;
		Since $E\subset[a,b]\subset\mathbb{R}$ is an elementary set, we can represent the elementary measure $m(E)$ of set $E$ as
		\begin{equation}
		m(E)=\sum_{i=1}^{n}\vert I_i\vert
		\end{equation}
		Therefore, for any $x\in I_i$ for $i=1,\ldots n$, we have that $1_E(x)=1$; and for any $x\in[b-a]\backslash E=\bigcup_{j=1}^{m}J_j$, we get that $1_E(x)=0$, which lets $1_E$ satisfy the condition of a piecewise constant function.&lt;br /&gt;
		Moreover, we have that
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=\sum_{i=1}^{n}1\vert I_i\vert+\sum_{j=1}^{m}0\vert J_j\vert=\sum_{i=1}^{n}\vert I_i\vert=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;darboux-int&quot;&gt;Darboux integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an integral, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. The &lt;strong&gt;lower Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$, denoted as $\underline{\int_{a}^{b}}f(x)\,dx$, is defined as
\begin{equation}
\underline{\int_a^b}f(x)\,dx\doteq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx,
\end{equation}
where $g$ ranges over all piecewise constant functions that are pointwise bounded above by $f$ (the hypothesis that $f$ is bounded ensures that the supremum is over a non-empty set).&lt;/p&gt;

&lt;p&gt;Similarly, we can define the &lt;strong&gt;upper Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$, denoted as $\overline{\int_a^b}f(x)\,dx$, as
\begin{equation}
\overline{\int_a^b}f(x)\,dx\doteq\inf_{h\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx
\end{equation}
It is easily seen that $\underline{\int_a^b}f(x)\,dx\leq\overline{\int_a^b}f(x)\,dx$. The equality holds when $f$ is &lt;strong&gt;Darboux integrable&lt;/strong&gt;, and we refer to this quantity as &lt;strong&gt;Darboux integral&lt;/strong&gt; of $f$ on $[a,b]$.&lt;/p&gt;

&lt;p&gt;Note that the upper and lower Darboux integrals are related by
\begin{equation}
\overline{\int_a^b}-f(x)\,dx=-\underline{\int_a^b}f(x)\,dx
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;equiv-riemann-darboux-int&quot;&gt;Equivalence of Riemann integral and Darboux integral&lt;/h4&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff it is Darboux integrable, in which case the Riemann integrals and Darboux integrals are the same.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given $f$ is Riemann integrable on $[a,b]$, we have that for any $\varepsilon&amp;gt;0$, there exists a tagged partition $((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*))$ of $[a,b]$ with $x_i^*\in I_i$ such that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
\end{equation}
For each interval $I_i$, there exist an $x_i^{(1)}$ such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i^{(1)})&amp;lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Thus, for any $\varepsilon&amp;gt;0$ we obtain
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert&amp;lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that for any $\varepsilon&amp;gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&amp;lt;\varepsilon\label{eq:erdi.1}
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Combining with \eqref{eq:erdi.1}, we have that as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Moreover, we also have that
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the lower Darboux integral of $f$ on $[a,b]$. Thus,
\begin{equation}
\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx\label{eq:erdi.2}
\end{equation}
Similarly, applying the same procedure as above, we also have that on each $I_i$ there exists an $x_i^{(2)}$ such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&amp;lt;\varepsilon
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Therefore,
\begin{equation}
\sum_{n=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx,
\end{equation}
as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$. Additionally, we also have
\begin{equation}
\sum_{i=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\geq\inf_{h\geq f, \text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx=\overline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the upper Darboux integral of $f$ on $[a,b]$. And hence
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\label{eq:erdi.3}
\end{equation}
From \eqref{eq:erdi.2} and \eqref{eq:erdi.3}, we end up with
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which happens iff
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx=\int_{a}^{b}f(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which claims that $f$ is Darboux integrable on $[a,b]$, with the Darboux integral is exactly the Riemann integral $\int_{a}^{b}f(x)\,dx$.&lt;/li&gt;
  &lt;li&gt;Given $f$ is Darboux integrable on $[a,b]$, we have that the upper and lower Darboux integrals are equal, and are equal to the Darboux integral of $f$ on $[a,b]$ which we denote as $\text{d.}\int_{a}^{b}f(x)\,dx\in\mathbb{R}$.
\begin{equation}
\underline{\int_a^b}f(x)\,dx=\overline{\int_a^b}f(x)\,dx=\text{d.}\int_{a}^{b}f(x)\,dx
\end{equation}
By definition of the lower Darboux integral, there exists a piecewise constant function $g(x)$ bounded above by $f$ (i.e., $g\leq f$ piecewise), such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;gt;\underline{\int_{a}^{b}}f(x)\,dx-\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon\label{eq:erdi.4}
\end{equation}
Likewise, by definition of the upper Darboux integral, there exists a piecewise constant function $h(x)$ bounded below by $f$ (i.e., $h\geq f$ piecewise), such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}h(x)\,dx&amp;lt;\overline{\int_{a}^{b}}f(x)\,dx+\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon\label{eq:erdi.5}
\end{equation}
From the independence of choice of partition of piecewise constant functions $g$ and $h$, there exists a partition $I_1,\ldots,I_n$ such that
\begin{align}
g(x)&amp;amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ h(x)&amp;amp;=d_i,\hspace{1cm}\forall x\in I_i
\end{align}
and
\begin{align}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;amp;=\sum_{i=1}^{n}c_i\vert I_i\vert,\label{eq:erdi.6} \\ \text{p.c.}\int_{a}^{b}h(x)\,dx&amp;amp;=\sum_{i=1}^{n}d_i\vert I_i\vert,\label{eq:erdi.7}
\end{align}
then it follows immediately that $c_i\leq d_i$. And since $g\leq f\leq h$ piecewise, on each interval $I_i$, we can find a $x_i^*$ such that $c_i\leq f(x_i^*)\leq d_i$. Additionally, combining with \eqref{eq:erdi.4}, \eqref{eq:erdi.5}, \eqref{eq:erdi.6} and \eqref{eq:erdi.7}, we have that for any $\varepsilon&amp;gt;0$
\begin{equation}
\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon&amp;lt;\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert&amp;lt;\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon
\end{equation}
Therefore, for any $\varepsilon&amp;gt;0$, we have
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\text{d.}\int_{a}^{b}f(x)\,dx\right\vert&amp;lt;\varepsilon,
\end{equation}
which claims that $f$ is Riemann integrable on $[a,b]$ with $\text{d.}\int_{a}^{b}f(x)\,dx$ is the Riemann integral of $f$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
Any continuous function $f:[a,b]\to\mathbb{R}$ is Riemann integrable. More generally, any bounded, &lt;strong&gt;piecewise continuous function&lt;/strong&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; $f:[a,b]\to\mathbb{R}$ is Riemann integrable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Consider a partition of piecewise continuous f on $[a,b]$ into finitely many intervals $I_1,\ldots,I_n$. Using the procedure that we used for the above proof, we have that on each interval $I_i$, there exists an $x_i$ such that for any $\varepsilon&amp;gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i)&amp;lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Hence,
\begin{equation}
\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i)\vert I_i\vert&amp;lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i)\vert I_i\vert-\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&amp;lt;\varepsilon,
\end{equation}
which implies that $f$ is Riemann integrable on $[a,b]$.&lt;/p&gt;

&lt;h3 id=&quot;riemann-int-properties&quot;&gt;Basic properties of Riemann integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be Riemann integrable. We then have that&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are Riemann integrable, with
		\begin{align}
		\int_{a}^{b}cf(x)\,dx&amp;amp;=c\int_{a}^{b}f(x)\,dx \\\\ \int_{a}^{b}\big(f(x)+g(x)\big)\,dx&amp;amp;=\int_{a}^{b}f(x)\,dx+\int_{a}^{b}g(x)\,dx
		\end{align}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;. If $f\leq g$ pointwise, then
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;. If $E$ is a Jordan measurable of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ is Riemann integrable, and
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;number-list&quot;&gt;
	&lt;li&gt;
		&lt;b&gt;Linearity&lt;/b&gt;.
		&lt;ul&gt;
			&lt;li&gt;
				Given $f$ Riemann integrable on $[a,b]$, we have that there exists a tagged partition $\mathcal{P}=((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*));(x_i^*\in I_i)$ of $[a,b]$ such that for any $\varepsilon&amp;gt;0$, we have
				\begin{equation}
				\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
				\end{equation}
				Thus, for any $c\in\mathbb{R}$
				\begin{equation}
				\left\vert\sum_{i=1}^{n}cf(x_i^*)\vert I_i\vert-\int_{a}^{b}cf(x)\,dx\right\vert\leq\vert c\vert\varepsilon=\varepsilon&apos;,
				\end{equation}
				where $\varepsilon&apos;&amp;gt;0$ arbitrarily. This implies that $cf$ is Riemann integrable on $[a,b]$ with Riemann integral $\int_{a}^{b}cf(x)\,dx=c\int_{a}^{b}f(x)\,dx$.
			&lt;/li&gt;
			&lt;li&gt;
				Given $f$ Riemann integrable on $[a,b]$, then $f$ is also Darboux integrable on $[a,b]$, which means
				\begin{align}
				\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx&amp;amp;=\inf_{f_2\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)\,dx \\ &amp;amp;\hspace{1cm}=\int_{a}^{b}f(x)\,dx\label{eq:rip.1}
				\end{align}
				Similarly, $g$ Riemann integrable on $[a,b]$ implies that $g$ is also Darboux integrable, or in particular
				\begin{align}
				\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx&amp;amp;=\inf_{g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_2(x)\,dx \\ &amp;amp;\hspace{1cm}=\int_{a}^{b}g(x)\,dx\label{eq:rip.2}
				\end{align}
				By the linearity property of piecewise constant functions, combined with \eqref{eq:rip.1} and \eqref{eq:rip.2}, we obtain
				\begin{align}
				&amp;amp;\sup_{f_1\leq f,g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)+g_1(x)\,dx \\ &amp;amp;\hspace{2cm}=\inf_{f_2\geq f,g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)+g_2(x)\,dx \\ &amp;amp;\hspace{2cm}=\int_{a}^{b}f(x)+g(x)\,dx,
				\end{align}
				which claims the Riemann integrability of $f+g$ on $[a,b]$.
			&lt;/li&gt;
		&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Monotonicity&lt;/b&gt;.&lt;br /&gt;
		Given $f$ and $g$, we obtain two consequential equations \eqref{eq:rip.1} and \eqref{eq:rip.2}. And since $f\leq g$ pointwise we have that
		\begin{equation}
		\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx\leq\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx
		\end{equation}
		or
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	&lt;/li&gt;
	&lt;li&gt;
		&lt;b&gt;Indicator&lt;/b&gt;.&lt;br /&gt;
		Given $E\subset [a,b]$ is Jordan measurable, we have
		\begin{equation}
		\sup_{A\subset E,A\text{ elementary}}m(A)=\inf_{B\supset E,B\text{ elementary}}m(B)=m(E)\label{eq:rip.3}
		\end{equation}
		Recall that we have proved that for any elementary set $E&apos;\subset[a,b]$, the indicator function $1_{E&apos;}:[a,b]\to\mathbb{R}$ is also piecewise constant with
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_{E&apos;}(x)\,dx=m(E&apos;)
		\end{equation}
		Moreover for any $A\subset E$, we have $1_A(x)\leq 1_E(x)$; and for any $B\supset E$, we have $1_B(x)\geq 1_E(x)$. Therefore the lower Darboux integral of $1_E$ on $[a,b]$ can be defined as
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\sup_{1_A\leq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_A(x)\,dx=\sup_{A\subset E,A\text{ elementary}}m(A)\label{eq:rip.4}
		\end{equation}
		And the upper Darboux integral of $1_E$ on $[a,b]$ can also be defined as
		\begin{equation}
		\overline{\int_{a}^{b}}1_E(x)\,dx=\inf_{1_B\geq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_B(x)\,dx=\inf_{B\supset E,B\text{ elementary}}m(B)\label{eq:rip.5}
		\end{equation}
		Combine \eqref{eq:rip.3}, \eqref{eq:rip.4} and \eqref{eq:rip.5}, we have
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\overline{\int_{a}^{b}}1_E(x)\,dx=m(E),
		\end{equation}
		which means $1_E$ is Darboux integrable on $[a,b]$ with the Darboux integrable $m(E)$. By the equivalence of Riemann and Darboux integral, $1_E$ is also Riemann integrable on $[a,b]$ with the Riemann integral
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These properties uniquely define the Riemann integral, in the sense that the functional $f\mapsto\int_{a}^{b}f(x)\,dx$ is the only map from the space of Riemann integrable functions on $[a,b]$ to $\mathbb{R}$ which obeys all of these above properties.&lt;/p&gt;

&lt;h3 id=&quot;riemann-int-area-interpret&quot;&gt;Area interpretation of the Riemann integral&lt;/h3&gt;
&lt;p&gt;Let $[a,b]$ be an interval, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff the sets $E_+\doteq\{(x,t):x\in[a,b];0\leq t\leq f(x)\}$ and $E_-\doteq\{(x,t):x\in[a,b];f(x)\leq t\leq 0\}$ are both Jordan measurable in $R^2$, in which case we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=m^2(E_+)-m^2(E_-),
\end{equation}
where $m^2$ denotes two-dimensional Jordan measure.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;taos-book&quot;&gt;Terence Tao. &lt;a href=&quot;https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/&quot;&gt;An introduction to measure theory&lt;/a&gt;. Graduate Studies in Mathematics, vol. 126.&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;span id=&quot;steins-book&quot;&gt;Elias M. Stein &amp;amp; Rami Shakarchi. &lt;a href=&quot;#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf&quot;&gt;Real Analysis: Measure Theory, Integration, and Hilbert Spaces&lt;/a&gt;. &lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function $f$ is said to be &lt;strong&gt;uniformly continuous&lt;/strong&gt; if for any real $\varepsilon&amp;gt;0$, there exists a real number $\delta&amp;gt;0$ such that for any $x, y$ with $d_1(x, y)&amp;lt;\delta$, we also have
\begin{equation*}
d_2(f(x),f(y))&amp;lt;\varepsilon
\end{equation*} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function $f:[a,b]\to\mathbb{R}$ is &lt;strong&gt;piecewise continuous&lt;/strong&gt; if we can partition $[a,b]$ into finitely many intervals, such that $f$ is continuous on each interval. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="jordan-measure" /><category term="riemann-integral" /><summary type="html">Part I of the measure theory series. Materials are mostly taken from Tao’s book, except for some needed notations extracted from Stein’s book.</summary></entry><entry><title type="html">Planning &amp;amp; Learning</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning.html" rel="alternate" type="text/html" title="Planning &amp;amp; Learning" /><published>2022-05-19T14:09:00+07:00</published><updated>2022-05-19T14:09:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/19/planning-learning.html">&lt;blockquote&gt;
  &lt;p&gt;Recall that when using &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html&quot;&gt;dynamic programming (DP) method&lt;/a&gt; in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html&quot;&gt;Monte Carlo methods&lt;/a&gt; and &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html&quot;&gt;temporal-difference learning&lt;/a&gt;, the models are unnecessary. Such methods with requirement of a model like the case of DP is called &lt;strong&gt;model-based&lt;/strong&gt;, while methods without using a model is called &lt;strong&gt;model-free&lt;/strong&gt;. Model-based methods primarily rely on &lt;strong&gt;planning&lt;/strong&gt;; and model-free methods, on the other hand, primarily rely on &lt;strong&gt;learning&lt;/strong&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#models-planning&quot;&gt;Models &amp;amp; Planning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#models&quot;&gt;Models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#planning&quot;&gt;Planning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dyna&quot;&gt;Dyna&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dyna-q&quot;&gt;Dyna-Q&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dyna-q-eg&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dyna-q-plus&quot;&gt;Dyna-Q+&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prioritized-sweeping&quot;&gt;Prioritized Sweeping&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#small-backups&quot;&gt;Small backups&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#heuristic-search&quot;&gt;Heuristic Search&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#preferences&quot;&gt;Preferences&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;models-planning&quot;&gt;Models &amp;amp; Planning&lt;/h2&gt;

&lt;h3 id=&quot;models&quot;&gt;Models&lt;/h3&gt;
&lt;p&gt;A &lt;strong&gt;model&lt;/strong&gt; of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.&lt;/p&gt;

&lt;p&gt;When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If a model produces a description of all possibilities and their probabilities, we call it &lt;strong&gt;distribution model&lt;/strong&gt;. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.&lt;/li&gt;
  &lt;li&gt;On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it &lt;strong&gt;sample model&lt;/strong&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to &lt;strong&gt;simulate&lt;/strong&gt; the environment in order to produce &lt;strong&gt;simulated experience&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;planning&quot;&gt;Planning&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Planning&lt;/strong&gt; in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;State-space planning&lt;/strong&gt; is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:
    &lt;ul&gt;
      &lt;li&gt;Involving computing value functions as a key intermediate step toward improving the policy.&lt;/li&gt;
      &lt;li&gt;Computing value functions by updates or backup applied to simulated experience.
  \begin{equation}
  \text{model}\xrightarrow[]{\hspace{1.5cm}}\text{simulated experience}\xrightarrow[]{\hspace{0.3cm}\text{backups}\hspace{0.3cm}}\text{backups}\xrightarrow[]{\hspace{1.5cm}}\text{policy}
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Plan-space planning&lt;/strong&gt; is a search through the space of plans.
    &lt;ul&gt;
      &lt;li&gt;Plan-space planning methods consist of &lt;strong&gt;evolutionary methods&lt;/strong&gt; and &lt;strong&gt;partial-order planning&lt;/strong&gt;, in which the ordering of steps is not completely determined at all states of planning.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.&lt;/p&gt;

&lt;p&gt;For instance, following is pseudocode of a planning method, called &lt;strong&gt;random-sample one-step tabular Q-planning&lt;/strong&gt;, based on &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#q-learning&quot;&gt;one-step tabular Q-learning&lt;/a&gt;, and on random samples from a sample model.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/rand-samp-one-step-q-planning.png&quot; alt=&quot;Random-sample one-step Q-planning&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;dyna&quot;&gt;Dyna&lt;/h2&gt;
&lt;p&gt;Within a planning agent, experience plays at least two roles:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;model learning&lt;/strong&gt;: improving the model;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;direct reinforcement learning (RL)&lt;/strong&gt;: improving the value function and policy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The figure below illustrates the possible relationships between experience, model, value functions and policy.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/exp-model-value-policy.png&quot; alt=&quot;Exp, model, values and policy relationships&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 300px; height: 250px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: The possible relationships between experience, model, values and policy&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called &lt;strong&gt;indirect RL&lt;/strong&gt;), which involved in planning.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;direct RL: simpler, not affected by bad models;&lt;/li&gt;
  &lt;li&gt;indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dyna-q&quot;&gt;Dyna-Q&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Dyna-Q&lt;/strong&gt; is the method having all of the processes shown in the diagram in &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt; - planning, acting, model-learning and direct RL - all occurring continually:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the &lt;em&gt;planning&lt;/em&gt; method is the random-sample one-step tabular Q-planning in the previous section;&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;direct RL&lt;/em&gt; method is the one-step tabular Q-learning;&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;model-learning&lt;/em&gt; method is also table-based and assumes the environment is deterministic.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;After each transition $S_t,A_t\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.&lt;/p&gt;

&lt;p&gt;During planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.&lt;/p&gt;

&lt;p&gt;Following is the general architecture of Dyna methods, of which Dyna-Q is an instance.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-arch.png&quot; alt=&quot;Dyna architecture&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 400px; height: 320px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: The general Dyna Architecture&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.&lt;/p&gt;

&lt;p&gt;Pseudocode of Dyna-Q method is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/tabular-dyna-q.png&quot; alt=&quot;Tabular Dyna-Q&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;dyna-q-eg&quot;&gt;Example&lt;/h4&gt;
&lt;p&gt;(This example is taken from &lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt; - example 8.1.)&lt;/p&gt;

&lt;p&gt;Consider a gridworld with some obstacles, called “maze” in this example, shown in the figure below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/dyna-maze.png&quot; alt=&quot;Dyna maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 400px; height: 200px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: The maze with some obstacles&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;As usual, four action, $\text{up}, \text{down}, \text{right}$ and $\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state.&lt;/p&gt;

&lt;p&gt;The task is discounted, episodic with $\gamma=0.95$.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-maze-dyna-q.png&quot; alt=&quot;Dyna maze solved with Dyna-Q&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: Using Dyna-Q with different setting of number of planning steps on the maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-08/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;dyna-q-plus&quot;&gt;Dyna-Q+&lt;/h3&gt;
&lt;p&gt;Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/blocking-maze.png&quot; alt=&quot;Blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: The maze before and after change&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps.&lt;/p&gt;

&lt;p&gt;In this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an &lt;strong&gt;exploration bonus&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment.&lt;/li&gt;
  &lt;li&gt;An special &lt;strong&gt;bonus reward&lt;/strong&gt; is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting:
\begin{equation}
r+\kappa\sqrt{\tau},
\end{equation}
for a small (time weight) $\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\tau$ time steps.&lt;/li&gt;
  &lt;li&gt;The agent actually plans how to visit long unvisited state-action pairs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/blocking-maze-dyna-q-qplus.png&quot; alt=&quot;Dyna-Q, Dyna-Q+ on blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Average performance of Dyna-Q and Dyna-Q+ on blocking maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-08/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/shortcut-maze.png&quot; alt=&quot;shortcut maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: The maze before and after change&lt;br /&gt;(the figure is taken from &lt;span&gt;&lt;a href=&quot;#rl-book&quot;&gt;RL book&lt;/a&gt;&lt;/span&gt;)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/shortcut-maze-dyna-q-qplus.png&quot; alt=&quot;Dyna-Q, Dyna-Q+ on blocking maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 8&lt;/b&gt;: Average performance of Dyna-Q and Dyna-Q+ on shortcut maze.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-08/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).&lt;/p&gt;

&lt;p&gt;The reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.&lt;/p&gt;

&lt;h2 id=&quot;prioritized-sweeping&quot;&gt;Prioritized Sweeping&lt;/h2&gt;
&lt;p&gt;Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.&lt;/p&gt;

&lt;p&gt;Pseudocode of prioritized sweeping is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-19/prioritized-sweeping.png&quot; alt=&quot;Prioritized sweeping&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2022-05-19/dyna-maze-prioritized-sweeping.png&quot; alt=&quot;Prioritized sweeping on dyna maze&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 9&lt;/b&gt;: Using prioritized sweeping on mazes.&lt;br /&gt;The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-08/maze.py&quot;&gt;here&lt;/a&gt;.&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;trajectory-sampling&quot;&gt;Trajectory Sampling&lt;/h2&gt;

&lt;h2 id=&quot;heuristic-search&quot;&gt;Heuristic Search&lt;/h2&gt;

&lt;h2 id=&quot;preferences&quot;&gt;Preferences&lt;/h2&gt;
&lt;p&gt;[1] &lt;span id=&quot;rl-book&quot;&gt;Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;[2] Richard S. Sutton. &lt;a href=&quot;https://doi.org/10.1016/B978-1-55860-141-3.50030-4&quot;&gt;Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming&lt;/a&gt;. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.&lt;/p&gt;

&lt;p&gt;[3] Harm van Seijen &amp;amp; Richard S. Sutton. &lt;a href=&quot;https://proceedings.mlr.press/v28/vanseijen13.pdf&quot;&gt;Efficient planning in MDPs by small backups&lt;/a&gt;. Proceedings
of the 30th International Conference on Machine Learning (ICML 2013).&lt;/p&gt;

&lt;p&gt;[3] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="planning" /><category term="learning" /><category term="dyna" /><category term="q-learning" /><category term="mcts" /><category term="my-rl" /><summary type="html">Recall that when using dynamic programming (DP) method in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with Monte Carlo methods and temporal-difference learning, the models are unnecessary. Such methods with requirement of a model like the case of DP is called model-based, while methods without using a model is called model-free. Model-based methods primarily rely on planning; and model-free methods, on the other hand, primarily rely on learning.</summary></entry><entry><title type="html">Policy Gradient Methods</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient.html" rel="alternate" type="text/html" title="Policy Gradient Methods" /><published>2022-05-04T14:00:00+07:00</published><updated>2022-05-04T14:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/04/policy-gradient.html">&lt;blockquote&gt;
  &lt;p&gt;So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a &lt;strong&gt;parameterized policy&lt;/strong&gt;, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called &lt;strong&gt;policy gradient methods&lt;/strong&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-grad-ep&quot;&gt;Policy Gradient for Episodic Problems&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-grad-theorem-ep&quot;&gt;The Policy Gradient Theorem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforce&quot;&gt;REINFORCE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reinforce-baseline&quot;&gt;REINFORCE with Baseline&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#actor-critic-methods&quot;&gt;Actor-Critic Methods&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-grad-cont&quot;&gt;Policy Gradient for Continuing Problems&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-grad-theorem-cont&quot;&gt;The Policy Gradient Theorem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#policy-prm-cont-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;policy-grad-ep&quot;&gt;Policy Gradient for Episodic Problems&lt;/h2&gt;
&lt;p&gt;We begin by considering episodic case, for which we define the performance measure $J(\boldsymbol{\theta})$ as the value of the start state of the episode. By assuming without loss of generality that every episode starts in some particular state $s_0$, we have:
\begin{equation}
J(\boldsymbol{\theta})\doteq v_{\pi_\boldsymbol{\theta}}(s_0),
\end{equation}
where $v_{\pi_\boldsymbol{\theta}}$ is the true value function for $\pi_\boldsymbol{\theta}$, the policy determined by $\boldsymbol{\theta}$.&lt;/p&gt;

&lt;h3 id=&quot;policy-grad-theorem-ep&quot;&gt;The Policy Gradient Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
The policy gradient theorem for the episodic case establishes that
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}),\tag{1}\label{1}
\end{equation}
where $\pi$ represents the policy corresponding to parameter vector $\boldsymbol{\theta}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written in terms of the action-value function, for any $s\in\mathcal{S}$, as:
\begin{align}
\nabla_\boldsymbol{\theta}v_\pi(s)&amp;amp;=\nabla_\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(s|a)q_\pi(a,s)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s’,r}p(s’,r|s,a)\big(r+v_\pi(s’)\big)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\sum_{a’}\big(\nabla_\boldsymbol{\theta}\pi(s’|a’,\boldsymbol{\theta})q_\pi(s’,a’) \\ &amp;amp;\hspace{2cm}+\pi(a’|s’,\boldsymbol{\theta})\sum_{s&apos;&apos;}p(s&apos;&apos;\vert s’,a’)\nabla_\boldsymbol{\theta}v_\pi(s&apos;&apos;)\big)\Big] \\ &amp;amp;=\sum_{x\in\mathcal{S}}\sum_{k=0}^{\infty}P(s\to x,k,\pi)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
After repeated unrolling as in the fifth step, where $P(s\to x,k,\pi)$ is the probability of transitioning from state $s$ to state $x$ in $k$ steps under policy $\pi$. It is then immediate that:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}v_\pi(s_0) \\ &amp;amp;=\sum_s\Big(\sum_{k=0}^{\infty}P(s_0\to s,k,\pi)\Big)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_s\eta(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_{s’}\eta(s’)\sum_s\frac{\eta(s)}{\sum_{s’}\eta(s’)}\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;=\sum_{s’}\eta(s’)\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\propto\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a),
\end{align}
where $\eta(s)$ denotes the number of time steps spent, on average, in state $s$ in a single episode:
\begin{equation}
\eta(s)=h(s)+\sum_{\bar{s}}\eta(\bar{s})\sum_a\pi(a|s,\boldsymbol{\theta})p(s|\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
where $h(s)$ denotes the probability that an episode begins in each state $s$; $\bar{s}$ denotes a preceding state of $s$. This leads to the result that we have used in the fifth step:
\begin{equation}
\mu(s)=\frac{\eta(s)}{\sum_{s’}\eta(s’)},\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;reinforce&quot;&gt;REINFORCE&lt;/h3&gt;
&lt;p&gt;Notice that in &lt;strong&gt;Theorem 1&lt;/strong&gt;, the right-hand side is a sum over states weighted by how often the states occur (distributed by $\mu(s)$) under the target policy $\pi$. Therefore, we can rewrite \eqref{1} as:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;\propto\sum_s\mu(s)\sum_a q_\pi(s,a)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta}) \\ &amp;amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right]\tag{2}\label{2}
\end{align}
Using SGD on maximizing $J(\boldsymbol{\theta})$ gives us the update rule:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\sum_a\hat{q}(S_t,a,\mathbf{w})\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta}),
\end{equation}
where $\hat{q}$ is some learned approximation to $q_\pi$ with $\mathbf{w}$ denoting the weight vector of its as usual. This algorithm is called &lt;strong&gt;all-actions&lt;/strong&gt; method because its update involves all of the actions.&lt;/p&gt;

&lt;p&gt;Continue our derivation in \eqref{2}, we have:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\mathbb{E}_\pi\left[\sum_a q_\pi(S_t,a)\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})\right] \\ &amp;amp;=\mathbb{E}_\pi\left[\sum_a\pi(a|S_t,\boldsymbol{\theta})q_\pi(S_t,a)\frac{\nabla_\boldsymbol{\theta}\pi(a|S_t,\boldsymbol{\theta})}{\pi(a|S_t,\boldsymbol{\theta})}\right] \\ &amp;amp;=\mathbb{E}_\pi\left[q_\pi(S_t,A_t)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right] \\ &amp;amp;=\mathbb{E}_\pi\left[G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta}}\right],
\end{align}
where $G_t$ is the return as usual; in the third step, we have replaced $a$ by the sample $A_t\sim\pi$; and in the fourth step, we have used the identity
\begin{equation}
\mathbb{E}_\pi\left[G_t|S_t,A_t\right]=q_\pi(S_t,A_t)
\end{equation}
With this gradient, we have the SGD update for time step $t$, called the &lt;strong&gt;REINFORCE&lt;/strong&gt; update, is then:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha G_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{3}\label{3}
\end{equation}
Pseudocode of the algorithm is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/reinforce.png&quot; alt=&quot;REINFORCE&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The vector
\begin{equation}
\frac{\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})}{\pi(a|s,\boldsymbol{\theta})}=\nabla_\boldsymbol{\theta}\ln\pi(a|s,\boldsymbol{\theta})
\end{equation}
in \eqref{3} is called the &lt;strong&gt;eligibility vector&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Consider using &lt;strong&gt;soft-max in action preferences&lt;/strong&gt; with linear action preferences, which means that:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\dfrac{\exp\Big[h(s,a,\boldsymbol{\theta})\Big]}{\sum_b\exp\Big[h(s,b,\boldsymbol{\theta})\Big]},
\end{equation}
where the preferences $h(s,a,\boldsymbol{\theta})$ is defined as:
\begin{equation}
h(s,a,\boldsymbol{\theta})=\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)
\end{equation}
Using the chain rule we can rewrite the eligibility vector as:
\begin{align}
\nabla_\boldsymbol{\theta}\ln\pi(a|s,\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}\ln{\frac{\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)\Big]}{\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big]}} \\ &amp;amp;=\nabla_\boldsymbol{\theta}\Big(\boldsymbol{\theta}^\text{T}\mathbf{x}(s,a)\Big)-\nabla_\boldsymbol{\theta}\ln\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big] \\ &amp;amp;=\mathbf{x}(s,a)-\dfrac{\sum_b\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b)\Big]\mathbf{x}(s,b)}{\sum_{b’}\exp\Big[\boldsymbol{\theta}^\text{T}\mathbf{x}(s,b’)\Big]} \\ &amp;amp;=\mathbf{x}(s,a)-\sum_b\pi(b|s,\boldsymbol{\theta})\mathbf{x}(s,b)
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;reinforce-baseline&quot;&gt;REINFORCE with Baseline&lt;/h3&gt;
&lt;p&gt;The policy gradient theorem \eqref{1} can be generalized to include a comparison of the action value to an arbitrary &lt;em&gt;baseline&lt;/em&gt; $b(s)$:
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})\propto\sum_s\mu(s)\sum_a\Big(q_\pi(s,a)-b(s)\Big)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})\tag{4}\label{4}
\end{equation}
The baseline can be any function, even a r.v, as long as it is independent with $a$. The equation is valid because:
\begin{align}
\sum_a b(s)\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})&amp;amp;=b(s)\nabla_\boldsymbol{\theta}\sum_a\pi(a|s,\boldsymbol{\theta}) \\ &amp;amp;=b(s)\nabla_\boldsymbol{\theta}1=0
\end{align}
Using the derivation steps analogous to REINFORCE, we end up with another version of REINFORCE that includes a general baseline:
\begin{equation}
\boldsymbol{\theta}_{t+1}\doteq\boldsymbol{\theta}_t+\alpha\Big(G_t-b(s)\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{5}\label{5}
\end{equation}
One natural baseline choice is the estimate of the state value, $\hat{v}(S_t,\mathbf{w})$, with $\mathbf{w}\in\mathbb{R}^d$ is the weight vector of its. Using this baseline, we have pseudocode of the generalization with baseline of REINFORCE algorithm \eqref{5} given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/reinforce-baseline.png&quot; alt=&quot;REINFORCE with Baseline&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;actor-critic-methods&quot;&gt;Actor-Critic Methods&lt;/h3&gt;
&lt;p&gt;In Reinforcement Learning, methods that learn both policy and value function at the same time are called &lt;strong&gt;actor-critic methods&lt;/strong&gt;, in which &lt;strong&gt;actor&lt;/strong&gt; refers to the learned policy and &lt;strong&gt;critic&lt;/strong&gt; is a reference to the learned value function. Although the REINFORCE with Baseline method in the previous section learns both policy and value function, but it is not an actor-critic method. Because its state-value function is used as a baseline, not as a critic, which is used for bootstrapping.&lt;/p&gt;

&lt;p&gt;We begin by considering one-step actor-critic methods. One-step actor-critic methods replace the full return, $G_t$, of REINFORCE \eqref{5} with the one-step return, $G_{t:t+1}$:
\begin{align}
\boldsymbol{\theta}_{t+1}&amp;amp;\doteq\boldsymbol{\theta}_t+\alpha\Big(G_{t:t+1}-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}\tag{6}\label{6} \\ &amp;amp;=\boldsymbol{\theta}_t+\alpha\Big(R_{t+1}+\hat{v}(S_{t+1},\mathbf{w})-\hat{v}(S_t,\mathbf{w})\Big)\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})} \\ &amp;amp;=\boldsymbol{\theta}_t+\alpha\delta_t\frac{\nabla_\boldsymbol{\theta}\pi(A_t|S_t,\boldsymbol{\theta})}{\pi(A_t|S_t,\boldsymbol{\theta})}
\end{align}
The natural state-value function learning method to pair with this is semi-gradient TD(0), which produces the pseudocode given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/one-step-actor-critic.png&quot; alt=&quot;One-step Actor-Critic&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;To generalize the one-step methods to the forward view of $n$-step methods and then to $\lambda$-return, in \eqref{6}, we simply replace the one-step return, $G_{t+1}$, by the $n$-step return, $G_{t:t+n}$, and the $\lambda$-return, $G_t^\lambda$, respectively.&lt;/p&gt;

&lt;p&gt;In order to obtain the backward view of the $\lambda$-return algorithm, we use separately eligible traces for the actor and critic, as in the pseudocode given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-05-04/actor-critic-eligible-traces.png&quot; alt=&quot;Actor-Critic with Eligible Traces&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;policy-grad-cont&quot;&gt;Policy Gradient with Continuing Problems&lt;/h2&gt;
&lt;p&gt;In the continuing tasks, we define the performance measure in terms of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#avg-reward&quot;&gt;average-reward&lt;/a&gt;, as:
\begin{align}
J(\boldsymbol{\theta})\doteq r(\pi)&amp;amp;\doteq\lim_{h\to\infty}\frac{1}{h}\sum_{t=1}^{h}\mathbb{E}\Big[R_t\big|S_0,A_{0:1}\sim\pi\Big] \\ &amp;amp;=\lim_{t\to\infty}\mathbb{E}\Big[R_t|S_0,A_{0:1}\sim\pi\Big] \\ &amp;amp;=\sum_s\mu(s)\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)r,\tag{7}\label{7}
\end{align}
where $\mu$ is the steady-state distribution under $\pi$, $\mu(s)\doteq\lim_{t\to\infty}P(S_t=s|A_{0:t}\sim\pi)$ which is assumed to exist and to be independent of $S_0$; and we also have that:
\begin{equation}
\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s’|s,a)=\mu(s’),\hspace{1cm}\forall s’\in\mathcal{S}
\end{equation}
Recall that in continuing tasks with average-reward setting, we use the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#differential-return&quot;&gt;differential return&lt;/a&gt;, which is defined in terms of differences between rewards and the average reward:
\begin{equation}
G_t\doteq R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+R_{t+3}-r(\pi)+\dots\tag{8}\label{8}
\end{equation}
And thus, we also use the differential version of value functions, which are defined as usual except that they use the differential return \eqref{8}:
\begin{align}
v_\pi(s)&amp;amp;\doteq\mathbb{E}_\pi\left[G_t|S_t=s\right] \\ q_\pi(s,a)&amp;amp;\doteq\mathbb{E}_\pi\left[G_t|S_t=s,A_t=s\right]
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;policy-grad-theorem-cont&quot;&gt;The Policy Gradient Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt;&lt;br /&gt;
The policy gradient theorem for continuing case with average-reward states that
\begin{equation}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s)q_\pi(s,a)
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
We have that the gradient of the state-value function w.r.t $\boldsymbol{\theta}$ can be written, for any $s\in\mathcal{S}$, as:
\begin{align}
\nabla_\boldsymbol{\theta}v_\pi(s)&amp;amp;=\boldsymbol{\theta}\Big[\sum_a\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)\Big],\hspace{1cm}\forall s\in\mathcal{S} \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}q_\pi(s,a)\Big] \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\nabla_\boldsymbol{\theta}\sum_{s’,r}p(s’,r|s,a)\big(r-r(\boldsymbol{\theta})+v_\pi(s’)\big)\Big] \\ &amp;amp;=\sum_a\Bigg[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\Big[-\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta})+\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]\Bigg]
\end{align}
Thus, the gradient of the performance measure w.r.t $\boldsymbol{\theta}$ is:
\begin{align}
\nabla_\boldsymbol{\theta}J(\boldsymbol{\theta})&amp;amp;=\nabla_\boldsymbol{\theta}r(\boldsymbol{\theta}) \\ &amp;amp;=\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\Bigg(\sum_a\Big[\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)\Big]-\nabla_\boldsymbol{\theta}v_\pi(s)\Bigg) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})\sum_{s’}p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a) \\ &amp;amp;\hspace{2cm}+\sum_{s’}\sum_s\mu(s)\sum_a\pi(a|s,\boldsymbol{\theta})p(s’|s,a)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)+\sum_{s’}\mu(s’)\nabla_\boldsymbol{\theta}v_\pi(s’)-\sum_s\mu(s)\nabla_\boldsymbol{\theta}v_\pi(s) \\ &amp;amp;=\sum_s\mu(s)\sum_a\nabla_\boldsymbol{\theta}\pi(a|s,\boldsymbol{\theta})q_\pi(s,a)
\end{align}&lt;/p&gt;

&lt;h2 id=&quot;policy-prm-cont-actions&quot;&gt;Policy Parameterization for Continuous Actions&lt;/h2&gt;
&lt;p&gt;For tasks having continuous action space with an infinite number of actions, instead of computing learned probabilities for each action, we can learn statistics of the probability distribution.&lt;/p&gt;

&lt;p&gt;In particular, to produce a policy parameterization, the policy can be defined as the &lt;a href=&quot;/mathematics/probability-statistics/2021/11/22/normal-dist.html&quot;&gt;Normal distribution&lt;/a&gt; over a real-valued scalar action, with mean and standard deviation given by parametric function approximators that depend on the state, as given:
\begin{equation}
\pi(a|s,\boldsymbol{\theta})\doteq\frac{1}{\sigma(s,\boldsymbol{\theta})\sqrt{2\pi}}\exp\left(-\frac{(a-\mu(s,\boldsymbol{\theta}))^2}{2\sigma(s,\boldsymbol{\theta})^2}\right),
\end{equation}
where $\mu:\mathcal{S}\times\mathbb{R}^{d’}\to\mathbb{R}$ and $\sigma:\mathcal{S}\times\mathbb{R}^{d’}\to\mathbb{R}^+$ are two parameterized function approximators.&lt;/p&gt;

&lt;p&gt;We continue by dividing the policy’s parameter vector, $\boldsymbol{\theta}=[\boldsymbol{\theta}_\mu, \boldsymbol{\theta}_\sigma]^\text{T}$, into two parts: one part, $\boldsymbol{\theta}_\mu$, is used for the approximation of the mean and the other, $\boldsymbol{\theta}_\sigma$, is used for the approximation of the standard deviation.&lt;/p&gt;

&lt;p&gt;The mean, $\mu$, can be approximated as a linear function, while the standard deviation, $\sigma$, must always be positive, which should be approximated as the exponential of a linear function, as:
\begin{align}
\mu(s,\boldsymbol{\theta})&amp;amp;\doteq\boldsymbol{\theta}_\mu^\text{T}\mathbf{x}_\mu(s) \\ \sigma(s,\boldsymbol{\theta})&amp;amp;\doteq\exp\Big(\boldsymbol{\theta}_\sigma^\text{T}\mathbf{x}_\sigma(s)\Big),
\end{align}
where $\mathbf{x}_\mu(s)$ and $\mathbf{x}_\sigma(s)$ are state feature vectors corresponding to each approximator.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Deepmind x UCL. &lt;a href=&quot;https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021&quot;&gt;Reinforcement Learning Lecture Series 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Richard S. Sutton &amp;amp; David McAllester &amp;amp; Satinder Singh &amp;amp; Yishay Mansour. &lt;a href=&quot;https://papers.nips.cc/paper/1999/hash/464d828b85b0bed98e80ade0a5c43b0f-Abstract.html&quot;&gt;Policy Gradient Methods for Reinforcement Learning with Function Approximation&lt;/a&gt;. NIPS 1999.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="policy-gradient" /><category term="actor-critic" /><category term="function-approximation" /><category term="my-rl" /><summary type="html">So far in the series, we have been choosing the actions based on the estimated action value function. On the other hand, we can instead learn a parameterized policy, $\boldsymbol{\theta}$, that can select actions without consulting a value function by updating $\boldsymbol{\theta}$ on each step in the direction of an estimate of the gradient of some performance measure w.r.t $\boldsymbol{\theta}$. Such methods are called policy gradient methods.</summary></entry><entry><title type="html">The exponential family</title><link href="http://localhost:4000/mathematics/probability-statistics/2022/05/04/exponential-family.html" rel="alternate" type="text/html" title="The exponential family" /><published>2022-05-04T14:00:00+07:00</published><updated>2022-05-04T14:00:00+07:00</updated><id>http://localhost:4000/mathematics/probability-statistics/2022/05/04/exponential-family</id><content type="html" xml:base="http://localhost:4000/mathematics/probability-statistics/2022/05/04/exponential-family.html">&lt;blockquote&gt;
  &lt;p&gt;A note on the exponential family.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#exp-fam&quot;&gt;The exponential family&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples&quot;&gt;Examples&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bern&quot;&gt;Bernoulli distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bin&quot;&gt;Binomial distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mult&quot;&gt;Multinomial distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pois&quot;&gt;Poisson distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gauss&quot;&gt;Gaussian distribution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mvn&quot;&gt;Multivariate Normal distribution&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cvxt&quot;&gt;Convexity&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;mmt-suff-stat&quot;&gt;Moments of sufficient statistic&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mean-var&quot;&gt;Means, variances&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mgf&quot;&gt;Moment generating functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cgf&quot;&gt;Cumulant generating functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cumulants&quot;&gt;Cumulants&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sufficiency&quot;&gt;Sufficiency&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mle&quot;&gt;Maximum likelihood estimates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;conj-prior&quot;&gt;Conjugate priors&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;exp-fam&quot;&gt;The exponential family&lt;/h2&gt;
&lt;p&gt;The &lt;strong&gt;exponential family&lt;/strong&gt; of distributions is defined as family of distributions of form
\begin{equation}
p(x;\eta)=h(x)\exp\Big[\eta^\text{T}T(x)-A(\eta)\Big],\label{eq:ef.1}
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\eta$ is known as the &lt;strong&gt;natural parameter&lt;/strong&gt;, or &lt;strong&gt;canonical parameter&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;$T(X)$ is referred to as a &lt;strong&gt;sufficient statistic&lt;/strong&gt;,&lt;/li&gt;
  &lt;li&gt;$A(\eta)$ is called the &lt;strong&gt;cumulant function&lt;/strong&gt;, which can be view as the logarithm of a normalization factor since integrating \eqref{eq:ef.1} w.r.t the measure $\nu$ gives us
\begin{equation}
A(\eta)=\log\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx),\label{eq:ef.2}
\end{equation}
This also implies that $A(\eta)$ will be determined once we have specified $\nu,T(x)$ and $h(x)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The set of parameters $\eta$ for which the integral in \eqref{eq:ef.2} is finite is known as the &lt;strong&gt;natural parameter space&lt;/strong&gt;
\begin{equation}
N=\left\{\eta:\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx)&amp;lt;\infty\right\}
\end{equation}
which explains why $\eta$ is also referred as &lt;strong&gt;natural parameter&lt;/strong&gt;. If $N$ is an non-empty open set, the exponential families are said to be &lt;strong&gt;regular&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An exponential family is known as &lt;strong&gt;minimal&lt;/strong&gt; if there are no linear constraints among the components of $\eta$ nor are there linear constraints among the components of $T(x)$.&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Each particular choice of $\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\eta$. As we vary $\eta$, we then get different distributions within this family.&lt;/p&gt;

&lt;h3 id=&quot;bern&quot;&gt;Bernoulli distribution&lt;/h3&gt;
&lt;p&gt;The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\sim\text{Bern}(\pi)$, is given by
\begin{align}
p(x;\pi)&amp;amp;=\pi^x(1-\pi)^{1-x} \\ &amp;amp;=\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &amp;amp;=\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which can be written in the form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;amp;=\frac{\pi}{1-\pi} \\ T(x)&amp;amp;=x \\ A(\eta)&amp;amp;=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&amp;amp;=1
\end{align}
Notice that the relationship between $\eta$ and $\pi$ is invertible since
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}},
\end{equation}
which is the &lt;strong&gt;sigmoid function&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;bin&quot;&gt;Binomial distribution&lt;/h3&gt;
&lt;p&gt;The probability mass function of a Binomial random variable $X$, denoted as $X\sim\text{Bin}(N,\pi)$, is defined as
\begin{align}
p(x;N,\pi)&amp;amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\pi^{x}(1-\pi)^{1-x} \\ &amp;amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &amp;amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which is in form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;amp;=\frac{\pi}{1-\pi} \\ T(x)&amp;amp;=x \\ A(\eta)&amp;amp;=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&amp;amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)
\end{align}
Similar to the Bernoulli case, we also have the invertible relationship between $\eta$ and $\pi$ as
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;pois&quot;&gt;Poisson distribution&lt;/h3&gt;
&lt;p&gt;The probability mass function of a Poisson random variable $X$, denoted as $X\sim\text{Pois}(\lambda)$, is given as
\begin{align}
p(x;\lambda)&amp;amp;=\frac{\lambda^x e^{-\lambda}}{x!} \\ &amp;amp;=\frac{1}{x!}\exp\left(x\log\lambda-\lambda\right),
\end{align}
which is also able to be written as an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;amp;=\log\lambda \\ T(x)&amp;amp;=x \\ A(\eta)&amp;amp;=\lambda=e^{\eta} \\ h(x)&amp;amp;=\frac{1}{x!}
\end{align}
Analogy to Bernoulli distribution, we also have that
\begin{equation}
\lambda=e^{\eta}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;gauss&quot;&gt;Gaussian distribution&lt;/h3&gt;
&lt;p&gt;The (univariate) Gaussian density of a random variable $X$, denoted as $X\sim\mathcal{N}(\mu,\sigma^2)$, is given by
\begin{align}
p(x;\mu,\sigma^2)&amp;amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ &amp;amp;=\frac{1}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right],
\end{align}
which allows us to write it as an instance of the exponential family with
\begin{align}
\eta&amp;amp;=\left[\begin{matrix}\mu/\sigma^2 \\ -1/2\sigma^2\end{matrix}\right] \\ T(x)&amp;amp;=\left[\begin{matrix}x\\ x^2\end{matrix}\right] \\ A(\eta)&amp;amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2) \\ h(x)&amp;amp;=\frac{1}{\sqrt{2\pi}}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;mult&quot;&gt;Multinomial distribution&lt;/h3&gt;
&lt;p&gt;Let $\mathbf{X}=(X_1,\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\mathbf{\pi}=(\pi_1,\ldots,\pi_K)$ with $\sum_{k=1}^{K}\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.&lt;/p&gt;

&lt;p&gt;Then $\mathbf{X}$ is said to have Multinomial distribution, denoted as $\mathbf{X}\sim\text{Mult}_K(N,\boldsymbol{\pi})$, if its probability mass function is given as with $\sum_{k=1}^{K}x_k=1$
\begin{align}
p(\mathbf{x};\boldsymbol{\pi},N,K)&amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\pi_1^{x_1}\pi_2^{x_2}\ldots\pi_n^{x_n} \\ &amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right)\label{eq:m.1}
\end{align}
It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\mathbf{x})$, which is
\begin{equation}
\sum_{k=1}^{K}x_k=1
\end{equation}
In order to remove this constraint, we substitute $1-\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \eqref{eq:m.1} be written by
\begin{align}
\hspace{-0.5cm}p(\mathbf{x};\boldsymbol{\pi},N,K)&amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right) \\ &amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{k=1}^{K-1}x_k\log\pi_k+\left(1-\sum_{k=1}^{K-1}x_k\right)\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right] \\ &amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{i=1}^{K-1}\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)x_i+\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right]\label{eq:m.2}
\end{align}
With this representation, and also for convenience, for $i=1,\ldots,K$ we continue by letting
\begin{equation}
\eta_i=\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)=\log\left(\frac{\pi_i}{\pi_K}\right)\label{eq:m.3}
\end{equation}
Take the exponential of both sides and summing over $K$, we have
\begin{equation}
\sum_{i=1}^{K}e^{\eta_i}=\frac{\sum_{i=1}^{K}\pi_i}{\pi_K}=\frac{1}{\pi_K}\label{eq:m.4}
\end{equation}
From this result, we have that the Multinomial distribution \eqref{eq:m.2} is therefore also a member of the exponential family with
\begin{align}
\eta&amp;amp;=\left[\begin{matrix}\log\left(\pi_1/\pi_K\right) \\ \vdots \\ \log\left(\pi_K/\pi_K\right)\end{matrix}\right] \\ T(\mathbf{x})&amp;amp;=\left[\begin{matrix}x_1,\ldots,x_K\end{matrix}\right]^\text{T} \\ A(\eta)&amp;amp;=-\log\left(1-\sum_{i=1}^{K-1}\pi_i\right)=-\log(\pi_K)=\log\left(\sum_{k=1}^{K}e^{\eta_k}\right) \\ h(\mathbf{x})&amp;amp;=\frac{N!}{x_1!x_2!\ldots x_K!}
\end{align}
Additionally, substituting the result \eqref{eq:m.4} into \eqref{eq:m.3} gives us for $i=1,\ldots,K$
\begin{equation}
\eta_i=\log\left(\pi_i\sum_{k=1}^{K}e^{\eta_k}\right),
\end{equation}
or we can express $\boldsymbol{\pi}$ in terms of $\eta$ by
\begin{equation}
\pi_i=\frac{e^{\eta_i}}{\sum_{k=1}^{K}e^{\eta_k}},
\end{equation}
which is the &lt;strong&gt;softmax function&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mvn&quot;&gt;Multivariate Normal distribution&lt;/h3&gt;

&lt;h2 id=&quot;cvxt&quot;&gt;Convexity&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;br /&gt;
The natural space $N$ is a convex set and the cumulant function $A(\eta)$ is a convex function. If the family is minimal, then $A(\eta)$ is strictly convex.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $\eta_1,\eta_2\in N$, thus from \eqref{eq:ef.2}, we have that
\begin{align}
\exp\big(A(\eta_1)\big)&amp;amp;=A_1, \\ \exp\big(A(\eta_2)\big)&amp;amp;=A_2
\end{align}
where $A_1,A_2$ are finite.&lt;/p&gt;

&lt;p&gt;To prove that $N$ is convex, we need to show that for any $\eta=\lambda\eta_1+(1-\lambda)\eta_2$ for $0\lt\lambda\lt 1$, we also have $\eta\in N$. From \eqref{eq:ef.2}, and by &lt;strong&gt;Hölder’s inequality&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, we have
\begin{align}
\exp\big(A(\eta)\big)&amp;amp;=\int h(x)\exp\big(\eta^\text{T}T(x)\big)\nu(dx) \\ &amp;amp;=\int h(x)\exp\Big[\big(\lambda\eta_1+(1-\lambda)\eta_2\big)^\text{T}T(x)\Big]\nu(dx) \\ &amp;amp;=\int \Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda}\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}\nu(dx) \\ &amp;amp;\leq\Bigg[\int h(x)\exp\big(\eta_1^\text{T}T(x)\big)\nu(dx)\Bigg]^\lambda\Bigg[\int h(x)\exp\big(\eta_2^\text{T}T(x)\big)\nu(dx)\Bigg]^{1-\lambda} \\ &amp;amp;=\Big[\exp\big(A(\eta_1)\big)\Big]^\lambda\Big[\exp\big(A(\eta_2)\big)\Big]^{1-\lambda} \\ &amp;amp;=A_1^\lambda A_2^{1-\lambda},\label{eq:c.1}
\end{align}
which proves that $A(\eta)$ is finite, or $\eta\in N$.&lt;/p&gt;

&lt;p&gt;Moreover, taking logarithm of both sides of \eqref{eq:c.1} gives us
\begin{equation}
\lambda A(\eta_1)+(1-\lambda)A(\eta_2)\geq A(\eta)=A\big(\lambda\eta_1+(1-\lambda)\eta_2\big),
\end{equation}
which also claims the convexity of $A(\eta)$.&lt;/p&gt;

&lt;p&gt;By Hölder’s inequality, the equality in \eqref{eq:c.1} holds when
\begin{equation}
\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}=c\Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda(1/\lambda-1)}
\end{equation}
or
\begin{equation}
\exp\big(\eta_2^\text{T}T(x)\big)=c\exp\big(\eta_1^\text{T}T(x)\big),
\end{equation}
and therefore
\begin{equation}
(\eta_2-\eta_1)^\text{T}T(x)=\log c,
\end{equation}
which is not minimal since $\eta_1,\eta_2$ are taken arbitrarily.&lt;/p&gt;

&lt;h2 id=&quot;mmt-suff-stat&quot;&gt;Moments of sufficient statistic&lt;/h2&gt;
&lt;p&gt;In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second &lt;strong&gt;cumulants&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;mean-var&quot;&gt;Means, variances&lt;/h3&gt;
&lt;p&gt;Let us first consider the first derivative of the cumulant function $A(\eta)$. By the &lt;strong&gt;dominated convergence theorem&lt;/strong&gt;, we have
\begin{align}
\frac{\partial A(\eta)}{\partial\eta^\text{T}}&amp;amp;=\frac{\partial}{\partial\eta^\text{T}}\log\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx) \\ &amp;amp;=\frac{\int T(x)\exp\big(\eta^\text{T}(x)\big)h(x)\nu(dx)}{\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx)} \\ &amp;amp;=\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx)\label{eq:mv.1} \\ &amp;amp;=\int T(x)p(x;\eta)\nu(dx) \\ &amp;amp;=\mathbb{E}[T(X)],
\end{align}
which is the mean of the sufficient statistic $T(x)$.&lt;/p&gt;

&lt;p&gt;Moreover, taking the second derivative of cumulant function by continuing with the result \eqref{eq:mv.1}, we have
\begin{align}
\frac{\partial^2 A(\eta)}{\partial\eta\partial\eta^\text{T}}&amp;amp;=\frac{\partial}{\partial\eta^\text{T}}\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;amp;=\int T(x)\left(T(x)-\frac{\partial}{\partial\eta^\text{T}}A(\eta)\right)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;amp;=\int T(x)\big(T(x)-E(T(X))\big)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;amp;=\mathbb{E}\left[T(X)T(X)^\text{T}\right]-\mathbb{E}[T(X)]\mathbb{E}[T(X)]^\text{T} \\ &amp;amp;=\text{Var}[T(X)],
\end{align}
which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$.&lt;/p&gt;

&lt;h3 id=&quot;mgf&quot;&gt;Moment generating functions&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;moment generating function&lt;/strong&gt; (or &lt;strong&gt;MGF&lt;/strong&gt;) of a random variable $X$, denoted as $M(t)$, is given by
\begin{equation}
M(t)=\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.&lt;/p&gt;

&lt;p&gt;The MGF of the sufficient statistic $T(X)$ then can be computed as
\begin{align}
M_{T(X)}(t)&amp;amp;=\mathbb{E}(e^{t^\text{T}T(X)}) \\ &amp;amp;=\int \exp\big((\eta+t)^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;amp;=\exp\big(A(\eta+t)-A(\eta)\big)\label{eq:mgf.1}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;cgf&quot;&gt;Cumulant generating functions&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;cumulant generating function&lt;/strong&gt; (or &lt;strong&gt;CGF&lt;/strong&gt;) of a random variable $X$, denoted by $K(t)$, is given as
\begin{equation}
K(t)=\log M(t)=\log\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.&lt;/p&gt;

&lt;p&gt;From the MGF of $T(X)$ in \eqref{eq:mgf.1}, the CGF of the sufficient statistic $T(X)$ therefore can be calculated by
\begin{equation}
K_{T(X)}(t)=\log M_{T(X)}(t)=A(\eta+t)-A(\eta)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;cumulants&quot;&gt;Cumulants&lt;/h3&gt;
&lt;p&gt;The $k$-th &lt;strong&gt;cumulant&lt;/strong&gt; of a random variable $X$ is defined to be the $k$-th derivative of $K_{X}(t)$ at $0$, i.e.,
\begin{equation}
c_k=K^{(k)}(0)
\end{equation}&lt;/p&gt;

&lt;p&gt;Thus, the mean of $T(X)$ is exactly the first cumulant, while the variance is the second cumulant of $T(X)$.&lt;/p&gt;

&lt;h2 id=&quot;sufficiency&quot;&gt;Sufficiency&lt;/h2&gt;

&lt;h2 id=&quot;mle&quot;&gt;Maximum likelihood estimates&lt;/h2&gt;
&lt;p&gt;Consider an i.i.d data set $\mathcal{D}=\{x_1,\ldots,x_N\}$, the likelihood function is then given by
\begin{align}
L(\eta)=p(\mathbf{X}\vert\eta)&amp;amp;=\prod_{n=1}^{N}p(x_n\vert\eta) \\ &amp;amp;=\prod_{n=1}^{N}h(x_n)\exp\big[\eta^\text{T}T(x_n)-A(\eta)\big] \\ &amp;amp;=\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right]\label{eq:mle.1}
\end{align}
Taking the logarithm of both sides gives us the log likelihood as
\begin{equation}
\ell(\eta)=\log L(\eta)=\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)
\end{equation}
Consider the gradient of the log likelihood w.r.t $\eta$, we have
\begin{align}
\nabla_\eta\ell(\eta)&amp;amp;=\nabla_\eta\left[\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &amp;amp;=\sum_{n=1}^{N}T(x_n)-N\nabla_\eta A(\eta)
\end{align}
Setting the gradient to zero, we have the value of $\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\eta$, denoted as $\eta_\text{ML}$ satisfies
\begin{equation}
\nabla_{\eta}A(\eta_\text{ML})=\frac{1}{N}\sum_{n=1}^{N}T(x_n)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;conj-prior&quot;&gt;Conjugate priors&lt;/h2&gt;
&lt;p&gt;Given a probability distribution $p(x\vert\eta)$, its prior $p(\eta)$ is said to be &lt;strong&gt;conjugate&lt;/strong&gt; to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as &lt;strong&gt;conjugate prior&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;For any member of the exponential family, there exists a conjugate prior that can be written in form
\begin{equation}
p(\eta\vert\mathcal{X},\theta)=f(\mathcal{X},\theta)\exp(\eta^\text{T}\mathcal{X}-\theta A(\eta)),\label{eq:cp.1}
\end{equation}
where $\theta&amp;gt;0$ and $\mathcal{X}$ are hyperparameters.&lt;/p&gt;

&lt;p&gt;By Bayes’ rule, and with the likelihood function as given in \eqref{eq:mle.1}, the posterior distribution can be computed as
\begin{align}
&amp;amp;\hspace{0.7cm}p(\eta\vert\mathbf{X},\mathcal{X},\theta) \\ &amp;amp;\propto p(\eta\vert\mathcal{X},\theta)p(\mathbf{X}\vert\eta) \\ &amp;amp;=f(\mathcal{X},\theta)\exp\big(\eta^\text{T}\mathcal{X}-\theta A(\eta)\big)\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &amp;amp;\propto\exp\left[\eta^\text{T}\left(\mathcal{X}+\sum_{n=1}^{N}T(x_n)\right)-(\theta+N)A(\eta)\right],
\end{align}
which is in the same form as \eqref{eq:cp.1} and therefore claims the conjugacy.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] M. Jordan. &lt;a href=&quot;https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf&quot;&gt;The Exponential Family: Basics&lt;/a&gt;. 2009.&lt;/p&gt;

&lt;p&gt;[2] Joseph K. Blitzstein &amp;amp; Jessica Hwang. &lt;a href=&quot;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573&quot;&gt;Introduction to Probability&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] Weisstein, Eric W. &lt;a href=&quot;https://mathworld.wolfram.com/HoeldersInequalities.html&quot;&gt;Hölder’s Inequalities&lt;/a&gt; From MathWorld–A Wolfram Web Resource.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Let $p,q&amp;gt;1$ such that
\begin{equation*}
\frac{1}{p}+\frac{1}{q}=1
\end{equation*}
The &lt;strong&gt;Hölder’s inequality&lt;/strong&gt; for integrals states that
\begin{equation*}
\int_a^b\vert f(x)g(x)\vert\,dx\leq\left(\int_a^b\vert f(x)\vert\,dx\right)^{1/p}\left(\int_a^b\vert g(x)\vert\,dx\right)^{1/q}
\end{equation*}
The equality holds with
\begin{equation*}
\vert g(x)\vert=c\vert f(x)\vert^{p-1}
\end{equation*} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="probability-statistics" /><category term="mathematics" /><category term="probability-statistics" /><category term="exponential-family" /><summary type="html">A note on the exponential family.</summary></entry><entry><title type="html">Eligible Traces</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces.html" rel="alternate" type="text/html" title="Eligible Traces" /><published>2022-03-13T14:11:00+07:00</published><updated>2022-03-13T14:11:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/03/13/eligible-traces.html">&lt;blockquote&gt;
  &lt;p&gt;Beside &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#n-step-td&quot;&gt;$n$-step TD&lt;/a&gt; methods, there is another mechanism called &lt;strong&gt;Eligible traces&lt;/strong&gt; that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-return&quot;&gt;The λ-return&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#off-lambda-return&quot;&gt;Offline λ-return&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#td-lambda&quot;&gt;TD(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#truncated-td&quot;&gt;Truncated TD Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#onl-lambda-return&quot;&gt;Online λ-return&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#true-onl-td-lambda&quot;&gt;True Online TD(λ)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#equivalence-bw-forward-backward&quot;&gt;Equivalence between forward and backward views&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dutch-traces-mc&quot;&gt;Dutch Traces in Monte Carlo&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sarsa-lambda&quot;&gt;Sarsa(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lambda-gamma&quot;&gt;Variable λ and \(\gamma\)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#off-policy-traces-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tree-backup-lambda&quot;&gt;Tree-Backup(λ)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-off-policy-methods-traces&quot;&gt;Other Off-policy Methods with Traces&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gtd-lambda&quot;&gt;GTD(λ)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gq-lambda&quot;&gt;GQ(λ)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#greedy-gq-lambda&quot;&gt;Greedy-GQ(λ)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#htd-lambda&quot;&gt;HTD(\(\lambda\))&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#em-td-lambda&quot;&gt;Emphatic TD(λ)&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#etd-stability&quot;&gt;Stability&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lambda-return&quot;&gt;The $\lambda$-return&lt;/h2&gt;
&lt;p&gt;Recall that in &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-td-prediction&quot;&gt;TD-Learning&lt;/a&gt; post, we have defined the $n$-step return as
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}V_{t+n-1}(S_{t+n})
\end{equation}
for all $n,t$ such that $n\geq 1$ and $0\leq t\lt T-n$. After the post of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html&quot;&gt;Function Approximation&lt;/a&gt;, for any parameterized function approximator, we can generalize that equation into:
\begin{equation}
G_{t:t+n}\doteq R_{t+1}+\gamma R_{t+2}+
\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\leq T-n
\end{equation}
where $\hat{v}(s,\mathbf{w})$ is the approximate value of state $s$ given weight vector $\mathbf{w}$.&lt;/p&gt;

&lt;p&gt;We already know that by selecting $n$-step return as the target for a tabular learning update, just as it is for an approximate &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#stochastic-grad&quot;&gt;SGD update&lt;/a&gt;, we can reach to an optimal point. In fact, a valid update can be also be done toward any average of $n$-step returns for different $n$. For example, we can choose
\begin{equation}
\frac{1}{2}G_{t:t+2}+\frac{1}{2}G_{t:t+4}
\end{equation}
as the target for our update.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;TD($\lambda$)&lt;/strong&gt; is a particular way of averaging $n$-step updates. This average contains all the $n$-step updates, each weighted proportionally to $\lambda^{n-1}$, for $\lambda\in\left[0,1\right]$, and is normalized by a factor of $1-\lambda$ to guarantee that the weights sum to $1$, as:
\begin{equation}
G_t^\lambda\doteq(1-\lambda)\sum_{n=1}^{\infty}\lambda^{n-1}G_{t:t+n}
\end{equation}
The $G_t^\lambda$ is called &lt;strong&gt;$\lambda$-return&lt;/strong&gt; of the update.&lt;/p&gt;

&lt;p&gt;This figure below illustrates the backup diagram of TD($\lambda$) algorithm.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/td-lambda-backup.png&quot; alt=&quot;Backup diagram of TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: The backup diagram of TD($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;off-lambda-return&quot;&gt;Offline $\lambda$-return&lt;/h3&gt;
&lt;p&gt;With the definition of $\lambda$-return, we can define the &lt;strong&gt;offline $\lambda$-return&lt;/strong&gt; algorithm, which use semi-gradient update and using $\lambda$-return as the target:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{v}(S_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}&lt;/p&gt;

&lt;p&gt;A result when applying offline $\lambda$-return on the random walk problem is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/offline-lambda-return.png&quot; alt=&quot;Offline lambda-return on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Using offline $\lambda$-return on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;td-lambda&quot;&gt;TD($\lambda$)&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;TD($\lambda$)&lt;/strong&gt; improves over the offline $\lambda$-return algorithm since:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It updates the weight vector $\mathbf{w}$ on every step of an episode rather than only at the end, which leads to a time improvement.&lt;/li&gt;
  &lt;li&gt;Its computations are equally distributed in time rather than all at the end of the episode.&lt;/li&gt;
  &lt;li&gt;It can be applied to continuing problems rather than just to episodic ones.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With function approximation, the eligible trace is a vector $\mathbf{z}_t\in\mathbb{R}^d$ with the same number of components as the weight vector $\mathbf{w}_t$. Whereas $\mathbf{w}_t$ is long-term memory, $\mathbf{z}_t$ on the other hand is a short-term memory, typically lasting less time than the length of an episode.&lt;/p&gt;

&lt;p&gt;In TD($\lambda$), starting at the initial value of zero at the beginning of the episode, on each time step, the eligible trace vector $\mathbf{z}_t$ is incremented by the value gradient, and then fades away by $\gamma\lambda$:
\begin{align}
\mathbf{z}_{-1}&amp;amp;\doteq\mathbf{0} \\ \mathbf{z}_t&amp;amp;\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T\label{eq:tl.1}
\end{align}
where $\gamma$ is the discount factor; $\lambda$ is also called &lt;strong&gt;trace-decay parameter&lt;/strong&gt;. On the other hand, the weight vector $\mathbf{w}_t$ is updated on each step proportional to the scalar &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#td_error&quot;&gt;TD errors&lt;/a&gt; and the eligible trace vector $\mathbf{z}_t$:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,\label{eq:tl.2}
\end{equation}
where the TD error is defined as
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t)
\end{equation}&lt;/p&gt;

&lt;p&gt;Pseudocode of &lt;strong&gt;semi-gradient TD($\lambda$)&lt;/strong&gt; is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/semi-grad-td-lambda.png&quot; alt=&quot;Semi-gradient TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Linear TD($\lambda$) has been proved to converge in the on-policy case if the step size parameter, $\alpha$, is reduced over time according to the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#stochastic-approx-condition&quot;&gt;usual conditions&lt;/a&gt;. And also in the continuing discounted case, for any $\lambda$, $\overline{\text{VE}}$ is proven to be within a bounded expansion of the lowest possible error:
\begin{equation}
\overline{\text{VE}}(\mathbf{w}_\infty)\leq\dfrac{1-\gamma\lambda}{1-\gamma}\min_\mathbf{w}\overline{\text{VE}}(\mathbf{w})
\end{equation}&lt;/p&gt;

&lt;p&gt;The figure below illustrates the result for using TD($\lambda$) on the usual random walk task.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/td-lambda.png&quot; alt=&quot;TD(lambda) on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Using TD($\lambda$) on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;truncated-td&quot;&gt;Truncated TD Methods&lt;/h2&gt;
&lt;p&gt;Since in the offline $\lambda$-return, the target $\lambda$-return is not known until the end of episode. And moreover, in the continuing case, since the $n$-step returns depend on arbitrary large $n$, it maybe never known.
However, the dependence becomes weaker for longer-delayed rewards, falling by $\gamma\lambda$ for each step of delay.&lt;/p&gt;

&lt;p&gt;A natural approximation is to truncate the sequence after some number of steps. In general, we define the &lt;strong&gt;truncated $\lambda$-return&lt;/strong&gt; for time $t$, given data only up to some later horizon, $h$, as:
\begin{equation}
G_{t:h}^\lambda\doteq(1-\lambda)\sum_{n=1}^{h-t-1}\lambda^{n-1}G_{t:t+n}+\lambda^{h-t-1}G_{t:h},\hspace{1cm}0\leq t\lt h\leq T
\end{equation}
With this definition of the return, and based on the function approximation version of the $n$-step TD we have defined &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#semi-grad-n-step-td-update&quot;&gt;before&lt;/a&gt;, we have the &lt;strong&gt;TTD($\lambda$)&lt;/strong&gt; is defined as:
\begin{equation}
\mathbf{w}_{t+n}\doteq\mathbf{w}_{t+n-1}+\alpha\left[G_{t:t+n}^\lambda-\hat{v}(S_t,\mathbf{w}_{t+n-1})\right]\nabla_\mathbf{w}\hat{w}(S_t,\mathbf{w}_{t+n-1}),\hspace{1cm}0\leq t\lt T
\end{equation}
We have the $k$-step $\lambda$-return can be written as:
\begin{align}
\hspace{-0.5cm}G_{t:t+k}^\lambda&amp;amp;=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}G_{t:t+n}+\lambda^{k-1}G_{t:t+k} \\ &amp;amp;=(1-\lambda)\sum_{n=1}^{k-1}\lambda^{n-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right]\nonumber \\ &amp;amp;\hspace{1cm}+\lambda^{k-1}\left[R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{k-1}R_{t+k}+\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})\right] \\ &amp;amp;=R_{t+1}+\gamma\lambda R_{t+2}+\dots+\gamma^{k-1}\lambda^{k-1}R_{t+k}\nonumber \\ &amp;amp;\hspace{1cm}+(1-\lambda)\left[\sum_{n=1}^{k-1}\lambda^{n-1}\gamma^n\hat{v}(S_{t+n},\mathbf{w}_{t+n-1})\right]+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1}) \\ &amp;amp;=\hat{v}(S_t,\mathbf{w}_{t-1})+\left[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1})\right]\nonumber \\ &amp;amp;\hspace{1cm}+\left[\lambda\gamma R_{t+2}+\lambda\gamma^2\hat{v}(S_{t+2},\mathbf{w}_{t+1})-\lambda\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\right]+\dots\nonumber \\ &amp;amp;\hspace{1cm}+\left[\lambda^{k-1}\gamma^{k-1}R_{t+k}+\lambda^{k-1}\gamma^k\hat{v}(S_{t+k},\mathbf{w}_{t+k-1})-\lambda^{k-1}\gamma^{k-1}\hat{v}(S_{t+k-1},\mathbf{w}_{t+k-2})\right] \\ &amp;amp;=\hat{v}(S_t,\mathbf{w}_{t-1})+\sum_{i=t}^{t+k-1}(\gamma\lambda)^{i-t}\delta_i’,\label{eq:tt.1}
\end{align}
with
\begin{equation}
\delta_t’\doteq R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_{t-1}),
\end{equation}
where in the third step of the derivation, we use the identity
\begin{equation}
(1-\lambda)(1+\lambda+\dots+\lambda^{k-2})=1-\lambda^{k-1}
\end{equation}
From \eqref{eq:tt.1}, we can see that the $k$-step $\lambda$-return can be written as sums of TD errors if the value function is held constant, which allows us to implement the TTD($\lambda$) algorithm efficiently.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/ttd-lambda-backup.png&quot; alt=&quot;Backup diagram of truncated TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: The backup diagram of truncated TD($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;onl-lambda-return&quot;&gt;Online $\lambda$-return&lt;/h2&gt;
&lt;p&gt;The idea of &lt;strong&gt;online $\lambda$-return&lt;/strong&gt; involves multiple passes over the episode, one at each horizon, each generating a different sequence of weight vectors.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{w}_t^h$ denote the weights used to generate the value at time $t$ in the sequence up to horizon $h$. The first weight vector $\mathbf{w}_0^h$ in each sequence is the one that inherited from the previous episode (thus they are the same for all $h$), and the last weight vector $\mathbf{w}_h^h$ in each sequence defines the weight-vector sequence of the algorithm. At the final horizon $h=T$, we obtain the final weight $\mathbf{w}_T^\text{T}$  which will be passed on to form the initial weights of the next episode.&lt;/p&gt;

&lt;p&gt;In particular, we can define the first three sequences as:
\begin{align}
h=1:\hspace{1cm}&amp;amp;\mathbf{w}_1^1\doteq\mathbf{w}_0^1+\alpha\left[G_{0:1}^\lambda-\hat{v}(S_0,\mathbf{w}_0^1)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^1), \\\nonumber \\ h=2:\hspace{1cm}&amp;amp;\mathbf{w}_1^2\doteq\mathbf{w}_0^2+\alpha\left[G_{0:2}^\lambda-\hat{v}(S_0,\mathbf{w}_0^2)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^2), \\ &amp;amp;\mathbf{w}_2^2\doteq\mathbf{w}_1^2+\alpha\left[G_{1:2}^\lambda-\hat{v}(S_t,\mathbf{w}_1^2)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^2), \\\nonumber \\ h=3:\hspace{1cm}&amp;amp;\mathbf{w}_1^3\doteq\mathbf{w}_0^3+\alpha\left[G_{0:3}^\lambda-\hat{v}(S_0,\mathbf{w}_0^3)\right]\nabla_\mathbf{w}\hat{v}(S_0,\mathbf{w}_0^3), \\ &amp;amp;\mathbf{w}_2^3\doteq\mathbf{w}_1^3+\alpha\left[G_{1:3}^\lambda-\hat{v}(S_1,\mathbf{w}_1^3)\right]\nabla_\mathbf{w}\hat{v}(S_1,\mathbf{w}_1^3), \\ &amp;amp;\mathbf{w}_3^3\doteq\mathbf{w}_2^3+\alpha\left[G_{2:3}^\lambda-\hat{v}(S_2,\mathbf{w}_2^3)\right]\nabla_\mathbf{w}\hat{v}(S_2,\mathbf{w}_2^3)
\end{align}
The general form for the update of the &lt;strong&gt;online $\lambda$-return&lt;/strong&gt; is
\begin{equation}
\mathbf{w}_{t+1}^h\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h),\hspace{1cm}0\leq t\lt h\leq T,\label{eq:olr.1}
\end{equation}
with $\mathbf{w}_t\doteq\mathbf{w}_t^t$, and $\mathbf{w}_0^h$ is the same for all $h$, we denote this vector as $\mathbf{w}_{init}$.&lt;/p&gt;

&lt;p&gt;The online $\lambda$-return algorithm is fully online, determining a new weight vector $\mathbf{w}_t$ at each time step $t$ during an episode, using only information available at time $t$. Whereas the offline version passes through all the steps at the time of termination but does not make any updates during the episode.&lt;/p&gt;

&lt;h2 id=&quot;true-onl-td-lambda&quot;&gt;True Online TD($\lambda$)&lt;/h2&gt;
&lt;p&gt;In the online $\lambda$-return, at each time step a sequence of updates is performed. The length of this sequence, and hence the computation per time step, increase over time.&lt;/p&gt;

&lt;p&gt;However, it is possible to compute the weight vector resulting from time step $t+1$, $\mathbf{w}_{t+1}$, directly from the weight vector resulting from the sequence at time step $t$, $\mathbf{w}_t$.&lt;/p&gt;

&lt;p&gt;Consider using linear approximation for our task, which gives us 
\begin{align}
\hat{v}(S_t,\mathbf{w}_t)&amp;amp;=\mathbf{w}_t^\text{T}\mathbf{x}_t; \\ \nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)&amp;amp;=\mathbf{x}_t,
\end{align}
where $\mathbf{x}_t=\mathbf{x}(S_t)$ as usual.&lt;/p&gt;

&lt;p&gt;We begin by rewriting \eqref{eq:olr.1}, as
\begin{align}
\mathbf{w}_{t+1}^h&amp;amp;\doteq\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\hat{v}(S_t,\mathbf{w}_t^h)\right]\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t^h) \\ &amp;amp;=\mathbf{w}_t^h+\alpha\left[G_{t:h}^\lambda-\left(\mathbf{w}_t^h\right)^\text{T}\mathbf{x}_t\right]\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t^h+\alpha\mathbf{x}_t G_{t:h}^\lambda,
\end{align}
where $\mathbf{I}$ is the identity matrix. With this equation, consider $\mathbf{w}_t^h$ in the cases of $t=1$ and $t=2$, we have:
\begin{align}
\mathbf{w}_1^h&amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_0^h+\alpha\mathbf{x}_0 G_{0:h}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_{init}+\alpha\mathbf{x}_0 G_{0:h}^\lambda, \\ \mathbf{w}_2^h&amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\mathbf{w}_1^h+\alpha\mathbf{x}_1 G_{1:h}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\left(\mathbf{I}-\alpha\mathbf{x}_0\mathbf{x}_0^\text{T}\right)\mathbf{w}_{init}+\alpha\left(\mathbf{I}-\alpha\mathbf{x}_1\mathbf{x}_1^\text{T}\right)\mathbf{x}_0 G_{0:h}^\lambda+\alpha\mathbf{x}_1 G_{1:h}^\lambda
\end{align}
In general, for $t\leq h$, we can write:
\begin{equation}
\mathbf{w}_t^h=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:h}^\lambda,
\end{equation}
where $\mathbf{A}_i^j$ is defined as:
\begin{equation}
\mathbf{A}_i^j\doteq\left(\mathbf{I}-\alpha\mathbf{x}_j\mathbf{x}_j^\text{T}\right)\left(\mathbf{I}-\alpha\mathbf{x}_{j-1}\mathbf{x}_{j-1}^\text{T}\right)\dots\left(\mathbf{I}-\alpha\mathbf{x}_i\mathbf{x}_i^\text{T}\right),\hspace{1cm}j\geq i,
\end{equation}
with $\mathbf{A}_{j+1}^j\doteq\mathbf{I}$. Hence, we can express $\mathbf{w}_t$ as:
\begin{equation}
\mathbf{w}_t=\mathbf{w}_t^t=\mathbf{A}_0^{t-1}\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{i:t}^\lambda\label{eq:totl.1}
\end{equation}
Using \eqref{eq:tt.1}, we have:
\begin{align}
G_{i:t+1}^\lambda-G_{i:t}^\lambda&amp;amp;=\mathbf{w}_i^\text{T}\mathbf{x}_i+\sum_{j=1}^{t}(\gamma\lambda)^{j-i}\delta_j’-\left(\mathbf{w}_i^\text{T}\mathbf{x}_i+\sum_{j=1}^{t-1}(\gamma\lambda)^{j-i}\delta_j’\right) \\ &amp;amp;=(\gamma\lambda)^{t-i}\delta_t’\label{eq:totl.2}
\end{align}
with the TD error, $\delta_t’$ is defined as earlier:
\begin{equation}
\delta_t’\doteq R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\label{eq:totl.3}
\end{equation}
Using \eqref{eq:totl.1}, \eqref{eq:totl.2} and \eqref{eq:totl.3}, we have:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda \\ &amp;amp;=\mathbf{A}_0^t\mathbf{w}_{init}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t+1}^\lambda+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\mathbf{A}_0^t\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i G_{i:t}^\lambda+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\left(\mathbf{A}_0^{t-1}\mathbf{w}_0+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i G_{t:t+1}^\lambda\right)\nonumber \\ &amp;amp;\hspace{1cm}+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i\left(G_{i:t+1}^\lambda-G_{i:t}^\lambda\right)+\alpha\mathbf{x}_t G_{t:t+1}^\lambda \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t\mathbf{x}_t\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’\nonumber \\ &amp;amp;\hspace{1cm}+\alpha\mathbf{x}_t\left(R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t+\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t-\mathbf{w}_t^\text{T}\mathbf{x}_t\right) \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’+\alpha\mathbf{x}_t\delta_t’-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_t(\gamma\lambda)^{t-i}\delta_t’-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t’-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\left(\delta_t+\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)-\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\mathbf{z}_t\delta_t+\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\left(\mathbf{z}_t-\mathbf{x}_t\right),\label{eq:totl.4}
\end{align}
where in the eleventh step, we define $\mathbf{z}_t$ as:
\begin{equation}
\mathbf{z}_t\doteq\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i},
\end{equation}
and in the twelfth step, we also define $\delta_t$ as:
\begin{align}
\delta_t&amp;amp;\doteq\delta_t’-\mathbf{w}_t^\text{T}\mathbf{x}_t+\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t \\ &amp;amp;=R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t,
\end{align}
which is the same as the TD error of TD($\lambda$) we have defined earlier.&lt;/p&gt;

&lt;p&gt;We then need to derive an update rule to recursively compute $\mathbf{z}_t$ from $\mathbf{z}_{t-1}$, as:
\begin{align}
\mathbf{z}_t&amp;amp;=\sum_{i=0}^{t}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i} \\ &amp;amp;=\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^t\mathbf{x}_i(\gamma\lambda)^{t-i}+\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\gamma\lambda\sum_{i=0}^{t-1}\mathbf{A}_{i+1}^{t-1}\mathbf{x}_i(\gamma\lambda)^{t-i-1}+\mathbf{x}_t \\ &amp;amp;=\left(\mathbf{I}-\alpha\mathbf{x}_t\mathbf{x}_t^\text{T}\right)\gamma\lambda\mathbf{z}_{t-1}+\mathbf{x}_t \\ &amp;amp;=\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\text{T}\mathbf{x}_t\right)\right)\mathbf{x}_t\label{eq:totl.5}
\end{align}
Equations \eqref{eq:totl.4} and \eqref{eq:totl.5} form the update of the &lt;strong&gt;true online TD($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t+\alpha\left(\mathbf{w}_t^\text{T}\mathbf{x}_t-\mathbf{w}_{t-1}^\text{T}\mathbf{x}_t\right)\left(\mathbf{z}t_t-\mathbf{x}_t\right),
\end{equation}
where
\begin{align}
\mathbf{z}_t&amp;amp;\doteq\gamma\lambda\mathbf{z}_{t-1}+\left(1-\alpha\gamma\lambda\left(\mathbf{z}_{t-1}^\text{T}\mathbf{x}_t\right)\right)\mathbf{x}_t,\label{eq:totl.6} \\ \delta_t&amp;amp;\doteq R_{t+1}+\gamma\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t
\end{align}
Pseudocode of the algorithm is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-onl-td-lambda.png&quot; alt=&quot;True Online TD(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;As other methods above, below is an illustration of using true online TD($\lambda$) on the random walk problem.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-online-td-lambda.png&quot; alt=&quot;True online TD(lambda) on random walk&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: Using True online TD($\lambda$) on 19-state random walk. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-12/random_walk.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The eligible trace \eqref{eq:totl.6} is called &lt;strong&gt;dutch trace&lt;/strong&gt; to distinguish it from the trace \eqref{eq:tl.1} of TD($\lambda$), which is called &lt;strong&gt;accumulating trace&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There is another kind of trace called &lt;strong&gt;replacing trace&lt;/strong&gt;, defined for the tabular case or for binary feature vectors
\begin{equation}
z_{i,t}\doteq\begin{cases}1 &amp;amp;\text{if }x_{i,t}=1 \\ \gamma\lambda z_{i,t-1} &amp;amp;\text{if }x_{i,t}=0\end{cases}
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;equivalence-bw-forward-backward&quot;&gt;Equivalence between forward and backward views&lt;/h3&gt;
&lt;p&gt;In this section, we will show that there is an interchange between forward and backward view.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Consider any forward view that updates towards some interim targets $Y_k^t$ with
\begin{equation}
\mathbf{w}_{k+1}^t=\mathbf{w}_k+\eta_k\left(Y_k^t-\mathbf{x}_k^\text{T}\mathbf{w}_k^t\right)\mathbf{x}_k+\mathbf{u}_k,\hspace{1cm}0\leq k\lt t,
\end{equation}
where $\mathbf{w}_0^t=\mathbf{w}_0$ for some initial $\mathbf{w}_0$; $\mathbf{u}_k\in\mathbb{R}^d$ is any vector that does not depend on $t$. Assume that the temporal differences $Y_k^{t+1}-Y_k^t$ for different $k$ are related through
\begin{equation}
Y_k^{t+1}-Y_k^t=c_k(Y_{k+1}^{t+1}-Y_{k+1}^t),\hspace{1cm}\forall k\lt t\label{eq:ebfb.1} 
\end{equation}
where $c_k$ is a scalar that does not depend on $t$. Then the final weights $\mathbf{w}_t^t$ at each time step $t$ are equal to the weight $\mathbf{w}_t$ as defined by $\mathbf{z}_0=\eta_0\mathbf{x}_0$ and the backward view
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t+(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t, \\ \mathbf{z}_t&amp;amp;=c_{t-1}\mathbf{z}_{t-1}+\eta_t\left(1-c_{t-1}\mathbf{x}_t^\text{T}\mathbf{z}_{t-1}\right)\mathbf{x}_t,\hspace{1cm}t\gt 0
\end{align}&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $\mathbf{F}_t\doteq\mathbf{I}-\eta_t\mathbf{x}_t\mathbf{x}_t^\text{T}$ be the &lt;em&gt;fading matrix&lt;/em&gt; such that $\mathbf{w}_{t+1}=\mathbf{F}_k\mathbf{w}_k^t+\eta_k Y_k^t\mathbf{x}_k$. For each step $t$, we have:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&amp;amp;=\mathbf{F}_t\mathbf{w}_t^{t+1}-\mathbf{w}_t^t+\eta_t Y_t^{t+1}\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t+(\mathbf{F}_t-\mathbf{I})\mathbf{w}_t^t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t Y_t^{t+1}\mathbf{x}_t-\eta_t\mathbf{x}_t\mathbf{x}_t^\text{T}\mathbf{w}_t^t+\mathbf{u}_t \\ &amp;amp;=\mathbf{F}_t(\mathbf{w}_t^{t+1}-\mathbf{w}_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t^t)\mathbf{x}_t+\mathbf{u}_t\label{eq:ebfb.2}
\end{align}
We also have that:
\begin{align}
\mathbf{w}_t^{t+1}-\mathbf{w}_t^t&amp;amp;=\mathbf{F}_{t-1}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &amp;amp;=\mathbf{F}_{t-1}\mathbf{F}_{t-2}(\mathbf{w}_{t-1}^{t+1}-\mathbf{w}_{t-1}^t)+\eta_{n-2}(Y_{t-2}^{t+1}-Y_{t-2}^t)\mathbf{F}_{t-1}\mathbf{x}_{t-2} \\ &amp;amp;\hspace{1cm}+\eta_{t-1}(Y_{t-1}^{t+1}-Y_{t-1}^t)\mathbf{x}_{t-1} \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=\mathbf{F}_{t-1}\dots\mathbf{F}_0(\mathbf{w}_0^{t+1}-\mathbf{w}_0^t)+\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}(Y_k^{t+1}-Y_k^t)\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}c_k(Y_{k+1}^{t+1}-Y_{k+1}^t)\mathbf{x}_k \\ &amp;amp;\hspace{0.3cm}\vdots \\ &amp;amp;=c_{t-1}\underbrace{\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k}_{\doteq\mathbf{z}_{t-1}}(Y_t^{t+1}-Y_t^t) \\ &amp;amp;=c_{t-1}\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t),\label{eq:ebfb.3}
\end{align}
where in the fifth step, we use the assumption \eqref{eq:ebfb.1}; the vector $\mathbf{z}_t$ defined in the sixth step can be computed recursively in terms of $\mathbf{z}_{t-1}$:
\begin{align}
\mathbf{z}_t&amp;amp;=\sum_{k=0}^{t}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k \\ &amp;amp;=\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-1}c_j\right)\mathbf{F}_1\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{F}_t\sum_{k=0}^{t-1}\eta_k\left(\prod_{j=k}^{t-2}c_j\right)\mathbf{F}_{t-1}\dots\mathbf{F}_{k+1}\mathbf{x}_k+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{F}_1\mathbf{z}_{t-1}+\eta_t\mathbf{x}_t \\ &amp;amp;=c_{t-1}\mathbf{z}_{t-1}+\eta_t(1-c_{t-1}\mathbf{x}_t^\text{T}\mathbf{z}_{t-1})\mathbf{x}_t
\end{align}
Plug \eqref{eq:ebfb.3} back into \eqref{eq:ebfb.2} we obtain:
\begin{align}
\mathbf{w}_{t+1}^{t+1}-\mathbf{w}_t^t&amp;amp;=c_{t-1}\mathbf{F}_t\mathbf{z}_{t-1}(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=(\mathbf{z}_t-\eta_t\mathbf{x}_t)(Y_t^{t+1}-Y_t^t)+\eta_t(Y_t^{t+1}-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t \\ &amp;amp;=(Y_t^{t+1}-Y_t^t)\mathbf{z}_t+\eta_t(Y_t^t-\mathbf{x}_t^\text{T}\mathbf{w}_t)\mathbf{x}_t+\mathbf{u}_t
\end{align}
Since $\mathbf{w}_{0,t}\doteq\mathbf{w}_0$, the desired result follows through induction.&lt;/p&gt;

&lt;h3 id=&quot;dutch-traces-mc&quot;&gt;Dutch Traces In Monte Carlo&lt;/h3&gt;

&lt;h2 id=&quot;sarsa-lambda&quot;&gt;Sarsa($\lambda$)&lt;/h2&gt;
&lt;p&gt;To apply the use off eligible traces on control problems, we begin by defining the $n$-step return, which is the same as what we have defined &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#n-step-return&quot;&gt;before&lt;/a&gt;:
\begin{equation}
\hspace{-0.5cm}G_{t:t+n}\doteq\ R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{n-1}R_{t+n}+\gamma^n\hat{q}(S_{t+n},A_{t+n},\mathbf{w}_{t+n-1}),\hspace{1cm}t+n\lt T\label{eq:sl.1}
\end{equation}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$. With this definition of the return, the action-value form of offline $\lambda$-return can be defined as:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t^\lambda-\hat{q}(S_t,A_t,\mathbf{w}_t)\right]\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}t=0,\dots,T-1
\end{equation}
where $G_t^\lambda\doteq G_{t:\infty}^\lambda$.&lt;/p&gt;

&lt;p&gt;The TD method for action values, known as &lt;strong&gt;Sarsa($\lambda$)&lt;/strong&gt;, approximates this forward view and has the same update rule as TD($\lambda$):
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t,
\end{equation}
except that the TD error, $\delta_t$, is defined in terms of action-value function:
\begin{equation}
\delta_t\doteq R_{t+1}+\gamma\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)-\hat{q}(S_t,A_t,\mathbf{w}_t),
\end{equation}
and so it is with eligible trace vector:
\begin{align}
\mathbf{z}_{-1}&amp;amp;\doteq\mathbf{0}, \\ \mathbf{z}&amp;amp;_t\doteq\gamma\lambda\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t),\hspace{1cm}0\leq t\lt T
\end{align}&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/sarsa-lambda-backup.png&quot; alt=&quot;Backup diagram of Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 390px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: The backup diagram of Sarsa($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Pseudocode of the Sarsa($\lambda$) is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/sarsa-lambda.png&quot; alt=&quot;Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There is also an action-value version of the online $\lambda$-return algorithm, and its efficient implementation as true online TD($\lambda$), called &lt;strong&gt;True online Sarsa($\lambda$)&lt;/strong&gt;, which can be achieved by using $n$-step return \eqref{eq:sl.1} instead (which also leads to the change of $\mathbf{x}_t=\mathbf{x}(S_t)$ to $\mathbf{x}_t=\mathbf{x}(S_t,A_t)$).&lt;/p&gt;

&lt;p&gt;Pseudocode of the true online Sarsa($\lambda$) is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/true-online-sarsa-lambda.png&quot; alt=&quot;True online Sarsa(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;lambda-gamma&quot;&gt;Variable $\lambda$ and $\gamma$&lt;/h2&gt;
&lt;p&gt;We can generalize the degree of bootstrapping and discounting beyond constant parameters to functions potentially dependent on the state and action. In other words, each time step $t$, we will have a different $\lambda$ and $\gamma$, denoted as $\lambda_t$ and $\gamma_t$.&lt;/p&gt;

&lt;p&gt;In particular, say $\lambda:\mathcal{S}\times\mathcal{A}\to[0,1]$ such that $\lambda_t\doteq\lambda(S_t,A_t)$ and similarly, $\gamma:\mathcal{S}\to[0,1]$ such that $\gamma_t\doteq\gamma(S_t)$.&lt;/p&gt;

&lt;p&gt;With this definition of $\gamma$, the return can be rewritten generally as:
\begin{align}
G_t&amp;amp;\doteq R_{t+1}+\gamma_{t+1}G_{t+1} \\ &amp;amp;=R_{t+1}+\gamma_{t+1}R_{t+2}+\gamma_{t+1}\gamma_{t+2}R_{t+3}+\dots \\ &amp;amp;=\sum_{k=t}^{\infty}\left(\prod_{i=t+1}^{k}\gamma_i\right)R_{k+1},
\end{align}
where we require that $\prod_{k=t}^{\infty}\gamma_k=0$ with probability $1$ for all $t$ to assure the sums are finite.&lt;/p&gt;

&lt;p&gt;The generalization of $\lambda$ also lets us rewrite the state-based $\lambda$-return as:
\begin{equation}
G_t^{\lambda s}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\Big),\label{eq:lg.1}
\end{equation}
where $G_t^{\lambda s}$ denotes that this $\lambda$
-return is bootstrapped from state values, and hence the $G_t^{\lambda a}$ denotes the $\lambda$-return that bootstraps from action values. The Sarsa form of action-based $\lambda$-return is defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),
\end{equation}
and the Expected Sarsa form of its can be defined as:
\begin{equation}
G_t^{\lambda a}\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda a}\Big),\label{eq:lg.2}
\end{equation}
where the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#expected-approximate-value&quot;&gt;expected approximate value&lt;/a&gt; is generalized to function approximation as:
\begin{equation}
\bar{V}_t\doteq\sum_a\pi(a|s)\hat{q}(s,a,\mathbf{w}_t)\label{eq:lg.3}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;off-policy-traces-control-variates&quot;&gt;Off-policy Traces with Control Variates&lt;/h2&gt;
&lt;p&gt;We can also apply the use of importance sampling with eligible traces.&lt;/p&gt;

&lt;p&gt;We begin with the new definition of $\lambda$-return, which is achieved by generalizing the $\lambda$-return \eqref{eq:lg.1} with the idea of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-return-control-variate-state-value&quot;&gt;control variates on $n$-step off-policy return&lt;/a&gt;:
\begin{equation}
G_t^{\lambda s}\doteq\rho_t\Big(R_{t+1}+\gamma_{t+1}\big((1-\lambda_{t+1})\hat{v}(S_{t+1},\mathbf{w}_t)+\lambda_{t+1}G_{t+1}^{\lambda s}\big)\Big)+(1-\rho_t)\hat{v}(S_t,\mathbf{w}_t),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
Much like the other returns, the truncated version of this return can be approximated simply in terms of sums of state-based TD errors:
\begin{equation}
G_t^{\lambda s}\approx\hat{v}(S_t,\mathbf{w}_t)+\rho_t\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
where the state-based TD error, $\delta_t^s$, is defined as:
\begin{equation}
\delta_t^s\doteq R_{t+1}+\gamma_{t+1}\hat{v}(S_{t+1},\mathbf{w}_t)-\hat{v}(S_t,\mathbf{w}_t),\label{eq:optcv.1}
\end{equation}
with the approximation becoming exact if the approximate value function does not change.&lt;/p&gt;

&lt;p&gt;With this approximation, we have that:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;=\mathbf{w}_t+\alpha\left(G_t^{\lambda s}-\hat{v}(S_t,\mathbf{w}_t)\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t) \\ &amp;amp;\approx\mathbf{w}_t+\alpha\rho_t\left(\sum_{k=t}^{\infty}\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i\right)\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)
\end{align}
This is one time step of a forward view. And in fact, the forward-view update, summed over time, is approximately equal to a backward-view update, summed over time. Since the sum of the forward-view update over time is:
\begin{align}
\sum_{t=1}^{\infty}(\mathbf{w}_{t+1}-\mathbf{w}_t)&amp;amp;\approx\sum_{t=1}^{\infty}\sum_{k=t}^{\infty}\alpha\rho_t\delta_k^s\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{k=1}^{\infty}\sum_{t=1}^{k}\alpha\rho_t\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\delta_k^s\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{k=1}^{\infty}\alpha\delta_k^s\sum_{t=1}^{k}\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,\label{eq:optcv.2}
\end{align}
where in the second step, we use the summation rule: $\sum_{t=x}^{y}\sum_{k=t}^{y}=\sum_{k=x}^{y}\sum_{t=x}^{k}$.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{z}_k$ is defined as:
\begin{align}
\mathbf{z}_k &amp;amp;=\sum_{t=1}^{k}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t, \mathbf{w}_t\right)\prod_{i=t+1}^{k} \gamma_i\lambda_i\rho_i \\ &amp;amp;=\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &amp;amp;=\gamma_k\lambda_k\rho_k\underbrace{\sum_{t=1}^{k-1}\rho_t\nabla_\mathbf{w}\hat{v}\left(S_t,\mathbf{w}_t\right)\prod_{i=t+1}^{k-1}\gamma_i\lambda_i\rho_i}_{\mathbf{z}_{k-1}}+\rho_k\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right) \\ &amp;amp;=\rho_k\big(\gamma_k\lambda_k\mathbf{z}_{k-1}+\nabla_\mathbf{w}\hat{v}\left(S_k,\mathbf{w}_k\right)\big)
\end{align}
Then we can rewrite \eqref{eq:optcv.2} as:
\begin{equation}
\sum_{t=1}^{\infty}\left(\mathbf{w}_{t+1}-\mathbf{w}_t\right)\approx\sum_{k=1}^{\infty}\alpha\delta_k^s\mathbf{z}_k,
\end{equation}
which is sum of the backward-view update over time, with the eligible trace vector is defined as:
\begin{equation}
\mathbf{z}_t\doteq\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w}_t)\big)\label{eq:optcv.3}
\end{equation}
Using this eligible trace with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$), we obtain a general TD($\lambda$) algorithm that can be applied to either on-policy or off-policy data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the on-policy case, the algorithm is exactly TD($\lambda$) because $\rho_t=1$ for all $t$ and \eqref{eq:optcv.3} becomes the accumulating trace \eqref{eq:tl.1} with extending to variable $\lambda$ and $\gamma$.&lt;/li&gt;
  &lt;li&gt;In the off-policy case, the algorithm often works well but, as a semi-gradient method, is not guaranteed to be stable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For action-value function, we generalize the definition of the $\lambda$-return \eqref{eq:lg.2} of Expected Sarsa with the idea of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-return-control-variate-action-value&quot;&gt;control variate&lt;/a&gt;:
\begin{align}
G_t^{\lambda a}&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\big[\rho_{t+1}G_{t+1}^{\lambda a}+\bar{V}_t(S_{t+1})\nonumber \\ &amp;amp;\hspace{2cm}-\rho_{t+1}\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\big]\Big) \\ &amp;amp;=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\rho_{t+1}\left[G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right]\Big),
\end{align}
where the expected approximate value $\bar{V}_t(S_{t+1})$ is as given by \eqref{eq:lg.3}.&lt;/p&gt;

&lt;p&gt;Similar to the others, this $\lambda$-return can also be written approximately as the sum of TD errors
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\rho_i,
\end{equation}
with the action-based TD error is defined in terms of the expected approximate value:
\begin{equation}
\delta_t^a=R_{t+1}+\gamma_{t+1}\bar{V}_t(S_{t+1})-\hat{q}(S_t,A_t,\mathbf{w}_t)\label{eq:optcv.4}
\end{equation}
Like the state value function case, this approximation also becomes exact if the approximate value function does not change.&lt;/p&gt;

&lt;p&gt;Similar to the state case \eqref{eq:optcv.3}, we can also define the eligible trace for action values:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$) and the expectation-based TD error \eqref{eq:optcv.4}, we end up with an Expected Sarsa($\lambda$) algorithm that can applied to either on-policy or off-policy data.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the on-policy case with constant $\lambda$ and $\gamma$, this becomes the Sarsa($\lambda$) algorithm.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tree-backup-lambda&quot;&gt;Tree-Backup($\lambda$)&lt;/h2&gt;
&lt;p&gt;Recall that in the post of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html&quot;&gt;TD-Learning&lt;/a&gt;, we have mentioned that there is an off-policy method without importance sampling called &lt;strong&gt;tree-backup&lt;/strong&gt;. Can we extend the idea of tree-backup to an eligible trace version? Yes, we can.&lt;/p&gt;

&lt;p&gt;As usual, we begin with establishing the $\lambda$-return by generalizing the $\lambda$-return of Expected Sarsa \eqref{eq:lg.2} with the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/01/31/td-learning.html#n-step-tree-backup-return&quot;&gt;$n$-step Tree-backup return&lt;/a&gt;:
\begin{align}
G_t^{\lambda a}&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\Bigg((1-\lambda_{t+1})\bar{V}_t(S_{t+1})+\lambda_{t+1}\Big[\sum_{a\neq A_{t+1}}\pi(a|S_{t+1})\hat{q}(S_{t+1},a,\mathbf{w}_t)\nonumber \\ &amp;amp;\hspace{2cm}+\pi(A_{t+1}|S_{t+1})G_{t+1}^{\lambda a}\Big]\Bigg) \\ &amp;amp;=R_{t+1}+\gamma_{t+1}\Big(\bar{V}_t(S_{t+1})+\lambda_{t+1}\pi(A_{t+1}|S_{t+1})\left(G_{t+1}^{\lambda a}-\hat{q}(S_{t+1},A_{t+1},\mathbf{w}_t)\right)\Big)
\end{align}
This return, as usual, can also be written approximately (ignoring changes in the approximate value function) as sum of TD errors:
\begin{equation}
G_t^{\lambda a}\approx\hat{q}(S_t,A_t,\mathbf{w}_t)+\sum_{k=t}^{\infty}\delta_k^a\prod_{i=t+1}^{k}\gamma_i\lambda_i\pi(A_i|S_i),
\end{equation}
with the TD error is defined as given by \eqref{eq:optcv.4}.&lt;/p&gt;

&lt;p&gt;Similar to how we derive the eligible trace \eqref{eq:optcv.3}, we can define a new eligible trace in terms of target-policy probabilities of the selected actions:
\begin{equation}
\mathbf{z}_t\doteq\gamma_t\lambda_t\pi(A_t|S_t)\mathbf{z}_{t-1}+\nabla_\mathbf{w}\hat{q}(S_t,A_t,\mathbf{w}_t)
\end{equation}
Using this eligible trace vector with the parameter update rule \eqref{eq:tl.2} of TD($\lambda$), we end up with the &lt;strong&gt;Tree-Backup($\lambda$)&lt;/strong&gt; or &lt;strong&gt;TB($\lambda$)&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2022-03-13/tree-backup-lambda-backup.png&quot; alt=&quot;Backup diagram of Tree Backup(lambda)&quot; style=&quot;display: block; margin-left: auto; margin-right: auto; width: 450px; height: 390px&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: The backup diagram of Tree Backup($\lambda$)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;other-off-policy-methods-traces&quot;&gt;Other Off-policy Methods with Traces&lt;/h2&gt;

&lt;h3 id=&quot;gtd-lambda&quot;&gt;GTD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GTD($\lambda$)&lt;/strong&gt; is the extended version of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#tdc&quot;&gt;&lt;strong&gt;TDC&lt;/strong&gt;&lt;/a&gt;, a state-value Gradient-TD method, with eligible traces.&lt;/p&gt;

&lt;p&gt;In this algorithm, we will define a new off-policy, $\lambda$-return, not like usual but as a function:
\begin{equation}
G_t^{\lambda}(v)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})v(S_{t+1})+\lambda_{t+1}G_{t+1}^{\lambda}(v)\Big]\label{eq:gl.1}
\end{equation}
where $v(s)$ denotes the value at state $s$, and $\lambda\in[0,1]$ is the trace-decay parameter.&lt;/p&gt;

&lt;p&gt;Let $T_\pi^\lambda$ denote the $\lambda$-weighted Bellman operator for policy $\pi$ such that:
\begin{align}
v_\pi(s)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(v_\pi)\big|S_t=s,\pi\Big] \\ &amp;amp;\doteq (T_\pi^\lambda v_\pi)(s)
\end{align}&lt;/p&gt;

&lt;p&gt;Consider using linear function approximation, or in particular, we are trying to approximate $v(s)$ by $v_\mathbf{w}(s)=\mathbf{w}^\text{T}\mathbf{x}(s)$. Our objective is to find the fixed point which satisfies:
\begin{equation}
v_\mathbf{w}=\Pi T_\pi^\lambda v_\mathbf{w},\label{eq:gl.2}
\end{equation}
where $\Pi v$ is a projection of $v$ into the space of representable functions $\{v_\mathbf{w}|\mathbf{w}\in\mathbb{R}^d\}$.
Let $\mu$ be the steady-state distribution of states under the behavior policy $b$. Then, the projection can be defined as:
\begin{equation}
\Pi v\doteq v_{\mathbf{w}},
\end{equation}
where
\begin{equation}
\mathbf{w}={\arg\min}_{\mathbf{w}\in\mathbb{R}^d}\left\Vert v-v_\mathbf{w}\right\Vert_\mu^2,
\end{equation}
In a linear case, in which $v_\mathbf{w}=\mathbf{X}\mathbf{w}$, the projection operator is linear and independent of $\mathbf{w}$:
\begin{equation}
\Pi=\mathbf{X}(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X})^{-1}\mathbf{X}^\text{T}\mathbf{D},
\end{equation}
where $\mathbf{D}$ denotes $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix whose diagonal elements are $\mu(s)$, and $\mathbf{X}$ denotes the $\vert\mathcal{S}\vert\times d$ matrix whose rows are the feature vectors $\mathbf{x}(s)^\text{T}$, one for each state $s$.&lt;/p&gt;

&lt;p&gt;With linear function approximation, we can rewrite the $\lambda$-return \eqref{eq:gl.1} as:
\begin{equation}
G_t^{\lambda}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda}(\mathbf{w})\Big]\label{eq:gl.3}
\end{equation}
Let
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t,
\end{equation}
and
\begin{equation}
\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\doteq\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\mathbf{x}(s), 
\end{equation}
where $\mathcal{P}_\mu^\pi$ is an operator.&lt;/p&gt;

&lt;p&gt;The fixed point in \eqref{eq:gl.2} can be found by minimizing the Mean Square Projected Bellman Error (MSPBE):
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big\Vert v_\mathbf{w}-\Pi T_\pi^\lambda v_\mathbf{w}\Big\Vert_\mu^2 \\ &amp;amp;=\Big\Vert\Pi(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w})\Big\Vert_\mu^2 \\ &amp;amp;=\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big)^\text{T}\mathbf{D}\Big(\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)\Big) \\ &amp;amp;=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\text{T}\Pi^\text{T}\mathbf{D}\Pi\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &amp;amp;=\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right)^\text{T}\mathbf{D}^\text{T}\mathbf{X}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{D}\left(v_\mathbf{w}-T_\pi^\lambda v_\mathbf{w}\right) \\ &amp;amp;=\Big(\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-\mathbf{w}\right)\Big)^\text{T}\left(\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}\right)^{-1}\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)\label{eq:gl.4}
\end{align}&lt;/p&gt;

&lt;p&gt;From the definition of $T_\pi^\lambda$ and $\delta_t^\lambda$, we have:
\begin{align}
(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{v})(s)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t\big|S_t=s,\pi\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})\big|S_t=s,\pi\Big]\label{eq:gl.5}
\end{align}
Therefore,
\begin{align}
\mathbf{X}^\text{T}\mathbf{D}\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)&amp;amp;=\sum_s\mu(s)\Big[\left(T_\pi^\lambda v_\mathbf{w}-v_\mathbf{w}\right)(s)\Big]\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)\mathbb{E}\Big[\delta_t^\lambda(\mathbf{w})|S_t=s,\pi\Big]\mathbf{x}(s) \\ &amp;amp;=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\label{eq:gl.6}
\end{align}
Moreover, we also have:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{D}\mathbf{X}=\sum_s\mu(s)\mathbf{x}(s)\mathbf{x}(s)^\text{T}=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]\label{eq:gl.7}
\end{equation}
Substitute \eqref{eq:gl.5}, \eqref{eq:gl.6} and \eqref{eq:gl.7} back to the \eqref{eq:gl.4}, we have:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)\label{eq:gl.8}
\end{equation}
In the objective function \eqref{eq:gl.8}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy problem, as usual, we use importance sampling.&lt;/p&gt;

&lt;p&gt;We then instead use an importance-sampling version of $\lambda$-return \eqref{eq:gl.3}:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})=\rho_t\left(R_{t+1}+\gamma_{t+1}\left[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\right]\right),
\end{equation}
where the single-step importance sampling ratio $\rho_t$ is defined as usual:
\begin{equation}
\rho_t\doteq\frac{\pi(A_t|S_t)}{b(A_t|S_t)}
\end{equation}
This also leads to an another version of $\delta_t^\lambda$, defined as:
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
With this definition of the $\lambda$-return, we have:
\begin{align}
&amp;amp;\hspace{-1cm}\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]\nonumber \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big)+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[\rho_t\big(R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big)\big|S_t=s\Big]+\rho_t\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big|S_t=s,\pi\Big]\nonumber \\ &amp;amp;\hspace{1cm}+\sum_{a,s’}p(s’|s,a)b(a|s)\frac{\pi(a|s)}{b(a|s)}\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}\big|S_t=s,\pi\Big]\nonumber \\ &amp;amp;\hspace{1cm}+\sum_{a,s’}p(s’|s,a)\pi(a|s)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’\Big]\big|S_t=s,\pi\Big],
\end{align}
which, as it continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s\Big]=\mathbb{E}\Big[G_t^{\lambda}(\mathbf{w})\big|S_t=s,\pi\Big]
\end{equation}
And eventually, we get:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t
\end{equation}
because the state distribution is based on behavior state-distribution $\mu$.&lt;/p&gt;

&lt;p&gt;With this result, our objective function \eqref{eq:gl.8} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &amp;amp;=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\label{eq:gl.9}
\end{align}
From the definition of $\delta_t^{\lambda\rho}$, we have:
\begin{align}
\delta_t^{\lambda\rho}(\mathbf{w})&amp;amp;=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big]\Big)-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t+\mathbf{w}^\text{T}\mathbf{x}_t\Big)\nonumber \\ &amp;amp;\hspace{2cm}-\rho_t\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}+\rho_t\gamma_{t+1}\lambda_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;=\rho_t\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\Big)+\rho_t\mathbf{w}^\text{T}\mathbf{x}_t-\mathbf{w}^\text{T}\mathbf{x}_t\nonumber \\ &amp;amp;\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big) \\ &amp;amp;=\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w}),
\end{align}
where the TD error, $\delta_t(\mathbf{w})$, is defined as usual:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
Also, we have that:
\begin{align}
\mathbb{E}\Big[(1-\rho_t)\mathbf{w}^\text{T}\mathbf{x}_t\mathbf{x}_t\Big]&amp;amp;=\sum_{s,a}\mu(s)b(a|s)\left(1-\frac{\pi(a|s)}{b(a|s)}\right)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)\left(\sum_a b(a|s)-\sum_a\pi(a|s)\right)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=\sum_s\mu(s)(1-1)\mathbf{w}^\text{T}\mathbf{x}(s)\mathbf{x}(s) \\ &amp;amp;=0
\end{align}
With these results, we have:
\begin{align}
\hspace{-1cm}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t\mathbf{x}_t+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t\Big]+0+\mathbb{E}_{\pi b}\Big[\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+(\rho_t-1)\mathbf{w}^\text{T}\mathbf{x}_t\nonumber \\ &amp;amp;\hspace{2cm}+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\big(\rho_t\delta_t(\mathbf{w})+\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-1}\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_t\delta_t(\mathbf{w})\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}\big)+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &amp;amp;\hspace{0.3cm}\vdots\nonumber \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\rho_t\big(\mathbf{x}_t+\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-1}+\rho_{t-2}\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\mathbf{x}_{t-2}+\dots\big)\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t=\rho_t(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1})
\end{equation}
Plugging this result back to \eqref{eq:gl.9} lets our objective function become:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\label{eq:gl.10}
\end{equation}
Similar to TDC, we also use gradient descent in order to find the minimum value of $\overline{\text{PBE}}(\mathbf{w})$. The gradient of our objective function w.r.t the weight vector $\mathbf{w}$ is:
\begin{align}
\hspace{-1.2cm}\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})&amp;amp;=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &amp;amp;=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\big(\gamma_{t+1}\mathbf{x}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\rho_t\big(\mathbf{x}_t+\gamma_t\lambda_t\mathbf{z}_{t-1}\big)^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\big(\mathbf{x}_t\rho_t\mathbf{x}_t^\text{T}+\mathbf{x}_t\rho_t\gamma_t\lambda_t\mathbf{z}_{t-1}^\text{T}\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}-\big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\mathbf{x}_{t+1}\gamma_{t+1}\lambda_{t+1}\mathbf{z}_t^\text{T}\big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}-\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbf{v}(\mathbf{w}),\label{eq:gl.11}
\end{align}
where in the seventh step, we have used shifting indices trick and the identities:
\begin{align}
\mathbb{E}\Big[\mathbf{x}_t\rho_t\mathbf{x}_t^\text{T}\Big]&amp;amp;=\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big], \\ \mathbb{E}\Big[\mathbf{x}_{t+1}\rho_t\gamma_t\lambda_t\mathbf{z}_t^\text{T}\Big]&amp;amp;=\mathbb{E}\Big[\mathbf{x}_{t+1}\gamma_t\lambda_t\mathbf{z}_t^\text{T}\Big]
\end{align}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from \eqref{eq:gl.11} and following TDC derivation steps we obtain the &lt;strong&gt;GTD($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\text{T}\mathbf{v}_t)\mathbf{x}_{t+1},
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the TD error $\delta_t^s$ is defined, as usual, as state-based TD error \eqref{eq:optcv.1};&lt;/li&gt;
  &lt;li&gt;the eligible trace vector $\mathbf{z}_t$ is defined as given in \eqref{eq:optcv.3} for state value;&lt;/li&gt;
  &lt;li&gt;and $\mathbf{v}_t$ is a vector of the same dimension as $\mathbf{w}$, initialized to $\mathbf{v}_0=\mathbf{0}$ with $\beta&amp;gt;0$ is a step-size parameter:
\begin{align}
\delta_t^s&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\rho_t(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t), \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta(\mathbf{v}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gq-lambda&quot;&gt;GQ($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;GQ($\lambda$)&lt;/strong&gt; is another eligible trace version of a Gradient-TD method but with action values. Its goal is to learn a parameter $\mathbf{w}_t$ such that $\hat{q}(s,a,\mathbf{w}_t)\doteq\mathbf{w}_t^\text{T}\mathbf{x}(s,a)\approx q_\pi(s,a)$ from data given by following a behavior policy $b$.&lt;/p&gt;

&lt;p&gt;Similar to the state-values case of GTD($\lambda$), we begin with the definition of $\lambda$-return (function):
\begin{equation}
G_t^\lambda(q)\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})q(S_{t+1},A_{t+1})+\lambda_{t+1}G_{t+1}^\lambda(q)\Big],\label{eq:gql.1}
\end{equation}
where $q(s,a)$ denotes the value of taking action $a$ at state $s$ and $\lambda\in[0,1]$ is the trace decay parameter.&lt;/p&gt;

&lt;p&gt;Let $T_\pi^\lambda$ denote the $\lambda$-weighted state-action version of the affine $\vert\mathcal{S}\times\mathcal{A}\vert\times\vert\mathcal{S}\times\mathcal{A}\vert$ Bellman operator for the target policy $\pi$ such that:
\begin{align}
q_\pi(s,a)&amp;amp;=\mathbb{E}\Big[G_t^\lambda(q_\pi)\big|S_t=s,A_t=a,\pi\Big] \\ &amp;amp;\doteq(T_\pi^\lambda q_\pi)(s,a)
\end{align}
Analogous to the state value functions, with linear function approximation (i.e., we are trying to estimate $q(s,a)$ by $q_\mathbf{w}(s,a)=\mathbf{w}^\text{T}\mathbf{x}(s,a)$), our objective is to find the fixed point $q_\mathbf{w}$ such that:
\begin{equation}
q_\mathbf{w}=\Pi T_\pi^\lambda q_\mathbf{w},
\end{equation}
where $\Pi$ is the projection operator defined as above. This point also can be found by minimizing the MSPBE objective function:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\left\Vert q_\mathbf{w}-\Pi T_\pi^\lambda q_\mathbf{w}\right\Vert_\mu^2 \\ &amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big),\label{eq:gql.2}
\end{align}
where the second step is acquired from the result \eqref{eq:gl.8}, and where the TD error $\delta_t^\lambda$ is defined as the above section:
\begin{equation}
\delta_t^\lambda(\mathbf{w})\doteq G_t^\lambda(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t
\end{equation}
where $G_t^\lambda$ as given in \eqref{eq:gl.3}.&lt;/p&gt;

&lt;p&gt;In the objective function \eqref{eq:gql.2}, the expectation terms are w.r.t the policy $\pi$, while the data is generated due to the behavior policy $b$. To solve this off-policy issue, as usual, we use importance sampling.&lt;/p&gt;

&lt;p&gt;We start with the definition of the $\lambda$-return \eqref{eq:gql.1}, which is a noisy estimate of the future return by following policy $\pi$. In order to have a noisy estimate for the return of target policy $\pi$ while following behavior policy $b$, we define another $\lambda$-return (function), based on importance sampling:
\begin{equation}
G_t^{\lambda\rho}(\mathbf{w})\doteq R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big],\label{eq:gql.3}
\end{equation}
where $\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$:
\begin{equation}
\bar{\mathbf{x}}_t\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a),
\end{equation}
where $\rho_t$ is the single-step importance sampling ratio, and $G_t^{\lambda\rho}(\mathbf{w})$ is a noisy guess of future rewards of target policy $\pi$, if the agent follows policy $\pi$ from time $t$.&lt;br /&gt;
Let
\begin{equation}
\delta_t^{\lambda\rho}(\mathbf{w})\doteq G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t\label{eq:gql.4}
\end{equation}
With the definition of the $\lambda$-return \eqref{eq:gql.3}, we have that:
\begin{align}
&amp;amp;\hspace{-0.9cm}\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big]\nonumber \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}\Big((1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\big|S_t=s,A_t=a\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &amp;amp;+\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &amp;amp;+\sum_{s’}p(s’|s,a)\sum_{a’}b(a’|s’)\frac{\pi(a’|s’)}{b(a’|s’)}\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\big|S_t=s,A_t=a,\pi\Big]\nonumber \\ &amp;amp;+\sum_{s’,a’}p(s’|s,a)\pi(a’|s’)\gamma_{t+1}\lambda_{t+1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big] \\ &amp;amp;\hspace{-1cm}=\mathbb{E}\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\nonumber \\ &amp;amp;+\gamma_{t+1}\lambda_{t_1}\mathbb{E}\Big[G_{t+1}^{\lambda\rho}(\mathbf{w})\big|S_{t+1}=s’,A_{t+1}=a’\Big]\big|S_t=s,A_t=a,\pi\Big],
\end{align}
which, as continues to roll out, gives us:
\begin{equation}
\mathbb{E}\Big[G_t^{\lambda\rho}(\mathbf{w})\big|S_t=s,A_t=a\Big]=\mathbb{E}\Big[G_t^\lambda(\mathbf{w})\big|S_t=s,A_t=a,\pi\Big]
\end{equation}
And eventually, it yields:
\begin{equation}
\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]=\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t,
\end{equation}
because the state-action distribution is based on the behavior state-action pair distribution, $\mu$.&lt;/p&gt;

&lt;p&gt;Hence, the objective function \eqref{eq:gql.2} can be written as:
\begin{align}
\overline{\text{PBE}}(\mathbf{w})&amp;amp;=\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big)^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\Big(\mathcal{P}_\mu^\pi\delta_t^\lambda(\mathbf{w})\mathbf{x}_t\Big) \\ &amp;amp;=\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t\Big]^{-1}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]\label{eq:gql.5}
\end{align}
From the definition of the importance-sampling based TD error\eqref{eq:gql.4}, we have:
\begin{align}
&amp;amp;\hspace{-0.8cm}\delta_t^{\lambda\rho}(\mathbf{w})\nonumber \\ &amp;amp;\hspace{-1cm}=G_t^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;\hspace{-1cm}=R_{t+1}+\gamma_{t+1}\Big[(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\Big]-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;\hspace{-1cm}=\Big[R_{t+1}+\gamma_{t+1}(1-\lambda_{t+1})\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\Big]+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_t \\ &amp;amp;\hspace{-1cm}=\Big(R_{t+1}+\gamma_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\Big)-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w}) \\ &amp;amp;\hspace{-1cm}=\delta_t(\mathbf{w})-\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}G_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &amp;amp;\hspace{1cm}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big) \\ &amp;amp;\hspace{-1cm}=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\Big(G_{t+1}^{\lambda\rho}(\mathbf{w})-\mathbf{w}^\text{T}\mathbf{x}_{t+1}\Big)+\gamma_{t+1}\lambda_{t+1}\Big(\rho_{t+1}\mathbf{w}^\text{T}\mathbf{x}_{t+1}-\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}\Big) \\ &amp;amp;\hspace{-1cm}=\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big),
\end{align}
where in the fifth step, we define:
\begin{equation}
\delta_t(\mathbf{w})\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t\label{eq:gql.6}
\end{equation}
Note that the last part of the above equation has expected value of vector zero under the behavior policy $b$ because:
\begin{align}
\mathbb{E}\Big[\rho_t\mathbf{x}_t\big|S_t\Big]&amp;amp;=\sum_a b(a|S_t)\frac{\pi(a|S_t)}{b(a|S_t)}\mathbf{x}(S_t,a) \\ &amp;amp;=\sum_a\pi(a|S_t)\mathbf{x}(S_t,a) \\ &amp;amp;=\bar{\mathbf{x}}_t
\end{align}
With the result obtained above, we have:
\begin{align}
\hspace{-1cm}\mathbb{E}\Big[\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]&amp;amp;=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &amp;amp;\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\Big)\mathbf{x}_t\Big]\nonumber \\ &amp;amp;\hspace{2cm}+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\mathbf{x}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_t\Big]+0 \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}_b\Big[\gamma_t\lambda_t\rho_t\Big(\delta_t(\mathbf{w})+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\nonumber \\ &amp;amp;\hspace{2cm}+\gamma_{t+1}\lambda_{t+1}\mathbf{w}^\text{T}\big(\rho_{t+1}\mathbf{x}_{t+1}-\bar{\mathbf{x}}_{t+1}\big)\Big)\mathbf{x}_{t-1}\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{x}_t\Big]+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\delta_t(\mathbf{w})\mathbf{x}_{t-1}\Big]\nonumber \\ &amp;amp;\hspace{2cm}+\mathbb{E}\Big[\gamma_t\lambda_t\rho_t\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\delta_{t+1}^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-1}\Big]+0 \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}\big)\Big]+\mathbb{E}\Big[\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}\Big] \\ &amp;amp;\hspace{0.3cm}\vdots\nonumber \\ &amp;amp;=\mathbb{E}_b\Big[\delta_t(\mathbf{w})\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{x}_{t-1}+\gamma_{t-1}\lambda_{t-1}\rho_{t-1}\gamma_t\lambda_t\rho_t\delta_t^{\lambda\rho}(\mathbf{w})\mathbf{x}_{t-2}+\dots\Big)\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big],
\end{align}
where
\begin{equation}
\mathbf{z}_t\doteq\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\label{eq:gql.7}
\end{equation}
Plugging this result back to our objective function \eqref{eq:gql.5} gives us:
\begin{equation}
\overline{\text{PBE}}(\mathbf{w})=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
Following the derivation of GTD($\lambda$), we have:
\begin{align}
&amp;amp;-\frac{1}{2}\nabla_\mathbf{w}\overline{\text{PBE}}(\mathbf{w})\nonumber \\ &amp;amp;=-\frac{1}{2}\nabla_\mathbf{w}\Bigg(\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]^\text{T}\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]\Bigg) \\ &amp;amp;=\nabla_\mathbf{w}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\big(\gamma_{t+1}\bar{\mathbf{x}}_{t+1}-\mathbf{x}_t\big)\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\Big(\mathbf{x}_t+\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}\Big)^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_t\lambda_t\rho_t\mathbf{x}_t\mathbf{z}_{t-1}^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_{t+1}\lambda_{t+1}\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\Big(\mathbf{x}_t\mathbf{x}_t^\text{T}+\gamma_{t+1}\lambda_{t+1}\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big)\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}-\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big] \\ &amp;amp;=\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]-\mathbb{E}\Big[\gamma_{t+1}(1-\lambda_{t+1})\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big]\mathbf{v}(\mathbf{w}),
\end{align}
where in the eighth step, we have used the identity:
\begin{equation}
\mathbb{E}\Big[\rho_{t+1}\mathbf{x}_{t+1}\mathbf{z}_t^\text{T}\Big]=\mathbb{E}\Big[\bar{\mathbf{x}}_{t+1}\mathbf{z}_t^\text{T}\Big],
\end{equation}
and where in the final step, we define:
\begin{equation}
\mathbf{v}(\mathbf{w})\doteq\mathbb{E}\Big[\mathbf{x}_t\mathbf{x}_t^\text{T}\Big]^{-1}\mathbb{E}\Big[\delta_t(\mathbf{w})\mathbf{z}_t\Big]
\end{equation}
By direct sampling from the above gradient-descent direction and weight-duplication trick, we obtain the &lt;strong&gt;GQ($\lambda$)&lt;/strong&gt; algorithm:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{z}_t-\alpha\gamma_{t+1}(1-\lambda_{t+1})(\mathbf{z}_t^\text{T}\mathbf{v}_t)\bar{\mathbf{x}}_{t+1},
\end{equation}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\bar{\mathbf{x}}_t$ is the average feature vector for $S_t$ under the target policy $\pi$;&lt;/li&gt;
  &lt;li&gt;$\delta_t^a$ is the expectation form of the TD error, defined as \eqref{eq:gql.6};&lt;/li&gt;
  &lt;li&gt;the eligible trace vector $\mathbf{z}_t$ is defined as \eqref{eq:gql.7} for action value;&lt;/li&gt;
  &lt;li&gt;and $\mathbf{v}_t$ is defined as in GTD($\lambda$):
\begin{align}
\bar{\mathbf{x}}_t&amp;amp;\doteq\sum_a\pi(a|S_t)\mathbf{x}(S_t,a), \\ \delta_t^a&amp;amp;\doteq R_{t+1}+\lambda_{t+1}\mathbf{w}^\text{T}\bar{\mathbf{x}}_{t+1}-\mathbf{w}^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\gamma_t\lambda_t\rho_t\mathbf{z}_{t-1}+\mathbf{x}_t, \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^a\mathbf{z}_t-\beta(\mathbf{v}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;greedy-gq-lambda&quot;&gt;Greedy-GQ($\lambda$)&lt;/h4&gt;
&lt;p&gt;If the target policy is $\varepsilon$-greedy, or otherwise biased towards the greedy policy for $\hat{q}$, then GQ($\lambda$) can be used as a control algorithm, called &lt;strong&gt;Greedy-GQ($\lambda$)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;In the case of $\lambda=0$, called GQ(0), Greedy-GQ($\lambda$) is defined by:
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\delta_t^a\mathbf{x}_t+\alpha\gamma_{t+1}(\mathbf{z}_t^\text{T}\mathbf{x}_t)\mathbf{x}(S_{t+1},a_{t+1}^{*}),
\end{equation}
where the eligible trace $\mathbf{z}_t$, TD error $\delta_t^a$ and $a_{t+1}^{*}$ are defined as:
\begin{align}
\mathbf{z}_t&amp;amp;\doteq\mathbf{z}_t+\beta\delta_t^a\mathbf{x}_t-\beta(\mathbf{z}_t^\text{T}\mathbf{x}_t)\mathbf{x}_t, \\ \delta_t^a&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\max_a\Big(\mathbf{w}_t^\text{T}\mathbf{x}(S_{t+1},a)\Big)-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ a_{t+1}^{*}&amp;amp;\doteq\arg\max_a\Big(\mathbf{w}_t^\text{T}\mathbf{x}(S_{t+1},a)\Big),
\end{align}
where $\beta&amp;gt;0$ is a step-size parameter.&lt;/p&gt;

&lt;h3 id=&quot;htd-lambda&quot;&gt;HTD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;HTD($\lambda$)&lt;/strong&gt; is a hybrid state-value algorithm combining aspects of GTD($\lambda$) and TD($\lambda$).&lt;/p&gt;

&lt;p&gt;HTD($\lambda$) has the following update:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\delta_t^s\mathbf{z}_t+\alpha\left(\left(\mathbf{z}_t-\mathbf{z}_t^b\right)^\text{T}\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{v}_{t+1}&amp;amp;\doteq\mathbf{v}_t+\beta\delta_t^s\mathbf{z}_t-\beta\left({\mathbf{z}_t^b}^\text{T}\mathbf{v}_t\right)\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right), \\ \mathbf{z}_t&amp;amp;\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+\mathbf{x}_t\right), \\ \mathbf{z}_t^b&amp;amp;\doteq\gamma_t\lambda_t\mathbf{z}_{t-1}^b+\mathbf{x}_t,
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;em-td-lambda&quot;&gt;Emphatic TD($\lambda$)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Emphatic TD($\lambda$) (ETD($\lambda$))&lt;/strong&gt; is the extension of the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#em-td&quot;&gt;one-step Emphatic-TD algorithm&lt;/a&gt; to eligible traces.&lt;/p&gt;

&lt;p&gt;Emphatic TD($\lambda$) or ETD($\lambda$) is defined by:
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\delta_t\mathbf{z}_t, \\ \delta_t&amp;amp;\doteq R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t, \\ \mathbf{z}_t&amp;amp;\doteq\rho_t\left(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\right), \\ M_t&amp;amp;\doteq\gamma_t i(S_t)+(1-\lambda_t)F_t, \\ F_t&amp;amp;\doteq\rho_{t-1}\gamma_t F_{t-1}+i(S_t),
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$M_t\geq 0$ is the general form of &lt;strong&gt;emphasis&lt;/strong&gt;;&lt;/li&gt;
  &lt;li&gt;$i:\mathcal{S}\to[0,\infty)$ is the &lt;strong&gt;interest function&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;$F_t\geq 0$ is the &lt;strong&gt;followon trace&lt;/strong&gt;, with $F_0\doteq i(S_0)$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;etd-stability&quot;&gt;Stability&lt;/h4&gt;
&lt;p&gt;Consider any stochastic algorithm of the form,
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t),
\end{equation}
where $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector that varies over time. Let
\begin{align}
\mathbf{A}&amp;amp;\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right], \\ \mathbf{b}&amp;amp;\doteq\lim_{t\to\infty}\mathbb{E}\left[\mathbf{b}_t\right]
\end{align}
We define the stochastic update to be &lt;strong&gt;stable&lt;/strong&gt; if and only if the corresponding deterministic algorithm,
\begin{equation}
\bar{\mathbf{w}}_{t+1}\doteq\bar{\mathbf{w}}_t+\alpha\left(\mathbf{b}-\mathbf{A}\bar{\mathbf{w}}_t\right),
\end{equation}
is convergent to a unique fixed point independent of the initial $\bar{\mathbf{w}}_0$. This will occur iff $\mathbf{A}$ has a full set of eigenvalues having positive real parts, which can be proved if $\mathbf{A}$ is positive definite.&lt;/p&gt;

&lt;p&gt;With this definition of stability, in order to exam the stability of ETD($\lambda$), we begin by considering the SGD update for the weight vector $\mathbf{w}$ at time step $t$.
\begin{align}
\mathbf{w}_{t+1}&amp;amp;\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma_{t+1}\mathbf{w}_t^\text{T}\mathbf{x}_{t+1}-\mathbf{w}_t^\text{T}\mathbf{x}_t\right)\mathbf{z}_t \\ &amp;amp;=\mathbf{w}_t+\alpha\left(\mathbf{z}_t R_{t+1}-\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\text{T}\mathbf{w}_t\right)\label{eq:es.1}
\end{align}
Let $\mathbf{A}_t\in\mathbb{R}^d\times\mathbb{R}^d$ be a matrix and $\mathbf{b}_t\in\mathbb{R}^d$ be a vector such that:
\begin{align}
\mathbf{A}_t&amp;amp;\doteq\mathbf{z}_t\left(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\right)^\text{T}, \\ \mathbf{b}_t&amp;amp;\doteq\mathbf{z}_t R_{t+1}
\end{align}
The stochastic update \eqref{eq:es.1} is then can be written as:
\begin{align}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left(\mathbf{b}_t-\mathbf{A}_t\mathbf{w}_t\right)
\end{align}
From the definition of $\mathbf{A}$, we have:
\begin{align}
\mathbf{A}&amp;amp;=\lim_{t\to\infty}\mathbb{E}\left[\mathbf{A}_t\right] \\ &amp;amp;=\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\rho_t\big(\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big)\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &amp;amp;=\sum_s\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]\mathbb{E}_b\Big[\rho_t\big(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1}\big)^\text{T}\big|S_t=s\Big] \\ &amp;amp;=\sum_s\underbrace{\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big]}_{\mathbf{z}(s)}\mathbb{E}_b\Big[\rho_k\big(\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big)^\text{T}\big|S_k=s\Big] \\ &amp;amp;=\sum_s\mathbf{z}(s)\mathbb{E}_\pi\Big[\mathbf{x}_k-\gamma_{k+1}\mathbf{x}_{k+1}\big|S_k=s\Big] \\ &amp;amp;=\sum_s\mathbf{z}(s)\Big(\mathbf{x}_t-\sum_{s’}\left[\mathbf{P}_\pi\right]_{ss’}\gamma(s’)\mathbf{x}(s’)\Big)^\text{T} \\ &amp;amp;=\mathbf{Z}\left(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\right)\mathbf{X},\label{eq:es.2}
\end{align}
where&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;in the fifth step, given $S_t=s$, $\mathbf{z}_{t-1}$ and $M_t$ are independent of $\rho_t(\mathbf{x}_t-\gamma_{t+1}\mathbf{x}_{t+1})^\text{T}$;&lt;/li&gt;
  &lt;li&gt;$\mathbf{P}_\pi$ represents the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of transition probabilities:
\begin{equation}
\left[\mathbf{P}_\pi\right]_{ij}\doteq\sum_a\pi(a|i)p(j|i,a),
\end{equation}
where $p(j|i,a)\doteq P(S_{t+1}=j|S_i=s,A_i=a)$.&lt;/li&gt;
  &lt;li&gt;$\mathbf{Z}$ is a $\vert\mathcal{S}\vert\times d$ matrix, whose rows are $\mathbf{z}(s)$’s (i.e., $\mathbf{Z}^\text{T}\doteq\left[\mathbf{z}(s_1),\dots,\mathbf{z}(s_{\vert\mathcal{S}\vert})\right]$), with $\mathbf{z}(s)\in\mathbb{R}^d$ is a vector defined by&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:
\begin{align}
\mathbf{z}(s)&amp;amp;\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_t\lambda_t\mathbf{z}_{t-1}+M_t\mathbf{x}_t\big|S_t=s\Big] \\ &amp;amp;=\underbrace{\mu_(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big]}_{m(s)}\mathbf{x}_t+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_t=s\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}p(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s) \\ &amp;amp;\hspace{2cm}\times\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\mu(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)} \\ &amp;amp;\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\mathbf{z}_{t-1}\big|S_{t-1}=\bar{s},A_{t-1}=\bar{a}\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s},\bar{a}}\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})} \\\ &amp;amp;\hspace{2cm}\times\lim_{t\to\infty}\mathbb{E}_b\Big[\gamma_{t-1}\lambda_{t-1}\mathbf{z}_{t-2}+M_{t-1}\mathbf{x}_{t-1}\big|S_t=s\Big] \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\Big(\sum_{\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\Big)\mathbf{z}(\bar{s}) \\ &amp;amp;=m(s)\mathbf{x}(s)+\gamma(s)\lambda(s)\sum_{\bar{s}}\left[\mathbf{P}_\pi\right]_{\bar{s}s}\mathbf{z}(\bar{s})\label{eq:es.3}
\end{align}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We now introduce three $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrices:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathbf{M}$, which has the $m(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big\vert S_t=s\Big]$ on its diagonal;&lt;/li&gt;
  &lt;li&gt;$\mathbf{\Gamma}$, which has the $\gamma(s)$ on its diagonal;&lt;/li&gt;
  &lt;li&gt;$\mathbf{\Lambda}$, which has the $\lambda(s)$ on its diagonal.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these matrices, we can rewrite \eqref{eq:es.3} in matrix form, as:
\begin{align}
\mathbf{Z}^\text{T}&amp;amp;=\mathbf{X}^\text{T}\mathbf{M}+\mathbf{Z}^\text{T}\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda} \\ \Rightarrow\mathbf{Z}^\text{T}&amp;amp;=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}
\end{align}
Substitute this equation back to \eqref{eq:es.2}, we obtain:
\begin{equation}
\mathbf{A}=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}\label{eq:es.4}
\end{equation}
Doing similar steps, we can also obtain the ETD($\lambda$)’s $\mathbf{b}$ vector:
\begin{equation}
\mathbf{b}=\mathbf{Z}\mathbf{r}_\pi=\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{r}_\pi,
\end{equation}
where $\mathbf{r}_\pi\in\mathbb{R}^{\vert\mathcal{S}\vert}$ is the vector of expected immediate rewards from each state under $\pi$.&lt;/p&gt;

&lt;p&gt;Since the positive definiteness of $\mathbf{A}$ implies the stability of the algorithm, from \eqref{eq:es.4}, it is sufficient to prove the positive definiteness of the &lt;strong&gt;key matrix&lt;/strong&gt; $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})$ because this matrix can be written in the form of:
\begin{equation}
\mathbf{X}^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{X}=\sum_{i=1}^{\vert\mathcal{S}\vert}\mathbf{x}_i^\text{T}\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})\mathbf{x}_i
\end{equation}
To prove this definiteness, we begin by writing the last part of the key matrix in form of the identity matrix minus a probability matrix.&lt;/p&gt;

&lt;p&gt;Let $\mathbf{P}_\pi^\lambda$ be the matrix with this probability as its $\{ij\}$-component. This matrix can be written as:
\begin{align}
\mathbf{P}_\pi^\lambda&amp;amp;=\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda})+\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{\Lambda}\mathbf{P}_\pi\mathbf{\Gamma})^2(\mathbf{I}-\mathbf{\Gamma}) \\ &amp;amp;=\left(\sum_{k=0}^{\infty}(\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^k\right)\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}\mathbf{P}_\pi\mathbf{\Gamma}(\mathbf{I}-\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &amp;amp;=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{P}_\pi\mathbf{\Gamma}-\mathbf{I}+\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda}) \\ &amp;amp;=\mathbf{I}-(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}),
\end{align}
or
\begin{equation}
\mathbf{I}-\mathbf{P}_\pi^\lambda=(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})
\end{equation}
Then our key matrix now can be written as:
\begin{equation}
\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma}\mathbf{\Lambda})^{-1}(\mathbf{I}-\mathbf{P}_\pi\mathbf{\Gamma})=\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)
\end{equation}
In order to prove the positive definiteness of $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$, analogous to the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2022/02/11/func-approx.html#td-fixed-pt-proof&quot;&gt;proof&lt;/a&gt; of the convergence to TD fixed point of semi-gradient TD, we use two lemmas:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;: &lt;em&gt;Any matrix $\mathbf{A}$ is positive definite iff the symmetric matrix $\mathbf{S}=\mathbf{A}+\mathbf{A}^\text{T}$ is positive definite&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Lemma 2&lt;/strong&gt;: &lt;em&gt;Any symmetric real matrix $\mathbf{S}$ is positive definite if all of its diagonal entries are positive and greater than the sum of the corresponding off-diagonal entries&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Since $\mathbf{M}$ is a diagonal matrix whose diagonal is a distribution and $\mathbf{P}_\pi^\lambda$ is a probability matrix, we have that the matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ has a diagonal of non-negative entries, and non-positive off-diagonal entries, and its row sums also are non-negative. Hence, our problem remains to show that the column sums of the key matrix are positive.&lt;/p&gt;

&lt;p&gt;To show this we need to analyze the matrix $\mathbf{M}$, and to do that we first analyze the vector $\mathbf{f}\in\mathbb{R}^{\vert\mathcal{S}\vert}$, which having $f(s)\doteq\mu(s)\lim_{t\to\infty}\mathbb{E}_b\left[F_t|S_t=s\right]$ as its components. We have:
\begin{align}
\hspace{-0.7cm}f(s)&amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[F_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[i(S_t)+\rho_{t-1}\gamma_t F_{t-1}\big|S_t=s\Big] \\ &amp;amp;=\mu(s)i(s)\nonumber \\ &amp;amp;+\mu(s)\gamma(s)\lim_{t\to\infty}\sum_{\bar{s},\bar{a}}P(S_{t-1}=\bar{s},A_{t-1}=\bar{a}|S_t=s)\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}]\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\mu(s)\gamma(s)\sum_{\bar{s},\bar{a}}\frac{\mu(\bar{s})b(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})}{\mu(s)}\frac{\pi(\bar{a}|\bar{s})}{b(\bar{a}|\bar{s})}\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\gamma(s)\sum_{\bar{s},\bar{a}}\pi(\bar{a}|\bar{s})p(s|\bar{s},\bar{a})\mu(\bar{s})\lim_{t\to\infty}\mathbb{E}_b\Big[F_{t-1}\big|S_{t-1}=\bar{s}\Big] \\ &amp;amp;=\mu(s)i(s)+\gamma(s)\sum_s\left[\mathbf{P}_\pi\right]_{\bar{s}s}f(\bar{s})\label{eq:es.5}
\end{align}
Let $\mathbf{i}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be the vector having components $[\mathbf{i}]_s\doteq\mu(s)i(s)$. Equation \eqref{eq:es.5} allows  us to write $\mathbf{f}$ in matrix-vector form, as:
\begin{align}
\mathbf{f}&amp;amp;=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\mathbf{f} \\ &amp;amp;=\mathbf{i}+\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\mathbf{i}+(\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})^2\mathbf{i}+\dots \\ &amp;amp;=\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\right)^{-1}
\end{align}
Back to the definition of $m(s)$, we have:
\begin{align}
m(s)&amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[M_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lim_{t\to\infty}\mathbb{E}_b\Big[\lambda_t i(S_t)+(1-\lambda_t)F_t\big|S_t=s\Big] \\ &amp;amp;=\mu(s)\lambda(s)i(s)+(1-\lambda(s))f(s)
\end{align}
Continuing as usual, we rewrite this equation in matrix-vector form by letting $\mathbf{m}\in\mathbb{R}^{\vert\mathcal{S}\vert}$ be a vector having $m(s)$ as its components:
\begin{align}
\mathbf{m}&amp;amp;=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})\mathbf{f} \\ &amp;amp;=\mathbf{\Lambda}\mathbf{i}+(\mathbf{I}-\mathbf{\Lambda})(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})^{-1}\mathbf{i} \\ &amp;amp;=\Big[\mathbf{\Lambda}(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T})+(\mathbf{I}-\mathbf{\Lambda})\Big]\left(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\right)\mathbf{i} \\ &amp;amp;=\Big(\mathbf{I}-\mathbf{\Lambda}\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\Big)\Big(\mathbf{I}-\mathbf{\Gamma}\mathbf{P}_\pi^\text{T}\Big)^{-1}\mathbf{i} \\ &amp;amp;=\Big(\mathbf{I}-{\mathbf{P}_\pi^\lambda}^\text{T}\Big)^{-1}\mathbf{i}
\end{align}
Let $\mathbf{1}$ denote the column vector with all components equal to $1$. And using the result above, we have the vector of column sums of the key matrix $\mathbf{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)$ is:
\begin{align}
\mathbf{1}^\text{T}{M}(\mathbf{I}-\mathbf{P}_\pi^\lambda)&amp;amp;=\mathbf{m}^\text{T}(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &amp;amp;=\mathbf{i}^\text{T}(\mathbf{I}-\mathbf{P}_\pi^\lambda)^{-1}(\mathbf{I}-\mathbf{P}_\pi^\lambda) \\ &amp;amp;=\mathbf{i}^\text{T}
\end{align}
Instead of having domain of $[0,\infty)$, if we further assume that $i(s)&amp;gt;0,\,\forall s\in\mathcal{S}$, then it implies immediately that the column sums are all positive, the key matrix is positive definite, so is the matrix $\mathbf{A}$, and the ETD($\lambda$) and its expected update are stable.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Doina Precup &amp;amp; Richard S. Sutton &amp;amp; Satinder Singh. &lt;a href=&quot;https://scholarworks.umass.edu/cs_faculty_pubs/80&quot;&gt;Eligibility Traces for Off-Policy Policy Evaluation&lt;/a&gt; (2000). ICML ‘00 Proceedings of the Seventeenth International Conference on Machine Learning. 80.&lt;/p&gt;

&lt;p&gt;[3] Deepmind x UCL. &lt;a href=&quot;https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021&quot;&gt;Reinforcement Learning Lecture Series 2021&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[4] Harm van Seijen &amp;amp; A. Rupam Mahmood &amp;amp; Patrick M. Pilarski &amp;amp; Marlos C. Machado &amp;amp; Richard S. Sutton. &lt;a href=&quot;http://jmlr.org/papers/v17/15-599.html&quot;&gt;True Online Temporal-Difference Learning&lt;/a&gt;. Journal of Machine Learning Research. 17(145):1−40, 2016.&lt;/p&gt;

&lt;p&gt;[5] Hado Van Hasselt &amp;amp; A. Rupam Mahmood &amp;amp; Richard S. Sutton. &lt;a href=&quot;https://www.researchgate.net/publication/263653431_Off-policy_TDl_with_a_true_online_equivalence&quot;&gt;Off-policy TD(λ) with a true online equivalence&lt;/a&gt;. Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014.&lt;/p&gt;

&lt;p&gt;[6] Hamid Reza Maei. &lt;a href=&quot;https://era.library.ualberta.ca/items/fd55edcb-ce47-4f84-84e2-be281d27b16a/view/373459a7-72d1-4de2-bcd5-5f51e2f745e9/Hamid_Maei_PhDThesis.pdf&quot;&gt;Gradient Temporal-Difference Learning Algorithms&lt;/a&gt;. PhD Thesis, University of Alberta, 2011.&lt;/p&gt;

&lt;p&gt;[7] Hamid Reza Maei &amp;amp; Richard S. Sutton &lt;a href=&quot;http://dx.doi.org/10.2991/agi.2010.22&quot;&gt;GQ($\lambda$): A general gradient algorithm for temporal-difference prediction learning with eligibility traces&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[8] Richard S. Sutton &amp;amp; A. Rupam Mahmood &amp;amp; Martha White. &lt;a href=&quot;https://arxiv.org/abs/1503.04269&quot;&gt;An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[9] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;$\mathbf{z}_t$ is a vector random variable, one per time step, while $\mathbf{z}(s)$ is a vector expectation, one per state. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="td-learning" /><category term="eligible-traces" /><category term="function-approximation" /><category term="importance-sampling" /><category term="my-rl" /><summary type="html">Beside $n$-step TD methods, there is another mechanism called Eligible traces that unify TD and Monte Carlo. Setting $\lambda$ in TD($\lambda$) from $0$ to $1$, we end up with a spectrum ranging from TD methods, when $\lambda=0$ to Monte Carlo methods with $\lambda=1$.</summary></entry></feed>