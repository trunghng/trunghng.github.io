<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-24T19:16:49+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Trust Region Policy Optimization</title><link href="http://localhost:4000/2022/11/23/trpo.html" rel="alternate" type="text/html" title="Trust Region Policy Optimization" /><published>2022-11-23T15:26:00+07:00</published><updated>2022-11-23T15:26:00+07:00</updated><id>http://localhost:4000/2022/11/23/trpo</id><content type="html" xml:base="http://localhost:4000/2022/11/23/trpo.html"><![CDATA[<blockquote>
  <p>TRPO.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#coupling-tvd">Coupling &amp; total variation distance</a></li>
      <li><a href="#mdp">Markov Decision Processes</a></li>
      <li><a href="#likelihood-ratio-pg">Likelihood Ratio Policy Gradient</a></li>
    </ul>
  </li>
  <li><a href="#policy-imp">Policy Improvement</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>We begin by recalling the familiar definition of MDPs, also combined with mentioning about likelihood ratio policy gradient, coupling and total variation distance.</p>

<h3 id="coupling-tvd">Coupling &amp; total variation distance</h3>
<p>Consider two probability measures$ \mu$ and $\nu$ on a probability space $\Omega$. One refers a <strong>coupling</strong> of $\mu$ and $\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\mu$ and $\nu$.</p>

<p>Specifically, if $p$ is a joint distribution of $X,Y$ on $\Omega$, then it implies that
\begin{align}
\sum_{y\in\Omega}p(x,y)&amp;=\sum_{y\in\Omega}P(X=x,Y=y)=P(X=x)=\mu(x) \\ \sum_{x\in\Omega}p(x,y)&amp;=\sum_{x\in\Omega}P(X=x,Y=y)=P(Y=y)=\nu(y)
\end{align}
For probability distributions $\mu$ and $\nu$ on $\Omega$ as above, the <strong>total variation distance</strong> between $\mu$ and $\nu$, denoted $\big\Vert\mu-\nu\big\Vert_\text{TV}$, is defined by
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}\doteq\max_{A\subset\Omega}\big\vert\mu(A)-\nu(A)\big\vert
\end{equation}
<strong>Proposition 1</strong><br />
Let $\mu$ and $\nu$ be probability distributions on $\Omega$, we then have
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
<strong>Proof</strong><br />
Let $B=\{x:\mu(x)\geq\nu(x)\}$ and let $A\subset\Omega$. We have
\begin{align}
\mu(A)-\nu(A)&amp;=\mu(A\cap B)+\mu(A\cap B^c)-\nu(A\cap B)-\nu(A\cap B^c) \\ &amp;\leq\mu(A\cap B)-\nu(A\cap B) \\ &amp;\leq\mu(B)-\nu(B)
\end{align}
Analogously, we also have
\begin{equation}
\nu(A)-\mu(A)\leq\nu(B^c)-\mu(B^c)
\end{equation}
Hence, combining these results gives us
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\left(\mu(B)-\nu(B)+\nu(B^c)-\mu(B^c)\right)=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
This proof also implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sum_{x\in\Omega;\,\mu(x)\geq\nu(x)}\mu(x)-\nu(x)
\end{equation}
<strong>Proposition 2</strong><br />
Let $\mu$ and $\nu$ be two probability measures defined in a probability space $\Omega$, we then have that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
<strong>Proof</strong><br />
For any $A\subset\Omega$ and for any coupling $(X,Y)$ of $\mu$ and $\nu$ we have
\begin{align}
\mu(A)-\nu(A)&amp;=P(X\in A)-P(Y\in A) \\ &amp;=P(X\in A,Y\notin A)+P(X\in A,Y\in A)-P(Y\in A) \\ &amp;\leq P(X\in A,Y\notin A) \\ &amp;\leq P(X\neq Y),
\end{align}
which implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\max_{A’\subset\Omega}\big\vert\mu(A’)-\nu(A’)\big\vert\leq P(X\neq Y)\leq\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
Thus, it suffices to construct a coupling $(X,Y)$ of $\mu$ and $\nu$ such that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=P(X\neq Y)
\end{equation}</p>

<h3 id="mdp">Markov Decision Processes</h3>
<p>An infinite-horizon discounted <strong>Markov Decision Process</strong> (<strong>MDP</strong>) is defined as the tuple $(\mathcal{S},\mathcal{A},P,r,\rho_0,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a finite set of states, or <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a finite set of actions, or <strong>action space</strong>.</li>
  <li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$ is the <strong>transition probability distribution</strong>, i.e. $P(s,a,s’)=P(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>.</li>
  <li>$\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.</li>
  <li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li>
</ul>

<p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$. Here, we consider the stochastic policy only.</p>

<p>We continue by letting $\eta(\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\pi$ thereafter
\begin{equation}
\eta(\pi)=\mathbb{E}_{s_0,a_0,\ldots}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right],
\end{equation}
where
\begin{equation}
s_0\sim\rho_0(s_0),\hspace{1cm}a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)
\end{equation}
For a policy $\pi$, the <strong>state value function</strong>, denoted as $V_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$, and the <strong>action value function</strong>, referred as $Q_\pi$, of a state-action pair $(s,a)\in\mathcal{S}\times\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as
\begin{align}
V_\pi(s_t)&amp;=\mathbb{E}_{a_t,s_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right], \\ Q_\pi(s_t,a_t)&amp;=\mathbb{E}_{s_{t+1},a_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right],
\end{align}
where
\begin{equation}
a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)\hspace{1cm}t\geq 0
\end{equation}
Along with these value functions, we will also define the <strong>advantage function</strong> for $\pi$, denoted $A_\pi$, given as
\begin{equation}
A_\pi(s_t,a_t)=Q_\pi(s_t,a_t)-V_\pi(s_t)
\end{equation}</p>

<h3 id="likelihood-ratio-pg">Likelihood Ratio Policy Gradient</h3>
<p>Let $H$ denote the <strong>horizon</strong> of an MDP<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Consider <strong>likelihood ratio policy gradient</strong> problem, in which the policy $\pi_\theta$ is parameterized by a vector $\theta\in\mathbb{R}^n$. The expected return of $\pi_\theta$ is then given by
\begin{equation}
\eta(\pi_\theta)=\mathbb{E}_{P(\tau;\theta)}\left[\left.\sum_{t=0}^{H}\gamma^t r(s_t,a_t)\right\vert\pi_\theta\right]=\sum_{\tau}P(\tau;\theta)R(\tau),
\end{equation}
where</p>
<ul>
  <li>$P(\tau;\theta)$ is the probability distribution induced by the policy $\pi_\theta$, i.e. $s_t$</li>
  <li>$\tau=(s_0,a_0,s_1,a_1,\ldots,s_H,a_H)$ are trajectories generated by rolls out, i.e. $\tau\sim P(\tau;\theta)$.</li>
  <li>$R(\tau)$ is the discounted cumulative rewards along the trajectory $\tau$, given as
\begin{equation}
R(\tau)=\sum_{t=0}^{H}\gamma^t r(s_t,a_t)
\end{equation}</li>
</ul>

<p>The likelihood ratio policy gradient performs a SGA (stochastic gradient ascent) over the policy parameter space $\Theta$ to find a local optimum of $\eta(\pi_\theta)$ by taking into account the policy gradient
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\nabla_\theta\sum_{\tau}P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\nabla_\theta\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_{\tau}P(\tau;\theta)\nabla_\theta\log P(\tau;\theta)R(\tau) \\ &amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big]\label{eq:lrp.1}
\end{align}
This gradient can be approximated with empirical estimate from $m$ trajectories $\tau^{(1)},\ldots,\tau^{(m)}$ under policy $\pi_\theta$
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big] \\ &amp;\approx\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\hat{g},
\end{align}
which is an unbiased estimate of the policy gradient.</p>

<p>Additionally, since
\begin{align}
\nabla_\theta\log P(\tau^{(i)};\theta)&amp;=\nabla_\theta\log\prod_{t=0}^{H}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\ &amp;=\nabla_\theta\sum_{t=0}^{H}\log P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})+\nabla_\theta\sum_{t=0}^{H}\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\ &amp;=\sum_{t=0}^{H}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}),
\end{align}
we can rewrite the policy gradient estimate in a form which no longer require the dynamics model
\begin{equation}
\hat{g}=\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})R(\tau^{(i)})
\end{equation}
Moreover, as
\begin{equation}
\mathbb{E}_{P(\tau\vert\theta)}\Big[\nabla_\theta\log P(\tau;\theta)\Big]=\nabla_\theta\sum_\tau P(\tau;\theta)=\nabla_\theta 1=\mathbf{0},
\end{equation}
a constant baseline in terms of $\theta$ (i.e. independent of $\theta$) $b$ can be inserted into \eqref{eq:lrp.1} to reduce the variance (where $b$ is a vector which can be optimized to minimize the variance). In particular
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta\log P(\tau;\theta)(R(\tau)-b)\Big] \\ &amp;\approx\frac{1}{m}\nabla_\theta\log P(\tau^{(i)};\theta)\left(R(\tau^{(i)})-b\right) \\ &amp;=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)})-b\right)=\hat{g}
\end{align}
By separating $R(\tau^{(i)})$ into sum of discounted rewards from the past and sum of discounted future rewards, we can continue to decompose the estimator $\hat{g}$ as
\begin{align}
\hat{g}&amp;=\sum_{i=1}^{m}\sum_{t=0}^{H}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)}-b)\right) \\ &amp;=\sum_{i=1}^{m}\sum_{t=0}^{H}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\Bigg[\sum_{k=0}^{t}r(s_k^{(i)},a_k^{(i)})+\left(\sum_{k=t+1}^{H}r(s_k^{(i)},a_k^{(i)})-b\right)\Bigg] \<br />
\end{align}</p>

<p>These following are some possible choices of baseline $b$</p>
<ul>
	<li></li>
</ul>

<h2 id="policy-imp">Policy improvement</h2>

<h2 id="references">References</h2>
<p>[1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. <a href="https://dl.acm.org/doi/10.5555/3045118.3045319">Trust Region Policy Optimization</a>. ICML’15, pp 1889–1897, 2015.</p>

<p>[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. <a href="https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">Markov chains and mixing times</a>. American Mathematical Society, 2009.</p>

<p>[3] Jie Tang, Pieter Abbeel. <a href="">On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a></p>

<p>[4] Sham Kakade,  John Langford. <a href="https://dl.acm.org/doi/10.5555/645531.656005">Approximately optimal approximate reinforcement learning</a>. ICML’2, pp. 267–274, 2002.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Any infinite-horizon discounted MDP, as defined in the preceding subsection, can be $\epsilon$-approximated by a finite horizon MDP, using a horizon
\begin{equation}
H_\epsilon=\left\lceil\log_\gamma\left(\frac{\epsilon(1-\gamma)}{R_\text{max}}\right)\right\rceil,
\end{equation}
where
\begin{equation}
R_\text{max}=\max_s\big\vert R(s)\big\vert
\end{equation} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="deep-reinforcement-learning" /><category term="policy-gradient" /><category term="my-rl" /><summary type="html"><![CDATA[Trust Region Policy Optimization]]></summary></entry><entry><title type="html">Deep Q-learning</title><link href="http://localhost:4000/2022/11/18/deep-q-learning.html" rel="alternate" type="text/html" title="Deep Q-learning" /><published>2022-11-18T15:26:00+07:00</published><updated>2022-11-18T15:26:00+07:00</updated><id>http://localhost:4000/2022/11/18/deep-q-learning</id><content type="html" xml:base="http://localhost:4000/2022/11/18/deep-q-learning.html"><![CDATA[<blockquote>
  <p>DQN.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a>
    <ul>
      <li><a href="#lin-func-approx">Linear function approximation</a></li>
      <li><a href="#dqn">Deep Q-learning</a>
        <ul>
          <li><a href="#exp-replay">Experience replay</a></li>
          <li><a href="#target-net">Target network</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#imp-vars">Some improved variants</a>
    <ul>
      <li><a href="#double-dqn">Double deep Q-learning</a></li>
      <li><a href="#prior-rep">Prioritized replay</a></li>
      <li><a href="#duel-net">Dueling network</a></li>
      <li><a href="#rainbow">Rainbow</a></li>
      <li><a href="#c-dqn">C-DQN</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the post <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s’$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{t+1}(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_t(s’)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_t\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banach’s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-11-18/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\sum_{a’}\pi(a’\vert s’)Q_\pi(s’,a’)\right] \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q^*(s’,a’)\right]\label{eq:qvi.2} \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{t+1}(s,a)=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q_t(s’,a’)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-11-18/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{t+1}(s,a)=\mathbb{E}_{s’\sim P(s’\vert s,a)}\left[R(s,a,s’)+\gamma\max_{a’}Q_t(s’,a’)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(s’\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \eqref{eq:ql.1} can be approximated by sampling, as</p>
<ul id="number-list">
	<li>
		At a state, taking (sampling) action $a$ (e.g. due to an $\varepsilon$-greedy policy), we get the next state:
		\begin{equation}
		s'\sim P(s'\vert s,a)
		\end{equation}
	</li>
	<li>Consider the old estimate $Q_t(s,a)$.</li>
	<li>
		Consider the new sample estimate (target):
		\begin{equation}
		Q_\text{target}=R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\label{eq:ql.2}
		\end{equation}
	</li>
	<li>
		Append the new estimate into a running average to iteratively update Q-values:
		\begin{align}
		Q_{t+1}(s,a)&amp;=(1-\alpha)Q_t(s,a)+\alpha Q_\text{target} \\ &amp;=(1-\alpha)Q_t(s,a)+\alpha\left[R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\right]
		\end{align}
	</li>
</ul>

<p>This update rule is in form of a <strong>stochastic process</strong>, and thus, is <a href="#q-learning-td-convergence">guaranteed to converge</a> to the optimal $Q^*$, under the <a href="/2022/01/31/td-learning.html#stochastic-approx-condition">stochastic approximation conditions</a> for the learning rate $\alpha$.
\begin{equation}
\sum_{t=1}^{\infty}\alpha_t(s,a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{t=1}^{\infty}\alpha_t^2(s,a)&lt;\infty,\label{eq:ql.3}
\end{equation}
for all $(s,a)\in\mathcal{S}\times\mathcal{A}$.</p>

<p>The method is so called <strong>Q-learning</strong>, with pseudocode given below.</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an <a href="/2022/02/11/func-approx.html">approximated solution</a>.</p>

<p>In particular, we have tried to find an approximated action-value function $Q_\boldsymbol{\theta}(s,a)$, parameterized by a learnable vector $\boldsymbol{\theta}$, of the action-value function $Q(s,a)$, as
\begin{equation}
Q_\boldsymbol{\theta}(s,a)
\end{equation}
Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\boldsymbol{\theta}$ so as to minimize the loss function
\begin{equation}
L(\boldsymbol{\theta})=\mathbb{E}_{s,a\sim\mu(\cdot)}\Big[\big(Q(s,a)-Q_\boldsymbol{\theta}(s,a)\big)^2\Big]
\end{equation}
The resulting SGD update had the form
\begin{align}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t-\frac{1}{2}\alpha\nabla_\boldsymbol{\theta}\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]^2 \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.1}
\end{align}
However, we could not perform the exact update \eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $y_t$, which can be any approximation of $Q(s_t,a_t)$<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.2}
\end{equation}</p>

<h3 id="lin-func-approx">Linear function approximation</h3>
<p>Recall that, we have applied <a href="/2022/02/11/func-approx.html#lin-func-approx">linear methods</a> as our function approximators:
\begin{equation}
Q_\boldsymbol{\theta}(s,a)=\boldsymbol{\theta}^\text{T}\mathbf{f}(s,a),
\end{equation}
where $\mathbf{f}(s,a)$ represents the <strong>feature vector</strong>, (or <strong>basis functions</strong>) of the state-action pair $(s,a)$.
Linear function approximation allowed us to rewrite \eqref{eq:nql.2} in a simplified form
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\mathbf{f}(s_t,a_t)\label{eq:nql.3}
\end{equation}
The corresponding SGD method for Q-learning and Q-learning with linear function approximation are respectively given in form of
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.4}
\end{equation}
and
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\mathbf{f}(s_t,a_t),\label{eq:nql.5}
\end{equation}
which both replace the $Q_\text{target}$ in \eqref{eq:ql.2} by the one parameterized by $\boldsymbol{\theta}$
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)
\end{equation}
However, in updating $\boldsymbol{\theta}_
{t+1}$, these methods both use the <strong>bootstrapping target</strong>:
\begin{equation}
R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’), 
\end{equation}
which depends on the current value $\boldsymbol{\theta}_t$, and thus will be biased. As a consequence, \eqref{eq:nql.4} does not guarantee to converge<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<p>Such methods are known as <strong>semi-gradient</strong> since they take into account the effect of changing the weight vector $\boldsymbol{\theta}_t$ on the estimate, but ignore its effect on the target.</p>

<h3 id="dqn">Deep Q-learning</h3>
<p>On the other hands, we have already known that a <strong>neural network</strong> with particular settings for hidden layers and activation functions can approximate <a href="/2022/09/02/neural-nets.html#unv-approx">any</a> continuous functions on a compact subsets of $\mathbb{R}^n$, so how about using it with the Q-learning algorithm?</p>

<p>Specifically, we will be using neural network with weight $\boldsymbol{\theta}$ as a function approximator for Q-learning update. The network is referred as <strong>Q-network</strong>, as the whole algorithm is so-called <strong>Deep Q-learning</strong>, and the agent is known as <strong>DQN</strong> in short for <strong>Deep Q-network</strong>.</p>

<p>The Q-network can be trained by minimizing a sequence of loss function $L_t(\boldsymbol{\theta}_t)$ that changes at each iteration $t$:
\begin{equation}
L_t(\boldsymbol{\theta}_t)=\mathbb{E}_{s,a\sim\rho(\cdot)}\Big[\big(y_t-Q_{\boldsymbol{\theta}_t}(s,a)\big)^2\Big],\label{eq:dqn.1}
\end{equation}
where
\begin{equation}
y_t=\mathbb{E}_{s’\sim\mathcal{E}}\left[R(s,a,s’)+\gamma\max_{a’}Q_{\boldsymbol{\theta}_{t-1}}(s’,a’)\vert s,a\right]
\end{equation}
is the target in iteration $t$, which follows as in \eqref{eq:nql.3}; and where $\rho(s,a)$ is referred as the behavior policy.</p>

<p>The TD target $y_t$ can approximated as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)
\end{equation}
To stabilize learning, DQN applies the following mechanisms.</p>

<h4 id="exp-replay">Experience replay</h4>
<p>Along with Q-network, the authors of deep-Q learning also introduce a technique called <strong>experience replay</strong>, which utilizes data efficiency and at the same time reduces the variance of the updates.</p>

<p>In particular, at each time step $t$, the <strong>experience</strong>, $e_t$, defined as
\begin{equation}
e_t=(s_t,a_t,r_t,s_{t+1})
\end{equation}
is added into a set $\mathcal{D}$ of size $N$, which is sampled uniformly at the training time to apply Q-learning updates. This method provides some advantages:</p>
<ul>
  <li>Each experience $e_t$ can be used in many weight updates.</li>
  <li>Uniformly sampling from $\mathcal{D}$ cancels out the correlations between consecutive experiences, i.e. $e_t, e_{t+1}$.</li>
</ul>

<h4 id="target-net">Target network</h4>
<p>DQN introduces a <strong>target network</strong> $\hat{Q}$ parameterized by $\boldsymbol{\theta}^-$to generate the TD target $y_t$ in \eqref{eq:dqn.1} as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a’}\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a’)\label{eq:tn.1}
\end{equation}
The target network $\hat{Q}$ is cloned from $Q$ every $C$ Q-learning update steps, i.e. $\boldsymbol{\theta}^-\leftarrow\boldsymbol{\theta}$.</p>

<h2 id="imp-vars">Improved variants</h2>

<h3 id="double-dqn">Double deep Q-learning</h3>
<p>As stated <a href="/2022/01/31/td-learning.html#max-bias">before</a> that the Q-learning method could lead to over optimistic value estimates. Moreover, Q-learning with function approximation, such as DQN, has also been <a href="#double-dqn-paper">proved</a> to induce maximization bias. These results are due to that in Q-learning and DQN, the $\max$ operator uses the same values to both select and evaluate an action.</p>

<p>To reduce the overoptimism effect due to overestimation in DQN, we use a double estimator version of deep Q-learning, called <strong>Double Deep-Q learning</strong>, as we have used double Q-learning to mitigate the maximization bias in Q-learning.</p>

<p>The <strong>double DQN</strong> agent is similar to DQN, except that it replaces the target \eqref{eq:tn.1}, which can be rewritten as:
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\,\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a)\right),
\end{equation}
with
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\,Q_{\boldsymbol{\theta}_t}(s_{t+1},a)\right)
\end{equation}</p>

<h3 id="prior-rep">Prioritized replay</h3>

<h3 id="duel-net">Dueling network</h3>

<h3 id="rainbow">Rainbow</h3>

<h3 id="c-dqn">C-DQN</h3>

<h2 id="references">References</h2>
<p>[1] <span id="q-learning-td-convergence">Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</span></p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p>

<p>[3] Pieter Abbeel. <a href="https://youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL Series</a>, YouTube, 2021.</p>

<p>[4] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[5] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[6] <span id="double-dqn-paper">Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</span></p>

<p>[7] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>. arXiv:1511.06581, 2015.</p>

<p>[8] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a>. arXiv:1511.05952, 2016.</p>

<p>[9] Taisuke Kobayashi, Wendyam Eric Lionel Ilboudo. <a href="https://arxiv.org/abs/2008.10861">t-Soft Update of Target Network for Deep Reinforcement Learning</a>. arXiv:2008.10861, 2020.</p>

<p>[10] Zhikang T. Wang, Masahito Ueda. <a href="https://arxiv.org/abs/2106.15419">Convergent and Efficient Deep Q Network Algorithm</a>. arXiv:2106.15419, 2022.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In <strong>Monte Carlo control</strong>, the update target $y_t$ is chosen as the <strong>full return</strong> $G_t$, i.e.
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
and in (episodic on-policy) TD control methods, we use the <strong>TD target</strong> as the choice for $y_t$, i.e. for one-step TD methods such as <strong>one-step Sarsa</strong>, the update rule for $\boldsymbol{\theta}$ is given as
\begin{align*}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+1}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t) \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[R(s_t,a_t,s_{t+1})+\gamma Q_{\boldsymbol{\theta}_t}(s_{t+1},a_{t+1})-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{align*}
and for $n$-step TD method, for instance, <strong>$n$-step Sarsa</strong>, we instead have
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+n}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
where
\begin{equation*}
G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^n Q_{\boldsymbol{\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\hspace{1cm}t+n&lt;T
\end{equation*}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ and where $R_{t+1}\doteq R(s_t,a_t,s_{t+1})$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The semi-gradient TD methods with linear function approximation, e.g. \eqref{eq:nql.5}, are guaranteed to converge to the <strong>TD fixed point</strong> due to the <a href="/2022/02/11/func-approx.html#td-fixed-pt-proof">result</a> we have mentioned. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="deep-reinforcement-learning" /><category term="function-approximation" /><category term="q-learning" /><category term="my-rl" /><summary type="html"><![CDATA[Deep Q-learning and variants]]></summary></entry><entry><title type="html">Natural Evolution Strategies</title><link href="http://localhost:4000/2022/10/07/nes.html" rel="alternate" type="text/html" title="Natural Evolution Strategies" /><published>2022-10-07T13:00:00+07:00</published><updated>2022-10-07T13:00:00+07:00</updated><id>http://localhost:4000/2022/10/07/nes</id><content type="html" xml:base="http://localhost:4000/2022/10/07/nes.html"><![CDATA[<blockquote>
  <p><strong>Natural Evolution Strategies</strong>, or <strong>NES</strong>, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#search-grad">Search gradients</a>
    <ul>
      <li><a href="#search-grad-gauss">Search gradients for MVN</a></li>
      <li><a href="#ntr-grad">Natural gradient</a></li>
    </ul>
  </li>
  <li><a href="#rbn-tchnq">Robustness techniques</a>
    <ul>
      <li><a href="#fn-shp">Fitness shaping</a></li>
      <li><a href="#adp-sampl">Adaption sampling</a></li>
    </ul>
  </li>
  <li><a href="#rot-sym-dist">Rotationally-symmetric distributions</a>
    <ul>
      <li><a href="#exp-param">Exponential parameterization</a></li>
      <li><a href="#exp-coords">Exponential local coordinates</a></li>
      <li><a href="#samp-rot-sym-dist">Sampling from rotationally symmetric distributions</a></li>
      <li><a href="#xnes">Exponential Natural Evolution Strategies</a></li>
    </ul>
  </li>
  <li><a href="#test-on-rast">Testing on Rastrigin function</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="search-grad">Search gradients</h2>
<p>Usually when working on <strong>Evolution Strategy</strong> methods, we select some candidate solutions, which generate better fitness values than the other ones, to be parents of the next generation. This means, majority of solution samples have been wasted since they may contain some useful information.</p>

<p>To utilize the use all fitness samples, the <strong>NES</strong> uses <strong>search gradients</strong> in updating the parameters for the search distribution.</p>

<p>Let $\mathbf{z}\in\mathbb{R}^n$ denote the solution sampled from the distribution $\pi(\mathbf{z},\theta)$ and let $f:\mathbb{R}^n\to\mathbb{R}$ be the fitness (or objective) function. The expected fitness value is then given by
\begin{equation}
J(\theta)=\mathbb{E}_\theta[f(\mathbf{z})]=\int f(\mathbf{z})\pi(\mathbf{z}\vert\theta)\,d\mathbf{z}\label{eq:sg.1}
\end{equation}
Taking the gradient of the above function w.r.t $\theta$ using the <strong>log-likelihood trick</strong> as in <a href="/2022/05/04/policy-gradient.html#reinforce">REINFORCE</a> gives us
\begin{align}
\nabla_\theta J(\theta)&amp;=\nabla_\theta\int f(\mathbf{z})\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\int f(\mathbf{z})\nabla_\theta\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\int f(\mathbf{z})\nabla_\theta\pi(\mathbf{z}\vert\theta)\frac{\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\,d\mathbf{z} \\ &amp;=\int\left[f(\mathbf{z})\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\right]\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\mathbb{E}_\theta\left[f(\mathbf{z})\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\right]
\end{align}
Using Monte Carlo method, given samples $\mathbf{z}_1,\ldots,\mathbf{z}_\lambda$ from the population of size $\lambda$, the search gradient is then can be approximated by
\begin{equation}
\nabla_\theta J(\theta)\approx\frac{1}{\lambda}\sum_{k=1}^{\lambda}f(\mathbf{z}_k)\nabla_\theta\log\pi(\mathbf{z}_k\vert\theta)\label{eq:sg.2}
\end{equation}
Given this gradient w.r.t $\theta$, we then can use a gradient-based method to repeatedly update the parameter $\theta$ in order to give us a more desired search distribution. In particular, we can use such as SGD method
\begin{equation}
\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta),\label{eq:sg.3}
\end{equation}
where $\alpha$ is the learning rate.</p>

<h3 id="search-grad-gauss">Search gradients for MVN</h3>
<p>Consider the case that our search distribution $\pi(\mathbf{z}\vert\theta)$ is in form of a Multivariate Normal  distribution, $\mathbf{z}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, where $\boldsymbol{\mu}\in\mathbb{R}^n$ and $\boldsymbol{\Sigma}\in\mathbb{R}^{n\times n}$.</p>

<p>In this case $\theta=(\boldsymbol{\mu},\boldsymbol{\Sigma})$ denotes a tuple of parameters for the search distribution, which is given by
\begin{equation}
\pi(\mathbf{z}\vert\theta)=\frac{1}{(2\pi)^{n/1}\left\vert\boldsymbol{\Sigma}\right\vert^{1/2}}\exp\left[-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right]
\end{equation}
Taking natural logarithm of both sides then gives us
\begin{align}
\log\pi(\mathbf{z}\vert\theta)&amp;=\log\left(\frac{1}{(2\pi)^{n/1}\left\vert\boldsymbol{\Sigma}\right\vert^{1/2}}\exp\left[-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right]\right) \\ &amp;=-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)
\end{align}
We continue by differentiating the above log-likelihood w.r.t $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. Starting with $\boldsymbol{\mu}$, the gradient is given by
\begin{align}
\nabla_\boldsymbol{\mu}\log\pi(\mathbf{z}\vert\theta)&amp;=\nabla_\boldsymbol{\mu}\left(-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right) \\ &amp;=-\frac{1}{2}\nabla_\boldsymbol{\mu}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right) \\ &amp;=\boldsymbol{\Sigma}^{-1}(\mathbf{z}-\boldsymbol{\mu})
\end{align}
And the gradient w.r.t $\boldsymbol{\Sigma}$ is computed as
\begin{align}
\nabla_\boldsymbol{\Sigma}\pi(\mathbf{z}\vert\theta)&amp;=\nabla_\boldsymbol{\Sigma}\left(-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right) \\ &amp;=-\frac{1}{2}\nabla_\boldsymbol{\Sigma}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right) \\ &amp;=\frac{1}{2}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}-\frac{1}{2}\boldsymbol{\Sigma}^{-1}
\end{align}
The SGD update \eqref{eq:sg.3} now is applied for each of $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ as
\begin{align}
\boldsymbol{\mu}&amp;\leftarrow\boldsymbol{\mu}+\alpha\nabla_\boldsymbol{\mu}J(\theta) \\ &amp;\leftarrow\boldsymbol{\mu}+\alpha\frac{1}{\lambda}\sum_{k=1}^{\lambda}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}_k-\boldsymbol{\mu}\right)f(\mathbf{z}_k)
\end{align}
and
\begin{align}
\boldsymbol{\Sigma}&amp;\leftarrow\boldsymbol{\Sigma}+\alpha\nabla_\boldsymbol{\Sigma}J(\theta) \\ &amp;\leftarrow\boldsymbol{\Sigma}+\alpha\frac{1}{\lambda}\sum_{k=1}^{\lambda}\left[\frac{1}{2}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}_k-\boldsymbol{\mu}\right)\left(\mathbf{z}_k-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}-\frac{1}{2}\boldsymbol{\Sigma}^{-1}\right]f(\mathbf{z}_k)
\end{align}</p>

<h3 id="ntr-grad">Natural gradient</h3>
<p>The <strong>natural gradient</strong> searches for the direction based on the distance between distributions $\pi(\mathbf{z}\vert\theta)$ and $\pi(\mathbf{z}\vert\theta’)$. One natural measure of distance between probability distributions is the <strong>Kullback-Leibler divergence</strong>, or <strong>KL divergence</strong>.</p>

<p>In other words, our work is to look for the direction of updating gradient, denoted as $\delta\theta$, such that
\begin{align}
\max_{\delta\theta}&amp;\,J(\theta+\delta\theta)\approx J(\theta)+\delta\theta^\text{T}\nabla_\theta J \\ \text{s.t.}&amp;\,D(\theta\Vert\theta+\delta\theta)=\varepsilon,
\end{align}
where $J(\theta)$ is given as in \eqref{eq:sg.1}; $\varepsilon$ is a small increment size; and where $D(\theta+\delta\theta\Vert\theta)$ is the KL divergence of $\pi(\mathbf{z}\vert\theta+\delta\theta)$ from $\pi(\mathbf{z}\vert\theta)$, defined as
\begin{align}
D(\theta\Vert\theta+\delta\theta)&amp;=\int\pi(\mathbf{z}\vert\theta)\log\frac{\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta+\delta\theta)}\,d\mathbf{z} \\ &amp;=\mathbb{E}_{\theta}\big[\log\pi(\mathbf{z}\vert\theta)-\log\pi(\mathbf{z}\vert\theta+\delta)\big]\label{eq:ng.1}
\end{align}
As $\delta\theta\to 0$, or in other words, consider the Taylor expansion of \eqref{eq:ng.1} about $\delta\theta=0$, we have
\begin{align}
&amp;\hspace{-1cm}D(\theta+\delta\theta\Vert\theta)\nonumber \\ &amp;\hspace{-0.8cm}=\mathbb{E}_{\theta}\big[\log\pi(\mathbf{z}\vert\theta)-\log\pi(\mathbf{z}\vert\theta+\delta\theta)\big] \\ &amp;\hspace{-0.8cm}\approx\mathbb{E}_\theta\left[\log\pi(\mathbf{z}\vert\theta)-\left(\log\pi(\mathbf{z}\vert\theta)+\delta\theta^\text{T}\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}+\frac{1}{2}\delta\theta^\text{T}\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\left(\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\right)^\text{T}\delta\theta\right)\right] \\ &amp;\hspace{-0.8cm}=-\mathbb{E}_\theta\left[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)+\frac{1}{2}\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\right] \\ &amp;\hspace{-0.8cm}=-\mathbb{E}_\theta\Big[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\Big]-\mathbb{E}_\theta\left[\frac{1}{2}\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\right] \\ &amp;\hspace{-0.8cm}=-\frac{1}{2}\int\pi(\mathbf{z}\vert\theta)\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\,d\mathbf{z} \\ &amp;\hspace{-0.8cm}=-\frac{1}{2}\delta\theta^\text{T}\mathbf{F}\delta\theta\label{eq:ng.2}
\end{align}
where in the fifth step, we have used that
\begin{align}
\mathbb{E}_\theta\Big[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\Big]&amp;=\delta\theta^\text{T}\int\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\int\pi(\mathbf{z}\vert\theta)\frac{1}{\pi(\mathbf{z}\vert\theta)}\nabla_\theta\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\nabla_\theta\int\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\nabla_\theta 1=0
\end{align}</p>

<p>The matrix $\mathbf{F}$ in \eqref{eq:ng.2} is known as the <strong>Fisher information matrix</strong> of the given parametric family of search distributions, defined as
\begin{align}
\mathbf{F}&amp;=\int\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\,d\mathbf{z} \\ &amp;=\mathbb{E}_\theta\big[\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\big]
\end{align}
Hence, we have the Lagrangian of our constrained optimization problem is
\begin{align}
\mathcal{L}(\theta,\delta\theta,\lambda)&amp;=J(\theta)+\delta\theta^\text{T}\nabla_\theta J(\theta)+\lambda\big(D(\theta+\delta\theta\Vert\theta)-\varepsilon\big) \\ &amp;=J(\theta)+\delta\theta^\text{T}\nabla_\theta J(\theta)-\lambda\left(\frac{1}{2}\delta\theta^\text{T}\mathbf{F}\delta\theta+\varepsilon\right),
\end{align}
where $\lambda&gt;0$ is the Lagrange multiplier.</p>

<p>It is easily seen that $\mathbf{F}$ is symmetric, thus taking the gradient of the Lagrangian w.r.t $\delta\theta$ and setting it to zero gives us
\begin{equation}
\lambda\mathbf{F}\delta\theta=\nabla_\theta J(\theta)
\end{equation}
If the Fisher information matrix $\mathbf{F}$ is invertible, the solution for $\delta\theta$ that maximizes $\mathcal{L}$ then can be computed as
\begin{equation}
\delta\theta=\frac{1}{\lambda}\mathbf{F}^{-1}\nabla_\theta J(\theta),\label{eq:ng.3}
\end{equation}
which defines the direction of the natural gradient $\tilde{\nabla}_\theta J(\theta)$. Since $\lambda&gt;0$ we therefore obtain
\begin{equation}
\tilde{\nabla}_\theta J(\theta)=\mathbf{F}^{-1}\nabla_\theta J(\theta)
\end{equation}
Continue with the value of $\delta\theta$ given in \eqref{eq:ng.3}, the dual function of our optimization is given as
\begin{align}
g(\lambda)&amp;=J(\theta)+\frac{1}{\lambda}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta  J(\theta)-\frac{1}{2}\frac{\lambda}{\lambda^2}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\mathbf{F}\mathbf{F}^{-1}\nabla_\theta J(\theta)-\lambda\varepsilon \\ &amp;=J(\theta)+\frac{1}{2}\lambda^{-1}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)-\lambda\varepsilon
\end{align}
Taking the gradient of $g$ w.r.t $\lambda$ and setting it to zero and since $\varepsilon&lt;0$ small gives us the solution for $\lambda$, which is
\begin{equation}
\lambda=\sqrt{\frac{\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)}{\varepsilon}},
\end{equation}
Hence, the SGD update for the parameter $\theta$ using natural gradient is
\begin{equation}
\theta\leftarrow\theta+\eta\tilde{\nabla}_\theta J(\theta)=\theta+\eta\mathbf{F}^{-1}\nabla_\theta J(\theta),\label{eq:ng.4}
\end{equation}
where $\eta$ is the learning rate, given as
\begin{equation}
\eta=\lambda^{-1}=\sqrt{\frac{\varepsilon}{\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)}}
\end{equation}
This learning rate can also be replaced by a more desirable one without changing the direction of our update. With this update rule for natural gradient, we obtain the general formulation of NES, as described in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-10-07/nes.png" alt="NES" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h2 id="rbn-tchnq">Robustness techniques</h2>

<h3 id="fn-shp">Fitness shaping</h3>
<p>NES uses the so-called <strong>fitness shaping</strong> technique, which helps to avoid early convergence due to the possible affection of outliers fitness value in \eqref{eq:sg.2}, e.g. there may exist an outlier whose fitness value, says $f(\mathbf{z}_i)$, is much greater than other solutions’ ones, $\{f(\mathbf{z}_k)\}_{k\neq i}$.</p>

<p>Rather than using fitness values $f(\mathbf{z}_k)$ in approximating the gradient in \eqref{eq:sg.2}, fitness shaping instead applies a rank-based transformation of $f(\mathbf{z}_k)$.</p>

<p>In particular, let $\mathbf{z}_{k:\lambda}$ denote the $k$-th best sample out of the population of size $\lambda$, $\mathbf{z}_1,\ldots,\mathbf{z}_\lambda$, i.e. $f(\mathbf{z}_{1:\lambda})\geq\ldots\geq f(\mathbf{z}_{\lambda:\lambda})$, the gradient estimate \eqref{eq:sg.2} now is rewritten as
\begin{equation}
\nabla_\theta J(\theta)=\sum_{k=1}^{\lambda}u_k\nabla_\theta\log\pi(\mathbf{z}_{k:\lambda}\vert\theta),\label{eq:fs.1}
\end{equation}
where $u_1\geq\ldots\geq u_\lambda$ are referred as <strong>utility values</strong>, which are preserved-order transformations of $f(\mathbf{z}_{1:\lambda}),\ldots,f(\mathbf{z}_{\lambda:\lambda})$.</p>

<p>The choice for utility function $u$ is a free parameter of the algorithm. In the original paper, the author proposed
\begin{equation}
u_k=\frac{\max\left(0,\log\left(\frac{\lambda}{2}+1\right)-\log k\right)}{\sum_{j=1}^{\lambda}\max\left(0,\log\left(\frac{\lambda}{2}+1\right)-\log j\right)}-\frac{1}{\lambda}
\end{equation}</p>

<h3 id="adp-sampl">Adaption sampling</h3>
<p>Beside fitness shaping, NES also applies another heuristic, called <strong>adaption sampling</strong>, to make the performance more robustly. This technique lets the algorithm determine the appropriate hyperparameters (in this case, NES chooses the learning rate $\eta$ be the one to adapt) more quickly.</p>

<p>In particular, for a successive parameter $\theta’$ of $\theta$, the corresponding learning rate $\eta$ used in its update \eqref{eq:ng.4} will be determined by comparing samples $\mathbf{z}’$ sampled from $\pi_\theta’$ with samples $\mathbf{z}$ sampled from $\pi_\theta$ according to a <strong>Mann-Whitney U-test</strong>.</p>

<h2 id="rot-sym-dist">Rotationally-symmetric distributions</h2>
<p>The <strong>rotationally-symmetric distributions</strong>, or <strong>radial distributions</strong> refer to class of distributions $p(\mathbf{x})$ such that
\begin{equation}
p(\mathbf{x})=p(\mathbf{U}\mathbf{x}),\label{eq:rsd.1}
\end{equation}
for all $\mathbf{x}\in\mathbb{R}^n$ and for all orthogonal matrices $\mathbf{U}\in\mathbb{R}^{n\times n}$.</p>

<p>Let $Q_\boldsymbol{\tau}(\mathbf{z})$ be a family of rotationally-symmetric distributions in $\mathbb{R}^n$ parameterized by $\boldsymbol{\tau}$. The property  \eqref{eq:rsd.1} allows us to represent $Q_\boldsymbol{\tau}(\mathbf{z})$ as
\begin{equation}
Q_\boldsymbol{\tau}(\mathbf{z})=q_\boldsymbol{\tau}(\Vert\mathbf{z}\Vert^2),
\end{equation}
for some family of functions $q_\boldsymbol{\tau}:\mathbb{R}_+\to\mathbb{R}_+$.</p>

<p>Consider the classes of search distributions in a form of
\begin{align}
\pi(\mathbf{z}\vert\boldsymbol{\mu},\boldsymbol{\Sigma},\boldsymbol{\tau})&amp;=\frac{1}{\vert\mathbf{A}\vert}q_\boldsymbol{\tau}\left(\left\Vert(\mathbf{A}^{-1})^\text{T}(\mathbf{z}-\boldsymbol{\mu})\right\Vert^2\right) \\ &amp;=\frac{1}{\left\vert\mathbf{A}^\text{T}\mathbf{A}\right\vert^{1/2}}q_\boldsymbol{\tau}\left((\mathbf{z}-\boldsymbol{\mu})^\text{T}(\mathbf{A}^\text{T}\mathbf{A})^{-1}(\mathbf{z}-\boldsymbol{\mu})\right),\label{eq:rsd.2}
\end{align}
with additional transformation parameters $\boldsymbol{\mu}\in\mathbb{R}^n$ and invertible matrices $\mathbf{A}\in\mathbb{R}^{n\times n}$.</p>

<p>It can be seen that Gaussian and its multivariate form, MVN, can be written in form of $\eqref{eq:rsd.2}$, and thus are members of these classes of distributions.</p>

<h3 id="exp-param">Exponential parameterization</h3>
<p>By \eqref{eq:ng.4}, the natural gradient update for a multivariate Gaussian search distribution, denoted $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, is
\begin{align}
\boldsymbol{\mu}&amp;\leftarrow\boldsymbol{\mu}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\mu} J(\boldsymbol{\mu},\boldsymbol{\Sigma}), \\ \boldsymbol{\Sigma}&amp;\leftarrow\boldsymbol{\Sigma}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\Sigma} J(\boldsymbol{\mu},\boldsymbol{\Sigma})
\end{align}
Thus, in updating the covariance matrix $\boldsymbol{\Sigma}$ as above, we have to ensure that $\boldsymbol{\Sigma}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\Sigma} J(\boldsymbol{\mu},\boldsymbol{\Sigma})$ is symmetric positive definite.</p>

<p>To accomplish this, we may represent the covariance matrix using the <strong>exponential parameterization</strong> for symmetric matrices. In particular, let
\begin{equation}
\mathcal{S}_n\doteq\{\mathbf{M}\in\mathbb{R}^{n\times n}:\mathbf{M}=\mathbf{M}^\text{T}\}
\end{equation}
denote the set of symmetric matrices of $\mathbb{R}^{n\times n}$ and let
\begin{equation}
\mathcal{P}_n\doteq\{\mathbf{M}\in\mathcal{S}_n:\mathbf{M}\succ 0\}
\end{equation}
represent the cone of symmetric positive definite matrices of $\mathbb{R}^{n\times n}$.</p>

<p>Using Taylor expansion for the exponential function, we then have the exponential map $\exp:\mathcal{S}_n\to\mathcal{P}_n$ can be written as
\begin{equation}
\exp(\mathbf{M})=\sum_{i=0}^{\infty}\frac{\mathbf{M}^i}{i!},\label{eq:ep.1}
\end{equation}
which is <strong>diffeomorphism</strong>, i.e. the map is bijective, plus the map and its inverse map, $\log:\mathcal{P}_n\to\mathcal{S}_n$, both are differentiable.</p>

<p>Therefore, we can represent the covariance matrix $\boldsymbol{\Sigma}\in\mathcal{P}_n$ as
\begin{equation}
\boldsymbol{\Sigma}=\exp(\mathbf{M}),\hspace{2cm}\mathbf{M}\in\mathcal{S}_n
\end{equation}
This representation lets the gradient update always end up as a valid covariance matrix. However, the computation for the Fisher information matrix $\mathbf{F}$ is consequently more complicated due to require partial derivatives of matrix exponential \eqref{eq:ep.1}.</p>

<h3 id="exp-coords">Exponential local coordinates</h3>
<p>It is noticeable from \eqref{eq:rsd.2} that the dependency of the distribution on $\mathbf{A}$ is only in terms of $\mathbf{A}^\text{T}\mathbf{A}$, which is a symmetric positive semi-definite matrix since for all non-zero vector $\mathbf{x}\in\mathbb{R}^n$ we have
\begin{equation}
\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{A}\mathbf{x}=\Vert\mathbf{A}\mathbf{x}\Vert^2\geq 0
\end{equation}
In the case of MVN, this matrix corresponds to the covariance matrix.</p>

<p>Therefore, rather than using exponential mapping in updating the positive definite matrices $\mathbf{A}^\text{T}\mathbf{A}$, we repeatedly linear transform the coordinate system in each iteration to a coordinate system in which the calculation for $\mathbf{F}$ is trivial.</p>

<p>Specifically, let the current search distribution be given by $(\boldsymbol{\mu},\mathbf{A})$, we use <strong>exponential local coordinates</strong>
\begin{equation}
(\boldsymbol{\delta},\mathbf{M})\mapsto(\boldsymbol{\mu}_\text{new},\mathbf{A}_\text{new})=\left(\boldsymbol{\mu}+\mathbf{A}^\text{T}\boldsymbol{\delta},\mathbf{A}\exp\left(\frac{1}{2}\mathbf{M}\right)\right)
\end{equation}
This coordinate system is local in the sense that the coordinates $(\boldsymbol{\delta},\mathbf{M})=(\mathbf{0},\mathbf{0})$ is mapped to $(\boldsymbol{\mu},\mathbf{A})$.</p>

<p>For the case that $\tau\in\mathbb{R}^{n’}$, $\boldsymbol{\delta}\in\mathbb{R}^n$ and $\mathbf{M}\in\mathbb{R}^{n(n+1)/2}$, the Fisher information matrix $\mathbf{F}$ in this coordinate system is an $m\times m$ matrix, where
\begin{equation}
m=n+\frac{n(n+1)}{2}+n’=\frac{n(n+3)}{2}+n’,
\end{equation}
and is given as
\begin{equation}
\mathbf{F}=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{V} \\ \mathbf{V}^\text{T}&amp;\mathbf{C}\end{matrix}\right],\label{eq:ec.1}
\end{equation}
where
\begin{equation}
\mathbf{V}=\frac{\partial^2\log\pi(\mathbf{z})}{\partial(\boldsymbol{\delta},\mathbf{M})\partial\boldsymbol{\tau}}\in\mathbb{R}^{(m-n’)\times n’},\hspace{1cm}\mathbf{C}=\frac{\partial^2\log\pi(\mathbf{z})}{\partial\boldsymbol{\tau}^2}\in\mathbb{R}^{n’\times n’}
\end{equation}
Using the <strong>Woodbury identity</strong> for $\mathbf{F}$ gives us its inverse
\begin{equation}
\mathbf{F}^{-1}=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{V} \\ \mathbf{V}^\text{T}&amp;\mathbf{C}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{I}+\mathbf{H}\mathbf{V}\mathbf{V}^\text{T}&amp;-\mathbf{H}\mathbf{v} \\ -\mathbf{H}\mathbf{V}^\text{T}&amp;\mathbf{H}\end{matrix}\right],
\end{equation}
where $\mathbf{H}=(\mathbf{C}-\mathbf{V}^\text{T}\mathbf{V})^{-1}$, and thus $\mathbf{H}$ is symmetric.</p>

<p>On the other hands, the gradient w.r.t each parameter of $\log\pi(\mathbf{z})$ are given as
\begin{equation}
\nabla_{\boldsymbol{\delta},\mathbf{M},\boldsymbol{\tau}}\log\pi(\mathbf{z}\vert\boldsymbol{\mu},\mathbf{A},\boldsymbol{\tau},\boldsymbol{\delta},\mathbf{M})\big\vert_{\,\boldsymbol{\delta}=\mathbf{0},\mathbf{M}=\mathbf{0}}=\mathbf{g}=\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M} \\ \mathbf{g}_\boldsymbol{\tau}\end{matrix}\right],
\end{equation}
where
\begin{align}
\mathbf{g}_\boldsymbol{\delta}&amp;=-2\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s},\label{eq:ec.2} \\ \mathbf{g}_\mathbf{M}&amp;=-\frac{1}{2}\mathbf{I}-\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}\mathbf{s}^\text{T},\label{eq:ec.3} \\ \mathbf{g}_\boldsymbol{\tau}&amp;=\frac{1}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\nabla_\boldsymbol{\tau}q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2),
\end{align}
where
\begin{equation}
q_\boldsymbol{\tau}’=\frac{\partial}{\partial(r^2)}q_\boldsymbol{\tau}
\end{equation}
denotes the derivative of $q_\boldsymbol{\tau}$ w.r.t $r^2$, while $\nabla_\boldsymbol{\tau}q_\boldsymbol{\tau}$ represents the gradient w.r.t $\boldsymbol{\tau}$.</p>

<p>The natural gradient for a sample $\mathbf{s}$ is then can be computed as
\begin{equation}
\tilde{\nabla}J=\mathbf{F}^{-1}\mathbf{g}=\mathbf{F}^{-1}\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M} \\ \mathbf{g}_\boldsymbol{\tau}\end{matrix}\right]=\left[\begin{matrix}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{H}\mathbf{V}\left(\mathbf{V}^\text{T}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{g}_\boldsymbol{\tau}\right) \\ \mathbf{H}\left(\mathbf{V}^\text{T}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{g}_\boldsymbol{\tau}\right)\end{matrix}\right],
\end{equation}
where
\begin{equation}
\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)=\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M}\end{matrix}\right]
\end{equation}</p>

<h3 id="samp-rot-sym-dist">Sampling from rotationally symmetric distributions</h3>
<p>To sample from this class of distributions, we first draw a sample $\mathbf{s}$ according to the standard density
\begin{equation}
\mathbf{s}\sim\pi(\mathbf{s}\vert\boldsymbol{\mu}=\mathbf{0},\mathbf{A}=\mathbf{I},\boldsymbol{\tau}),
\end{equation}
We continue to transform this sample into
\begin{equation}
\mathbf{z}=\boldsymbol{\mu}+\mathbf{A}^\text{T}\mathbf{s}\sim\pi(\mathbf{z}\vert\boldsymbol{\mu},\mathbf{A},\boldsymbol{\tau})
\end{equation}
In general, sampling $\mathbf{s}$ can be decomposed into sampling $r^2$ according to
\begin{equation}
r^2\sim\tilde{q}_\boldsymbol{\tau}(r^2)=\int_{\Vert\mathbf{z}\Vert^2=r^2}Q_\boldsymbol{\tau}\,d\mathbf{z}=\frac{2\pi^{n/2}}{\Gamma(n/2)}(r^2)^{(d-1)/2}q_\boldsymbol{\tau}(r^2)
\end{equation}
and a unit vector $\mathbf{u}\in\mathbb{R}^n$.</p>

<h3 id="xnes">Exponential Natural Evolution Strategies</h3>
<p>Recall that the Multivariate Gaussian can be expressed in form of a radial distribution \eqref{eq:ep.1}. In this case, we have that
\begin{equation}
q_\boldsymbol{\tau}(r^2)=\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right),\label{eq:xnes.1}
\end{equation}
which does not depend on $\boldsymbol{\tau}$. This lets the Fisher information matrix in \eqref{eq:ec.1} be simplified to the most trivial form, which is the identity matrix $\mathbf{I}$.</p>

<p>Differentiating \eqref{eq:xnes.1} w.r.t $r^2$ then gives us
\begin{equation}
q_\boldsymbol{\tau}’(r^2)=\frac{\partial}{\partial(r^2)}\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right)=-\frac{1}{2}\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right)=-\frac{1}{2}q_\boldsymbol{\tau}(r^2),
\end{equation}
which by \eqref{eq:ec.2} and \eqref{eq:ec.3} implies that
\begin{equation}
\mathbf{g}_\boldsymbol{\delta}=-2\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}=\mathbf{s}
\end{equation}
and
\begin{equation}
\mathbf{g}_\mathbf{M}=-\frac{1}{2}\mathbf{I}-\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}\mathbf{s}^\text{T}=\frac{1}{2}(\mathbf{s}\mathbf{s}^\text{T}-\mathbf{I})
\end{equation}
Hence, the natural gradient is then given as
\begin{align}
\nabla_\boldsymbol{\delta}J&amp;=\sum_{k=1}^{\lambda}f(\mathbf{z}_k)\mathbf{s}_k \\ \nabla_\mathbf{M}J&amp;=\sum_{k=1}^{\lambda}f(\mathbf{z}_k)(\mathbf{s}_k\mathbf{s}_k^\text{T}-\mathbf{I}),
\end{align}
which can be improved with fitness shaping using the update formula \eqref{eq:fs.1} as
\begin{align}
\nabla_\boldsymbol{\delta}J&amp;=\sum_{k=1}^{\lambda}u_k\mathbf{s}_{k:\lambda}, \\ \nabla_\mathbf{M}J&amp;=\sum_{k=1}^{\lambda}u_k(\mathbf{s}_{k:\lambda}\mathbf{s}_{k:\lambda}^\text{T}-\mathbf{I}),
\end{align}
where $\mathbf{s}_{k:\lambda}$ denotes the $k$-th best sample in local coordinates. The resulting algorithm is thus known as <strong>Exponential Natural Evolution Strategies</strong>, or <strong>xNES</strong>, with the corresponding pseudocode shown below.</p>
<figure>
	<img src="/assets/images/2022-10-07/xnes.png" alt="xNES" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h2 id="test-on-rast">Testing on Rastrigin function</h2>
<p>Analogy to <a href="/2022/09/14/cma-es.html#test-on-rast">CMA-ES</a>, let us test NES on the Rastrigin function, which is, recall that, given by the formula
\begin{equation}
f(\mathbf{x})=10 n+\sum_{i=1}^{n}x_i^2-10\cos\left(2\pi x_i\right)
\end{equation}
$f(\mathbf{x})$ reaches its global minimum $0$ at $\mathbf{x}=\mathbf{0}$. The experimental setup we are going to use are provided in <a href="#nes-paper">Wierstra et al. 2014</a>. Similar to our test with CMA-ES, each function evaluation is counted as success when it reaches $f_\text{stop}=10^{-10}$.</p>

<p>The result after running our experiment is illustrated in the figure below.</p>
<figure>
	<img src="/assets/images/2022-10-07/nes-rastrigin.png" alt="NES on rastrigin" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Success rate to reach $f_\text{stop}=10^{-10}$ versus population size for Rastrigin function.<br /> The code can be found <span><a href="https://github.com/trunghng/evolution-strategies/blob/main/testing_ground.py">here</a></span></figcaption>
</figure>

<h2 id="references">References</h2>
<p>[1] Daan Wierstra, Tom Schaul, Jan Peters, Jürgen Schmidhuber. <a href="https://people.idsia.ch/~juergen/nes2008.pdf">Natural Evolution Strategies</a>. IEEE World Congress on Computational Intelligence, 2008.</p>

<p>[2] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jürgen Schmidhuber. <a href="https://arxiv.org/abs/1106.4487">Natural Evolution Strategies</a>. arXiv:1106.4487, 2011.</p>

<p><span id="nes-paper">[3] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, Jürgen Schmidhuber. <a href="https://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf">Natural Evolution Strategies</a>. Journal of Machine Learning Research 15, 2014.</span></p>

<p>[4] Ha, David. <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/">A Visual Guide to Evolution Strategies</a>. blog.otoro.net, 2017.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="evolution-strategy" /><category term="neuroevolution" /><summary type="html"><![CDATA[Natural Evolution Strategy]]></summary></entry><entry><title type="html">CMA Evolution Strategy</title><link href="http://localhost:4000/2022/09/14/cma-es.html" rel="alternate" type="text/html" title="CMA Evolution Strategy" /><published>2022-09-14T13:00:00+07:00</published><updated>2022-09-14T13:00:00+07:00</updated><id>http://localhost:4000/2022/09/14/cma-es</id><content type="html" xml:base="http://localhost:4000/2022/09/14/cma-es.html"><![CDATA[<blockquote>
  <p>A note on CMA - Evolution Strategy
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#bsc-eqn">Basic equation</a></li>
  <li><a href="#upd-mean">Updating the mean</a></li>
  <li><a href="#adp-cov">Adapting the covariance matrix</a>
    <ul>
      <li><a href="#est-scratch">Estimating from scratch</a></li>
      <li><a href="#rank-lambda-mu-update">Rank-$\gamma$ update</a></li>
      <li><a href="#rank-one-update">Rank-one update</a></li>
      <li><a href="#final-update">Final update</a></li>
    </ul>
  </li>
  <li><a href="#ctrl-sigma">Controlling the step-size</a></li>
  <li><a href="#test-on-rast">Testing on Rastrigin function</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>The <strong>condition number</strong> of a matrix $\mathbf{A}$ is defined by
\begin{equation}
\kappa(\mathbf{A})\doteq\Vert\mathbf{A}\Vert\Vert\mathbf{A}^{-1}\Vert,
\end{equation}
where $\Vert\mathbf{A}\Vert=\sup_{\Vert\mathbf{x}\Vert=1}\Vert\mathbf{Ax}\Vert$.</p>

<p>For $\mathbf{A}$ that is non-singular, $\kappa(\mathbf{A})=\infty$.</p>

<p>For $\mathbf{A}$ which is positive definite, we thus have $\Vert\mathbf{A}\Vert=\lambda_\text{max}$ where $\lambda_\text{max}$ denotes the largest eigenvalue of $\mathbf{A}$, correspondingly $\lambda_\text{min}$ denotes the smallest eigenvalue of $\mathbf{A}$. The condition number of $\mathbf{A}$ therefore can be written as
\begin{equation}
\kappa(\mathbf{A})=\frac{\lambda_\text{max}}{\lambda_\text{min}}\geq 1,
\end{equation}
since corresponding to each eigenvalue $\lambda$ of $\mathbf{A}$, the inverse matrix $\mathbf{A}^{-1}$ takes $1/\lambda$ as its eigenvalue.</p>

<h2 id="bsc-eqn">Basic equation</h2>
<p>In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\ldots$
\begin{equation}
\mathbf{x}_k^{(t+1)}\sim\boldsymbol{\mu}^{(t)}+\sigma^{(t)}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})\sim\mathcal{N}\left(\boldsymbol{\mu}^{(t)},{\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}\right),\hspace{1cm}k=1,\ldots,\lambda\label{eq:be.1}
\end{equation}
where</p>
<ul>
  <li>$\mathbf{x}_k^{(t+1)}\in\mathbb{R}^n$: the $k$-th sample at generation $t+1$.</li>
  <li>$\boldsymbol{\mu}^{(t)}\in\mathbb{R}^n$: mean of the search distribution at generation $t$.</li>
  <li>$\sigma^{(t)}\in\mathbb{R}$: step-size at generation $t$.</li>
  <li>$\boldsymbol{\Sigma}^{(t)}$: covariance matrix at generation $t$.</li>
  <li>${\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$.</li>
  <li>$\lambda\geq 2$: sample size.</li>
</ul>

<h2 id="update-mean">Updating the mean</h2>
<p>The mean $\boldsymbol{\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\gamma$ selected points from the sample $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$:
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)},\label{eq:um.1}
\end{equation}
where</p>
<ul>
  <li>$\sum_{i=1}^{\gamma}w_i=1$ with $w_1\geq w_2\geq\ldots\geq w_{\gamma}&gt;0$.</li>
  <li>$\gamma\leq\lambda$: number of selected points.</li>
  <li>$\mathbf{x}_{i:\lambda}^{(t+1)}$: $i$-th best sample out of $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$ from \eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have
\begin{equation}
f(\mathbf{x}_{1:\lambda}^{(t+1)})\geq f(\mathbf{x}_{2:\lambda}^{(t+1)})\geq\ldots\geq f(\mathbf{x}_{\lambda:\lambda}^{(t+1)})
\end{equation}</li>
</ul>

<p>We can rewrite \eqref{eq:um.1} as an update rule for the mean $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\boldsymbol{\mu}^{(t)}+\alpha_\boldsymbol{\mu}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right),
\end{equation}
where $\alpha_\boldsymbol{\mu}\leq 1$ is the learning rate, which is usually set to $1$.</p>

<p>When choosing the weight values $w_i$ and population size $\gamma$ for recombination, we take into account the <strong>variance effective selection mass</strong>, denoted as $\gamma_\text{eff}$, given by
\begin{equation}
\gamma_\text{eff}\doteq\left(\frac{\Vert\mathbf{w}\Vert_1}{\Vert\mathbf{w}\Vert_2}\right)=\frac{\Vert\mathbf{w}\Vert_1^2}{\Vert\mathbf{w}\Vert_2^2}=\frac{1}{\sum_{i=1}^{\gamma}w_i^2}
\end{equation}
where $\mathbf{w}$ is defined as the weight vector
\begin{equation}
\mathbf{w}=(w_1,\ldots,w_\gamma)^\text{T}
\end{equation}</p>

<h2 id="adp-cov">Adapting the covariance matrix</h2>
<p>The covariance matrix can be estimated from scratch using the population of the current generation or can be estimated with covariance matrix from previous generations.</p>

<h3 id="est-scratch">Estimating from scratch</h3>
<p>Rather than using the empirical covariance matrix as an estimator for $\boldsymbol{\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation
\begin{equation}
\boldsymbol{\Sigma}_\lambda^{(t+1)}=\frac{1}{\lambda{\sigma^{(t)}}^2}\sum_{i=1}^{\lambda}\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T}\label{eq:es.1}
\end{equation}
Notice that in the above estimation \eqref{eq:es.1}, we have used all of the $\lambda$ samples. We thus can estimate a better covariance matrix by select some of the best individual out of $\lambda$ samples, which is analogous to how we update the mean $\boldsymbol{\mu}$.</p>

<p>In particular, we instead consider the estimation
\begin{equation}
\boldsymbol{\Sigma}_{\gamma}^{(t+1)}=\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T},\label{eq:es.2}
\end{equation}
where $\gamma\leq\lambda$ is the number of selected points; the weights $w_i$ and selected points $\mathbf{x}_{i:\lambda}^{(t+1)}$ are defined as given in the update for $\boldsymbol{\mu}$.</p>

<h3 id="rank-lambda-mu-update">Rank-$\gamma$ update</h3>
<p>In order to ensure that \eqref{eq:es.2} is a reliable estimator, the selected population must be large enough. However, to get a fast search, the population size $\lambda$ must be small, which lets the selected sample size consequently small also. Thus, we can not get a reliable estimator for a good covariance matrix from \eqref{eq:es.2}. However, we can use the history as a helping hand.</p>

<p>In particular, if we have experienced a sufficient number of generations, the mean of the $\boldsymbol{\Sigma}_\gamma$ from all previous generations
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\frac{1}{t+1}\sum_{i=0}^{t}\boldsymbol{\Sigma}_\gamma^{(i+1)}\label{eq:rlmu.1}
\end{equation}
would be a reliable estimator.</p>

<p>In addition, it is reasonable that the recent generations will have more affection to the current generation than the distant ones. Hence, rather than assigning estimated covariance matrices $\boldsymbol{\Sigma}_\gamma$ from preceding generations the same weight as in \eqref{eq:rlmu.1}, it would be a better choice to give the more recent generations the higher weight.</p>

<p>Specifically, starting with an initial $\boldsymbol{\Sigma}^{(0)}=\mathbf{I}$, we consider the update, called <strong>rank-$\gamma$ update</strong>, for the covariance matrix at generation $t+1$ using <strong>exponential smoothing</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\boldsymbol{\Sigma}_\gamma^{(t+1)} \\ &amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T} \\ &amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},\label{eq:rlmu.2}
\end{align}
where</p>
<ul>
  <li>$\alpha_\gamma\leq 1$: learning rate.</li>
  <li>$w_1,\ldots,w_\gamma$ and $\mathbf{x}_{1:\lambda}^{(t+1)},\ldots,\mathbf{x}_{\lambda:\lambda}^{(g+1)}$ are defined as usual.</li>
  <li>$\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$.</li>
</ul>

<p>The update \eqref{eq:rlmu.2} can be generalized to $\lambda$ weights values which neither necessarily sum to $1$, nor be non-negative anymore, as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;=\left(1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T}\label{eq:rlmu.3} \\ &amp;={\boldsymbol{\Sigma}^{(t)}}^{1/2}\left[\mathbf{I}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\left(\mathbf{z}_{i:\lambda}^{(t+1)}{\mathbf{z}_{i:\lambda}^{(t+1)}}^\text{T}-\mathbf{I}\right)\right]{\boldsymbol{\Sigma}^{(t)}}^{1/2},
\end{align}
where</p>
<ul>
  <li>$w_1\geq\ldots\geq w_\gamma&gt;0\geq w_{\gamma+1}\geq\ldots\geq w_\lambda\in\mathbb{R}$, and usually $\sum_{i=1}^{\gamma}w_i=1$ and $\sum_{i=1}^{\lambda}w_i\approx 0$.</li>
  <li>$\mathbf{z}_{i:\lambda}^{(t+1)}={\boldsymbol{\Sigma}^{(t)}}^{1/2}\mathbf{y}_{i:\lambda}^{(t+1)}$ is the mutation vector.</li>
</ul>

<h3 id="rank-one-update">Rank-one-update</h3>
<p>We first consider a method that produces an $n$-dimensional normal distribution with zero mean. Specifically, let $\mathbf{y}_1,\ldots,\mathbf{y}_{t_0}\in\mathbb{R}^n$, for $t_0\geq n$ be vectors span $\mathbb{R}^n$. We thus have that
\begin{align}
\mathcal{N}(0,1)\mathbf{y}_1+\ldots+\mathcal{N}(0,1)\mathbf{y}_{t_0}&amp;\sim\mathcal{N}(\mathbf{0},\mathbf{y}_1\mathbf{y}_1^\text{T})+\ldots+\mathcal{N}(\mathbf{0},\mathbf{y}_{t_0}\mathbf{y}_{t_0}^\text{T}) \\ &amp;\sim\mathcal{N}\left(\mathbf{0},\sum_{i=1}^{t_0}\mathbf{y}_i\mathbf{y}_i^\text{T}\right)
\end{align}
The covariance matrix $\mathbf{y}_i\mathbf{y}_i^\text{T}$ has rank one, with only one eigenvalue $\Vert\mathbf{y}_i\Vert^2$ and a corresponding eigenvector within the form $\alpha\mathbf{y}_i$ for $\alpha\in\mathbb{R}$. Using the above equation, we can generate any MVN distribution.</p>

<p>Consider the update \eqref{eq:rlmu.3} with $\gamma=1$ and let $\mathbf{y}_{t+1}=\left(\mathbf{x}_{1:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$, the <strong>rank-one update</strong> for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ is given by
\begin{equation}
\boldsymbol{\Sigma}^{t+1}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{y}_{t+1}\mathbf{y}_{t+1}^\text{T}
\end{equation}
The latter summand in the RHS has rank one and adds the maximum likelihood term for $\mathbf{y}_{t+1}$ into the covariance matrix $\boldsymbol{\Sigma}^{(t)}$, which makes the probability of generating $\mathbf{y}_{t+1}$ in the generation $t+1$ increase.</p>

<p>We continue by noticing that to update the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$, in \eqref{eq:rlmu.3}, we have used the selected steps
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}=\frac{\mathbf{x}_{i:\lambda}^{(g+1)}-\boldsymbol{\mu}^{(g)}}{\sigma^{(g)}}
\end{equation}
However, since
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}{\mathbf{y}_{i:\lambda}^{(g+1)}}^\text{T}=-\mathbf{y}_{i:\lambda}^{(g+1)}\left(-\mathbf{y}_{i:\lambda}^{(g+1)}\right)^\text{T},
\end{equation}
which means the sign information is lost when computing the covariance matrix. To track the sign information to the update rule of $\boldsymbol{\Sigma}^{(t+1)}$, we use <strong>evolution path</strong>, which defined as a sequence of successive steps over number of generations.</p>

<p>In particular, analogy to \eqref{eq:rlmu.3}, we use exponential smoothing to establish the evolution path, $\mathbf{p}_c\in\mathbb{R}^n$, which starting with an initial value $\mathbf{p}_c^{(0)}=\mathbf{0}$ and being updated with
\begin{align}
\mathbf{p}_c^{(t+1)}&amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{(1-(1-\alpha_c)^2)\mu_\text{eff}}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\sum_{i=1}^{\gamma}\frac{w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)}{\sigma^{(t)}} \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where</p>
<ul>
  <li>$\mathbf{p}_c^{(t)}\in\mathbb{R}^n$ is the evolution path at generation $t$.</li>
  <li>$\alpha_c\leq 1$ is the learning rate.</li>
  <li>$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_c^{(t+1)}$ such that
\begin{equation}
\mathbf{p}_c^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),
\end{equation}
since by $\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$ we have that
\begin{equation}
\mathbf{p}_c^{(t)}\sim\mathbf{y}_{i:\lambda}^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),\hspace{1cm}\forall i=1,\ldots,\gamma
\end{equation}
which by $\gamma_\text{eff}=\left(\sum_{i=1}^{\gamma}w_i^2\right)^{-1}$ implies that
\begin{equation}
\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}\sim\frac{1}{\sqrt{\gamma_\text{eff}}}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})
\end{equation}</li>
</ul>

<p>The <strong>rank-one update</strong> for the covariance matrix $\boldsymbol{\Sigma}^{(t)}$ via the evolution path $\mathbf{p}_c^{(t+1)}$ then given as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T},\label{eq:rou.1}
\end{equation}
An empirical validated choice for the learning rate $\alpha_1$ is $\alpha_1\approx 2/n^2$.</p>

<h3 id="final-update">Final update</h3>
<p>Combining rank-$\gamma$ update \eqref{eq:rlmu.3} and rank-one update \eqref{eq:rou.1} together, we obtain the final update for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\left(1-\alpha_1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},
\end{equation}
where</p>
<ul>
  <li>$\alpha_1\approx 2/n^2$.</li>
  <li>$\alpha_\gamma\approx\min(\gamma_\text{eff}/n^2,1-\alpha_1)$.</li>
  <li>$\mathbf{y}_{i:\lambda}^{(t+1)}=\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$.</li>
  <li>$\sum_{i=1}^{\lambda}w_i\approx-\alpha_1/\alpha_\gamma$.</li>
</ul>

<h2 id="ctrl-sigma">Controlling the step-size</h2>
<p>To control the step-size $\sigma^{(t)}$, similar to how we cumulatively update the covariance matrix by rank-one covariance matrices, we also use an evolution path, which is defined as sum of successive steps $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$.</p>

<p>However, in this step-size adaption, we utilize a conjugate evolution path $\mathbf{p}_\sigma$, which begins with an initial value $\mathbf{p}_\sigma^{(0)}=\mathbf{0}$ and is repeatedly updated by
\begin{align}
\mathbf{p}_\sigma^{(t+1)}&amp;=(1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{(1-(1-\alpha_\sigma)^2)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\frac{\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}} \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where</p>
<ul>
  <li>$\mathbf{p}_\sigma^{(t)}\in\mathbb{R}^n$ is the conjugate evolution path at generation $t$.</li>
  <li>$\alpha_\sigma&lt;1$ is the learning rate.</li>
  <li>$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_\sigma^{(t+1)}$, which analogously to the covariance matrix adaption, lets
\begin{equation}
\mathbf{p}_\sigma^{(t+1)}\sim\mathcal{N}(\mathbf{0},\mathbf{I})
\end{equation}</li>
  <li>The covariance matrix ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$ is defined as
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\doteq\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T},\label{eq:cs.1}
\end{equation}
where
\begin{equation}
\hspace{-0.8cm}\boldsymbol{\Sigma}^{(t)}=\mathbf{Q}^{(t)}\boldsymbol{\Lambda}^{(t)}{\mathbf{Q}^{(t)}}^\text{T}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{q}_1^{(t)}&amp;\ldots&amp;\mathbf{q}_n^{(t)} \\ \vert&amp;&amp;\vert\end{matrix}\right]\left[\begin{matrix}\lambda_1^{(t)}&amp;&amp; \\ &amp;\ddots&amp; \\ &amp;&amp; \lambda_n^{(t)}\end{matrix}\right]\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{q}_1^{(t)}&amp;\ldots&amp;\mathbf{q}_n^{(t)} \\ \vert&amp;&amp;\vert\end{matrix}\right]^\text{T}
\end{equation}
is an eigendecomposition of the positive definite covariance matrix $\boldsymbol{\Sigma}^{(t)}$, where $\mathbf{Q}^{(t)}\in\mathbb{R}^{n\times n}$ is an orthonormal matrix whose columns are unit eigenvectors $\mathbf{q}_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$ and $\boldsymbol{\Lambda}^{(t)}\in\mathbb{R}^{n\times n}$ is a diagonal matrix whose diagonal entries are eigenvalues $\lambda_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$.<br />
Moreover, for each eigenvalue, eigenvector pair $(\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ of $\boldsymbol{\Sigma}^{(t)}$ we have
\begin{equation}
\lambda_i^{(t)}{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}={\boldsymbol{\Sigma}^{(t)}}^{-1}\boldsymbol{\Sigma}^{(t)}\mathbf{q}_i^{(t)}=\mathbf{q}_i^{(t)},
\end{equation}
or
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}=\frac{1}{\lambda_i^{(t)}}\mathbf{q}_i^{(t)},
\end{equation}
or in other words, $(1/\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ is an eigenvalue, eigenvector pair of ${\boldsymbol{\Sigma}^{(t)}}^{-1}$. Therefore, the inverse of $\boldsymbol{\Sigma}^{(t)}$, which is also positive definite can be written by
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}=\mathbf{Q}^{(t)}\left[\begin{matrix}1/\lambda_1^{(t)}&amp;&amp; \\ &amp;\ddots&amp; \\ &amp;&amp; 1/\lambda_n^{(t)}\end{matrix}\right]{\mathbf{Q}^{(t)}}^\text{T}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1}{\mathbf{Q}^{(t)}}^\text{T},
\end{equation}
which allows us to obtain the representation \eqref{eq:cs.1} of ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$.</li>
</ul>

<p>The transformation ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T}$ re-scales length of the step $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$ without changing its direction. In more specific:</p>
<ul id="number-list">
	<li>
		${\mathbf{Q}^{(t)}}^\text{T}$ transform the original space into the coordinate space with columns of $\mathbf{Q}^{(t)}$, which is also the eigenvectors of $\boldsymbol{\Sigma}^{(t)}$ or the principle axes of $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})$, as its principle axes.
	</li>
	<li>
		${\boldsymbol{\Lambda}^{(t)}}^{-1/2}$ re-scales the principle axes to have the same length.
	</li>
	<li>
		$\mathbf{Q}^{(t)}$ transforms the coordinate system back to the original space.
	</li>
</ul>

<p>It means that this transformation makes the expected length of $\mathbf{p}_\sigma^{(t+1)}$ independent of its direction.</p>

<p>We then update $\sigma^{(t)}$ by according to the ratio of its length with its expected length $\Vert\mathbf{p}_\sigma^{(t+1)}\Vert/\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert$, given by
\begin{equation}
\log\sigma^{(t+1)}=\log\sigma^{(t)}+\frac{\alpha_\sigma}{d_\sigma}\left(\frac{\Vert\mathbf{p}_\sigma^{(t+1)}\Vert}{\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert}-1\right),
\end{equation}
where $d_\sigma\approx 1$ is the <strong>damping parameter</strong>, which controls the update size. Therefore, since $\sigma^{(t)}&gt;0$, we have the update rule for $\sigma^{(t)}$ is given by
\begin{equation}
\sigma^{(t+1)}=\sigma^{(t)}\exp\left(\frac{\alpha_\sigma}{d_\sigma}\left(\frac{\Vert\mathbf{p}_\sigma^{(t+1)}\Vert}{\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert}-1\right)\right)
\end{equation}</p>

<h2 id="test-on-rast">Testing on Rastrigin function</h2>
<p>Let us give the CMA-ES algorithm a try on the <a href="https://en.wikipedia.org/wiki/Rastrigin_function"><strong>Rastrigin function</strong></a>, $f:\mathbb{R}^n\to\mathbb{R}$, which is given by
\begin{equation}
f(\mathbf{x})=10 n+\sum_{i=1}^{n}x_i^2-10\cos\left(2\pi x_i\right)
\end{equation}
The global minimum of $f(\mathbf{x})$ is $0$ at $\mathbf{x}=\mathbf{0}$. We will be using the experimental settings given in this <a href="#cmaes-exp">paper</a> proposed by CMA-ES’s original author. Each time we end up with a result less than $f_\text{stop}=10^{-10}$, we count it a success run.</p>

<p>The result obtained is illustrated in the following figure.</p>
<figure>
	<img src="/assets/images/2022-09-14/cmaes-rastrigin.png" alt="CMA-ES on rastrigin" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Success rate to reach $f_\text{stop}=10^{-10}$ versus population size for Rastrigin function.<br /> The code can be found <span><a href="https://github.com/trunghng/evolution-strategies/blob/main/testing_ground.py">here</a></span></figcaption>
</figure>

<h2 id="references">References</h2>
<p>[1] Nikolaus Hansen. <a href="https://arxiv.org/abs/1604.00772">The CMA Evolution Strategy: A Tutorial</a>. 	arXiv:1604.00772, 2016.</p>

<p>[2] Nikolaus Hansen, Youhei Akimoto &amp; Petr Baudis. <a href="https://github.com/CMA-ES/pycma">CMA-ES/pycma on Github</a>. Zenodo, <a href="https://doi.org/10.5281/zenodo.2559634">DOI:10.5281/zenodo.2559634</a>, February 2019.</p>

<p><span id="cmaes-exp">[3] Nikolaus Hansen, Stefan Kern. <a href="https://doi.org/10.1007/978-3-540-30217-9_29">Evaluating the CMA Evolution Strategy on Multimodal Test Functions</a>. Parallel Problem Solving from Nature - PPSN VIII. PPSN 2004.</span></p>

<p>[4] Ha, David. <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/">A Visual Guide to Evolution Strategies</a>. blog.otoro.net, 2017.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The simplest form of <strong>exponential smoothing</strong> is given by the formula
\begin{align*}
s_0&amp;=x_0 \\ s_t&amp;=\alpha x_t+(1-\alpha)s_{t-1},\hspace{1cm}t&gt;0
\end{align*}
where $0&lt;\alpha&lt;1$ is referred as the <strong>smoothing factor</strong>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="evolution-strategy" /><category term="neuroevolution" /><summary type="html"><![CDATA[A note CMA-ES]]></summary></entry><entry><title type="html">Neural networks</title><link href="http://localhost:4000/2022/09/02/neural-nets.html" rel="alternate" type="text/html" title="Neural networks" /><published>2022-09-02T13:00:00+07:00</published><updated>2022-09-02T13:00:00+07:00</updated><id>http://localhost:4000/2022/09/02/neural-nets</id><content type="html" xml:base="http://localhost:4000/2022/09/02/neural-nets.html"><![CDATA[<blockquote>
  <p>A note on Neural networks.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#ff-func">Feed-forward network functions</a>
    <ul>
      <li><a href="#unv-approx">Universal approximation property</a></li>
      <li><a href="#w-s-sym">Weight-space symmetries</a></li>
    </ul>
  </li>
  <li><a href="#net-training">Network training</a>
    <ul>
      <li><a href="#output-prob-itp">Network outputs probabilistic interpretation</a>
        <ul>
          <li><a href="#univ-output">Univariate regression</a></li>
          <li><a href="#mult-output">Multivariate regression</a></li>
          <li><a href="#bi-clf">Binary classification</a></li>
          <li><a href="#mult-clf">Multi-class classification</a></li>
        </ul>
      </li>
      <li><a href="#param-opt">Parameter optimization</a></li>
      <li><a href="#backprop">Backpropagation</a>
        <ul>
          <li><a href="#erf-drv">Error-function derivatives</a></li>
          <li><a href="#jacobian-mtx">Jacobian matrix</a></li>
          <li><a href="#hessian-mtx">Hessian matrix</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#bayes-nn">Bayesian neural networks</a>
    <ul>
      <li><a href="#posterior-param-dist">Posterior parameter distribution</a></li>
    </ul>
  </li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="ff-func">Feed-forward network functions</h2>
<p>Recall that the <a href="/2022/08/13/glm.html">linear models</a> used in regression and classification are based on linear combination of fixed nonlinear basis function $\phi_j(\mathbf{x})$ and take the form
\begin{equation}
y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\mathbf{x})\right),\label{1}
\end{equation}
where in the case of regression, $f$ is the function $f(x)=x$, while in the classification case, $f$ takes the form of a nonlinear activation function (e.g., the <a href="/2022/08/13/glm.html#logistic-sigmoid-func">sigmoid function</a>).</p>

<p><strong>Neural networks</strong> extend this model \eqref{1} by letting each basis functions $\phi_j(\mathbf{x})$ be a nonlinear function of a linear combination of the inputs, where the coefficients in the combination are the adaptive parameters.</p>

<p>More formally, neural networks is a series of layers, in which each layer represents a functional transformation. Let us consider the first layer by constructing $M$ linear combinations of the input variable $x_1,\ldots,x_D$ in the form
\begin{equation}
a_j=\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)},\label{2}
\end{equation}
where</p>
<ul>
  <li>$j=1,\ldots,M$;</li>
  <li>the superscript $^{(1)}$ indicates that we are working with parameters of the first layer;</li>
  <li>$w_{ji}^{(1)}$’s are called the <strong>weights</strong>;</li>
  <li>$w_{j0}^{(1)}$’s are known as the <strong>biases</strong>;</li>
  <li>$a_j$’s are referred as <strong>activations</strong>.</li>
</ul>

<p>The activations $a_j$’s are then transformed using a differentiable, nonlinear <strong>activation function</strong> $h(\cdot)$, which correspond to $f(\cdot)$ in \eqref{1} to give
\begin{equation}
z_j=h(a_j),\label{3}
\end{equation}
where $z_j$ are called the <strong>hidden units</strong>. Repeating the same procedure as \eqref{2}, which was following \eqref{1}, $z_j$’s are taken as the inputs of the second layer to give $K$ outputs
\begin{equation}
a_k=\sum_{j=1}^{M}w_{kj}^{(2)}z_j+w_{k0}^{(2)},\label{4}
\end{equation}
where $k=1,\ldots,K$.</p>

<p>This process will be repeated in $L$ times with $L$ is the number of layers. At the last layer, for instance, the second layer of our example network, the outputs, also called <strong>output unit activations</strong>, $a_k$’s are transformed using an appropriate activation function to give a set of network output $y_k$. For example, in multiple binary classification problems, we can choose the logistic sigmoid as our activation function that
\begin{equation}
y_k=\sigma(a_k)\label{5}
\end{equation}
Combining all these steps \eqref{2}, \eqref{3}, \eqref{4} and \eqref{5} together, our neural network with sigmoidal output unit activation functions can be defined as
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right),\label{6}
\end{equation}
where all of the weights and biases are comprises together into a parameter vector $\mathbf{w}$. As suggested in <a href="/2022/08/13/glm.html#dummy-coeff">linear regression</a>, we can also let the bias $w_{j0}^{(1)}$ be coefficient of a dummy input variable $x_0=1$ that makes \eqref{2} can be written as
\begin{equation}
a_j=\sum_{i=0}^{D}w_{ji}^{(1)}x_i
\end{equation}
This results that our subsequent layers are also able to be written in a more convenient form, which lets the entire network \eqref{6} take the form
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=0}^{M}w_{kj}^{(2)}h\left(\sum_{i=0}^{D}w_{ji}^{(1)}x_i\right)\right)
\end{equation}
Our network is also an example of a <strong>multilayer perception</strong>, or <strong>MLP</strong>, which is a combination of <a href="/2022/08/13/glm.html#perceptron">perceptron models</a>. The key difference is that while the neural network uses continuous sigmoidal nonlinearities in the hidden units, which is differentiable w.r.t the parameters, the perceptron algorithm uses step-function nonlinearities, which is in contrast non-differentiable.</p>

<p>The network network we have been considering so far is <strong>feed-forward neural network</strong>, whose outputs are deterministic functions of the inputs. Each (hidden or output) unit in such a network computes a function given by
\begin{equation}
z_k=h\left(\sum_{j}w_{kj}z_j\right),
\end{equation}
where the sum runs all over units sending connections to unit $k$ (bias included).</p>

<h3 id="unv-approx">Universal approximation property</h3>
<p>Feed-forward networks with <strong>hidden layers</strong> (i.e., the layers in which the training data does not show the desired output, e.g., the first layer of our network, the second layer on the other hands is called the <strong>output layer</strong>) provide <strong>universal approximation</strong> property.</p>

<p>In concrete, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any <strong>squashing</strong> activation function (e.g., the logistic sigmoid function) an approximate any continuous function on a compact subsets of $\mathbb{R}^n$.</p>

<h3 id="w-s-sym">Weight-space symmetries</h3>

<h2 id="net-training">Network training</h2>

<h3 id="output-prob-itp">Network outputs probabilistic interpretation</h3>

<h4 id="univ-output">Univariate regression</h4>
<p>Consider the <a href="/2022/08/13/glm.html#least-squares-reg">regression problem</a> in which the target variable $t$ has Gaussian distribution with an $\mathbf{x}$ dependent mean
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=\mathcal{N}(t\vert y(\mathbf{x},\mathbf{w}),\beta^{-1}),
\end{equation}
For the conditional distribution above, it is sufficient to take the output unit activation function to be the function $h(x)=x$, because such a network can approximate any continuous function from $\mathbf{x}$ to $y$.</p>

<p>Given the data set $(\mathbf{X},\mathbf{t})=\{\mathbf{x}_n,t_n\}$, where $\mathbf{x}_n$’s are i.i.d for $n=1,\ldots,N$, and where
\begin{align}
\mathbf{X}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{x}_1&amp;\ldots&amp;\mathbf{x}_N \\ \vert&amp;&amp;\vert\end{matrix}\right],\hspace{1cm}\mathbf{t}=\left[\begin{matrix}t_1 \\ \vdots \\ t_N\end{matrix}\right]
\end{align}
The likelihood function therefore can be given by
\begin{align}
p(t\vert\mathbf{X},\mathbf{w},\beta)&amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w},\beta) \\ &amp;=\prod_{n=1}^{N}\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1})
\end{align}
With a minor change as usual that taking negative natural logarithm of both sides gives us
\begin{align}
-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w},\beta)&amp;=-\sum_{n=1}^{N}\log\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1}) \\ &amp;=\frac{\beta}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2-\frac{N}{2}\log\beta+\frac{N}{2}\log 2\pi
\end{align}
Therefore, maximizing the likelihood function $p(\mathbf{t}\vert\mathbf{X},\mathbf{x},\beta)$ is equivalent to minimizing the sum-of-squares error function given as
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2,
\end{equation}
This also means the value of $\mathbf{w}$ that minimizes $E(\mathbf{w})$ will be $\mathbf{w}_\text{ML}$, which implies that the corresponding solution for $\beta$ will be given by
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{N}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w}_\text{ML})-t_n\big)^2
\end{equation}</p>

<h4 id="mult-output">Multivariate regression</h4>
<p>Similarly, we consider the multiple target variables case, in which the conditional distribution of the target therefore takes the form
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}\vert\mathbf{y}(\mathbf{x},\mathbf{w}),\beta^{-1}\mathbf{I})
\end{equation}
Repeating the same procedure as the univariate case, maximizing likelihood function is also equivalent to minimizing the sum-of-squares error function given by
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w})-\mathbf{t}_n\big\Vert^2,
\end{equation}
which gives us the solution for the noise precision $\beta$ in the multivariate case as
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{NK}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w}_\text{ML})-\mathbf{t}_n\big\Vert^2,
\end{equation}
where $K$ is the number of target variables.</p>

<h4 id="bi-clf">Binary classification</h4>
<p>Consider the problem of binary classification which outputs $t=1$ to denote class $\mathcal{C}_1$ and otherwise to denote class $\mathcal{C}_2$.</p>

<p>In particular, we consider a network having a single output whose activation function is a logistic sigmoid
\begin{equation}
y=\sigma(a)\doteq\frac{1}{1+\exp(-a)},
\end{equation}
which follows immediately that $0\leq y(\mathbf{x},\mathbf{w})\leq 1$.</p>

<p>This suggests us interpreting $y(\mathbf{x},\mathbf{w})$ as the conditional probability for class $\mathcal{C}_1$, $p(\mathcal{C}_1\vert\mathbf{x})$, and hence the corresponding conditional probability for class $\mathcal{C}_2$ will be $p(\mathcal{C}_2\vert\mathbf{x})=1-y(\mathbf{x},\mathbf{w})$. Or in other words, the conditional distribution $p(t\vert\mathbf{x},\mathbf{w})$ of targets $t$ given inputs $\mathbf{x}$ is then a Bernoulli distribution of the form
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=y(\mathbf{x},\mathbf{w})^t\big(1-y(\mathbf{x},\mathbf{w})\big)^{1-t}
\end{equation}
If we consider a training set of $N$ independent observations as in the two regression tasks above, the likelihood function of our classification task will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n}
\end{align}
Taking the negative natural logarithm of the likelihood as above gives us the cross-entropy error function
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n} \\ &amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n),
\end{align}
where $y_n=y(\mathbf{x}_n,\mathbf{w})$.</p>

<p>Moreover, consider the partial derivative of this error function w.r.t the activation $a_i$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_i}&amp;=\frac{\partial}{\partial a_i}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;=-\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_i}-\frac{1-t_i}{1-y_i}\frac{\partial(1-y_i)}{\partial a_i} \\ &amp;=\frac{\partial y_i}{\partial a_i}\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;=y_i(1-y_i)\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;=y_i-t_i,\label{eq:bin-clf-drv-error-a}
\end{align}
where in the forth step, we have use the identity of the <a href="/2022/08/13/glm.html#sigmoid-derivative">derivative of sigmoid function</a> that
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}</p>

<h4 id="mult-clf">Multi-class classification</h4>
<p>For the multi-class classification that assigns input variables to $K$ separated classes, we can use the network with $K$ outputs each of which has a logistic sigmoid activation function. Each output $t_k\in\{0,1\}$ for $k=1,\ldots,K$ indicates whether the input will be assigned to class $\mathcal{C}_k$</p>

<p>We first consider the case that the class labels are independent given the input vector, which means the conditional distributions for class $C_k$’s will be $K$ i.i.d Bernoulli distributions, in which the conditional probability for class $\mathcal{C}_k$ will take the form
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w})=y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{equation}
Therefore, the joint distribution of them, the conditional distribution of the target variables will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})&amp;=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w}) \\ &amp;=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{align}
Let $\mathbf{T}$ denote the combination of all the targets $\mathbf{t}_n$, i.e.,
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right],
\end{equation}
the likelihood function therefore takes the form of
\begin{align}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_k}\label{eq:mult-clf-llh}
\end{align}
Analogy to the binary case, taking the negative natural logarithm of the likelihood \eqref{eq:mult-clf-llh} gives us the corresponding cross-entropy error function for the multi-class case, given as
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_{nk}} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}),\label{eq:mult-clf-error}
\end{align}
where $y_{nk}$ is short for $y_k(\mathbf{x}_n,\mathbf{w})$.</p>

<p>Similar to the binary case, consider the partial derivative of the error function \eqref{eq:mult-clf-error} w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}) \\ &amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)\frac{\partial y_{ij}}{\partial a_{ij}} \\ &amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)y_{ij}(1-y_{ij}) \\ &amp;=y_{ij}-t_{ij}\label{eq:mult-drv-error-a}
\end{align}
which takes the same form as \eqref{eq:bin-clf-drv-error-a}</p>

<p>On the other hands, if each input is assigned only to one of $K$ classes (mutually exclusive), the conditional distributions for class $C_k$ will be instead given as
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x})=p(t_k=1\vert\mathbf{x})=y_k(\mathbf{x},\mathbf{w}),
\end{equation}
and thus the conditional distribution of the targets is
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})=\prod_{k=1}^{K}p(t_k=1\vert\mathbf{x})^{t_k}=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}
\end{equation}
The likelihood is therefore given as
\begin{equation}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w})=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}},
\end{equation}
which gives us the following cross-entropy error function by taking the negative natural logarithm
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_{nk}} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_k(\mathbf{x}_n,\mathbf{w})\label{eq:mult-me-clf-error}
\end{align}
As discussed in <a href="/2022/08/13/glm.html#softmax-reg">Softmax regression</a>, we see that the output unit activation function is given by the softmax function
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\frac{\exp\big[a_k(\mathbf{x},\mathbf{w})\big]}{\sum_{j=1}^{K}\exp\big[a_j(\mathbf{x},\mathbf{w})\big]}
\end{equation}
Taking the derivative of the error function \eqref{eq:mult-me-clf-error}  w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk} \\ &amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}\frac{\partial y_{ik}}{\partial a_{ij}} \\ &amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}y_{ik}(1\{j=k\}-y_{ij})\label{53} \\ &amp;=y_{ij}\sum_{k=1}^{K}t_{ik}-\sum_{k=1}^{K}t_{ik}1\{j=k\} \\ &amp;=y_{ij}-t_{ij}
\end{align}
where we have used the identity of the <a href="/2022/08/13/glm.html#softmax-derivative">derivative of the softmax function</a> in the forth step to obtain \eqref{53}.</p>

<h3 id="param-opt">Parameter optimization</h3>
<p>In training neural network to find a value of $\mathbf{w}$ to minimize the error function $E(\mathbf{w})$, we usually start with some initial value $\mathbf{w}_0$ and iteratively update the weight vector $\mathbf{w}$, in which the weight at time step $\tau+1$ is given as
\begin{equation}
\mathbf{w}^{(t+1)}=\mathbf{w}^{(\tau)}+\Delta\mathbf{w}^{(\tau)},
\end{equation}
where $\Delta\mathbf{w}^{(\tau)}$ is some update rule.</p>

<p>At each time step $\tau$, there are two distinct stages:</p>
<ul>
  <li>Stage 1 refers to evaluating the derivatives of the error function w.r.t the weights, which can be accomplished efficiently using <strong>backpropagation</strong> that will be discussed in the next section.</li>
  <li>Stage 2 relates to using those computed derivatives to calculate the adjustments to be made to the weights $\mathbf{w}$. <strong>Gradient descent</strong>, for instance, is the simplest approach in which each time step the weights take a small step in the direction of the negative gradient, as
\begin{equation}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta\nabla_\mathbf{w}E(\mathbf{w}^{(\tau)}),
\end{equation}
where $\eta&gt;0$ is called the <strong>learning rate</strong> of the update.</li>
</ul>

<h3 id="backprop">Backpropagation</h3>
<p>In this section, we will consider the use of <strong>backpropagation</strong> technique to evaluate the first and second derivatives of error-functions w.r.t the weights and also the derivatives of the network outputs w.r.t the inputs.</p>

<h4 id="erf-drv">Error-function derivatives</h4>
<p>We first consider the case of evaluating the first order derivative of the error function w.r.t to the weight parameter $\mathbf{w}$.</p>

<p>Consider a simple linear model where the outputs $y_k$’s are linear combinations of the input variable $x_i$’s
\begin{equation}
y_k=\sum_{i}w_{ki}x_i,
\end{equation}
together with the error function, in which the error function for the $n$ data point is defined as
\begin{equation}
E_n(\mathbf{w})=\frac{1}{2}\sum_{k}(y_{nk}-t_{nk})^2,
\end{equation}
where $y_{nk}=y_k(\mathbf{x}_n,\mathbf{w})$.</p>

<p>The gradient of this error function w.r.t to a weight $w_{ji}$ then can be computed by
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}}=(y_{nj}-t_{nj})x_{ni}
\end{equation}
In a general feed-forward network, each unit is a weighted sum of its inputs
\begin{equation}
a_j=\sum_{i}w_{ji}z_i
\end{equation}</p>

<h4 id="jacobian-mtx">Jacobian matrix</h4>

<h4 id="hessian-mtx">Hessian matrix</h4>

<h2 id="bayes-nn">Bayesian neural networks</h2>

<h3 id="posterior-param-dist">Posterior parameter distribution</h3>

<h2 id="preferences">Preferences</h2>
<p>[1] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p>

<p>[2] Ian Goodfellow &amp; Yoshua Bengio &amp; Aaron Courville. <a href="https://www.deeplearningbook.org">Deep Learning</a>. MIT Press, 2016.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="neural-network" /><summary type="html"><![CDATA[A note on neural networks]]></summary></entry><entry><title type="html">Measure theory - III: the Lebesgue integral</title><link href="http://localhost:4000/2022/08/21/measure-theory-p3.html" rel="alternate" type="text/html" title="Measure theory - III: the Lebesgue integral" /><published>2022-08-21T13:00:00+07:00</published><updated>2022-08-21T13:00:00+07:00</updated><id>http://localhost:4000/2022/08/21/measure-theory-p3</id><content type="html" xml:base="http://localhost:4000/2022/08/21/measure-theory-p3.html"><![CDATA[<blockquote>
  <p>Part III of the measure theory series. Materials are mostly taken from <a href="/2022/08/21/measure-theory-p3.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/08/21/measure-theory-p3.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#int-simp-funcs">Integration of simple functions</a>
    <ul>
      <li><a href="#simp-func">Simple function</a></li>
      <li><a href="#int-unsgn-simp-func">Integral of unsigned simple functions</a></li>
      <li><a href="#well-dfn-simp-int">Well-definedness of simple integral</a></li>
      <li><a href="#alm-evwhr-spt">Almost everywhere and support</a></li>
      <li><a href="#bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</a></li>
      <li><a href="#abs-cvg-simp-int">Absolutely convergence simple integral</a></li>
      <li><a href="#bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</a></li>
    </ul>
  </li>
  <li><a href="#msr-funcs">Measurable functions</a>
    <ul>
      <li><a href="#unsgn-msr-funcs">Unsigned measurable functions</a></li>
      <li><a href="#equiv-ntn-msrb">Equivalent notions of measurability</a></li>
      <li><a href="#cmplx-msrb">Complex measurability</a></li>
      <li><a href="#equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</a></li>
      <li><a href="#eg-msr-func">Examples of measurable function</a></li>
    </ul>
  </li>
  <li><a href="#unsgn-lebesgue-int">Unsigned Lebesgue integrals</a>
    <ul>
      <li><a href="#lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</a></li>
    </ul>
  </li>
  <li><a href="#abs-intb">Absolute integrability</a></li>
  <li><a href="#littlewoods-prncpl">Littlewood’s three principles</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="int-simp-funcs">Integration of simple functions</h2>
<p>Analogy to how the <a href="/2022/06/16/measure-theory-p1.html#riemann-integrability"><strong>Riemann integral</strong></a> was established by using the integral for <a href="/2022/06/16/measure-theory-p1.html#pc-func"><strong>piecewise constant functions</strong></a>, the <strong>Lebesgue integral</strong> is set up using the integral for <strong>simple functions</strong>.</p>

<h3 id="simp-func">Simple function</h3>
<p>A (complex-valued) <strong>simple function</strong> $f:\mathbb{R}^d\to\mathbb{C}$ is a finite linear combination
\begin{equation}
f=c_1 1_{E_1}+\ldots+c_k 1_{E_k},\label{eq:sf.1}
\end{equation}
of indicator functions $1_{E_i}$ of Lebesgue measurable sets $E_i\subset\mathbb{R}^d$ for $i=1,\ldots,k$, for natural number $k\geq 0$ and where $c_1,\ldots,c_k\in\mathbb{C}$ are complex numbers.</p>

<p>An <strong>unsigned simple function</strong> $f:\mathbb{R}^d\to[0,+\infty]$ is given as \eqref{eq:sf.1} but with the $c_i$ taking values in $[0,+\infty]$ rather than $\mathbb{C}$.</p>

<h3 id="int-unsgn-simp-func">Integral of a unsigned simple function</h3>
<p>If $f=c_1 1_{E_1}+\ldots+c_k 1_{E_k}$ is an unsigned simple function, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq c_1 m(E_1)+\ldots+c_k m(E_k),
\end{equation}
which means $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\in[0,+\infty]$.</p>

<h3 id="well-dfn-simp-int">Well-definedness of simple integral</h3>
<p><strong>Lemma 1</strong><br />
<em>Let $k,k’\geq 0$ be natural, $c_1,\ldots,c_k,c_1’,\dots,c_{k’}’\in[0,+\infty]$ and $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’\subset\mathbb{R}^d$ be Lebesgue measurable sets such that the identity
\begin{equation}
c_1 1_{E_1}+\ldots+c_k 1_{E_k}=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’}\label{eq:lemma1.1}
\end{equation}
holds identically on $\mathbb{R}^d$. Then we have</em>
\begin{equation}
c_1 m(E_1)+\ldots+c_k m(E_k)=c_1’ m(E_1’)+\ldots+c_{k’}’ m(E_{k’}’)
\end{equation}</p>

<p><strong>Proof</strong><br />
The $k+k’$ sets $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ partition $\mathbb{R}^d$ into $2^{k+k’}$ disjoint sets, each of which is an intersection of some of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ and their complements<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Removing any sets that are empty, we end up with a partition of $R^d$ of $m$ non-empty disjoint sets $A_1,\ldots,A_m$ for some $0\leq m\leq 2^{k+k’}$. It easily seen that $A_1,\ldots,A_m$ are then Lebesgue measurable due to the Lebesgue measurability of $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$.</p>

<p>With this set up, each of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ are unions of some of the $A_1,\ldots,A_m$. Or in other words, we have
\begin{equation}
E_1=\bigcup_{j\in J_i}A_j,
\end{equation}
and
\begin{equation}
E_{i’}’=\bigcup_{j’\in J_{i’}’}A_j’,
\end{equation}
for all $i=1,\ldots,k$ and $i’=1,\ldots,k’$, and some subsets $J_i,J_{i’}’\subset\{1,\ldots,m\}$. By finite additivity property of Lebesgue measure, we therefore have
\begin{equation}
m(E_i)=\sum_{j\in J_i}m(A_j)
\end{equation}
and
\begin{equation}
m(E_{i’}’)=\sum_{j\in J_{i’}’}m(A_j)
\end{equation}
Hence, the problem remains to show that
\begin{equation}
\sum_{i=1}^{k}c_i\sum_{j\in J_i}m(A_j)=\sum_{i’=1}^{k’}c_{i’}’\sum_{j\in J_{i’}’}m(A_j)\label{eq:lemma1.2}
\end{equation}
Fix $1\leq j\leq m$, we have that at each point $x$ in the non-empty set $A_j$, $1_{E_i}(x)$ is equal to $1_{J_i}(j)$, and similarly $1_{E_{i’}’}(x)$ is equal to $1_{J_{i’}’}(j)$. Then from \eqref{eq:lemma1.1} we have
\begin{equation}
\sum_{i=1}^{k}c_i 1_{J_i}(j)=\sum_{i’=1}^{k’}c_{i’}’1_{J_{i’}’}(j)
\end{equation}
Multiplying both sides by $m(A_j)$ and then summing over all $j=1,\ldots,m$, we obtain \eqref{eq:lemma1.2}</p>

<h3 id="alm-evwhr-spt">Almost everywhere and support</h3>
<p>A property $P(x)$ of a point $x\in\mathbb{R}^d$ is said to hold <strong>(Lebesgue) almost everywhere</strong> in $\mathbb{R}^d$ or for <strong>(Lebesgue) almost every point</strong> $x\in\mathbb{R}^d$, if the set of $x\in\mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure of zero (i.e. $P$ is true outside of a null set).</p>

<p>Two functions $f,g:\mathbb{R}^d\to Z$ into an arbitrary range $Z$ are referred to <strong>agree almost everywhere</strong> if we have $f(x)=g(x)$ almost every $x\in\mathbb{R}^d$.</p>

<p>The <strong>support</strong> of a function $f:\mathbb{R}^d\to\mathbb{C}$ or $f:\mathbb{R}^d\to[0,+\infty]$ is defined to be the set $\{x\in\mathbb{R}^d:f(x)\neq 0\}$ where $f$ is non-zero.</p>

<p><strong>Remark 2</strong></p>
<ul>
  <li>If $P(x)$ holds for almost every $x$, and $P(x)$ implies $Q(x)$, then $Q(x)$ holds for almost every $x$.</li>
  <li>If $P_1(x),P_2(x),\ldots$ are an at most countable family of properties, each of which individually holds for almost every $x$, then they will simultaneously holds for almost every $x$, since the countable union of null sets is still a null set.</li>
</ul>

<h3 id="bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</h3>
<p>Let $f,g:\mathbb{R}^d\to[0,+\infty]$ be simple unsigned functions.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in[0,+\infty]$.
	</li>
	<li>
		<b>Finiteness</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$ iff $f$ is finite almost everywhere, and its support has finite measure.
	</li>
	<li>
		<b>Vanishing</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=0$ iff $f$ is zero almost everywhere.
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Monotonicity</b>. If $f(x)\leq g(x)$ for almost every $x\in\mathbb{R}^d$, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\leq\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
Since $f,g:\mathbb{R}^d\to[0,+\infty]$ are simple unsigned functions, we can assume that
\begin{align}
f&amp;=c_1 1_{E_1}+\ldots+c_k 1_{E_k}, \\ g&amp;=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’},
\end{align}
where $c_1,\ldots,c_k,c_1’,\ldots,c_{k’}’\in[0,+\infty]$.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b><br />
		We have
		\begin{align}
		\hspace{-1cm}\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx&amp;=c_1 m(E_1)+\ldots+c_k m(E_k)+c_1' m(E_1')+\ldots+c_{k'}' m(E_{k'}') \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For any $c\in[0,+\infty]$, we have
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=c\left(c_1 m(E_1)+\ldots+c_k m(E_k)\right) \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Finiteness</b><br />
		Given $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$, then for every $i=1,\ldots,k$ we have that
		\begin{equation}
		c_i m(E_i)&lt;\infty\label{eq:bpsui.1}
		\end{equation}
		Suppose that $f$ is not finite almost everywhere, which means that there exists $1\leq i\leq k$ such that $E_i$ is a non-null set and $c_i=\infty$, or
		\begin{equation}
		c_i m(E_i)=\infty,
		\end{equation}
		which is in contrast with \eqref{eq:bpsui.1}.<br />
		Suppose that the support of $f$ has infinite measure, or in other word
		\begin{equation}
		c_i\neq 0,\hspace{1cm}i=1,\ldots,k\label{eq:bpsui.2}
		\end{equation}
		and
		\begin{equation}
		m\left(\bigcup_{n=1}^{k}E_n\right)=\infty,
		\end{equation}
		Since any $k$ subsets $E_1,\ldots,E_k$ of $\mathbb{R}^d$ partition $\mathbb{R}^d$ into $2^k$ disjoint sets, say $F_1,\ldots,F_{2^k}$. Hence, by finite additivity property of Lebesgue measure, we have
		\begin{equation}
		\sum_{n=1}^{2^k}m(F_n)=\infty,
		\end{equation}
		which implies that there exists $1\leq n\leq 2^k$ such that $m(F_n)=\infty$. And therefore, for a particular $1\leq i\leq k$ such that $F_n\subset E_i$, by monotonicity property of Lebesgue measure
		\begin{equation}
		m(E_i)\geq m(F_n)=\infty
		\end{equation}
		Thus, combining with \eqref{eq:bpsui.2} gives us
		\begin{equation}
		c_i m(E_1)=\infty,
		\end{equation}
		which again contradicts to \eqref{eq:bpsui.1}.<br />
		Given $f$ is finite almost everywhere and its support has finite measure, suppose that its integral is infinite, or
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=\infty,
		\end{equation}
		which implies that there exists $1\leq i\leq k$ such that either<br />
		(1) $c_i=\infty$ and $E_i$ is a non-null set, or<br />
		(2) $c_i\neq 0$ and $m(E)=\infty$.<br />
		If (1) happens, we then have that
		\begin{equation}
		f\geq c_i 1_{E_i}=\infty,
		\end{equation}
		which contradicts to our hypothesis.<br />
		If (2) happens, by monotonicity of Lebesgue measure, the support of $f$ then has infinite measure, which also contradicts to our hypothesis.
	</li>
	<li>
		<b>Vanishing</b><br />
		Given $\text{Simp}\int_{\mathbf{R^d}}f(x)\,dx=0$, we then have
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=0,
		\end{equation}
		which implies that for every $1\leq i\leq k$, we have that $c_i=0$ or $E_i$ is a null set.
		Therefore, $f$ is zero almost everywhere because in this case $f$ takes the value of non-zero iff $x$ is in a particular null set $E_j$.<br />
		Given $f$ is zero almost everywhere, for every $i=1,\ldots,k$, we have that either<br />
		(1) $c_i=0$, or<br />
		(2) $c_i\neq 0$ and $x\notin E_i$ with $E_i$ is a null set, or<br />
		(3) $c_i=0$ and and $x\notin E_i$ with $E_i$ is a null set.<br />
		Therefore, the integral of $f$
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=c_1 m(E_1)+\ldots+c_k m(E_k)=0
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
		Given $f$ and $g$ agree almost everywhere, we have that at any point $x\in\mathbb{R}^d$ such that $f(x)=g(x)$, by <b>lemma 1</b>, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		For more convenient, let $K=\{E_i\cap E_{i'}':1\leq i\leq k,1\leq i'\leq k'\}$. The set $K$ then has cardinality of $kk'$. Thus, without loss of generality, we can denote $K$ as
		\begin{equation}
		K=\{K_1,\ldots,K_{kk'}\}
		\end{equation}
		With this definition of $K$, the functions $f$ and $g$ can be rewritten by
		\begin{equation}
		f=a_1 1_{K_1}+\ldots+a_{kk'}1_{K_{kk'}}\label{eq:bpsui.3}
		\end{equation}
		and
		\begin{equation}
		g=b_1 1_{K_1}+\ldots+b_{kk'}1_{K_{kk'}}\label{eq:bpsui.4}
		\end{equation}
		On the other hand, the set in which $f(x)\neq g(x)$ is a null set. Thus by \eqref{eq:bpsui.3} and \eqref{eq:bpsui.4}, we have $x\in A$, where some $A\subset K$ is a null set, and for each $i$ such that $K_i\subset A$ (thus is also a null set, or $m(K_i)=0$), $a_i\neq b_i$, otherwise if $K_i\notin A$, $a_i=b_i$. Therefore, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		which proves our claim.
	</li>
	<li>
		<b>Monotonicity</b><br />
		Using the same procedure as the proof for equivalence, our claim can be proved.
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
		This follows directly from definition
	</li>
</ul>

<h3 id="abs-cvg-simp-int">Absolutely convergence simple integral</h3>
<p>A complex valued simple function $f:\mathbb{R}^d\to\mathbb{C}$ is known as <strong>absolutely integrable</strong> if
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}\vert f(x)\vert\,dx&lt;\infty
\end{equation}
If $f$ is absolutely integrable, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined for real signed $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}f_+(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}f_-(x)\,dx,
\end{equation}
where
\begin{align}
f_+(x)&amp;\doteq\max\left(f(x),0\right), \\ f_-(x)&amp;\doteq\max\left(-f(x),0\right),
\end{align}
and for complex-valued $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}\text{Re}\,f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}\,f(x)\,dx
\end{equation}</p>

<h3 id="bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</h3>
<p>Let $f,g:\mathbb{R}^d\to\mathbb{C}$ be absolutely integrable simple functions</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in\mathbb{C}$. Also we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
We first consider the case of real-valued $f$ and $g$.</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		Using the identity
		\begin{equation}
		f+g=(f+g)_{+}-(f+g)_{-}=(f_{+}-f_{-})+(g_{+}-g_{-})
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>
<p>For complex-valued $f$ and $g$ we have:</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		By definition of complex-valued simple integral and by linearity of simple unsigned integral we have
		\begin{align}
		&amp;\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx\nonumber \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}(f(x)+g(x))\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}(f(x)+g(x))\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}\text{Re}g(x)\,dx\nonumber \\ &amp;\hspace{2cm}+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}g(x)\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For the complex conjugate $\overline{f}$, we have its integral can be written as
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx-\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{align}
		Also, for any $c\in\mathbb{C}$, using linearity of simple unsigned integrals once again gives us
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}c\,\text{Re}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}c\,\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+c i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>

<h2 id="msr-funcs">Measurable functions</h2>
<p>Just as how the piecewise constant integral can be extended to the Riemann integral, the unsigned simple integral can be extended to the unsigned Lebesgue integral, by expanding the class of unsigned simple functions to the broader class of <strong>unsigned Lebesgue measurable functions</strong>.</p>

<h3 id="unsgn-msr-funcs">Unsigned measurable functions</h3>
<p>An unsigned function $f:\mathbb{R}^d\to[0,+\infty]$ is <strong>unsigned Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise limit of unsigned simple functions, i.e. if there exists a sequence $f_1,f_2,\ldots:\mathbb{R}\to[0,+\infty]$ of unsigned simple functions such that $f_n(x)\to f(x)$ for every $x\in\mathbb{R}^d$.</p>

<h3 id="equiv-ntn-msrb">Equivalent notions of measurability</h3>
<p><strong>Lemma 3</strong><br />
Let $f:\mathbb{R}\to[0,+\infty]$ be an unsigned function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is unsigned Lebesgue measurable.
	</li>
	<li>
		$f$ is the pointwise limit of unsigned simple functions $f_n$ (hence $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for all $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of unsigned simple function $f_n$ (thus $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for almost every $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f(x)=\sup_n f_n(x)$, where $0\leq f_1\leq f_2\leq\ldots$ is an increasing sequence of unsigned simple functions, each of which are bounded with finite measure support.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&gt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\geq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&lt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\leq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every interval $I\subset[0,+\infty)$, the set $f^{-1}(I)\doteq\{x\in\mathbb{R}^d:f(x)\in I\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) open set $U\subset[0,+\infty)$, the set $f^{-1}(U)\doteq\{x\in\mathbb{R}^d:f(x)\in U\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) closed set $K\subset[0,+\infty)$, the set $f^{-1}(K)\doteq\{x\in\mathbb{R}^d:f(x)\in K\}$ is Lebesgue measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="eg-msr-func">Examples of measurable function</h3>
<ul id="roman-list">
	<li>
		Every continuous function $f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		Every unsigned simple function is measurable.
	</li>
	<li>
		The supremum, infimum, limit superior, or limit inferior of unsigned measurable functions is unsigned measurable.
	</li>
	<li>
		An unsigned function that is equal almost everywhere to an unsigned measurable function, is also measurable.
	</li>
	<li>
		If a sequence $f_n$ of unsigned measurable functions converges pointwise almost everywhere to an unsigned limit $f$, then $f$ is also measurable.
	</li>
	<li>
		If $f:\mathbb{R}^d\to[0,+\infty]$ is measurable and $\phi:[0,+\infty]\to[0,+\infty]$ is continuous, then $\phi\circ f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		If $f,g$ are unsigned measurable functions, then $f+g$ and $fg$ are measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="cmplx-msrb">Complex measurability</h3>
<p>An almost everywhere defined complex-valued function $f:\mathbb{R}^d\to\mathbb{C}$ is <strong>Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise almost everywhere limit of complex-valued simple functions.</p>

<h3 id="equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</h3>
<p>Let $f:\mathbb{R}^d\to\mathbb{C}$ be an almost everywhere defined complex-valued function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is measurable.
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of complex-valued simple functions.
	</li>
	<li>
		The (magnitudes of the) positive and negative parts of $\text{Re}(f)$ and $\text{Im}(f)$ are unsigned measurable functions.
	</li>
	<li>
		$f^{-1}(U)$ is Lebesgue measurable for every open set $U\subset\mathbb{C}$.
	</li>
	<li>
		$f^{-1}(K)$ is Lebesgue measurable for every closed set $K\subset\mathbb{C}$.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h2 id="unsgn-lebesgue-int">Unsigned Lebesgue integrals</h2>

<h3 id="lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</h3>
<p>Let $f:\mathbb{R}^d\to[0,+\infty]$ be an unsigned functions (not necessarily measurable). We define the <strong>lower unsigned Lebesgue integral</strong>, denoted as $\underline{\int_{\mathbb{R}^d}}f(x)\,dx$, to be the quantity
\begin{equation}
\underline{\int_\mathbb{R}^d}f(x)\,dx\doteq\sup_{0\leq g\leq f;g\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx,
\end{equation}
where $g$ ranges over all unsigned simple functions $g:\mathbb{R}^d\to[0,+\infty]$ that are pointwise bounded by $f$.</p>

<p>We can also define the <strong>upper unsigned Lebesgue integral</strong> as
\begin{equation}
\overline{\int_\mathbb{R}^d}f(x)\,dx\doteq\inf_{h\geq f;h\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}h(x)\,dx
\end{equation}</p>

<h2 id="abs-intb">Absolute integrability</h2>

<h2 id="littlewoods-prncpl">Littlewood’s three principles</h2>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>.</span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It should be simpler to consider the case of $k=2$, in particular with two sets $E_1,E_2\subset\mathbb{R}^d$. These two sets partition $\mathbb{R}^d$ into four disjoint sets: $E_1\cap E_2,E_1\cap E_2^c,E_1^c\cap E_2,E_1^c\cap E_2^c$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-integral" /><summary type="html"><![CDATA[Note on measure theory part 3]]></summary></entry><entry><title type="html">Generalized Linear Models</title><link href="http://localhost:4000/2022/08/13/glm.html" rel="alternate" type="text/html" title="Generalized Linear Models" /><published>2022-08-13T13:00:00+07:00</published><updated>2022-08-13T13:00:00+07:00</updated><id>http://localhost:4000/2022/08/13/glm</id><content type="html" xml:base="http://localhost:4000/2022/08/13/glm.html"><![CDATA[<blockquote>
  <p>Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#ind-basis">Independence, basis in vector space</a>
        <ul>
          <li><a href="#lin-ind">Linear independence</a></li>
          <li><a href="#basis">Basis of a vector space</a></li>
        </ul>
      </li>
      <li><a href="#lagrange-mult">Lagrange Multipliers</a></li>
    </ul>
  </li>
  <li><a href="#lin-models-reg">Linear models for Regression</a>
    <ul>
      <li><a href="#lin-basis-func-models">Linear basis function models</a>
        <ul>
          <li><a href="#least-squares-reg">Least squares</a></li>
          <li><a href="#geo-least-squares">Geometrical interpretation of least squares</a></li>
          <li><a href="#lms">The LMS algorithm</a></li>
          <li><a href="#reg-least-squares">Regularized least squares</a></li>
          <li><a href="#mult-outputs">Multiple outputs</a></li>
        </ul>
      </li>
      <li><a href="#bayes-lin-reg">Bayesian Linear Regression</a>
        <ul>
          <li><a href="#param-dist">Parameter distribution</a></li>
          <li><a href="#pred-dist-reg">Predictive distribution</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#lin-models-reg">Linear models for Classification</a>
    <ul>
      <li><a href="#disc-funcs">Discriminant functions</a>
        <ul>
          <li><a href="#least-squares-clf">Least squares</a></li>
          <li><a href="#fisher-ld">Fisher’s linear discriminant</a>
            <ul>
              <li><a href="#fisher-ld-bin-clf">Binary classification</a></li>
              <li><a href="#fisher-ld-clf">Multi-class classification</a></li>
            </ul>
          </li>
          <li><a href="#perceptron">The perceptron algorithm</a></li>
        </ul>
      </li>
      <li><a href="#prob-gen-models">Probabilistic Generative Models</a>
        <ul>
          <li><a href="#gauss-gen-models">Gaussian Generative Models</a>
            <ul>
              <li><a href="#max-likelihood-sols">Maximum likelihood solutions</a>
                <ul>
                  <li><a href="#ggm-bin-clf">Binary classification</a></li>
                  <li><a href="#ggm-clf">Multi-class classification</a></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#prob-disc-models">Probabilistic Discriminative Models</a>
        <ul>
          <li><a href="#log-reg">Logistic Regression</a></li>
          <li><a href="#softmax-reg">Softmax Regression</a></li>
          <li><a href="#newtons-method">Newton’s method</a>
            <ul>
              <li><a href="#nm-lin-reg">Linear Regression</a></li>
              <li><a href="#nm-log-reg">Logistic Regression</a></li>
              <li><a href="#nm-softmax-reg">Softmax Regression</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#bayes-log-reg">Bayesian Logistic Regression</a>
        <ul>
          <li><a href="#laplace-approx">The Laplace approximation</a></li>
          <li><a href="#approx-posterior">Approximation of the posterior</a></li>
          <li><a href="#pred-dist-clf">Predictive distribution</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#glm">Generalized linear models</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="ind-basis">Independence, basis in vector space</h3>

<h4 id="lin-ind">Linear independence</h4>
<p>The sequence of vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ is said to be <strong>linearly independent</strong> (or <strong>independent</strong>) if
\begin{equation}
c_1\mathbf{x}_1+\ldots+c_n\mathbf{x}_n=\mathbf{0}
\end{equation}
only when $c_1,\ldots,c_n$ are all zero.</p>

<p>Considering those $n$ vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ as $n$ columns of a matrix $\mathbf{A}$
\begin{equation}
\mathbf{A}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{x}_1 &amp; \ldots &amp; \mathbf{x}_n \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
we have that the columns of $\mathbf{A}$ are independent when
\begin{equation}
\mathbf{A}\mathbf{x}=\mathbf{0}\hspace{0.5cm}\Leftrightarrow\hspace{0.5cm}\mathbf{x}=\mathbf{0},
\end{equation}
or in other words, the rank of $\mathbf{A}$ is equal to the number of columns of $\mathbf{A}$.</p>

<h4 id="basis">Basis of a vector space</h4>
<p>We say that vectors $\mathbf{v}_1,\ldots,\mathbf{v}_k$ span a space $S$ when the space consists of all combinations of those vectors. Or in other words, any vector $\mathbf{u}\in S$ can be displayed as linear combination of $\mathbf{v}_i$.<br />
In this case, $S$ is the smallest space containing those vectors.</p>

<p>A <strong>basis</strong> for a vector space $S$ is a sequence of vectors $\mathbf{v}_1,\ldots,\mathbf{v}_d$ having two properties:</p>
<ul id="roman-list">
	<li>$\mathbf{v}_1,\ldots,\mathbf{v}_d$ are independent</li>
	<li>$\mathbf{v}_1,\ldots,\mathbf{v}_d$ span $S$</li>
</ul>
<p>In $S$, every basis for that space has the same number of vectors, which is the dimension of $S$. Therefore, there are exactly $n$ vectors in every basis for $\mathbb{R}^n$.</p>

<p>With that definition of a basis $\mathbf{v}_1,\dots,\mathbf{v}_d$ of $S$, for each vector $\mathbf{u}\in S$, there exists only one sequence $c_1,\ldots,c_d$ such that
\begin{equation}
\mathbf{u}=c_1\mathbf{v}_1+\ldots+c_d\mathbf{v}_d
\end{equation}</p>

<h3 id="lagrange-mult">Lagrange Multipliers</h3>
<p>Consider the problem of finding the maximum (or minimum) of $w=f(x_1,x_2,x_3)$ subject to a constraint relating $x_1,x_2$ and $x_3$
\begin{equation}
g(x_1,x_2,x_3)=0
\end{equation}
Apart from solving $x_3$ in terms of $x_1$ and $x_2$ in the constraint and substituting into the original function, which now becomes an unconstrained, here we can also solve this problem as a constrained one.</p>

<p>The idea is we are using the observation that the gradient vector $\nabla f(\mathbf{x})$ and $\nabla g(\mathbf{x})$ are parallel, because:</p>

<p>Suppose $f(\mathbf{x})$ has a local maximum at $\mathbf{x}^*$ on the constraint surface $g(\mathbf{x})=0$.</p>

<p>Let $\mathbf{r}(t)=\langle x_1(t),x_2(t),x_3(t)\rangle$ be a parameterized curve on the constraint surface such that and $\mathbf{r}(t)$ has
\begin{equation}
(x_1(0),x_2(0),x_3(0))^\text{T}=\mathbf{x}
\end{equation}
And also, let $h(t)=f(x_1(t),x_2(t),x_3(t))$, then it implies that $h$ has a maximum at $t=0$, which lets
\begin{equation}
h’(0)=0
\end{equation}
Taking the derivative of $h$ w.r.t, we obtain
\begin{equation}
h’(t)=\nabla f(\mathbf{x})\big\vert_{\mathbf{r}(t)}\mathbf{r}’(t)
\end{equation}
Therefore,
\begin{equation}
\nabla f(\mathbf{x})\big\vert_{\mathbf{x}^*}\mathbf{r}’(0)=0,
\end{equation}
which implies that $\nabla f(\mathbf{x})$ is perpendicular to any curve in the constraint space that goes through $\mathbf{x}^*$. And since $\nabla g(\mathbf{x})$ perpendicular to the constraint surface $g(x)=0$, then $\nabla g(\mathbf{x})$ is also perpendicular to those curves. This implies that $\nabla f(\mathbf{x})$ is parallel to $\nabla g(\mathbf{x})$.</p>

<p>With this property, we can write $\nabla f(\mathbf{x})$ in terms of $\nabla g(\mathbf{x})$, as
\begin{equation}
\nabla f(\mathbf{x})=\lambda\nabla g(\mathbf{x}),
\end{equation}
where $\lambda\neq 0$ is a constant called <strong>Lagrange multiplier</strong>.</p>

<p>With this definition of Lagrange multiplier, we continue to define the <strong>Lagrangian</strong> function, given as
\begin{equation}
\mathcal{L}(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda g(\mathbf{x})
\end{equation}
Then letting the partial derivative of Lagrangian w.r.t $\lambda$ be zero gives us the constraint
\begin{equation}
0=\frac{\partial \mathcal{L}(\mathbf{x},\lambda)}{\partial\lambda}=g(\mathbf{x})
\end{equation}
With Lagrangian, in order to find the maximum of $f(\mathbf{x})$ that satisfies $g(\mathbf{x})=0$, we will instead be trying to solve
\begin{equation}
\max_\mathbf{x}\min_\lambda\mathcal{L}(\mathbf{x},\lambda),
\end{equation}
This can be solved by letting derivatives of the Lagrangian $\mathcal{L}$ w.r.t $x_i$ (i.e. components of $\mathbf{x}$) and $\lambda$ be zero
\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_i}=0,\hspace{1cm}\frac{\partial\mathcal{L}}{\partial\lambda}=0
\end{equation}
and solve for $x_i$ and $\lambda$.</p>

<h2 id="lin-models-reg">Linear models for Regression</h2>
<p>Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\mathbf{x}$ of input variables.</p>

<h3 id="lin-basis-func-models">Linear basis function models</h3>
<p>The simplest linear model used for regression tasks is <strong>linear regression</strong>, which is defined as a linear combination of the input variables
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+\ldots+w_Dx_D,
\end{equation}
where $\mathbf{x}=(x_1,\ldots,x_D)^\text{T}$ is the input variables, while $w_i$’s are the parameters parameterizing the space of linear function mapping from the input space $\mathcal{X}$ of $\mathbf{x}$ to $\mathcal{Y}$.</p>

<p>With the idea of spanning a space by its basis vectors, we can generalize it to establishing a function space by linear combinations of simpler basis functions. Or in other words, we can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\mathbf{x}$, as
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1\phi_1(\mathbf{x})+\ldots+w_{M-1}\phi_{M-1}(\mathbf{x})=w_0+\sum_{i=1}^{M-1}w_i\phi_i(\mathbf{x}),\label{eq:lbfm.1}
\end{equation}
where $\phi_i(\mathbf{x})$’s are called the <strong>basis functions</strong>; $w_0$ is called a <strong>bias parameter</strong>. By letting <span id="dummy-coeff">$w_0$</span> be a coefficient corresponding to a dummy basis function $\phi_0(\mathbf{x})=1$, \eqref{eq:lbfm.1} can be written in a more convenient way
\begin{equation}
y(\mathbf{x},\mathbf{w})=\sum_{i=0}^{M-1}w_i\phi_i(\mathbf{x})=\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\label{eq:lbfm.2}
\end{equation}
where $\mathbf{w}=(w_0,\ldots,w_{M-1})^\text{T}$ and $\boldsymbol{\phi}=(\phi_0,\ldots,\phi_{M-1})^\text{T}$, with $\phi_0(\cdot)=1$.</p>

<p>There are various choices of basis functions:</p>
<ul id="number-list">
	<li>
		<b>Polynomial basis</b>. Each basis function $\phi_i$ is a powers of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=x^i
		\end{equation}
		An example of polynomial basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/polynomial-basis.png" alt="polynomial basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Example of polynomial basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
	<li>
		<b>Gaussian basis function</b>. Each basis function $\phi_i$ is a Gaussian function of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=\exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right)
		\end{equation}
		An example of Gaussian basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/gaussian-basis.png" alt="Gaussian basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: Example of Gaussian basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
	<li>
		<b>Sigmoidal basis function</b>. Each basis function $\phi_i$ is defined as
		\begin{equation}
		\phi_i(x)=\sigma\left(\frac{x-\mu_i}{\sigma_i}\right),
		\end{equation}
		where $\sigma(\cdot)$ is the logistic sigmoid function
		\begin{equation}
		\sigma(x)=\frac{1}{1+\exp(-x)}
		\end{equation}
		An example of sigmoidal basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/sigmoidal-basis.png" alt="sigmoidal basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: Example of sigmoidal basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
</ul>

<h4 id="least-squares-reg">Least squares</h4>
<p>Assume that the target variable $t$ and the inputs $\mathbf{x}$ is related via the equation
\begin{equation}
t=y(\mathbf{x},\mathbf{w})+\epsilon,
\end{equation}
where $\epsilon$ is an error term that captures random noise such that $\epsilon\sim\mathcal{N}(0,\sigma^2)$, which means the density of $\epsilon$ can be written as
\begin{equation}
p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\epsilon^2}{2\sigma^2}\right),
\end{equation}
which implies that
\begin{equation}
p(t|\mathbf{x};\mathbf{w},\beta)=\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t-y(\mathbf{x},\mathbf{w}))^2\beta}{2}\right),\label{eq:lsr.1}
\end{equation}
where $\beta=1/\sigma^2$ is the precision of $\epsilon$, or
\begin{equation}
t|\mathbf{x};\mathbf{w},\beta\sim\mathcal{N}(y(\mathbf{x},\mathbf{w}),\beta^{-1})
\end{equation}
Consider a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$ with corresponding target values $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$ and assume that these data points are drawn independently from the distribution above, we obtain the batch version of \eqref{eq:lsr.1}, called the <strong>likelihood function</strong>, given as
\begin{align}
L(\mathbf{w},\beta)=p(\mathbf{t}|\mathbf{X};\mathbf{w},\beta)&amp;=\prod_{i=1}^{N}p(t_i|\mathbf{x}_i;\mathbf{w},\beta) \\ &amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\label{eq:lsr.2}
\end{align}
By maximum likelihood, we will be looking for values of $\mathbf{w}$ and $\beta$ that maximize the likelihood. We do this by considering maximizing a simpler likelihood, called <strong>log likelihood</strong>, denoted as $\ell(\mathbf{w},\beta)$, defined as
\begin{align}
\ell(\mathbf{w},\beta)=\log{L(\mathbf{w},\beta)}&amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right) \\ &amp;=\sum_{i=1}^{N}\log\left[\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\right] \\ &amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\sum_{i=1}^{N}\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2} \\ &amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\label{eq:lsr.3},
\end{align}
where $E_D(\mathbf{w})$ is the sum-of-squares error function, defined as
\begin{equation}
E_D(\mathbf{w})\doteq\frac{1}{2}\sum_{i=1}^{N}\left(t_i-y(\mathbf{x}_i,\mathbf{w})\right)^2\label{eq:lsr.4}
\end{equation}
Consider the gradient of \eqref{eq:lsr.3} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}\ell(\mathbf{w},\beta)&amp;=\nabla_\mathbf{w}\left[\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\right] \\ &amp;\propto\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\big(t_i-y(\mathbf{x}_i,\mathbf{w})\big)^2 \\ &amp;=\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\left(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}\big(\mathbf{x}_i\right)\big)^2 \\ &amp;=\sum_{i=1}^{N}(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i))\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}
\end{align}
By gradient descent, letting this gradient to zero gives us
\begin{equation}
\sum_{i=1}^{N}t_i\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}-\mathbf{w}^\text{T}\sum_{i=1}^{N}\boldsymbol{\phi}(\mathbf{x}_i)\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}=0,
\end{equation}
which implies that
\begin{equation}
\mathbf{w}_\text{ML}=\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},\label{eq:lsr.5}
\end{equation}
which is known as the <strong>normal equations</strong> for the least squares problem. In \eqref{eq:lsr.5}, $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M}$ is called the <strong>design matrix</strong>, whose elements are given by $\boldsymbol{\Phi}_{ij}=\phi_j(\mathbf{x}_i)$
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;\ddots&amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
and the quantity
\begin{equation}
\boldsymbol{\Phi}^\dagger\doteq\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}
\end{equation}
is called the <strong>Moore-Penrose pseudoinverse</strong> of the matrix $\boldsymbol{\Phi}$.</p>

<p>On the other hand, consider the gradient of \eqref{eq:lsr.3} w.r.t $\beta$ and set it equal to zero, we obtain
\begin{equation}
\beta=\frac{N}{\sum_{i=1}^{N}\big(t_i-\mathbf{w}_\text{ML}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2}
\end{equation}</p>

<h4 id="geo-least-squares">Geometrical interpretation of least squares</h4>
<p>As mentioned before, we have applied the idea of spanning a vector space by its basis vectors when constructing basis functions.</p>

<p>In particular, consider an $N$-dimensional space whose axes are given by $t_i$, which implies that
\begin{equation}
\mathbf{t}=(t_1,\ldots,t_N)^\text{T}
\end{equation}
is a vector contained in the space.</p>
<figure>
	<img src="/assets/images/2022-08-13/geo-least-squares.png" alt="geometry of least squares" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 300px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: Geometrical interpretation of the least-squares solution. The figure is taken from <span><a href="#bishops-book">Bishop’s book</a></span></figcaption>
</figure>

<p>Each basis function $\phi_j(\mathbf{x}_i)$, evaluated at the $N$ data points, then can also be presented as a vector in the same space, denoted by $\boldsymbol{\varphi}_j$, as illustrated in <strong>Figure 4</strong> above. Therefore, the design matrix $\boldsymbol{\Phi}$ can be represented as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \boldsymbol{\varphi}_{0}&amp;\ldots&amp;\boldsymbol{\varphi}_{M-1} \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
When the number $M$ of basis functions is smaller than the number $N$ of data points, the $M$ vectors $\phi_j(\mathbf{x}_i)$ will span a linear subspace $\mathcal{S}$ of $M$ dimensions.</p>

<p>We define $\mathbf{y}$ to be an $N$-dimensional vector whose the $i$-th element is given by $y(\mathbf{x}_i,\mathbf{w})$
\begin{equation}
\mathbf{y}=\big(y(\mathbf{x}_1,\mathbf{w}),\ldots,y(\mathbf{x}_N,\mathbf{w})\big)^\text{T}
\end{equation}
Since $\mathbf{y}$ is a linear combination of $\boldsymbol{\varphi}_i$, then $\mathbf{y}\in\mathcal{S}$.
Then the sum-of-squares error \eqref{eq:lsr.4} is exactly (with a factor of $1/2$) the squared Euclidean distance between $\mathbf{y}$ and $\mathbf{t}$. Therefore, the least square solution to $\mathbf{w}$ is the one that makes $\mathbf{y}$ closest to $\mathbf{t}$.</p>

<p>This solution corresponds to the orthogonal projection of $t$ onto the subspace $S$ spanned by $\boldsymbol{\varphi}_i$, because we have that
\begin{align}
\mathbf{y}^\text{T}(\mathbf{t}-\mathbf{y})&amp;=\left(\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right) \\ &amp;=\left(\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right) \\ &amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t} \\ &amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ &amp;=0,
\end{align}</p>

<h4 id="lms">The LMS algorithm</h4>
<p>The <strong>least-means-squares</strong>, or <strong>LMS</strong> algorithm for the sum-of-squares error \eqref{eq:lsr.4}, which start with some initial vector $\mathbf{w}_0$ of $\mathbf{w}$, and repeatedly perform the update
\begin{equation}
\mathbf{w}_{t+1}=\mathbf{w}_t+\eta(t_n-\mathbf{w}_t^\text{T}\boldsymbol{\phi}_n)\boldsymbol{\phi}_n,
\end{equation}
where $\boldsymbol{\phi}_n$ denotes $\boldsymbol{\phi}(\mathbf{x}_n)$, and $\eta$ is called the <strong>learning rate</strong> which controls the update amount.</p>

<h4 id="reg-least-squares">Regularized least squares</h4>
<p>To control over-fitting, in the error function \eqref{eq:lsr.4}, we add an regularization term, which makes the total error function to be minimized take the form
\begin{equation}
E_D(\mathbf{w})+\lambda E_W(\mathbf{w}),\label{eq:rls.1}
\end{equation}
where $\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\mathbf{w})$ and the regularization term $E_W(\mathbf{w})$. One simple possible form of regularizer is given as
\begin{equation}
E_W(\mathbf{w})=\frac{1}{2}\mathbf{w}^\text{T}\mathbf{w}
\end{equation}
The total error function \eqref{eq:rls.1} then can be written as
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\mathbf{w}^\text{T}\mathbf{w}\label{eq:rls.2}
\end{equation}
Setting the gradient of this error to zero and solving for $\mathbf{w}$, we have the solution
\begin{equation}
\mathbf{w}= (\lambda\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}\label{eq:rls.3}
\end{equation}
This particular choice of regularizer is called <strong>weight decay</strong> because it encourages weight values to decay towards zero in sequential learning.</p>

<p>Another choice of regularizer which is more general lets the regularized error have the form
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\sum_{j=1}^{M}\vert w_j\vert^q,
\end{equation}
where $q=2$ corresponds to the regularizer \eqref{eq:rls.2}.</p>

<h4 id="mult-outputs">Multiple outputs</h4>
<p>When the target of our model is instead in multiple-dimensional form, denoted as $\mathbf{t}$, we can generalize our model to be
\begin{equation}
\mathbf{y}(\mathbf{x},\mathbf{w})=\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),
\end{equation}
where $\mathbf{y}\in\mathbb{R}^K, \mathbf{W}\in\mathbb{R}^{M\times K}$ is the matrix of parameters, $\boldsymbol{\phi}\in\mathbb{R}^M$ with $\phi_i(\mathbf{x})$ as the $i$-th element, and with $\phi_0(\mathbf{x})=1$.</p>

<p>With this generalization, \eqref{eq:lsr.1} can be also be rewritten as
\begin{equation}
p(\mathbf{t}|\mathbf{x};\mathbf{W},\beta)=\sqrt{\frac{\beta}{2\pi\vert\mathbf{I}\vert}}\exp\left[-\frac{1}{2}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)^\text{T}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)\beta\mathbf{I}^{-1}\right],
\end{equation}
or in other words
\begin{equation}
\mathbf{t}|\mathbf{x};\mathbf{W},\beta\sim\mathcal{N}(\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1}\mathbf{I})
\end{equation}
With a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$, our target values can also be vectorized into $\mathbf{T}\in\mathbb{R}^{N\times K}$ given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.1cm}\mathbf{t}_1^\text{T}\hspace{0.1cm}- \\ \vdots \\ -\hspace{0.1cm}\mathbf{t}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right],
\end{equation}
and likewise with the input matrix $\mathbf{X}$ vectorized from input vectors $\mathbf{x}_1,\ldots,\mathbf{x}_N$. With these definitions, the multi-dimensional likelihood can be defined as
\begin{align}
L(\mathbf{W},\beta)&amp;=p(\mathbf{T}|\mathbf{X};\mathbf{W},\beta) \\ &amp;=\prod_{i=1}^{N}p(\mathbf{t}_i|\mathbf{x}_i;\mathbf{W},\beta) \\ &amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right]
\end{align}
And thus the log likelihood now becomes
\begin{align}
\ell(\mathbf{W},\beta)&amp;=\log L(\mathbf{W},\beta) \\ &amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;=\sum_{i=1}^{N}\log\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;=\frac{N}{2}\log\frac{\beta}{2\pi}-\frac{\beta}{2}\sum_{i=1}^{N}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)
\end{align}
Taking the gradient of the log likelihood w.r.t $\mathbf{W}$, setting it to zero and solving for $\mathbf{W}$ gives us
\begin{equation}
\mathbf{W}_\text{ML}=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{T}
\end{equation}</p>

<h3 id="bayes-lin-reg">Bayesian linear regression</h3>

<h4 id="param-dist">Parameter distribution</h4>
<p>Consider the noise precision parameter $\beta$ as a constant. From the equation \eqref{eq:lsr.2}, we see that the likelihood function $L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})$ takes the form of an exponential of a quadratic form in $\mathbf{w}$. Thus, if we choose the prior $p(\mathbf{w})$ as a Gaussian, the corresponding posterior will also become a Gaussian due to being computed as a product of two exponentials of quadratic forms of $\mathbf{w}$. This makes the prior be a conjugate distribution for the likelihood function, and hence be given by
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ is the mean vector and $\mathbf{S}_0$ is the covariance matrix.</p>

<p>By the <a href="/2021/11/22/normal-dist.html#marg-cond-gaussian">result</a>, we have that the corresponding posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, which is a conditional Gaussian distribution, is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N),\label{eq:pd.1}
\end{equation}
where the mean $\mathbf{m}_N$ and the precision matrix $\mathbf{S}_N^{-1}$ are defined as
\begin{align}
\mathbf{m}_N&amp;=\mathbf{S}_N(\mathbf{S}_0^{-1}\mathbf{m}_0+\beta\boldsymbol{\Phi}^\text{T}\mathbf{t}), \\ \mathbf{S}_N^{-1}&amp;=\mathbf{S}_0^{-1}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Therefore, by MAP, we have
\begin{align}
\mathbf{w}_\text{MAP}&amp;=\underset{\mathbf{w}}{\text{argmax}}\,\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\Big] \\ &amp;=\underset{\mathbf{w}}{\text{argmin}}\,(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)
\end{align}
By this <a href="/2021/11/22/normal-dist.html#precision-eigenvalue">property</a> of the covariance matrix, we have that the precision matrix $\mathbf{S}_N^{-1}$ and the covariance matrix $\mathbf{S}_N$ have the same set of eigenvalues, which are non-negative due to the fact that $\mathbf{S}_N$ is positive semi-definite. This also means that $\mathbf{S}_N$ is positive semi-definite, and thus
\begin{equation}
(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\geq0
\end{equation}
Therefore, the maximum posterior weight vector is also the mean vector
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N\label{eq:pd.2}
\end{equation}
Consider an infinite broad prior $\mathbf{S}_0=\alpha^{-1}\mathbf{I}$ with $\alpha\to 0$, in this case the mean $\mathbf{m}_N$ reduces to the maximum likelihood value $\mathbf{w}_\text{ML}$ given by \eqref{eq:lsr.5}. And if $N=0$, then the posterior distribution reverts to the prior.</p>

<p>Furthermore, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$, the posterior given in \eqref{eq:pd.1} can be regarded as the prior distribution for that data point. If the model is given as \eqref{eq:lsr.1}, the likelihood function of the newly added data point is then given in form
\begin{equation}
p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})=\left(\frac{\beta}{2\pi}\right)^{1/2}\exp\left(-\frac{(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})\beta}{2}\right),
\end{equation}
where $\boldsymbol{\phi}_{N+1}=\boldsymbol{\phi}(\mathbf{x}_{N+1})$.
Therefore, the posterior distribution of the data point $(\mathbf{x}_{N+1},t_{N+1})$ can be computed as
\begin{align}
&amp;\hspace{0.7cm}p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t}) \\ &amp;\propto p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})p(\mathbf{w}\vert\mathbf{t}) \\ &amp;=\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)-\frac{1}{2}(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})^2\beta\Big] \\ &amp;=\exp\Big[-\frac{1}{2}\big(\mathbf{w}^\text{T}\mathbf{S}_N^{-1}\mathbf{w}+\beta\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{w}\big)+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big] \\ &amp;=\exp\Big[-\frac{1}{2}\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)\mathbf{w}+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big],
\end{align}
where $c$ is a constant w.r.t $\mathbf{w}$, i.e., $c$ is independent of $\mathbf{w}$, which claims that the posterior distribution is also a Gaussian, given by
\begin{equation}
p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1})\label{eq:pd.3}
\end{equation}
where the precision matrix $\mathbf{S}_{N+1}$ is defined as
\begin{equation}
\mathbf{S}_{N+1}^{-1}\doteq\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T},
\end{equation}
and the mean $\mathbf{m}_{N+1}$ is given by
\begin{equation}
\mathbf{m}_{N+1}\doteq\mathbf{S}_{N+1}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)
\end{equation}
Consider the prior as a Gaussian, defined by
\begin{equation}
p(\mathbf{w}\vert\alpha)=\mathcal{N}(\mathbf{w}\vert\mathbf{0},\alpha^{-1}\mathbf{I}),
\end{equation}
Therefore, the corresponding posterior over $\mathbf{w}$, $p(\mathbf{w}\vert\mathbf{t})$, will be given as \eqref{eq:pd.1} with
\begin{align}
\mathbf{m}_N&amp;=\beta\mathbf{S}_N\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ \mathbf{S}_N^{-1}&amp;=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Taking the natural logarithm of the posterior distribution gives us the sum of the log likelihood and the log of the prior, as a function of $\mathbf{w}$, given by
\begin{equation}
\log p(\mathbf{w}\vert\mathbf{t})=-\frac{\beta}{2}\sum_{n=1}^{N}\big(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_n)\big)^2-\frac{\alpha}{2}\mathbf{w}^\text{T}\mathbf{w}+c
\end{equation}
Therefore, maximizing this posterior is equivalent to minimizing the sum of the sum-of-squares error function with addition of a quadratic regularization term, which is exactly the equation \eqref{eq:rls.2} with $\lambda=\alpha/\beta$.</p>

<p>In addition, by $\eqref{eq:pd.2}$, we have that
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N=\beta\left(\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}=\left(\frac{\alpha}{\beta}\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{equation}
which for setting $\lambda=\alpha/\beta$ gives us exactly the solution \eqref{eq:rls.3} for the regularized least squares \eqref{eq:rls.2}.</p>

<h4 id="pred-dist-reg">Predictive distribution</h4>
<p>The <strong>predictive distribution</strong> that gives us the information to make predictions $t$ for new values $\mathbf{x}$ is defined as
\begin{equation}
p(t\vert\mathbf{x},\mathbf{t},\alpha,\beta)=\int p(t\vert\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)\,d\mathbf{w}\label{eq:pdr.1}
\end{equation}
in which $\mathbf{t}$ is the vector of target values from the training set.</p>

<p>The conditional distribution $p(t\vert\mathbf{x},\mathbf{w},\beta)$ of the target variable is given by \eqref{eq:lsr.1}, and the posterior weight distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$ is given by \eqref{eq:pd.1}. Thus, as a <a href="/2021/11/22/normal-dist.html#marg-cond-gaussian">marginal Gaussian distribution</a>, the distribution \eqref{eq:pdr.1} can be rewritten as
\begin{align}
p(t\vert\mathbf{x},\mathbf{t},\mathbf{w},\beta)&amp;=\int\mathcal{N}(t\vert\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1})\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N)\,d\mathbf{w} \\ &amp;=\mathcal{N}(t\vert\mathbf{m}_N^\text{T}\boldsymbol{\phi}(\mathbf{x}),\sigma_N^2(\mathbf{x})),
\end{align}
where the variance $\sigma_N^2(\mathbf{x})$ of the predictive distribution is defined as
\begin{equation}
\sigma_N^2(\mathbf{x})\doteq\beta^{-1}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.2}
\end{equation}
The first term in \eqref{eq:pdr.2} represents the noise on the data, while the second term reflects the uncertainty associated with the parameters $\mathbf{w}$.</p>

<p>It is worth noting that as additional data points are observed, the posterior distribution becomes narrower. In particular, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$. Therefore, as given by the result \eqref{eq:pd.3}, its posterior distribution is
\begin{equation}
p(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1}),
\end{equation}
where
\begin{align}
\mathbf{m}_{N+1}&amp;=\mathbf{S}_{N+1}(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}), \\ \mathbf{S}_{N+1}^{-1}&amp;=\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}
\end{align}
Therefore, the variance of the corresponding predictive distribution for the newly added data point is then given as
\begin{equation}
\sigma_{N+1}^2(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_{N+1}\boldsymbol{\phi}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\big(\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)^{-1}\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.3}
\end{equation}
Using the matrix identity
\begin{equation}
(\mathbf{M}+\mathbf{v}\mathbf{v}^\text{T})^{-1}=\mathbf{M}^{-1}-\frac{(\mathbf{M}^{-1}\mathbf{v})(\mathbf{v}^\text{T}\mathbf{M}^{-1})}{1+\mathbf{v}^\text{T}\mathbf{M}^{-1}\mathbf{v}},
\end{equation}
in the equation \eqref{eq:pdr.3} gives us
\begin{align}
\sigma_{N+1}^2(\mathbf{x})&amp;=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\left(\mathbf{S}_N-\frac{\beta\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\right)\boldsymbol{\phi}(\mathbf{x}) \\ &amp;=\sigma_N^2(\mathbf{x})-\beta\frac{\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\leq\sigma_N^2(\mathbf{x}),
\end{align}
since
\begin{equation}
\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})=\left\Vert\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\right\Vert_2^2\geq0,
\end{equation}
and since
\begin{equation}
1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}&gt;0
\end{equation}
due to $\mathbf{S}_N$ is the covariance matrix of the posterior distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$, which implies that it is positive semi-definite.</p>

<p>In other words, as $N\to\infty$, the second term in \eqref{eq:pdr.2} goes to zero, and the variance of the predictive distribution solely depends on $\beta$.</p>

<h2 id="linear-models-for-classification">Linear models for Classification</h2>
<p>In Machine Learning literature, <strong>classification</strong> refers the to task of taking an input vector $\mathbf{x}$ and assigning it to one of $K$ classes $\mathcal{C}_k$ for $k=1,\ldots,K$. Usually, each input will be assigned only to a single class. In this case, the input space is divided by the <strong>decision boundaries</strong> (or <strong>decision surfaces</strong>) into <strong>decision regions</strong>.</p>

<p>Taking an input space of $D$ dimensions, linear models are defined to be linear functions of the input vector $x$, and thus are a $(D-1)$-dimensional hyperplane.</p>

<h3 id="disc-funcs">Discriminant functions</h3>
<p>A discriminant is a function that takes an input vector $x$ and assigns it to one of $K$ class, denoted as $\mathcal{C}_k$</p>

<p>The simplest discriminant function is a linear function of the input vector
\begin{equation}
y(\mathbf{x})=\mathbf{w}^\text{T}\mathbf{x}+w_0,
\end{equation}
where $\mathbf{w}$ is called the <strong>weight vector</strong>, and $w_0$ is the <strong>bias</strong>.</p>

<p>In the case of binary classification, an input $\mathbf{x}$ is assigned to class $\mathcal{C}_1$ if $y(\mathbf{x})\geq 0$ and otherwise $y(\mathbf{x})\lt 0$, it belongs to class $\mathcal{C}_2$, thus the decision boundary is defined by
\begin{equation}
y(\mathbf{x})=0,
\end{equation}
which corresponds to a $(D-1)$-dimensional hyperplane with an $D$-dimensional input space.</p>

<p>Consider $\mathbf{x}_A$ and $\mathbf{x}_B$ lying on the hyperplane, thus $y(\mathbf{x}_A)=y(\mathbf{x}_B)=0$, which gives us that
\begin{equation}
0=y(\mathbf{x}_A)-y(\mathbf{x}_B)=\mathbf{w}^\text{T}\mathbf{x}_A-\mathbf{w}^\text{T}\mathbf{x}_B=\mathbf{w}^\text{T}(\mathbf{x}_A-\mathbf{x}_B)
\end{equation}
This claims that $\mathbf{w}$ is perpendicular to any vector within the decision boundary, and thus $\mathbf{w}$ is a normal vector of the decision boundary itself.</p>

<p>Hence, projecting a point $\mathbf{x}_0$ into the hyperplane, we have that the distant of $\mathbf{x}_0$ to the hyperplane is given by
\begin{equation}
\text{dist}(\mathbf{x}_0,y(\mathbf{x}))=\frac{y(\mathbf{x}_0)}{\Vert\mathbf{w}\Vert},
\end{equation}
which implies that
\begin{equation}
\text{dist}(\mathbf{0},y(\mathbf{x}))=\frac{w_0}{\Vert\mathbf{w}\Vert}
\end{equation}
To generalize the binary classification problem into multiple-class ones, we consider a $K$-class discriminant comprising $K$ linear functions of the form
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}
\end{equation}
Then for each input $\mathbf{x}$, it will be assigned to class $\mathcal{C}_k$ if $y_k(\mathbf{x})&gt;y_i(\mathbf{x}),\forall i\neq k$, or in other words $\mathbf{x}$ is assigned to a class $C_k$ that
\begin{equation}
k=\underset{i=1,\ldots,K}{\text{argmax}}\,y_i(\mathbf{x})
\end{equation}
The boundary between two class $\mathcal{C}_i$ and $\mathcal{C}_j$ is therefore given by
\begin{equation}
y_i(\mathbf{x})=y_j(\mathbf{x}),
\end{equation}
or
\begin{equation}
(\mathbf{w}_i-\mathbf{w}_j)^\text{T}\mathbf{x}+w_{i,0}-w_{j,0}=0,
\end{equation}
which is an $(D-1)$-dimensional hyperplane.</p>

<h4 id="least-squares-clf">Least squares</h4>
<p>Recall that in the regression task, we used least squares to find the models in form of linear functions of the parameters. We can also apply least squares approach to classification problems.</p>

<p>To begin, we have that for $k=1,\ldots,K$, each class $\mathcal{C}_k$ is represented the model
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}\label{eq:lsc.1}
\end{equation}
By giving the bias parameter $w_{k,0}$ a dummy input variable $x_0=0$, we can rewrite \eqref{eq:lsc.1} in a more convenient form
\begin{equation}
y_k(\mathbf{x})=\widetilde{\mathbf{w}}_k^\text{T}\widetilde{\mathbf{x}},
\end{equation}
where
\begin{equation}
\widetilde{\mathbf{w}}_k=\left(w_{k,0},\mathbf{w}_k^\text{T}\right)^\text{T};\hspace{1cm}\widetilde{\mathbf{x}}=\left(1,\mathbf{x}^\text{T}\right)^\text{T}
\end{equation}
Thus, we can vectorize the $K$ linear models into
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}},\label{eq:lsc.2}
\end{equation}
where $\widetilde{\mathbf{W}}$ is the parameter matrix whose $k$-th column is the $(D+1)$-dimensional vector $\widetilde{\mathbf{w}}_k$
\begin{equation}
\widetilde{\mathbf{W}}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \widetilde{\mathbf{w}}_1&amp;\ldots&amp;\widetilde{\mathbf{w}}_K \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
Consider a training set $\{\mathbf{x}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$, analogy to the parameter matrix $\widetilde{\mathbf{W}}$, we can vectorize those input variables and target values into
\begin{equation}
\widetilde{\mathbf{X}}=\left[\begin{matrix}-\hspace{0.15cm}\widetilde{\mathbf{x}}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\widetilde{\mathbf{x}}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
and
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
With these definition, the sum-of-squares error function then can be written as
\begin{equation}
E_D(\widetilde{\mathbf{W}})=\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big]
\end{equation}
Taking the derivative of $E_D(\widetilde{\mathbf{W}})$ w.r.t $\widetilde{\mathbf{W}}$, we obtain
\begin{align}
\nabla_\widetilde{\mathbf{W}}E_D(\widetilde{\mathbf{W}})&amp;=\nabla_\widetilde{\mathbf{W}}\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big] \\ &amp;=\frac{1}{2}\nabla_\widetilde{\mathbf{W}}\text{Tr}\Big[\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\mathbf{T}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}+\mathbf{T}^\text{T}\mathbf{T}\Big] \\ &amp;=\frac{1}{2}\Big[2\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}\Big] \\ &amp;=\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}
\end{align}
Setting this derivative equal to zero, we obtain the least squares solution for $\widetilde{\mathbf{W}}$ as
\begin{equation}
\widetilde{\mathbf{W}}=(\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}})^{-1}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}=\widetilde{\mathbf{X}}^\dagger\mathbf{T}
\end{equation}
Therefore, the discriminant function \eqref{eq:lsc.2} can be rewritten as
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}}=\mathbf{T}^\text{T}\big(\widetilde{\mathbf{X}}^\dagger\big)^\text{T}\widetilde{\mathbf{x}}
\end{equation}</p>

<h4 id="fisher-lin-disc">Fisher’s linear discriminant</h4>
<p>One way to view a linear classification model is in terms of dimensional reduction. In particular, given an $D$-dimensional input $\mathbf{x}$, we project it down to one dimension using
\begin{equation}
y=\mathbf{w}^\text{T}\mathbf{x}
\end{equation}</p>

<h5 id="fisher-ld-bin-clf">Binary classification</h5>
<p>Consider a binary classification in which there are $N_1$ points of class $\mathcal{C}_1$ and $N_2$ points of class $\mathcal{C}_2$, thus the mean vectors of those two classes are given by
\begin{align}
\mathbf{m}_1&amp;=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}\mathbf{x}_n, \\ \mathbf{m}_2&amp;=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}\mathbf{x}_n
\end{align}
The simplest measure of the separation of the classes, when projected onto $\mathbf{w}$, is the separation of the projected class means, which suggests us choosing $\mathbf{w}$ in order to maximize
\begin{equation}
m_2-m_1=\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1),
\end{equation}
where for $k=1,\ldots,K$
\begin{equation}
m_k=\mathbf{w}^\text{T}\mathbf{m}_k
\end{equation}
is the mean of the projected data from class $\mathcal{C}_k$.</p>

<p>To make the computation simpler, we normalize the projection simply by making a constraint of $\mathbf{w}$ being a unit vector, which means
\begin{equation}
\big\Vert\mathbf{w}\big\Vert_2=\sum_{i}w_i=1
\end{equation}
Therefore, by Lagrange multiplier, in order to maximize $m_2-m_1$, we have that
\begin{equation}
\mathbf{w}\propto(\mathbf{m}_2-\mathbf{m}_1)
\end{equation}
To solve this problem, we use the Fisher’s LD approach to minimize the class overlap by maximizing the ratio of the <strong>between-class variance</strong> to the <strong>within-class variance</strong>.</p>

<p>The within-class variance of projected data from class $\mathbf{w}_k$ is defined as
\begin{equation}
s_k^2\doteq\sum_{n\in\mathcal{C}_k}(y_n-m_k)^2,
\end{equation}
where $y_n=\mathbf{w}^\text{T}\mathbf{x}_n$ is the projected of $\mathbf{x}_n$. Thus the total within-class variance for the whole data set is $s_1^2+s_2^2$.</p>

<p>The between-class variance is simply defined to be the squared of the difference of means, given as
\begin{equation}
(m_2-m_1)^2
\end{equation}
Hence, the ratio of the between-class variance to the within-class variance, called the <strong>Fisher criterion</strong>, can be defined as
\begin{align}
J(\mathbf{w})&amp;=\frac{(m_2-m_1)^2}{s_1^2+s_2^2} \\ &amp;=\frac{\big\Vert\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1)\big\Vert_2^2}{\sum_{n\in\mathcal{C}_1}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_1)\big\Vert_2^2+\sum_{n\in\mathcal{C}_2}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_2)\big\Vert_2^2} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}},\label{eq:flbc.1}
\end{align}
where
\begin{equation}
\mathbf{S}_\text{B}\doteq(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T},
\end{equation}
is called the <strong>between-class covariance matrix</strong> and
\begin{equation}
\mathbf{S}_\text{W}\doteq\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\mathbf{m}_1)(\mathbf{x}_n-\mathbf{m}_1)^\text{T}+\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\mathbf{m}_2)(\mathbf{x}_n-\mathbf{m}_2)^\text{T},
\end{equation}
is called the <strong>total within-class covariance matrix</strong>.</p>

<p>As usual, taking the gradient of \eqref{eq:flbc.1} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}J(\mathbf{w})&amp;=\nabla_\mathbf{w}\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}(\mathbf{S}_\text{B}+\mathbf{S}_\text{B}^\text{T})\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}(\mathbf{S}_\text{W}+\mathbf{S}_\text{W}^\text{T})\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;\propto\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{align}
Setting the gradient equal to zero and solving for $\mathbf{w}$, we obtain that $\mathbf{w}$ satisfies
\begin{equation}
\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}=\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{equation}
Since $\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}$ and $\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}$ are two scalars, we then have
\begin{equation}
\mathbf{S}_\text{W}\mathbf{w}\propto\mathbf{S}_\text{B}\mathbf{w}
\end{equation}
Multiply both side by $\mathbf{S}_\text{W}^{-1}$, we obtain
\begin{align}
\mathbf{w}&amp;\propto\mathbf{S}_\text{W}^{-1}\mathbf{S}_\text{B}\mathbf{w} \\ &amp;=\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w} \\ &amp;\propto\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1),\label{eq:flbc.2}
\end{align}
since $(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w}$ is a scalar.</p>

<p>If the within-class covariance matrix $\mathbf{S}_\text{W}$ is isotropic<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, we then have
\begin{equation}
\mathbf{w}\propto\mathbf{m}_2-\mathbf{m}_1
\end{equation}
The result \eqref{eq:flbc.2} is called <strong>Fisher’s linear discriminant</strong>.</p>

<p>With this $\mathbf{w}$, we can project our data down into one dimension and from projected data, we construct a discriminant by selecting a threshold $y_0$ such that $\mathbf{x}$ belongs to class $\mathcal{C}_1$ if $y(\mathbf{x})\gg y_0$ and otherwise it belongs to $\mathcal{C}_1$.</p>

<h5 id="fisher-ld-clf">Multi-class classification</h5>
<p>To generalize the Fisher discriminant to the case of $K&gt;2$, we first assume that $D&gt;K$ and consider the $D’&gt;1$ linear features
\begin{equation}
y=\mathbf{w}_k^\text{T}\mathbf{x},
\end{equation}
where $k=1,\ldots,D’$. Thus, as usual we can vectorize these feature values as
\begin{equation}
\mathbf{y}=\mathbf{W}^\text{T}\mathbf{x},\label{eq:flc.1}
\end{equation}
where
\begin{equation}
\mathbf{y}=(y_1,\ldots,y_k)^\text{T},\hspace{2cm}\mathbf{W}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{w}_1&amp;\ldots&amp;\mathbf{w}_{D’} \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
The mean vector for each class is unchanged, which is given as
\begin{equation}
\mathbf{m}_k=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n,
\end{equation}
where $N_k$ is the number of points in class $\mathcal{C}_k$  for $k=1,\ldots,K$.</p>

<p>The within-class variance covariance matrix $\mathbf{S}_\text{W}$ now can be simply generalized as
\begin{equation}
\mathbf{S}_\text{W}=\sum_{k=1}^{K}\mathbf{S}_k,\label{eq:flc.2}
\end{equation}
where
\begin{equation}
\mathbf{S}_k=\sum_{n\in\mathcal{C}_k}(\mathbf{x}_n-\mathbf{m}_k)(\mathbf{x}_n-\mathbf{m}_k)^\text{T}
\end{equation}
To find the generalization of the between-class covariance matrix $\mathbf{S}_\text{B}$, we first consider the total covariance matrix
\begin{equation}
\mathbf{S}_T=\sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{m})(\mathbf{x}_n-\mathbf{m})^\text{T},
\end{equation}
where
\begin{equation}
\mathbf{m}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n=\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k
\end{equation}
is the mean of the whole data set, where $N=\sum_{k=1}^{K}N_k$ is the number of the data points. The total covariance matrix can be decomposed into the sum of the within-class covariance matrix $\mathbf{S}_\text{W}$, as given in \eqref{eq:flc.2} with a matrix $\mathbf{S}_\text{B}$, defined as a measure of the between-class covariance
\begin{equation}
\mathbf{S}_\text{T}=\mathbf{S}_\text{W}+\mathbf{S}_\text{B},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{B}=\sum_{k=1}^{K}N_k(\mathbf{m}_k-\mathbf{m})(\mathbf{m}_k-\mathbf{m})^\text{T}
\end{equation}
Using \eqref{eq:flc.1}, we project the whole data set into the $D’$-dimensional space of $\mathbf{y}$, the corresponding within-class covariance matrix of the transformed data are given as
\begin{align}
\mathbf{s}_\text{W}&amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)^\text{T} \\ &amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}(\mathbf{y}_n-\boldsymbol{\mu}_k)(\mathbf{y}_n-\boldsymbol{\mu}_k)^\text{T} \\ &amp;=\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T}
\end{align}
and also the transformed between-class covariance matrix
\begin{align}
\mathbf{s}_\text{B}&amp;=\sum_{k=1}^{K}N_k(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})^\text{T} \\ &amp;=\sum_{k=1}^{K}(\boldsymbol{\mu}_k-\boldsymbol{\mu})(\boldsymbol{\mu}_k-\boldsymbol{\mu})^\text{T} \\ &amp;=\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T},
\end{align}
where
\begin{align}
\boldsymbol{\mu}_k&amp;=\mathbf{W}^\text{T}\mathbf{m}_k=\mathbf{W}^\text{T}\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{y}_n, \\ \boldsymbol{\mu}&amp;=\mathbf{W}^\text{T}\mathbf{m}=\mathbf{W}^\text{T}\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{\mu}_k
\end{align}
Analogy to the case of binary classification with Fisher’s criterion \eqref{eq:flbc.1}, we need a new measure that is large when the between-class covariance is large and when the within-class covariance is small. A simple choice of criterion is given as
\begin{equation}
J(\mathbf{W})=\text{Tr}\left(\mathbf{s}_\text{W}^{-1}\mathbf{s}_\text{B}\right)
\end{equation}
or
\begin{equation}
J(\mathbf{w})=\text{Tr}\big[(\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T})^{-1}(\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T})\big]
\end{equation}
for which the linear basis model follow the same rule as the above</p>

<h4 id="perceptron">The perceptron algorithm</h4>
<p>Another example of linear discriminant model is the perceptron algorithm</p>

<h3 id="prob-gen-models">Probabilistic Generative Models</h3>
<p>When solving the classification problems, we divide the strategy into two stage</p>
<ul id="number-list">
	<li>
		<b>Inference stage</b>. In this stage we use training data to learn a model for $p(\mathcal{C}_k|\mathbf{x})$ 
	</li>
	<li>
		<b>Decision stage</b>. In this stage we use those posterior probabilities to make optimal class assignments.
	</li>
</ul>
<p>We can solve both inference and decision problems at the same time by learning a function, which is the discriminant function, maps inputs $\mathbf{x}$ directly into decisions.</p>

<p>When using the generative approach to solve the problem of classification, we first model the class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ and the class priors $p(\mathcal{C}_k)$ then apply Bayes’ theorem to compute the posterior probabilities $p(\mathcal{C}_k\vert\mathbf{x})$.</p>

<p>Consider the binary case, in which specifically the posterior probability for class $\mathcal{C}_1$ can be computed as
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)+p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)} \\ &amp;=\frac{1}{1+\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}} \\ &amp;=\frac{1}{1+\exp(-a)}=\sigma(a)\label{eq:pgm.1}
\end{align}
where
\begin{equation}
a=\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}
\end{equation}
and where $\sigma(\cdot)$ is the <span id="logistic-sigmoid-func">logistic sigmoid function</span>, defined before as
\begin{equation}
\sigma(a)\doteq\frac{1}{1+\exp(-a)}
\end{equation}
For the case of multi-class, $K&gt;2$, the posterior probability for class $\mathcal{C}_k$ can be generalized as
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{i=1}^{K}p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)}=\dfrac{\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big)\Big]}{\sum_{i=1}^{K}\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)\big)\Big]} \\ &amp;=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)}=\sigma(\mathbf{a})_k\label{eq:pgm.2}
\end{align}
where
\begin{align}
a_k&amp;=\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big), \\ \mathbf{a}&amp;=(a_1,\ldots,a_K)^\text{T},
\end{align}
and the function $\sigma:\mathbb{R}^K\to(0,1)^K$, known as the <strong>normalized exponential</strong> or <strong>softmax function</strong> - a generalization of sigmoid into multi-dimensional, in which the $k$-th element is defined as
\begin{equation}
\sigma(\mathbf{a})_k\doteq\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
for $k=1,\ldots,K$ and $\mathbf{a}=(a_1,\ldots,a_K)^\text{T}$.</p>

<h4 id="gauss-gen-models">Gaussian Generative models</h4>
<p>If the class-conditional probabilities are Gaussian, or specifically Multivariate Normal and share the same covariance matrix $\boldsymbol{\Sigma}$, then for $k=1,\ldots,K$,
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Thus, the density for class $\mathcal{C}_k$ is defined as
\begin{equation}
p(\mathbf{x}\vert\mathcal{C}_k)=\frac{1}{(2\pi)^{D/2}\big\vert\boldsymbol{\Sigma}\big\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)\right]
\end{equation}
In the binary case, in which the densities above become Bivariate Normal, by \eqref{eq:pgm.1} we have that
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;=\sigma\left(\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}\right) \\ &amp;=\sigma\left(\log\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_2)\Big]p(\mathcal{C}_2)}{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_1)\Big]p(\mathcal{C}_1)}\right) \\ &amp;=\sigma\Bigg(-\frac{1}{2}\Big[-\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2-\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\nonumber \\ &amp;\hspace{2cm}+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\Big]-\log\frac{p(\mathcal{C}_2)}{p(\mathcal{C}_1)}\Bigg) \\ &amp;=\sigma\left(\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2\right)^\text{T}\mathbf{x}-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\label{eq:ggm.1}
\end{align}
Let
\begin{align}
\mathbf{w}&amp;=\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2), \\ w_0&amp;=-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)},
\end{align}
we have \eqref{eq:ggm.1} can be rewritten in more convenient form as
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
From the derivation, we see that by making an assumption of having the same covariance matrix $\boldsymbol{\Sigma}$ across the densities helped us remove out the quadratic terms of $\mathbf{x}$, which leads us to ending up with a logistic sigmoid of a linear function of $\mathbf{x}$.</p>

<p>For the multi-dimensional case, $K&gt;2$, by \eqref{eq:pgm.2}, we have that the density for class $\mathcal{C}_k$ is
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;=\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)+\log p(\mathcal{C}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_i)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_i)+\log p(\mathcal{C}_i)\Big]} \\ &amp;=\frac{\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_k+\log p(\mathbf{w}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_i-\frac{1}{2}\boldsymbol{\mu}_i^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_i+\log p(\mathbf{w}_i)\Big]}
\end{align}
Or in other words, we can simplify each element of $\mathbf{a}$ into a linear function as
\begin{equation}
a_k\doteq a_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0},
\end{equation}
where
\begin{align}
\mathbf{w}_k&amp;=\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k, \\ w_{k,0}&amp;=-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k+\log p(\mathcal{C}_k)
\end{align}
The simplification we can make also come from the assumption of sharing the same covariance matrix between densities, which is analogous to the binary case that cancelled out the quadratic terms.</p>

<h5 id="max-likelihood-sols">Maximum likelihood solutions</h5>
<p>Once we have specified a parametric functional form of $p(\mathbf{x}\vert\mathcal{C}_k)$, using maximum likelihood, we can solve for the values of the parameters and also the prior probabilities $p(\mathcal{C}_k)$.</p>

<h6 id="ggm-bin-clf">Binary classification</h6>
<p>In particular, first off for the binary case, in which each class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ is a Bivariate Normal, with a shared covariance matrix, as
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Consider the data set $\{\mathbf{x}_n,t_n\}$ for $n=1,\ldots,N$, i.e., $t_n=1$ denotes class $\mathcal{C}_1$ and $t_n=0$ denotes class $\mathcal{C}_2$. Let the class prior probability $p(\mathcal{C}_1)=\pi$, thus $p(\mathcal{C}_2)=1-\pi$. Or
\begin{align}
p(\mathbf{x}_n,\mathcal{C}_1)&amp;=p(\mathcal{C}_1)p(\mathbf{x}_n\vert\mathcal{C}_1)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}), \\ p(\mathbf{x}_n,\mathcal{C}_2)&amp;=p(\mathcal{C}_2)p(\mathbf{x}_n\vert\mathcal{C}_2)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})
\end{align}
We also have that
\begin{equation}
p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n}
\end{equation}
Therefore, the likelihood can be defined as
\begin{align}
L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})&amp;=p(\mathbf{t}\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\prod_{n=1}^{N}p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n} \\ &amp;=\prod_{n=1}^{N}\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]^{t_n}\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]^{1-t_n},
\end{align}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$. As usual, we continue to consider the log likelihood $\ell(\cdot)$
\begin{align}
&amp;\hspace{0.7cm}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\log L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]\label{eq:gbc.1}
\end{align}
Taking the gradient of the log likelihood w.r.t $\pi$ we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_\pi\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\pi+(1-t_n)\log(1-\pi) \\ &amp;=\sum_{n=1}^{N}\left[\frac{t_n}{\pi}-\frac{1-t_n}{1-\pi}\right]
\end{align}
Setting the derivative to zero and solve for $\pi$ as usual, we have
\begin{equation}
\sum_{n=1}^{N}t_n-\pi=0
\end{equation}
Thus, we obtain the solution
\begin{equation}
\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2},
\end{equation}
where $N_1,N_2$ denote the total number of data points in class $\mathcal{C}_1$ and $\mathcal{C}_2$ respectively.</p>

<p>On the other hand, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\mu}_1$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_{\boldsymbol{\mu}_1}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right] \\ &amp;\propto\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\big(-\boldsymbol{x}_n^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}_n+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\big) \\ &amp;=\sum_{n=1}^{N}t_n\Big[\big(\boldsymbol{\Sigma}^{-1}+(\boldsymbol{\Sigma}^{-1})^\text{T}\big)\big(\boldsymbol{\mu}_1-\mathbf{x}_n\big)\Big] \\ &amp;\propto\sum_{n=1}^{N}t_n(\boldsymbol{\mu}_1-\mathbf{x}_n)
\end{align}
Setting the above gradient to zero and solve for $\boldsymbol{\mu}_1$, we obtain the solution
\begin{equation}
\boldsymbol{\mu}_1=\frac{1}{N_1}\sum_{n=1}^{N}t_n\mathbf{x}_n,
\end{equation}
which is simply the mean of all input vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_1$.</p>

<p>Similarly, with the same procedure, we have that the maximum likelihood solution for $\boldsymbol{\mu}_2$ is the mean of all inputs vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_2$, as
\begin{equation}
\boldsymbol{\mu}_2=\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\mathbf{x}_n
\end{equation}
Lastly, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\Sigma}$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})+(1-t_n)\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right]\nonumber \\ &amp;\hspace{2cm}+(1-t_n)\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2)\right] \\ &amp;\propto\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\log\big\vert\boldsymbol{\Sigma}\big\vert+t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\nonumber \\ &amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2) \\ &amp;=N\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\nonumber \\ &amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gbc.2}
\end{align}
The first term of the gradient can be computed as
\begin{align}
\frac{\partial\log\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\frac{\partial\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\text{adj}(\boldsymbol{\Sigma})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ij},
\end{align}
since $\boldsymbol{\Sigma}$ is symmetric and so is its inverse. This implies that
\begin{equation}
\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert=\boldsymbol{\Sigma}^{-1}\label{eq:gbc.3}
\end{equation}
Let $\mathbf{S}$ be a matrix defined as
\begin{equation}
\mathbf{S}=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}
\end{equation}
Therefore, the derivative \eqref{eq:gbc.2} can be rewritten as
\begin{equation}
\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=N\boldsymbol{\Sigma}^{-1}-N\boldsymbol{\Sigma}^{-1}\mathbf{S}\boldsymbol{\Sigma}^{-1}
\end{equation}
Setting this gradient to zero and solve for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{S},
\end{equation}
where $\mathbf{S}$ can be continued to derive as
\begin{align}
\mathbf{S}&amp;=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T} \\ &amp;=\frac{N_1}{N}\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+\frac{N_2}{N}\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T},
\end{align}
which is the weighted average of the covariance matrices corresponded to each of the two classes $\mathcal{C}_1,\mathcal{C}_2$.</p>

<h6 id="ggm-clf">Multi-class classification</h6>
<p>To generalize the Gaussian generative binary classification, we consider a model for $K&gt;2$ classes defined by prior class probabilities $p(\mathcal{C}_k)=\pi_k$ and Multivariate Normal class-conditional densities with shared covariance matrix, given as
\begin{equation}
p({\boldsymbol{\phi}}\vert\mathcal{C}_k)=\mathcal{N}(\boldsymbol{\phi}\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}),
\end{equation}
where $\boldsymbol{\phi}$ is the input feature vector.</p>

<p>Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Therefore, we have that
\begin{equation}
p(\boldsymbol{\phi}_n,\mathcal{C}_k)=p(\mathcal{C}_k)p(\boldsymbol{\phi}_n\vert\mathcal{C}_k)=\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Analogy to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma})=\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k}
\end{equation}
To simplify the notation, we let $\mathbf{w}$ denote
\begin{equation}
\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma}
\end{equation}
And let $\mathbf{T}$ be a matrix that associates those targets $\mathbf{t}_n$’s together, given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Thus, the likelihood is given as
\begin{align}
L(\mathbf{w})&amp;=p(\mathbf{T}\vert\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k} \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k}
\end{align}
And thus, the log likelihood $\ell(\cdot)$ can be computed as
\begin{align}
\ell(\mathbf{w})&amp;=\log L(\mathbf{w}) \\ &amp;=\log\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k} \\ &amp;=\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]\label{eq:gc.1}
\end{align}
As usual, we continue by using maximum likelihood, which begins by taking gradient of the log likelihood w.r.t to the parameters. However, when maximizing the likelihood w.r.t $\pi_k$, we have to compute subject to a constraint that
\begin{equation}
\sum_{k=1}^{K}\pi_k=1
\end{equation}
Therefore, using a Lagrange multiplier $\lambda$, we instead maximize the Lagrangian w.r.t $\pi_k$, which is
\begin{equation}
\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda)=\ell(\mathbf{w})+\lambda\left(\sum_{k=1}^{K}\pi_k-1\right)
\end{equation}
Differentiating $\mathcal{L}$ w.r.t $\pi_k$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_{\pi_k}\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda) \\ &amp;=\nabla_{\pi_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big]+\nabla_{\pi_k}\lambda\left(\sum_{i=1}^{K}\pi_i-1\right) \\ &amp;=\lambda+\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\pi_k}\log\pi_k \\ &amp;=\lambda+\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\pi_k}
\end{align}
Setting the derivative equal to zero and solve for $\pi_k$, we have
\begin{equation}
\pi_k=-\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\lambda}=\frac{N_k}{\lambda},
\end{equation}
where $N_k$ denotes the number of data points in class $\mathcal{C}_k$. Moreover, since $\sum_{k=1}^{K}\pi_k=1$, we have
\begin{equation}
1=-\sum_{k=1}^{K}\frac{N_k}{\lambda}=\frac{-N}{\lambda},
\end{equation}
which implies that
\begin{equation}
\lambda=-N
\end{equation}
Hence, the maximum likelihood solution for $\pi_k$ is
\begin{equation}
\pi_k=-\frac{N_k}{\lambda}=\frac{N_k}{N}
\end{equation}
We continue by taking the gradient of the log likelihood \eqref{eq:gc.1} w.r.t $\boldsymbol{\mu}_k$, as
\begin{align}
\nabla_{\boldsymbol{\mu}_k}\ell(\mathbf{w})&amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;=-\frac{1}{2}\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\boldsymbol{\mu}_k}\Big[\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-2\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big] \\ &amp;=\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big]
\end{align}
Setting the above gradient equal to zero and solve for $\boldsymbol{\mu}_k$ we obtain the solution
\begin{equation}
\boldsymbol{\mu}_k=\frac{1}{\sum_{n=1}^{N}(\mathbf{t}_n)_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n,
\end{equation}
which is the mean of feature vectors assigned to class $\mathcal{C}_k$.</p>

<p>Finally, consider the gradient of \eqref{eq:gc.1} w.r.t $\boldsymbol{\Sigma}$, combined with the result \eqref{eq:gbc.3} we have
\begin{align}
\nabla_\boldsymbol{\Sigma}\ell(\mathbf{w})&amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;=-\frac{N}{2}\boldsymbol{\Sigma}^{-1}+\frac{1}{2}\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1} \\ &amp;\propto N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gc.2}
\end{align}
Let $\mathbf{S}_k$ be the covariance of the data associated with class $\mathcal{C}_k$, defined as
\begin{equation}
\mathbf{S}_k=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}
\end{equation}
Therefore, letting the derivative \eqref{eq:gc.2} equal to zero, we have
\begin{equation}
N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{k=1}^{K}N_k\mathbf{S}_k\Big]\boldsymbol{\Sigma}^{-1}=0
\end{equation}
Solving this equation for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\sum_{k=1}^{K}\frac{N_k}{N}\mathbf{S}_k
\end{equation}</p>

<h3 id="prob-disc-models">Probabilistic Discriminative Models</h3>

<h4 id="log-reg">Logistic Regression</h4>
<p>Recall that in the previous section of generative approach, in particular for the binary case we knew that the posterior probability for class $\mathcal{C}_1$ can be defined as the logistic sigmoid of a linear function of the input vector $\mathbf{x}$
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
In general, the posterior probabilities can be written as the logistic sigmoid of a linear function of instead feature vector $\boldsymbol{\phi}$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma\big(\mathbf{w}^\text{T}\boldsymbol{\phi}+w_0\big)
\end{equation}
This model is called <strong>logistic regression</strong>, although it is applied for classification tasks.
Consider a data set $\{\boldsymbol{\phi}_n,t_n\}$, where $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\mathbf{x}_n)$ and $t_n\in\{0,1\}$, with $n=1,\ldots,N$. Therefore,
\begin{equation}
p(t_n\vert\mathbf{w})=y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $y_n=p(\mathcal{C}_1\vert\boldsymbol{\phi}_n)$.</p>

<p>Comprise $t_n$’s into $\mathbf{t}\doteq(t_1,\ldots,t_N)^\text{T}$, then we have that the likelihood function can be defined as
\begin{equation}
L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}\tag{35}\label{eq:lr.1}
\end{equation}
Taking the negative logarithm of the likelihood gives us the <strong>cross-entropy</strong> error function, as
\begin{align}
E(\mathbf{w})=-\log L(\mathbf{w})&amp;=-\log\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \\ &amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n)\label{eq:lr.2}
\end{align}
Differentiating the error function $E(\mathbf{w})$ w.r.t $\mathbf{w}$ we have that
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;=\sum_{n=1}^{N}\frac{(1-t_n)\nabla_\mathbf{w}y_n}{1-y_n}-\frac{t_n\nabla_\mathbf{w}y_n}{y_n} \\ &amp;=\sum_{n=1}^{N}\frac{(1-t_n)y_n(1-y_n)\boldsymbol{\phi}_n}{1-y_n}-\frac{t_n y_n(1-y_n)\boldsymbol{\phi}_n}{y_n} \\ &amp;=\sum_{n=1}^{N}(1-t_n)y_n\boldsymbol{\phi}_n-t_n(1-y_n)\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n,\label{eq:lr.3}
\end{align}
where in the third step, we have used the identity of the <span id="sigmoid-derivative">derivative of the logistic sigmoid function</span>
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}
and the chain rule to compute the gradient of $y_n$ w.r.t $\mathbf{w}$ as
\begin{align}
\nabla_\mathbf{w}y_n&amp;=\nabla_\mathbf{w}\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;=\frac{d\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}{d(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}\nabla_\mathbf{w}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big(1-\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big)\boldsymbol{\phi}_n \\ &amp;=y_n(1-y_n)\boldsymbol{\phi}_n
\end{align}</p>

<h4 id="softmax-reg">Softmax Regression</h4>
<p>Analogy to the generalization of the binary case into logistic regression, for the multi-class case, the posterior probability for class $\mathcal{C}_k$ can be written as the softmax function of a linear function of feature vectors $\boldsymbol{\phi}$ as
\begin{equation}
p(\mathcal{C}_k\vert\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
where $a_k$’s is called the <strong>activations</strong>, defined as
\begin{equation}
a_k=\mathbf{w}_k^\text{T}\boldsymbol{\phi}
\end{equation}
Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Similar to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K)=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\boldsymbol{\phi}_n)^{(\mathbf{t}_n)_k}=\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k},
\end{equation}
where $(y_{n})_k=y_k(\boldsymbol{\phi}_n)$.</p>

<p>Let $\mathbf{T}$ be a $N\times K$ matrix comprising $\mathbf{t}_n$’s together as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the likelihood function can be written by
\begin{align}
L(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=p(\mathbf{T}\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k}
\end{align}
We also obtain the cross-entropy error function by taking the negative logarithm of the likelihood, as
\begin{align}
E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=-\log L(\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log(y_{n})_k\label{eq:sr.1}
\end{align}
As usual, taking the gradient of the error function $E(\mathbf{w}_1,\ldots,\mathbf{w}_K)$ w.r.t $\mathbf{w}_k$ we have
\begin{align}
\nabla_{\mathbf{w}_k}E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=\nabla_{\mathbf{w}_k}-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\log(y_{n})_i \\ &amp;=-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\frac{(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n}{(y_n)_i} \\ &amp;=\sum_{n=1}^{N}\Big[(y_n)_k\sum_{i=1}^{K}(\mathbf{t}_n)_i-\sum_{i=1}^{K}(\mathbf{t}_n)_i 1\{i=k\}\Big]\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n\label{eq:sr.2}
\end{align}
where in the second step, we have used the <span id="softmax-derivative">identity</span>
\begin{align}
\frac{\partial y_k}{\partial a_j}&amp;=\frac{\big(\partial\exp(a_k)/\partial\exp(a_j)\big)\sum_{i=1}^{K}\exp(a_i)-\exp(a_j)\exp(a_k)}{\big(\sum_{i=1}^{K}\exp(a_i)\big)^2} \\ &amp;=\frac{\exp(a_k)1\{k=j\}}{\sum_{i=1}^{K}\exp(a_i)}-y_k y_j \\ &amp;=y_k(1\{k=j\}-y_j)
\end{align}
where $1\{k=j\}$ is the indicator function, which returns $1$ if $k=j$ and returns $0$ otherwise. Hence, by chain rule, we obtain the gradient of $(y_n)_i$ w.r.t $\mathbf{w}_k$ given by
\begin{align}
\nabla_{\mathbf{w}_k}(y_n)_i&amp;=\frac{\partial(y_n)_i}{\partial a_k}\frac{\partial a_k(\mathbf{w}_k,\boldsymbol{\phi}_n)}{\partial\mathbf{w}_k} \\ &amp;=(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n
\end{align}</p>

<h4 id="newtons-method">Newton’s method</h4>
<figure>
	<img src="/assets/images/2022-08-13/newtons-method.gif" alt="Newton's method)" style="display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 5</b>: Illustration of the Newton's method. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/classification/newtons-method.py">here</a></span></figcaption>
</figure>

<p>\begin{equation}
\mathbf{w}^{(\text{new})}=\mathbf{w}^{(\text{old})}-\mathbf{H}^{-1}\nabla_\mathbf{w}E(\mathbf{w})
\end{equation}</p>

<h5 id="nm-lin-reg">Linear Regression</h5>
<p>Consider applying the Newton’s method to the sum-of-squares error function \eqref{eq:lsr.4} for the linear regression model \eqref{eq:lbfm.2}. The gradient and Hessian of this error function are
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}\left(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}_n)\right)^2 \\ &amp;=\sum_{n=1}^{N}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
and
\begin{equation}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})=\nabla_\mathbf{w}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big)=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi},
\end{equation}
where $\boldsymbol{\Phi}$, as defined before, is the $N\times M$ design matrix
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;\ddots&amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
Hence, we have that the Newton’s update of the model is given by
\begin{align}
\mathbf{w}^{(\text{new})}&amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big) \\ &amp;=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
which is exactly the standard least-squares solution.</p>

<h5 id="nm-log-reg">Logistic Regression</h5>
<p>Consider using the Newton’s method to the logistic regression model with the cross-entropy error function \eqref{eq:lr.2}. By the result \eqref{eq:lr.3}, we have the gradient and Hessian of this error function are given as
\begin{equation}
\nabla_\mathbf{w}E(\mathbf{w})=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t})
\end{equation}
and
\begin{align}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\label{eq:nlr.1} \\ &amp;=\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi},
\end{align}
where $\mathbf{R}$ is the $N\times N$ diagonal matrix with diagonal elements
\begin{equation}
\mathbf{R}_{n n}=y_n(1-y_n)
\end{equation}
It is noticeable that hessian matrix $\mathbf{H}$ is positive definite because for any vector $\mathbf{v}$
\begin{equation}
\mathbf{v}^\text{T}\mathbf{H}\mathbf{v}=\mathbf{v}^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{v}&gt;0,
\end{equation}
since $\mathbf{R}$ is positive definite due to $y_n\in(0,1)$ letting all the diagonal elements of $\mathbf{R}$ are positive. This positive definiteness claims that the cross-entropy error function is a concave function of $\mathbf{w}$ and thus has a unique minimum.</p>

<p>Back to our main attention, the Newton’s update of the model then takes the form
\begin{align}
\mathbf{w}^{(\text{new})}&amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t}) \\ &amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\Big[\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}(\mathbf{y}-\mathbf{t})\Big] \\ &amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
where $\mathbf{z}$ is an $N$-dimensional vector given by
\begin{equation}
\mathbf{z}=\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\mathbf{R}^{-1}(\mathbf{y}-\mathbf{t})
\end{equation}
This algorithm is known as <strong>iterative reweighted least squares</strong>, or <strong>IRLS</strong>.</p>

<h5 id="nm-softmax-reg">Softmax Regression</h5>
<p>Consider applying the Newton’s method to the cross-entropy error function \eqref{eq:sr.1} for the softmax regression model.</p>

<p>First, let $\mathbf{W}$ be the $M\times K$ matrix that comprises $\mathbf{w}_1,\ldots,\mathbf{w}_K$ together, as
\begin{equation}
\mathbf{W}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{w}_1&amp;\ldots&amp;\mathbf{w}_K \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
By the result \eqref{eq:sr.2}, we have that the $k$-th column of the gradient of this error function is given by
\begin{equation}
\nabla_{\mathbf{w}_k}E(\mathbf{W})=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}_k-\mathbf{T}_k),
\end{equation}
where $\boldsymbol{\Phi}$ be the $N\times M$ design matrix, given as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}_1^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right]
\end{equation}
and where $\mathbf{Y}_k,\mathbf{T}_k$ are the $k$th columns of the $N\times K$ matrices
\begin{equation}
\mathbf{Y}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{y}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{y}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right];\hspace{2cm}\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the gradient of the error function w.r.t $\mathbf{W}$ can be written as
\begin{equation}
\nabla_\mathbf{W}E(\mathbf{W})=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}-\mathbf{T})
\end{equation}
Now we consider the hessian matrix $\mathbf{H}$ of the error function, whose block $(k,j)$ is given by
\begin{align}
\mathbf{H}_{k j}&amp;=\nabla_{\mathbf{w}_j}\nabla_{\mathbf{w}_k} E(\mathbf{W}) \\ &amp;=\nabla_{\mathbf{w}_j}\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}
\end{align}
Analogous to the binary case, the hessian $\mathbf{H}$ for the multi-class logistic regression model is positive semi-definite. To prove it, since $\mathbf{H}$ is an $MK\times MK$ matrix, consider an $MK$-dimensional vector $\mathbf{u}$. Thus, $\mathbf{u}$ can be represented as
\begin{equation}
\mathbf{u}=\left[\begin{matrix}\mathbf{u}_1^\text{T}&amp;\ldots&amp;\mathbf{u}_K^\text{T}\end{matrix}\right]^\text{T},
\end{equation}
where each $\mathbf{u}_k$ is a vector of length $M$, for $k=1,\ldots,K$. Therefore, we have
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\mathbf{H}_{k j}\mathbf{u}_j \\ &amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}\sum_{j=1}^{K}(y_n)_k(y_n)_j\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j\right] \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right]\label{eq:nsr.1}
\end{align}
Consider $f:\mathbb{R}^M\to\mathbb{R}$, defined as
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}
\end{equation}
Thus, it follows immediately from the definition of $f$ that $f$ is convex since
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}=\Vert\mathbf{x}^\text{T}\boldsymbol{\phi}_n\Vert_2^2\geq 0
\end{equation}
Let us apply <strong>Jensen’s inequality</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> for $f$ with observing that $\sum_{k=1}^{K}(y_n)_k=\sum_{j=1}^{K}(y_n)_j=1$, then \eqref{eq:nsr.1} can be continued to derive as
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right] \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k f\left(\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;\geq\sum_{n=1}^{N}\left[f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;=0,
\end{align}
which claims the positive semi-definiteness of $\mathbf{H}$. Therefore, the error function $E(\mathbf{w})$ is concave and thus has a unique minimum.</p>

<h3 id="bayes-log-reg">Bayesian Logistic Regression</h3>
<p>When using Bayesian approach for logistic regression model, unlike the case of linear regression \eqref{eq:pd.1}, the posterior distribution now is no longer Gaussian. This makes the evaluation of posterior be intractable when integrating over the parameter $\mathbf{w}$.</p>

<p>Therefore, it is necessary to use some approximation methods.</p>

<h4 id="laplace-approx">The Laplace approximation</h4>
<p>The goal of <strong>Laplace approximation</strong> is to fit a Gaussian distribution to a probability density defined over a set of continuous variables</p>

<p>We begin by consider applying Laplace method to one-dimensional variables $z$ with the density function $p(z)$ is defined as
\begin{equation}
p(z)=\frac{1}{Z}f(z),
\end{equation}
where $Z=\int f(z)\,dz$ is the normalization coefficient, and is unknown.</p>

<p>The idea behind Laplace method is to place a Gaussian $q(z)$ on a mode of the distribution $p(z)$. A mode $z_0$ of $p(z)$ is where the distribution reaches its global maximum, which also means the derivative of $p(z)$ at $z_0$ is zero
\begin{equation}
\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}=0
\end{equation}
Therefore, the Taylor expansion of $\log f(z)$ about $z=z_0$ can be written by
\begin{align}
\log f(z)&amp;\simeq\log f(z_0)+\log f(z)\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}(z-z_0)+\frac{1}{2}\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}(z-z_0)^2 \\ &amp;=\log f(z_0)-\frac{A}{2}(z-z_0)^2,
\end{align}
where
\begin{equation}
A=-\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}
\end{equation}
Thus, taking the exponential gives us
\begin{equation}
f(z)\simeq f(z_0)\exp\left(-\frac{A}{2}(z-z_0)^2\right),
\end{equation}
which is in a form of an unnormalized Gaussian distribution. Hence, we can obtain a Gaussian approximation $q(z)$ of $p(z)$ by adding a normalization parameter to form a Normal distribution, as
\begin{equation}
q(z)=\left(\frac{A}{2\pi}\right)^{1/2}\exp\left(-\frac{A}{2}(z-z_0)^2\right)=\mathcal{N}(\mathbf{z}\vert z_0,A^{-1})
\end{equation}
We can extend the Laplace approximation into multi-dimensional variable $\mathbf{z}$, which is finding an Gaussian approximation of distribution
\begin{equation}
p(\mathbf{z})=\frac{1}{Z}f(\mathbf{z}),
\end{equation}
where $z$ is a vector of length $M\geq 2$.</p>

<p>Analogy to the univariate case, the first step is to consider the Taylor expansion of $\log f(\mathbf{z})$ about its stationary point $\mathbf{z}_0$, which means $\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}=0$. We have
\begin{align}
\log f(\mathbf{z})&amp;\simeq f(\mathbf{z}_0)+\log f(\mathbf{z})\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}+\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}(\mathbf{z}-\mathbf{z}_0) \\ &amp;=\log f(\mathbf{z}_0)-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0),
\end{align}
where
\begin{equation}
\mathbf{A}=-\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(z)\vert_{\mathbf{z}=\mathbf{z}_0}
\end{equation}
Taking the exponentials of both sides lets us obtain
\begin{equation}
f(\mathbf{z})\simeq f(\mathbf{z}_0)\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right),
\end{equation}
which is in form of an unnormalized multivariate Gaussian. Adding a normalization parameter gives us the Gaussian approximation $q(\mathbf{z})$ of $p(\mathbf{z})$
\begin{equation}
q(\mathbf{z})=\frac{\vert\mathbf{A}\vert^{1/2}}{(2\pi)^{M/2}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right)=\mathcal{N}(\mathbf{z}\vert\mathbf{z}_0,\mathbf{A}^{-1})
\end{equation}</p>

<h4 id="approx-posterior">Approximation of the posterior</h4>
<p>Consider the prior to be a Gaussian, which is
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ and $\mathbf{S}_0$ are known. Along with this is the likelihood function, which is defined by \eqref{eq:lr.1}, as
\begin{equation}
p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$, and $y_n=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n)$. Therefore, by Bayes’ theorem, the posterior is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})\propto p(\mathbf{w})p(\mathbf{t}\vert\mathbf{w}),
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{w}\vert\mathbf{t})&amp;=-\frac{1}{2}(\mathbf{w}-\mathbf{m}_0)^\text{T}\mathbf{S}_0^{-1}(\mathbf{w}-\mathbf{m}_0)\nonumber \\ &amp;\hspace{2cm}+\sum_{n=1}^{N}\big[t_n\log y_n+(1-t_n)\log(1-y_n)\big]+c,
\end{align}
where $c$ is independent of $\mathbf{w}$.</p>

<p>By Laplace approximation, to find a Gaussian approximation of the posterior, the first step is looking for the point which maximizes the posterior, which is the $\mathbf{w}_\text{MAP}$. This point also defines the mean of the approximation. The corresponding covariance matrix $\mathbf{S}_N$ of the Gaussian is given by
\begin{align}
\mathbf{S}_N&amp;=-\nabla_\mathbf{w}\nabla_\mathbf{w}\log p(\mathbf{w}\vert\mathbf{t}) \\ &amp;=\mathbf{S}_0^{-1}+\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T},
\end{align}
where the second step is obtained by using the result \eqref{eq:nlr.1}. Therefore, the Gaussian approximation $q(\mathbf{w})$ for the posterior distribution is given by
\begin{equation}
q(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{w}_\text{MAP},\mathbf{S}_N)\label{eq:ap.1}
\end{equation}</p>

<h4 id="pred-dist-clf">Predictive distribution</h4>
<p>With the Gaussian approximation \eqref{eq:ap.1}, the predict distribution for class $\mathcal{C}_1$, given a new feature vector $\boldsymbol{\phi}(\mathbf{x})$, is then given by marginalizing w.r.t the posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})=\int p(\mathcal{C}_1\vert\boldsymbol{\phi}\mathbf{w})p(\mathbf{w}\vert\mathbf{t})\,d\mathbf{w}\simeq\int\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi})q(\mathbf{w})\,d\mathbf{w}
\end{equation}
And thus, the predictive distribution for class $\mathcal{C}_2$ is given by
\begin{equation}
p(\mathcal{C}_2\vert\boldsymbol{\phi},\mathbf{t})=1-p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})
\end{equation}</p>

<h2 id="glm">Generalized linear models</h2>

<h2 id="references">References</h2>
<p>[1] <span id="bishops-book">Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</span></p>

<p>[2] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra, 5th edition</a>, 2016.</p>

<p>[3] MIT 18.06. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Linear Algebra</a>.</p>

<p>[4] MIT 18.02. <a href="https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/">Multivariable Calculus</a>.</p>

<p>[5] <a href="https://stats.stackexchange.com/users/28666/amoeba">amoeba</a>. <a href="https://stats.stackexchange.com/q/204599">What is an isotropic (spherical) covariance matrix?</a>. Cross Validated.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A covariance matrix $\mathbf{C}$ is <strong>isotropic</strong> (or <strong>spherical</strong>) if it is proportional to the identity matrix $\mathbf{I}$
\begin{equation*}
\mathbf{C}=\lambda\mathbf{I},
\end{equation*}
where $\lambda\in\mathbb{R}$ is a constant. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>For positive numbers $p_1,\ldots,p_n$ such that $\sum_{i=1}^{n}p_i=1$ and $f$ is a continuous function, if $f$ is <strong>convex</strong>, then
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\leq\sum_{i=1}^{n}p_if(x_i),
\end{equation*}
and if $f$ is <strong>concave</strong>, we instead have
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\geq\sum_{i=1}^{n}p_if(x_i),
\end{equation*} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="linear-regression" /><category term="logistic-regression" /><category term="linear-discriminant-analysis" /><summary type="html"><![CDATA[A note on linear models]]></summary></entry><entry><title type="html">Measure theory - II: Lebesgue measure</title><link href="http://localhost:4000/2022/07/03/measure-theory-p2.html" rel="alternate" type="text/html" title="Measure theory - II: Lebesgue measure" /><published>2022-07-03T13:00:00+07:00</published><updated>2022-07-03T13:00:00+07:00</updated><id>http://localhost:4000/2022/07/03/measure-theory-p2</id><content type="html" xml:base="http://localhost:4000/2022/07/03/measure-theory-p2.html"><![CDATA[<blockquote>
  <p>Part II of the measure theory series. Materials are mostly taken from <a href="/2022/07/03/measure-theory-p2.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/07/03/measure-theory-p2.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#lebesgue-measure">Lebesgue measure</a>
    <ul>
      <li><a href="#lebesgue-outer-measure-properties">Properties of Lebesgue outer measure</a>
        <ul>
          <li><a href="#fnt-add-spt-sets">Finite additivity for separated sets</a></li>
          <li><a href="#outer-measure-elem-sets">Outer measure of elementary sets</a></li>
          <li><a href="#fnt-add-alm-dsjnt-boxes">Finite additivity for almost disjoint boxes</a></li>
          <li><a href="#outer-msr-cntbl-uni-alm-dsjnt-boxes">Outer measure of countable unions of almost disjoint boxes</a></li>
          <li><a href="#open-sets-cntbl-uni-alm-dsjnt-boxes">Open sets as countable unions of almost disjoint boxes</a></li>
          <li><a href="#outer-msr-open-sets">Outer measure of open sets</a></li>
          <li><a href="#outer-msr-arb-sets">Outer measure of arbitrary sets - Outer regularity</a></li>
        </ul>
      </li>
      <li><a href="#lebesgue-measurability">Lebesgue measurability</a>
        <ul>
          <li><a href="#exist-lebesgue-msr-sets">Existence of Lebesgue measurable sets</a></li>
          <li><a href="#crt-msrb">Criteria for measurability</a></li>
          <li><a href="#msr-axiom">The measure axioms</a></li>
          <li><a href="#mnt-cvg-theorem-msr-sets">Monotone convergence theorem for measurable sets</a></li>
          <li><a href="#dmnt-cvg-theorem-msr-sets">Dominated convergence theorem for measurable sets</a></li>
          <li><a href="#inn-rglr">Inner regularity</a></li>
          <li><a href="#crt-fnt-msr">Criteria for finite measure</a></li>
          <li><a href="#caratheodory-crt">Carathéodory criterion, one direction</a></li>
          <li><a href="#inn-msr">Inner measure</a></li>
          <li><a href="#trans-inv">Translation invariance</a></li>
          <li><a href="#change-vars">Change of variables</a></li>
          <li><a href="#uniq-lebesgue-msr">Uniqueness of Lebesgue measure</a></li>
        </ul>
      </li>
      <li><a href="#non-measurable-sets">Non-measurable sets</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lebesgue-measure">Lebesgue measure</h2>
<p>Recall that the Jordan outer measure of a set $E\subset\mathbb{R}^d$ has been defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
From the finite additivity and subadditivity of elementary measure, we can also write the Jordan outer measure as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B_1\cup\dots\cup B_k\supset E;B_1,\dots,B_k\text{ boxes}}\vert B_1\vert+\dots+\vert B_k\vert,
\end{equation}
which means the Jordan outer measure is the infimal cost required to cover $E$ by a finite union of boxes. By replacing the finite union of boxes by a countable union of boxes, we obtain the <strong>Lebesgue outer measure</strong> $m^{*}(E)$ of $E$:
\begin{equation}
m^{*}(E)\doteq\inf_{\bigcup_{n=1}^{\infty}B_n\supset E;B_1,B_2,\dots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
which is be seen as the infimal cost required to cover $E$ by a countable union of boxes.</p>

<p>A set $E\subset\mathbb{R}^d$ is said to be <strong>Lebesgue measurable</strong> if, for every $\varepsilon&gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $E$ such that $m^{*}(U\backslash E)\leq\varepsilon$. If $E$ is Lebesgue measurable, we refer to
\begin{equation}
m(E)\doteq m^{*}(E)
\end{equation}
as the <strong>Lebesgue measure</strong> of $E$.</p>

<h3 id="lebesgue-outer-measure-properties">Properties of Lebesgue outer measure</h3>
<p><strong>Remark 1</strong>. (<strong>The outer measure axioms</strong>)</p>
<ul id="roman-list">
	<li><b>Empty set</b>. $m^*(\emptyset)=0$.</li>
	<li><b>Monotonicity</b>. If $E\subset F\subset\mathbb{R}^d$, then $m^*(E)\leq m^*(F)$.</li>
	<li><b>Countable subadditivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of sets, then $m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{\infty}m^*(E_n)$.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>This follows from the definition of Lebesgue outer measure.</li>
	<li>
		Since $E\subset F\subset\mathbb{R}^d$, then any set containing $F$ also includes $E$, but not every set having $E$ contains $F$. That means
		\begin{equation}
		\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\supset\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		Thus,
		\begin{equation}
		\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\leq\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		or
		\begin{equation}
		m^*(E)&lt; m^*(F)
		\end{equation}
	</li>
	<li>
		By the definition of Lebesgue outer measure, for any positive integer $i$, we have
		\begin{equation}
		m^*(E_i)=\inf_{\bigcup_{n=1}^{\infty}B_n\supset E_i;B_1,B_2,\ldots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert
		\end{equation}
		Thus, by definition of infimum and by <span><a href="/2022/06/16/measure-theory-p1.html#countable-choice-axiom">axiom of countable choice</a></span>, for each $E_i$ in the sequence $(E_n)_{n\in\mathbb{N}}$, there exists a family of boxes $B_{i,1},B_{i,2},\ldots$ in the doubly sequence $(B_{i,j})_{(i,j)\in\mathbb{N}^2}$ covering $E_i$ such that
		\begin{equation}
		\sum_{j=1}^{\infty}\vert B_{i,j}\vert\lt m^*(E_i)+\frac{\varepsilon}{i},
		\end{equation}
		for any $\varepsilon&gt;0$, and for $i=1,2,\ldots$. Plus, we also have
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}
		\end{equation}
		Moreover, by the <span><a href="/2022/06/16/measure-theory-p1.html#tonelli-theorem">Tonelli’s theorem for series</a></span>, we have
		\begin{equation}
		\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}=\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}
		\end{equation}
		Therefore once again, by definition of outer measure and definition of infimum, we obtain
		\begin{align}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;=\inf_{\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert\leq\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert \\\\ &amp;\lt\sum_{i=1}^{\infty}m^*(E_i)+\frac{\varepsilon}{2^i}=\sum_{i=1}^{\infty}m^*(E_i)+\varepsilon
		\end{align}
		And since $\varepsilon&gt;0$ was arbitrary, we can conclude that
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{i=n}^{\infty}m^*(E_n)
		\end{equation}
	</li>
</ul>

<p><strong>Corollary 2</strong><br />
Combining empty set with countable subadditivity axiom gives us the <strong>finite subadditivity</strong> property
\begin{equation}
m^{*}\left(E_1\cup\ldots\cup E_k\right)\leq m^{*}(E_1)+\ldots+m^{*}(E_k),\hspace{1cm}\forall k\geq 0
\end{equation}</p>

<h4 id="fnt-add-spt-sets">Finite additivity for separated sets</h4>
<p><strong>Lemma 3</strong>  <br />
<em>Let $E,F\subset\mathbb{R}^d$ be such that $\text{dist}(E,F)&gt;0$, where
\begin{equation}
\text{dist}(E,F)\doteq\inf\left\{\vert x-y\vert:x\in E,y\in F\right\}
\end{equation}
is the distance between $E$ and $F$. Then $m^*(E\cup F)=m^*(E)+m^*(F)$.</em></p>

<p><strong>Proof</strong><br />
From subadditivity property, we have $m^*(E\cup F)\leq m^*(E)+m^*(F)$. Then it suffices to prove the inverse, that
\begin{equation}
m^*(E\cup F)\geq m^*(E)+m^*(F)
\end{equation}
Let $\varepsilon&gt;0$. By definition of Lebesgue outer measure, we can cover $E\cup F$ by a countable family $B_1,B_2,\ldots$ of boxes such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E\cup F)+\varepsilon
\end{equation}
Suppose it was the case that each box intersected at most one of $E$ and $F$. Then we could divide this family into two subfamilies $B_1’,B_2’,\ldots$ and $B_1'',B_2'',B_3'',\ldots$, the first of which covered $E$, while the second of which covered $F$. From definition of Lebesgue outer measure, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}
and
\begin{equation}
m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n''\vert
\end{equation}
Summing up these two equation, we obtain
\begin{equation}
m^*(E)+m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
and thus
\begin{equation}
m^*(E)+m^*(F)\leq m^*(E\cup F)+\varepsilon
\end{equation}
Since $\varepsilon$ was arbitrary, this gives $m^*(E)+m^*(F)\leq m^*(E\cup F)$ as required.</p>

<p>Now we consider the case that some of the boxes $B_n$ intersect both $E$ and $F$.</p>

<p>Since given any $r&gt;0$, we can always partition a box $B_n$ into a finite number of smaller boxes, each of which has diameter<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> at most $r$, with the total volume of these sub-boxes equal to the volume of the original box $B_n$. Therefore, given any $r&gt;0$, we may assume without loss of generality that the boxes $B_1,B_2,\ldots$ covering $E\cup F$ have diameter at most $r$. Or in particular, we may assume that all such boxes have diameter strictly less than $\text{dist}(E,f)$.</p>

<p>Once we do this, then it is no longer possible for any box to intersect both $E$ and $F$, which allows the previous argument be applicable.</p>

<p><strong>Example 1</strong><br />
Let $E,F\subset\mathbb{R}^d$ be disjoint closed sets, with at least one of $E,F$ being compact. Then $\text{dist}(E,F)&gt;0$.</p>

<p><strong>Proof</strong></p>

<h4 id="outer-measure-elem-sets">Outer measure of elementary sets</h4>
<p><strong>Lemma 4</strong>  <br />
<em>Let $E$ be an elementary set. Then the Lebesgue outer measure of $E$ is equal to the elementary measure of $E$:</em>
\begin{equation}
m^*(E)=m(E)
\end{equation}</p>

<p><strong>Proof</strong><br />
Since
\begin{equation}
m^*(E)\leq m^{*,(J)}(E)=m(E),
\end{equation}
then it suffices to show that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
We first consider the case that $E$ is closed. Since $E$ is elementary, $E$ is also bounded, which implies that $E$ is compact.</p>

<p>Let $\varepsilon&gt;0$ be arbitrary, then we can find a countable family $B_1,B_2,\ldots$ of boxes that cover $E$
\begin{equation}
E\subset\bigcup_{n=1}^{\infty}B_n,
\end{equation}
such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We have that for each box $B_n$, we can find an open box $B_n’$ containing $B_n$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n}
\end{equation}
The $B_n’$ still cover $E$ and we have
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq\sum_{n=1}^{\infty}\left(\vert B_n\vert+\frac{\varepsilon}{2^n}\right)=\left(\sum_{n=1}^{\infty}\vert B_n\vert\right)+\varepsilon\leq m^*(E)+2\varepsilon\label{eq:lemma5.1}
\end{equation}
As the $B_n’$ are open, apply the <span><a href="/2022/06/16/measure-theory-p1.html#heine-borel-theorem"><strong>Heine-Borel theorem</strong></a>, we obtain
\begin{equation}
E\subset\bigcup_{n=1}^{N}B_n’,
\end{equation}
for some finite $N$. Thus, using the finite subadditivity property of elementary measure, combined with the result \eqref{eq:lemma5.1}, we obtain
\begin{equation}
m(E)\leq\sum_{n=1}^{N}m(B_n’)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&gt;0$ was arbitrary, we can conclude that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
Now we turn to considering the case that $E$ is not closed. Then we can write $E$ as the finite union of disjoint boxes
\begin{equation}
E=Q_1\cup\ldots\cup Q_k,
\end{equation}
which need not be closed.</span></p>

<p>Analogy to before, we have that for every $\varepsilon&gt;0$ and every $1\leq j\leq k$, we can find a closed sub-box $Q_j’$ of $Q_j$ such that
\begin{equation}
\vert Q_j’\vert\geq\vert Q_j\vert-\frac{\varepsilon}{k}
\end{equation}
Then $E$ now contains the finite union of $Q_1’\cup\ldots\cup Q_k’$ disjoint closed boxes, which is a closed elementary set. By the finite additivity property of elementary measure, the monotonicity property of Lebesgue measure, combined with the result we have proved in the first case, we have
\begin{align}
m^*(E)&amp;\geq m^*(Q_1’\cup\ldots\cup Q_k’) \\ &amp;=m(Q_1’\cup\ldots\cup Q_k’) \\ &amp;=m(Q_1’)+\ldots+m(Q_k’) \\ &amp;\geq m(Q_1)+\ldots+m(Q_k)-\varepsilon \\ &amp;= m(E)-\varepsilon,
\end{align}
for every $\varepsilon&gt;0$. And since $\varepsilon&gt;0$ was arbitrary, our claim has been proved.</p>

<p><strong>Corollary 6</strong><br />
From the lemma above and the monotonicity property, 
for every $E\in\mathbb{R}^d$, we have
\begin{equation}
m_{*,(J)}(E)\leq m^{*}(E)\leq m^{*,(J)}(E)\label{eq:cor6.1}
\end{equation}</p>

<p><strong>Corollary 7</strong><br />
Not every bounded open set or compact set (bounded closed) is Jordan measurable.</p>

<p><strong>Proof</strong><br />
Consider the countable set $\mathbf{Q}\cap[0,1]$, which we enumerate as $\{q_1,q_2,\ldots\}$. Let $\varepsilon&gt;0$ be a small number, and consider that
\begin{equation}
U\doteq\bigcup_{n=1}^{\infty}(q_n-\varepsilon/2^n,q_n+\varepsilon/2^n),
\end{equation}
which is a union of open sets and thus is open. On the other hand, by countable subadditivity property of Lebesgue outer measure, we have
\begin{align}
m^{*}(U)&amp;=m^{*}\left(\sum_{n=1}^{\infty}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right)\right) \\ &amp;\leq\sum_{n=1}^{\infty}m^{*}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right) \\ &amp;=\sum_{n=1}^{\infty}\frac{2\varepsilon}{2^n}=2\varepsilon
\end{align}
As $U$ dense in $[0,1]$ (i.e.,$\overline{U}$ contains $[0,1]$), we have
\begin{equation}
m^{*}(U)=m^{*,(J)}(\overline{U})\geq m^{*,(J)}([0,1])=1
\end{equation}
Then for $\varepsilon\lt 1$, we have that
\begin{equation}
m^{*}(U)\lt 1\leq m^{*,(J)}(U)
\end{equation}
Combining with \eqref{eq:cor6.1}, we obtain that the bounded open set $U$ is not Jordan measurable.</p>

<h4 id="fnt-add-alm-dsjnt-boxes">Finite additivity for almost disjoint boxes</h4>
<p>Two boxes are <strong>almost disjoint</strong> if their interiors are disjoint, e.g., $[0,1]$ and $[1,2]$ are almost disjoint. If a box has the same elementary as its interior, we see that the finite additivity property
\begin{equation}
m(B_1\cup\ldots\cup B_n)=\vert B_1\vert+\ldots+\vert B_n\vert\label{eq:faadb.1}
\end{equation}
also holds for almost disjoint boxes $B_1,\ldots,B_n$.</p>

<h4 id="outer-msr-cntbl-uni-alm-dsjnt-boxes">Outer measure of countable unions of almost disjoint boxes</h4>
<p><strong>Lemma 8</strong><br />
<em>Let $E=\bigcup_{n=1}^{\infty}B_n$ be a countable union of almost disjoint boxes $B_1,B_2,\ldots$. Then</em>
\begin{equation}
m^*(E)=\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
Thus, for example, $\mathbb{R}^d$ has an infinite outer measure.</p>

<p><strong>Proof</strong><br />
From countable subadditivity property of Lebesgue measure and <strong>Lemma 5</strong>, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}m^*(B_n)=\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
so it suffices to show that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)
\end{equation}
Since for each integer $N$, $E$ contains the elementary set $B_1\cup\ldots\cup B_N$, then by monotonicity property and <strong>Lemma 5</strong>
\begin{align}
m^*(E)&amp;\geq m^*(B_1\cup\ldots\cup B_N)=m(B_1\cup\ldots\cup B_N)
\end{align}
And thus by \eqref{eq:faadb.1}, we have
\begin{equation}
\sum_{n=1}^{N}\vert B_n\vert\leq m^*(E)
\end{equation}
Letting $N\to\infty$ we obtain the claim.</p>

<p><strong>Corollary 9</strong><br />
If $E=\bigcup_{n=1}^{\infty}B_n=\bigcup_{n=1}^{\infty}B_n’$ can be decomposed in two different ways as the countable union of almost disjoint boxes, then
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert=\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}</p>

<p><strong>Example 2</strong><br />
If a set $E\subset\mathbb{R}^{d}$ is expressible as the countable union of almost disjoint boxes, then
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}</p>

<p><strong>Proof</strong><br />
For $B_n$’s are disjoint boxes, we begin by express $E$ as 
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n\label{eq:eg2.1}
\end{equation}
Hence, by <strong>Lemma 8</strong>, we have
\begin{equation}
m^{*}(E)=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.2}
\end{equation}
Moreover, \eqref{eq:eg2.1} can be continued to derive as
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n=\left(\bigcup_{n=1}^{N}B_n\right)\cup\left(\bigcup_{n=N+1}^{\infty}B_n\right)=\left(\bigcup_{n=1}^{N}B_n\right)\cup B,
\end{equation}
where we have defined $B=\bigcup_{n=N+1}^{\infty}B_n$. And thus, we also have that $B_1,\ldots,B_N,B$ are almost disjoint boxes, which claims that $E$ is an elementary set. Therefore, $E$ is also Jordan measurable. Using finite additivity property of Jordan measurability yields
\begin{equation}
m_{*,(J)}(E)=m(E)=\left(\sum_{n=1}^{N}\vert B_n\vert\right)+\vert B\vert=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.3}
\end{equation}
Combining \eqref{eq:eg2.2} and \eqref{eq:eg2.3} together gives us
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}</p>

<h4 id="open-sets-cntbl-uni-alm-dsjnt-boxes">Open sets as countable unions of almost disjoint boxes</h4>
<p><strong>Lemma 10</strong><br />
<em>Let $E\subset\mathbb{R}^d$ be an open set. Then $E$ can be expressed as the countable union of almost disjoint boxes (and, in fact, as the countable union of almost disjoint closed cubes)</em>.</p>

<p><strong>Proof</strong><br />
We begin by defining a <strong>closed dyadic cube</strong> to be a cube $Q$ of the form
\begin{equation}
Q=\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right]\times\ldots\times\left[\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d;n\geq 0$.</p>

<p>We have that such closed dyadic cubes of a fixed sidelength $2^{-n}$ are almost disjoint and cover all of $\mathbb{R}^d$. And also, each dyadic cube of sidelength $2^{-n}$ is contained in exactly one “parent” of sidelength $2^{-n+1}$ (which, conversely, has $2^d$ “children” of sidelength $2^{-n}$), giving the dyadic cubes a structure analogous to that of a binary tree.</p>

<p>As a consequence of these facts, we also obtain the <strong>dyadic nesting property</strong>: given any two closed dyadic cubes (not necessarily same sidelength), then either they are almost disjoint, or one of them is contained in the other.</p>

<p>If $E$ is open, and $x\in E$, then by definition there is an open ball centered at $x$ that is contained in $E$. Also, it is easily seen that there is also a closed dyadic cube containing $x$ that is contained in $E$. Hence, if we let $\mathcal{Q}$ be the collection of all the dyadic cubes $Q$ that are contained in $E$, we see that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}}Q
\end{equation}
Let $\mathcal{Q}^*$ denote cubes in $\mathcal{Q}$ such that they are not contained in any other cube in $\mathcal{Q}$. From the nesting property, we see that every cube in $\mathcal{Q}$ is contained in exactly one maximal cube in $\mathcal{Q}^*$, and that any two such maximal cubes in $\mathcal{Q}^*$ are almost disjoint. Thus, we have that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}^*}Q,
\end{equation}
which is union of almost disjoint cubes. As $\mathcal{Q}^*$ is at most countable, the claim follows.</p>

<h4 id="outer-msr-open-sets">Outer measure of open sets</h4>
<p><strong>Corollary 11</strong><br />
The Lebesgue outer measure of any open set is equal to the Jordan inner measure of that set, or of the total volume of any partitioning of that set into almost disjoint boxes.</p>

<h4 id="outer-msr-arb-sets">Outer measure of arbitrary sets - Outer regularity</h4>
<p><strong>Lemma 12</strong>.<br />
<em>Let $E\subset\mathbb{R}^d$ be an arbitrary set. Then we have</em>
\begin{equation}
m^*(E)=\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}</p>

<p><strong>Proof</strong><br />
From monotonicity property, we have
\begin{equation}
m^*(E)\leq\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}
Then, it suffices to show that
\begin{equation}
m^*(E)\geq\inf_{E\subset U,U\text{ open}}m^*(U),
\end{equation}
which is obvious in the case that $m^*(E)$ is infinite. Thus, we now assume that $m^*(E)$ is finite.</p>

<p>Let $\varepsilon&gt;0$. By the definition of Lebesgue outer measure, there exists a countable family $B_1,B_2,\ldots$ of boxes covering $E$ such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We can enlarge each of these boxes $B_n$ to an open box $B_n’$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n},
\end{equation}
for any $\varepsilon&gt;0$. Then the set $\bigcup_{n=1}^{\infty}B_n’$, being a union of open sets, is itself open, and contains $E$, and
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq m^*(E)+\varepsilon+\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=m^*(E)+2\varepsilon
\end{equation}
By countable subadditivity property, it implies that
\begin{equation}
m^*\left(\bigcup_{n=1}^{\infty}B_n’\right)\leq m^*(E)+2\varepsilon
\end{equation}
and thus
\begin{equation}
\inf_{E\subset U,U\text{ open}}m^*(U)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&gt;0$ was arbitrary, the claim follows.</p>

<h3 id="lebesgue-measurability">Lebesgue measurability</h3>

<h4 id="exist-lebesgue-msr-sets">Existence of Lebesgue measurable sets</h4>
<p><strong>Lemma 13</strong>.</p>
<ul id="roman-list" style="font-style: italic;">
	<li>Every open set is Lebesgue measurable.</li>
	<li>Every closed set is Lebesgue measurable.</li>
	<li>Every set of Lebesgue outer measure zero is measurable. (Such sets are called <b>null sets</b>.)</li>
	<li>The empty set $\emptyset$ is Lebesgue measurable.</li>
	<li>If $E\subset\mathbb{R}^d$ is Lebesgue measurable, then so its complement $\mathbb{R}^d\backslash E$.</li>
	<li>If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the union $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.</li>
	<li>If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the intersection $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>This follows from definition.</li>
	<li>
		We have that every closed set is a the countable union of closed and bounded set, so by (vi), if suffices to verify the claim when $E$ is bounded and closed.<br />
		Let $U\supset E$ be an open set, we thus have that $U\backslash E$ is also open due to the compactness of $E$. By <b>lemma 10</b>, we can represent the open set $U\backslash E$ as a countable union of almost disjoint boxes as
		\begin{equation}
		U\backslash E=\bigcup_{n=1}^{\infty}B_n
		\end{equation}
		The problem remains to prove that for any $\varepsilon&gt;0$
		\begin{equation}
		\sum_{n=1}^{\infty}\vert B_n\vert&lt;\varepsilon
		\end{equation}
	</li>
	<li>This follows from definition.</li>
	<li>This follows from definition.</li>
	<li>
		Given $E$ is Lebesgue measurable, for each positive integer $n$, we can find an open set $U_n$ containing $E$ such that
		\begin{equation}
		m^*(U_n\backslash E)\leq\frac{1}{n}
		\end{equation}
		Let $F_n=U_n^c=\mathbb{R}^d\backslash U_n$. Thus, we have $F_n\subset\mathbb{R}^d\backslash E$ and
		\begin{equation}
		m^*\big((\mathbb{R}^d\backslash E)\backslash F_n\big)=m^*\big((\mathbb{R}^d\backslash E)\backslash(\mathbb{R}^d\backslash U_n)\big)=m^*(U_n\backslash E)\leq\frac{1}{n}\label{eq:lemma13.1}
		\end{equation}
		In addition, since $F_n\subset\mathbb{R}^d\backslash E$, the countable union of them, denoted as $F$, is also a subset of $\mathbb{R}^d\backslash E$
		\begin{equation}
		F=\bigcup_{n=1}^{\infty}F_n\subset\mathbb{R}^d\backslash E
		\end{equation}
		Moreover, from \eqref{eq:lemma13.2}, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{N}F_n\right)=m^*\left(\bigcap_{n=1}^{N}(\mathbb{R}^d\backslash E)\backslash F_n\right)\leq\frac{1}{N}
		\end{equation}
		Let $N$ approaches $\infty$, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{\infty}F_n\right)\leq 0
		\end{equation}
		By non-negativity property, we then have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=0,
		\end{equation}
		Hence, $\mathbb{R}^d\backslash E$ is a union of $F$ with a set of Lebesgue outer measure of zero. The set $F$, in the other hand, is a countable union of closed set $F_n$'s (since each $U_n$ is an open set). Therefore, by (ii), (iii) and (vi), we have that $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.
	</li>
	<li>
		For each Lebesgue measurable set $E_n$, for any $\varepsilon&gt;0$ and for $U_n$ is an open set containing $E_n$ we have 
		\begin{equation}
		m^{*}(U_n\backslash E_n)\leq\frac{\varepsilon}{2^n}\label{eq:lemma13.2}
		\end{equation}
		Moreover, since $E_n\subset U_n$, then
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{n=1}^{\infty}U_n,
		\end{equation}
		which is also an open set. Therefore, from \eqref{eq:lemma13.2} and by countable subadditivity, we have
		\begin{equation}
		m^*\left(\left(\bigcup_{n=1}^{\infty}U_n\right)\backslash\left(\bigcup_{n=1}^{\infty}E_n\right)\right)\leq\sum_{n=1}^{\infty}m^*(U_n\backslash E_n)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon,
		\end{equation}
		which proves that $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	</li>
	<li>
		Given $E_1,E_2,E_3,\ldots\subset\mathbb{R}^d$ are Lebesgue measurable, by (v), the complement of them,
		\begin{equation}
		E_1^c,E_2^c,E_3^c,\ldots\subset\mathbb{R}^d,
		\end{equation}
		are also Lebesgue measurable. By <b>De Morgan's laws</b>, we have
		\begin{equation}
		\left(\bigcap_{n=1}^{\infty}E_n\right)^c=\bigcup_{n=1}^{\infty}E_n^c,
		\end{equation}
		which is Lebesgue measurable by (vi). Thus, $\left(\bigcap_{n=1}^{\infty}E_n\right)^c$ is also Lebesgue measurable. This means, using (v) once again, we obtain that $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	</li>
</ul>

<h4 id="crt-msrb">Criteria for measurability</h4>
<p>Let $E\subset\mathbb{R}^d$, then the following are equivalent:</p>
<ul id="roman-list">
	<li>$E$ is Lebesgue measurable.</li>
	<li><b>Outer approximation by open</b>. For every $\varepsilon&gt;0$, $E$ can be contained in an open set $U$ with $m^*(U\backslash E)\leq\varepsilon$.</li>
	<li><b>Almost open</b>. For every $\varepsilon&gt;0$, we can find an open set $U$ such that $m^*(U\Delta E)\leq\varepsilon$. ($E$ differs from an open set by a set of outer measure at most $\varepsilon$.)</li>
	<li><b>Inner approximation by closed</b>. For every $\varepsilon&gt;0$, we can find a closed set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.</li>
	<li><b>Almost closed</b>. For every $\varepsilon&gt;0$, we can find a closed set $F$ such that $m^*(F\Delta E)\leq\varepsilon$. ($E$ differs from a closed set by a set of outer measure at most $\varepsilon$.)</li>
	<li><b>Almost measurable</b>. For every $\varepsilon&gt;0$, we can find a Lebesgue measurable set $E_\varepsilon$ such that $m^*(E_\varepsilon\Delta E)\leq\varepsilon$. ($E$ differs from a measurable set by a set of outer measure at most $\varepsilon$.)</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		This follows from definition
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
		Given $E$ is Lebesgue measurable, for any $\varepsilon&gt;0$, we can find an open set $U$ containing $E$ such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		And since $E\subset U$, we have that
		\begin{equation}
		m^(E\backslash U)=m^*(\emptyset)=0,
		\end{equation}
		which implies that for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(U\Delta E)=m^*(U\backslash E)+m^*(E\backslash U)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (iv)<br />
		By the claim (v) in <b>lemma 13</b>, given Lebesgue measurable set $E\subset\mathbb{R}^d$, we have that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable. Therefore, there exists an open set $U$ containing $\mathbb{R}^d\backslash E$ such that for any $\varepsilon&gt;0$ we have
		\begin{equation}
		m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.1}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}\backslash U$, thus $F$ is a closed set contained in $E$. Moreover, from \eqref{eq:cm.1} we also have for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(E\backslash F)=m^*\left(E\backslash(\mathbb{R}^d\backslash U)\right)=m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (v)<br />
		Given Lebesgue measurable set $E\subset\mathbb{R}^d$, using the claim (v) in <b>lemma 13</b> gives us that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.<br />
		From claim (iii), for any $\varepsilon&gt;0$, we can find an open set $U$ such that
		\begin{equation}
		m^*\left(U\Delta(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.2}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}^d\backslash$. We then have that $F$ is a closed set. In addition, $U\Delta(\mathbb{R}^d\backslash E)$ can be rewritten by
		\begin{align}
		U\Delta(\mathbb{R}^d\backslash E)&amp;=\left(U\backslash(\mathbb{R}^d\backslash E)\right)\cup\left((\mathbb{R}^d\backslash E)\backslash U\right) \\ &amp;=\left(E\backslash(\mathbb{R}^d\backslash U)\right)\cup\left((\mathbb{R}^d\backslash U)\backslash E\right) \\ &amp;=(\mathbb{R}^d\backslash U)\backslash E \\ &amp;=F\Delta E,
		\end{align}
		which lets \eqref{eq:cm.2} can be written as, for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(F\Delta E)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (vi)<br />
		Given $E$ is Lebesgue measurable, by claim (v), for any $\varepsilon&gt;0$ we can find a closed set $E_\varepsilon$ such that
		\begin{equation}
		m^*(E_\varepsilon\Delta E)\leq\varepsilon
		\end{equation}
		While by property (ii) of <b>lemma 13</b>, we have that $E_\varepsilon$ is Lebesgue measurable, which proves our claim.
	</li>
	<li>
		(vi) $\Rightarrow$ (i)<br />
		Given (vi), for any $\varepsilon&gt;0$, we can find a Lebesgue measurable set $E_\varepsilon^{(n)}$ such that
		\begin{equation}
		m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\frac{\varepsilon}{2^n}
		\end{equation}
		Therefore, by countable subadditivity property of Lebesgue outer measurability
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon
		\end{equation}
	</li>
</ul>

<p><strong>Remark 14</strong>  <br />
Every Jordan measurable set is Lebesgue measurable.</p>

<p><strong>Proof</strong><br />
This follows directly from <strong>corollary 6</strong>.</p>

<p><strong>Remark 15</strong>  <br />
The <a href="/2022/06/16/measure-theory-p1.html#cantor-set"><strong>Cantor set</strong></a> is compact, uncountable, and a null set.</p>

<p><strong>Proof</strong></p>
<ul>
  <li>Since $\mathcal{C}\subseteq[0,1]$ is closed and bounded, by the <a href="/2022/06/16/measure-theory-p1.html#heine-borel-theorem">Heine-Borel theorem</a>, $\mathcal{C}$ is then compact.</li>
  <li></li>
</ul>

<h4 id="msr-axiom">The measure axioms</h4>
<p><strong>Lemma 16</strong></p>
<ul id="roman-list" style="font-style: italic;">
	<li><b>Empty set</b>. $m(\emptyset)=0$.</li>
	<li><b>Countable additivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then</li>
	\begin{equation}
	m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
	\end{equation}
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		<b>Empty set</b><br />
		We have that empty set $\emptyset$ is Lebesgue measurable since for every $\varepsilon&gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $\emptyset$ such that $m^*(U\backslash\emptyset)\leq\varepsilon$. Thus,
		\begin{equation}
		m(\emptyset)=m^*(\emptyset)=0
		\end{equation}
	</li>
	<li>
		<b>Countable additivity</b><br />
		We begin by considering the case that $E_n$ are all compact sets.
		<br />
		By repeated use of <b>Lemma 12</b> and <b>Example ?</b>, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Thus, using monotonicity property, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Let $N\to\infty$, we obtain
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
		On the other hand, by countable subadditivity, we also have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Therefore, we can conclude that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Next, we consider the case that $E_n$ are bounded but not necessarily compact.
		<br />
		Let $\varepsilon&gt;0$. By criteria for measurability, we know that each $E_n$ is the union of a compact set $K_n$ and a set of outer measure at most $\varepsilon/2^n$. Thus
		\begin{equation}
		m(E_n)\leq m(K_n)+\frac{\varepsilon}{2^n}
		\end{equation}
		And hence
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq\left(\sum_{n=1}^{\infty}m(K_n)\right)+\varepsilon
		\end{equation}
		From the first case, we know that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)=\sum_{n=1}^{\infty}m(K_n)
		\end{equation}
		while from monotonicity property of Lebesgue measure
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Putting these results together we obtain
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)+\varepsilon,
		\end{equation}
		for every $\varepsilon&gt;0$. And since $\varepsilon$ was arbitrary, we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		while from countable subadditivity property we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\geq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Therefore, the claim follows.
		<br />
		Finally, we consider the case that $E_n$ are not bounded or closed with the idea of decomposing each $E_n$ as a countable disjoint union of bounded Lebesgue measurable sets.
		<br />
	</li>
</ul>

<p><strong>Remark 17</strong><br />
The countable additivity also implies the <strong>finite additivity</strong> property of Lebesgue  measure
\begin{equation}
m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n),
\end{equation}
where $E_1,\ldots,E_N$ are Lebesgue measurable.</p>

<h4 id="mnt-cvg-theorem-msr-sets">Monotone convergence theorem for measurable sets</h4>
<ul id="roman-list">
	<li>
		<b>Upward monotone convergence</b>. Let $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ be a countable non-decreasing sequence of Lebesgue measurable sets. Then
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		<b>Downward monotone convergence</b>. Let $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ be a countable non-increasing sequence of Lebesgue measurable sets. If at least one of the $m(E_n)$ is finite, then
		\begin{equation}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		The hypothesis that at least one of the $m(E_n)$ is finite in the downward monotone convergence theorem cannot be dropped.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		<b>Upward monotone convergence</b><br />
		Since $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ is a countable non-decreasing sequence of Lebesgue measurable sets, by countable additivity, we have
		\begin{align}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash\bigcup_{n'=1}^{n-1}E_{n'}\right) \\ &amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash E_{n-1}\right) \\ &amp;=\left(\sum_{n=2}^{\infty}m(E_n)-m(E_{n-1})\right)+m(E_1) \\ &amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	</li>
	<li>
		<b>Downward monotone convergence</b><br />
		Since $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ is a countable non-increasing sequence of Lebesgue measurable sets, the sequence of their complement $E_1^c\subset E_2^c\subset\ldots\subset\mathbb{R}^d$ is therefore a countable non-decreasing sequence of Lebesgue measurable sets. Using the claim (i) and by De Morgan's laws, we have
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m(\mathbb{R}^d)-m\left(\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m(\mathbb{R}^d)-\lim_{n\to\infty}m(E_n^c) \\ &amp;=m(\mathbb{R}^d)-m(\mathbb{R}^d)+\lim_{n\to\infty}m(E_n) \\ &amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	</li>
	<li>
		Consider sequence $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ of non-increasing Lebesgue measurable sets where each $E_n$ is given by
		\begin{equation}
		E_n\doteq[n,+\infty)
		\end{equation}
		Therefore, by De Morgan's laws, the Lebesgue measure of their countable intersection is
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}(-\infty,n)\right) \\ &amp;=m(\mathbb{R}^d\backslash\mathbb{R}^d) \\ &amp;=m(\emptyset)=0,
		\end{align}
		while for every $n$, we have
		\begin{equation}
		m(E_n)=m\left([n,+\infty)\right)=\infty
		\end{equation}
	</li>
</ul>

<h4 id="dmnt-cvg-theorem-msr-sets">Dominated convergence theorem for measurable sets</h4>
<p>We say that a sequence $E_n$ of sets in $\mathbb{R}^d$ <strong>converges pointwise</strong> to another set $E$ in $\mathbb{R}^d$ if the indicator function $1_{E_n}$ converges pointwise to $1_E$.</p>
<ul id="roman-list">
	<li>
		If the $E_n$ are all Lebesgue measurable, and converge pointwise to $E$, then $E$ is Lebesgue measurable also.
	</li>
	<li>
		<b>Dominated convergence theorem</b>. Suppose that the $E_n$ are all contained in another Lebesgue measurable set $F$ of finite measure. Then $m(E_n)$ converges to $m(E)$.
	</li>
	<li>
		The dominated convergence theorem fails if the $E_n$'s are not contained in a set of finite measure, even if we assume that the $m(E_n)$ are all uniformly bounded.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		We have
	</li>
</ul>

<p><strong>Remark 18</strong><br />
Let $E\subset\mathbb{R}^d$, then $E$ is contained in a Lebesgue measurable set of measure exactly equal to $m^*(E)$.</p>

<p><strong>Proof</strong></p>

<h4 id="inn-rglr">Inner regularity</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable. Then
\begin{equation}
m(E)=\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}</p>

<p><strong>Proof</strong><br />
By monotonic we have that
\begin{equation}
m(E)\geq\sup_{K\subset E,K\text{ compact}}m(K),
\end{equation}
thus it suffices to show that
\begin{equation}
m(E)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Consider the case that $E$ is bounded. By the <strong>criteria for Lebesgue measurability</strong>, we have that for any $\varepsilon&gt;0$, there exist a bounded and closed, and thus compact by the Heine-Borel theorem, set $K’$ contained in $E$ such that
\begin{equation}
m(E\backslash K’)\leq\varepsilon
\end{equation}
Moreover, by claim (ii) of <strong>lemma 13</strong>, we have that $K’$ is Lebesgue measurable. Using finite additivity property of Lebesgue measure gives us
\begin{equation}
\varepsilon\geq m(E\backslash K’)=m(E)-m(K’),
\end{equation}
which means
\begin{equation}
m(E)\leq m(K’)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Now consider {the case that $E$ is an unbounded set. Let $(K_r)_{r=1,2,\ldots}$ be the sequence sets in which each $K_r$ is defined as
\begin{equation}
K_r\doteq E\cap B(\mathbf{0},r),\label{eq:ir.1}
\end{equation}
where $B(\mathbf{0},r)$ is a closed ball centered at $\mathbf{0}\in\mathbb{R}^d$ with radius $r$
\begin{equation}
B(\mathbf{0},r)=\{\mathbf{x}:\vert\mathbf{x}\vert\leq r\}
\end{equation}
which means $K_1\subset K_2\subset\ldots\subset E$ is an increasing sequence of compact set (since \eqref{eq:ir.1} also implies that $K_r\subset B(\mathbf{0},r)$, and hence bounded and closed, then using the Heine-Borel theorem to obtain the compactness of $K_r$). By the <strong>monotone convergence theorem</strong>, we have
\begin{equation}
m\left(\bigcup_{r=1}^{\infty}K_r\right)=\lim_{r\to\infty}m(K_r)
\end{equation}
On the other hand, the countable union of $K_r$ can be written as
\begin{equation}
\bigcup_{r=1}^{\infty}K_r=\bigcup_{r=1}^{\infty}E\cap B(\mathbf{0},r)=E\cap\bigcup_{r=1}^{\infty}B(\mathbf{0},r)=E\cap\mathbb{R}^d=E,
\end{equation}
which therefore gives us
\begin{equation}
m(E)=\lim_{r\to\infty}m(K_r)\label{eq:ir.2}
\end{equation}
Moreover, by monotonicity property $m(E)\geq m(K_r),\forall r$. Hence, \eqref{eq:ir.2} implies that for any $\varepsilon&gt;0$, there exists $r’$ such that for all $r\geq r’$
\begin{equation}
\varepsilon&gt;\vert m(E)-m(K_{r’})\vert=m(E)-m(K_{r’})
\end{equation}
This means that
\begin{equation}
m(A)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Our claim then follows.</p>

<h4 id="crt-fnt-msr">Criteria for finite measure</h4>
<p>Let $E\subset\mathbb{R}^d$, then the following are equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable with finite measure.
	</li>
	<li>
		<b>Outer approximation by open</b>. For every $\varepsilon&gt;0$, we can contain $E$ in an open set $U$ of finite measure with $m^*(U\backslash E)\leq\varepsilon$.
	</li>
	<li>
		<b>Almost open bounded</b>. For every $\varepsilon&gt;0$, there exists a bounded open set $U$ such that $m^*(E\Delta U)\leq\varepsilon$. (In other words, $E$ differs from a bounded set by a set of arbitrarily small Lebesgue outer measure.)
	</li>
	<li>
		<b>Inner approximation by compact</b>. For every $\varepsilon&gt;0$, we can find a compact set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.
	</li>
	<li>
		<b>Almost compact</b>. $E$ differs from a compact set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost bounded measurable</b>. $E$ differs from a bounded Lebesgue measurable set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost finite measure</b>. $E$ differs from a Lebesgue measurable set with finite measure by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost elementary</b>. $E$ differs from an elementary set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost dyadically elementary</b>. For every $\varepsilon&gt;0$, there exists an integer $n$ and a finite union $F$ of closed dyadic cubes of sidelength $2^{-n}$ such that $m^*(E\Delta F)\leq\varepsilon$.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		Given $E$ is Lebesgue measurable with finite measure, by definition, for any $\varepsilon&gt;0$, there exists an open set $U$ o such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		Then, by finite subadditivity property of Lebesgue outer measure
		\begin{equation}
		m^*(U)\leq m^*(E)+\varepsilon,
		\end{equation}
		which implies that $m^*(U)$ finite due to finiteness of $m^*(E)$ and $\varepsilon$, and hence $U$ has finite measure since $m(U)\leq m^*(U)$.
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
	</li>
</ul>

<h4 id="caratheodory-crt">Carathéodory criterion, one direction</h4>
<p>Let $E\subset\mathbb{R}^d$, the following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable.
	</li>
	<li>
		For every elementary set $A$
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	</li>
	<li>
		For every box $B$, we have
		\begin{equation}
		\vert B\vert=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		We begin with an observation that, by finite additivity property of Lebesgue measure
		\begin{equation}
		m(A)=m(A\cap E)+m(A\backslash E)\leq m^*(A\cap E)+m^*(A\backslash E)\label{eq:cc.1}
		\end{equation}
		Given $A$ is elementary, by <span><a href="/2022/06/16/measure-theory-p1.html#measure-elementary-set"><b>lemma 10</b></a></span>, we can express $A$ as a finite union of disjoint boxes
		\begin{equation}
		A=\bigcup_{n=1}^{N}B_n
		\end{equation}
		Continuing using finite subadditivity of Lebesgue outer measure and finite additivity of Lebesgue measure, \eqref{eq:cc.1} then can be continued to derive as
		\begin{align}
		m(A)&amp;\leq m^*(A\cap E)+m^*(A\backslash E) \\ &amp;=m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\cap E\right)+m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\backslash E\right) \\ &amp;=m^*\left(\bigcup_{n=1}^{N}B_n\cap E\right)+m^*\left(\bigcup_{n=1}^{N}B_n\backslash E\right) \\ &amp;\leq\sum_{n=1}^{N}m^*(B_n\cap E)+m^*(B_n\backslash E) \\ &amp;=\sum_{n=1}^{N}m^*(B_n)=\sum_{n=1}^{N}m(B_n)=m\left(\bigcup_{n=1}^{N}B_n\right)=m(A),
		\end{align}
		which implies that
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
		Since every box $B$ is Lebesgue measurable, then given $E$ is also Lebesgue measurable, by <b>lemma 13</b>, their difference and intersection are also Lebesgue measurable, which means by additivity property of Lebesgue measure we have
		\begin{equation}
		\vert B\vert=m(B)=m(B\cap E)+m(B\backslash E)=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	</li>
	<li>
		(ii) $\Rightarrow$ (i)<br />
	</li>
</ul>

<h4 id="inn-msr">Inner measure</h4>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set. The <strong>Lebesgue inner measure</strong> $m_*(E)$ of $E$ is defined by
\begin{equation}
m_*(E)\doteq m(A)-m^*(A\backslash E),
\end{equation}
for any elementary set $A$ containing $E$. Then</p>
<ul id="roman-list">
	<li>
		If $A,A'$ are two elementary sets containing $E$, then
		\begin{equation}
		m(A)-m^*(A\backslash E)=m(A')-m^*(A'\backslash E)
		\end{equation}
	</li>
	<li>
		We have that $m_*(E)\leq m^*(E)$, and that equality holds iff $E$ is Lebesgue measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<p><strong>Example 3</strong><br />
Let $E\subset \mathbb{R}^d$, and define a $G_\delta$ <em>set</em> to be a countable intersection $\bigcap_{n=1}^{\infty}U_n$ of open sets, and define an $F_\delta$ <em>set</em> to be a countable union $\bigcup_{n=1}^{\infty}F_n$ of closed sets. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable.
	</li>
	<li>
		$E$ is a $G_\delta$ set with a null set removed.
	</li>
	<li>
		$E$ is the union of an $F_\delta$ set and a null set.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h4 id="trans-inv">Translation invariance</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, then $E+x$ is also Lebesgue measurable for any $x\in\mathbb{R}^d$, and $m(E+x)=m(E)$.</p>

<p><strong>Proof</strong></p>

<h4 id="change-vars">Change of variables</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, and $T:\mathbb{R}^d\to\mathbb{R}^d$ be a linear transformation, then $T(E)$ is Lebesgue measurable, and $m(T(E))=\vert\text{det}(T)\vert m(E)$.</p>

<p><strong>Note</strong><br />
If $T:\mathbb{R}^d\to\mathbb{R}^{d’}$ is a linear map to a space $\mathbb{R}^{d’}$ of strictly smaller dimension than $\mathbb{R}^d$, then $T(E)$ need not be Lebesgue measurable.</p>

<p><strong>Proof</strong></p>

<p><strong>Remark 19</strong><br />
Let $d,d’\geq 1$ be natural numbers</p>
<ul id="roman-list">
	<li>
		If $E\subset\mathbb{R}^d$ and $F\subset\mathbb{R}^{d'}$, then
		\begin{equation}
		(m^{d+d'})^*(E\times F)\leq(m^d)^*(E)(m^{d'})^*(F)
		\end{equation}
	</li>
	<li>
		Let $E\subset\mathbb{R}^d,F\subset\mathbb{R}^{d'}$ be Lebesgue measurable sets. Then $E\times F\subset\mathbb{R}^{d+d'}$ is Lebesgue measurable, with \begin{equation}
		m^{d+d'}(E\times F)=m^d(E).m^{d'}(F)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>

<h4 id="uniq-lebesgue-msr">Uniqueness of Lebesgue measure</h4>
<p>Lebesgue measure $E\mapsto m(E)$ is the only map from Lebesgue measurable sets to $[0,+\infty]$ that obeys the following axioms:</p>
<ul id="roman-list">
	<li>
		<b>Empty set</b>. $m(\emptyset)=0$.
	</li>
	<li>
		<b>Countable additivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then 
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		<b>Translation invariance</b>. If $E$ is Lebesgue measurable and $x\in\mathbb{R}^d$, then $m(E+x)=m(E)$.
	</li>
	<li>
		<b>Normalisation</b>. $m([0,1]^d)=1$.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="non-measurable-sets">Non-measurable sets</h3>
<p><strong>Remark 20</strong><br />
There exists a subset $E\subset[0,1]$ which is not Lebesgue measurable.</p>

<p><strong>Remark 21</strong> (Outer measure is not finitely additive)<br />
There exists disjoint bounded subsets $E,F\subset\mathbb{R}$ such that
\begin{equation}
m^*(E\cap F)\neq m^*(E)+m^*(F)
\end{equation}</p>

<p><strong>Remark 22</strong><br />
Let $\pi:\mathbb{R}^2\to\mathbb{R}$ be the coordinate projection $\pi(x,y)\doteq x$. Then there exists a measurable $E\subset\mathbb{R}^2$ such that $\pi(E)$ is not measurable.</p>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. </span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The <strong>diameter</strong> of a set $B$ is defined as
\begin{equation*}
\text{dia}(B)\doteq\sup\{\vert x-y\vert:x,y\in B\}
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-measure" /><summary type="html"><![CDATA[Note on measure theory part 2]]></summary></entry><entry><title type="html">Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral</title><link href="http://localhost:4000/2022/06/16/measure-theory-p1.html" rel="alternate" type="text/html" title="Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral" /><published>2022-06-16T13:00:00+07:00</published><updated>2022-06-16T13:00:00+07:00</updated><id>http://localhost:4000/2022/06/16/measure-theory-p1</id><content type="html" xml:base="http://localhost:4000/2022/06/16/measure-theory-p1.html"><![CDATA[<blockquote>
  <p>Part I of the measure theory series. Materials are mostly taken from <a href="/2022/06/16/measure-theory-p1.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/06/16/measure-theory-p1.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#pts-sets">Points, sets</a></li>
      <li><a href="#open-closed-compact-sets">Open, closed, compact sets</a></li>
      <li><a href="#rects-cubes">Rectangles, cubes</a></li>
      <li><a href="#cantor-set">The Cantor set</a></li>
      <li><a href="#others">Others</a></li>
    </ul>
  </li>
  <li><a href="#elementary-measure">Elementary measure</a>
    <ul>
      <li><a href="#intervals-boxes-elementary-sets">Intervals, boxes, elementary sets</a></li>
      <li><a href="#measure-elementary-set">Measure of an elementary set</a></li>
      <li><a href="#elementary-measure-properties">Properties of elementary measure</a></li>
      <li><a href="#uniqueness-elementary-measure">Uniqueness of elementary measure</a></li>
    </ul>
  </li>
  <li><a href="#jordan-measure">Jordan measure</a>
    <ul>
      <li><a href="#jordan-measurability-characterisation">Characterisation of Jordan measurability</a></li>
      <li><a href="#jordan-measurability-properties">Properties of Jordan measurability</a></li>
      <li><a href="#jordan-null-sets">Jordan null sets</a></li>
      <li><a href="#metric-formula-jordan-measurability">Metric entropy formulation of Jordan measurability</a></li>
      <li><a href="#uniqueness-jordan-measure">Uniqueness of Jordan measure</a></li>
      <li><a href="#topo-jordan-measurability">Topological of Jordan measurability</a></li>
      <li><a href="#caratheodory-type-property">Carathéodory type property</a></li>
    </ul>
  </li>
  <li><a href="#connect-riemann-int">Connection with the Riemann integral</a>
    <ul>
      <li><a href="#riemann-integrability">Riemann integrability</a></li>
      <li><a href="#pc-func">Piecewise constant functions</a>
        <ul>
          <li><a href="#pc-int-properties">Basic properties of piecewise constant integral</a></li>
        </ul>
      </li>
      <li><a href="#darboux-int">Darboux integral</a>
        <ul>
          <li><a href="#equiv-riemann-darboux-int">Equivalence of Riemann integral and Darboux integral</a></li>
        </ul>
      </li>
      <li><a href="#riemann-int-properties">Basic properties of the Riemann integral</a></li>
      <li><a href="#riemann-int-area-interpret">Area interpretation of the Riemann integral</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="pts-sets">Points, sets</h3>
<p>A <strong>point</strong> $x\in\mathbb{R}^d$ consists of a $d$-tuple of real numbers
\begin{equation}
x=\left(x_1,x_2,\dots,x_d\right),\hspace{1cm}x_i\in\mathbb{R}, i=1,\dots,d
\end{equation}
Addition between points and multiplication of a point by a real scalar is elementwise.</p>

<p>The <strong>norm</strong> of $x$ is denoted by $\vert x\vert$ and is defined to be the standard <strong>Euclidean norm</strong> given by
\begin{equation}
\vert x\vert=\left(x_1^2+\dots+x_d^2\right)^{1/2}
\end{equation}
We can then calculate the <strong>distance</strong> between two points $x$ and $y$, which is
\begin{equation}
\text{dist}(x,y)=\vert x-y\vert
\end{equation}
The <strong>complement</strong> of a set $E$ in $\mathbb{R}^d$ is denoted as $E^c$, and defined by
\begin{equation}
E^c=\{x\in\mathbb{R}^d:x\notin E\}
\end{equation}
If $E$ and $F$ are two subsets of $\mathbb{R}^d$, we denote the complement of $F$ in $E$ by
\begin{equation}
E-F=\{x\in\mathbb{R}^d:x\in E;\,x\notin F\}
\end{equation}
The <strong>distance</strong> between two sets $E$ and $F$ is defined by
\begin{equation}
\text{dist}(E,F)=\inf_{x\in E,\,y\in F}\vert x-y\vert
\end{equation}</p>

<h3 id="open-closed-compact-sets">Open, closed and compact sets</h3>
<p>The <strong>open ball</strong> in $\mathbb{R}^d$ centered at $x$ and of radius $r$ is defined by
\begin{equation}
B(x,r)=\{y\in\mathbb{R}^d:\vert y-x\vert&lt; r\}
\end{equation}
A subset $E\subset\mathbb{R}^d$ is <strong>open</strong> if for every $x\in E$ there exists $r&gt;0$ with $B(x,r)\subset E$. And a set is <strong>closed</strong> if its complement is open.<br />
Any (not necessarily countable) union of open sets is open, while in general, the intersection of only finitely many open sets is open. A similar statement holds for the class of closed sets, if we interchange the roles of unions and intersections.</p>

<p>A set $E$ is <strong>bounded</strong> if it is contained in some ball of finite radius. A set is <strong>compact</strong> if it is bounded and is also closed. Compact sets enjoy the <strong>Heine-Borel</strong> covering property:</p>

<p><span id="heine-borel-theorem"><strong>Theorem 1</strong>. (<strong>Heine-Borel theorem</strong>)</span><br />
<em>Assume $E$ is compact, $E\subset\bigcup_\alpha\mathcal{O}_\alpha$, and each $\mathcal{O}_\alpha$ is open. Then there are finitely many of the open sets $\mathcal{O}_{\alpha_1},\mathcal{O}_{\alpha_2},\dots,\mathcal{O}_{\alpha_N}$, such that $E\subset\bigcup_{j=1}^{N}\mathcal{O}_{\alpha_j}$.</em></p>

<p>In words, <em>any</em> covering of a compact set by a collection of open sets contains a <em>finite</em> subcovering.</p>

<p>A point $x\in\mathbb{R}^d$ is a <strong>limit point</strong> of the set $E$ if for every $r&gt;0$, the ball $B(x,r)$ contains points of $E$. This means that there are points in $E$ which are arbitrarily close to $x$. An <strong>isolated point</strong> of $E$ is a point $x\in E$ such that there exists an $r&gt;0$ where $B(x,r)\cap E=\{x\}$.</p>

<p>A point $x\in E$ is an <strong>interior point</strong> of $E$ if there exists $r&gt;0$ such that $B(x,r)\subset E$. The set of all interior points of $E$ is called the <strong>interior</strong> of $E$.</p>

<p>The <strong>closure</strong> of $E$, denoted as $\bar{E}$, consists the union of $E$ and all its limit points. The <strong>boundary</strong> of $E$, denoted as $\partial E$, is the set of points which are in the closure of $E$ but not in the interior of $E$.</p>

<p>A closed set $E$ is <strong>perfect</strong> if $E$ does not have any isolated point.</p>

<p>The <strong>boundary</strong> of $E$, denoted by $\partial E$, is the set of points in $\bar{E}$ not belonging to the interior of $E$.</p>

<p><strong>Remark</strong>:</p>
<ul>
  <li>The closure of a set is a closed set.</li>
  <li>Every point in $E$ is a limit point of $E$.</li>
  <li>A set is closed iff it contains all its limit points.</li>
</ul>

<h3 id="rects-cubes">Rectangles, cubes</h3>
<p>A (closed) <strong>rectangle</strong> $R$ in $\mathbb{R}^d$ is given by the product of $d$ one-dimensional closed and bounded intervals
\begin{equation}
R\doteq[a_1,b_1]\times[a_2,b_2]\times\ldots\times[a_d,b_d],
\end{equation}
where $a_j\leq b_j$, for $j=1,\ldots,d$, are real numbers. In other words, we have
\begin{equation}
R=\left\{\left(x_1,\ldots,x_d\right)\in\mathbb{R}^d:a_j\leq x_j\leq b_j,\forall j=1,\ldots,d\right\}
\end{equation}
With this definition, a rectangle is closed and has sides parallel to the coordinate axis. In $\mathbb{R}$, the rectangles are the closed and bounded intervals; they becomes the usual rectangles as we usually see in $\mathbb{R}^2$; while in $\mathbb{R}^3$, they are the closed parallelepipeds.</p>
<figure>
	<img src="/assets/images/2022-06-16/rectangles.png" alt="Rectangles in R^d" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Rectangles in $\mathbb{R}^d,d=1,2,3$</figcaption>
</figure>

<p>The lengths of the sides of the rectangle $R$ in $\mathbb{R}^d$ are $b_1-a_1,\ldots,b_d-a_d$. The <strong>volume</strong> of its, denoted as $\vert R\vert$, is defined as
\begin{equation}
\vert R\vert\doteq(b_1-a_1)\dots(b_d-a_d)
\end{equation}
An open rectangle is the product of open intervals, and the interior of the rectangle $R$ is then
\begin{equation}
(a_1,b_1)\times\ldots\times(a_d,b_d)
\end{equation}
A <strong>cube</strong> is a rectangle for which $b_1-a_1=\ldots=b_d-a_d$.</p>

<p>A union of rectangles is said to be <strong>almost disjoint</strong> if the interiors of the rectangles are disjoint.</p>

<p><strong>Lemma 2</strong><br />
<em>If a rectangle is the almost disjoint union of finitely many other rectangles, say $R=\bigcup_{k=1}^{N}R_k$, then</em>
\begin{equation}
\vert R\vert=\sum_{k=1}^{N}\vert R_k\vert
\end{equation}</p>

<p><strong>Lemma 3</strong><br />
<em>If $R,R_1,\ldots,R_N$ are rectangles, and $R\subset\bigcup_{k=1}^{N}R_k$, then</em>
\begin{equation}
\vert R\vert\leq\sum_{k=1}^{N}\vert R_k\vert
\end{equation}</p>

<p><strong>Theorem 4</strong><br />
<em>Every open $\mathcal{O}\subset\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals</em>.</p>

<p><strong>Theorem 5</strong><br />
<em>Every open $\mathcal{O}\subset\mathbb{R}^d,d\geq 1$, can be written as countable union of almost disjoint closed cubes</em>.</p>

<h3 id="cantor-set">The Cantor set</h3>
<p>Let $C_0=[0,1]$ denote the closed unit interval and let $C_1$ represent the set obtained from deleting the middle third open interval from $[0,1]$, as
\begin{equation}
C_1=[0,1/3]\cup[2/3,1]
\end{equation}
We repeat this procedure of deleting the middle third open interval for each subinterval of $C_1$. In the second stage we obtain
\begin{equation}
C_1=[0,1/9]\cup[2/9,1/3]\cup[2/3,7/9]\cup[8/9,1]
\end{equation}
We continue to repeat this process for each subinterval of $C_2$, and so on. The result of this process is a sequence $(C_k)_{k=0,1,\ldots}$ of compact sets with
\begin{equation}
C_0\supset C_1\supset C_2\supset\ldots\supset C_k\supset C_{k+1}\supset\ldots
\end{equation}
The <strong>Cantor set</strong> $\mathcal{C}$ is defined as the intersection of all $C_k$’s
\begin{equation}
\mathcal{C}=\bigcap_{k=0}^{\infty}C_k
\end{equation}
The set $\mathcal{C}$ is not empty, since all end-points of the intervals in $C_k$ (all $k$) belong to $\mathcal{C}$.</p>

<h3 id="others">Others</h3>
<p>Given any sequence $x_1,x_2,\ldots\in[0,+\infty]$. We can always form the sum
\begin{equation}
\sum_{n=1}^{n}x_n\in[0,+\infty]
\end{equation}
as the limit of the partial sums $\sum_{n=1}^{N}x_n$, which may be either finite or infinite. An equivalence definition of this infinite sum is as the supremum of all finite subsums:
\begin{equation}
\sum_{n=1}^{\infty}x_n=\sup_{F\subset\mathbb{N},F\text{ finite}}\sum_{n\in F}x_n
\end{equation}
From this equation, given any collection $(x_\alpha)_{\alpha\in A}$ of numbers $x_\alpha\in[0,+\infty]$ indexed by an arbitrary set $A$, we can define the sum $\sum_{\alpha\in A}x_\alpha$ as
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sup_{F\subset A,F\text{ finite}}\sum_{\alpha\in F}x_\alpha\label{eq:others.1}
\end{equation}
Or moreover, given any bijection $\phi:B\to A$, we has the change of variables formula
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sum_{\beta\in B}x_{\phi(\beta)}
\end{equation}</p>

<p><span id="tonelli-theorem"><strong>Theorem 6</strong>. (<strong>Tonelli’s theorem for series</strong>)</span><br />
<em>Let $(x_{n,m})_{n,m\in\mathbb{N}}$ be a doubly infinite sequence of extended nonnegative reals $x_{n,m}\in[0,+\infty]$. Then</em>
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}x_{n,m}
\end{equation}</p>

<p><strong>Proof</strong><br />
We will prove the equality between the first and second expression, the proof for the equality between the first and the third one is similar.</p>

<p>We begin by showing that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Let $F\subset\mathbb{N}^2$ be any finite set. Then $F\subset\{1,\ldots,N\}\times\{1,\ldots,N\}$ for some finite $N$. Since $x_{n,m}$ are nonnegative, we have
\begin{align}
\sum_{(n,m)\in F}x_{n,m}&amp;\leq\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,\ldots,N\}}x_{n,m} \\ &amp;=\sum_{n=1}^{N}\sum_{m=1}^{N}x_{n,m} \\ &amp;\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{align}
for any finite subset $F$ of $\mathbb{R}^2$. Then by \eqref{eq:others.1}, we have
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sup_{F\subset\mathbb{N}^2,F\text{ finite}}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
The problem now remains to prove that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{equation}
which will be proved if we can show that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Fix $N$, we have since each $\sum_{m=1}^{\infty}$ is the limit of $\sum_{m=1}^{M}x_{n,m}$, LHS is the limit of $\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}$ as $M\to\infty$. Thus, it suffices to show that for each finite $M$
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}=\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,ldots,M\}}x_{n,m}
\end{equation}
which is true for all finite $M,N$. And it concludes our proof.</p>

<p><strong>Axiom 7</strong>. (<strong>Axiom of choice</strong>)<br />
<em>Let $(E_\alpha)_{\alpha\in A}$ be a family of non-empty set $E_\alpha$, indexed by an index set $A$. Then we can find a family $(x_\alpha)_{\alpha\in A}$ of elements $x_\alpha$ of $E_\alpha$, indexed by the same set $A$.</em></p>

<p><span id="countable-choice-axiom"><strong>Corollary 8</strong>. (<strong>Axiom of countable choice</strong>)</span><br />
<em>Let $E_1,E_2,\ldots$ be a sequence of non-empty sets. Then we can find a sequence $x_1,x_2,\ldots$ such that $x_n\in E_n,\forall n=1,2,\ldots$.</em></p>

<h2 id="elementary-measure">Elementary measure</h2>

<h3 id="intervals-boxes-elementary-sets">Intervals, boxes, elementary sets</h3>
<p>An <strong>interval</strong> is a subset of $\mathbb{R}$ having one of the forms
\begin{align}
[a,b]&amp;\doteq\{x\in\mathbb{R}:a\leq x\leq b\}, \\ [a,b)&amp;\doteq\{x\in\mathbb{R}:a\leq x\lt b\}, \\ (a,b]&amp;\doteq\{x\in\mathbb{R}:a\lt x\leq b\}, \\ (a,b)&amp;\doteq\{x\in\mathbb{R}:a\lt x\lt b\},
\end{align}
where $a\leq b$ are real numbers.<br />
The <strong>length</strong> of an interval $I=[a,b],[a,b),(a,b],(a,b)$ is denoted as $\vert I\vert$ and is defined by
\begin{equation}
\vert I\vert\doteq b-a
\end{equation}
A <strong>box</strong> in $\mathbb{R}^d$ is a Cartesian product $B\doteq I_1\times\ldots\times I_d$ of $d$ intervals $I_1,\ldots,I_d$ (not necessarily the same length). The <strong>volume</strong> $\vert B\vert$ of such a box $B$ is defined as
\begin{equation}
\vert B\vert\doteq \vert I_1\vert\times\ldots\times\vert I_d\vert
\end{equation}
An <strong>elementary set</strong> is any subset of $\mathbb{R}^d$ which is the union of a finite number of boxes.</p>

<p><strong>Remark 9</strong> (<strong>Boolean closure</strong>)<br />
If $E,F\subset\mathbb{R}^d$ are elementary sets, then</p>
<ul>
  <li>the union $E\cup F$,</li>
  <li>the intersection $E\cap F$,</li>
  <li>the set theoretic difference $E\backslash F\doteq\{x\in E:x\notin F\}$,</li>
  <li>the symmetric difference $E\Delta F\doteq(E\backslash F)\cup(F\backslash E)$ 
are also elementary,</li>
  <li>if $x\in\mathbb{R}^d$, then the translate $E+x\doteq\{y+x:y\in E\}$ is also an elementary set.</li>
</ul>

<p><strong>Proof</strong><br />
With their definitions as elementary sets, we can assume that
\begin{align}
E&amp;=B_1\cup\ldots\cup B_k, \\ F&amp;=B_1’\cup\ldots\cup B_{k’}’,
\end{align}
where each $B_i$ and $B_i’$ is a $d$-dimensional box. By set theory, we have that</p>
<ul>
  <li>The union of $E$ and $F$ can be written as
\begin{equation}
E\cup F=B_1\cup\ldots\cup B_k\cup B_1’\cup\ldots\cup B_{k’}’,
\end{equation}
which is an elementary set.</li>
  <li>The intersection of $E$ and $F$ can be written as
\begin{align}
E\cap F&amp;=\left(B_1\cup\ldots\cup B_k\right)\cup\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\cap B_j’\right),
\end{align}
which is also an elementary set.</li>
  <li>The set theoretic difference of $E$ and $F$ can be written as
\begin{align}
E\backslash F&amp;=\left(B_1\cup\ldots\cup B_k\right)\backslash\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right),
\end{align}
which is, once again, an elementary set.</li>
  <li>With this display, the symmetric difference of $E$ and $F$ can be written as
\begin{align}
E\Delta F&amp;=\left(E\backslash F\right)\cup\left(F\backslash E\right) \\ &amp;=\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right)\Bigg]\cup\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_j’\backslash B_i\right)\Bigg],
\end{align}
which satisfies conditions of an elementary set.</li>
  <li>Since $B_i$’s are $d$-dimensional boxes, we can express them as
\begin{equation}
B_i=I_{i,1}\times\ldots I_{i,d},
\end{equation}
where each $I_{i,j}$ is an interval in $\mathbb{R}^d$. Without loss of generality, we assume that they are all closed. In particular, for $j=1,\ldots,d$
\begin{equation}
I_{i,j}=(a_{i,j},b_{i,j})
\end{equation}
Thus, for any $x\in\mathbb{R}^d$, we have that
\begin{align}
E+x&amp;=\left\{y+x:y\in E\right\} \\ &amp;=\Big\{y+x:y\in B_1\cup\ldots\cup B_k\Big\} \\ &amp;=\Big\{y+x:y\in\bigcup_{i=1}^{k}B_i\Big\} \\ &amp;=\left\{y+x:y\in\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j},b_{i,j})\right\} \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j}+x,b_{i,j}+x),
\end{align}
which is an elementary set.</li>
</ul>

<h3 id="measure-elementary-set">Measure of an elementary set</h3>
<p><strong>Lemma 10</strong><br />
<em>Let $E\subset\mathbb{R}^d$ be an elementary set</em>.</p>
<ul id="roman-list" style="font-style: italic;">
	<li>$E$ <i>can be expressed as the finite union of disjoint boxes.</i></li>
	<li>If $E$ is partitioned as the finite union $B_1\cup\ldots\cup B_k$ of disjoint boxes, then the quantity $m(E)\doteq\vert B_1\vert+\ldots+\vert B_k\vert$ is independent of the partition. In other words, given any other partition $B_1'\cup\ldots\cup B_{k'}'$ of $E$, we have</li>
	\begin{equation}
	\vert B_1\vert+\ldots+\vert B_k\vert=\vert B_1'\vert+\ldots+\vert B_{k'}'\vert
	\end{equation}
</ul>

<p>We refer to $m(E)$ as the <strong>elementary measure</strong> of $E$.</p>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>Consider the one-dimensional case, with these $k$ intervals, we can put their $2k$ endpoints into an increasing-order list (discarding repetitions). By looking at the open intervals between these end points, together with the endpoints themselves (viewed as intervals of length zero), we see that there exists a finite collection of disjoint intervals $J_1,\dots,J_{k'}$, such that each of the $I_1,\dots,I_k$ are union of some collection of the $J_1,\dots,J_{k'}$. And since each interval is a one-dimensional box, our statement has been proved with $d=1$.<br />
	In order to prove the multi-dimensional case, we begin by expressing $E$ as
	\begin{equation}
	E=\bigcap_{i=1}^{k}B_i,
	\end{equation}
	where each box $B_i=I_{i,1}\times\dots\times I_{i,d}$. For each $j=1,\dots,d$, since we has proved the one-dimensional case, we can express $I_{1,j},\dots I_{k,j}$ as the union of subcollections of collections $J_{1,j},\dots,J_{k',j}$ of disjoint intervals. Taking Cartesian product, we can express the $B_1,\dots,B_k$ as finite unions of box $J_{i_1,1}\times\dots\times J_{i_d,d}$, where $1\leq i_j\leq k_j'$ for all $1\leq j\leq d$. Moreover such boxes are disjoint, which proved our argument.</li>
	<li> We have that the length for an interval $I$ can be computed as
	\begin{equation}
	\vert I\vert=\lim_{N\to\infty}\frac{1}{N}\#\left(I\cap\frac{1}{N}\mathbb{Z}\right),
	\end{equation}
	where $\#A$ represents the cardinality of a finite set $A$ and 
	\begin{equation}
	\frac{1}{N}\mathbb{Z}\doteq\left\{\frac{x}{N}:x\in\mathbb{Z}\right\}
	\end{equation}
	Thus, volume of the box, say $B$, established from $d$ intervals $I_1,\dots,I_d$ by taking Cartesian product of them can be written as
	\begin{equation}
	\vert B\vert=\lim_{N\to\infty}\frac{1}{N^d}\#\left(B\cap\frac{1}{N}\mathbb{Z}^d\right)
	\end{equation}
	Therefore, with $k$ disjoint boxes $B_1,\dots,B_k$, we have that
	\begin{align}
	\vert B_1\vert+\dots+\vert B_k\vert&amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k}B_i\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cap\frac{1}{N}\mathbb{Z}^d\right) \\\\ &amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k'}B_i'\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;=\vert B_1'\vert+\dots+\vert B_{k'}'\vert
	\end{align}
	</li>
</ul>

<h3 id="elementary-measure-properties">Properties of elementary measure</h3>
<p>From the definition of elementary measure, it is easily seen that, for any elementary sets $E$ and $F$ (not necessarily disjoint),</p>
<ul id="number-list">
	<li>
		$m(E)$ is a nonnegative real number (<b>non-negativity</b>), and has <b>finite additivity property</b>:
		\begin{equation}
		m(E\cup F)=m(E)+m(F)
		\end{equation}
		And by induction, it also implies that
		\begin{equation}
		m(E_1\cup\dots\cup E_k)=m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,E_k$ are disjoint elementary sets.
	</li>
	<li>
		$m(\emptyset)=0$.
	</li>
	<li>
		$m(B)=\vert B\vert$ for all box $B$.
	</li>
	<li>
		From non-negativity, finite additivity and <b>Remark 9</b>, we conclude the <b>monotonicity</b> property, i.e., $E\subset F$ implies that
		\begin{equation}
		m(E)\leq m(F)
		\end{equation}
	</li>
	<li>
		From the above and finite additivity, we also obtain the <b>finite subadditivity</b> property
		\begin{equation}
		m(E\cup F)\leq m(E)+m(F)
		\end{equation}
		And by induction, we then have
		\begin{equation}
		m(E_1\cup\dots\cup E_k)\leq m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,
		E_k$ are elementary sets (not necessarily disjoint).
	</li>
	<li>
		We also have the <b>translation invariance</b> property
		\begin{equation}
		m(E+x)=m(E),\hspace{1cm}\forall x\in\mathbb{R}^d
		\end{equation}
	</li>
</ul>

<h3 id="uniqueness-elementary-measure">Uniqueness of elementary measure</h3>
<p>Let $d\geq 1$ and let $m’:\mathcal{E}(\mathbb{R}^d)\to\mathbb{R}^+$ be a map from the collection $\mathcal{E}(\mathbb{R}^d)$ of elementary subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity, and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all elementary sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.</p>

<p><strong>Proof</strong><br />
Set $c\doteq m’([0,1)^d)$, we then have that $c\in\mathbb{R}^+$ by the non-negativity property. Using the translation invariance property, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)=\dots=m’\left(\left[\frac{n-1}{n},1\right)^d\right)
\end{equation}
On other hand, using the finite additivity property, for any positive integer $n$, we obtain that
\begin{align}
m’([0,1)^d)&amp;=m’\left(\left[0,\frac{1}{n}\right)^d\cup\left[\frac{1}{n},\frac{2}{n}\right)^d\cup\dots\cup\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;=m’\left(\left[0,\frac{1}{n}\right)^d\right)+m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)+\dots+m’\left(\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;=n m’\left(\left[0,\frac{1}{n}\right)^d\right)
\end{align}
Thus,
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{c}{n},\hspace{1cm}\forall n\in\mathbb{Z}^+
\end{equation}
Moreover, since $m\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{1}{n}$, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=cm\left(\left[0,\frac{1}{n}\right)^d\right)
\end{equation}
It then follows by induction that
\begin{equation}
m’(E)=cm(E)
\end{equation}</p>

<p><strong>Remark 11</strong><br />
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be elementary sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also elementary, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.</p>

<p><strong>Proof</strong><br />
Without loss of generality, assume that $d_1\leq d_2$. With their definitions as elementary sets, we can assume that
\begin{align}
E_1&amp;=B_1\cup\dots\cup B_{k_1}, \\ E_2&amp;=B_1’\cup\dots\cup B_{k_2}’,
\end{align}
where each $B_i$ is a $d_1$-dimensional box while each $B_i’$ is a $d_2$-dimensional box. And using <strong>Lemma 5</strong>, without loss of generality, we can assume that $B_i$ are disjoint boxes and $B_i’$ are also disjoint, which implies that
\begin{align}
m^{d_1}(E_1)&amp;=m^{d_1}(B_1)+\dots+m^{d_1}(B_{k_1}),\label{eq:remark11.1} \\ m^{d_2}(E_2)&amp;=m^{d_2}(B_1’)+\dots+m^{d_2}(B_{k_2}’)\label{eq:remark11.2}
\end{align}
By set theory, we have that
\begin{align}
E_1\times E_2&amp;=\Big(B_1\cup\dots\cup B_{k_1}\Big)\times\Big(B_1’\cup\dots\cup B_{k_2}’\Big) \\ &amp;=\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right),\label{eq:remark11.3}
\end{align}
which is an elementary set.</p>

<p>Since $B_1,\dots,B_{k_1}$ are disjoint and $B_1’,\dots,B_{k_2}’$ are disjoint, the Cartesian products $B_i\times B_j’$ for $i=1,\dots,k_1$ and $j=1,\dots,k_2$ are also disjoint. From \eqref{eq:remark11.3} and using the finite additivity property, we have that
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;=m^{d_1+d_2}\Bigg(\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right)\Bigg) \\ &amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right)\label{eq:remark11.4}
\end{align}
On the one hand, using the definition of boxes, and without loss of generality we can express, for each $i=1,\dots,k_1$, that:
\begin{equation}
B_i=(a_{i,1},b_{i,1})\times\dots\times(a_{i,d_1},b_{i,d_1}),
\end{equation}
where $a_{i,j},b_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_1$. Hence,
\begin{equation}
m^{d_1}(B_i)=\prod_{j=1}^{d_1}(b_{i,j}-a_{i,j}),\hspace{1cm}i=1,\dots,k_1\label{eq:remark11.5}
\end{equation}
Similarly, we also have that
\begin{equation}
m^{d_2}(B_i’)=\prod_{j=1}^{d_2}(d_{i,j}-c_{i,j}),\hspace{1cm}i=1,\dots,k_2\label{eq:remark11.6}
\end{equation}
where $c_{i,j},d_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_2$.</p>

<p>Moreover, on the other hand, we also have that the $(d_1+d_2)$-dimensional box $B_i\times B_j’$ can be expressed as
\begin{equation}
B_i\times B_j’=(e_1,f_1)\times\dots\times(e_{d_1+d_2},f_{d_1+d_2}),\label{eq:remark11.7}
\end{equation}
where $e_k=a_{i,k};f_k=b_{i,k}$ for all $k=1,\dots,d_1$ and $e_k=c_{j,k-d_1};f_k=d_{j,k-d_1}$ for all $k=d_1+1,\dots,d_2$.</p>

<p>From \eqref{eq:remark11.5}, \eqref{eq:remark11.6} and \eqref{eq:remark11.5}, for any $i=1,\dots,k_1$ and for any $j=1,\dots,k_2$, we have
\begin{align}
m^{d_1+d_2}(B_i\times B_j’)&amp;=\prod_{k=1}^{d_1+d_2}(f_k-e_k) \\ &amp;=\Bigg(\prod_{k=1}^{d_1}(b_{i,k}-a_{i,k})\Bigg)\Bigg(\prod_{k=1}^{d_2}(d_{j,k}-c_{j,k})\Bigg) \\ &amp;=m^{d_1}(B_i)\times m^{d_2}(B_j’)
\end{align}
With this result, combined with \eqref{eq:remark11.1} and \eqref{eq:remark11.2}, equation \eqref{eq:remark11.4} can be written as
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right) \\ &amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1}(B_i)\times m^{d_2}(B_j’) \\ &amp;=m^{d_1}(E_1)\times m^{d_2}(E_2),
\end{align}
which concludes our proof.</p>

<h2 id="jordan-measure">Jordan measure</h2>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set.</p>
<ul>
  <li>The <strong>Jordan inner measure</strong> $m_{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m_{*,(J)}(E)\doteq\sup_{A\subset E,A\text{ elementary}}m(A)
\end{equation}</li>
  <li>The <strong>Jordan outer measure</strong> $m^{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E,B\text{ elementary}}m(B)
\end{equation}</li>
  <li>If $m_{*,(J)}(E)=m^{*,(J)}(E)$, then we say that $E$ is <strong>Jordan measurable</strong>, and call
\begin{equation}
m(E)\doteq m_{*,(J)}(E)=m^{*,(J)}(E)
\end{equation}
the <strong>Jordan measure</strong> of $E$.</li>
</ul>

<h3 id="jordan-measurability-characterisation">Characterisation of Jordan measurability</h3>
<p>Let $E\subset\mathbb{R}^d$ be bounded. These following statements are equivalence</p>
<ul id="number-list">
	<li>$E$ is Jordan measurable.</li>
	<li>For every $\varepsilon&gt;0$, there exists elementary sets $A\subset E\subset B$ such that $m(B\backslash A)\leq\varepsilon$.</li>
	<li>For every $\varepsilon&gt;0$, there exists an elementary set $A$ such that $m^{*,(J)}(A\Delta E)\leq\varepsilon$.</li>
</ul>

<p><strong>Proof</strong><br />
In order to prove these three statements are equivalence, we will be proving that (1) implies (2); (2) implies (3); and that (2) implies (1).</p>
<ul>
  <li>(1) implies (2).<br />
Since $E$ is Jordan measurable, we have that
\begin{equation}
m(E)=\sup_{A\subset E;A\text{ elementary}}m(A)=\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
By the definition of supremum, there exists an elementary set $A\subset E$ such that for any $\varepsilon&gt;0$ 
\begin{equation}
m(A)\geq m(E)-\frac{\varepsilon}{2}\label{eq:jmc.1}
\end{equation}
In addition, by the definition of infimum, there also exists an elementary set $B\supset E$ such that for any $\varepsilon&gt;0$
\begin{equation}
m(B)\leq m(E)+\frac{\varepsilon}{2}\label{eq:jmc.2}
\end{equation}
From \eqref{eq:jmc.1} and \eqref{eq:jmc.2}, we have that for any $\varepsilon&gt;0$
\begin{equation}
m(B\backslash A)=m(B)-m(A)\leq\varepsilon
\end{equation}</li>
  <li>(2) implies (3).<br />
With (2) satisfied, we have that we can find elementary sets $A\subset E\subset B$ such that
\begin{equation}
m(B\backslash A)\leq\varepsilon,\hspace{1cm}\forall\varepsilon&gt;0
\end{equation}
Since $A\subset E\subset B$ and by the definition of symmetric difference, we have
\begin{equation}
A\Delta E=(A\backslash E)\cup(E\backslash A)=(E\backslash A)\subset(B\backslash A)
\end{equation}
Hence
\begin{equation}
m^{*,(J)}(A\Delta E)\leq m(B\backslash A)\leq\varepsilon
\end{equation}</li>
  <li>(2) implies (1).<br />
Let $(A_n)_{n\in\mathbb{N}}$ and $(B_n)_{n\in\mathbb{N}}$ be sequences of elementary sets such that $A_n\subset E\subset B_n$ for all $n\in\mathbb{N}$. Statement (2) says that for all $\varepsilon&gt;0$, there exists $i,j\in\mathbb{N}$ such that
\begin{equation}
m(B_j\backslash A_i)\leq\varepsilon
\end{equation}
or
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon\label{eq:jmc.3}
\end{equation}
Let $A_\text{sup}$ and $B_\text{inf}$ be two sets in the two sequences above with
\begin{align}
m(A_\text{sup})&amp;=\sup_{n\in\mathbb{N}}m(A_n), \\ m(B_\text{inf})&amp;=\inf_{n\in\mathbb{N}}m(B_n),
\end{align}
which means
\begin{align}
m_{*,(J)}(E)&amp;=m(A_\text{sup}) \\ m^{*,(J)}(E)&amp;=m(B_\text{inf})
\end{align}
Using the monotonicity property of elementary measure, we have that
\begin{equation}
m(A_\text{sup})\leq m(B_\text{inf})
\end{equation}
Assume that $m(B_\text{inf})&gt;m(A_\text{sup})$, and consider an $\varepsilon&gt;0$ such that $\varepsilon&lt; m(B_\text{inf})-m(A_\text{sup})$. We can continue to derive \eqref{eq:jmc.3} as
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon&lt; m(A_i)+m(B_\text{inf})-m(A_\text{sup})&lt; m(B_\text{inf}),
\end{equation}
which is false with the definition of $B_\text{inf}$. Therefore, our assumption is also false, which means
\begin{equation}
m(A_\text{sup})=m(B_\text{inf})
\end{equation}
or
\begin{equation}
m_{*,(J)}(E)=m^{*,(J)}(E),
\end{equation}
or in other words, $E$ is Jordan measurable.</li>
</ul>

<p><strong>Corollary 12</strong></p>
<ul>
  <li>Every elementary set $E$ is Jordan measurable.</li>
  <li>On elementary sets, Jordan measure is elementary measure.</li>
</ul>

<p>Jordan measurability also inherits many of the properties of elementary measure.</p>

<h3 id="jordan-measurability-properties">Properties of Jordan measurability</h3>
<p>Let $E,F\in\mathbb{R}^d$ be Jordan measurable sets. Then</p>
<ul id="number-list">
	<li>
		<b>Boolean closure</b>. $E\cup F,E\cap F,E\backslash F,E\Delta F$ are also Jordan measurable sets.
	</li>
	<li>
		<b>Non-negativity</b>. $m(E)\geq 0$.
	</li>
	<li>
		<b>Finite additivity</b>. If $E,F$ are disjoint, then $m(E\cup F)=m(E)+m(F)$.
	</li>
	<li>
		<b>Monotonicity</b>. If $E\subset F$, then $m(E)\leq m(F)$.
	</li>
	<li>
		<b>Finite subadditivity</b>. $m(E\cup F)\leq m(E)+m(F)$.
	</li>
	<li>
		<b>Translation invariance</b>. For any $x\in\mathbb{R}^d$, $E+x$ is Jordan measurable, and $m(E+x)=m(E)$.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ol id="number-list">
	<li>
		<b>Boolean closure</b>.
		<ul>
			<li>
				By characterisation of Jordan measurability, we can find elementary sets $A_1\subset E\subset B_1$ and $A_2\subset F\subset B_2$ such that for any $\varepsilon&gt;0$
				\begin{align}
				m(B_1\backslash A_1)&amp;\leq\frac{\varepsilon}{2}, \\ m(B_2\backslash A_2)&amp;\leq\frac{\varepsilon}{2}
				\end{align}
				Thus, we have that
				\begin{equation}
				\left(A_1\cap A_2\right)\subset\left(E\cap F\right)\subset\left(B_1\cap B_2\right)
				\end{equation}
				and
				\begin{equation}
				\left(A_1\cup A_2\right)\subset\left(E\cup F\right)\subset\left(B_1\cup B_2\right)
				\end{equation}
				Moreover, for any $\varepsilon&gt;0$, we have that
				\begin{align*}
				m\big((B_1\cup B_2)\backslash(A_1\cup A_2)\big)&amp;=m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;=m(B_1)+m(B_2\backslash B_1)-m(A_1\cup A_2) \\ &amp;\leq m(B_1)+m(B_2\backslash A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1)-m(A_1)+m(B_2\backslash A_1)+m(A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1)-m(A_1)+m(B_2\cup A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1\backslash A_1)+m\big((B_2\cup A_1)\backslash(A_1\cup A_2)\big) \\ &amp;=m(B_1\backslash A_1)+m(B_2\backslash A_2) \\ &amp;\leq\varepsilon/2+\varepsilon/2 \\ &amp;=\varepsilon,
				\end{align*}
				which implies that $E\cup F$ is Jordan measurable.
			</li>
			<li>
				From the result above, and by monotonicity, finite additivity, finite subadditivity properties of elementary measure, for any $\varepsilon&gt;0$, we also have that
				\begin{align*}
				m\big((B_1\cap B_2)\backslash(A_1\cap A_2)\big)&amp;=m(B_1\cap B_2)-m(A_1\cap A_2) \\ &amp;=m\Big(\big(B_1\cup B_2\big)\backslash\big((B_1\backslash B_2)\cup(B_2\backslash B_1)\big)\Big) \\ &amp;\hspace{1cm}-m\Big(\big(A_1\cup A_2\big)\backslash\big((A_1\backslash A_2)\cup(A_2\backslash A_1)\big)\Big) \\ &amp;=m(B_1\cup B_2)-m(B_1\backslash B_2)-m(B_2\backslash B_1) \\ &amp;\hspace{1cm}-m(A_1\cup A_2)+m(A_1\backslash A_2)+m(A_2\backslash A_1) \\ &amp;=m(B_1\cup B_2)-m(A_1\cup A_2)+m(A_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;\hspace{1cm}+m(A_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2)+m(B_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;\hspace{1cm}+m(B_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;\leq\varepsilon,
				\end{align*}
				which also implies that $E\cap F$ is Jordan measurable.
			</li>
			<li></li>
		</ul>
	</li>
	<li>
		<b>Non-negativity</b>.<br />
		Given $E$ being Jordan measurable set, we have
		\begin{equation}
		m(E)=\sup_{A\subset E,A\text{ elementary}}m(A)\geq m(\emptyset)=0,
		\end{equation}
		by the monotonicity property of elementary measure.
	</li>
	<li>
		<b>Finite additivity</b>.<br />
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite additivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset F,A_2\text{ elementary}}m(A_2) \\ &amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1)+m(A_2) \\ &amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2)=m(E\cup F)
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>.<br />
		Given $E\subset F$ are Jordan measurable sets, the we have
		\begin{equation}
		m(E)\leq\sup_{A\subset F,A\text{ elementary}}m(A)=m(F)
		\end{equation}
	</li>
	<li>
		<b>Finite subadditivity</b>.<br />
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite subadditivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset E,A_2\text{ elementary}}m(A_2) \\ &amp;\geq\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2) \\ &amp;=m(E\cup F)=m(E\cup F)
		\end{align}
	</li>
	<li>
		<b>Translation invariance</b>.<br />
		By the translation invariance property of elementary measure, for any $x\in\mathbb{R}^d$, the Jordan inner measure of $E+x$ can be written as
		\begin{align}
		m_{*,(J)}(E+x)&amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A) \\ &amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A-x) \\ &amp;=\sup_{A-x\subset E,A-x\text{ elementary}}m(A-x)=m(E)
		\end{align}
		Similarly, we also have the Jordan outer measure of $E+x$ is also equal to the Jordan measure of $E$
		\begin{equation}
		m^{*,(J)}(E+x)=m(E)
		\end{equation}
		Hence,
		\begin{equation}
		m_{*,(J)}(E+x)=m^{*,(J)}(E+x)=m(E),
		\end{equation}
		or in other words, $E+x$ is Jordan measurable with $m(E+x)=m(E)$.
	</li>
</ol>

<p><strong>Remark 13</strong> (Regions under graphs are Jordan measurable)<br />
Let $B$ be a closed box in $\mathbb{R}^d$, and let $f:B\to\mathbb{R}$ be a continuous function. Then</p>
<ul id="number-list">
	<li>
		The graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ is Jordan measurable in $\mathbb{R}^{d+1}$ with Jordan measure zero.
	</li>
	<li>
		The set $\{(x,t):x\in B;0\leq t\leq f(x)\}\subset\mathbb{R}^{d+1}$ is Jordan measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		For any closed box $C\in\mathbb{R}^d$, we have $\{(x,f(x)):x\in C\}\subset\mathbb{R}^{d+1}$ with $f:C\to\mathbb{R}$ is a compact set. And when $f$ continuous in a compact set we also have $f$ is <span>uniformly continuous<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></span>, which means for any $\varepsilon&gt;0$, there exists $\delta$ such that for every $x,y\in C$
		\begin{equation}
		\vert f(x)-f(y)\vert&lt;\varepsilon,
		\end{equation}
		with $\vert x-y\vert&lt;\delta$. Therefore, we can divide $C$ into finitely many almost disjoint boxes $C_1,\ldots,C_n$ such that $\vert x_i-y_i\vert&lt;\delta$ for every $x_i,y_i\in C_i$ and for any $\varepsilon&gt;0$
		\begin{equation}
		\vert f(x_i)-f(y_i)\vert&lt;\varepsilon
		\end{equation}
		Moreover, for each such box $C_i$ with center of the box $x_i$ we also have
		\begin{equation}
		\left\{(x,f(x)):x\in C_i\right\}\subset C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		Therefore
		\begin{equation}
		\left\{(x,f(x)):x\in C\right\}=\bigcup_{i=1}^{n}\left\{(x,f(x)):x\in C_i\right\}\subset\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		With this result, and by the monotonicity, finite additivity of elementary measure, we have the Jordan outer measure of the graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ can be written as
		\begin{align}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)&amp;=\inf_{C\supset B,C\text{ closed box}}m\left(\left\{(x,f(x)):x\in C\right\}\right) \\ &amp;\leq m^{d+1}\left(\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;=\sum_{i=1}^{n}m^d(C_i)\times m^1\left(\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;=2n\varepsilon m^d(C)&lt;2n\varepsilon\delta
		\end{align}
		And since $\varepsilon&gt;0$ arbitrarily, we finally obtain
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)=0
		\end{equation}
		Plus that, since
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq m_{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq 0,
		\end{equation}
		we have that
		\begin{equation}
		m^{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=m_{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=0,
		\end{equation}
		or in other words, the graph $\left(\{(x,f(x)):x\in B\}\right)$ is Jordan measurable on $\mathbb{R}^{d+1}$ with Jordan measure zero.
	</li>
	<li>
		Let $E=\big\{(x,t):x\in B;0\leq t\leq f(x)\big\}$ and let $I$, $O$ be sets defined as for an arbitrary $\varepsilon&gt;0$
		\begin{align}
		I&amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)-\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)-\frac{\varepsilon}{2}\right], \\ O&amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)+\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]
		\end{align}
		Therefore, it follows immediately that $I\subset E\subset O$ and moreover
		\begin{align}
		m^{d+1}(O\backslash I)&amp;=m^{d+1}\left(B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash B\times\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;=m^d(B)\times m^1\left(\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;=m^d(B)\times\varepsilon
		\end{align}
		And since $\varepsilon&gt;0$ arbitrarily, we can claim that $E$ is Jordan measurable.
	</li>
</ul>

<p><strong>Remark 14</strong></p>
<ul>
	<li>
		All open and closed Euclidean balls, $B(x,r)\doteq\{y\in\mathbb{R}^d:\vert y-x\vert&lt; r\}$ and $\overline{B(x,r)}\doteq\{y\in\mathbb{R}^d:\vert y-x\vert\leq r\}$, in $\mathbb{R}^d$ are Jordan measurable, with Jordan measure $c_dr^d$ for some constant $c_d$ depending only on $d$.
	</li>
	<li>
		Establish the crude bounds
		\begin{equation}
		\left(\frac{2}{\sqrt{d}}\leq c_d\leq 2^d\right)
		\end{equation} 
	</li>
</ul>

<h3 id="jordan-null-sets">Jordan null sets</h3>
<p>A <strong>Jordan null set</strong> is a Jordan measurable set of Jordan measure zero. We have that any subset of a Jordan null set is also a Jordan null set.</p>

<p><strong>Proof</strong><br />
Let $E\subset F$ where F is a Jordan null set. Also let $A\subset E$, it follows that $A\subset F$, and hence
\begin{equation}
m(A)\leq m_{*,(J)}(F)=0
\end{equation}
Since $m(E)=0$, we can choose a set $B\supset F$ such that $m(B)\leq\varepsilon$ for $\varepsilon&gt;0$ arbitrarily. Thus, $E\subset B$ and moreover
\begin{equation}
m(B\backslash A)\leq\varepsilon,
\end{equation}
which claims that $E$ is Jordan measurable with measurable of zero since $m(E)\leq m(F)=0$. Or in other words, $E$ is also a Jordan null set.</p>

<p><strong>Remark 15</strong><br />
For any Jordan measurable set $E\subset\mathbb{R}^d$, its Jordan measure can be written as
\begin{equation}
m(E)\doteq\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cup\frac{1}{N}\mathbb{Z}^d\right)
\end{equation}</p>

<p><strong>Proof</strong></p>

<h3 id="metric-formula-jordan-measurability">Metric entropy formulation of Jordan measurability</h3>
<p>A <strong>dyadic cube</strong> is defined to be a half-open box of the form
\begin{equation}
\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right)\times\ldots\times\left(\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d$. Let $E\subset\mathbb{R}^d$ be a bounded set. For each integer $n$, let $\mathcal{E}_*(E,2^{-n})$ denote the number of dyadic cubes of sidelength $2^{-n}$ that are contained in $E$, and let $\mathcal{E}^*(E,2^{-n})$ be the number of dyadic cubes of sidelength $2^{-n}$ that intersect $E$. Then $E$ is Jordan measurable iff
\begin{equation}
\lim_{n\to\infty}2^{-dn}(\mathcal{E}^*(E,2^{-n}))-\mathcal{E}_*(E, 2^{-n})=0,
\end{equation}
in which case we have
\begin{equation}
m(E)=\lim_{n\to\infty}2^{-dn}\mathcal{E}_*(E,2^{-n})=\lim_{n\to\infty}2^{-dn}\mathcal{E}^*(E,2^{-n})
\end{equation}</p>

<h3 id="uniqueness-jordan-measure">Uniqueness of Jordan measure</h3>
<p>Let $d\geq 1$ and let $m’:\mathcal{J}(\mathbb{R}^d)\to\mathbb{R}^+$  be a map from the collection of Jordan measurable subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all Jordan measurable sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.</p>

<p><strong>Proof</strong><br />
Follow the same steps as the proof of the uniqueness of elementary measure, the argument above can easily be proved.</p>

<p><strong>Remark 16</strong><br />
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be Jordan measurable sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also Jordan measurable, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.</p>

<p><strong>Proof</strong><br />
Let $A_1\subset E_1$ such that $A_1$ is elementary and
\begin{equation}
m^{d_1}(A_1)=\sup_{A\subset E_1,A\text{ elementary}}m(A)=m_{*,(J)}(E_1)=m^{d_1}(E_1)
\end{equation}
Let $B_1\supset E_1$ such that $B_1$ is elementary and
\begin{equation}
m^{d_1}(B_1)=\inf_{B\supset E_1,B\text{ elementary}}m(B)=m^{*,(J)}(E_1)=m^{d_1}(E_1),
\end{equation}
which implies that
\begin{equation}
m^{d_1}(A_1)=m^{d_1}(B_1)=m^{d_1}(E_1)
\end{equation}
Analogously, we define $A_2\subset E_2\subset B_2$ such that
\begin{align}
m^{d_2}(A_2)&amp;=\sup_{A\subset E_2,A\text{ elementary}}m(A)=m_{*,(J)}(E_2)=m^{d_2}(E_2) \\ m^{d_2}(B_2)&amp;=\inf_{B\supset E_2,B\text{ elementary}}m(B)=m^{*,(J)}(E_2)=m^{d_1}(E_2)
\end{align}
And thus, we also have
\begin{equation}
m^{d_2}(A_2)=m^{d_2}(B_2)=m^{d_2}(E_2)
\end{equation}
On the one hand, with these definitions, we have
\begin{equation}
m^{d_1+d_2}(A_1\times A_2)=\sup_{A\subset E_1\times E_2,A\text{ elementary}}=m_{*,(J)}(E_1\times E_2)\label{eq:remark15.1}
\end{equation}
and
\begin{equation}
m^{d_1\times d_2}(B_1\times B_2)=\sup_{B\supset E_1\times E_2,A\text{ elementary}}=m^{*,(J)}(E_1\times E_2)\label{eq:remark15.2}
\end{equation}
On the other hands, By <strong>remark 11</strong>, we have that $A_1\times A_2$ and $B_1\times B_2$ are also elementary sets and
\begin{align}
m^{d_1}(A_1)\times m^{d_2}(A_2)&amp;=m^{d_1+d_2}(A_1\times A_2)\label{eq:remark15.3} \\ m^{d_1}(B_1)\times m^{d_2}(B_2)&amp;=m^{d_1+d_2}(B_1\times B_2)\label{eq:remark15.4}
\end{align}
From \eqref{eq:remark15.1}, \eqref{eq:remark15.2}, \eqref{eq:remark15.3} and \eqref{eq:remark15.4}, we can claim that $E_1\times E_2$ is Jordan measurable and
\begin{equation}
m^{d_1}(E_1)\times m^{d_2}(E_2)=m^{d_1+d_2}(E_1\times E_2)
\end{equation}</p>

<h3 id="topo-jordan-measurability">Topological of Jordan measurability</h3>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set</p>
<ul id="number-list">
	<li>$E$ and the closure $\bar{E}$ of $E$ have the same Jordan outer measure.</li>
	<li>$E$ and the interior $E^\circ$ of $E$ have the same Jordan inner measure.</li>
	<li>$E$ is Jordan measurable iff the <b>topological boundary</b> $\partial E$ of $E$ has Jordan outer measure zero.</li>
	<li>The <b>bullet-riddled square</b> $[0,1]^2\backslash\mathbf{Q}^2$, and set of bullets $[0,1]^2\cup Q^2$, both have Jordan inner measure zero and Jordan outer measure one. In particular, both sets are not Jordan measurable.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		Since $E\subset\overline{E}$, it is easily seen that
		\begin{equation}
		m^{*,(J)}(E)\leq m^{*,(J)}(\overline{E})
		\end{equation}
		Thus, the problem remains to prove that
		\begin{equation}
		m^{*,(J)}(E)\geq m^{*,(J)(\overline{E})}
		\end{equation}
		Let $B_1,\ldots,B_N$ be $N$ disjoint boxes such that
	</li>
</ul>

<h3 id="caratheodory-type-property">Carathéodory type property</h3>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set, and $F\subset\mathbb{R}^d$ be an elementary set. Then we have that
\begin{equation}
m^{*,(J)}(E)=m^{*,(J)}(E\cap F)+m^{*,(J)}(E\backslash F)
\end{equation}</p>

<h2 id="connect-riemann-int">Connection with the Riemann integral</h2>
<p>We then consider the relationship between Jordan measure and the <strong>Rieman integral</strong>, or the equivalent <strong>Darboux integral</strong>.</p>

<h3 id="riemann-integrability">Riemann integrability</h3>
<p>Let $[a,b]$ be an interval of positive length, and $f:[a,b]\to\mathbb{R}$ be a function. A <strong>tagged partition</strong>
\begin{equation}
\mathcal{P}=\left(\left(x_0,x_1,\dots,x_n\right),\left(x_1^{*},\dots,x_n^{*}\right)\right)
\end{equation}
of $[a,b]$ is a finite sequence of real numbers $a=x_0&lt; x_1&lt;\dots&lt; x_n=b$, together with additional numbers $x_{i-1}\leq x_i^{*}\leq x_i$ for each $i=1,\dots,n$. Let $\delta x_i\doteq x_i-x_{i-1}$, the quantity
\begin{equation}
\Delta(\mathcal{P})\doteq\sup_{1\leq i\leq n}\delta x_i
\end{equation}
is called the <strong>norm</strong> of the tagged partition. The <strong>Riemann sum</strong> $\mathcal{R}(f,\mathcal{P})$ of $f$ w.r.t the tagged partition $\mathcal{P}$ is defined as
\begin{equation}
\mathcal{R}(f,\mathcal{P})\doteq\sum_{i=1}^{n}f(x_i^{*})\delta x_i
\end{equation}
we say that $f$ is <strong>Riemann integrable</strong> on $[a,b]$ if there exists a real number, denoted as $\int_{a}^{b}f(x)\,dx$ and referred to as the <strong>Riemann integral</strong> on $[a,b]$, for which we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=\lim_{\Delta\mathcal{P}\to 0}\mathcal{R}(f,\mathcal{P}),
\end{equation}
by which we mean that for every $\varepsilon&gt;0$ there exists $\delta&gt;0$ such that
\begin{equation}
\left\vert\mathcal{R}(f,\mathcal{P})-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon,
\end{equation}
for every tagged partition $\mathcal{P}$ with $\Delta(\mathcal{P})\leq\delta$.</p>

<h3 id="pc-func">Piecewise constant functions</h3>
<p>Let $[a,b]$ be an interval. a <strong>piecewise constant function</strong> $f:[a,b]\to\mathbb{R}$ is a function for which there exists a partition of $[a,b]$ into infinitely many intervals $I_1,\dots,I_n$ such that $f$ is equal to a constant $c_i$ on each of the intervals $I_i$. Then, the expression
\begin{equation}
\sum_{i=1}^{n}c_i\vert I_i\vert
\end{equation}
is independent of the choice of partition used to demonstrate the piecewise constant nature of $f$. We denote this quantity as $\text{p.c.}\int_{a}^{b}f(x)\,dx$, and refer it to as <strong>piecewise constant integral</strong> of $f$ on $[a,b]$.</p>

<p><strong>Proof</strong><br />
Consider two partitions of the interval $[a,b]$ into finitely many intervals $(I_i)_{i=1,\ldots,n}=I_1,\ldots,I_n$ and $(J_i)_{i=1,\ldots,m}=J_1,\ldots,J_m$ such that:
\begin{align}
f(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ f(x)&amp;=d_i,\hspace{1cm}\forall x\in J_i
\end{align}
Thus, we have that:
\begin{equation}
c_i=d_j,\hspace{1cm}\forall x\in\left(I_i\cap J_j\right)
\end{equation}
With this result, we have:
\begin{align}
\sum_{i=1}^{n}c_i\vert I_i\vert&amp;=\sum_{i=1}^{n}c_i\left\vert\bigcup_{j=1}^{m}\left(I_i\cap J_j\right)\right\vert \\ &amp;=\sum_{i=1}^{n}\sum_{j=1}^{m}c_i\left\vert I_i\cap J_j\right\vert \\ &amp;=\sum_{j=1}^{m}\sum_{i=1}^{n}d_j\left\vert I_i\cap J_j\right\vert \\ &amp;=\sum_{j=1}^{m}d_j\left\vert\bigcup_{i=1}^{n}\left(J_j\cap I_i\right)\right\vert \\ &amp;=\sum_{j=1}^{m}d_j\vert J_j\vert,
\end{align}
which claims the independence of the choices of partition of $f$.</p>

<h4 id="pc-int-properties">Basic properties of piecewise constant integral</h4>
<p>Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be piecewise constant functions. Then</p>
<ul id="number-list">
	<li>
		<b>Linearity</b>. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are piecewise constant functions, with
		\begin{align}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx&amp;=c\text{p.c.}\int_{a}^{b}f(x)\,dx \\\\ \text{p.c.}\int_{a}^{b}\left(f(x)+g(x)\right)\,dx&amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>. If $f\leq g$ pointwise, i.e., $f(x)\leq g(x),\forall x\in[a,b]$, then
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx\leq\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>. If $E$ is an elementary subset of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ (defined by setting $1_E(x)\doteq 1$ if $x\in E$ and 0 otherwise) is piecewise constant, and
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		<b>Linearity</b><br />
		For any $c\in\mathbb{R}$, we have:
		\begin{equation}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx=\sum_{i=1}^{n}cc_i\vert I_i\vert=c\sum_{i=1}^{n}c_i\vert I_i\vert=c\text{p.c.}\int_{a}^{b}f(x)\,dx
		\end{equation}
		From the partitioning independence of piecewise constant functions, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{equation}
		f(x)=c_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		and
		\begin{equation}
		g(x)=d_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		Thus, we have
		\begin{align}
		\text{p.c.}\int_{a}^{b}f(x)+g(x)\,dx&amp;=\sum_{i=1}^{n}\left(c_i+d_i\right)\vert I_i\vert \\ &amp;=\sum_{i=1}^{n}c_i\vert I_i\vert+\sum_{i=1}^{n}d_i\vert I_i\vert \\ &amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b><br />
		Analogy to the above proof, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{align}
		f(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ g(x)&amp;=d_i,\hspace{1cm}\forall x\in I_i,
		\end{align}
		Since $f\leq g$ pointwise, in any interval $I_i$, we also have that $c_i=f(x)\leq g(x)=d_i$. Therefore,
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx=\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert=\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b><br />
		Since $E\subset[a,b]\subset\mathbb{R}$ is an elementary set, we can represent the elementary measure $m(E)$ of set $E$ as
		\begin{equation}
		m(E)=\sum_{i=1}^{n}\vert I_i\vert
		\end{equation}
		Therefore, for any $x\in I_i$ for $i=1,\ldots n$, we have that $1_E(x)=1$; and for any $x\in[b-a]\backslash E=\bigcup_{j=1}^{m}J_j$, we get that $1_E(x)=0$, which lets $1_E$ satisfy the condition of a piecewise constant function.<br />
		Moreover, we have that
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=\sum_{i=1}^{n}1\vert I_i\vert+\sum_{j=1}^{m}0\vert J_j\vert=\sum_{i=1}^{n}\vert I_i\vert=m(E)
		\end{equation}
	</li>
</ul>

<h3 id="darboux-int">Darboux integral</h3>
<p>Let $[a,b]$ be an integral, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. The <strong>lower Darboux integral</strong> of $f$ on $[a,b]$, denoted as $\underline{\int_{a}^{b}}f(x)\,dx$, is defined as
\begin{equation}
\underline{\int_a^b}f(x)\,dx\doteq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx,
\end{equation}
where $g$ ranges over all piecewise constant functions that are pointwise bounded above by $f$ (the hypothesis that $f$ is bounded ensures that the supremum is over a non-empty set).</p>

<p>Similarly, we can define the <strong>upper Darboux integral</strong> of $f$ on $[a,b]$, denoted as $\overline{\int_a^b}f(x)\,dx$, as
\begin{equation}
\overline{\int_a^b}f(x)\,dx\doteq\inf_{h\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx
\end{equation}
It is easily seen that $\underline{\int_a^b}f(x)\,dx\leq\overline{\int_a^b}f(x)\,dx$. The equality holds when $f$ is <strong>Darboux integrable</strong>, and we refer to this quantity as <strong>Darboux integral</strong> of $f$ on $[a,b]$.</p>

<p>Note that the upper and lower Darboux integrals are related by
\begin{equation}
\overline{\int_a^b}-f(x)\,dx=-\underline{\int_a^b}f(x)\,dx
\end{equation}</p>

<h4 id="equiv-riemann-darboux-int">Equivalence of Riemann integral and Darboux integral</h4>
<p>Let $[a,b]$ be an interval, and $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff it is Darboux integrable, in which case the Riemann integrals and Darboux integrals are the same.</p>

<p><strong>Proof</strong></p>
<ul>
  <li>Given $f$ is Riemann integrable on $[a,b]$, we have that for any $\varepsilon&gt;0$, there exists a tagged partition $((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*))$ of $[a,b]$ with $x_i^*\in I_i$ such that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
\end{equation}
For each interval $I_i$, there exist an $x_i^{(1)}$ such that for any $\varepsilon&gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i^{(1)})&lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Thus, for any $\varepsilon&gt;0$ we obtain
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert&lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that for any $\varepsilon&gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon\label{eq:erdi.1}
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Combining with \eqref{eq:erdi.1}, we have that as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Moreover, we also have that
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the lower Darboux integral of $f$ on $[a,b]$. Thus,
\begin{equation}
\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx\label{eq:erdi.2}
\end{equation}
Similarly, applying the same procedure as above, we also have that on each $I_i$ there exists an $x_i^{(2)}$ such that for any $\varepsilon&gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Therefore,
\begin{equation}
\sum_{n=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx,
\end{equation}
as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$. Additionally, we also have
\begin{equation}
\sum_{i=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\geq\inf_{h\geq f, \text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx=\overline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the upper Darboux integral of $f$ on $[a,b]$. And hence
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\label{eq:erdi.3}
\end{equation}
From \eqref{eq:erdi.2} and \eqref{eq:erdi.3}, we end up with
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which happens iff
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx=\int_{a}^{b}f(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which claims that $f$ is Darboux integrable on $[a,b]$, with the Darboux integral is exactly the Riemann integral $\int_{a}^{b}f(x)\,dx$.</li>
  <li>Given $f$ is Darboux integrable on $[a,b]$, we have that the upper and lower Darboux integrals are equal, and are equal to the Darboux integral of $f$ on $[a,b]$ which we denote as $\text{d.}\int_{a}^{b}f(x)\,dx\in\mathbb{R}$.
\begin{equation}
\underline{\int_a^b}f(x)\,dx=\overline{\int_a^b}f(x)\,dx=\text{d.}\int_{a}^{b}f(x)\,dx
\end{equation}
By definition of the lower Darboux integral, there exists a piecewise constant function $g(x)$ bounded above by $f$ (i.e., $g\leq f$ piecewise), such that for any $\varepsilon&gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}g(x)\,dx&gt;\underline{\int_{a}^{b}}f(x)\,dx-\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon\label{eq:erdi.4}
\end{equation}
Likewise, by definition of the upper Darboux integral, there exists a piecewise constant function $h(x)$ bounded below by $f$ (i.e., $h\geq f$ piecewise), such that for any $\varepsilon&gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}h(x)\,dx&lt;\overline{\int_{a}^{b}}f(x)\,dx+\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon\label{eq:erdi.5}
\end{equation}
From the independence of choice of partition of piecewise constant functions $g$ and $h$, there exists a partition $I_1,\ldots,I_n$ such that
\begin{align}
g(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ h(x)&amp;=d_i,\hspace{1cm}\forall x\in I_i
\end{align}
and
\begin{align}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;=\sum_{i=1}^{n}c_i\vert I_i\vert,\label{eq:erdi.6} \\ \text{p.c.}\int_{a}^{b}h(x)\,dx&amp;=\sum_{i=1}^{n}d_i\vert I_i\vert,\label{eq:erdi.7}
\end{align}
then it follows immediately that $c_i\leq d_i$. And since $g\leq f\leq h$ piecewise, on each interval $I_i$, we can find a $x_i^*$ such that $c_i\leq f(x_i^*)\leq d_i$. Additionally, combining with \eqref{eq:erdi.4}, \eqref{eq:erdi.5}, \eqref{eq:erdi.6} and \eqref{eq:erdi.7}, we have that for any $\varepsilon&gt;0$
\begin{equation}
\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon&lt;\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert&lt;\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon
\end{equation}
Therefore, for any $\varepsilon&gt;0$, we have
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\text{d.}\int_{a}^{b}f(x)\,dx\right\vert&lt;\varepsilon,
\end{equation}
which claims that $f$ is Riemann integrable on $[a,b]$ with $\text{d.}\int_{a}^{b}f(x)\,dx$ is the Riemann integral of $f$.</li>
</ul>

<p><strong>Example</strong><br />
Any continuous function $f:[a,b]\to\mathbb{R}$ is Riemann integrable. More generally, any bounded, <strong>piecewise continuous function</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> $f:[a,b]\to\mathbb{R}$ is Riemann integrable.</p>

<p><strong>Solution</strong><br />
Consider a partition of piecewise continuous f on $[a,b]$ into finitely many intervals $I_1,\ldots,I_n$. Using the procedure that we used for the above proof, we have that on each interval $I_i$, there exists an $x_i$ such that for any $\varepsilon&gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i)&lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Hence,
\begin{equation}
\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i)\vert I_i\vert&lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i)\vert I_i\vert-\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon,
\end{equation}
which implies that $f$ is Riemann integrable on $[a,b]$.</p>

<h3 id="riemann-int-properties">Basic properties of Riemann integral</h3>
<p>Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be Riemann integrable. We then have that</p>
<ul id="number-list">
	<li>
		<b>Linearity</b>. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are Riemann integrable, with
		\begin{align}
		\int_{a}^{b}cf(x)\,dx&amp;=c\int_{a}^{b}f(x)\,dx \\\\ \int_{a}^{b}\big(f(x)+g(x)\big)\,dx&amp;=\int_{a}^{b}f(x)\,dx+\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>. If $f\leq g$ pointwise, then
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>. If $E$ is a Jordan measurable of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ is Riemann integrable, and
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		<b>Linearity</b>.
		<ul>
			<li>
				Given $f$ Riemann integrable on $[a,b]$, we have that there exists a tagged partition $\mathcal{P}=((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*));(x_i^*\in I_i)$ of $[a,b]$ such that for any $\varepsilon&gt;0$, we have
				\begin{equation}
				\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
				\end{equation}
				Thus, for any $c\in\mathbb{R}$
				\begin{equation}
				\left\vert\sum_{i=1}^{n}cf(x_i^*)\vert I_i\vert-\int_{a}^{b}cf(x)\,dx\right\vert\leq\vert c\vert\varepsilon=\varepsilon',
				\end{equation}
				where $\varepsilon'&gt;0$ arbitrarily. This implies that $cf$ is Riemann integrable on $[a,b]$ with Riemann integral $\int_{a}^{b}cf(x)\,dx=c\int_{a}^{b}f(x)\,dx$.
			</li>
			<li>
				Given $f$ Riemann integrable on $[a,b]$, then $f$ is also Darboux integrable on $[a,b]$, which means
				\begin{align}
				\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx&amp;=\inf_{f_2\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)\,dx \\ &amp;\hspace{1cm}=\int_{a}^{b}f(x)\,dx\label{eq:rip.1}
				\end{align}
				Similarly, $g$ Riemann integrable on $[a,b]$ implies that $g$ is also Darboux integrable, or in particular
				\begin{align}
				\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx&amp;=\inf_{g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_2(x)\,dx \\ &amp;\hspace{1cm}=\int_{a}^{b}g(x)\,dx\label{eq:rip.2}
				\end{align}
				By the linearity property of piecewise constant functions, combined with \eqref{eq:rip.1} and \eqref{eq:rip.2}, we obtain
				\begin{align}
				&amp;\sup_{f_1\leq f,g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)+g_1(x)\,dx \\ &amp;\hspace{2cm}=\inf_{f_2\geq f,g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)+g_2(x)\,dx \\ &amp;\hspace{2cm}=\int_{a}^{b}f(x)+g(x)\,dx,
				\end{align}
				which claims the Riemann integrability of $f+g$ on $[a,b]$.
			</li>
		</ul>
	</li>
	<li>
		<b>Monotonicity</b>.<br />
		Given $f$ and $g$, we obtain two consequential equations \eqref{eq:rip.1} and \eqref{eq:rip.2}. And since $f\leq g$ pointwise we have that
		\begin{equation}
		\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx\leq\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx
		\end{equation}
		or
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>.<br />
		Given $E\subset [a,b]$ is Jordan measurable, we have
		\begin{equation}
		\sup_{A\subset E,A\text{ elementary}}m(A)=\inf_{B\supset E,B\text{ elementary}}m(B)=m(E)\label{eq:rip.3}
		\end{equation}
		Recall that we have proved that for any elementary set $E'\subset[a,b]$, the indicator function $1_{E'}:[a,b]\to\mathbb{R}$ is also piecewise constant with
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_{E'}(x)\,dx=m(E')
		\end{equation}
		Moreover for any $A\subset E$, we have $1_A(x)\leq 1_E(x)$; and for any $B\supset E$, we have $1_B(x)\geq 1_E(x)$. Therefore the lower Darboux integral of $1_E$ on $[a,b]$ can be defined as
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\sup_{1_A\leq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_A(x)\,dx=\sup_{A\subset E,A\text{ elementary}}m(A)\label{eq:rip.4}
		\end{equation}
		And the upper Darboux integral of $1_E$ on $[a,b]$ can also be defined as
		\begin{equation}
		\overline{\int_{a}^{b}}1_E(x)\,dx=\inf_{1_B\geq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_B(x)\,dx=\inf_{B\supset E,B\text{ elementary}}m(B)\label{eq:rip.5}
		\end{equation}
		Combine \eqref{eq:rip.3}, \eqref{eq:rip.4} and \eqref{eq:rip.5}, we have
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\overline{\int_{a}^{b}}1_E(x)\,dx=m(E),
		\end{equation}
		which means $1_E$ is Darboux integrable on $[a,b]$ with the Darboux integrable $m(E)$. By the equivalence of Riemann and Darboux integral, $1_E$ is also Riemann integrable on $[a,b]$ with the Riemann integral
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>
<p>These properties uniquely define the Riemann integral, in the sense that the functional $f\mapsto\int_{a}^{b}f(x)\,dx$ is the only map from the space of Riemann integrable functions on $[a,b]$ to $\mathbb{R}$ which obeys all of these above properties.</p>

<h3 id="riemann-int-area-interpret">Area interpretation of the Riemann integral</h3>
<p>Let $[a,b]$ be an interval, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff the sets $E_+\doteq\{(x,t):x\in[a,b];0\leq t\leq f(x)\}$ and $E_-\doteq\{(x,t):x\in[a,b];f(x)\leq t\leq 0\}$ are both Jordan measurable in $R^2$, in which case we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=m^2(E_+)-m^2(E_-),
\end{equation}
where $m^2$ denotes two-dimensional Jordan measure.</p>

<p><strong>Proof</strong></p>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126, 2011.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. 2007</span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A function $f$ is said to be <strong>uniformly continuous</strong> if for any real $\varepsilon&gt;0$, there exists a real number $\delta&gt;0$ such that for any $x, y$ with $d_1(x, y)&lt;\delta$, we also have
\begin{equation*}
d_2(f(x),f(y))&lt;\varepsilon
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A function $f:[a,b]\to\mathbb{R}$ is <strong>piecewise continuous</strong> if we can partition $[a,b]$ into finitely many intervals, such that $f$ is continuous on each interval. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="jordan-measure" /><category term="riemann-integral" /><summary type="html"><![CDATA[Note on measure theory part 1]]></summary></entry><entry><title type="html">Planning &amp;amp; Learning</title><link href="http://localhost:4000/2022/05/19/planning-learning.html" rel="alternate" type="text/html" title="Planning &amp;amp; Learning" /><published>2022-05-19T14:09:00+07:00</published><updated>2022-05-19T14:09:00+07:00</updated><id>http://localhost:4000/2022/05/19/planning-learning</id><content type="html" xml:base="http://localhost:4000/2022/05/19/planning-learning.html"><![CDATA[<blockquote>
  <p>Recall that when using <a href="/2021/07/25/dp-in-mdp.html">dynamic programming (DP) method</a> in solving reinforcement learning problems, we required the availability of a model of the environment. Whereas with <a href="/2021/08/21/monte-carlo-in-rl.html">Monte Carlo methods</a> and <a href="/2022/01/31/td-learning.html">temporal-difference learning</a>, the models are unnecessary. Such methods with requirement of a model like the case of DP is called <strong>model-based</strong>, while methods without using a model is called <strong>model-free</strong>. Model-based methods primarily rely on <strong>planning</strong>; and model-free methods, on the other hand, primarily rely on <strong>learning</strong>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#models-planning">Models &amp; Planning</a>
    <ul>
      <li><a href="#models">Models</a></li>
      <li><a href="#planning">Planning</a></li>
    </ul>
  </li>
  <li><a href="#dyna">Dyna</a>
    <ul>
      <li><a href="#dyna-q">Dyna-Q</a>
        <ul>
          <li><a href="#dyna-q-eg">Example</a></li>
        </ul>
      </li>
      <li><a href="#dyna-q-plus">Dyna-Q+</a></li>
    </ul>
  </li>
  <li><a href="#prioritized-sweeping">Prioritized Sweeping</a>
    <ul>
      <li><a href="#small-backups">Small backups</a></li>
    </ul>
  </li>
  <li><a href="#trajectory-sampling">Trajectory Sampling</a></li>
  <li><a href="#heuristic-search">Heuristic Search</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="models-planning">Models &amp; Planning</h2>

<h3 id="models">Models</h3>
<p>A <strong>model</strong> of the environment represents anything that an agent can use to predict responses - in particular, next state and corresponding reward - of the environment to its chosen actions.</p>

<p>When the model is stochastic, there are several next states and rewards corresponding, each with some probability of occurring.</p>
<ul>
  <li>If a model produces a description of all possibilities and their probabilities, we call it <strong>distribution model</strong>. For example, consider the task of tossing coin multiple times, the distribution model will produce the probability of head and the probability of tail, which is 50% for each with a fair coin.</li>
  <li>On the other hand, if the model produces an individual sample (head or tail) according to the probability distribution, we call it <strong>sample model</strong>.</li>
</ul>

<p>Both types of models above can be used to mimic or simulate experience. Given a starting state and a policy, a sample model would generate an entire episode, while a distribution model could produce all possible episodes and their probabilities. We say that the model is used to <strong>simulate</strong> the environment in order to produce <strong>simulated experience</strong>.</p>

<h3 id="planning">Planning</h3>
<p><strong>Planning</strong> in reinforcement learning is the process of taking a model as input then output a new policy or an improved policy for interacting with the modeled environment
\begin{equation}
\text{model}\hspace{0.5cm}\xrightarrow[]{\hspace{1cm}\text{planning}\hspace{1cm}}\hspace{0.5cm}\text{policy}
\end{equation}
There are two types of planning:</p>
<ul>
  <li><strong>State-space planning</strong> is a search through the state space for an optimal policy or an optimal path to a goal, with two basic ideas:
    <ul>
      <li>Involving computing value functions as a key intermediate step toward improving the policy.</li>
      <li>Computing value functions by updates or backup applied to simulated experience.
  \begin{equation}
  \text{model}\xrightarrow[]{\hspace{1.5cm}}\text{simulated experience}\xrightarrow[]{\hspace{0.3cm}\text{backups}\hspace{0.3cm}}\text{backups}\xrightarrow[]{\hspace{1.5cm}}\text{policy}
  \end{equation}</li>
    </ul>
  </li>
  <li><strong>Plan-space planning</strong> is a search through the space of plans.
    <ul>
      <li>Plan-space planning methods consist of <strong>evolutionary methods</strong> and <strong>partial-order planning</strong>, in which the ordering of steps is not completely determined at all states of planning.</li>
    </ul>
  </li>
</ul>

<p>Both learning and planning methods estimate value functions by backup operations. The difference is planning uses simulated experience generated by a model compared to the uses of simulated experience generated by the environment in learning methods. This common structure lets several ideas and algorithms can be transferred between learning and planning with some modifications in the update step.</p>

<p>For instance, following is pseudocode of a planning method, called <strong>random-sample one-step tabular Q-planning</strong>, based on <a href="/2022/01/31/td-learning.html#q-learning">one-step tabular Q-learning</a>, and on random samples from a sample model.</p>
<figure>
	<img src="/assets/images/2022-05-19/rand-samp-one-step-q-planning.png" alt="Random-sample one-step Q-planning" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="dyna">Dyna</h2>
<p>Within a planning agent, experience plays at least two roles:</p>
<ul>
  <li><strong>model learning</strong>: improving the model;</li>
  <li><strong>direct reinforcement learning (RL)</strong>: improving the value function and policy</li>
</ul>

<p>The figure below illustrates the possible relationships between experience, model, value functions and policy.</p>
<figure>
    <img src="/assets/images/2022-05-19/exp-model-value-policy.png" alt="Exp, model, values and policy relationships" style="display: block; margin-left: auto; margin-right: auto; width: 300px; height: 250px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: The possible relationships between experience, model, values and policy<br />(the figure is taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>

<p>Each arrows in the diagram represents a relationship of influence and presumed improvement. It is noticeable in the diagram that experience can improve value functions and policy either directly or indirectly via model (called <strong>indirect RL</strong>), which involved in planning.</p>
<ul>
  <li>direct RL: simpler, not affected by bad models;</li>
  <li>indirect RL: make fuller use of experience, i.e., getting better policy with fewer environment interactions.</li>
</ul>

<h3 id="dyna-q">Dyna-Q</h3>
<p><strong>Dyna-Q</strong> is the method having all of the processes shown in the diagram in <strong><em>Figure 1</em></strong> - planning, acting, model-learning and direct RL - all occurring continually:</p>
<ul>
  <li>the <em>planning</em> method is the random-sample one-step tabular Q-planning in the previous section;</li>
  <li>the <em>direct RL</em> method is the one-step tabular Q-learning;</li>
  <li>the <em>model-learning</em> method is also table-based and assumes the environment is deterministic.</li>
</ul>

<p>After each transition $S_t,A_t\to S_{t+1},R_{t+1}$, the model records its table entry for $S_t,A_t$ the prediction that $S_{t+1},R_{t+1}$ will deterministically follow. This lets the model simply return the last resultant next state and corresponding reward of a state-action pair when meeting them in the future.</p>

<p>During planning, the Q-planning algorithm randomly samples only from state-action pair that have previously been experienced. This helps the model to not be queried with a pair whose information is unknown.</p>

<p>Following is the general architecture of Dyna methods, of which Dyna-Q is an instance.</p>
<figure>
    <img src="/assets/images/2022-05-19/dyna-arch.png" alt="Dyna architecture" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 320px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The general Dyna Architecture<br />(the figure is taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>

<p>In most cases, the same reinforcement learning method is used for both learning from real experience and planning from simulated experience, which is - in this case of Dyna-Q - the Q-learning update.</p>

<p>Pseudocode of Dyna-Q method is shown below.</p>
<figure>
	<img src="/assets/images/2022-05-19/tabular-dyna-q.png" alt="Tabular Dyna-Q" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="dyna-q-eg">Example</h4>
<p>(This example is taken from <a href="#rl-book">RL book</a> - example 8.1.)</p>

<p>Consider a gridworld with some obstacles, called “maze” in this example, shown in the figure below.</p>
<figure>
	<img src="/assets/images/2022-05-19/dyna-maze.png" alt="Dyna maze" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 200px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: The maze with some obstacles<br />(the figure is taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>
<p>As usual, four action, $\text{up}, \text{down}, \text{right}$ and $\text{left}$ will take agent to its neighboring state, except when the agent is standing on the edge or is blocked by the obstacles, they do nothing, i.e., the agent stays still. Starting at state $S$, each transition to a non-goal state will give a reward of zero, while moving to the goal state, $G$, will reward $+1$. The episode resets when the agent reaches the goal state.</p>

<p>The task is discounted, episodic with $\gamma=0.95$.</p>
<figure>
    <img src="/assets/images/2022-05-19/dyna-maze-dyna-q.png" alt="Dyna maze solved with Dyna-Q" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: Using Dyna-Q with different setting of number of planning steps on the maze.<br />The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py">here</a>.</span></figcaption>
</figure>

<h3 id="dyna-q-plus">Dyna-Q+</h3>
<p>Consider a maze like the one on the left of the figure below. Suppose that after applying Dyna-Q has learned the optimal path, we make some changes to transform the gridworld into the one on the right that block the found optimal path.</p>
<figure>
    <img src="/assets/images/2022-05-19/blocking-maze.png" alt="Blocking maze" style="display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 5</b>: The maze before and after change<br />(the figure is taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>
<p>With this modification, eventually a new optimal path will be found by the Dyna-Q agent but this will takes hundreds more steps.</p>

<p>In this case, we want the agent to explore in order to find changes in the environment, but not so much that performance is greatly degraded. To encourage the exploration, we give it an <strong>exploration bonus</strong>:</p>
<ul>
  <li>Keeps track for each state-action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment.</li>
  <li>An special <strong>bonus reward</strong> is added for transitions caused by state-action pairs related how long ago they were tried: the long unvisited, the more reward for visiting:
\begin{equation}
r+\kappa\sqrt{\tau},
\end{equation}
for a small (time weight) $\kappa$; where $r$ is the modeled reward for a transition; and the transition has not been tried in $\tau$ time steps.</li>
  <li>The agent actually plans how to visit long unvisited state-action pairs.</li>
</ul>

<p>The following plot shows the performance comparison between Dyna-Q and Dyna-Q+ on this blocking task, with changing in the environment happens after 1000 steps.</p>
<figure>
    <img src="/assets/images/2022-05-19/blocking-maze-dyna-q-qplus.png" alt="Dyna-Q, Dyna-Q+ on blocking maze" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 6</b>: Average performance of Dyna-Q and Dyna-Q+ on blocking maze.<br />The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py">here</a>.</span></figcaption>
</figure>

<p>We also make a comparison between with and without giving an exploration bonus to the Dyna-Q agent on the shortcut maze below.</p>
<figure>
    <img src="/assets/images/2022-05-19/shortcut-maze.png" alt="shortcut maze" style="display: block; margin-left: auto; margin-right: auto; width: 600px; height: 150px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 7</b>: The maze before and after change<br />(the figure is taken from <span><a href="#rl-book">RL book</a></span>)</figcaption>
</figure>

<p>Below is the result of using two agents solving the shortcut maze with environment modification appears after 3000 steps.</p>
<figure>
    <img src="/assets/images/2022-05-19/shortcut-maze-dyna-q-qplus.png" alt="Dyna-Q, Dyna-Q+ on blocking maze" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 8</b>: Average performance of Dyna-Q and Dyna-Q+ on shortcut maze.<br />The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py">here</a>.</span></figcaption>
</figure>
<p>It can be seen from the plot above that the difference between Dyna-Q+ and Dyna-Q narrowed slightly over the first part of the experiment (the one using the left maze as its environment).</p>

<p>The reason for that is both agents were spending much more time steps than the case of blocking maze, which let the gap created by the faster convergence of Dyna-Q+ with Dyna-Q be narrowed down by exploration task, which Dyna-Q+ had to do but not Dyna-Q. This result will be more noticeable if they were stick to this first environment more time steps.</p>

<h2 id="prioritized-sweeping">Small Prioritized Sweeping</h2>
<p>Recall that in the Dyna methods presented above, the search control process selected a state-action pair randomly from all previously experienced pairs. It means that we can improve the planning if the search control instead focused on some particular state-action pairs.</p>

<p>Pseudocode of prioritized sweeping is shown below.</p>
<figure>
	<img src="/assets/images/2022-05-19/prioritized-sweeping.png" alt="Prioritized sweeping" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<figure>
    <img src="/assets/images/2022-05-19/dyna-maze-prioritized-sweeping.png" alt="Prioritized sweeping on dyna maze" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 400px" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 9</b>: Using prioritized sweeping on mazes.<br />The code can be found <span><a href="https://github.com/trunghng/reinforcement-learning-an-introduction/blob/main/chapter-08/maze.py">here</a>.</span></figcaption>
</figure>

<p>###</p>

<h2 id="trajectory-sampling">Trajectory Sampling</h2>

<h2 id="heuristic-search">Heuristic Search</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] <span id="rl-book">Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>. MIT press, 2018.</span></p>

<p>[2] Richard S. Sutton. <a href="https://doi.org/10.1016/B978-1-55860-141-3.50030-4">Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming</a>. Proceedings of the Seventh International Conference, Austin, Texas, June 21–23, 1990.</p>

<p>[3] Harm van Seijen &amp; Richard S. Sutton. <a href="https://proceedings.mlr.press/v28/vanseijen13.pdf">Efficient planning in MDPs by small backups</a>. Proceedings
of the 30th International Conference on Machine Learning (ICML 2013), 2013.</p>

<p>[4] Shangtong Zhang. <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">Reinforcement Learning: An Introduction implementation</a>. Github</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="reinforcement-learning" /><category term="dyna" /><category term="q-learning" /><category term="mcts" /><category term="my-rl" /><summary type="html"><![CDATA[Planning & Learning]]></summary></entry></feed>