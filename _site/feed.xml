<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-12-31T12:01:35+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Bayesian Optimization</title><link href="http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html" rel="alternate" type="text/html" title="Bayesian Optimization" /><published>2021-11-22T14:46:00+07:00</published><updated>2021-11-22T14:46:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html">&lt;blockquote&gt;
  &lt;p&gt;Dont know yet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#basics&quot;&gt;Mathematical Basics&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gauss-dist&quot;&gt;Gaussian (Normal) Distribution&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#std-norm&quot;&gt;Standard Normal&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mvn&quot;&gt;Multivariate Normal Distribution&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#bvn&quot;&gt;Bivariate Normal&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kernels&quot;&gt;Kernels&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#kernel-func&quot;&gt;Kernel functions&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#rbf-kernels&quot;&gt;RBF kernels&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#mercer-kernels&quot;&gt;Mercer (positive definite) kernels&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#lin-kernels&quot;&gt;Linear kernels&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#matern-kernels&quot;&gt;Matern kernels&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gpr&quot;&gt;Gaussian Process Regression&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bayes-opt&quot;&gt;Bayesian Opitmization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#surrogate-model&quot;&gt;Surrogate Model&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#acquisition-func&quot;&gt;Acquisition Functions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#opt-alg&quot;&gt;Optimization Algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#exp-imp&quot;&gt;Expected Improvement&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#implementation&quot;&gt;Implementation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$&lt;/p&gt;
&lt;h2 id=&quot;basics&quot;&gt;Background&lt;/h2&gt;
&lt;p&gt;Before diving into details, we need some necessary basic concepts.&lt;/p&gt;

&lt;h3 id=&quot;gauss-dist&quot;&gt;Gaussian (Normal) Distribution&lt;/h3&gt;
&lt;p&gt;A random variable $X$ is said to be &lt;strong&gt;Gaussian&lt;/strong&gt; or to have the &lt;strong&gt;Normal distribution&lt;/strong&gt; with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$&lt;/p&gt;

&lt;h4 id=&quot;std-normal&quot;&gt;Standard Normal&lt;/h4&gt;
&lt;p&gt;When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution &lt;em&gt;Standard Normal&lt;/em&gt;.
\begin{equation}
X\sim\mathcal{N}(0,1)
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2}
\end{equation}
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\,dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt
\end{equation}
Below is some visualizations of Normal distribution.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-11-22/normal.png&quot; alt=&quot;normal distribution&quot; width=&quot;900&quot; height=&quot;380px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/bayes-opt/blob/main/gauss-dist.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mvn&quot;&gt;Multivariate Normal Distribution&lt;/h3&gt;
&lt;p&gt;A $k$-dimensional random vector $\mathbf{X}=(X_1,\dots,X_k)^T$ is said to have a &lt;strong&gt;Multivariate Normal (MVN)&lt;/strong&gt; distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_kX_k
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_k$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})
\end{equation}
where
\begin{equation}
	\mathbf{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^T=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^T
\end{equation}
is the $k$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{k\times k}$ with
\begin{equation}
	\mathbf{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)
\end{equation}
We also have that $\mathbf{\Sigma}\geq 0$ (positive semi-definite matrix)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Thus, the PDF of an MVN is defined as
\begin{equation}
f_X(x_1,\ldots,x_k)=\dfrac{1}{(2\pi)^{k/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[\dfrac{1}{2}(\mathbf{x}-\mathbf{\mu})^T\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right]
\end{equation}
With this idea, &lt;em&gt;Standard Normal&lt;/em&gt; distribution in multi-dimensional case can be defined as a Gaussian with mean $\mathbf{\mu}=0$ (here $0$ is an $k$-dimensional vector) and identity covariance matrix $\mathbf{\Sigma}=\mathbf{I}_{k\times k}$.&lt;/p&gt;

&lt;h4 id=&quot;bvn&quot;&gt;Bivariate Normal&lt;/h4&gt;
&lt;p&gt;When the number of dimensions in $\mathbf{X}$, $k=2$, this special case of MVN is called the &lt;strong&gt;Bivariate Normal (BVN)&lt;/strong&gt;. An example of an BVN is shown as following.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-11-22/bvn.png&quot; alt=&quot;monte carlo method&quot; width=&quot;750&quot; height=&quot;350px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;amp;0.5\\0.8&amp;amp;1\end{smallmatrix}\right]\right)$. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/bayes-opt/blob/main/mvn.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;kernels&quot;&gt;Kernels&lt;/h3&gt;

&lt;h4 id=&quot;kernel-func&quot;&gt;Kernel functions&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Kernel function&lt;/strong&gt; is a real-valued function of two arguments
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)\in\mathbb{R},
\end{equation}
for $\textbf{x},\textbf{x}’\in\mathcal{X}$, which typically is symmetric (i.e., $\kappa(\textbf{x},\textbf{x}’)=\kappa(\textbf{x}’,\textbf{x})$), and nonnegative ($\kappa(\textbf{x},\textbf{x}’)\geq0$). These are some examples of kernel functions.&lt;/p&gt;

&lt;h5 id=&quot;rbf-kernels&quot;&gt;RBF kernels&lt;/h5&gt;
&lt;p&gt;The &lt;strong&gt;squared exponential kernel&lt;/strong&gt; (SE kernel) or &lt;strong&gt;Gaussian kernel&lt;/strong&gt; is defined by
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{1}{2}(\textbf{x}-\textbf{x}’)^T\mathbf{\Sigma}^{-1}(\textbf{x}-\textbf{x}’)\right)
\end{equation}
If $\mathbf{\Sigma}$ is a diagonal matrix, this can be written as
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{1}{2}\sum_{j=1}^{D}\frac{1}{\sigma_j^2}(x_j-x_j’)^2\right)
\end{equation}
We can interpret the $\sigma_j$ as defining the &lt;strong&gt;characteristic length scale&lt;/strong&gt; of dimension $j$. The corresponding dimension of $\sigma_j$ is ignored if $\sigma_j=\infty$. This is known as the &lt;strong&gt;ARD kernel&lt;/strong&gt; (Automatic Relevance Determination).&lt;br /&gt;
If $\Sigma$ is spherical&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, we get the isotropic kernel
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{\Vert\textbf{x}-\textbf{x}’\Vert}{2\sigma^2}\right)\tag{1}\label{1}
\end{equation}
Here $\sigma^2$ is known as the &lt;strong&gt;bandwidth&lt;/strong&gt;. Since \eqref{1} can be seen as a function of $\Vert\textbf{x}-\textbf{x}’\Vert$, it is an example of a &lt;strong&gt;radial basis function&lt;/strong&gt; or &lt;strong&gt;RBF kernel&lt;/strong&gt;.&lt;/p&gt;

&lt;h5 id=&quot;mercer-kernels&quot;&gt;Mercer (positive definite) kernels&lt;/h5&gt;
&lt;p&gt;If the Gram matrix, defined by
\begin{equation}
\mathbf{K}=\begin{bmatrix}\kappa(\mathbf{x}_1,\mathbf{x}_1)&amp;amp;\ldots&amp;amp;\kappa(\mathbf{x}_1,\mathbf{x}_N) \\&amp;amp;\vdots&amp;amp;\\\kappa(\mathbf{x}_N,\mathbf{x}_1)&amp;amp;\ldots&amp;amp;\kappa(\mathbf{x}_N,\mathbf{x}_N)\end{bmatrix},
\end{equation}
is positive definite for any set $\left\{x_i\right\}_{i=1}^N$, the kernel $\kappa$ is called a &lt;strong&gt;Mercer kernel&lt;/strong&gt;, or &lt;strong&gt;positive definite kernel&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The importance of Mercer kernels is the following reusult, which is known as  &lt;strong&gt;Mercer’s theorem&lt;/strong&gt;. If the Gram matrix is positive definite, we can compute an eigenvector decomposition of it as
\begin{equation}
\mathbf{K}=\mathbf{U}^T\mathbf{\Lambda}\mathbf{U},
\end{equation}
where $\mathbf{\Lambda}$ is a diagonal matrix of eigenvalues $\lambda_i&amp;gt;0$. Consider an element of $\mathbf{K}$
\begin{equation}
k_{ij}=\left(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:i}\right)^T\left(\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:j}\right)
\end{equation}
If we let $\phi(\textbf{x}_i)=\mathbf{\Lambda}^{\frac{1}{2}}\mathbf{U}_{:i}$, then we can write
\begin{equation}
k_{ij}=\phi(\textbf{x}_i)^T\phi(\textbf{x}_j)
\end{equation}
Thus we see that the intries in the kernel matrix can be computed by performing an inner product of some feature vectors that are implicitly defined by the eigenvectors $\mathbf{U}$. In general, if the kernel is Mercer, then there exists a function $\phi$ mapping $\textbf{x}\in\mathcal{X}$ to $\mathbb{R}^D$ such that
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\phi(\textbf{x})^T\phi(\textbf{x}’),
\end{equation}
where $\phi$ depends on the eigenfunctions of $\kappa$.&lt;/p&gt;

&lt;h5 id=&quot;lin-kernels&quot;&gt;Linear kernels&lt;/h5&gt;
&lt;p&gt;Deriving the feature vector implied by a kernel is only possible if the kernel is &lt;em&gt;Mercer&lt;/em&gt;. However, deriving a kernel from a feature vector is easy. We have
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\phi(\textbf{x})^T\phi(\textbf{x}’)
\end{equation}
If $\phi(\textbf{x})=\textbf{x}$, we obtain a simple kernel called &lt;strong&gt;linear kernel&lt;/strong&gt;
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\textbf{x}^T\textbf{x}’
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;matern-kernels&quot;&gt;Matern kernels&lt;/h5&gt;
&lt;p&gt;Let $\mathcal{X}\subset\mathbb{R}^D,\nu&amp;gt;0,\ell&amp;gt;0$. The &lt;strong&gt;Matern kenel&lt;/strong&gt; $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is defined by
\begin{equation}
\kappa(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}r}{\ell}\right)^{\nu}K_{\nu}\left(\frac{\sqrt{2\nu}r}{\ell}\right)\tag{2}\label{2}
\end{equation}
where $\Gamma$ is the &lt;em&gt;gamma function&lt;/em&gt;; $r=\Vert\textbf{x}-\textbf{x}’\Vert$ for $x,x’\in\mathcal{X}$ and $K_{\nu}$ is a &lt;em&gt;modified Bessel function&lt;/em&gt;. As $\nu\to\infty$, this approaches the SE kernel.&lt;/p&gt;

&lt;p&gt;When $\nu$ is half-integer - i.e., $\nu=p+1/2$, for $p\geq0$, equation \eqref{2} can be reduced to a product of an exponential function and a polynomial of degree $p$, which can be written as
\begin{equation}
\kappa_{\nu=p+1/2}(r)=\exp\left(-\dfrac{\sqrt(2\nu)r}{\ell}\right)\dfrac{\Gamma(p+1)}{\Gamma(2p+1)}\sum_{i=0}^{p}\dfrac{(p+i)!}{i!(p-i)!}\left(\dfrac{\sqrt{8\nu}r}{\ell}\right)^{p-i}
\end{equation}&lt;/p&gt;

&lt;p&gt;For an another special case, setting $\nu=1/2$ lets \eqref{2} become
\begin{equation}
\kappa_{\nu=1/2}=\exp\left(\dfrac{-r}{\ell}\right),
\end{equation}
which is known as &lt;strong&gt;Laplace&lt;/strong&gt; or &lt;strong&gt;exponential kernel&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;gpr&quot;&gt;Gaussian Process Regression&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;Gaussian process&lt;/strong&gt; (GP) is a collection of random variables, any number of which have a joint Gaussian distribution.&lt;/p&gt;

&lt;p&gt;A Gaussian process is completely specified by its mean function and covariance function. We define mean function $m(\textbf{x})$ and the covariance function or kernel $\kappa(\textbf{x},\textbf{x}’)$ of a process $f(\textbf{x})$ as
\begin{align}
m(\textbf{x})&amp;amp;=\mathbb{E}\left[f(\textbf{x})\right] \\ \kappa(\textbf{x},\textbf{x}’)&amp;amp;=\mathbb{E}\left[(f(\textbf{x})-m(\textbf{x}))(f(\textbf{x}’)-m(\textbf{x}’))^T\right]
\end{align}
and denote the Gaussian process as
\begin{equation}
f(x)\sim\mathcal{GP}\left(m(\textbf{x}),\kappa(\textbf{x},\textbf{x}’)\right)
\end{equation}
Hence, in this case, $\kappa$ is a Mercer kernel. And for any finite set of point $\textbf{x}_1,\dots,\textbf{x}_k\in\mathbb{R}^d$, the definition of GP define an MVN
\begin{equation}
f(\textbf{X})\sim\mathcal{N}\left(\mathbf{\mu},\textbf{K}\right),
\end{equation}
where $\textbf{X}=\left(\textbf{x}_1\dots\right)$&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] C. E. Rasmussen &amp;amp; C. K. I. Williams. &lt;a href=&quot;http://www.gaussianprocess.org/gpml/&quot;&gt;Gaussian Processes for Machine Learning&lt;/a&gt;, MIT Press, 2006&lt;/p&gt;

&lt;p&gt;[2] Joseph K. Blitzstein &amp;amp; Jessica Hwang. &lt;a href=&quot;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573&quot;&gt;Introduction to Probability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Stanford CS229. &lt;a href=&quot;http://cs229.stanford.edu&quot;&gt;Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] Kevin P. Murphy. &lt;a href=&quot;https://probml.github.io/pml-book/book0.html&quot;&gt;Machine Learning: A Probabilistic Perspective&lt;/a&gt;, MIT Press, 2012&lt;/p&gt;

&lt;p&gt;[5] Christopher M. Bishop. &lt;a href=&quot;https://www.amazon.com/Pattern-Recognition-Learning-Information-Setatistics/dp/0387310738&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] Peter I. Frazier. &lt;a href=&quot;https://arxiv.org/abs/1807.02811&quot;&gt;A Tutorial on Bayesian Optimization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] Martin Krasser. &lt;a href=&quot;https://krasserm.github.io/2018/03/21/bayesian-optimization/&quot;&gt;Bayesian Optimization&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[8] &lt;a href=&quot;https://stats.stackexchange.com/users/28666/amoeba&quot;&gt;amoeba&lt;/a&gt;, &lt;a href=&quot;https://stats.stackexchange.com/q/204599&quot;&gt;What is an isotropic (spherical) covariance matrix?&lt;/a&gt;, StackExchange&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The definition of covariance matrix $\mathbf{\Sigma}$ can be rewritten as
\begin{equation}
\mathbf{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation}
Let $\mathbf{z}\in\mathbb{R}^k$, we have
\begin{equation}
\Var(\mathbf{z}^T\mathbf{X})=\mathbf{z}^T\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^T\mathbf{\Sigma}\mathbf{z}
\end{equation}
And since $\Var(\mathbf{z}^T\mathbf{X})\geq0$, we also have that $\mathbf{z}^T\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\mathbf{\Sigma}$ is a positive semi-definite matrix. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A covariance matrix $\mathbf{C}$ is called &lt;em&gt;isotrophic&lt;/em&gt;, or &lt;em&gt;spherical&lt;/em&gt; if it is proportionate to the identity matrix
\begin{equation}
\mathbf{C}=\lambda\mathbf{I}
\end{equation} &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="machine-learning" /><category term="artificial-intelligent" /><category term="machine-learning" /><category term="gaussian-process" /><category term="optimization-control" /><category term="probability-statistics" /><summary type="html">Dont know yet</summary></entry><entry><title type="html">Power Series</title><link href="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html" rel="alternate" type="text/html" title="Power Series" /><published>2021-09-21T15:40:00+07:00</published><updated>2021-09-21T15:40:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/21/power-series</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html">&lt;blockquote&gt;
  &lt;p&gt;Recall that in the previous post, &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html&quot;&gt;Infinite Series of Constants&lt;/a&gt;, we mentioned a type of series called &lt;strong&gt;power series&lt;/strong&gt; a lot. In the content of this post, we will be diving deeper into details of that series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#power-series&quot;&gt;Power Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-conv&quot;&gt;The Interval of Convergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#eg1&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dif-int-power-series&quot;&gt;Differentiation and Integration of Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dif-power-series&quot;&gt;Differentiation of Power Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#int-power-series&quot;&gt;Integration of Power Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eg2&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#taylor-series-formula&quot;&gt;Taylor Series, Taylor’s Formula&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#taylor-series&quot;&gt;Taylor Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#taylors-formula&quot;&gt;Taylor’s Formula&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#op-power-series&quot;&gt;Operations on Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mult&quot;&gt;Multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#div&quot;&gt;Division&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sub&quot;&gt;Substitution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#even-odd-funcs&quot;&gt;Even and Odd Functions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uni-conv-power-series&quot;&gt;Uniform Convergence for Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cont-sum&quot;&gt;Continuity of the Sum&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#int&quot;&gt;Integrating term by term&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dif&quot;&gt;Differentiating term by term&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;power-series&quot;&gt;Power Series&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;power series&lt;/strong&gt; is a series of the form
\begin{equation}
\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots,
\end{equation}
where the coefficient $a_n$ are constants and $x$ is a variable.&lt;/p&gt;

&lt;h2 id=&quot;int-conv&quot;&gt;The Interval of Convergence&lt;/h2&gt;
&lt;p&gt;Similar to what we have done in the post of &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html&quot;&gt;infinite series of constants&lt;/a&gt;, we begin studying properties of power series by considering their convergence behavior.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a power series $\sum a_nx^n$ converges at $x_1$, with $x_1\neq 0$, then it converges &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-conv&quot;&gt;absolutely&lt;/a&gt; at all $x$ with $\vert x\vert&amp;lt;\vert x_1\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\vert x\vert&amp;gt;\vert x_1\vert$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we have that if $\sum a_nx^n$ converges, then $a_nx^n\to 0$. In particular, if $n$ is sufficiently large, then $\vert a_n{x_1}^n\vert&amp;lt;1$, and therefore
\begin{equation}
\vert a_nx^n\vert=\vert a_n{x_1}^n\vert\left\vert\dfrac{x}{x_1}\right\vert^n&amp;lt;r^n,\tag{1}\label{1}
\end{equation}
where $r=\vert\frac{x}{x_1}\vert$. Suppose that $\vert x\vert&amp;lt;\vert x_1\vert$, we have
\begin{equation}
r=\left\vert\dfrac{x}{x_1}\right\vert&amp;lt;1,
\end{equation}
which leads to the result that geometric series $\sum r^n$ converges (with the sum $\frac{1}{1-r}$). And hence, from \eqref{1} and by the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, the series $\sum\vert a_nx^n\vert$ also converges.&lt;/p&gt;

&lt;p&gt;Moreover, if $\sum a_n{x_1}^n$ diverges, then $\sum\vert a_n{x_1}^n\vert$ also diverges. By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, for any $x$ such that $\vert x\vert&amp;gt;\vert x_1\vert$, we also have that $\sum\vert a_nx^n\vert$ diverges. This leads to the divergence of $\sum a_nx^n$, because if the series $\sum a_nx^n$ converges, so does $\sum\vert a_nx^n\vert$, which contradicts to our result.&lt;/p&gt;

&lt;p&gt;These are some main facts about the convergence behavior of an arbitrary power series and some properties of its:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a power series $\sum a_nx^n$, precisely one of the following is true:
    &lt;ul&gt;
      &lt;li&gt;The series converges only for $x=0$.&lt;/li&gt;
      &lt;li&gt;The series is absolutely convergent for all $x$.&lt;/li&gt;
      &lt;li&gt;There exists a positive real number $R$ such that the series is absolutely convergent for $\vert x\vert&amp;lt;R$ and divergent for $\vert x\vert&amp;gt;R$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The positive real number $R$ is called &lt;strong&gt;radius of convergence&lt;/strong&gt; of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$.&lt;/li&gt;
  &lt;li&gt;The set of all $x$’s for which a power series converges is called its &lt;strong&gt;interval of convergence&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;When the series converges only for $x=0$, we define $R=0$; and we define $R=\infty$ when the series converges for all $x$.&lt;/li&gt;
  &lt;li&gt;Every power series $\sum a_nx^n$ has a radius of convergence $R$, where $0\leq R\leq\infty$, with the property that the series converges absolutely if $\vert x\vert&amp;lt;R$ and diverges if $\vert x\vert&amp;gt;R$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eg1&quot;&gt;Example&lt;/h3&gt;
&lt;p&gt;Find the interval of convergence of the series
\begin{equation}
\sum_{n=0}^{\infty}\dfrac{x^n}{n+1}=1+\dfrac{x}{2}+\dfrac{x^2}{3}+\ldots
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
In order to find the interval of convergence of a series, we begin by identifying its radius of convergence.&lt;/p&gt;

&lt;p&gt;Consider a power series $\sum a_nx^n$. Suppose that this limit exists, and has $\infty$ as an allowed value, we have
\begin{equation}
\lim_{n\to\infty}\dfrac{\vert a_{n+1}x^{n+1}\vert}{a_nx^n}=\lim_{n\to\infty}\left\vert\dfrac{a_{n+1}}{a_n}\right\vert.\vert x\vert=\dfrac{\vert x\vert}{\lim_{n\to\infty}\left\vert\frac{a_n}{a_{n+1}}\right\vert}=L
\end{equation}
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#ratio-test&quot;&gt;ratio test&lt;/a&gt;, we have $\sum a_nx^n$ converges absolutely if $L&amp;lt;1$ and diverges in case of $L&amp;gt;1$. Or in other words, the series converges absolutely if
\begin{equation}
\vert x\vert&amp;lt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert,
\end{equation}
or diverges if
\begin{equation}
\vert x\vert&amp;gt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}
From the definition of radius of convergence, we can choose the radius of converge of $\sum a_nx^n$ as
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;Back to our problem, for the series $\sum\frac{x^n}{n+1}$, we have its radius of convergence is
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert=\lim_{n\to\infty}\dfrac{\frac{1}{n+1}}{\frac{1}{n+2}}=\lim_{n\to\infty}\dfrac{n+2}{n+1}=1
\end{equation}
At $x=1$, the series becomes the &lt;em&gt;harmonic series&lt;/em&gt; $1+\frac{1}{2}+\frac{1}{3}+\ldots$, which diverges; and at $x=-1$, it is the &lt;em&gt;alternating harmonic series&lt;/em&gt; $1-\frac{1}{2}+\frac{1}{3}-\ldots$, which converges. Hence, the interval of convergence of the series is $[-1,1)$.&lt;/p&gt;

&lt;h2 id=&quot;dif-int-power-series&quot;&gt;Differentiation and Integration of Power Series&lt;/h2&gt;

&lt;p&gt;It is easily seen that the sum of the series $\sum_{n=0}^{\infty}a_nx^n$  is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots\tag{2}\label{2}
\end{equation}
This relation between the series and the function is also expressed by saying that $\sum a_nx^n$ is a &lt;strong&gt;power series expansion&lt;/strong&gt; of $f(x)$.&lt;/p&gt;

&lt;p&gt;These are some crucial facts about that relation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(i) The function $f(x)$ defined by \eqref{2} is continuous on the open interval $(-R,R)$.&lt;/li&gt;
  &lt;li&gt;(ii) The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula
\begin{equation}
f’(x)=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots\tag{3}\label{3}
\end{equation}&lt;/li&gt;
  &lt;li&gt;(iii) If $x$ is any point in $(-R,R)$, then
\begin{equation}
\int_{0}^{x}f(t)\,dt=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{4}\label{4}
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
We have that series \eqref{3} and \eqref{4} converge on the interval $(-R,R)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We begin by proving the convergence on $(-R,R)$ of \eqref{3}.&lt;br /&gt;
Let $x$ be a point in the interval $(-R,R)$ and choose $\epsilon&amp;gt;0$ so that $\vert x\vert+\epsilon&amp;lt;R$. Since $\vert x\vert+\epsilon$ is in the interval, $\sum\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert$ converges.&lt;br /&gt;
We continue by proving the inequality
\begin{equation}
\vert nx^{n-1}\vert\leq\left(\vert x\vert+\epsilon\right)^n\hspace{1cm}\forall n\geq n_0,
\end{equation}
where $\epsilon&amp;gt;0$, $n_0$ is a positive integer.&lt;br /&gt;
We have
\begin{align}
\lim_{n\to\infty}n^{1/n}&amp;amp;=\lim_{n\to\infty} \\ &amp;amp;=\lim_{n\to\infty}\exp\left(\frac{\ln n}{n}\right) \\ &amp;amp;=\exp\left(\lim_{n\to\infty}\frac{\ln n}{n}\right) \\ &amp;amp;={\rm e}^0=1,
\end{align}
where in the fourth step, we use the &lt;em&gt;L’Hospital theorem&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Therefore, as $n\to\infty$
\begin{equation}
n^{1/n}\vert x\vert^{1-1/n}\to\vert x\vert
\end{equation}
Then for all sufficiently large $n$’s
\begin{align}
n^{1/n}\vert x\vert^{1-1/n}&amp;amp;\leq\vert x\vert+\epsilon \\ \vert nx^{n-1}\vert&amp;amp;\leq\left(\vert x\vert+\epsilon\right)^n
\end{align}
This implies that
\begin{equation}
\vert na_nx^{n-1}\vert\leq\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert
\end{equation}
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we have that the series $\sum\vert na_nx^{n-1}\vert$ converges, and so does $\sum na_nx^{n-1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since $\sum\vert a_nx^n\vert$ converges and
\begin{equation}
\left\vert\dfrac{a_nx^n}{n+1}\right\vert\leq\vert a_nx^n\vert,
\end{equation}
the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt; implies that $\sum\left\vert\frac{a_nx^n}{n+1}\right\vert$ converges, and therefore
\begin{equation}
x\sum\frac{a_nx^n}{n+1}=\sum\frac{1}{n+1}a_nx^{n+1}
\end{equation}
also converges.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dif-power-series&quot;&gt;Differentiation of Power Series&lt;/h3&gt;

&lt;p&gt;If we instead apply (ii) to the function $f’(x)$ in \eqref{3}, then it follows that $f’(x)$ is also differentiable. Doing the exact same process to $f’&apos;(x)$, we also have that $f’&apos;(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term&lt;/em&gt;.
\begin{equation}
\dfrac{d}{dx}\left(\sum a_nx^n\right)=\sum\dfrac{d}{dx}(a_nx^n)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;int-power-series&quot;&gt;Integration of Power Series&lt;/h3&gt;

&lt;p&gt;Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \eqref{4} as
\begin{equation}
\int\left(\sum a_nx^n\right)\,dx=\sum\left(\int a_nx^n\,dx\right)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;eg2&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;Find a power series expansion of ${\rm e}^x$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Since ${\rm e}^x$ is the only function that equals its own derivatives&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We therefore want each term to be the derivative of the one that follows it.&lt;/p&gt;

&lt;p&gt;Starting with $1$ as the constant term, the next should be $x$, then $\frac{1}{2}x^2$, then $\frac{1}{2.3}x^3$, and so on. This produces the series
\begin{equation}
1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots,\tag{5}\label{5}
\end{equation}
which converges for all $x$ because
\begin{equation}
R=\lim_{n\to\infty}\dfrac{\frac{1}{n!}}{\frac{1}{(n+1)!}}=\lim_{n\to\infty}(n+1)=\infty
\end{equation}
We have constructed the series \eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$,
\begin{equation}
{\rm e}^x=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;taylor-series-formula&quot;&gt;Taylor Series, Taylor’s Formula&lt;/h2&gt;

&lt;h3 id=&quot;taylor-series&quot;&gt;Taylor Series&lt;/h3&gt;
&lt;p&gt;Assume that $f(x)$ is the sum of a power series with positive radius of convergence
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots,\hspace{1cm}R&amp;gt;0\tag{6}\label{6}
\end{equation}
By the results obtained from previous section, differentiating \eqref{6} term by term we have
\begin{align}
f^{(1)}(x)&amp;amp;=a_1+2a_2x+3a_3x^2+\ldots \\ f^{(2)}(x)&amp;amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\ldots \\ f^{(3)}(x)&amp;amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\ldots
\end{align}
and in general,
\begin{equation}
f^{(n)}(x)=n!a_n+A(x),\tag{7}\label{7}
\end{equation}
where $A(x)$ contains $x$ as a factor.&lt;/p&gt;

&lt;p&gt;Since these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \eqref{7} we obtain
\begin{equation}
f^{(n)}(0)=n!a_n
\end{equation}
so
\begin{equation}
a_n=\dfrac{f^{(n)}(0)}{n!}
\end{equation}
Putting this result in \eqref{6}, our series becomes
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\ldots\tag{8}\label{8}
\end{equation}
This power series is called &lt;strong&gt;Taylor series&lt;/strong&gt; of $f(x)$ [at $x=0$], which is named after the person who introduced it, Brook Taylor.&lt;/p&gt;

&lt;p&gt;If we use the convention that $0!=1$, then \eqref{8} can be written as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(0)}{n!}x^n
\end{equation}
The numbers $a_n=\frac{f^{(n)}(0)}{n!}$ are called the &lt;strong&gt;Taylor coefficients&lt;/strong&gt; of $f(x)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
Given a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?&lt;br /&gt;
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\ldots
\end{equation}
Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is
\begin{align}
f(x)&amp;amp;=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;amp;=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\ldots\tag{9}\label{9}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;taylors-formula&quot;&gt;Taylor’s Formula&lt;/h3&gt;
&lt;p&gt;If we break off the Taylor series on the right side of \eqref{8} at the term containing $x^n$ and define the &lt;em&gt;remainder&lt;/em&gt; $R_n(x)$ by the equation
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\tag{10}\label{10}
\end{equation}
then the Taylor series on the right side of \eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when
\begin{equation}
\lim_{n\to\infty}R_n(x)=0
\end{equation}
Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing
\begin{equation}
R_n(x)=S_n(x)x^{n+1}
\end{equation}
for $x\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for $0\leq t\leq x$ (or $x\leq t\leq 0$) by writing
\begin{multline}
F(t)=f(x)-f(t)-f^{(1)}(t)(x-t)-\dfrac{f^{(2)}(t)}{2!}(x-t)^2-\ldots \\ -\dfrac{f^{(n)}(t)}{n!}(x-t)^n-S_n(x)(x-t)^{n+1}
\end{multline}
It is easily seen that $F(x)=0$. Also, from equation \eqref{10}, we have that $F(0)=0$. Then by the &lt;em&gt;Mean Value Theorem&lt;/em&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, $F’(c)=0$ for some constant $c$ between $0$ and $x$.&lt;/p&gt;

&lt;p&gt;By differentiating $F(t)$ w.r.t $t$, and evaluate it at $t=c$, we have
\begin{equation}
F’(c)=-\dfrac{f^{(n+1)}(c)}{n!}(x-c)^n+S_n(x)(n+1)(x-c)^n=0
\end{equation}
so
\begin{equation}
S_n(x)=\dfrac{f^{(n+1)}(c)}{(n+1)!}
\end{equation}
and
\begin{equation}
R_n(x)=S_n(x)x^{n+1}=\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}
\end{equation}
which makes \eqref{10} become
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1},
\end{equation}
where $c$ is some number between $0$ and $x$. This equation is called &lt;strong&gt;Taylor’s formula with derivative remainder&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, with this formula we can rewrite \eqref{9} as
\begin{multline}
f(x)=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots \\ +\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1},\tag{11}\label{11}
\end{multline}
where $c$ is some number between $a$ and $x$.&lt;/p&gt;

&lt;p&gt;The polynomial part of \eqref{11}
\begin{multline}
\sum_{j=0}^{n}\dfrac{f^{(j)}(a)}{j!}(x-a)^j=f(a)+f^{(1)}(a)(x-a) \\ +\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n
\end{multline}
is called the &lt;strong&gt;nth-degree Taylor polynomial at&lt;/strong&gt; $x=a$.&lt;/p&gt;

&lt;p&gt;On the other hand, the remainder part of \eqref{11}
\begin{equation}
R_n(x)=\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1}
\end{equation}
is often called &lt;strong&gt;Lagrange’s remainder formula&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
It is worth remarking that power series expansions are &lt;em&gt;unique&lt;/em&gt;. This means that if a function $f(x)$ can be expressed as a sum of a power series by &lt;em&gt;any method&lt;/em&gt;, then this series must be the Taylor series of $f(x)$.&lt;/p&gt;

&lt;h2 id=&quot;op-power-series&quot;&gt;Operations on Power Series&lt;/h2&gt;

&lt;h3 id=&quot;mult&quot;&gt;Multiplication&lt;/h3&gt;
&lt;p&gt;Suppose we are given two power series expansions
\begin{align}
f(x)&amp;amp;=\sum a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+\ldots\tag{12}\label{12} \\ g(x)&amp;amp;=\sum b_nx^n=b_0+b_1x+b_2x^2+b_3x^3+\ldots\tag{13}\label{13}
\end{align}
both valid on $(-R,R)$. If we multiply these two series term by term, we obtain the power series
\begin{multline}
a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2 \\ +(a_0b_3+a_1b_2+a_2b_1+a_3b_0)x^3+\ldots
\end{multline}
Briefly, we have multiplied \eqref{12} and \eqref{13} to obtain
\begin{equation}
f(x)g(x)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n}a_kb_{n-k}\right)x^n\tag{14}\label{14}
\end{equation}
By the &lt;strong&gt;Theorem 10&lt;/strong&gt; from &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-vs-cond&quot;&gt;Absolute vs Conditionally Convergence&lt;/a&gt;, we have that this product of the series \eqref{12} and \eqref{13} actually converges on the interval $(-R,R)$ to the product of the functions $f(x)$ and $g(x)$, as indicated by \eqref{14}.&lt;/p&gt;

&lt;h3 id=&quot;div&quot;&gt;Division&lt;/h3&gt;
&lt;p&gt;With the two series \eqref{12} and \eqref{13}, we have
\begin{equation}
\dfrac{\sum a_nx^n}{\sum b_nx^n}=\left(\sum a_nx^n\right).\left(\dfrac{1}{\sum b_nx^n}\right)
\end{equation}
This suggests us that if we can expand $\frac{1}{\sum b_nx^n}$ in a power series with positive radius of convergence $\sum c_nx^n$, and multiply this series by $\sum a_nx^n$, we can compute the division of our two series $\sum a_nx^n$ and $\sum b_nx^n$.&lt;/p&gt;

&lt;p&gt;To do the division properly, it is necessary to assume that $b_0\neq0$ (for the case $x=0$). Moreover, without any loss of generality, we may assume that $b_0=1$, because with the assumption that $b_0\neq0$, we simply factor it out
\begin{equation}
\dfrac{1}{b_0+b_1x+b_2x^2+\ldots}=\dfrac{1}{b_0}.\dfrac{1}{1+\frac{b_1}{b_0}x+\frac{b_2}{b_0}x^2+\ldots}
\end{equation}&lt;/p&gt;

&lt;p&gt;We begin by determining the $c_n$’s. Since $\frac{1}{\sum b_nx^n}=\sum c_nx^n$, then $(\sum b_nx^n)(\sum c_nx^n)=1$, so
\begin{multline}
b_0c_0+(b_0c_1+b_1c_0)x+(b_0c_2+b_1c_1+b_2c_0)x^2+\ldots \\ +(b_0c_n+b_1c_{n-1}+\ldots+b_nc_0)x^n+\ldots=1,
\end{multline}
and since $b_0=1$, we can determine the $c_n$’s recursively
\begin{align}
c_0&amp;amp;=1 \\ c_1&amp;amp;=-b_1c_0 \\ c_2&amp;amp;=-b_1c_1-b_2c_0 \\ &amp;amp;\vdots \\ c_n&amp;amp;=-b_1c_{n-1}-b_2c_{n-2}-\ldots-b_nc_0 \\ &amp;amp;\vdots
\end{align}
Now our work reduces to proving that the power series $\sum c_nx^n$ with these coefficients has positive radius of convergence, and for this it suffices to show that the series converges for at least one nonzero $x$.&lt;/p&gt;

&lt;p&gt;Let $r$ be any number such that $0&amp;lt;r&amp;lt;R$, so that $\sum b_nr^n$ converges. Then there exists a constant $K\geq 1$ with the property that $\vert b_nr^n\vert\leq K$ or $\vert b_n\vert\leq\frac{K}{r^n}$ for all $n$. Therefore,
\begin{align}
\vert c_0\vert&amp;amp;=1\leq K, \\ \vert c_1\vert&amp;amp;=\vert b_1c_0\vert=\vert b_1\vert\leq \dfrac{K}{r}, \\ \vert c_2\vert&amp;amp;\leq\vert b_1c_1\vert+\vert b_2c_0\vert\leq\dfrac{K}{r}.\dfrac{K}{r}+\dfrac{K}{r^2}.K=\dfrac{2K^2}{r^2}, \\ \vert c_3\vert&amp;amp;\leq\vert b_1c_2\vert+\vert b_2c_1\vert+\vert b_3c_0\vert\leq\dfrac{K}{r}.\dfrac{2K^2}{r^2}+\dfrac{K}{r^2}.\dfrac{K}{r}+\dfrac{K}{r^3}.K \\ &amp;amp;\hspace{5.3cm}\leq(2+1+1)\dfrac{K^3}{r^3}=\dfrac{4K^3}{r^3}=\dfrac{2^2K^3}{r^3},
\end{align}
since $K^2\leq K^3$ since $K\geq1$. In general,
\begin{align}
\vert c_n\vert&amp;amp;\leq\vert c_1b_{n-1}\vert+\vert c_2b_{n-2}\vert+\ldots+\vert b_nc_0\vert \\ &amp;amp;\leq\dfrac{K}{r}.\dfrac{2^{n-2}K^{n-1}}{r^{n-1}}+\dfrac{K}{r^2}.\dfrac{2^{n-3}K^{n-2}}{r^{n-2}}+\ldots+\dfrac{K}{r^n}.K \\ &amp;amp;\leq(2^{n-2}+2^{n-3}+\ldots+1+1)\dfrac{K^n}{r^n}=\dfrac{2^{n-1}K^n}{r^n}\leq\dfrac{2^nK^n}{r^n}
\end{align}
Hence, for any $x$ such that $\vert x\vert&amp;lt;\frac{r}{2K}$, we have that the series $\sum c_nx^n$ converges absolutely, and therefore converges, or in other words, $\sum c_nx^n$ has nonzero radius of convergence.&lt;/p&gt;

&lt;h3 id=&quot;sub&quot;&gt;Substitution&lt;/h3&gt;
&lt;p&gt;If a power series
\begin{equation}
f(X)=a_0+a_1x+a_2x^2+\ldots\tag{15}\label{15}
\end{equation}
converges for $\vert x\vert&amp;lt;R$ and if $\vert g(x)\vert&amp;lt;R$, then we can find $f(g(x))$ by substituting $g(x)$ for $x$ in \eqref{15}.&lt;/p&gt;

&lt;p&gt;Suppose $g(x)$ is given by a power series,
\begin{equation}
g(x)=b_0+b_1x+b_2x^2+\ldots,\tag{16}\label{16}
\end{equation}
therefore,
\begin{align}
f(g(x))&amp;amp;=a_0+a_1g(x)+a_2g(x)^2+\ldots \\ &amp;amp;=a_0+a_1(b+0+b_1x+\ldots)+a_2(b_0+b_1x+\ldots)^2+\ldots
\end{align}
The power series formed in this way converges to $f(g(x))$ whenever \eqref{16} is absolutely convergent and $\vert g(x)\vert&amp;lt;R$.&lt;/p&gt;

&lt;h3 id=&quot;even-odd-funcs&quot;&gt;Even and Odd Functions&lt;/h3&gt;
&lt;p&gt;A function $f(x)$ defined on $(-R,R)$ is said to be &lt;strong&gt;even&lt;/strong&gt; if
\begin{equation}
f(-x)=f(x),
\end{equation}
and &lt;strong&gt;odd&lt;/strong&gt; if
\begin{equation}
f(-x)=-f(x)
\end{equation}
Then if $f(x)$ is an even function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n}x^{2n}=a_0+a_2x^2+a_4x^4+\ldots
\end{equation}
and if $f(x)$ is an odd function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n+1}x^{2n+1}=a_1x+a_3x^3+a_5x^5+\ldots
\end{equation}
since if $f(x)=\sum_{n=0}^{\infty}a_nx^n$ is even, then $\sum_{n=0}^{\infty}a_nx^n=\sum_{n=0}^{\infty}(-1)^na_nx^n$, so by the uniqueness of the Taylor series expansion, we have that $a_n=(-1)^na_n$; similarly, $a_n=(-1)^{n+1}a_n$ if $f(x)$ is an odd function.&lt;/p&gt;

&lt;h2 id=&quot;uni-conv-power-series&quot;&gt;Uniform Convergence for Power Series&lt;/h2&gt;
&lt;p&gt;Consider a power series $\sum a_nx^n$ with positive radius of convergence $R$, and let $f(x)$ be its sum.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;#dif-int-power-series&quot;&gt;section&lt;/a&gt; above, we stated that $f(x)$ is continuous and differentiable on $(-R,R)$, and we can differentiate and integrate it term by term. So let’s prove these statements!&lt;/p&gt;

&lt;p&gt;Let $S_n(x)$ be the $n$-th partial sum of the series, so that
\begin{equation}
S_n(x)=\sum_{i=0}^{n}a_ix^i=a_0+a_1x+a_2x^2+\ldots+a_nx^n
\end{equation}
Similar to what we did in &lt;a href=&quot;#taylors-formula&quot;&gt;Taylor’s formula&lt;/a&gt;, we write
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
Thus, the remainder
\begin{equation}
R_n(x)=a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots
\end{equation}&lt;/p&gt;

&lt;p&gt;For each $x$ in the interval of convergence, we know that $R_n(x)\to0$ as $n\to\infty$; that is, for any given $\epsilon&amp;gt;0$, and for an integer $n_0$ large enough, we have
\begin{equation}
\vert R_n(x)\vert&amp;lt;\epsilon\hspace{1cm}n\geq n_0,\tag{17}\label{17}
\end{equation}
This is true for each $x$ individually, and is an equivalent way of expressing the fact that $\sum a_nx^n$ converges to $f(x)$.&lt;/p&gt;

&lt;p&gt;Moreover, for every $x$ in the given a closed interval $\vert x\vert\leq\vert x_1\vert&amp;lt;R$, we have
\begin{align}
\vert R_n(x)\vert&amp;amp;=\left\vert a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots\right\vert \\ &amp;amp;\leq\left\vert a_{n+1}x^{n+1}\right\vert+\left\vert a_{n+2}x^{n+2}\right\vert+\ldots \\ &amp;amp;\leq\left\vert a_{n+1}{x_1}^{n+1}\right\vert+\left\vert a_{n+2}{x_1}^{n+2}\right\vert+\ldots
\end{align}
Because of the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-conv&quot;&gt;absolute convergence&lt;/a&gt; of $\sum a_n{x_1}^n$, the last sum can be made $&amp;lt;\epsilon$ by taking $n$ large enough, $n\geq n_0$. Therefore, we have that \eqref{17} holds for all $x$ inside the closed interval $\vert x\vert\leq\vert x_1\vert$ inside the interval of convergence $(-R,R)$.&lt;/p&gt;

&lt;p&gt;Or in other words, $R_n(x)$ can be made small &lt;em&gt;independently of $x$ in the given closed interval&lt;/em&gt; $\vert x\vert\leq\vert x_1\vert$, which is equivalent to saying that the series $\sum a_nx^n$ is &lt;strong&gt;uniformly convergent&lt;/strong&gt; in this interval&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cont-sum&quot;&gt;Continuity of the Sum&lt;/h3&gt;
&lt;p&gt;In order to prove that $f(x)$ is continuous on $(-R,R)$, it suffices to prove that $f(x)$ is continuous at each point $x_0$ in the interval of convergence.&lt;/p&gt;

&lt;p&gt;Consider a closed subinterval $\vert x\vert\leq\vert x_1\vert&amp;lt;R$ containing $x_0$ in its interior. If $\epsilon&amp;gt;0$ is given, then by uniform convergence we can find an $n$ such that $\vert R_n(x)\vert&amp;lt;\epsilon$ for all $x$’s in the subinterval.&lt;/p&gt;

&lt;p&gt;Since the polynomial $S_n(x)$ is continuous at $x_0$, we can find $\delta&amp;gt;0$ small that $\vert x-x_0\vert&amp;lt;\delta$ implies $x$ lies in the subinterval and $\vert S_n(x)-S_n(x_0)\vert&amp;lt;\epsilon$. Putting these conditions together we find that $\vert x-x_0\vert&amp;lt;\delta$ implies
\begin{align}
\vert f(x)-f(x_0)\vert&amp;amp;=\left\vert S_n(x)+R_n(x)-\left(S_n(x_0)+R_n(x_0)\right)\right\vert \\ &amp;amp;=\left\vert\left(S_n(x)-S_n(x_0)\right)+R_n(x)-R_n(x_0)\right\vert \\ &amp;amp;\leq\left\vert S_n(x)-S_n(x_0)\right\vert+\left\vert R_n(x)\right\vert+\left\vert R_n(x_0)\right\vert \\ &amp;amp;&amp;lt;\epsilon+\epsilon+\epsilon=3\epsilon
\end{align}
which proves the continuity of $f(x)$ at $x_0$.&lt;/p&gt;

&lt;h3 id=&quot;int&quot;&gt;Integrating term by term&lt;/h3&gt;
&lt;p&gt;With what we have just proved that $f(x)=\sum a_nx^n$ is continuous on $(-R,R)$, we can therefore integrate this function between $a$ and $b$ that lie inside the interval
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx
\end{equation}
We need to prove that the right side of this equation can be integrated term by term, which is
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx=\sum\int_{a}^{b}a_nx^n\,dx\tag{18}\label{18}
\end{equation}
In order to prove this, we begin by observing that $S_n(x)$ is a polynomial, and for that reason it is continuous. Thus, all there of the functions in
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
are continuous on $(-R,R)$. This allows us to write
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}S_n(x)\,dx+\int_{a}^{b}R_n(x)\,dx
\end{equation}
Moreover, we can integrate $S_n(x)$ term by term
\begin{align}
\int_{a}^{b}S_n(x)\,dx&amp;amp;=\int_{a}^{b}\left(a_0+a_1x+a_2x^2+\ldots+a_nx^n\right)\,dx \\ &amp;amp;=\int_{a}^{b}a_0\,dx+\int_{a}^{b}a_1x\,dx+\int_{a}^{b}a_2x^2\,dx+\ldots+\int_{a}^{b}a_nx^n\,dx
\end{align}
To prove \eqref{18}, it therefore suffices to show that as $n\to\infty$
\begin{equation}
\int_{a}^{b}R_n(x)\,dx\to 0
\end{equation}
By uniform convergence, if $\epsilon&amp;gt;0$ is given and $\vert x\vert\leq\vert x_1\vert&amp;lt;R$ is a closed subinterval of $(-R,R)$ that contains both $a,b$, then $\vert R_n(x)\vert&amp;lt;\epsilon$ for all $x$ in the subinterval and $n$ large enough. Hence,
\begin{equation}
\left\vert\int_{a}^{b}R_n(x)\,dx\right\vert\leq\int_{a}^{b}\left\vert R_n(x)\right\vert\,dx&amp;lt;\epsilon\vert b-a\vert
\end{equation}
for any $n$ large enough, which proves our statement.&lt;/p&gt;

&lt;p&gt;As a special case of \eqref{18}, we take the limits $0$ and $x$ instead of $a$ and $b$, and obtain
\begin{align}
\int_{a}^{b}f(t)\,dt&amp;amp;=\sum\dfrac{1}{n+1}a_nx^{n+1} \\ &amp;amp;=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{19}\label{19}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;dif&quot;&gt;Differentiating term by term&lt;/h3&gt;
&lt;p&gt;We now prove that the function $f(x)$ is not only continuous but also differentiable on $(-R,R)$, and that its derivative can be calculated by differentiating term by term
\begin{equation}
f’(x)=\sum na_nx^{n-1}
\end{equation}
It is easily seen that the series on right side of this equation is exact the series on the right side of \eqref{3}, which is convergent on $(-R,R)$ as we proved. If we denote its sum by $g(x)$
\begin{equation}
g(x)=\sum na_nx^{n-1}=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots,
\end{equation}
then \eqref{19} tells us that
\begin{align}
\int_{0}^{x}g(t)\,dt&amp;amp;=a_1x+a_2x^2+a_3x^3+\ldots \\ &amp;amp;=f(x)-a_0
\end{align}
Since the left side of this has a derivative, so does the right side, and by differentiating we obtain
\begin{equation}
f’(x)=g(x)=\sum na_nx^{n-1}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] George F.Simmons. &lt;a href=&quot;https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424&quot;&gt;Calculus With Analytic Geometry - 2nd Edition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Marian M. &lt;a href=&quot;https://www.springer.com/gp/book/9780387789323&quot;&gt;A Concrete Approach to Classical Analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] MIT 18.01. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&quot;&gt;Single Variable Calculus&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;L’Hospital&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Assume $f$ and $g$ are real and differentiable on $]a,b[$ and $g’(x)\neq 0$ for all $x\in]a,b[$, where $-\infty\leq a&amp;lt;b\leq\infty$. Suppose as $x\to a$,
\begin{equation}
\dfrac{f’(x)}{g’(x)}\to A\,(\in[-\infty,\infty])
\end{equation}
If as $x\to a$, $f(x)\to 0$ and $g(x)\to 0$ or if $g(x)\to+\infty$ as $x\to a$, then
\begin{equation}
\dfrac{f(x)}{g(x)}\to A
\end{equation}
as $x\to a$.&lt;/em&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider the function $f(x)=a^x$.&lt;br /&gt;
Using the definition of the derivative, we have
\begin{align}
\dfrac{d}{dx}f(x)&amp;amp;=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h} \\ &amp;amp;=\lim_{h\to 0}\dfrac{a^{x+h}-a^x}{h} \\ &amp;amp;=a^x\lim_{h\to 0}\dfrac{a^h-1}{h}
\end{align}
Therefore,
\begin{equation}
\lim_{h\to 0}\dfrac{a^h-1}{h}=1
\end{equation}
then, let $n=\frac{1}{h}$, we have
\begin{equation}
a=\lim_{h\to 0}\left(1+\dfrac{1}{h}\right)^{1/h}=\lim_{n\to\infty}\left(1+\dfrac{1}{n}\right)^n={\rm e}
\end{equation}
Thus, $f(x)=a^x={\rm e}^x$. Every function $y=c{\rm e}^x$ also satisfies the differential equation $\frac{dy}{dx}=y$, because
\begin{equation}
\dfrac{dy}{dx}=\dfrac{d}{dx}c{\rm e}^x=c\dfrac{d}{dx}{\rm e}^x=c{\rm e}^x=y
\end{equation}&lt;br /&gt;
The rest of our proof is to prove that these are only functions that are unchanged by differentiation.&lt;br /&gt;
To prove this, suppose $f(x)$ is any function with that property. By the quotient rule,
\begin{equation}
\dfrac{d}{dx}\dfrac{f(x)}{e^x}=\dfrac{f’(x)e^x-e^x f(x)}{e^{2x}}=\dfrac{e^x f(x)-e^x f(x)}{e^{2x}}=0
\end{equation}
which implies that
\begin{equation}
\dfrac{f(x)}{e^x}=c,
\end{equation}
for some constant $c$, and so $f(x)=ce^x$. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Mean Value Theorem&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If a function $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable in the open interval $(a,b)$, then there exists at least one number $c$ between $a$ and $b$ with the property that&lt;/em&gt;
\begin{equation}
f’(c)=\frac{f(b)-f(a)}{b-a}
\end{equation} &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We will talk more about uniform convergence in the post of sequences. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="power-series" /><category term="taylor-series" /><category term="random-stuffs" /><summary type="html">Recall that in the previous post, Infinite Series of Constants, we mentioned a type of series called power series a lot. In the content of this post, we will be diving deeper into details of that series.</summary></entry><entry><title type="html">Infinite Series of Constants</title><link href="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html" rel="alternate" type="text/html" title="Infinite Series of Constants" /><published>2021-09-06T11:20:00+07:00</published><updated>2021-09-06T11:20:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">&lt;blockquote&gt;
  &lt;p&gt;No idea what to say yet :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#infinite-series&quot;&gt;Infinite Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#examples&quot;&gt;Examples&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convergent-sequences&quot;&gt;Convergent Sequences&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sequences&quot;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lim-seq&quot;&gt;Limits of Sequences&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison tests&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#comparison-test&quot;&gt;Comparison test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#limit-comparison-test&quot;&gt;Limit comparison test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-test-euler-c&quot;&gt;The Integral test. Euler’s constant&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#integral-test&quot;&gt;Integral test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#euler-c&quot;&gt;Euler’s constant&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ratio-root&quot;&gt;The Ratio test. Root test&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ratio-test&quot;&gt;Ratio test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#root-test&quot;&gt;Root test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#extended-ratio-test&quot;&gt;The Extended Ratio tests of Raabe and Gauss&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#kummers-theorem&quot;&gt;Kummer’s theorem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#raabes-test&quot;&gt;Raabe’s test&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#gausss-test&quot;&gt;Gauss’s test&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#alt-test-abs-conv&quot;&gt;The Alternating Series test. Absolute Convergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#alt-series&quot;&gt;Alternating Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#alt-series-test&quot;&gt;Alternating Series test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abs-conv&quot;&gt;Absolute Convergence&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abs-vs-cond&quot;&gt;Absolute vs. Conditionally Convergence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dirichlets-test&quot;&gt;Dirichlet’s test&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#abel-part-sum&quot;&gt;Abel’s partial summation formula&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#d-test&quot;&gt;Dirichlet’s test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;infinite-series&quot;&gt;Infinite Series&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;infinite series&lt;/strong&gt;, or simply a &lt;strong&gt;series&lt;/strong&gt;, is an expression of the form
\begin{equation}
a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{\infty}a_n
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Infinite decimal&lt;/em&gt;
\begin{equation}
.a_1a_2\ldots a_n\ldots=\dfrac{a_1}{10}+\dfrac{a_2}{10^2}+\ldots+\dfrac{a_n}{10^n}+\ldots,
\end{equation}
where $a_i\in\{0,1,\dots,9\}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Power series expansion&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Geometric series
\begin{equation}
\dfrac{1}{1-x}=\sum_{n=0}^{\infty}x^n=1+x+x^2+x^3+\dots,\hspace{1cm}\vert x\vert&amp;lt;1
\end{equation}&lt;/li&gt;
      &lt;li&gt;Exponential function
\begin{equation}
{\rm e}^x=\sum_{n=0}^{\infty}\dfrac{x^n}{n!}=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots
\end{equation}&lt;/li&gt;
      &lt;li&gt;Sine and cosine formulas
\begin{align}
\sin x&amp;amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n+1}}{(2n+1)!}=x-\dfrac{x^3}{3!}+\dfrac{x^5}{5!}-\dfrac{x^7}{7!}+\ldots \\ \cos x&amp;amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n}}{(2n)!}=1-\dfrac{x^2}{2!}+\dfrac{x^4}{4!}-\dfrac{x^6}{6!}+\ldots
\end{align}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convergent-sequences&quot;&gt;Convergent Sequences&lt;/h2&gt;

&lt;h3 id=&quot;sequences&quot;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$’s are said to form a &lt;strong&gt;sequence&lt;/strong&gt; (denoted as $\{x_n\}$)
\begin{equation}
x_1,x_2,\dots,x_n,\dots
\end{equation}
We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.&lt;/p&gt;

&lt;p&gt;A sequence $\{x_n\}$ is said to be &lt;em&gt;bounded&lt;/em&gt; if there exists $A, B$ such that $A\leq x_n\leq B, \forall n$. $A, B$ respectively are called &lt;em&gt;lower bound&lt;/em&gt;, &lt;em&gt;upper bound&lt;/em&gt; of the sequence. A sequence that is not bounded is said to be &lt;em&gt;unbounded&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lim-seq&quot;&gt;Limits of Sequences&lt;/h3&gt;
&lt;p&gt;A sequence $\{x_n\}$ is said to have a number $L$ as &lt;strong&gt;limit&lt;/strong&gt; if for each $\epsilon&amp;gt;0$, there exists a positive integer $n_0$ that
\begin{equation}
\vert x_n-L\vert&amp;lt;\epsilon\hspace{1cm}n\geq n_0
\end{equation}
We say that $x_n$ &lt;em&gt;converges to&lt;/em&gt; $L$ &lt;em&gt;as&lt;/em&gt; $n$ &lt;em&gt;approaches infinite&lt;/em&gt; ($x_n\to L$ as $n\to\infty$) and denote this as
\begin{equation}
\lim_{n\to\infty}x_n=L
\end{equation}&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A sequence is said to &lt;strong&gt;converge&lt;/strong&gt; or to be &lt;strong&gt;convergent&lt;/strong&gt; if it has a limit.&lt;/li&gt;
  &lt;li&gt;A convergent sequence is bounded, but not all bounded sequences are convergent.&lt;/li&gt;
  &lt;li&gt;If $x_n\to L,y_n\to M$, then
\begin{align}
&amp;amp;\lim(x_n+y_n)=L+M \\ &amp;amp;\lim(x_n-y_n)=L-M \\ &amp;amp;\lim x_n y_n=LM \\ &amp;amp;\lim\dfrac{x_n}{y_n}=\dfrac{L}{M}\hspace{1cm}M\neq0
\end{align}&lt;/li&gt;
  &lt;li&gt;An &lt;em&gt;increasing&lt;/em&gt; (or &lt;em&gt;decreasing&lt;/em&gt;) sequence converges if and only if it is bounded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/h2&gt;
&lt;p&gt;Recall from the previous sections that if $a_1,a_2,\dots,a_n,\dots$ is a &lt;em&gt;sequence&lt;/em&gt; of numbers, then
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots\tag{1}\label{1}
\end{equation}
is called an &lt;em&gt;infinite series&lt;/em&gt;. We begin by establishing the sequence of &lt;em&gt;partial sums&lt;/em&gt;
\begin{align}
s_1&amp;amp;=a_1 \\ s_2&amp;amp;=a_1+a_2 \\ &amp;amp;\,\vdots \\ s_n&amp;amp;=a_1+a_2+\dots+a_n \\ &amp;amp;\,\vdots
\end{align}
The series \eqref{1} is said to be &lt;strong&gt;convergent&lt;/strong&gt; if the sequences $\{s_n\}$ converges. And if $\lim s_n=s$, then we say that \eqref{1} converges to $s$, or that $s$ is the sum of the series.
\begin{equation}
\sum_{n=1}^{\infty}a_n=s
\end{equation}
If the series does not converge, we say that it &lt;strong&gt;diverges&lt;/strong&gt; or is &lt;strong&gt;divergent&lt;/strong&gt;, and no sum is assigned to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (&lt;em&gt;harmonic series&lt;/em&gt;)&lt;br /&gt;
Let’s consider the convergence of &lt;em&gt;harmonic series&lt;/em&gt;
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots\tag{2}\label{2}
\end{equation}
Let $m$ be a positive integer and choose $n&amp;gt;2^{m+1}$. We have
\begin{align}
s_n&amp;amp;&amp;gt;1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\dots+\frac{1}{2^{m+1}} \\ &amp;amp;=\left(1+\frac{1}{2}\right)+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\ldots+\frac{1}{8}\right)+\ldots+\left(\frac{1}{2^m+1}+\ldots+\frac{1}{2^{m+1}}\right) \\ &amp;amp;&amp;gt;\frac{1}{2}+2.\frac{1}{4}+4.\frac{1}{8}+\ldots+2^m.\frac{1}{2^{m+1}} \\ &amp;amp;=(m+1)\frac{1}{2}
\end{align}
This proves that $s_n$ can be made larger than the sum of any number of $\frac{1}{2}$’s and therefore as large as we please, by taking $n$ large enough, so the $\{s_n\}$ are unbounded, which leads to that \eqref{2} is a divergent series.
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots=\infty
\end{equation}&lt;/p&gt;

&lt;p&gt;The simplest general principle that is useful to study the convergence of a series is the &lt;strong&gt;$\mathbf{n}$-th term test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;nth-term-test&quot;&gt;$\mathbf{n}$-th term test&lt;/h3&gt;
&lt;p&gt;If the series $\{a_n\}$ converges, then $a_n\to 0$ as $n\to\infty$; or equivalently, if $\neg(a_n\to0)$ as $n\to\infty$, then the series must necessarily diverge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
When $\{a_n\}$ converges, as $n\to\infty$ we have
\begin{equation}
a_n=s_n-s_{n-1}\to s-s=0
\end{equation}
This result shows that $a_n\to 0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e., it does not imply the convergence of the series when $a_n\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;h2 id=&quot;gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Any finite number of 0’s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges).&lt;/li&gt;
  &lt;li&gt;When two convergent series are added term by term, the resulting series converges to the expected sum; i.e., if $\sum_{n=1}^{\infty}a_n=s$ and $\sum_{n=1}^{\infty}b_n=t$, then
\begin{equation}
\sum_{n=1}^{\infty}(a_n+b_n)=s+t
\end{equation}
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  Let $\{s_n\}$ and $\{t_n\}$ respectively be the sequences of partial sums of $\sum_{n=1}^{\infty}a_n$ and $\sum_{n=1}^{\infty}b_n$. As $n\to\infty$ we have
  \begin{align}
  (a_1+b_1)+(a_2+b_2)+\dots+(a_n+b_n)&amp;amp;=\sum_{i=1}^{n}a_i+\sum_{i=1}^{n}b_i \\ &amp;amp;=s_n+t_n\to s+t
  \end{align}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Similarly, $\sum_{n=1}^{\infty}(a_n-b_n)=s-t$ and $\sum_{n=1}^{\infty}ca_n=cs$ for any constant $c$.&lt;/li&gt;
  &lt;li&gt;Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  If $\sum_{n=1}^{\infty}a_n=s$, then
  \begin{equation}
  \lim_{n\to\infty}(a_0+a_1+a_2+\dots+a_n)=\lim_{n\to\infty} a_0+\lim_{n\to\infty}(a_1+a_2+\dots+a_n)=a_0+s
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison Tests&lt;/h2&gt;
&lt;p&gt;The easiest infinite series to work with are those whose terms are all nonnegative numbers. The reason, as we saw in the above &lt;a href=&quot;#conv-div-series&quot;&gt;section&lt;/a&gt;, is that if $a_n\geq0$, then the series $\sum a_n$ converges if and only if its sequence $\{s_n\}$ of partial sums is bounded (since $s_{n+1}=s_n+a_{n+1}$).&lt;/p&gt;

&lt;p&gt;Thus, in order to establish the convergence of a series of nonnegative terms, it suffices to show that its terms approach zero fast enough, or at least as fast as the terms of a known convergent series of nonnegative terms to keep the partial sums bounded.&lt;/p&gt;

&lt;h3 id=&quot;comparison-test&quot;&gt;Comparison test&lt;/h3&gt;
&lt;p&gt;If $0\leq a_n\leq b_n$, then&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\sum a_n$ converges if $\sum b_n$ converges.&lt;/li&gt;
  &lt;li&gt;$\sum b_n$ diverges if $\sum a_n$ diverges.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
If $s_n, t_n$ respectively are the partial sums of $\sum a_n,\sum b_n$, then
\begin{equation}
0\leq s_n=\sum_{i=1}^{n}a_i\leq\sum_{i=1}^{n}b_i=t_n
\end{equation}
Then if $\{t_n\}$ is bounded, then so is $\{s_n\}$; and if $\{s_n\}$ is unbounded, then so is $\{t_n\}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
Consider convergence behavior of two series
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{2^n+1};\hspace{2cm}\sum_{n=1}^{\infty}\frac{1}{\ln n}
\end{equation}
The first series converges, because
\begin{equation}
\frac{1}{2^n+1}&amp;lt;\frac{1}{2^n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{2^n}=1$, which is a convergent series. At the same time, the second series diverges, since
\begin{equation}
\frac{1}{n}\leq\frac{1}{\ln n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges.&lt;/p&gt;

&lt;p&gt;One thing worth remarking is that the condition $0\leq a_n\leq b_n$ for the comparison test need not hold for all $n$, but only for all $n$ from some point on.&lt;/p&gt;

&lt;p&gt;The comparison test is simple, but in some cases where it is difficult to establish the necessary inequality between the n-th terms of the two series. And since limits are often easier to work with than inequalities, we have the following test.&lt;/p&gt;

&lt;h3 id=&quot;limit-comparison-test&quot;&gt;Limit comparison test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n, \sum b_n$ are series with positive terms such that
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=1\tag{3}\label{3}
\end{equation}
then either both series converge or both series diverge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
we observe that \eqref{3} implies that for all sufficient large $n$, we have
\begin{align}
\frac{1}{2}&amp;amp;\leq\frac{a_n}{b_n}\leq 2 \\ \text{or}\hspace{1cm}\frac{1}{2}b_n&amp;amp;\leq a_n\leq 2b_n
\end{align}
which leads to the fact that $\sum a_n$ and $\sum b_n$ have the same convergence behavior.&lt;/p&gt;

&lt;p&gt;The condition \eqref{3} can be generalized by
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=L,
\end{equation}
where $0&amp;lt;L&amp;lt;\infty$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; ($p$&lt;em&gt;-series&lt;/em&gt;)&lt;br /&gt;
Consider the convergence behavior of the series
\begin{equation}
\sum_{n=1}^{\infty}\dfrac{1}{n^p}=1+\dfrac{1}{2^p}+\dfrac{1}{3^p}+\dfrac{1}{4^p}+\ldots,\tag{4}\label{4}
\end{equation}
where $p$ is a positive constant.&lt;/p&gt;

&lt;p&gt;If $p\leq 1$, then $n^p\leq n$ or $\frac{1}{n}\leq\frac{1}{n^p}$. Thus, by comparison with the harmonic series $\sum\frac{1}{n}$, we have that \eqref{4} diverges.&lt;/p&gt;

&lt;p&gt;If $p&amp;gt;1$, let $n$ be given and choose $m$ so that $n&amp;lt;2^m$. Then
\begin{align}
s_n&amp;amp;\leq s_{2^m-1} \\ &amp;amp;=1+\left(\dfrac{1}{2^p}+\dfrac{1}{3^p}\right)+\left(\dfrac{1}{4^p}+\ldots+\dfrac{1}{7^p}\right)+\ldots+\left[\dfrac{1}{(2^{m-1})^p}+\ldots+\dfrac{1}{(2^m-1)^p}\right] \\ &amp;amp;\leq 1+\dfrac{2}{2^p}+\dfrac{4}{4^p}+\ldots+\dfrac{2^{m-1}}{(2^{m-1})^p}
\end{align}
Let $a=\frac{1}{2^{p-1}}$, then $a&amp;lt;1$ since $p&amp;gt;1$, and
\begin{equation}
s_n\leq 1+a+a^2+\ldots+a^{m-1}=\dfrac{1-a^m}{1-a}&amp;lt;\dfrac{1}{1-a}
\end{equation}
which proves that $\{s_n\}$ has an upper bound. Thus \eqref{4} converges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a convergent series of nonnegative terms is rearranged in any manner, then the resulting series also converges and has the same sum.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider two series $\sum a_n$ and $\sum b_n$, where $\sum a_n$ is a convergent series of nonnegative terms and $\sum b_n$ is formed form $\sum a_n$ by rearranging its terms.&lt;/p&gt;

&lt;p&gt;Let $p$ be a positive integer and consider the $p$-partial sum $t_p=b_1+\ldots+b_p$ of $\sum b_n$. Since each $b$ is some $a$, then there exists an $m$ such that each term in $t_p$ is one of the terms in $s_m=a_1+\ldots+a_m$. This shows us that $t_p\leq s_m\leq s$. Thus, $\sum b_n$ converges to a sum $t\leq s$.&lt;/p&gt;

&lt;p&gt;On the other hand, $\sum a_n$ is also a rearrangement of $\sum b_n$, so by the same procedure, similarly we have that $s\leq t$, and therefore $t=s$.&lt;/p&gt;

&lt;h2 id=&quot;int-test-euler-c&quot;&gt;The Integral test. Euler’s constant&lt;/h2&gt;
&lt;p&gt;In this section, we will be going through a more detailed class of infinite series with nonnegative terms which is those whose terms form a decreasing sequence of positive numbers.&lt;/p&gt;

&lt;p&gt;We begin by considering a series
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots
\end{equation}
whose terms are positive and decreasing. Suppose $a_n=f(n)$, as shown is &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/integral-test.png&quot; alt=&quot;integral test&quot; width=&quot;500px&quot; height=&quot;230px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;On the left in this figure we see that the rectangles of areas $a_1,a_2,\dots,a_n$ have a greater combined area than the area under the curve from $x=1$ to $x=n1$, so
\begin{equation}
a_1+a_2+\dots+a_n\geq\int_{1}^{n+1}f(x)\,dx\geq\int_{1}^{n}f(x)\,dx\tag{5}\label{5}
\end{equation}
On the right side of the figure, the rectangles lie under the curve, which makes
\begin{align}
a_2+a_3+\dots+a_n&amp;amp;\leq\int_{1}^{n}f(x)\,dx \\ a_1+a_2+\dots+a_n&amp;amp;\leq a_1+\int_{1}^{n}f(x)\,dx\tag{6}\label{6}
\end{align}
Putting \eqref{5} and \eqref{6} together we have
\begin{equation}
\int_{1}^{n}f(x)\,dx\leq a_1+a_2+\dots+a_n\leq a_1+\int_{1}^{n}f(x)\,dx\tag{7}\label{7}
\end{equation}
The result we obtained in \eqref{7} allows us to establish the &lt;strong&gt;integral test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;integral-test&quot;&gt;Integral test&lt;/h3&gt;

&lt;p&gt;If $f(x)$ is a positive decreasing function for $x\geq1$ such that $f(n)=a_n$ for each positive integer $n$, then the series and integral
\begin{equation}
\sum_{n=1}^{\infty}a_n;\hspace{2cm}\int_{1}^{\infty}f(x)\,dx
\end{equation}
converge or diverge together.&lt;/p&gt;

&lt;p&gt;The integral test holds for any interval of the form $x\geq k$, not just for $x\geq 1$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; (&lt;em&gt;Abel’s series&lt;/em&gt;)&lt;br /&gt;
Let’s consider the convergence behavior of the series
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n\ln n}\tag{8}\label{8}
\end{equation}
By the integral test, we have that \eqref{8} diverges, because
\begin{equation}
\sum_{2}^{\infty}\frac{dx}{x\ln x}=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x\ln x}=\lim_{b\to\infty}\left(\ln\ln x\Big|_{2}^{b}\right)=\lim_{b\to\infty}\left(\ln\ln b-\ln\ln 2\right)=\infty
\end{equation}
More generally, if $p&amp;gt;0$, then
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n(\ln n)^p}
\end{equation}
converges if $p&amp;gt;1$ and diverges if $0&amp;lt;p\leq 1$. For if $p\neq 1$, we have
\begin{align}
\int_{2}^{\infty}\frac{dx}{x(\ln x)^p}&amp;amp;=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x(\ln x)^p} \\ &amp;amp;=\lim_{b\to\infty}\left[\dfrac{(\ln x)^{1-p}}{1-p}\Bigg|_2^b\right] \\ &amp;amp;=\lim_{b\to\infty}\left[\dfrac{(\ln b)^{1-p}-(\ln 2)^{1-p}}{1-p}\right]
\end{align}
exists if and only if $p&amp;gt;1$.&lt;/p&gt;

&lt;h3 id=&quot;euler-c&quot;&gt;Euler’s constant&lt;/h3&gt;
&lt;p&gt;From \eqref{7} we have that
\begin{equation}
0\leq a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\leq a_1
\end{equation}
Denoting $F(n)=a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx$, the above expression becomes
\begin{equation}
0\leq F(n)\leq a_1
\end{equation}
Moreover, $\{F(n)\}$ is a decreasing sequence, because
\begin{align}
F(n)-F(n+1)&amp;amp;=\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]-\left[a_1+a_2+\ldots+a_{n+1}-\int_{1}^{n+1}f(x)\,dx\right] \\ &amp;amp;=\int_{n}^{n+1}f(x)\,dx-a_{n+1}\geq 0
\end{align}
where the last step can be seen by observing the right side of &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Since any decreasing sequence of nonnegative numbers converges, we have that
\begin{equation}
L=\lim_{n\to\infty}F(n)=\lim_{n\to\infty}\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]\tag{9}\label{9}
\end{equation}
exists and satisfies the inequalities $0\leq L\leq a_1$.&lt;/p&gt;

&lt;p&gt;Let $a_n=\frac{1}{n}$ and $f(x)=\frac{1}{x}$, the last quantity in \eqref{9} becomes
\begin{equation}
\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)\tag{10}\label{10}
\end{equation}
since
\begin{equation}
\int_{1}^{n}\dfrac{dx}{x}=\ln x\Big|_1^n=\ln n
\end{equation}
The value of the limit \eqref{10} is called &lt;strong&gt;Euler’s constant&lt;/strong&gt; (denoted as $\gamma$).
\begin{equation}
\gamma=\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;ratio-root&quot;&gt;The Ratio test. Root test&lt;/h2&gt;

&lt;h3 id=&quot;ratio-test&quot;&gt;Ratio test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n$ is a series of positive terms such that
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}}{a_n}=L,\tag{11}\label{11}
\end{equation}
then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;if $L&amp;lt;1$, the series &lt;em&gt;converges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L&amp;gt;1$, the series &lt;em&gt;diverges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L=1$, the test is &lt;em&gt;inconclusive&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Let $L&amp;lt;1$ and choose any number $r$ such that $L&amp;lt;r&amp;lt;1$. From \eqref{11}, we have that there exists an $n_0$ such that
\begin{align}
\dfrac{a_{n+1}}{a_n}&amp;amp;\leq r=\dfrac{r^{n+1}}{r_n},\hspace{1cm}\forall n\geq n_0 \\ \dfrac{a_{n+1}}{r^{n+1}}&amp;amp;\leq\dfrac{a_n}{r^n},\hspace{2cm}\forall n\geq n_0
\end{align}
which means that $\{\frac{a_n}{r^n}\}$ is a decreasing sequence for $n\geq n_0$; in particular, $\frac{a_n}{r^n}\leq\frac{a_{n_0}}{r^{n_0}}$ for $n\geq n_0$. Thus, if we let $K=\frac{a_{n_0}}{r^{n_0}}$, then we get
\begin{equation}
a_n\leq Kr^n,\hspace{1cm}\forall n\geq n_0\tag{12}\label{12}
\end{equation}
However, $\sum Kr^n$ converges since $r&amp;lt;1$. Hence, by the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, \eqref{12} implies that $\sum a_n$ converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $L&amp;gt;1$, we have that $\frac{a_{n+1}}{a_n}\geq 1$, or equivalently $a_{n+1}\geq a_n$, for all $n\geq n_0$, for some constant $n_0$. That means $\neg(a_n\to 0)$ as $n\to\infty$ (since $\sum a_n$ is a series of positive terms).&lt;br /&gt;
By the &lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we know that the series diverges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider the $p$-series $\sum\frac{1}{n^p}$. For all values of $p$, as $n\to\infty$ we have
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^p}{(n+1)^p}=\left(\dfrac{n}{n+1}\right)^p\to 1
\end{equation}
As in the above example, we have that this series converges if $p&amp;gt;1$ and diverges if $p\leq 1$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;root-test&quot;&gt;Root test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n$ is a series of nonnegative terms such that
\begin{equation}
\lim_{n\to\infty}\sqrt[n]{a_n}=L,\tag{13}\label{13}
\end{equation}
then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;if $L&amp;lt;1$, the series &lt;em&gt;converges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L&amp;gt;1$, the series &lt;em&gt;diverges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L=1$, the test is &lt;em&gt;inconclusive&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Let $L&amp;lt;1$ and $r$ is any number such that $L&amp;lt;r&amp;lt;1$. From \eqref{13}, we have that there exist $n_0$ such that
\begin{align}
\sqrt[n]{a_n}&amp;amp;\leq r&amp;lt;1,\hspace{1cm}\forall n\geq n_0 \\ a_n&amp;amp;\leq r^n&amp;gt;1,\hspace{1cm}\forall n\geq n_0
\end{align}
And since the geometric series $\sum r^n$ converges, we clearly have that $\sum a_n$ also converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;gt;1$, then $\sqrt[n]{a_n}\geq 1$ for all $n\geq n_0$, for some $n_0$, so $a_n\geq 1$ for all $n\geq n_0$. That means as $n\to\infty$, $\neg(a_n\to 0)$. Therefore, by the &lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we have that the series diverges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For $L=1$, we provide 2 examples. One is the divergent series $\sum\frac{1}{n}$ and the other is the convergent series $\sum\frac{1}{n^2}$ (since $\sqrt[n]{n}\to 1$ as $n\to\infty$).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;extended-ratio-test&quot;&gt;The Extended Ratio tests of Raabe and Gauss&lt;/h3&gt;

&lt;h4 id=&quot;kummers-theorem&quot;&gt;Kummer’s theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt; (&lt;em&gt;Kummer’s&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Assume that $a_n&amp;gt;0,b_n&amp;gt;0$ and $\sum\frac{1}{b_n}$ diverges. If
\begin{equation}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)=L,\tag{14}\label{14}
\end{equation}
then $\sum a_n$ converges if $L&amp;gt;0$ and diverges if $L&amp;lt;0$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;gt;0$, then there exists $h$ such that $L&amp;gt;h&amp;gt;0$. From \eqref{14}, for some positive integer $n_0$ we have
\begin{align}
b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}&amp;amp;\geq h&amp;gt;0,\hspace{1cm}\forall n\geq n_0 \\ a_n b_n-a_{n+1}b_{n+1}&amp;amp;\geq ha_n&amp;gt;0,\hspace{1cm}\forall n\geq n_0\tag{15}\label{15}
\end{align}
Hence, $\{a_n b_n\}$ is a decreasing sequence of positive numbers for $n\geq n_0$, so $K=\lim a_n b_n$ exists.&lt;br /&gt;
Moreover, we have that
\begin{equation}
\sum_{n=n_0}^{\infty}a_nb_n-a_{n+1}b_{n+1}=a_{n_0}b_{n_0}-\lim_{n\to\infty}a_nb_n=a_{n_0}b_{n_0}-K
\end{equation}
Therefore, by \eqref{15} and the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we can conclude that $\sum ha_n$ converges, which means that $\sum a_n$ also converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;lt;0$, for some positive integer $n_0$ we have
\begin{equation}
a_nb_n-a_{n+1}b_{n+1}\leq 0,\hspace{1cm}\forall n\geq n_0
\end{equation}
Hence, $\{a_nb_n\}$ is a increasing sequence of positive number for all $n\geq n_0$, for some positive integer $n_0$. This also means for all $n\geq n_0$,
\begin{align}
a_nb_n&amp;amp;\geq a_{n_0}b_{n_0} \\ a_n&amp;amp;\geq (a_{n_0}b_{n_0}).\dfrac{1}{b_n}
\end{align}
Therefore $\sum a_n$ diverges (since $\sum\frac{1}{b_n}$ diverges).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;raabes-test&quot;&gt;Raabe’s test&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3&lt;/strong&gt; (&lt;em&gt;Raabe’s test&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n},
\end{equation}
where $A_n\to 0$, then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Take $n=b_n$ in &lt;em&gt;Kummber’s theorem&lt;/em&gt;. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;amp;=\lim\left[n-\left(1-\dfrac{A}{n}+\dfrac{A_n}{n}\right)(n+1)\right] \\ &amp;amp;=\lim\left[-1+\dfrac{A(n+1)}{n}-\dfrac{A_n(n+1)}{n}\right] \\ &amp;amp;=A-1
\end{align}
and by &lt;em&gt;Kummer’s theorem&lt;/em&gt; we have that $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Raabe’s test&lt;/em&gt; can be formulated as followed: If $a_n&amp;gt;0$ and
\begin{equation}
\lim n\left(1-\dfrac{a_{n+1}}{a_n}\right)=A,
\end{equation}
then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;When $A=1$ in &lt;em&gt;Raabe’s test&lt;/em&gt;, we turn to &lt;strong&gt;Gauss’s test&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;gausss-test&quot;&gt;Gauss’s test&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 4&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n^{1+c}},
\end{equation}
where $c&amp;gt;0$ and $A_n$ is bounded as $n\to\infty$, then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A\leq 1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If $A\neq 1$, the statement follows exactly from &lt;em&gt;Raabe’s test&lt;/em&gt;, since $\frac{A_n}{n^c}\to 0$ as $n\to\infty$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $A=1$, we begin by taking $b_n=n\ln n$ in &lt;em&gt;Kummer’s theorem&lt;/em&gt;. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;amp;=\lim\left[n\ln n-\left(1-\dfrac{1}{n}+\dfrac{A_n}{n^{1+c}}\right)(n+1)\ln(n+1)\right] \\ &amp;amp;=\lim\left[n\ln n-\dfrac{n^2-1}{n}\ln(n+1)-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;amp;=\lim\left[n\ln\left(\dfrac{n}{n+1}\right)+\dfrac{\ln(n+1)}{n}-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;amp;=-1+0-0=-1&amp;lt;0,
\end{align}
where in fourth step we use the &lt;em&gt;Stolz–Cesàro theorem&lt;/em&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. Therefore, by &lt;em&gt;Kummer’s theorem&lt;/em&gt;, we have that the series is divergent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem 5&lt;/strong&gt; (&lt;em&gt;Gauss’s test&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^k+\alpha n^{k-1}+\ldots}{n^k+\beta n^{k-1}+\ldots},\tag{16}\label{16}
\end{equation}
then $\sum a_n$ converges if $\beta-\alpha&amp;gt;1$ and diverges if $\beta-\alpha\leq 1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
If the quotient on the right of \eqref{16} is worked out by long division, we get
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{\beta-\alpha}{n}+\dfrac{A_n}{n^2},
\end{equation}
where $A_n$ is a quotient of the form
\begin{equation}
\dfrac{\gamma n^{k-2}+\ldots}{n^{k-2}+\ldots}
\end{equation}
and is therefore clearly bounded as $n\to\infty$. The statement now follows from &lt;strong&gt;Theorem 4&lt;/strong&gt; with $c=1$.&lt;/p&gt;

&lt;h2 id=&quot;alt-test-abs-conv&quot;&gt;The Alternating Series test. Absolute Convergence&lt;/h2&gt;
&lt;p&gt;Previously, we have been working with series of positive terms and nonnegative terms. It’s time to consider series with both positive and negative terms. The simplest are those whose terms are alternatively positive and negative.&lt;/p&gt;

&lt;h3 id=&quot;alt-series&quot;&gt;Alternating Series&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Alternating series&lt;/strong&gt; is series with the form
\begin{equation}
\sum_{n=1}^{\infty}(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+\ldots,\tag{17}\label{17}
\end{equation}
where $a_n$’s are all positive numbers.&lt;/p&gt;

&lt;p&gt;From the definition of alternating series, we establish &lt;strong&gt;alternating series test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;alt-series-test&quot;&gt;Alternating Series test&lt;/h3&gt;
&lt;p&gt;If the alternating series \eqref{17} has the property that&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$a_1\geq a_2\geq a_3\geq\ldots$&lt;/li&gt;
  &lt;li&gt;$a_n\to 0$ as $n\to\infty$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;then $\sum a_n$ converges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
On the one hand, we have that a typical even partial sum $s_{2n}$ can be written as
\begin{equation}
s_{2n}=(a_1-a_2)+(a_3-a_4)+\ldots+(a_{2n-1}-a_{2n}),
\end{equation}
where each expression in parentheses is nonnegative since $\{a_n\}$ is a decreasing sequence. Hence, we also have that $s_{2n}\leq s_{2n+2}$, which leads to the result that the even partial sums form an increasing sequence.&lt;/p&gt;

&lt;p&gt;Moreover, we can also display $s_{2n}$ as
\begin{equation}
s_{2n}=a_1-(a_2-a_3)-(a_4-a_5)-\ldots-(a_{2n-2}-a_{2n-1})-a_{2n},
\end{equation}
where each expression in parentheses once again is nonnegative. Thus, we have that $s_{2n}\leq a_1$, so ${s_{2n}}$ has an upper bound. Since every bounded increasing sequence converges, there exists a number $s$ such that
\begin{equation}
\lim_{n\to\infty}s_{2n}=s
\end{equation}&lt;/p&gt;

&lt;p&gt;On the other hand, the odd partial sums approach the same limit, because
\begin{align}
s_{2n+1}&amp;amp;=a_1-a_2+a_3-a_4+\ldots-a_{2n}+a_{2n+1} \\ &amp;amp;=s_{2n}+a_{2n+1}
\end{align}
and therefore
\begin{equation}
\lim_{n\to\infty}s_{2n+1}=\lim_{n\to\infty}s_{2n}+\lim_{n\to\infty}a_{2n+1}=s+0=s
\end{equation}
Since both sequence of even sums and sequence of odd partial sums converges to $s$ as $n$ tends to infinity, this shows us that $\{s_n\}$ also converges to $s$, and therefore the alternating series \eqref{17} converges to the sum $s$.&lt;/p&gt;

&lt;h3 id=&quot;abs-conv&quot;&gt;Absolute Convergence&lt;/h3&gt;
&lt;p&gt;A series $\sum a_n$ is said to be &lt;strong&gt;absolutely convergent&lt;/strong&gt; if $\sum\vert a_n\vert$ converges.&lt;/p&gt;

&lt;p&gt;These are some properties of absolute convergence.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Absolute convergence implies convergence.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Suppose that $\sum a_n$ is an absolutely convergent series, or $\sum\vert a_n\vert$ converges. We have that
\begin{equation}
0\leq a_n+\vert a_n\vert\leq 2\vert a_n\vert
\end{equation}
And since $\sum 2\vert a_n\vert$ converges, by &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we also have that $\sum(a_n+\vert a_n\vert)$ converges.&lt;br /&gt;
Since both $\sum\vert a_n\vert$ and $\sum(a_n+\vert a_n\vert)$ converge, so does their difference, which is $\sum a_n$.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A convergent series that is not absolutely convergent is said to be &lt;strong&gt;conditionally convergent&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Any conditionally convergent series can be made to converge to any given number as its sum, or even to diverge, by &lt;em&gt;suitably changing the order of its terms without changing the terms themselves&lt;/em&gt; (check out &lt;strong&gt;Theorem 8&lt;/strong&gt; to see the proof).&lt;/li&gt;
      &lt;li&gt;On the other hand, any absolutely convergent series can be rearranged in any manner without changing its convergence behavior or its sum (check out &lt;strong&gt;Theorem 7&lt;/strong&gt; to see the proof).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;abs-vs-cond&quot;&gt;Absolute vs Conditionally Convergence&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Consider a series $\sum a_n$ and define $p_n$ and $q_n$ by
\begin{align}
p_n&amp;amp;=\dfrac{\vert a_n\vert+a_n}{2} \\ q_n&amp;amp;=\dfrac{\vert a_n\vert-a_n}{2}
\end{align}
If $\sum a_n$ converges conditionally, then both $\sum p_n$ and $\sum q_n$ diverges.&lt;br /&gt;
If $\sum a_n$ converges absolutely, then $\sum p_n$ and $\sum q_n$ both converge and the sums of these series are related by the equation&lt;/em&gt;
\begin{equation}
\sum a_n=\sum p_n-\sum q_n
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From the formulas of $p_n$ and $q_n$, we have
\begin{align}
a_n&amp;amp;=p_n-q_n\tag{18}\label{18} \\ \vert a_n\vert&amp;amp;=p_n+q_n\tag{19}\label{19}
\end{align}&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We begin by proving the first statement.&lt;br /&gt;
When $\sum a_n$ converges, from \eqref{18}, we have $\sum p_n$ and $\sum q_n$ both must have the same convergence behavior (i.e., converge or diverge at the same time).&lt;br /&gt;
If they both converge, then from \eqref{19}, we have that $\sum\vert a_n\vert$ converges, contrary to the hypothesis, so $\sum p_n$ and $\sum q_n$ are both divergent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To prove the second statement, we assume that $\sum\vert a_n\vert$ converges. We have
\begin{equation}
p_n=\dfrac{\vert a_n\vert+a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which shows us that $\sum p_n$ converges. Similarly, for $q_n$, we have
\begin{equation}
q_n=\dfrac{\vert a_n\vert-a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which also lets us obtain that $\sum q_n$ converges.&lt;br /&gt;
Therefore
\begin{equation}
\sum p_n-\sum q_n=\sum(p_n-q_n)=\sum a_n
\end{equation}
&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem 7&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $\sum a_n$ is an absolutely convergent series with sum $s$, and if $a_n$’s are rearranged in any way to from a new series $\sum b_n$, then this new series is also absolutely convergent with sum $s$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $\sum\vert a_n\vert$ is a convergent series of nonnegative terms with sum $s$ and since the $b_n$’s are just the $a_n$’s in a different order, it follows from &lt;strong&gt;Theorem 1&lt;/strong&gt; that $\sum\vert b_n\vert$ also converges to $s$, and therefore $\sum b_n$ is absolutely convergent with sum $t$, for some positive $t$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt; allows us to write
\begin{equation}
s=\sum a_n=\sum p_n-\sum q_n
\end{equation}
and
\begin{equation}
t=\sum b_n=\sum P_n-\sum Q_n
\end{equation}
where each of the series on the right is convergent and consists of nonnegative. But the $P_n$’s and $Q_n$’s are simply the $p_n$’s and $q_n$’s in a different order. Hence, by &lt;strong&gt;Theorem 1&lt;/strong&gt;, we have $\sum P_n=\sum p_n$ and $\sum Q_n=\sum q_n$. And therefore, $t=s$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 8&lt;/strong&gt; (&lt;em&gt;Riemann’s rearrangement theorem&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Let $\sum a_n$ be a conditionally convergent series. Then its terms can be rearranged to yield a convergent series whose sum is an arbitrary preassigned number, or a series that diverges to $\infty$, or a series that diverges to $-\infty$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $\sum a_n$ converges conditionally, we begin by using &lt;strong&gt;Theorem 6&lt;/strong&gt; to form the two divergent series of nonnegative terms $\sum p_n$ and $\sum q_n$.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To prove the first statement, let $s$ be any number and construct a rearrangement of the given series as follows. Start by writing down $p$’s in order until the partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}
\end{equation}
is first $\geq s$; next we continue with $-q$’s until the total partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}-q_1-q_2-\ldots-q_{m_1}
\end{equation}
is first $\leq s$; then we continue with $p$’s until the total partial sum
\begin{equation}
p_1+\ldots+p_{n_1}-q_1-\ldots-q_{m_1}+p_{n_1+1}+\ldots+p_{n_2}
\end{equation}
is first $\geq s$; and so on.&lt;br /&gt;
The possibility of each of these steps is guaranteed by the divergence of $\sum p_n$ and $\sum q_n$; and the resulting rearrangement of $\sum a_n$ converges to $s$ because $p_n\to 0$ and $q_n\to 0$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In order to make the rearrangement diverge to $\infty$, it suffices to write down enough $p$’s to yield
\begin{equation}
p_1+p_2+\ldots+p_{n_1}\geq 1,
\end{equation}
then to insert $-q_1$, and then to continue with $p$’s until
\begin{equation}
p_1+\ldots+p_{n_1}-q_1+p_{n_1+1}+\ldots+p_{n_2}\geq 2,
\end{equation}
then to insert $-q_2$, and so on.&lt;br /&gt;
We can produce divergence to $-\infty$ by a similar construction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the principal application of &lt;strong&gt;Theorem 7&lt;/strong&gt; relates to the &lt;em&gt;multiplication of series&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If we multiply two series
\begin{align}
\sum_{n=0}^{\infty}a_n&amp;amp;=a_0+a_1+\ldots+a_n+\ldots\tag{20}\label{20} \\ \sum_{n=0}^{\infty}b_n&amp;amp;=b_0+b_1+\ldots+b_n+\ldots\tag{21}\label{21}
\end{align}
by forming all possible product $a_i b_j$ (as in the case of finite sums), then we obtain the following doubly infinite array&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/series-mult.png&quot; alt=&quot;series multiplication&quot; width=&quot;300px&quot; height=&quot;210px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are various ways of arranging these products into a single infinite series, of which two are important. The first one is to group them by diagonals, as indicated in the arrows in &lt;strong&gt;Figure 2&lt;/strong&gt;:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1)+(a_0b_2+a_1b_1+a_2b_0)+\ldots\tag{22}\label{22}
\end{equation}
This series can be defined as $\sum_{n=0}^{\infty}c_n$, where
\begin{equation}
c_n=a_0b_n+a_1b_{n-1}+\ldots+a_nb_0
\end{equation}&lt;/p&gt;

&lt;p&gt;It is called the &lt;em&gt;product&lt;/em&gt; (or &lt;em&gt;Cauchy product&lt;/em&gt;) of the two series $\sum a_n$ and $\sum b_n$.&lt;/p&gt;

&lt;p&gt;The second crucial method of arranging these products into a series is by squares, as shown in &lt;strong&gt;Figure 2&lt;/strong&gt;:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1+a_1b_0)+(a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0)+\ldots\tag{23}\label{23}
\end{equation}
The advantage of this arrangement is that the $n$-th partial sum $s_n$ of \eqref{23} is given by
\begin{equation}
s_n=(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n)\tag{24}\label{24}
\end{equation}
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 9&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If the two series \eqref{20} and \eqref{21} have nonnegative terms and converges to $s$ and $t$, then their product \eqref{22} converges to $st$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
It is clear from \eqref{24} that \eqref{23} converges to $st$. Let’s denote the series \eqref{22} and \eqref{23} without parenthesis by $(22’)$ and $(23’)$.&lt;/p&gt;

&lt;p&gt;We have the series $(23’)$ of nonnegative terms still converges to $st$ because, for if $m$ is an integer such that $n^2\leq m\leq (n+1)^2$, then the $m$-th partial sum of $(23’)$ lies between $s_{n-1}$ and $s_n$, and both of these converge to $st$.&lt;/p&gt;

&lt;p&gt;By &lt;strong&gt;Theorem 7&lt;/strong&gt;, the terms of $(23’)$ can be rearranged to yield $(22’)$ without changing the sum $st$; and when parentheses are suitably inserted, we see that \eqref{8} converges to $st$.&lt;/p&gt;

&lt;p&gt;We now extend &lt;strong&gt;Theorem 9&lt;/strong&gt; to the case of absolute convergence.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 10&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If the series $\sum_{n=0}^{\infty}a_n$ and $\sum_{n=0}^{\infty}b_n$ are absolutely convergent, with sum $s$ and $t$, then their product
\begin{multline}
\sum_{n=0}^{\infty}(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)=a_0b_0+(a_0b_1+a_1b_0)\,+ \\ (a_0b_2+a_1b_1+a_2b_0)+\ldots+(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)+\ldots\tag{25}\label{25}
\end{multline}
is absolutely convergent, with sum $st$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
The series $\sum_{n=0}^{\infty}\vert a_n\vert$ and $\sum_{n=0}^{\infty}\vert b_n\vert$ are convergent and have nonnegative terms. So by the &lt;strong&gt;Theorem 9&lt;/strong&gt; above, their product
\begin{multline}
\vert a_0\vert\vert b_0\vert+\vert a_0\vert\vert b_1\vert+\vert a_1\vert\vert b_0\vert+\ldots+\vert a_0\vert\vert b_n\vert+\vert a_1\vert\vert b_{n-1}\vert+\ldots+\vert a_n\vert\vert b_0\vert+\ldots \\ =\vert a_0b_0\vert+\vert a_0b_1\vert+\vert a_1b_0\vert+\ldots+\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert+\ldots\tag{26}\label{26}
\end{multline}
converges, and therefore the series
\begin{equation}
a_0b_0+a_0b_1+a_1b_0+\ldots+a_0b_n+\ldots+a_nb_0+\ldots\tag{27}\label{27}
\end{equation}
is absolutely convergent. It follows from &lt;strong&gt;Theorem 7&lt;/strong&gt; that the sum of \eqref{27} will not change if we rearrange its terms and write it as
\begin{equation}
a_0b_0+a_0b_1+a_1b_1+a_1b_0+a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0+\ldots\tag{28}\label{28}
\end{equation}
We now observe that the sum of the first $(n+1)^2$ terms of \eqref{28} is
\begin{equation}
(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n),
\end{equation}
so it is clear that \eqref{28}, and with it \eqref{27}, converges to $st$.&lt;/p&gt;

&lt;p&gt;Thus, \eqref{25} also converges to $st$, since \eqref{25} is retrieved by suitably inserted parentheses in \eqref{27}.&lt;/p&gt;

&lt;p&gt;Moreover, we also have
\begin{equation}
\vert a_0b_n+a_1b_{n-1}+\ldots+a_nb_0\vert\leq\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert
\end{equation}
and the series
\begin{equation}
\vert a_0b_0\vert+(\vert a_0b_1\vert+\vert a_1b_0\vert)+\ldots+(\vert a_0b_n\vert+\ldots+\vert a_nb_0\vert)+\ldots
\end{equation}
obtained from \eqref{26} by inserting parentheses. By the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, \eqref{25} converges absolutely.&lt;/p&gt;

&lt;p&gt;Hence, we can conclude that \eqref{25} is absolutely convergent, with sum $st$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We have already gone through convergence tests applied only to series of positive (or nonnegative) terms. Let’s end this lengthy post with the alternating series test. ^^!&lt;/p&gt;

&lt;h2 id=&quot;dirichlets-test&quot;&gt;Dirichlet’s test&lt;/h2&gt;

&lt;h3 id=&quot;abel-part-sum&quot;&gt;Abel’s partial summation formula&lt;/h3&gt;
&lt;p&gt;Consider series $\sum_{n=1}^{\infty}a_n$, sequence $\{b_n\}$. If $s_n=a_1+a_2+\ldots+a_n$, then
\begin{equation}
a_1b_1+a_2b_2+\ldots+a_nb_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots+s_{n-1}(b_{n-1}-b_n)+s_nb_n\tag{29}\label{29}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $a_1=s_1$ and $a_n=s_n-s_{n-1}$ for $n&amp;gt;1$, we have
\begin{align}
a_1b_1&amp;amp;=s_1b_1 \\ a_2b_2&amp;amp;=s_2b_2-s_1b_2 \\ a_3b_3&amp;amp;=s_3b_3-s_2b_3 \\ &amp;amp;\vdots \\ a_nb_n&amp;amp;=s_nb_n-s_{n-1}b_n
\end{align}
On adding these equations, and grouping suitably, we obtain \eqref{29}.&lt;/p&gt;

&lt;h3 id=&quot;d-test&quot;&gt;Dirichlet’s test&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;If the series $\sum_{n=1}^{\infty}a_n$ has bounded partial sums, and if $\{b_n\}$ is a decreasing sequence of positive numbers such that $b_n\to 0$, then the series
\begin{equation}
\sum_{n=1}^{\infty}a_nb_n=a_1b_1+a_2b_2+\ldots+a_nb_n+\ldots\tag{30}\label{30}
\end{equation}
converges&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $S_n=a_1b_1+a_2b_2+\ldots+a_nb_n$ denote the $n$-th partial sum of \eqref{30}, then \eqref{29} tells us that
\begin{equation}
S_n=T_n+s_nb_n,
\end{equation}
where
\begin{equation}
T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots
\end{equation}
Since ${s_n}$ is bounded there exists a positive constant $m$ such that $\vert s_n\vert\leq m,\forall n$, so $\vert s_nb_n\vert\leq mb_n$. And since $b_n\to 0$, we have that $s_nb_n\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;p&gt;Moreover, since $\{b_n\}$ is a decreasing sequence of positive numbers, we have that
\begin{equation}
\begin{aligned}
\vert s_1(b_1-b_2)\vert+\vert s_2(b_3-b_3)\vert+\ldots&amp;amp;\,+\vert s_{n-1}(b_{n-1}-b_n)\vert \\ &amp;amp;\leq m(b_1-b_2)+m(b_2-b_3)+\ldots+m(b_{n-1}-b_n) \\ &amp;amp;=m(b_1-b_n)\leq mb_1
\end{aligned}
\end{equation}
which implies that $T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots$ converges absolutely, and thus, it converges to a sum $t$. Therefore
\begin{equation}
\lim_{n\to\infty}S_n=\lim_{n\to\infty}T_n+s_nb_n=\lim_{n\to\infty}T_n+\lim_{n\to\infty}s_nb_n=t+0=t
\end{equation}
which lets us conclude that the series \eqref{30} converges.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] George F.Simmons. &lt;a href=&quot;https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424&quot;&gt;Calculus With Analytic Geometry - 2nd Edition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Marian M. &lt;a href=&quot;https://www.springer.com/gp/book/9780387789323&quot;&gt;A Concrete Approach to Classical Analysis&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] MIT 18.01. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&quot;&gt;Single Variable Calculus&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We will be going through power series in more detailed in another &lt;a href=&quot;/mathematics/calculus/2021/09/21/power-series.html&quot;&gt;post&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Stolz–Cesaro&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Let $\{a_n\}$ be a sequence of real numbers and $\{b_n\}$ be a strictly monotone and divergent sequence. Then
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}-a_n}{b_{n+1}-b_n}=L\hspace{1cm}(\in\left[-\infty,+\infty\right])
\end{equation}
implies
\begin{equation}
\lim_{n\to\infty}\dfrac{a_n}{b_n}=L
\end{equation}&lt;/em&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="random-stuffs" /><summary type="html">No idea what to say yet :D</summary></entry><entry><title type="html">Monte Carlo Methods in Reinforcement Learning</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html" rel="alternate" type="text/html" title="Monte Carlo Methods in Reinforcement Learning" /><published>2021-08-21T13:03:00+07:00</published><updated>2021-08-21T13:03:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html">&lt;blockquote&gt;
  &lt;p&gt;Recall that in the previous post, &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html&quot;&gt;&lt;strong&gt;Dynamic Programming Algorithms for Solving Markov Decision Processes&lt;/strong&gt;&lt;/a&gt;, we made an assumption about the complete knowledge of the environment. With &lt;strong&gt;Monte Carlo&lt;/strong&gt; methods, we only require &lt;em&gt;experience&lt;/em&gt; - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#mc-methods&quot;&gt;Monte Carlo Methods&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mc-rl&quot;&gt;Monte Carlo Methods in Reinforcement Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mc-prediction&quot;&gt;Monte Carlo Prediction&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#first-mc-every-mc&quot;&gt;First-visit MC vs. every-visit MC&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mc-control&quot;&gt;Monte Carlo Control&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#mc-est-action-value&quot;&gt;Monte Carlo Estimation of Action Values&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#es&quot;&gt;Exploring Starts&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#mc-policy-iteration&quot;&gt;Monte Carlo Policy Iteration&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#on-policy-mc-control&quot;&gt;On-policy Monte Carlo Control&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#off-policy-mc-pred&quot;&gt;Off-policy Monte Carlo Prediction&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#coverage&quot;&gt;Assumption of Coverage&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#is&quot;&gt;Importance Sampling&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#is-off-policy&quot;&gt;Off-policy Monte Carlo Prediction via Importance Sampling&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#imp-off-policy-is&quot;&gt;Incremental Implementation for Off-policy MC Prediction using IS&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#incremental-method&quot;&gt;Incremental Method&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#applying-off-policy-is&quot;&gt;Applying to Off-policy MC Prediction using IS&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#off-policy-mc-control&quot;&gt;Off-policy Monte Carlo Control&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example - Racetrack&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#discounting-aware-is&quot;&gt;Discounting-aware Importance Sampling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#per-decision-is&quot;&gt;Per-decision Importance Sampling&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mc-methods&quot;&gt;Monte Carlo Methods&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Monte Carlo&lt;/strong&gt;, named after a casino in Monaco, simulates complex probabilistic events using simple random events, such as tossing a pair of dice to simulate the casino’s overall business model.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/mc-pi.gif&quot; alt=&quot;monte carlo method&quot; width=&quot;480&quot; height=&quot;360px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Using Monte Carlo method to approximate the value of $\pi$. The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/monte-carlo-methods/blob/main/monte_carlo_pi.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Monte Carlo methods have been used in several different tasks:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Simulating a system and its probability distribution $\pi(x)$
\begin{equation}
x\sim\pi(x)
\end{equation}&lt;/li&gt;
  &lt;li&gt;Estimating a quantity through Monte Carlo integration
\begin{equation}
c=\mathbb{E}_\pi\left[f(x)\right]=\int\pi(x)f(x)\,dx
\end{equation}&lt;/li&gt;
  &lt;li&gt;Optimizing a target function to find its modes (maxima or minima)
\begin{equation}
x^*=\arg\max\pi(x)
\end{equation}&lt;/li&gt;
  &lt;li&gt;Learning a parameters from a training set to optimize some loss functions, such as the maximum likelihood estimation from a set of examples $\{x_i,i=1,2,\dots,M\}$
\begin{equation}
\Theta^*=\arg\max\sum_{i=1}^{M}\log p(x_i;\Theta)
\end{equation}&lt;/li&gt;
  &lt;li&gt;Visualizing the energy landscape of a target function&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;mc-rl&quot;&gt;Monte Carlo Methods in Reinforcement Learning&lt;/h2&gt;
&lt;p&gt;Monte Carlo (MC) methods are ways of solving the reinforcement learning problem based on averaging sample returns. Here, we define Monte Carlo methods only for episodic tasks. Or in other words, they learn from complete episodes of experience.&lt;/p&gt;

&lt;h3 id=&quot;mc-prediction&quot;&gt;Monte Carlo Prediction&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;Since the value of a state $v_\pi(s)=\mathbb{E}_\pi\left[G_t|S_t=s\right]$ is defined as the expectation of the return when the process is started from the given state $s$, an obvious way of estimating this value from experience is to compute observed mean returns after visits to that state. As more returns are observed, the average should converge to the expected value. This is an instance of the so-called &lt;em&gt;Monte Carlo method&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In particular, suppose we wish to estimate $v_\pi(s)$ given a set of episodes obtained by following $\pi$ and passing through $s$. Each time state $s$ appears in an episode, we call it a &lt;em&gt;visit&lt;/em&gt; to $s$. There are two types of Monte Carlo methods:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;First-visit MC method&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;estimates $v_\pi(s)$ as the average of the returns that have followed the &lt;em&gt;first visit&lt;/em&gt; to $s$.&lt;/li&gt;
      &lt;li&gt;We call the first time $s$ is visited in an episode the &lt;em&gt;first visit&lt;/em&gt; to $s$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Every-visit MC method&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;estimates $v_\pi(s)$ as the average of the returns that have followed all visits to to $s$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The sample mean return for state $s$ is:
\begin{equation}
v_\pi(s)=\dfrac{\sum_{t=1}^{T}𝟙\left(S_t=s\right)G_t}{\sum_{t=1}^{T}𝟙\left(S_t=s\right)},
\end{equation}
where $𝟙(\cdot)$ is an indicator function. In the case of &lt;em&gt;first-visit MC&lt;/em&gt;, $𝟙\left(S_t=s\right)$ returns $1$ only in the first time $s$ is encountered in an episode. And for &lt;em&gt;every-visit MC&lt;/em&gt;, $𝟙\left(S_t=s\right)$ gives value of $1$ every time $s$ is visited.&lt;/p&gt;

&lt;p&gt;Here is pseudocode of the &lt;em&gt;first-visit MC prediction&lt;/em&gt;, for estimating $V\approx v_\pi$&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/mc-prediction.png&quot; alt=&quot;iterative policy evaluation pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;first-mc-every-mc&quot;&gt;First-visit MC vs. every-visit MC&lt;/h4&gt;
&lt;p&gt;Both methods converge to $v_\pi(s)$ as the number of visits (or first visits) to $s$ goes to infinity. Each average is itself an unbiased estimate, and the standard deviation of its error falls as $\frac{1}{\sqrt{n}}$, where $n$ is the number of returns averaged.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/first-visit-every-visit.png&quot; alt=&quot;first-visit MC vs every-visit MC&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Summary of Statistical Results comparing first-visit and every-visit MC method&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;mc-control&quot;&gt;Monte Carlo Control&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;

&lt;h4 id=&quot;mc-est-action-value&quot;&gt;Monte Carlo Estimation of Action Values&lt;/h4&gt;
&lt;p&gt;When model is not available, it is particular useful to estimate &lt;em&gt;action values&lt;/em&gt; rather than &lt;em&gt;state values&lt;/em&gt; (which alone are insufficient to determine a policy). We must explicitly estimate the value of each action in order for the values to be useful in suggesting a policy. Thus, one of our primary goals for MC methods is to estimate $q_*$. To achieve this, we first consider the policy evaluation problem for action values.&lt;/p&gt;

&lt;p&gt;Similar to when using MC method to estimate $v_\pi(s)$, we can use both first-visit MC and every-visit MC to approximate the value of $q_\pi(s,a)$. The only thing we need to keep in mind is, in this case, we work with visits to a state-action pair rather than to a state. Likewise, we define two types of MC methods for estimating $q_\pi(s,a)$:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;First-visit MC method&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;estimates $q_\pi(s,a)$ as the average of the returns following the first time in each episode that the state $s$ was visited and the action $a$ was selected&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Every-visit MC method&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;estimates $q_\pi(s,a)$ as the average of the returns that have followed all the visits to state-action pair $(s,a)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;es&quot;&gt;Exploring Starts&lt;/h5&gt;
&lt;p&gt;However, here we must exercise &lt;em&gt;exploration&lt;/em&gt;. Because many state-action pairs may never be visited, and if $\pi$ is a deterministic policy, then returns of only single one action for each state will be observed. That leads to the consequence that the other actions will not be evaluated since there are no returns to average.&lt;/p&gt;

&lt;p&gt;There is one way to achieve this, which is called &lt;em&gt;exploring starts&lt;/em&gt; - an assumption that assumes the episodes &lt;em&gt;start in a state-action pair&lt;/em&gt;, and that every pair has a &lt;em&gt;nonzero&lt;/em&gt; probability of being selected as the start. This assumption assures that all state-action pairs will be visited an infinite number of times in the limit of an infinite number of episodes.&lt;/p&gt;

&lt;h4 id=&quot;mc-policy-iteration&quot;&gt;Monte Carlo Policy Iteration&lt;/h4&gt;
&lt;p&gt;To learn the optimal policy by MC, we apply the idea of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#gpi&quot;&gt;GPI&lt;/a&gt;:
\begin{equation}
\pi_0\overset{\small \text{E}}{\rightarrow}q_{\pi_0}\overset{\small \text{I}}{\rightarrow}\pi_1\overset{\small \text{E}}{\rightarrow}q_{\pi_1}\overset{\small \text{I}}{\rightarrow}\pi_2\overset{\small \text{E}}{\rightarrow}\dots\overset{\small \text{I}}{\rightarrow}\pi_*\overset{\small \text{E}}{\rightarrow}q_*
\end{equation}
In particular,&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Policy evaluation&lt;/em&gt; (denoted as $\overset{\small\text{E}}{\rightarrow}$): estimates action value function $q_\pi(s,a)$ using the episode generated from $s, a$, following by current policy $\pi$
\begin{equation}
q_\pi(s,a)=\dfrac{\sum_{t=1}^{T}𝟙\left(S_t=s,A_t=a\right)G_t}{\sum_{t=1}^{T}𝟙\left(S_t=s,A_t=a\right)}
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Policy improvement&lt;/em&gt; (denoted as $\overset{\small\text{I}}{\rightarrow}$): makes the policy &lt;em&gt;greedy&lt;/em&gt; with the current value function (action value function in this case)
\begin{equation}
\pi(s)\doteq\arg\max_{a\in\mathcal{A(s)}} q(s,a)
\end{equation}
The policy improvement can be done by constructing each $\pi_{k+1}$ as the greedy policy w.r.t $q_{\pi_k}$ because
\begin{align}
q_{\pi_k}\left(s,\pi_{k+1}(s)\right)&amp;amp;=q_{\pi_k}\left(s,\arg\max_a q_{\pi_k}(s,a)\right) \\ &amp;amp;=\max_a q_{\pi_k}(s,a) \\ &amp;amp;\geq q_{\pi_k}\left(s,\pi_k(s)\right) \\ &amp;amp;\geq v_{\pi_k}(s)
\end{align}
Therefore, by the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-improvement&quot;&gt;policy improvement theorem&lt;/a&gt;, we have that $\pi_{k+1}\geq\pi_k$.&lt;/li&gt;
&lt;/ol&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/gpi.png&quot; alt=&quot;GPI&quot; width=&quot;150&quot; height=&quot;150px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: MC policy iteration&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
To solve this problem with Monte Carlo policy iteration, in the 1998 version of ‘‘&lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt;”, authors of the book introduced &lt;strong&gt;Monte Carlo ES&lt;/strong&gt; (MCES), for Monte Carlo with &lt;em&gt;Exploring Starts&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In MCES, value function is approximated by simulated returns and a greedy policy is selected at each iteration. Although MCES does not converge to any sub-optimal policy, the convergence to optimal fixed point is still an open question. For solutions in particular settings, you can check out some results like Tsitsiklis (2002), Chen (2018), Liu (2020).&lt;br /&gt;
Down below is pseudocode of the Monte Carlo ES.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/mces.png&quot; alt=&quot;monte carlo es pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;on-policy-mc-control&quot;&gt;On-policy Monte Carlo Control&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;In the previous section, we used the assumption of &lt;a href=&quot;#es&quot;&gt;exploring starts&lt;/a&gt; (ES) to design a Monte Carlo control method called MCES. In this part, without making that impractical assumption, we will be talking about another Monte Carlo control method.&lt;/p&gt;

&lt;p&gt;In &lt;em&gt;on-policy control methods&lt;/em&gt;, the policy is generally &lt;em&gt;soft&lt;/em&gt; (i.e., $\pi(a|s)&amp;gt;0,\forall s\in\mathcal{S},a\in\mathcal{A(s)}$, but gradually shifted closer and closer to a deterministic optimal policy). We can not simply improve the policy by following a greedy policy, since no exploration will take place. Then to get rid of ES, we use the on-policy MC method with $\varepsilon$-&lt;em&gt;greedy&lt;/em&gt; policies, e.g, most of the time they choose an action that maximal estimated action value, but with probability of $\varepsilon$ they instead select an action at random. Specifically,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$Pr(\small\textit{non-greedy action})=\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$&lt;/li&gt;
  &lt;li&gt;$Pr(\small\textit{greedy action})=1-\varepsilon+\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The $\varepsilon$-greedy policies are examples of $\varepsilon$-&lt;em&gt;soft&lt;/em&gt; policies, defined as ones for which $\pi(a\vert s)\geq\frac{\varepsilon}{\vert\mathcal{A(s)}\vert}$ for all states and actions, for some $\varepsilon&amp;gt;0$. Among $\varepsilon$-soft policies, $\varepsilon$-greedy policies are in some sense those that closest to greedy.&lt;/p&gt;

&lt;p&gt;We have that any $\varepsilon$-greedy policy w.r.t $q_\pi$ is an &lt;em&gt;improvement&lt;/em&gt; over any $\varepsilon$-soft policy is assured by the &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html#policy-improvement&quot;&gt;policy improvement theorem&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $\pi’$ be the $\varepsilon$-greedy. The conditions of the policy improvement theorem apply because for any $s\in\mathcal{S}$, we have:
\begin{align}
q_\pi\left(s,\pi’(s)\right)&amp;amp;=\sum_a\pi’(a|s)q_\pi(s,a) \\ &amp;amp;=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\max_a q_\pi(s,a) \\ &amp;amp;\geq\dfrac{\varepsilon}{\vert\mathcal{A(s)}\vert}\sum_a q_\pi(s,a)+(1-\varepsilon)\sum_a\dfrac{\pi(a|s)-\frac{\varepsilon}{\vert\mathcal{A}(s)\vert}}{1-\varepsilon}q_\pi(s,a) \\ &amp;amp;=\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a)+\sum_a\pi(a|s)q_\pi(s,a)-\dfrac{\varepsilon}{\vert\mathcal{A}(s)\vert}\sum_a q_\pi(s,a) \\ &amp;amp;=v_\pi(s)
\end{align}
(In the third step, we use the fact that the latter $\sum$ is a weighted average over $q_\pi(s,a)$). Thus, by the theorem, $\pi’\geq\pi$. The equality holds when both $\pi’$ and $\pi$ are optimal policies among the $\varepsilon$-soft ones.&lt;/p&gt;

&lt;p&gt;Pseudocode of the complete algorithm is given below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/on-policy-mc-control.png&quot; alt=&quot;monte carlo es pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;off-policy-mc-pred&quot;&gt;Off-policy Monte Carlo Prediction&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/h3&gt;
&lt;p&gt;When working with control methods, we have to solve a dilemma about &lt;em&gt;exploitation&lt;/em&gt; and &lt;em&gt;exploration&lt;/em&gt;. In other words, we have to evaluate a policy from episodes generated by following an exploratory policy.&lt;/p&gt;

&lt;p&gt;A straightforward way to solve this problem is to use two different policies, one that is learned about and becomes the optimal policy, and one that is more exploratory and is used to generate behavior. The policy is being learned about is called the &lt;em&gt;target policy&lt;/em&gt;, whereas &lt;em&gt;behavior policy&lt;/em&gt; is the one which is used to generate behavior.&lt;/p&gt;

&lt;p&gt;In this section, we will be considering the off-policy method on prediction task, on which both target (denoted as $\pi$) and behavior (denoted as $b$) policies are fixed and given. Particularly, we wish to estimate $v_\pi$ or $q_\pi$ from episodes retrieved from following another policy $b$, where $\pi\neq b$.&lt;/p&gt;

&lt;h4 id=&quot;coverage&quot;&gt;Assumption of Coverage&lt;/h4&gt;
&lt;p&gt;In order to use episodes from $b$ to estimate values for $\pi$, we require that every action taken under $\pi$ is also taken, at least occasionally, under $b$. That means, we assume that $\pi(a|s)&amp;gt;0$ implies $b(s|a)&amp;gt;0$, which leads to a result that $b$ must be stochastic, while $\pi$ may be deterministic since $\pi\neq b$. This is the assumption of &lt;strong&gt;coverage&lt;/strong&gt;.&lt;/p&gt;

&lt;h4 id=&quot;is&quot;&gt;Importance Sampling&lt;/h4&gt;
&lt;p&gt;Let $X$ be a variable (or set of variables) that takes on values in some space $\textit{Val}(X)$. &lt;strong&gt;Importance sampling&lt;/strong&gt; (IS) is a general approach for estimating the expectation of a function $f(x)$ relative to some distribution $P(X)$, typically called the &lt;em&gt;target distribution&lt;/em&gt;. We can estimate this expectation by generating samples $x[1],\dots,x[M]$ from $P$, and then estimating
\begin{equation}
\mathbb{E}_P\left[f\right]\approx\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])
\end{equation}
In some cases, it might be impossible or computationally very expensive to generate samples from $P$, we instead prefer to generate samples from a different distribution, $Q$, known as the &lt;em&gt;proposal distribution&lt;/em&gt; (or &lt;em&gt;sampling distribution&lt;/em&gt;).&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Unnormalized Importance Sampling&lt;/strong&gt;&lt;br /&gt;
If we generate samples from $Q$ instead of $P$, we cannot simply average the $f$-value of the samples generated. We need to adjust our estimator to compensate for the incorrect sampling distribution. The most obvious way of adjusting our estimator is based on the observation that
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&amp;amp;=\sum_x f(x)P(x) \\ &amp;amp;=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &amp;amp;=\mathbb{E}_{Q(X)}\left[f(X)\dfrac{P(X)}{Q(X)}\right]\tag{1}\label{1}
\end{align}
Based on this observation \eqref{1}, we can use the standard estimator for expectations relative to $Q$. We generate a set of sample $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, and then estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])\dfrac{P(x[m])}{Q(x[m])}\tag{2}\label{2},
\end{equation}
where $\hat{\mathbb{E}}$ denotes empirical expectation. We call this estimator the &lt;strong&gt;unnormalized importance sampling estimator&lt;/strong&gt;, this method is also often called &lt;strong&gt;unweighted importance sampling&lt;/strong&gt;. The factor $\frac{P(x[m])}{Q(x[m])}$ (denoted as $w(x[m])$) can be viewed as a correction weight to the term $f(x[m])$, which we would have used had $Q$ been our target distribution.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Normalized Importance Sampling&lt;/strong&gt;&lt;br /&gt;
In many situations, we have that $P$ is known only up to a normalizing constant $Z$. Particularly, what we have access to is a distribution $\tilde{P}(X)=ZP(X)$.&lt;br /&gt;
Thus, rather than to define the weights relative to $P$ as above, we define:
\begin{equation}
w(X)\doteq\dfrac{\tilde{P}(X)}{Q(X)}
\end{equation}
We have that the weight $w(X)$ is a random variable, and has expected value equal to $Z$:
\begin{equation}
\mathbb{E}_{Q(X)}\left[w(X)\right]=\sum_x Q(x)\dfrac{\tilde{P}(x)}{Q(x)}=\sum_x\tilde{P}(x)=Z
\end{equation}
Hence, this quantity is the normalizing constant of the distribution $\tilde{P}$. We can now rewrite \eqref{1} as:
\begin{align}
\mathbb{E}_{P(X)}\left[f(X)\right]&amp;amp;=\sum_x P(x)f(x) \\ &amp;amp;=\sum_x Q(x)f(x)\dfrac{P(x)}{Q(x)} \\ &amp;amp;=\dfrac{1}{Z}\sum_x Q(x)f(x)\dfrac{\tilde{P}(x)}{Q(x)} \\ &amp;amp;=\dfrac{1}{Z}\mathbb{E}_{Q(X)}\left[f(X)w(X)\right] \\ &amp;amp;=\dfrac{\mathbb{E}_{Q(X)}\left[f(X)w(X)\right]}{\mathbb{E}_{Q(X)}\left[w(X)\right]}\tag{3}\label{3}
\end{align}
We can use an empirical estimator for both the numerator and denominator. Given $M$ samples $\mathcal{D}=\{x[1],\dots,x[M]\}$ from $Q$, we can estimate:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{\sum_{m=1}^{M}f(x[m])w(x[m])}{\sum_{m=1}^{M}w(x[m])}\tag{4}\label{4}
\end{equation}
We call this estimator the &lt;strong&gt;normalized importance sampling estimator&lt;/strong&gt; (or &lt;strong&gt;weighted importance sampling estimator&lt;/strong&gt;).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;is-off-policy&quot;&gt;Off-policy Monte Carlo Prediction via Importance Sampling&lt;/h4&gt;
&lt;p&gt;We apply IS to off-policy learning by weighting returns according to the relative probability of their trajectories occurring under the target and behavior policies, called the &lt;em&gt;importance sampling ratio&lt;/em&gt; (which we denoted as $w$ as above, but now we change the notation to $\rho$ in order to follows the book).&lt;/p&gt;

&lt;p&gt;The probability of the subsequent state-action trajectory, $A_t,S_{t+1},A_{t+1},\dots,S_T$, occurring under any policy $\pi$ given starting state $s$ is:
\begin{align}
Pr(A_t,S_{t+1},\dots,S_T|S_t,A_{t:T-1}\sim\pi)&amp;amp;=\pi(A_t|S_t)p(S_{t+1}|S_t,A_t)\dots p(S_T|S_{T-1},A_{T-1}) \\ &amp;amp;=\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)
\end{align}
Thus, the importance sampling ratio as we defined is:
\begin{equation}
\rho_{t:T-1}\doteq\dfrac{\prod_{k=t}^{T-1}\pi(A_k|S_k)p(S_{k+1}|S_t,A_t)}{\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+1}|S_t,A_t)}=\prod_{k=1}^{T-1}\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}
\end{equation}
which depends only on the two policies and the sequence, not on the MDP.&lt;/p&gt;

&lt;p&gt;Since $v_b(s)=\mathbb{E}\left[G_t|S_t=s\right]$, then we have
\begin{equation}
\mathbb{E}\left[\rho_{t:T-1}G_t|S_t=s\right]=v_\pi(s)
\end{equation}
To estimate $v_\pi(s)$, we simply scale the returns by the ratios and average the results:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\vert\mathcal{T}(s)\vert},\tag{5}\label{5}
\end{equation}
where $\mathcal{T}(s)$ is the set of all states in which $s$ is visited (only for every-visit). For a first-visit,$\mathcal{T}(s)$ would only include time steps that were first visits to $s$ within their episodes. $T(t)$ denotes the first time of termination following time $t$, and $G_t$ denotes the return after $t$ up through $T(t)$.&lt;/p&gt;

&lt;p&gt;When importance sampling is done as simple average in this way, we call it &lt;em&gt;ordinary importance sampling&lt;/em&gt; (OIS) (which corresponds to &lt;em&gt;unweighted importance sampling&lt;/em&gt; in the previous section).&lt;/p&gt;

&lt;p&gt;And the one corresponding to &lt;em&gt;weighted importance sampling&lt;/em&gt; (WIS), which uses a weighted average, is defined as:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}G_t}{\sum_{t\in\mathcal{T}(s)}\rho_{t:T(t)-1}},\tag{6}\label{6}
\end{equation}
or zero if the denominator is zero.&lt;/p&gt;

&lt;h4 id=&quot;imp-off-policy-is&quot;&gt;Incremental Implementation for Off-policy MC Prediction using IS&lt;/h4&gt;

&lt;h5 id=&quot;incremental-method&quot;&gt;Incremental Method&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Incremental method&lt;/strong&gt; is a way of updating averages with small, constant computation required to process each new reward instead of maintaining a record of all the rewards and then performing this computation whenever the estimated value was needed. It follows the general rule:
\begin{equation}
NewEstimate\leftarrow OldEstimate+StepSize\left[Target-OldEstimate\right]
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;applying-off-policy-is&quot;&gt;Applying to Off-policy MC Prediction using IS&lt;/h5&gt;
&lt;p&gt;In ordinary IS, the returns are scaled by the IS ratio $\rho_{t:T(t)-1}$, then simply averaged, as in \eqref{5}. Thus, it’s easy to apply incremental method to OIS.&lt;/p&gt;

&lt;p&gt;For WIS, as in the equation \eqref{6}, we have to form a weighted average of the returns, and a slightly different incremental incremental algorithm is required.
Suppose we have a sequence of returns $G_1,G_2,\dots,G_{n-1}$, all starting in the same state and each with a corresponding random weight $W_i$ (e.g., $W_i=\rho_{t_i:T(t_i)}$). We wish to form the estimate
\begin{equation}
V_n\doteq\dfrac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},\hspace{1cm}n\geq2
\end{equation}
and keep it up-to-date as we obtain a single additional return $G_n$. In addition to keeping track of $V_n$, we must maintain for each state the cumulative sum $C_n$ of the weights given to the first $n$ returns. The update rule for $V_n$ is
\begin{equation}
V_{n+1}\doteq V_n+\dfrac{W_n}{C_n}\big[G_n-V_n\big],\hspace{1cm}n\geq1,
\end{equation}
and
\begin{equation}
C_{n+1}\doteq C_n+W_{n+1},
\end{equation}
where $C_0=0$. And here is pseudocode of our algorithm.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/off-policy-mc-prediction.png&quot; alt=&quot;off-policy MC prediction pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;off-policy-mc-control&quot;&gt;Off-policy Monte Carlo Control&lt;/h3&gt;
&lt;p&gt;Similarly, we develop the algorithm for off-policy MC control, based on GPI and WIS, for estimating $\pi_*$ and $q_*$, which is shown below.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/off-policy-mc-control.png&quot; alt=&quot;off-policy MC control pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;The target policy $\pi\approx\pi_*$ is the greedy policy w.r.t $Q$, which is an estimate of $q_\pi$. The behavior policy, $b$, can be anything, but in order to assure convergence of $\pi$ to the optimal policy, an infinite number of returns must be obtained for each pair of state and action. This can be guaranteed by choosing $b$ to be $\varepsilon$-soft.&lt;/p&gt;

&lt;p&gt;The policy $\pi$ converges to optimal at all encountered states even though actions are selected according to a different soft policy $b$, which may change between or even within episodes.&lt;/p&gt;

&lt;h4 id=&quot;example&quot;&gt;Example - Racetrack&lt;/h4&gt;
&lt;p&gt;(This example is taken from &lt;em&gt;Exercise 5.12&lt;/em&gt;, &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt; book.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;&lt;br /&gt;
Consider driving a race car around a turn like that shown in &lt;strong&gt;&lt;em&gt;Figure 4&lt;/em&gt;&lt;/strong&gt;. You want to go as fast as possible, but not so fast as to run off the track. In our simplified racetrack, the car is at one of a discrete set of grid positions, the cells in the diagram. The velocity is also discrete, a number of grid cells moved horizontally and vertically per time step. The actions are increments to the velocity components. Each may be changed by +1, -1, or 0 in each step, for a total of nine (3 x 3) actions. Both velocity components are restricted to be nonnegative and less than 5, and they cannot both be zero except at the starting line. Each episode begins in one of the randomly selected start states with both velocity components zero and ends when the car crosses the finish line. The rewards are -1 for each step until the car crosses the finish line. If the car hits the track boundary, it is moved back to a random position on the starting line, both velocity components are reduced to zero, and the episode continues. Before updating the car’s location at each time step, check to see if the projected path of the car intersects the track boundary. If it intersects the finish line, the episode ends; if it intersects anywhere else, the car is considered to have hit the track boundary and is sent back to the starting line. To make the task more challenging, with probability 0.1 at each time step the velocity increments are both zero, independently of the intended increments. Apply a Monte Carlo control method to this task to compute the optimal policy from each starting state. Exhibit several trajectories following the optimal policy (but turn the noise off for these trajectories).&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/racetrack.png&quot; alt=&quot;racetrack&quot; width=&quot;200&quot; height=&quot;300px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: A turn for the racetrack task&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution code&lt;/strong&gt;&lt;br /&gt;
The source code can be found &lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-5/racetrack.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;button type=&quot;button&quot; class=&quot;collapsible&quot; id=&quot;codeP&quot;&gt;Click to show the code&lt;/button&gt;&lt;/p&gt;
&lt;div class=&quot;codePanel&quot; id=&quot;codePdata&quot;&gt;
  &lt;p&gt;&lt;br /&gt;
We begin by importing some useful packages.&lt;/p&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;

  &lt;p&gt;Next, we define our environment&lt;/p&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;RaceTrack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NOISE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_VELOCITY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MIN_VELOCITY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starting_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_load_track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_generate_start_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_generate_start_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_generate_start_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starting_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starting_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;take_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_terminal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_update_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_update_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;# update velocity
&lt;/span&gt;		&lt;span class=&quot;c1&quot;&gt;# with probability of 0.1, keep the velocity unchanged
&lt;/span&gt;		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minimum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_VELOCITY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MIN_VELOCITY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;# update car position
&lt;/span&gt;		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tstep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_VELOCITY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tstep&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MAX_VELOCITY&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;
				&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;velocity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;position&lt;/span&gt;


	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_load_track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;W&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;o&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;-&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;# rotate the track in order to sync the track with actions
&lt;/span&gt;		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fliplr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;starting_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_terminal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;car_position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;p&gt;We continue by defining our behavior policy and algorithm.&lt;/p&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;behavior_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;off_policy_MC_control&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RaceTrack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# for epsilon-soft greedy policy
&lt;/span&gt;	&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ep&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;trajectory&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_terminal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;s_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;s_vx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_vy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binomial&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_vx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s_vy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;behavior_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;trajectory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trajectory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;trajectory&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;a_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;s_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;q_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e5&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;a_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;act&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;sa_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;act&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;act&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sa_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;q_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sa_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;a_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;act&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a_max&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epsilon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
  &lt;p&gt;And wrapping everything up with the main function.&lt;/p&gt;
  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;WWWWWWWWWWWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWWooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;Woooooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;Woooooooooooooooo+&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WooooooooooWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WoooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWooooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWoooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWWooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWWooooooWWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;&apos;WWWW------WWWWWWWW&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;off_policy_MC_control&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;episodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;track_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RaceTrack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sp_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sp_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sv_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;take_action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_terminal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;track_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;track&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flipud&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;./racetrack_off_policy_control.png&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;We end up with this result after running the code.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/racetrack-result.png&quot; alt=&quot;racetrack&apos;s result&quot; width=&quot;450&quot; height=&quot;400px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: Example - Racetrack&apos;s result&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;discounting-aware-is&quot;&gt;Discounting-aware Importance Sampling&lt;/h3&gt;
&lt;p&gt;Recall that in the above &lt;a href=&quot;#is&quot;&gt;section&lt;/a&gt;, we defined the estimator for $\mathbb{E}_P[f]$ as:
\begin{equation}
\hat{\mathbb{E}}_\mathcal{D}(f)=\dfrac{1}{M}\sum_{m=1}^{M}f(x[m])\dfrac{P(x[m])}{Q(x[m])}
\end{equation}
This estimator is unbiased because each of the samples it averages is unbiased:
\begin{equation}
\mathbb{E}_{Q}\left[\dfrac{P(x[m])}{Q(x[m])}f(x[m])\right]=\int_x Q(x)\dfrac{P(x)}{Q(x)}f(x)\,dx=\int_x P(x)f(x)\,dx=\mathbb{E}_{P}\left[f(x[m])\right]
\end{equation}
This IS estimate is unfortunately often of unnecessarily high variance. To be more specific, for example, the episodes last 100 steps and $\gamma=0$. Then $G_0=R_1$ will be weighted by
\begin{equation}
\rho_{0:99}=\dfrac{\pi(A_0|S_0)}{b(A_0|S_0)}\dots\dfrac{\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}
\end{equation}
but actually, it really needs to be weighted by
$\rho_{0:1}=\frac{\pi(A_0|S_0)}{b(A_0|S_0)}$.
The other 99 factors $\frac{\pi(A_1|S_1)}{b(A_1|S_1)}\dots\frac{\pi(A_{99}|S_{99})}{b(A_{99}|S_{99})}$ are irrelevant because after the first reward, the return has already been determined. These later factors are all independent of the return and of expected value $1$; they do not change the expected update, but they add enormously to its variance. They could even make the variance &lt;em&gt;infinite&lt;/em&gt; in some cases.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-08-21/inf-var.png&quot; alt=&quot;infinite variance&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Infinite variance when using OIS (Eg5.5 - RL: An Introduction book). The code can be found &lt;span&gt;&lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-5/infinite-variance.py&quot;&gt;here&lt;/a&gt;&lt;/span&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;One of the methods used to avoid this large extraneous variance is &lt;strong&gt;discounting-aware IS&lt;/strong&gt;. The idea is to think of discounting as determining a probability of termination or, equivalently, a &lt;em&gt;degree&lt;/em&gt; of partial termination.&lt;/p&gt;

&lt;p&gt;We begin by defining &lt;em&gt;flat partial returns&lt;/em&gt;:
\begin{equation}
\bar{G}_{t:h}\doteq R_{t+1}+R_{t+2}+\dots+R_h,\hspace{1cm}0\leq t&amp;lt;h\leq T,
\end{equation}
where ‘‘flat” denotes the absence of discounting, and ‘‘partial” denotes that these returns do not extend all the way to termination but instead stop at $h$, called the &lt;em&gt;horizon&lt;/em&gt;. The conventional full return $G_t$ can be viewed as a &lt;em&gt;sum of flat partial returns&lt;/em&gt;:
\begin{align}
G_t&amp;amp;\doteq R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots+\gamma^{T-t-1}R_T \\ &amp;amp;=(1-\gamma)R_{t+1} \\ &amp;amp;\hspace{0.5cm}+(1-\gamma)\gamma(R_{t+1}+R_{t+2}) \\ &amp;amp;\hspace{0.5cm}+(1-\gamma)\gamma^2(R_{t+1}+R_{t+2}+R_{t+3}) \\ &amp;amp;\hspace{0.7cm}\vdots \\ &amp;amp;\hspace{0.5cm}+(1-\gamma)\gamma^{T-t-2}(R_{t+1}+R_{t+2}+\dots+R_{T-1}) \\ &amp;amp;\hspace{0.5cm}+\gamma^{T-t-1}(R_{t+1}+R_{t+2}+\dots+R_T) \\ &amp;amp;=(1-\gamma)\sum_{h=t+1}^{T-1}\left(\gamma^{h-t-1}\bar{G}_{t:h}\right)+\gamma^{T-t-1}\bar{G}_{t:T}
\end{align}
Now we need to scale the &lt;em&gt;flat partial returns&lt;/em&gt; by an &lt;em&gt;IS ratio&lt;/em&gt; that is similarly truncated. As $\bar{G}_{t:h}$ only involves rewards up to a horizon $h$, we only need the ratio of the probabilities up to $h$. We define:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Discounting-aware OIS&lt;/strong&gt; estimator
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\bar{G}_{t:h}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar{G}_{t:T(t)}\right]}{\vert\mathcal{T}(s)\vert}
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discounting-aware WIS&lt;/strong&gt; estimator
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\bar{G}_{t:h}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\bar{G}_{t:T(t)}\right]}{\sum_{t\in\mathcal{T}(s)}\left[(1-\gamma)\sum_{h=t+1}^{T(t)-1}\left(\gamma^{h-t-1}\rho_{t:h-1}\right)+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\right]}
\end{equation}
These two estimators take into account the discount rate $\gamma$ but have no effect if $\gamma=1$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;per-decision-is&quot;&gt;Per-decision Importance Sampling&lt;/h3&gt;
&lt;p&gt;There is another way beside discounting-aware that may be able to reduce variance, even if $\gamma=1$.&lt;/p&gt;

&lt;p&gt;Recall that in the off-policy estimator \eqref{5} and \eqref{6}, each term of the sum in the numerator is itself a sum:
\begin{align}
\rho_{t:T-1}G_t&amp;amp;=\rho_{t:T-1}\left(R_{t+1}+\gamma R_{t+2}+\dots+\gamma^{T-t-1}R_T\right) \\ &amp;amp;=\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\tag{7}\label{7}
\end{align}
We have that
\begin{equation}
\rho_{t:T-1}R_{t+k}=\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\dfrac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\dots\dfrac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}R_{t+k}
\end{equation}
Of all these factors, only the first $k$ factors, $\frac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\frac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}$, and the last (the reward $R_{t+k}$) are related. All the others are for event that occurred after the reward. Moreover, we have that
\begin{equation}
\mathbb{E}\left[\dfrac{\pi(A_i|S_i)}{b(A_i|S_i)}\right]\doteq\sum_a b(a|S_i)\dfrac{\pi(a|S_i)}{b(a|S_i)}=1
\end{equation} 
Therefore, we obtain
\begin{align}
\mathbb{E}\Big[\rho_{t:T-1}R_{t+k}\Big]&amp;amp;=\mathbb{E}\left[\dfrac{\pi(A_t|S_t)}{b(A_t|S_t)}\dots\dfrac{\pi(A_{t+k-1}|S_{t+k-1})}{b(A_{t+k-1}|S_{t+k-1})}\right]\mathbb{E}\left[\dfrac{\pi(A_k|S_k)}{b(A_k|S_k)}\right]\dots\mathbb{E}\left[\dfrac{\pi(A_{T-1}|S_{T-1})}{b(A_{T-1}|S_{T-1})}\right] \\ &amp;amp;=\mathbb{E}\Big[\rho_{t:t+k-1}R_{t+k}\Big].1\dots 1 \\ &amp;amp;=\mathbb{E}\Big[\rho_{t:t+k-1}R_{t+k}\Big]
\end{align}
Plug the result we just got into the expectation of \eqref{7}, we have
\begin{align}
\mathbb{E}\Big[\rho_{t:T-1}G_t\Big]&amp;amp;=\mathbb{E}\Big[\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\Big] \\ &amp;amp;=\mathbb{E}\Big[\rho_{t:t}R_{t+1}+\gamma\rho_{t:t+1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T\Big] \\ &amp;amp;=\mathbb{E}\Big[\tilde{G}_t\Big],
\end{align}
where $\tilde{G}_t=\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\dots+\gamma^{T-t-1}\rho_{t:T-1}R_T$.&lt;/p&gt;

&lt;p&gt;We call this idea &lt;strong&gt;per-decision IS&lt;/strong&gt;. Hence, we develop &lt;strong&gt;per-decision OIS&lt;/strong&gt; estimator, using $\tilde{G}_t$:
\begin{equation}
V(s)\doteq\dfrac{\sum_{t\in\mathcal{T}(s)}\tilde{G}_t}{\vert\mathcal{T}(s)\vert}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] Adrian Barbu &amp;amp; Song-Chun Zhu. &lt;a href=&quot;https://link.springer.com/book/10.1007/978-981-13-2971-5&quot;&gt;Monte Carlo Methods&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] David Silver. &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;UCL course on RL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] Csaba Szepesvári. &lt;a href=&quot;https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924&quot;&gt;Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] Singh, S.P., Sutton, R.S. &lt;a href=&quot;https://doi.org/10.1007/BF00114726&quot;&gt;Reinforcement learning with replacing eligibility traces&lt;/a&gt;. Mach Learn 22, 123–158 (1996)&lt;/p&gt;

&lt;p&gt;[6] John N. Tsitsiklis. &lt;a href=&quot;https://www.mit.edu/~jnt/Papers/J089-02-jnt-optimistic.pdf&quot;&gt;On the Convergence of Optimistic Policy Iteration&lt;/a&gt;. Journal of Machine Learning Research 3 (2002) 59–72&lt;/p&gt;

&lt;p&gt;[7] Yuanlong Chen. &lt;a href=&quot;https://arxiv.org/abs/1808.08763&quot;&gt;On the convergence of optimistic policy iteration for stochastic shortest path problem&lt;/a&gt; (2018)&lt;/p&gt;

&lt;p&gt;[8] Jun Liu. &lt;a href=&quot;https://arxiv.org/abs/2007.10916&quot;&gt;On the Convergence of Reinforcement Learning with Monte Carlo Exploring Starts&lt;/a&gt; (2020)&lt;/p&gt;

&lt;p&gt;[9] Daphne Koller &amp;amp; Nir Friedman. &lt;a href=&quot;https://mitpress.mit.edu/books/probabilistic-graphical-models&quot;&gt;Probabilistic Graphical Models: Principles and Techniques&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[10] A. Rupam Mahmood, Hado P. van Hasselt, Richard S. Sutton. &lt;a href=&quot;https://papers.nips.cc/paper/2014/hash/be53ee61104935234b174e62a07e53cf-Abstract.html&quot;&gt;Weighted importance sampling for off-policy learning with linear function approximation&lt;/a&gt;. Advances in Neural Information Processing Systems 27 (NIPS 2014)&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We are gonna talk about Monte Carlo methods in more detail in another post. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A prediction task in RL is where we are given a policy and our goal is to measure how well it performs. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Along with prediction, a control task in RL is where the policy is not fixed, and our goal is to find the optimal policy. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;On-policy is a category of RL algorithms that attempts to evaluate or improve the policy that is used to make decisions. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In contrast to on-policy, off-policy methods evaluate or improve a policy different from that used to generate the data. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="monte-carlo" /><category term="importance-sampling" /><category term="my-rl" /><summary type="html">Recall that in the previous post, Dynamic Programming Algorithms for Solving Markov Decision Processes, we made an assumption about the complete knowledge of the environment. With Monte Carlo methods, we only require experience - sample sequences of states, actions, and rewards from simulated or real interaction with an environment.</summary></entry><entry><title type="html">Dynamic Programming Algorithms for Solving Markov Decision Processes</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html" rel="alternate" type="text/html" title="Dynamic Programming Algorithms for Solving Markov Decision Processes" /><published>2021-07-25T15:30:00+07:00</published><updated>2021-07-25T15:30:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html">&lt;blockquote&gt;
  &lt;p&gt;In two previous posts, &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;&lt;strong&gt;Markov Decision Process (MDP) and Bellman equations&lt;/strong&gt;&lt;/a&gt; and &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html&quot;&gt;&lt;strong&gt;Optimal Policy Existence&lt;/strong&gt;&lt;/a&gt;, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with &lt;strong&gt;Dynamic Programming&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-dp&quot;&gt;What is Dynamic Programming?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dp-in-mdps&quot;&gt;Dynamic Programming applied in Markov Decision Processes&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-evaluation&quot;&gt;Policy Evaluation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-improvement&quot;&gt;Policy Improvement&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#policy-iteration&quot;&gt;Policy Iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#value-iteration&quot;&gt;Value Iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#gpi&quot;&gt;Generalized Policy Iteration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example - Gambler’s Problem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-is-dp&quot;&gt;What is Dynamic Programming?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Programming (DP)&lt;/strong&gt; is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/dp.png&quot; alt=&quot;dynamic programming&quot; width=&quot;360&quot; height=&quot;200px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;: Using Dynamic Programming to find the shortest path in graph&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;dp-in-mdps&quot;&gt;Dynamic Programming applied in Markov Decision Processes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DP is a very general method for solving problems having two properties:
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Optimal substructure&lt;/em&gt;
  	- Principle of optimality applies.
  	- Optimal solution can be decomposed into sub-problems.&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;Overlapping sub-problems&lt;/em&gt;
  	- Sub-problems recur many times.
  	- Solutions can be cached and reused.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MDPs satisfy both properties since:
    &lt;ul&gt;
      &lt;li&gt;Bellman equation gives recursive decomposition.&lt;/li&gt;
      &lt;li&gt;Value function stores and reuses solutions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DP assumes the model is already known.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;policy-evaluation&quot;&gt;Policy Evaluation&lt;/h3&gt;
&lt;p&gt;Recall from the definition of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html#bellman-equations&quot;&gt;Bellman equation&lt;/a&gt; that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]\tag{1}\label{1}
\end{equation}
If the environment’s dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.&lt;br /&gt;
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;amp;=\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_k(s’)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts&quot;&gt;Banach’s fixed points theorem&lt;/a&gt; and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called &lt;strong&gt;iterative policy evaluation&lt;/strong&gt;.&lt;br /&gt;
We have the backup diagram for this update.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/backup-iterative-policy-evaluation.png&quot; alt=&quot;Backup diagram for iterative policy evalution update&quot; width=&quot;360&quot; height=&quot;200px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;: Backup diagram for Iterative policy evaluation update&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
When implementing &lt;em&gt;iterative policy evaluation&lt;/em&gt;, for all $s\in\mathcal{S}$, we can use:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;one array to store the value functions, and update them ‘‘in-place” (&lt;em&gt;asynchronous DP&lt;/em&gt;)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\color{red}{v(s’)}\right]
\end{equation}&lt;/li&gt;
  &lt;li&gt;two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (&lt;em&gt;synchronous DP&lt;/em&gt;)
\begin{align}
\color{red}{v_{new}(s)}&amp;amp;\leftarrow\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\color{red}{v_{old}(s’)}\right]\\ \color{red}{v_{old}}&amp;amp;\leftarrow\color{red}{v_{new}}
\end{align}
Here is the pseudocode of the &lt;em&gt;in-place iterative policy evaluation&lt;/em&gt;, given a policy $\pi$, for estimating $V\approx v_\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/iterative-policy-evaluation.png&quot; alt=&quot;iterative policy evalution pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
    &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&quot;policy-improvement&quot;&gt;Policy Improvement&lt;/h3&gt;
&lt;p&gt;The reason why we compute the value function for a given policy $\pi$ is to find better policies. Given the computed value function $v_\pi$ for an deterministic policy $\pi$, we already know how good it is for a state $s$ to choose action $a=\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\neq\pi$, will it be better?&lt;br /&gt;
In particular, in state $s$, selecting action $a$ and thereafter following the policy $\pi$, we have:
\begin{align}
q_\pi(s,a)&amp;amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{2}\label{2} \\ &amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]
\end{align}
&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Policy improvement&lt;/em&gt;)&lt;br /&gt;
Let $\pi,\pi’$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,
\begin{equation}
q_\pi(s,\pi’(s))\geq v_\pi(s)\tag{3}\label{3}
\end{equation}
Then $\pi’\geq\pi$, which means for all $s\in\mathcal{S}$, we have $v_{\pi’}(s)\geq v_\pi(s)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Deriving \eqref{3} combined with \eqref{2}, we have&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;:
\begin{align}
v_\pi(s)&amp;amp;\leq q_\pi(s,\pi’(s)) \\ &amp;amp;=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi’(s)\right]\tag{by \eqref{2}} \\ &amp;amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right] \\ &amp;amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi’(S_{t+1}))|S_t=s\right]\tag{by \eqref{3}} \\ &amp;amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma\mathbb{E}_{\pi’}\left[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi’(S_{t+1})\right]|S_t=s\right] \\ &amp;amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\right] \\ &amp;amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|S_t=s\right] \\ &amp;amp;\quad\vdots \\ &amp;amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\dots|S_t=s\right] \\ &amp;amp;=v_{\pi’}(s)
\end{align}&lt;/p&gt;

&lt;p&gt;Consider the new &lt;em&gt;greedy policy&lt;/em&gt;, $\pi’$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\pi$, given by
\begin{align}
\pi’(s)&amp;amp;\doteq\arg\max_a q_\pi(s,a) \\ &amp;amp;=\arg\max_a\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{4}\label{4} \\ &amp;amp;=\arg\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]
\end{align}
By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.&lt;br /&gt;
Suppose the new greedy policy, $\pi’$, is as good as, but not better than, $\pi$. Or in other words, $v_\pi=v_{\pi’}$. And from \eqref{4}, we have for all $s\in\mathcal{S}$,
\begin{align}
v_{\pi’}(s)&amp;amp;=\max_a\mathbb{E}\left[R_{t+1}+\gamma v_{\pi’}(S_{t+1})|S_t=s,A_t=a\right] \\ &amp;amp;=\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_{\pi’}(s’)\right]
\end{align}
which is the Bellman optimality equation for action-value function. And therefore, $v_{\pi’}$ must be $v_*$. Hence, &lt;em&gt;policy improvement&lt;/em&gt; must give us a strictly better policy except when the original one is already optimal&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h3&gt;
&lt;p&gt;Once we have obtained a better policy, $\pi’$, by improving a policy $\pi$ using $v_\pi$, we can repeat the same process by computing $v_{\pi’}$, and improve it to yield an even better $\pi’’$. Repeating it again and again, we get an iterative procedure to improve the policy
\begin{equation}
\pi_0\xrightarrow[]{\text{evaluation}}v_{\pi_0}\xrightarrow[]{\text{improvement}}\pi_1\xrightarrow[]{\text{evaluation}}v_{\pi_1}\xrightarrow[]{\text{improvement}}\pi_2\xrightarrow[]{\text{evaluation}}\dots\xrightarrow[]{\text{improvement}}\pi_*\xrightarrow[]{\text{evaluation}}v_*
\end{equation}
Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations.
This algorithm is called &lt;strong&gt;policy iteration&lt;/strong&gt;. And here is the pseudocode of the policy iteration.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/policy-iteration.png&quot; alt=&quot;policy iteration pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h3&gt;
&lt;p&gt;When using &lt;em&gt;policy iteration&lt;/em&gt;, each of its iterations involves policy evaluation, which requires multiple sweeps through the state set, and thus affects the computation performance.&lt;br /&gt;
Policy evaluation step of policy iteration, in fact, can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called &lt;strong&gt;value iteration&lt;/strong&gt;, which follows this update:
\begin{align}
v_{k+1}&amp;amp;\doteq\max_a\mathbb{E}\left[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a\right] \\ &amp;amp;=\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_k(s’)\right],
\end{align}
for all $s\in\mathcal{S}$. Once again, thanks to &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts&quot;&gt;Banach’s fixed point theorem&lt;/a&gt;, for an arbitrary $v_0$, we have that the sequence $\{v_k\}\to v_*$ as $k\to\infty$.&lt;br /&gt;
We have the backup diagram for this update&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/backup-value-iteration.png&quot; alt=&quot;Backup diagram of value iteration update&quot; width=&quot;360&quot; height=&quot;200px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 3&lt;/b&gt;: Backup diagram of Value Iteration update&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
And here is the pseudocode of the value iteration.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/value-iteration.png&quot; alt=&quot;value iteration pseudocode&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;gpi&quot;&gt;Generalized Policy Iteration&lt;/h3&gt;
&lt;p&gt;The &lt;strong&gt;Generalized Policy Iteration (GPI)&lt;/strong&gt; algorithm refers to the idea of combining policy evaluation and policy improvement together to improve the original policy.&lt;br /&gt;
In GPI, the value function is repeatedly driven toward the true value of the current policy and at the same time the policy is being improved optimality with respect to its value function, as in the following diagram.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/gpi.png&quot; alt=&quot;GPI&quot; width=&quot;200&quot; height=&quot;320px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 4&lt;/b&gt;: Generalized Policy Iteration&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;
Once it reaches the stationary state (when both evaluation and improvement no long produce any updates), then the current value function and policy must be optimal.&lt;br /&gt;
The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They competing in the sense that on the one hand, making policy greedy w.r.t the value function typically makes value function incorrect for the new policy. And on the other hand, approximating the value function closer to the true value of the policy typically forces the policy is no longer to be greedy. But in the long run, they two processes cooperate to find a single joint solution: the optimal value function and an optimal policy.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/gpi-rel.png&quot; alt=&quot;GPI interaction&quot; width=&quot;360&quot; height=&quot;200px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 5&lt;/b&gt;: Interaction between the evaluation and improvement processes in GPI&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;example&quot;&gt;Example - Gambler’s Problem&lt;/h3&gt;
&lt;p&gt;This example is taken from &lt;em&gt;Example 4.3&lt;/em&gt; in the &lt;em&gt;Reinforcement Learning: An Introduction&lt;/em&gt; book&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-07-25/example.png&quot; alt=&quot;example&quot; width=&quot;500&quot; height=&quot;500px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 6&lt;/b&gt;: Example - Gambler&apos;s Problem&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Solution code&lt;/strong&gt;&lt;br /&gt;
The code can be found &lt;a href=&quot;https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-4/gambler.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;button type=&quot;button&quot; class=&quot;collapsible&quot; id=&quot;codeP&quot;&gt;Click to show the code&lt;/button&gt;&lt;/p&gt;
&lt;div class=&quot;codePanel&quot; id=&quot;codePdata&quot;&gt;
  &lt;p&gt;&lt;br /&gt;&lt;/p&gt;

  &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#For convenience, we introduce 2 dummy states: 0 and terminal state
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;non-terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;HEAD_PROB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;GAMMA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# discount factor
&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;value_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;V_set&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;V_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;old_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;next_tail_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;head_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;non-terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;tail_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;non-terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEAD_PROB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GAMMA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; \
                    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEAD_PROB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tail_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GAMMA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_tail_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;new_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_value&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;old_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Max value changed: &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;V_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;break&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;next_tail_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;head_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GOAL&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;non-terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tail_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rewards&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;non-terminal&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HEAD_PROB&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;head_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GAMMA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_head_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;
                          &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HEAD_PROB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tail_reward&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GAMMA&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;next_tail_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;actions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;V_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;policy&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&apos;__main__&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-13&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;value_funcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimal_policy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_iteration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimal_value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value_funcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimal_value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;211&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sweep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value_funcs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;sweep {}&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sweep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Capital&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Value estimates&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;best&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;212&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;states&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optimal_policy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Capital&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Final policy (stake)&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;./gambler.png&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;And here is our results after running the code&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&quot;/assets/images/2021-07-25/gambler.png&quot; alt=&quot;gambler&quot; width=&quot;450&quot; height=&quot;900px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
    &lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 7&lt;/b&gt;: Example - Gambler&apos;s Problem Result&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] David Silver. &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;UCL course on RL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Csaba Szepesvári. &lt;a href=&quot;https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924&quot;&gt;Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] A. Lazaric. &lt;a href=&quot;http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf&quot;&gt;Markov Decision Processes and Dynamic Programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://en.wikipedia.org/wiki/Dynamic_programming&quot;&gt;Dynamic Programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Shangtong Zhang’s repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] &lt;a href=&quot;https://stats.stackexchange.com/a/258783&quot;&gt;Policy Improvement theorem&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In the third step, the expression
\begin{equation}
\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]
\end{equation}
means ‘‘the discounted expected value when starting in state $s$, choosing action according to $\pi’$ for the next time step, and following $\pi$ thereafter”. And so on for the two, or n next steps. Therefore, we have that:
\begin{equation}
\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi’(s)\right]
\end{equation} &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The idea of policy improvement also extends to stochastic policies. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Value iteration can be used in conjunction with action-value function, which takes the following update:
\begin{align}
q_{k+1}(s,a)&amp;amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma\max_{a’}q_k(S_{t+1},a’)|S_t=s,A_t=a\right] \\ &amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\max_{a’}q_k(s’,a’)\right]
\end{align}
Yep, that’s right, the sequence $\{q_k\}\to q_*$ as $k\to\infty$ at a geometric rate thanks to &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts&quot;&gt;Banach’s fixed point theorem&lt;/a&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="dynamic-programming" /><category term="my-rl" /><summary type="html">In two previous posts, Markov Decision Process (MDP) and Bellman equations and Optimal Policy Existence, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with Dynamic Programming.</summary></entry><entry><title type="html">Optimal Policy Existence</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html" rel="alternate" type="text/html" title="Optimal Policy Existence" /><published>2021-07-10T13:03:00+07:00</published><updated>2021-07-10T13:03:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html">&lt;blockquote&gt;
  &lt;p&gt;In the previous post about &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;&lt;strong&gt;Markov Decision Processes, Bellman equations&lt;/strong&gt;&lt;/a&gt;, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#norms-contractions-banach-fixed-pts&quot;&gt;Norms, Contractions, Banach’s Fixed-point Theorem&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#norms&quot;&gt;Norms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#contractions&quot;&gt;Contractions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#banach-fixed-pts&quot;&gt;Banach’s Fixed-point Theorem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bellman-operator&quot;&gt;Bellman Operator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#proof&quot;&gt;Proof of the existence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before catching the pokémon, we need to prepare ourselves some pokéball.&lt;/p&gt;

&lt;h2 id=&quot;norms-contractions-banach-fixed-pts&quot;&gt;Norms, Contractions, Banach’s Fixed-point Theorem&lt;/h2&gt;

&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Norm&lt;/em&gt;)&lt;br /&gt;
Given a vector space $\mathcal{V}\subseteq\mathbb{R}^d$, a function $f:\mathcal{V}\to\mathbb{R}^+_0$ is a &lt;em&gt;norm&lt;/em&gt; if and only if&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If $f(v)=0$ for some $v\in\mathcal{V}$, then $v=0$&lt;/li&gt;
  &lt;li&gt;For any $\lambda\in\mathbb{R},v\in\mathcal{V},f(\lambda v)=|\lambda|v$&lt;/li&gt;
  &lt;li&gt;For any $u,v\in\mathbb{R}, f(u+v)\leq f(u)+f(v)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (&lt;em&gt;Norm&lt;/em&gt;)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\ell^p$ norms: for $p\geq 1$,
\begin{equation}
\Vert v\Vert_p=\left(\sum_{i=1}^{d}|v_i|^p\right)^{1/p}
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^\infty$ norms:
\begin{equation}
\Vert v\Vert_\infty=\max_{1\leq i\leq d}|v_i|
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^{\mu,p}$: the weighted variants of these norm are defined as
\begin{equation}
\Vert v\Vert_p=\begin{cases}\left(\sum_{i=1}^{d}\frac{|v_i|^p}{w_i}\right)^{1/p}&amp;amp;\text{if }1\leq p&amp;lt;\infty\\ \max_{1\leq i\leq d}\frac{|v_i|}{w_i}&amp;amp;\text{if }p=\infty\end{cases}
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^{2,P}$: the matrix-weighted 2-norm is defined as
\begin{equation}
\Vert v\Vert^2_P=v^TPv
\end{equation}
Similarly, we can define norms over spaces of functions. For example, if $\mathcal{V}$ is the vector space of functions over domain $\mathcal{X}$ which are &lt;em&gt;uniformly bounded&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, then
\begin{equation}
\Vert f\Vert_\infty=\sup_{x\in\mathcal{X}}\vert f(x)\vert
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Convergence in norm&lt;/em&gt;)&lt;br /&gt;
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a &lt;em&gt;normed vector space&lt;/em&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. Let $v_n\in\mathcal{V}$ is a sequence of vectors ($n\in\mathbb{N}$). The sequence ($v_n,n\geq 0$) is said to &lt;em&gt;converge to&lt;/em&gt; $v\in\mathcal{V}$ in the norm $\Vert\cdot\Vert$, denoted as $v_n\to_{\Vert\cdot\Vert}v$ if
\begin{equation}
\lim_{n\to\infty}\Vert v_n-v\Vert=0,
\end{equation}
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Cauchy sequence&lt;/em&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;)&lt;br /&gt;
Let ($v_n;n\geq 0$) be a sequence of vectors of a normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$. Then $v_n$ is called a &lt;em&gt;Cauchy sequence&lt;/em&gt; if
\begin{equation}
\lim_{n\to\infty}\sup_{m\geq n}\Vert v_n-v_m\Vert=0
\end{equation}
Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Completeness&lt;/em&gt;)&lt;br /&gt;
A normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ is called &lt;em&gt;complete&lt;/em&gt; if every Cauchy sequence in $\mathcal{V}$ is convergent in the norm of the vector space.&lt;/p&gt;

&lt;h3 id=&quot;contractions&quot;&gt;Contractions&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Lipschitz function&lt;/em&gt;, &lt;em&gt;Contraction&lt;/em&gt;) &lt;br /&gt;
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a normed vector space. A mapping $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ is called &lt;em&gt;L-Lipschitz&lt;/em&gt; if for any $u,v\in\mathcal{V}$,
\begin{equation}
\Vert\mathcal{T}u-\mathcal{T}v\Vert\leq L\Vert u-v\Vert
\end{equation}
A mapping $\mathcal{T}$ is called a &lt;em&gt;non-expansion&lt;/em&gt; if it is &lt;em&gt;Lipschitzian&lt;/em&gt; with $L\leq 1$. It is called a &lt;em&gt;contraction&lt;/em&gt; if it is &lt;em&gt;Lipschitzian&lt;/em&gt; with $L&amp;lt;1$. In this case, $L$ is called the &lt;em&gt;contraction factor of&lt;/em&gt; $\mathcal{T}$ and $\mathcal{T}$ is called an &lt;em&gt;L-contraction&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
If $\mathcal{T}$ is &lt;em&gt;Lipschitz&lt;/em&gt;, it is also continuous in the sense that if $v_n\to_{\Vert\cdot\Vert}v$, then also $\mathcal{T}v_n\to_{\Vert\cdot\Vert}\mathcal{T}v$. This is because $\Vert\mathcal{T}v_n-\mathcal{T}v\Vert\leq L\Vert v_n-v\Vert\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;h3 id=&quot;banach-fixed-pts&quot;&gt;Banach’s Fixed-point Theorem&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Banach space&lt;/em&gt;)&lt;br /&gt;
A complete, normed vector space is called a &lt;em&gt;Banach space&lt;/em&gt;.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Fixed point&lt;/em&gt;)&lt;br /&gt;
Let $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be some mapping. The vector $v\in\mathcal{V}$ is called a &lt;em&gt;fixed point of&lt;/em&gt; $\mathcal{T}$ if $\mathcal{T}v=v$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Banach’s fixed-point&lt;/em&gt;)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;    &lt;br /&gt;
Let $\mathcal{V}$ be a Banach space and $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be a $\gamma$-contraction mapping. Then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mathcal{T}$ admits a &lt;em&gt;unique fixed point&lt;/em&gt; $v$.&lt;/li&gt;
  &lt;li&gt;For any $v_0\in\mathcal{V}$, if $v_{n+1}=\mathcal{T}v_n$, then $v_n\to_{\Vert\cdot\Vert}v$ with a &lt;em&gt;geometric convergence rate&lt;/em&gt;&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;:
\begin{equation}
\Vert v_n-v\Vert\leq\gamma^n\Vert v_0-v\Vert
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;bellman-operator&quot;&gt;Bellman Operator&lt;/h2&gt;
&lt;p&gt;Previously, we defined Bellman equation for state-value function $v_\pi(s)$ as:
\begin{align}
v_\pi(s)&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right] \\\text{or}\quad v_\pi(s)&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_\pi(s’)\right)\tag{1}\label{1}
\end{align}
If we let
\begin{align}
\mathcal{P}^\pi_{ss’}&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}^a_{ss’}; \\\mathcal{R}^\pi_s&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s
\end{align}
then we can rewrite \eqref{1} in another form as
\begin{equation}
v_\pi(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v_\pi(s’)\tag{2}\label{2}
\end{equation}
&lt;br /&gt;
&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Bellman operator&lt;/em&gt;)&lt;br /&gt;
We define the &lt;em&gt;Bellman operator&lt;/em&gt; underlying $\pi,\mathcal{T}:\mathbb{R}^\mathcal{S}\to\mathbb{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^\pi v)(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v(s’)
\end{equation}
&lt;br /&gt;
With the help of $\mathcal{T}^\pi$, equation \eqref{2} can be rewrite as:
\begin{equation}
\mathcal{T}^\pi v_\pi=v_\pi\tag{3}\label{3}
\end{equation}
Similarly, we can rewrite the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $v_*$
\begin{align}
v_*(s)&amp;amp;=\max_{a\in\mathcal{A}}\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_*(s’)\right] \\ &amp;amp;=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_*(s’)\right)\tag{4}\label{4}
\end{align}
and thus, we can define the &lt;em&gt;Bellman optimality operator&lt;/em&gt; $\mathcal{T}^*:\mathcal{R}^\mathcal{S}\to\mathcal{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^* v)(s)=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v(s’)\right)
\end{equation}
And thus, with the help of $\mathcal{T}^*$, we can rewrite the equation \eqref{4} as:
\begin{equation}
\mathcal{T}^*v_*=v_*\tag{5}\label{5}
\end{equation}
&lt;br /&gt;
Now everything is all set, we can move on to the next step.&lt;/p&gt;

&lt;h2 id=&quot;proof&quot;&gt;Proof of the existence&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Let $B(\mathcal{S})$ be the space of &lt;em&gt;uniformly bounded functions&lt;/em&gt; with domain $\mathcal{S}$:
\begin{equation}
B(\mathcal{S})=\{v:\mathcal{S}\to\mathbb{R}:\Vert v\Vert_\infty&amp;lt;+\infty\}
\end{equation}&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We will view $B(\mathcal{S})$ as a normed vector space with the norm $\Vert\cdot\Vert_\infty$. It is easily seen that $(B(\mathcal{S}),\Vert\cdot\Vert_\infty)$ is complete: If ($v_n;n\geq0$) is a Cauchy sequence in it then for any $s\in\mathcal{S}$, ($v_n(s);n\geq0$) is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of ($v_n(s)$), we can show that $\Vert v_n-v\Vert_\infty\to0$. Vaguely speaking, this holds because ($v_n;n\geq0$) is a Cauchy sequence in the norm $\Vert\cdot\Vert_\infty$  so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pick any stationary policy $\pi$.&lt;/li&gt;
  &lt;li&gt;We have that $\mathcal{T}^\pi$ is &lt;em&gt;well-defined&lt;/em&gt; since: if $u\in B(\mathcal{S})$, then also $\mathcal{T}^\pi u\in B(S)$.&lt;/li&gt;
  &lt;li&gt;From equation \eqref{3}, we have that $v_\pi$ is a fixed point to $\mathcal{T}^\pi$.&lt;br /&gt;
We also have that $\mathcal{T}^\pi$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$ since for any $u, v\in B(\mathcal{S})$,
\begin{align}
\Vert\mathcal{T}^\pi u-\mathcal{T}^\pi v\Vert_\infty&amp;amp;=\gamma\max_{s\in\mathcal{S}}\left|\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left(u(s’)-v(s’)\right)\right| \\ &amp;amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\Vert u-v\Vert_\infty \\ &amp;amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
where the last line follows from $\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}=1$.&lt;/li&gt;
  &lt;li&gt;It follows that in order to find $v_\pi$, we can construct the sequence $v_0,\mathcal{T}^\pi v_0,(\mathcal{T}^\pi)^2 v_0,\dots$, which, by Banach’s fixed-point theorem will converge to $v_\pi$ at a geometric rate.&lt;/li&gt;
  &lt;li&gt;From the definition \eqref{5} of $\mathcal{T}^*$, we have that $\mathcal{T}^*$ is well-defined.&lt;/li&gt;
  &lt;li&gt;Using the fact that $\left|\max_{a\in\mathcal{A}}f(a)-\max_{a\in\mathcal{A}}g(a)\right|\leq\max_{a\in\mathcal{A}}\left|f(a)-g(a)\right|$, similarly, we have:
\begin{align}
\Vert\mathcal{T}^*u-\mathcal{T}^*v\Vert_\infty&amp;amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\Vert u-v\Vert_\infty \\ &amp;amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
which tells us that $\mathcal{T}^*$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;br /&gt;
Let $v$ be the fixed point of $\mathcal{T}^*$ and assume that there is policy $\pi$ which is greedy w.r.t $v:\mathcal{T}^\pi v=\mathcal{T}^* v$. Then $v=v_*$ and $\pi$ is an optimal policy.&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
Pick any stationary policy $\pi$. Then $\mathcal{T}^\pi\leq\mathcal{T}^*$ in the sense that for any function $v\in B(\mathcal{S})$, $\mathcal{T}^\pi v\leq\mathcal{T}^* v$ holds ($u\leq v$ means that $u(s)\leq v(s),\forall s\in\mathcal{S}$).&lt;br /&gt;
Hence, for all $n\geq0$,
\begin{equation}
v_\pi=\mathcal{T}^\pi v_\pi\leq\mathcal{T}^*v_\pi\leq(\mathcal{T}^*)^2 v_\pi\leq\dots\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
or
\begin{equation}
v_\pi\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
Since $\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\mathcal{T}^*$. Thus, $v_\pi\leq v$. And since $\pi$ was arbitrary, we obtain that $v_*\leq v$.&lt;br /&gt;
Pick a policy $\pi$ such that $\mathcal{T}^\pi v=\mathcal{T}^*v$, then $v$ is also a fixed point of $\mathcal{V}^\pi$. Since $v_\pi$ is the unique fixed point of $\mathcal{T}^\pi$, we have that $v=v_\pi$, which shows that $v_*=v$ and that $\pi$ is an optimal policy.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Csaba Szepesvári. &lt;a href=&quot;https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924&quot;&gt;Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] A. Lazaric. &lt;a href=&quot;http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf&quot;&gt;Markov Decision Processes and Dynamic Programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://ai.stackexchange.com/a/11133&quot;&gt;What is the Bellman operator in reinforcement learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://en.wikipedia.org/wiki/Normed_vector_space&quot;&gt;Normed vector space&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function is called &lt;em&gt;uniformly bounded&lt;/em&gt; exactly when $\Vert f\Vert_\infty&amp;lt;+\infty$. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A &lt;em&gt;normed vector space&lt;/em&gt; is a vector space over the real or complex number, on which a norm is defined. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We are gonna talk further about &lt;em&gt;sequences&lt;/em&gt; in another &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#convergent-sequences&quot;&gt;post&lt;/a&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
Pick any $v_0\in\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\mathcal{T}$. c. Finally, we show that $\mathcal{T}$ has a single fixed point. Assume that $\mathcal{T}$ is a $\gamma$-contraction.&lt;br /&gt;
a. To show that $(v_n)$ converges, it suffices  to show that $(v_n)$ is a Cauchy sequence. We have:
\begin{align}
\Vert v_{n+1}-v_n\Vert&amp;amp;=\Vert\mathcal{T}v_{n}-\mathcal{T}v_{n-1}\Vert \\ &amp;amp;\leq\gamma\Vert v_{n}-v_{n-1}\Vert \\ &amp;amp;\quad\vdots \\ &amp;amp;\leq\gamma^n\Vert v_1-v_0\Vert
\end{align}
From the properties of norms, we have:
\begin{align}
\Vert v_{n+k}-v_n\Vert&amp;amp;\leq\Vert v_{n+1}-v_n\Vert+\dots+\Vert v_{n+k}-v_{n+k-1}\Vert \\ &amp;amp;\leq\left(\gamma^n+\dots+\gamma^{n+k-1}\right)\Vert v_1-v_0\Vert \\ &amp;amp;=\gamma^n\dfrac{1-\gamma^{k}}{1-\gamma}\Vert v_1-v_0\Vert
\end{align}
and so
\begin{equation}
\lim_{n\to\infty}\sup_{k\geq0}\Vert v_{n+k}-v_n\Vert=0,
\end{equation}
shows us that $(v_n;n\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.&lt;br /&gt;
b. Recall that the definition of the sequence $(v_n;n\geq0)$
\begin{equation}
v_{n+1}=\mathcal{T}v_n
\end{equation}
Taking the limes as $n\to\infty$ of both sides, one the one hand, we get that $v_{n+1}\to _{\Vert\cdot\Vert}v$. On the other hand, $\mathcal{T}v_n\to _{\Vert\cdot\Vert}\mathcal{T}v$, since $\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\mathcal{T}v$, which tells us that $v$ is a fixed point of $\mathcal{T}$.&lt;br /&gt;
c. Let us assume that $v,v’$ are both fixed points of $\mathcal{T}$. Then,
\begin{align}
\Vert v-v’\Vert&amp;amp;=\Vert\mathcal{T}v-\mathcal{v’}\Vert \\ &amp;amp;\leq\gamma\Vert v-v’\Vert \\ \text{or}\quad(1-\gamma)\Vert v-v’\Vert&amp;amp;\leq0
\end{align}
Thus, we must have that $\Vert v-v’\Vert=0$. Therefore, $v-v’=0$ or $v=v’$.&lt;br /&gt;
And finally,
\begin{align}
\Vert v_n-v\Vert&amp;amp;=\Vert\mathcal{T}v_{n-1}-\mathcal{T}v\Vert \\ &amp;amp;\leq\gamma\Vert v_{n-1}-v\Vert \\ &amp;amp;\quad\vdots \\ &amp;amp;\leq\gamma^n\Vert v_0-v\Vert
\end{align} &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Also, there’s gonna be a post about &lt;em&gt;rate of convergence&lt;/em&gt;. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="mathematics" /><category term="my-rl" /><summary type="html">In the previous post about Markov Decision Processes, Bellman equations, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.</summary></entry><entry><title type="html">Measures</title><link href="http://localhost:4000/random-stuffs/measure-theory/2021/07/03/measure.html" rel="alternate" type="text/html" title="Measures" /><published>2021-07-03T07:00:00+07:00</published><updated>2021-07-03T07:00:00+07:00</updated><id>http://localhost:4000/random-stuffs/measure-theory/2021/07/03/measure</id><content type="html" xml:base="http://localhost:4000/random-stuffs/measure-theory/2021/07/03/measure.html">&lt;blockquote&gt;
  &lt;p&gt;When talking about &lt;em&gt;measure&lt;/em&gt;, you might associate it with the idea of &lt;em&gt;length&lt;/em&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;em&gt;area&lt;/em&gt;, or even three dimensions with &lt;em&gt;volume&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;p&gt;Despite of having different number of dimensions, all &lt;em&gt;length&lt;/em&gt;, &lt;em&gt;area&lt;/em&gt;, &lt;em&gt;volume&lt;/em&gt; share the same properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Non-negative&lt;/em&gt;: In principle, length, area, and volume can be any positive value. But negative length has no meaning. Same thing happens with negative area and negative volume.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Additivity&lt;/em&gt;: To get from Hanoi to Singapore by air, you have to transit at Ho Chi Minh city (HCMC). If we cut that path into two non-overlapping pieces, say Hanoi - HCMC, and HCMC - Singapore, then the total length of the two pieces must be equal to the length of original path. If we divide a rectangular into non-overlapping pieces, the area of pieces combined must be the same as the original one. The same is true for volume as well.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Empty Set&lt;/em&gt;: An empty cup of water has volume zero.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Other Null Sets&lt;/em&gt;: The length of a point is 0. The area of a line, or a curve is 0. The volume of a plane or a surface is 0.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Translation Invariance&lt;/em&gt;: Length, area and volume are unchanged (&lt;em&gt;invariant&lt;/em&gt;) under shifts (&lt;em&gt;translation&lt;/em&gt;) in space.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Hyper-rectangles&lt;/em&gt;: An interval of form $[a, b]\subset\mathbb{R}^3$ has length $b-a$. The area of a rectangle $[a_1,b_1]\times[a_2,b_2]$ is $(b_1-a_1)(b_2-a_2)$. And the volume of a rectangular $[a_1,b_1]\times[a_2,b_2]\times[a_3,b_3]$ is $(b_1-a_1)(b_2-a_2)(b_3-a_3)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lego.jpg&quot; alt=&quot;Lego&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lebesgue-measure&quot;&gt;Lebesgue Measure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#measures&quot;&gt;Measures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-measure-idea&quot;&gt;Integration with respect to a Measure: The Idea&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prop-int&quot;&gt;Properties of the Integral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-measure-detail&quot;&gt;Integration with respect to a Measure: The Details&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#construct-measure&quot;&gt;Constructing Measures from old ones&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-types&quot;&gt;Other types of Measures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-resources&quot;&gt;Other Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lebesgue-measure&quot;&gt;Lebesgue Measure&lt;/h2&gt;
&lt;p&gt;Is an extension of the classical notion of length in $\mathbb{R}$, area in $\mathbb{R}^2$ to any $\mathbb{R}^k$ using k-dimensional hyper-rectangles.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;br /&gt;
Given an open set $S\equiv\sum_k(a_k,b_k)$ containing disjoint intervals, the &lt;strong&gt;Lebesgue measure&lt;/strong&gt; is defined by:
\begin{equation}
\mu_L(S)\equiv\sum_{k}(b_k-a_k)
\end{equation}
Given a closed set $S’\equiv[a,b]-\sum_k(a_k,b_k)$,
\begin{equation}
\mu_L(S’)\equiv(b-a)-\sum_k(b_k-a_k)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;measures&quot;&gt;Measures&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;br /&gt;
Let $\mathcal{X}$ be any set. A &lt;em&gt;measure&lt;/em&gt; on $\mathcal{X}$ is a function $\mu$ that maps the set of subsets on $\mathcal{X}$ to $[0,\infty]$ ($\mu:2^\mathcal{X}\rightarrow[0,\infty]$) that satisfies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mu(\emptyset)=0$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Countable additivity property&lt;/em&gt;: for any countable and pairwise disjoint collection of subsets of $\mathcal{X},\mathcal{A_1},\mathcal{A_2},\dots$,
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)=\sum_i\mu(\mathcal{A_i})
\end{equation}
$\mu(\mathcal{A})$ is called &lt;em&gt;measure of the set $\mathcal{A}$&lt;/em&gt;, or &lt;em&gt;measure of $\mathcal{A}$&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;: If $\mathcal{A}\subset\mathcal{B}$, then $\mu(\mathcal{A})\leq\mu(\mathcal{B})$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Subadditivity&lt;/em&gt;: If $\mathcal{A_1},\mathcal{A_2},\dots$ is a countable collection of sets, not necessarily disjoint, then
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)\leq\sum_i\mu(\mathcal{A_i})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Cardinality of a set&lt;/em&gt; \(\#\mathcal{A}\)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;A point mass at 0&lt;/em&gt;. Consider a measure \(\delta_{\{0\}}\) on $\mathbb{R}$ defined to give measure 1 to any set that contains 0 and measure 0 to any set that does not
\begin{equation}\delta_{{0}}(\mathcal{A})=\#\left(A\cap\{0\}\right)=\begin{cases}
1\quad\textsf{if }0\in\mathcal{A} \\ 0\quad\textsf{otherwise}
\end{cases}\end{equation}
for $\mathcal{A}\subset\mathbb{R}$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Counting measure on the integers&lt;/em&gt;. Consider a measure $\mu_\mathbb{Z}$ that assigns to each set $\mathcal{A}$ the number of integers contained in $\mathcal{A}$
\begin{equation}
\delta_\mathbb{Z}(\mathcal{A})=\#\left(\mathcal{A}\cap\mathbb{Z}\right)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Geometric measure&lt;/em&gt;. Suppose that $0&amp;lt;r&amp;lt;1$. We define a measure on $\mathbb{R}$ that assigns to a set $\mathcal{A}$ a geometrically weighted sum over non-negative integers in $\mathcal{A}$
\begin{equation}
\mu(\mathcal{A})=\sum_{i\in\mathcal{A}\cap\mathbb{Z}^+}r^i
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Binomial measure&lt;/em&gt;. Let $n\in\mathbb{N}^+$ and let $0&amp;lt;p&amp;lt;1$. We define $\mu$ as:
\begin{equation}
\mu(\mathcal{A})=\sum_{k\in\mathcal{A}\cap\{0,1,\dots,n\}}{n\choose k}p^k(1-p)^{n-k}
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bivariate Gaussian&lt;/em&gt;. We define a measure on $\mathbb{R}^2$ by:
\begin{equation}
\mu({\mathcal{A}})=\int_\mathcal{A}\dfrac{1}{2\pi}\exp\left({\dfrac{-1}{2}(x^2+y^2)}\right)\,dx\,dy
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Uniform on a Ball in $\mathbb{R}^3$&lt;/em&gt;. Let $\mathcal{B}$ be the set of points in $\mathbb{R}^3$ that are within a distance 1 from the origin (unit ball in $\mathbb{R}^3$). We define a measure on $\mathbb{R}^3$ as:
\begin{equation}
\mu(\mathcal{A})=\dfrac{3}{4\pi}\mu_L(\mathcal{A}\cap\mathcal{B})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;int-measure-idea&quot;&gt;Integration with respect to a Measure: The Idea&lt;/h2&gt;
&lt;p&gt;Consider $f:\mathcal{X}\rightarrow\mathbb{R}$, where $\mathcal{X}$ is any set and a measure $\mu$ on $\mathcal{X}$ and compute the integral of $f$ w.r.t $\mu$: $\int f(x)\,\mu(dx)$. We have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\mu_L(dx)=\int g(x)\,dx
\end{equation}
Because $\mu_L(dx)\equiv\mu_L([x,x+dx[)=dx$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_{\{\alpha\}}(dx)=g(\alpha)
\end{equation}
Consider the infinitesimal $\delta_{\{\alpha\}}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\neq\alpha$, then the infinitesimal interval $[x,x+dx[$ does not contain $\alpha$, so
\begin{equation}
\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=0
\end{equation}
If $x=\alpha,\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=1$. Thus, when we add up all of the infinitesimals, we get $g(\alpha)\cdot1=g(\alpha)$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathbb{Z}(dx)=\sum_{i\in\mathbb{Z}}g(i)
\end{equation}
Similarly, consider the infinitesimal $\delta_\mathbb{Z}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\notin\mathbb{Z}$, then $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=0$. And otherwise if $x\in\mathbb{Z}$, $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=1$ since an infinitesimal interval can contain at most one integer. Hence, $g(x)\,\delta_\mathbb{Z}=g(x)$ if $x\in\mathbb{Z}$ and $=0$ otherwise. When we add up all of the infinitesimals over $x$, we get the sum above.&lt;/li&gt;
  &lt;li&gt;Suppose $\mathcal{C}$ is a countable set. We can define &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{C}$ to map $\mathcal{A}\rightarrow\#(\mathcal{A}\cap\mathcal{C})$ (recall that $\delta_\mathcal{C}(\mathcal{A})=\#(\mathcal{A}\cap\mathcal{C})$). For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v),
\end{equation}
using the same basic argument as in the above example.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above examples, we have that &lt;em&gt;integrals w.r.t to Lebesgue measure are just ordinary integrals, and that integrals w.r.t Counting measure are just ordinary summation&lt;/em&gt;.&lt;br /&gt;
Consider measures built from Lebesgue and Counting measure, we have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=f(x)\,\mu_L(dx)$, then for any function $g$,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,f(x)\,\mu_L(dx)=\int g(x)\,f(x)\,dx
\end{equation}
We say that $f$ is the density of $\mu$ w.r.t Lebesgue measure in this case.&lt;/li&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=p(x)\delta_\mathcal{C}(dx)$ for a countable set $\mathcal{C}$, then for any function g,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,p(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v)\,f(v)
\end{equation}
We say that $p$ is the density of $\mu$ w.r.t Counting measure on $\mathcal{C}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;prop-int&quot;&gt;Properties of the Integral&lt;/h2&gt;
&lt;p&gt;A function is said to be &lt;em&gt;integrable&lt;/em&gt; w.r.t $\mu$ if $\int|f(x)|\,\mu(dx)&amp;lt;\infty$. An integrable function has a well-defined and finite integral. If $f(x)\geq0$, the integral is always well-defined but may be $\infty$.&lt;br /&gt;
Suppose $\mu$ is a measure on $\mathcal{X},\mathcal{A}\subset\mathcal{X}$, and $g$ is a real-valued function on $\mathcal{X}$. We define the integral of $g$ over the set $\mathcal{A}$, denoted by $\int_\mathcal{A}g(x)\,\mu(dx)$, as
\begin{equation}
\int_\mathcal{A}g(x)\,\mu(dx)=\int g(x)\,𝟙_\mathcal{A}(x)\,\mu(dx),
\end{equation}
where \(𝟙_\mathcal{A}\) is an &lt;em&gt;indicator function&lt;/em&gt; (\(𝟙_\mathcal{A}(x)=1\) if $x\in\mathcal{A}$, and $=0$ otherwise).&lt;/p&gt;

&lt;p&gt;Let $\mu$ is a measure on $\mathcal{X},\mathcal{A},\mathcal{B}\subset\mathcal{X},c\in\mathbb{R}$ and $f,g$ are integrable functions. The following properties hold for every $\mu$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Constant Functions&lt;/em&gt;
\begin{equation}
\int_\mathcal{A}c\,\mu(dx)=c\cdot\mu(\mathcal{A})
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity&lt;/em&gt;
\begin{align}
\int_\mathcal{A}cf(x)\,\mu(dx)&amp;amp;=c\int_\mathcal{A}f(x)\,\mu(dx) \\\int_\mathcal{A}\left(f(x)+g(x)\right)\,\mu(dx)&amp;amp;=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{A}g(x)\,\mu(dx)
\end{align}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;. If $f\leq g$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{A}g(x)\,\mu(dx),\forall\mathcal{A}$. This implies:
    &lt;ul&gt;
      &lt;li&gt;If $f\geq0$, then $\int f(x)\,\mu(dx)\geq0$.&lt;/li&gt;
      &lt;li&gt;If $f\geq0$ and $\mathcal{A}\subset\mathcal{B}$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{B}f(x)\,\mu(dx)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Null Sets&lt;/em&gt;. If $\mu(\mathcal{A})=0$, then $\int_\mathcal{A}f(x)\,\mu(dx)=0$.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Absolute Values&lt;/em&gt;
\begin{equation}
\left|\int f(x)\,\mu(dx)\right|\leq\int\left|f(x)\right|\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotone Convergence&lt;/em&gt;. If $0\leq f_1\leq f_2\leq\dots$ is an increasing sequence of integrable functions that converge to $f$, then
\begin{equation}
\lim_{k\to\infty}\int f_k(x)\,\mu(dx)=\int f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity in region of integration&lt;/em&gt;. If $\mathcal{A}\cap\mathcal{B}=\emptyset$,
\begin{equation}
\int_{\mathcal{A}\cup\mathcal{B}}f(x)\,\mu(dx)=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{B}f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;int-measure-detail&quot;&gt;Integration with respect to a Measure: The Details&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;. Define the integral for simple functions.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simple function&lt;/em&gt;: is a function that takes only a finite number of different values.
        &lt;ul&gt;
          &lt;li&gt;All constant functions are simple functions.&lt;/li&gt;
          &lt;li&gt;The indicator function ($𝟙_\mathcal{A}$) of a set $\mathcal{A}\subset\mathcal{X}$ is a simple function (taking values in $\{0,1\}$).&lt;/li&gt;
          &lt;li&gt;Any constant times an indicator ($c𝟙_\mathcal{A}$) is also a simple function (taking values in $\{0,c\}$).&lt;/li&gt;
          &lt;li&gt;Similarly, given disjoint sets $\mathcal{A_1},\mathcal{A_2}$, the linear combination \(c_1𝟙_\mathcal{A_1}+c_2𝟙_\mathcal{A_2}\) is a simple function (taking values in $\{0,c_1,c_2\}$)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
          &lt;li&gt;In fact, any simple function can be expressed as a linear combination of a finite number of indicator functions. That is, if $f$ is &lt;em&gt;any&lt;/em&gt; simple function on $\mathcal{X}$, then there exists some finite integer $n$, non-zero constants $c_1,\dots,c_n$ and &lt;em&gt;disjoint&lt;/em&gt; sets $\mathcal{A_1},\dots\mathcal{A_n}\subset\mathcal{X}$ such that
   \begin{equation}
   f=c_1𝟙_\mathcal{A_1}+\dots+c_n𝟙_\mathcal{A_n}
   \end{equation}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;So, if $f:\mathcal{X}\to\mathbb{R}$ is a simple function as just defined, we have that
\begin{equation}
\int \mu(dx)=c_1\mu(\mathcal{A_1})+\dots+c_n\mu(\mathcal{A_n})
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;. Define the integral for general non-negative functions, approximating the general function by simple functions.
    &lt;ul&gt;
      &lt;li&gt;The idea is that we can approximate any general non-negative function $f:\mathcal{X}\to[0,\infty[$ well by some non-negative simple functions that $\leq f$ &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
      &lt;li&gt;If $f:\mathcal{X}\to[0,\infty[$ is a general function and $0\leq s\leq f$ is a simple function (then $\int s(x)\,\mu(dx)\leq\int f(x)\,\mu(dx)$). The closer that $s$ approximates $f$, the closer we expect $\int s(x)\,\mu(dx)$ and $\int f(x)\,\mu(x)$ to be.&lt;/li&gt;
      &lt;li&gt;To be more precise, we define the integral $\int f(x)\,\mu(dx)$ to be the smallest value $I$ such that $\int s(x)\,\mu(x)\leq I$, for all simple functions $0\leq s\leq f$.
\begin{equation}
\int f(x)\,\mu(dx)\approx\sup\left\{\int s(x)\,\mu(dx)\right\}
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;. Define the integral for general real-valued functions by separately integrating the positive and negative parts of the function.
    &lt;ul&gt;
      &lt;li&gt;If $f:\mathcal{X}\to\mathbb{R}$ is a general function, we can define its &lt;em&gt;positive part&lt;/em&gt; $f^+$ and its &lt;em&gt;negative part&lt;/em&gt; $f^-$ by
\begin{align}
f^+(x)&amp;amp;=\max\left(f(x),0\right) \\ f^-(x)&amp;amp;=\max\left(-f(x),0\right)
\end{align}&lt;/li&gt;
      &lt;li&gt;Since both $f^+$ and $f^-$ are non-negative functions and $f=f^+-f^-$, we have
\begin{equation}
\int f(x)\,\mu(dx)=\int f^+(x)\,\mu(dx)-\int f^-(x)\,\mu(dx)
\end{equation}
This is a well-defined number (possibly infinite) if and only if at least one of $f^+$ and $f^-$ has a finite integral.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;construct-measure&quot;&gt;Constructing Measures from old ones&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Sums and multiples&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Consider the point mass measures at 0 and 1, \(\delta_{\\{0\\}},\delta_1\), and construct a two new measures on $\mathbb{R}$, \(\mu=\delta_{\\{0\\}}+\delta_{\\{1\\}}\) and \(v=4\delta_{\\{0\\}}\), defined by
\begin{align}
\mu(\mathcal{A})&amp;amp;=\delta_{\{0\}}(\mathcal{A})+\delta_{\{0\}}(\mathcal{A}) \\ v(\mathcal{A})&amp;amp;=4\delta_{\{0\}}(\mathcal{A})
\end{align}&lt;/li&gt;
      &lt;li&gt;The measure $\mu$ counts how many elements of \(\\{0,1\\}\) are in its argument. Thus, the counting measure of the integers can be re-expressed as
\begin{equation}
\delta_\mathbb{Z}=\sum_{i=-\infty}^{\infty}\delta_{\{i\}}
\end{equation}&lt;/li&gt;
      &lt;li&gt;By combining the operations of summation and multiplication, we can write the Geometric measure in the above example 
\begin{equation}
\sum_{i=0}^{\infty}r^i\delta_{\{i\}}
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Restriction to a Subset&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $\mathcal{B}\subset\mathcal{X}$. We can define a new measure on $\mathcal{B}$ which maps $\mathcal{A}\subset\mathcal{B}\to\mu(\mathcal{A})$. This is called the restriction of $\mu$ to the set $\mathcal{B}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Measure Induced by a Function&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $g:\mathcal{X}\to\mathcal{Y}$. We can use $\mu$ and $g$ to define a new measure $v$ on $\mathcal{Y}$ by
\begin{equation}
v(\mathcal{A})=\mu(g^{-1}(\mathcal{A})),
\end{equation}
for $\mathcal{A}\subset\mathcal{Y}$. This is called the &lt;em&gt;measure induced from $\mu$ by $g$&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Therefore, for any $f:\mathcal{Y}\to\mathbb{R}$,
\begin{equation}
\int f(y)\,v(dy)=\int f(g(x))\,\mu(dx)
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integrating a Density&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $f:\mathcal{X}\to\mathbb{R}$. We can define a new measure $v$ on $\mathcal{X}$ as
\begin{equation}
v(\mathcal{A})=\int_\mathcal{A}f(x)\,\mu(dx)\tag{1}\label{1}
\end{equation}
We say that $f$ is the &lt;em&gt;density&lt;/em&gt; of the measure $v$ w.r.t $\mu$.&lt;/li&gt;
      &lt;li&gt;If $v,\mu$ are measures for which the equation \eqref{1} holds for every $\mathcal{A}\subset\mathcal{X}$, we say that $v$ has a density $f$ w.r.t $\mu$. This implies two useful results:
        &lt;ul&gt;
          &lt;li&gt;$\mu(\mathcal{A})=0$ implies $v(\mathcal{A})=0$.&lt;/li&gt;
          &lt;li&gt;$v(dx)=f(x)\,\mu(dx)$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-types&quot;&gt;Other types of Measures&lt;/h2&gt;
&lt;p&gt;Suppose that $\mu$ is a measure on $\mathcal{X}$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If $\mu(\mathcal{X})=\infty$, we say that $\mu$ is an &lt;em&gt;infinite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;\infty)$, we say that $\mu$ is a &lt;em&gt;finite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;1)$, we say that $\mu$ is a &lt;em&gt;probability measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If there exists a countable set $\mathcal{S}$ such that $\mu(\mathcal{X}-\mathcal{S})=0$, we say that $\mu$ is a &lt;em&gt;discrete measure&lt;/em&gt;. Equivalently, $\mu$ has a density w.r.t &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{S}$.&lt;/li&gt;
  &lt;li&gt;If $\mu$ has a density w.r.t Lebesgue measure, we say that $\mu$ is a &lt;em&gt;continuous measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu$ is neither &lt;em&gt;continuous&lt;/em&gt; nor &lt;em&gt;discrete&lt;/em&gt;, we say that $\mu$ is a &lt;em&gt;mixed measure&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Literally, this post is mainly written from a source that I’ve lost the reference :(. Hope that I can update this line soon.&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://mathworld.wolfram.com/LebesgueMeasure.html&quot;&gt;Lebesgue Measure&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://www.countbayesie.com/blog/2015/8/17/a-very-brief-and-non-mathematical-introduction-to-measure-theory-for-probability&quot;&gt;Measure Theory for Probability: A Very Brief Introduction&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-resources&quot;&gt;Other Resources&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cyW5z-M2yzw&quot;&gt;Music and Measure Theory - 3Blue1Brown&lt;/a&gt; - this is one of my favourite Youtube channels.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If $\mathcal{A_1},\mathcal{A_2}$ were not disjoint, we could define $\mathcal{B_1}=\mathcal{A_1}-\mathcal{A_2}$, $\mathcal{B_2}=\mathcal{A_2}-\mathcal{A_1}$, and $\mathcal{B_3}=\mathcal{A_1}\cap\mathcal{A_2}$. Then the function is equal to \(c_1𝟙_\mathcal{B_1}+c_2𝟙_\mathcal{B_2}+(c_1+c_2)𝟙_\mathcal{B_3}\). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="random-stuffs" /><summary type="html">When talking about measure, you might associate it with the idea of length, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with area, or even three dimensions with volume.</summary></entry><entry><title type="html">Markov Decision Processes, Bellman equations</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html" rel="alternate" type="text/html" title="Markov Decision Processes, Bellman equations" /><published>2021-06-27T08:00:00+07:00</published><updated>2021-06-27T08:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html">&lt;blockquote&gt;
  &lt;p&gt;You may have known or heard vaguely about a computer program called &lt;strong&gt;AlphaGo&lt;/strong&gt; - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called &lt;strong&gt;self-play&lt;/strong&gt; against its other instances, with &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-rl&quot;&gt;What is Reinforcement Learning?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mdp&quot;&gt;Markov Decision Processes (MDPs)&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#return&quot;&gt;Return&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#policy&quot;&gt;Policy&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#value-function&quot;&gt;Value Function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#opt-policy-opt-value-func&quot;&gt;Optimal Policy and Optimal Value Function&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bellman-equations&quot;&gt;Bellman Equations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#bellman-backup-diagram&quot;&gt;Bellman Backup Diagram&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bellman-optimality-equations&quot;&gt;Bellman Optimality Equations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backup-vq&quot;&gt;Backup diagram for $v_*$ and $q_*$&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;what-is-rl&quot;&gt;What is Reinforcement Learning?&lt;/h4&gt;
&lt;p&gt;Say, there is an unknown &lt;strong&gt;environment&lt;/strong&gt; that we’re trying to put an &lt;strong&gt;agent&lt;/strong&gt; on. By interacting with the &lt;strong&gt;agent&lt;/strong&gt; through taking &lt;strong&gt;actions&lt;/strong&gt; that gives rise to &lt;strong&gt;rewards&lt;/strong&gt; continually, the &lt;strong&gt;agent&lt;/strong&gt; learns a &lt;strong&gt;policy&lt;/strong&gt; that maximize the cumulative &lt;strong&gt;rewards&lt;/strong&gt;.&lt;br /&gt;
&lt;strong&gt;Reinforcement Learning (RL)&lt;/strong&gt;, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called &lt;strong&gt;policy&lt;/strong&gt;) for the &lt;strong&gt;agent&lt;/strong&gt; from experimental trials and relative simple feedback received. With the optimal &lt;strong&gt;policy&lt;/strong&gt;, the &lt;strong&gt;agent&lt;/strong&gt; is capable to actively adapt to the environment to maximize future &lt;strong&gt;rewards&lt;/strong&gt;.
&lt;img src=&quot;/assets/images/robot.png&quot; alt=&quot;RL&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;mdp&quot;&gt;Markov Decision Processes (MDPs)&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Markov decision processes (MDPs)&lt;/strong&gt; formally describe an environment for &lt;strong&gt;RL&lt;/strong&gt;. And almost all &lt;strong&gt;RL&lt;/strong&gt; problems can be formalised as &lt;strong&gt;MDPs&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition (MDP)&lt;/strong&gt;&lt;br /&gt;
A &lt;strong&gt;Markov Decision Process&lt;/strong&gt; is a tuple $⟨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma⟩$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{S}$ is a set of states called &lt;em&gt;state space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal{A}$ is a set of actions called &lt;em&gt;action space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal{P}$ is a state transition probability matrix&lt;br /&gt;
  \(\mathcal{P}^a_{ss&apos;}=P(S_{t+1}=s&apos;|S_t=s,A_t=a)\)&lt;/li&gt;
  &lt;li&gt;$\mathcal{R}$ is a reward function&lt;br /&gt;
  \(\mathcal{R}^a_s=\mathbb{E}\left[R_{t+1}|S_t=s,A_t=a\right]\)&lt;/li&gt;
  &lt;li&gt;$\gamma\in[0, 1]$ is a discount factor for future reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;MDP&lt;/strong&gt; is an extension of &lt;a href=&quot;/random-stuffs/probability-statistics/2021/06/19/markov-chain.html&quot;&gt;Markov chain&lt;/a&gt;. If only one action exists for each state, and all rewards are the same, an &lt;strong&gt;MDP&lt;/strong&gt; reduces to a &lt;em&gt;Markov chain&lt;/em&gt;. All states in &lt;strong&gt;MDP&lt;/strong&gt; has &lt;a href=&quot;/random-stuffs/probability-statistics/2021/06/19/markov-chain.html#markov-property&quot;&gt;Markov property&lt;/a&gt;, referring to the fact that the current state captures all relevant information from the history.
\begin{equation}
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\dots,S_t)
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;return&quot;&gt;Return&lt;/h5&gt;
&lt;p&gt;In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the &lt;strong&gt;expected return&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Return&lt;/em&gt;)&lt;br /&gt;
The &lt;strong&gt;return&lt;/strong&gt; $G_t$ is the total discounted reward from t
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called &lt;em&gt;discount rate&lt;/em&gt; (or &lt;em&gt;discount factor&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;discount rate&lt;/em&gt; $\gamma$ determines the present value of future rewards: a reward received
k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.&lt;/p&gt;

&lt;h5 id=&quot;policy&quot;&gt;Policy&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;, which is denoted as $\pi$, is the behaviour function of the agent. $\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either &lt;em&gt;deterministic&lt;/em&gt; or &lt;em&gt;stochastic&lt;/em&gt;.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Deterministic policy&lt;/em&gt;:	$\quad\pi(s)=a$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Stochastic policy&lt;/em&gt;: $\quad\pi(a|s)=P(A_t=a|S_t=s)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;value-function&quot;&gt;Value Function&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Value function&lt;/strong&gt; measures &lt;em&gt;how good&lt;/em&gt; a particular state is (or &lt;em&gt;how good&lt;/em&gt; it is to perform a given action in a given state).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;state-value function&lt;/em&gt;)&lt;br /&gt;
The &lt;strong&gt;state-value function&lt;/strong&gt; of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;action-value function&lt;/em&gt;)&lt;br /&gt;
Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
\end{equation}&lt;/p&gt;

&lt;p&gt;Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}&lt;/p&gt;

&lt;h5 id=&quot;opt-policy-opt-value-func&quot;&gt;Optimal Policy and Optimal Value Function&lt;/h5&gt;
&lt;p&gt;For finite MDPs (finite state and action space), we can precisely define an &lt;strong&gt;optimal policy&lt;/strong&gt;. &lt;em&gt;Value functions&lt;/em&gt; define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words,
\begin{equation}
\pi\geq\pi’\iff v_\pi(s)\geq v_{\pi’} \forall s\in\mathcal{S}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Optimal policy&lt;/em&gt;)&lt;br /&gt;
For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal to all other policies,
\begin{equation}
\pi_*\geq\pi,\forall\pi
\end{equation}&lt;/p&gt;

&lt;p&gt;The proof of the above theorem is gonna be provided in another &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html&quot;&gt;post&lt;/a&gt; since we need some additional tools to do that.&lt;/p&gt;

&lt;p&gt;There may be more than one &lt;strong&gt;optimal policy&lt;/strong&gt;, they share the same &lt;em&gt;state-value function&lt;/em&gt;, called &lt;strong&gt;optimal state-value function&lt;/strong&gt; though.
\begin{equation}
v_*(s)=\max_{\pi}v_\pi(s)
\end{equation}
&lt;strong&gt;Optimal policies&lt;/strong&gt; also share the same &lt;em&gt;action-value function&lt;/em&gt;, call &lt;strong&gt;optimal action-value function&lt;/strong&gt;
\begin{equation}
q_*(s,a)=\max_{\pi}q_\pi(s,a)
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;bellman-equations&quot;&gt;Bellman Equations&lt;/h4&gt;
&lt;p&gt;A fundamental property of &lt;em&gt;value functions&lt;/em&gt; used throughout RL is that they satisfy recursive relationships
\begin{align}
v_\pi(s)&amp;amp;\doteq \mathbb{E}_\pi[G_t|S_t=s] \\&amp;amp;=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s] \\&amp;amp;=\sum_{s’,r,g’,a}p(s’,r,g’,a|s)(r+\gamma g’) \\&amp;amp;=\sum_{a}p(a|s)\sum_{s’,r,g’}p(s’,r,g’|a,s)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r,g’}p(s’,r|a,s)p(g’|s’,r,a,s)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\sum_{g’}p(g’|s’)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma\sum_{g’}p(g’|s’)g’\right] \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma v_\pi(s’)\right],
\end{align}
where $p(s’,r|s,a)=P(S_{t+1}=s’,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the &lt;em&gt;Bellman equation for&lt;/em&gt; $v_\pi(s)$. It expresses a relationship between the value state $s$, $v_\pi(s)$ and the values of its successor states $s’$, $v_\pi(s’)$.&lt;/p&gt;

&lt;p&gt;Similarly, we define the &lt;em&gt;Bellman equation for&lt;/em&gt; $q_\pi(s,a)$
\begin{align}
q_\pi(s,a)&amp;amp;\doteq\mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\&amp;amp;=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\sum_{a’}\pi(a’|s’)q_\pi(s’,a’)\right]
\end{align}&lt;/p&gt;

&lt;h5 id=&quot;bellman-backup-diagram&quot;&gt;Bellman Backup Diagram&lt;/h5&gt;
&lt;p&gt;Backup diagram of &lt;em&gt;state-value function&lt;/em&gt; and &lt;em&gt;action-value function&lt;/em&gt; respectively&lt;/p&gt;
&lt;p float=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/state.png&quot; width=&quot;350&quot; /&gt;
  &lt;img src=&quot;/assets/images/action.png&quot; width=&quot;350&quot; /&gt; 
&lt;/p&gt;

&lt;h5 id=&quot;bellman-optimality-equations&quot;&gt;Bellman Optimality Equations&lt;/h5&gt;
&lt;p&gt;Since $v_*$ is the value function for a policy, it must satisfy the &lt;em&gt;Bellman equation for state-values&lt;/em&gt;. Moreover, it is also the optimal value function, then we have
\begin{align}
v_*(s)&amp;amp;=\max_{a\in\mathcal{A(s)}}q_{\pi_*}(s,a) \\&amp;amp;=\max_{a}\mathbb{E}_{\pi_*}[G_t|S_t=s,A_t=a] \\&amp;amp;=\max_{a}\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;amp;=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\&amp;amp;=\max_{a}\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_*(s’)]
\end{align}
The last two equations are two forms of the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $v_*$. Similarly, we have the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $q_*$
\begin{align}
q_*(s,a)&amp;amp;=\mathbb{E}\left[R_{t+1}+\gamma\max_{a’}q_*(S_{t+1},a’)|S_t=s,A_t=a\right] \\&amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\max_{a’}q_*(s’,a’)\right]
\end{align}&lt;/p&gt;

&lt;h5 id=&quot;backup-vq&quot;&gt;Backup diagram for $v_*$ and $q_*$&lt;/h5&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/opt.png&quot; alt=&quot;backup diagram for optimal value func&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] David Silver. &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;UCL course on RL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;A (Long) Peek into Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] &lt;a href=&quot;https://deepmind.com/research/case-studies/alphago-the-story-so-far&quot;&gt;AlphaGo&lt;/a&gt;&lt;/p&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="my-rl" /><summary type="html">You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.</summary></entry><entry><title type="html">Markov Chain</title><link href="http://localhost:4000/random-stuffs/probability-statistics/2021/06/19/markov-chain.html" rel="alternate" type="text/html" title="Markov Chain" /><published>2021-06-19T22:27:00+07:00</published><updated>2021-06-19T22:27:00+07:00</updated><id>http://localhost:4000/random-stuffs/probability-statistics/2021/06/19/markov-chain</id><content type="html" xml:base="http://localhost:4000/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">&lt;blockquote&gt;
  &lt;p&gt;Since I have no idea how to begin with this post, why not just dive straight into details :P&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#markov-property&quot;&gt;Markov Property&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transition-matrix&quot;&gt;Transition Matrix&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nstep-trans-prob&quot;&gt;n-step Transition Probability&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#marginal-dist-xn&quot;&gt;Marginal Distribution of $X_n$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#properties&quot;&gt;Properties&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#stationary-distribution&quot;&gt;Stationary Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reversibility&quot;&gt;Reversibility&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exp-app&quot;&gt;Examples and Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Markov chain&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; is a stochastic process in which the random variables follow a special property called &lt;em&gt;Markov&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;markov-property&quot;&gt;Markov Property&lt;/h4&gt;
&lt;p&gt;A sequence of random variables $X_0, X_1, X_2, \dots$ taking values in the &lt;em&gt;state space&lt;/em&gt; $S=${$1, 2,\dots, M$}. For all $n\geq0$,
\begin{equation}
P(X_{n+1}=j|X_n=i)=P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},X_{n-2}=i_{n-2},\dots,X_0=i_0)
\end{equation}
In other words, knowledge of the preceding state is all we need to determine the probability distribution of the current state&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h4 id=&quot;transition-matrix&quot;&gt;Transition Matrix&lt;/h4&gt;
&lt;p&gt;The quantity $P(X_{n+1}=j|X_n=i)$ is &lt;em&gt;transition probability&lt;/em&gt; from state $i$ to $j$.&lt;br /&gt;
If we denote that $q_{ij}=P(X_{n+1}=j|X_n=i)$ and let $Q=(q_{ij})$, which is a $M\times M$ matrix, there we have the &lt;em&gt;transition matrix&lt;/em&gt; $Q$ of the chain.&lt;br /&gt;
Therefore, each row of $Q$ is a conditional probability mass function (PMF) of $X_{n+1}$ given $X_n$. And hence, sum of its entries is 1.&lt;/p&gt;

&lt;h5 id=&quot;nstep-trans-prob&quot;&gt;n-step Transition Probability&lt;/h5&gt;
&lt;p&gt;The n-step &lt;em&gt;transition probability&lt;/em&gt; from $i$ to $j$ is the probability of being at $i$ and $n$ steps later being at $j$, and be denoted as $q_{ij}^{(n)}$,
\begin{equation}
q_{ij}^{(n)}=P(X_n=j|X_0=i)
\end{equation}
We have that
\begin{equation}
q_{ij}^{(2)}=\sum_{k}^{}q_{ik}q_{kj}
\end{equation}
since it has to go through an intermediary step $k$ to reach $j$ in 2 steps from $i$. It’s easily seen that the right hand side is $Q_{ij}^2$. And by induction, we have that:
\begin{equation}
q_{ij}^{(n)}=Q_{ij}^{n}
\end{equation}
$Q^n$ is also called the &lt;em&gt;n-step transition matrix&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&quot;marginal-dist-xn&quot;&gt;Marginal Distribution of $X_n$&lt;/h5&gt;
&lt;p&gt;Let $t=(t_1,\dots,t_M)^T$, where $t_i=P(X_0=i)$. By the law of total probability (LOTP), we have that:
\begin{align}
P(X_n=j)&amp;amp;=\sum_{i=1}^{M}P(X_0=i)P(X_n=j|X_0=i) \\&amp;amp;=\sum_{i=1}^{M}t_iq_{ij}^{(n)}
\end{align}
or the marginal distribution of $X_n$ is given by $tQ^n$.&lt;/p&gt;

&lt;h4 id=&quot;properties&quot;&gt;Properties&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;State $i$ of a Markov chain is defined as &lt;em&gt;recurrent&lt;/em&gt; or &lt;em&gt;transient&lt;/em&gt; depending upon whether or not the Markov chain will eventually return to it. Starting with &lt;em&gt;recurrent&lt;/em&gt; state i, the chain will return to it with the probability of 1. Otherwise, it is &lt;em&gt;transient&lt;/em&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: Number of returns to &lt;em&gt;transient&lt;/em&gt; state is distributed by &lt;em&gt;Geom($p$)&lt;/em&gt;, with $p&amp;gt;0$ is the probability of never returning to $i$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A Markov chain is defined as &lt;em&gt;irreducible&lt;/em&gt; if there exists a chain of steps between any $i,j$ that has positive probability. That is for any $i,j$, there is some $n&amp;gt;0,\in\mathbb{N}$ such that $Q^n_{ij}&amp;gt;0$. If not &lt;em&gt;irreducible&lt;/em&gt;, it’s called &lt;em&gt;reducible&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: &lt;em&gt;Irreducible&lt;/em&gt; implies all states &lt;em&gt;recurrent&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A state $i$ has &lt;em&gt;period&lt;/em&gt; $k&amp;gt;0$ if $k$ is the greatest common divisor (gcd) of the possible numbers of steps it can take to return to $i$ when starting at $i$.
And thus, $k=gcd(n)$ such that $Q^n_{ii}&amp;gt;0$. $i$ is called &lt;em&gt;aperiodic&lt;/em&gt; if $k_i=1$, and &lt;em&gt;periodic&lt;/em&gt; otherwise. The chain itself is called &lt;em&gt;aperiodic&lt;/em&gt; if all its states are &lt;em&gt;aperiodic&lt;/em&gt;, and &lt;em&gt;periodic&lt;/em&gt; otherwise.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;stationary-distribution&quot;&gt;Stationary Distribution&lt;/h4&gt;
&lt;p&gt;A vector $s=(s_1,\dots,s_M)^T$ such that $s_i\geq0$ and $\sum_{i}s_i=1$ is a &lt;em&gt;stationary distribution&lt;/em&gt; for a Markov chain if
\begin{equation}
\sum_{i}s_iq_{ij}=s_j
\end{equation}
for all $j$, or equivalently $sQ=s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Existence and uniqueness of stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Any &lt;em&gt;irreducible&lt;/em&gt; Markov chain has a unique &lt;em&gt;stationary distribution&lt;/em&gt;. In this distribution, every state has positive probability.&lt;/p&gt;

&lt;p&gt;The theorem is a consequence of a result from &lt;em&gt;Perron-Frobenius theorem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Convergence to stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be a Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$ and &lt;em&gt;transition matrix&lt;/em&gt; $Q$, such that some power $Q^m$ has all entries positive (or in the other words, the chain is &lt;em&gt;irreducible&lt;/em&gt; and &lt;em&gt;aperiodic&lt;/em&gt;). Then $P(X_n=i)$ converges to $s_i$ as $n\rightarrow\infty$ (or $Q^n$ converges to a matrix in which each row is $s$).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Expected time to run&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be an &lt;em&gt;irreducible&lt;/em&gt; Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$. Let $r_i$ be the expected time it takes the chain to return to $i$, given that it starts at $i$. Then $s_i=1/r_i$&lt;/p&gt;

&lt;h4 id=&quot;reversibility&quot;&gt;Reversibility&lt;/h4&gt;
&lt;p&gt;Let $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain. Suppose there is an $s=(s_1,\dots,s_M)^T$ with $s_i\geq0,\sum_{i}s_i=1$, such that
\begin{equation}
s_iq_{ij}=s_jq_{ji}
\end{equation}
for all states $i,j$. This equation is called &lt;em&gt;reversibility&lt;/em&gt; or &lt;em&gt;detailed balance&lt;/em&gt; condition. And if the condition holds, we say that the chain is &lt;em&gt;reversible&lt;/em&gt; w.r.t $s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt; (&lt;em&gt;Reversible implies stationary&lt;/em&gt;)&lt;br /&gt;
    Suppose that $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain that is &lt;em&gt;reversible&lt;/em&gt; w.r.t to an $s=(s_1,\dots,s_M)^T$ with with $s_i\geq0,\sum_{i}s_i=1$. Then $s$ is a &lt;em&gt;stationary distribution&lt;/em&gt; of the chain. (&lt;em&gt;proof&lt;/em&gt;:$\sum_{j}s_jq_{ji}=\sum_{j}s_iq_{ij}=s_i\sum_{j}q_{ij}=s_i$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt;&lt;br /&gt;
    If each column of $Q$ sum to 1, then the &lt;em&gt;uniform distribution&lt;/em&gt; over all states $(1/M,\dots,1/M)$, is a &lt;em&gt;stationary distribution&lt;/em&gt;. (This kind of matrix is called &lt;em&gt;doubly stochastic matrix&lt;/em&gt;).&lt;/p&gt;

&lt;h4 id=&quot;exp-app&quot;&gt;Examples and Applications&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;&lt;em&gt;Finite-state machines&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;&lt;em&gt;random walks&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Diced board games such as Ludo, Monopoly,…&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/PageRank&quot;&gt;&lt;em&gt;Google PageRank&lt;/em&gt;&lt;/a&gt; - the heart of Google search&lt;/li&gt;
  &lt;li&gt;Markov Decision Process (MDP), which is gonna be the content of next &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;post&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;And various other applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Joseph K. Blitzstein &amp;amp; Jessica Hwang. &lt;a href=&quot;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573&quot;&gt;Introduction to Probability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://brilliant.org/wiki/markov-chains/&quot;&gt;Brillant’s Markov chain&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://en.wikipedia.org/wiki/Perron–Frobenius_theorem&quot;&gt;Perron-Frobenius theorem&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is more like intuitive and less formal definition of Markov chain, we will have a more concrete definition with the help of &lt;em&gt;Measure theory&lt;/em&gt; after the post about it. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The Markov chain here is &lt;em&gt;time-homogeneous&lt;/em&gt; Markov chain, in which the probability of any state transition is independent of time. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Well, it only matters where you are, not where you’ve been. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="probability-statistics" /><category term="mathematics" /><category term="probability-statistics" /><summary type="html">Since I have no idea how to begin with this post, why not just dive straight into details :P</summary></entry><entry><title type="html">My very first post</title><link href="http://localhost:4000/mathematics/linear-algebra/2021/06/05/fibonacci-generator.html" rel="alternate" type="text/html" title="My very first post" /><published>2021-06-05T17:00:00+07:00</published><updated>2021-06-05T17:00:00+07:00</updated><id>http://localhost:4000/mathematics/linear-algebra/2021/06/05/fibonacci-generator</id><content type="html" xml:base="http://localhost:4000/mathematics/linear-algebra/2021/06/05/fibonacci-generator.html">&lt;blockquote&gt;
  &lt;p&gt;Enjoy my index-zero-ed post while staying tuned for next ones!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fibonacci&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	generate i-th number of the Fibonacci sequence, python code obvs :p
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why did numbers \(\frac{1+\sqrt{5}}{2}\) and \(\frac{1-\sqrt{5}}{2}\) come out of nowhere?&lt;/p&gt;

&lt;p&gt;In fact, these two numbers are eigenvalues of matrix \(A=\left(\begin{smallmatrix}1 &amp;amp; 1\\1 &amp;amp; 0\end{smallmatrix}\right)\), which is retrieved from
\begin{equation}
u_{k+1}=Au_k,
\end{equation}
where \(u_k=\left(\begin{smallmatrix}F_{k+1}\\F_k\end{smallmatrix}\right)\).
And thus, \(u_k=A^k u_0\).&lt;/p&gt;

&lt;p&gt;Then, the thing is, how can we compute \(A^k\) quickly? This is where diagonalizing plays its role. Diagonalizing produces a factorization:
\begin{equation}
A=S\Lambda S^{-1},
\end{equation}
where \(S=\left(\begin{smallmatrix}x_1 &amp;amp; \dots &amp;amp; x_n\end{smallmatrix}\right)\) is eigenvector matrix, \(\Lambda=\left(\begin{smallmatrix}\lambda_1&amp;amp;&amp;amp;\\&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;\lambda_n\end{smallmatrix}\right)\) is a diagonal matrix established from eigenvalues of \(A\).&lt;/p&gt;

&lt;p&gt;When taking the power of \(A\),
\begin{equation}
A^k u_0=(S\Lambda S^{-1})\dots(S\Lambda S^{-1})u_0=S\Lambda^k S^{-1} u_0,
\end{equation}
writing \(u_0\) as a combination \(c_1x_1+\dots+c_nx_n\) of the eigenvectors, we have that \(c=S^{-1}u_0\). Hence:
\begin{equation}
u_k=A^ku_0=c_1{\lambda_1}^kx_1+\dots+c_n{\lambda_n}^kx_n
\end{equation}
&lt;em&gt;Fact&lt;/em&gt;: The \(\frac{1+\sqrt{5}}{2}\approx 1.618\) is so-called “&lt;strong&gt;golden ratio&lt;/strong&gt;”. And &lt;em&gt;for some reason a rectangle with sides 1.618 and 1 looks especially graceful&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Gilbert Strang. &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] MIT 18.06. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/&quot;&gt;Linear Algebrea&lt;/a&gt;&lt;/p&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="linear-algebra" /><category term="mathematics" /><category term="linear-algebra" /><category term="random-stuffs" /><summary type="html">Enjoy my index-zero-ed post while staying tuned for next ones!</summary></entry></feed>