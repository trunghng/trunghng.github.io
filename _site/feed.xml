<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-30T16:20:23+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Trust Region Policy Optimization</title><link href="http://localhost:4000/2022/11/23/trpo.html" rel="alternate" type="text/html" title="Trust Region Policy Optimization" /><published>2022-11-23T15:26:00+07:00</published><updated>2022-11-23T15:26:00+07:00</updated><id>http://localhost:4000/2022/11/23/trpo</id><content type="html" xml:base="http://localhost:4000/2022/11/23/trpo.html"><![CDATA[<blockquote>
  <p>TRPO.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#mdp">Markov Decision Processes</a></li>
      <li><a href="#coupling-tvd">Coupling &amp; Total variation distance</a></li>
    </ul>
  </li>
  <li><a href="#policy-imp">Policy Improvement</a></li>
  <li><a href="#param-policy-opt">Parameterized Policy Optimization by Trust Region</a></li>
  <li><a href="#sampled-bsd-est">Sampled-based estimation</a>
    <ul>
      <li><a href="#sgl">Single path</a></li>
      <li><a href="#vine">Vine</a></li>
    </ul>
  </li>
  <li><a href="#fin-alg">Final algorithm</a></li>
  <li><a href="#ppo">Proximal Policy Optimization</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>We begin by recalling definition of MDPs, coupling and total variation distance.</p>

<h3 id="mdp">Markov Decision Processes</h3>
<p>An infinite-horizon discounted <strong>Markov Decision Process</strong> (<strong>MDP</strong>) is defined as the tuple $(\mathcal{S},\mathcal{A},P,r,\rho_0,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a finite set of states, or <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a finite set of actions, or <strong>action space</strong>.</li>
  <li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$ is the <strong>transition probability distribution</strong>, i.e. $P(s,a,s’)=P(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>.</li>
  <li>$\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.</li>
  <li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li>
</ul>

<p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$. Here, we consider the stochastic policy only.</p>

<p>We continue by letting $\eta(\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\pi$ thereafter
\begin{equation}
\eta(\pi)=\mathbb{E}_{s_0,a_0,\ldots}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right],
\end{equation}
where
\begin{equation}
s_0\sim\rho_0(s_0),\hspace{1cm}a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)
\end{equation}
For a policy $\pi$, the <strong>state value function</strong>, denoted as $V_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$, and the <strong>action value function</strong>, referred as $Q_\pi$, of a state-action pair $(s,a)\in\mathcal{S}\times\mathcal{A}$ specifies how good it is to take action $a$ at state $s$. Specifically, these values are defined by the expected return, as
\begin{align}
V_\pi(s_t)&amp;=\mathbb{E}_{a_t,s_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right], \\ Q_\pi(s_t,a_t)&amp;=\mathbb{E}_{s_{t+1},a_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right],
\end{align}
where
\begin{equation}
a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)\hspace{1cm}t\geq 0
\end{equation}
Along with these value functions, we will also define the <strong>advantage function</strong> for $\pi$, denoted $A_\pi$, given as
\begin{equation}
A_\pi(s_t,a_t)=Q_\pi(s_t,a_t)-V_\pi(s_t)
\end{equation}</p>

<h3 id="coupling-tvd">Coupling &amp; Total variation distance</h3>
<p>Consider two probability measures $\mu$ and $\nu$ on a probability space $(\Omega,\mathcal{F},P)$. One refers a <strong>coupling</strong> of $\mu$ and $\nu$ as a pair of random variables $(X,Y)$ such that the marginal distribution of $X$ and $Y$ are respectively $\mu$ and $\nu$.</p>

<p>Specifically, if $p$ is a joint distribution of $X,Y$ on $\Omega$, then it implies that
\begin{align}
\sum_{y\in\Omega}p(x,y)&amp;=\sum_{y\in\Omega}P(X=x,Y=y)=P(X=x)=\mu(x) \\ \sum_{x\in\Omega}p(x,y)&amp;=\sum_{x\in\Omega}P(X=x,Y=y)=P(Y=y)=\nu(y)
\end{align}
For probability distributions $\mu$ and $\nu$ on $\Omega$ as above, the <strong>total variation distance</strong> between $\mu$ and $\nu$, denoted $\big\Vert\mu-\nu\big\Vert_\text{TV}$, is defined by
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}\doteq\sup_{A\subset\Omega}\big\vert\mu(A)-\nu(A)\big\vert
\end{equation}
<strong>Proposition 1</strong><br />
Let $\mu$ and $\nu$ be probability distributions on $\Omega$, we then have
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
<strong>Proof</strong><br />
Let $B=\{x:\mu(x)\geq\nu(x)\}$ and let $A\subset\Omega$. We have
\begin{align}
\mu(A)-\nu(A)&amp;=\mu(A\cap B)+\mu(A\cap B^c)-\nu(A\cap B)-\nu(A\cap B^c) \\ &amp;\leq\mu(A\cap B)-\nu(A\cap B) \\ &amp;\leq\mu(B)-\nu(B)
\end{align}
Analogously, we also have
\begin{equation}
\nu(A)-\mu(A)\leq\nu(B^c)-\mu(B^c)
\end{equation}
Hence, combining these results gives us
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\frac{1}{2}\left(\mu(B)-\nu(B)+\nu(B^c)-\mu(B^c)\right)=\frac{1}{2}\sum_{x\in\Omega}\big\vert\mu(x)-\nu(x)\big\vert
\end{equation}
This proof also implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sum_{x\in\Omega;\,\mu(x)\geq\nu(x)}\mu(x)-\nu(x)
\end{equation}
<strong>Proposition 2</strong><br />
Let $\mu$ and $\nu$ be two probability measures defined in a probability space $\Omega$, we then have that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
<strong>Proof</strong><br />
For any $A\subset\Omega$ and for any coupling $(X,Y)$ of $\mu$ and $\nu$ we have
\begin{align}
\mu(A)-\nu(A)&amp;=P(X\in A)-P(Y\in A) \\ &amp;=P(X\in A,Y\notin A)+P(X\in A,Y\in A)-P(Y\in A) \\ &amp;\leq P(X\in A,Y\notin A) \\ &amp;\leq P(X\neq Y),
\end{align}
which implies that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=\sup_{A’\subset\Omega}\big\vert\mu(A’)-\nu(A’)\big\vert\leq P(X\neq Y)\leq\inf_{(X,Y)\text{ coupling of }\mu,\nu}P(X\neq Y)
\end{equation}
Thus, it suffices to construct a coupling $(X,Y)$ of $\mu$ and $\nu$ such that
\begin{equation}
\big\Vert\mu-\nu\big\Vert_\text{TV}=P(X\neq Y)
\end{equation}</p>

<h2 id="policy-imp">Policy improvement</h2>
<p>We begin by proving an identity that expresses the expected return $\eta(\tilde{\pi})$ of a policy $\tilde{\pi}$ in terms of the advantage over another policy $\pi$, accumulated over time steps.</p>

<p><strong>Lemma 3</strong><br />
Given two policies $\pi,\tilde{\pi}$, we have
\begin{equation}
\eta(\tilde{\pi})=\eta(\pi)+\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]\label{eq:pi.1}
\end{equation}
<strong>Proof</strong><br />
By definition of advantage function $A_\pi$ of policy $\pi$, we have
\begin{align}
\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t A_\pi(s_t,a_t)\right]&amp;=\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\left(Q_\pi(s_t,a_t)-V_\pi(s_t)\right)\right] \\  &amp;=\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t\big(r(s_t,a_t)+\gamma V_\pi(s_{t+1})-V_\pi(s_t)\big)\right] \\ &amp;=\mathbb{E}_{\tilde{\pi}}\left[-V_\pi(s_0)+\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\ &amp;=-\mathbb{E}_{s_0}\big[V_\pi(s_0)\big]+\mathbb{E}_{\tilde{\pi}}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\ &amp;=-\eta(\pi)+\eta(\tilde{\pi}),
\end{align}
where in the third step, since $\gamma\in(0,1)$ as $t\to\infty$, we have that $\gamma^t V_\pi(s_{t+1})\to 0$.</p>

<p>Let $\rho_\pi$ be the unnormalized discounted visitation frequencies for state $s$:
\begin{equation}
\rho_\pi(s)\doteq P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots
\end{equation}
where $s_0\sim\rho_0$ and the actions are chosen according to $\pi$. This allows us to rewrite \eqref{eq:pi.1} as
\begin{align}
\eta(\tilde{\pi})&amp;=\eta(\pi)+\sum_{t=0}^{\infty}\sum_{s}P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)\gamma^t A_\pi(s,a) \\ &amp;=\eta(\pi)+\sum_{s}\sum_{t=0}^{\infty}\gamma^t P(s_t=s\vert\tilde{\pi})\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a) \\ &amp;=\eta(\pi)+\sum_{s}\rho_\tilde{\pi}(s)\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.2}
\end{align}
This result implies that any policy update $\pi\to\tilde{\pi}$ that has a nonnegative expected advantage at every state $s$, i.e. $\sum_{a}\tilde{\pi}(a\vert s)A_\pi(s,a)\geq 0$, is guaranteed to make an improvement on $\eta$ (or unchanged in case the expected advantage take the value of zero for every $s$). By letting $\tilde{\pi}$ be the deterministic policy that
\begin{equation}
\tilde{\pi}(s)=\underset{a}{\text{argmax}}\,A_\pi(s,a),
\end{equation}
we obtain the <a href="/2021/07/25/dp-in-mdp.html#policy-improvement"><strong>policy improvement</strong></a> result used in <a href="/2021/07/25/dp-in-mdp.html#policy-iteration"><strong>policy iteration</strong></a>.</p>

<p>However, there are cases when \eqref{eq:pi.2} is difficult to be optimized, especially when the expected advantage is negative, i.e. $\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)$, due to estimation and approximation error in the approximate setting. We instead consider a local approximation to $\eta$:
\begin{equation}
L_\pi(\tilde{\pi})=\eta(\pi)+\sum_s\rho_\pi(s)\sum_a\tilde{\pi}(a\vert s)A_\pi(s,a)\label{eq:pi.5}
\end{equation}</p>

<p>If $\pi$ is a policy parameterized by $\theta$, in which $\pi_\theta(a\vert s)$ s differentiable w.r.t $\theta$, we then have for any parameter value $\theta_0$
\begin{align}
L_{\pi_{\theta_0}}(\pi_{\theta_0})&amp;=\eta(\pi_{\theta_0}) \\ \nabla_\theta L_{\pi_{\theta_0}}(\pi_\theta)\big\vert_{\theta=\theta_0}&amp;=\nabla_\theta\eta(\pi_\theta)\big\vert_{\theta=\theta_0},
\end{align}
which suggests that a sufficiently small step $\pi_{\theta_0}\to\tilde{\pi}$ that leads to an improvement on $L_{\pi_{\theta_\text{old}}}$ will also make an improvement on $\eta$.</p>

<p>To measure the improvement on updating $\pi_\text{old}\to\pi_\text{new}$, we choose the total variance distance metric, as defined above with an observation that each policy $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$ can be viewed as a distribution function defined on $\mathcal{S}\times\mathcal{A}$. Thus, those results and definitions mentioned above for probability measures $\mu$ and $\nu$ defined on $\Omega$ can also be applied to policies $\pi$ and $\tilde{\pi}$ specified on $\mathcal{S}\times\mathcal{A}$.</p>

<p>In addition, we need to define some notations:</p>
<ul id="number-list">
	<li>
		Let
		\begin{equation}
		\big\Vert\pi-\tilde{\pi}\big\Vert_{\text{TV}}^{\text{max}}\doteq\max_s\big\Vert\pi(\cdot\vert s)-\tilde{\pi}(\cdot\vert s)\big\Vert_\text{TV}
		\end{equation}
	</li>
	<li>
		A policy pair $(\pi,\tilde{\pi})$ is referred as <b>$\alpha$-coupled</b> if it defines a joint distribution $(a,\tilde{a})\vert s$ such that
		\begin{equation}
		P(a\neq\tilde{a}\vert s)\leq\alpha,\hspace{1cm}\forall s
		\end{equation}
		$\pi$ and $\tilde{\pi}$ will respectively denote the marginal distributions of $a$ and $\tilde{a}$.<br /><br />
		<b>Proposition 4</b><br />
		Let $(\pi,\tilde{\pi})$ be $\alpha$-coupled policy pair, for all $s$, we have
		\begin{equation}
		\big\vert\bar{A}(s)\big\vert\leq 2\alpha\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert,
		\end{equation}
		where $\bar{A}(s)$ is the expected advantage of $\tilde{\pi}$ over $\pi$ at state $s$, given as
		\begin{equation}
		\bar{A}(s)=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big]
		\end{equation}
		<b>Proof</b><br />
		By definition of the advantage function, it is easily noticed that $\mathbb{E}_{a\sim\pi}\big[A_\pi(s,a)\big]=0$, which lets us obtain
		\begin{align}
		\bar{A}(s)&amp;=\mathbb{E}_{\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})\big] \\ &amp;=\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big] \\ &amp;=P(a\neq\tilde{a}\vert s)\mathbb{E}_{a\sim\pi,\tilde{a}\sim\tilde{\pi}\vert a\neq\tilde{a}}\big[A_\pi(s,\tilde{a})-A_\pi(s,a)\big],
		\end{align}
		which by definition of $\alpha$-coupling implies that
		\begin{equation}
		\big\vert\bar{A}(s)\big\vert\leq\alpha\cdot 2\max_{s,\tilde{a}}\big\vert A_\pi(s,\tilde{a})\big\vert
		\end{equation}
	</li>
</ul>

<p><strong>Theorem 5</strong><br />
Let $\alpha=\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^\text{max}$. The following holds
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-\frac{4\epsilon\gamma}{(1-\gamma)^2}\alpha^2,
\end{equation}
where
\begin{equation}
\epsilon=\max_{s,a}\big\vert A_\pi(s,a)\big\vert
\end{equation}
<strong>Proof</strong></p>

<p>On the other hand, by <strong>Pinsker’s inequality</strong>, which bounds the total variation distance in terms of the <strong>Kullback-Leibler divergence</strong>, denoted $D_\text{KL}$, we have that
\begin{equation}
\big\Vert\pi-\tilde{\pi}\big\Vert_\text{TV}^2\leq\frac{1}{2}D_\text{KL}(\pi\Vert\tilde{\pi})\leq D_\text{KL}(\pi\Vert\tilde{\pi}),\label{eq:pi.3}
\end{equation}
since $D_\text{KL}(\cdot\Vert\cdot)\geq 0$. Thus, let
\begin{equation}
D_\text{KL}^\text{max}(\pi,\tilde{\pi})\doteq\max_s D_\text{KL}\big(\pi(\cdot\vert s)\Vert\tilde{\pi}(\cdot\vert s)\big),
\end{equation}
with the result \eqref{eq:pi.3} and by <strong>Theorem 5</strong>, we have
\begin{equation}
\eta(\tilde{\pi})\geq L_\pi(\tilde{\pi})-CD_\text{KL}^\text{max}(\pi,\tilde{\pi}),\label{eq:pi.4}
\end{equation}
where
\begin{equation}
C=\frac{4\epsilon\gamma}{(1-\gamma)^2}
\end{equation}
The policy improvement bound \eqref{eq:pi.4} allows us to specify a policy iteration, as given in the following pseudocode</p>
<figure>
	<img src="/assets/images/2022-11-23/policy-iteration-nondec-exp-return.png" alt="Non-decreasing expected return policy iteration" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>
<p>It is worth noticing that \eqref{eq:pi.4} allows the policy iteration above to guarantee to generating a sequence of non-decreasing expected returns
\begin{equation}
\eta(\pi_0)\leq\eta(\pi_1)\leq\eta(\pi_2)\leq\ldots
\end{equation}
To see this, let
\begin{equation}
M_i(\pi)\doteq L_{\pi_i}(\pi)-CD_\text{KL}^\text{max}(\pi_i,\pi),
\end{equation}
by \eqref{eq:pi.4}, we then have
\begin{equation}
\eta(\pi_{i+1})\geq M_i(\pi_{i+1}),
\end{equation}
which implies that
\begin{equation}
\eta(\pi_{i+1})-\eta(\pi_i)=\eta(\pi_{i+1})-M_i(\pi_i)\geq M_i(\pi_{i+1})-M_i(\pi_i)
\end{equation}</p>

<h2 id="param-policy-opt">Parameterized Policy Optimization by Trust Region</h2>
<p>We now consider the policy optimization problem in which the policy is parameterized by $\theta$.</p>

<p>We begin by simplifying notations. In particular, let $\eta(\theta)\doteq\eta(\pi_\theta)$, let $L_\theta(\tilde{\theta})\doteq L_{\pi_\theta}(\pi_\tilde{\theta})$ and $D_\text{KL}(\theta\Vert\tilde{\theta})\doteq D_\text{KL}(\pi_\theta\Vert\pi_\tilde{\theta})$, which allows us to represent
\begin{equation}
D_\text{KL}^\text{max}(\theta,\tilde{\theta})\doteq D_\text{KL}^\text{max}(\pi_\theta,\pi_\tilde{\theta})=\max_s D_\text{KL}\big(\pi_\theta(\cdot\vert s)\Vert\pi_\tilde{\theta}(\cdot\vert s)\big)
\end{equation}
Also let $\theta_\text{old}$ denote the previous policy parameters that we want to improve. Hence, by the previous section, we have
\begin{equation}
\eta(\theta)\geq L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta),
\end{equation}
where the equality holds at $\theta=\theta_\text{old}$. This means, we get a guaranteed improvement to the true objective function $\eta$ by solving the following optimization problem
\begin{equation}
\underset{\theta}{\text{maximize}}\,\,\big[L_{\theta_\text{old}}(\theta)-CD_\text{KL}^\text{max}(\theta_\text{old},\theta)\big]
\end{equation}
To speed up the algorithm, we make some robust modification. Specifically, we instead solve a <strong>trust region problem</strong>:
\begin{align}
\underset{\theta}{\text{maximize}}&amp;\,\,L_{\theta_\text{old}}(\theta)\nonumber \\ \text{s.t.}&amp;\,\,\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\leq\delta,\label{eq:ppo.1}
\end{align}
where $\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}$ is the average KL divergence, given as
\begin{equation}
\overline{D}_\text{KL}^{\rho_{\theta_\text{old}}}(\theta_\text{old},\theta)\doteq\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]
\end{equation}
Let us pay attention to our objective function, $L_{\theta_\text{old}}(\theta)$, for a while. By the definition of $L$, given in \eqref{eq:pi.5}, combined with using an <a href="/2022/05/25/likelihood-ratio-pg-is.html#likelihood-ratio-pg-is">importance sampling estimator</a>, we can rewrite the objective function of \eqref{eq:ppo.1} as 
\begin{align}
L_{\theta_\text{old}}(\theta)&amp;=\sum_s\rho_{\theta_\text{old}}(s)\sum_a\pi_\theta(a\vert s)A_{\theta_\text{old}}(s,a) \\ &amp;=\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]
\end{align}
where $A_{\theta_\text{old}}\doteq A_{\pi_{\theta_\text{old}}}$; and $q$ represents the sampling distribution. The trust region problem now is given as
\begin{align}
\underset{\theta}{\text{maximize}}&amp;\,\,\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\nonumber \\ \text{s.t.}&amp;\,\,\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]\leq\delta
\end{align}
which is thus equivalent to<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>
\begin{align}
\underset{\theta}{\text{maximize}}&amp;\,\,\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}Q_{\theta_\text{old}}(s,a)\right]\nonumber \\ \text{s.t.}&amp;\,\,\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\Big[D_\text{KL}\big(\pi_{\theta_\text{old}}(\cdot\vert s)\Vert\pi_\theta(\cdot\vert s)\big)\Big]\leq\delta\label{eq:ppo.2}
\end{align}</p>

<h2 id="sampled-bsd-est">Sampled-based estimation</h2>
<p>The objective and constraint functions of \eqref{eq:ppo.2} can be approximated using Monte Carlo simulation. Following are two possible sampling approaches to construct the estimated objective and constraint functions.</p>

<h3 id="sgl">Single path</h3>
<p>This sampling scheme has the following procedure</p>
<ul id="number-list">
	<li>
		Sample $s_0\sim\rho_0$ to get a set of $m$ start states $\mathcal{S}_0=\{s_0^{(1)},\ldots,s_0^{(m)}\}$.
	</li>
	<li>
		For each $s_0^{(i)}\in\mathcal{S}_0$, generate a trajectory $\tau^{(i)}=\big(s_0^{(i)},a_0^{(i)},s_1^{(i)},a_1^{(i)},\ldots,s_{T-1}^{(i)},a_{T-1}^{(i)},s_T^{(i)}\big)$ by rolling out the policy $\pi_{\theta_\text{old}}$ for $T$ steps. Thus $q(a^{(i)}\vert s^{(i)})=\pi_{\theta_\text{old}}(a^{(i)}\vert s^{(i)})$.
	</li>
	<li>
		At each state-action pair $(s_t^{(i)},a_t^{(i)})$, compute the action-value function $Q_{\theta_\text{old}}(s,a)$ by taking the discounted sum of future rewards along $\tau^{(i)}$.
	</li>
</ul>

<h3 id="vine">Vine</h3>
<p>This sampling approach follows the following process</p>
<ul id="number-list">
	<li>
		Sample $s_0\sim\rho_0$ and simulate the policy $\pi_{\theta_i}$ to generate $m$ trajectories.
	</li>
	<li>
		Choose a rollout set, which is a subset $s_1,\ldots,s_N$ of $N$ states along the trajectories.
	</li>
	<li>
		For each state $s_n$ with $1\leq n\leq N$, sample $K$ actions according to $a_{n,k}\sim q(\cdot\vert s_n)$, where $q(\cdot\vert s_n)$ includes the support of $\pi_{\theta_i}(\cdot\vert s_n)$.
	</li>
	<li>
		For each action $a_{n,k}$, estimate $\hat{Q}_{\theta_i}(s_n,a_{n,k})$ by performing a rollout starting from $s_n$ and taking action $a_{n,k}$
	</li>
	<li>
		Given the estimated action-value function, $\hat{Q}_{\theta_i}(s_n,a_{n,k})$, for each state-action pair $(s_n,a_{n,k})$, compute the estimator, $L_n(\theta)$, of $L_{\theta_\text{old}}$ at state $s_n$ as:
		<ul id="roman-list">
			<li>
				For small, finite action spaces, in which generating a rollout for every possible action from a given state is possible, thus
				\begin{equation}
					L_n(\theta)=\sum_{k=1}^{K}\pi_\theta(a_k\vert s_n)\hat{Q}(s_n,a_k),
				\end{equation}
				where $\mathcal{A}=\{a_1,\ldots,a_K\}$ is the action space.
			</li>
			<li>
				For large or continuous state spaces, use importance sampling
				\begin{equation}
				L_n(\theta)=\frac{\sum_{k=1}^{K}\frac{\pi_\theta(a_{n,k}\vert s_n)}{\pi_{\theta_\text{old}}(a_{n,k}\vert s_n)}\hat{Q}(s_n,a_{n,k})}{\sum_{k=1}^{K}\frac{\pi_\theta(a_{n,k}\vert s_n)}{\pi_{\theta_\text{old}}(a_{n,k}\vert s_n)}},
				\end{equation}
				assuming that $K$ actions $a_{n,1},\ldots,a_{n,K}$ are performed from state $s_n$.
			</li>
		</ul>
	</li>
	<li>
		Average over $s_n\sim\rho(\pi)$ to obtain an estimator for $L_{\theta_\text{old}}$, as well the policy gradient.
	</li>
</ul>

<h2 id="fin-alg">Final algorithm</h2>

<h2 id="ppo">Proximal Policy Optimization</h2>

<h2 id="references">References</h2>
<p>[1] John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, Pieter Abbeel. <a href="https://dl.acm.org/doi/10.5555/3045118.3045319">Trust Region Policy Optimization</a>. ICML’15, pp 1889–1897, 2015.</p>

<p>[2] David A. Levin, Yuval Peres, Elizabeth L. Wilmer. <a href="https://pages.uoregon.edu/dlevin/MARKOV/markovmixing.pdf">Markov chains and mixing times</a>. American Mathematical Society, 2009.</p>

<p>[3] Sham Kakade,  John Langford. <a href="https://dl.acm.org/doi/10.5555/645531.656005">Approximately optimal approximate reinforcement learning</a>. ICML’2, pp. 267–274, 2002.</p>

<p>[4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov. <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization Algorithms</a>. arXiv:1707.06347, 2017.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>To be more specific, by definition of the advantage, i.e. $A_{\theta_\text{old}}(s,a)=Q_{\theta_\text{old}}(s,a)-V_{\theta_\text{old}}(s)$, we have:
\begin{align}
&amp;\hspace{0.7cm}\mathbb{E}_{s\sim\rho_{\theta_\text{old}},a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}Q_{\theta_\text{old}}(s,a)\right]\nonumber \\ &amp;=\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)+V_{\theta_\text{old}}(s)\right]\right]\nonumber \\ &amp;=\mathbb{E}_{s\sim\rho_{\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]+\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[V_{\theta_\text{old}}(s)\sum_{a}\pi_\theta(a\vert s)\right]\nonumber \\ &amp;=\mathbb{E}_{s\sim\rho_{\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]+\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\big[V_{\theta_\text{old}}(s)\big]\nonumber \\ &amp;\underset{\max_\theta}{\propto}\mathbb{E}_{s\sim\rho_{\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]\nonumber \\ &amp;\underset{\max_\theta}{\propto}\frac{1}{1-\gamma}\mathbb{E}_{s\sim\rho_{\theta_\text{old}}}\left[\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\right]\nonumber \\ &amp;=\sum_s\rho_{\theta_\text{old}}(s)\mathbb{E}_{a\sim q}\left[\frac{\pi_\theta(a\vert s)}{q(a\vert s)}A_{\theta_\text{old}}(s,a)\right]\nonumber
\end{align}
where we have used the notation
\begin{equation}
\text{LHS}\underset{\max_\theta}{\propto}\text{RHS}\nonumber
\end{equation}
to denote that $\underset{\theta}{\text{maximize}}\,\,\text{LHS}$ is equivalent to $\underset{\theta}{\text{maximize}}\,\,\text{RHS}$. Also, the sixth step comes from definition of $\rho_\pi$, i.e. for $s_0\sim\rho_0$ and the actions are chosen according to $\pi$, we have
\begin{equation}
\rho_\pi(s)=P(s_0=s)+\gamma P(s_1=s)+\gamma^2 P(s_2=s)+\ldots,\nonumber
\end{equation}
which implies that by summing across all $s$, we obtain
\begin{align}
\sum_{s}\rho_\pi(s)&amp;=\sum_s P(s_0=s)+\gamma\sum_s P(s_1=s)+\gamma^2\sum_s P(s_2=s)+\ldots\nonumber \\ &amp;=1+\gamma+\gamma^2+\ldots\nonumber \\ &amp;=\frac{1}{1-\gamma}\nonumber
\end{align} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="deep-reinforcement-learning" /><category term="policy-gradient" /><category term="my-rl" /><summary type="html"><![CDATA[Trust Region Policy Optimization]]></summary></entry><entry><title type="html">Deep Q-learning</title><link href="http://localhost:4000/2022/11/18/deep-q-learning.html" rel="alternate" type="text/html" title="Deep Q-learning" /><published>2022-11-18T15:26:00+07:00</published><updated>2022-11-18T15:26:00+07:00</updated><id>http://localhost:4000/2022/11/18/deep-q-learning</id><content type="html" xml:base="http://localhost:4000/2022/11/18/deep-q-learning.html"><![CDATA[<blockquote>
  <p>DQN.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a>
    <ul>
      <li><a href="#lin-func-approx">Linear function approximation</a></li>
      <li><a href="#dqn">Deep Q-learning</a>
        <ul>
          <li><a href="#exp-replay">Experience replay</a></li>
          <li><a href="#target-net">Target network</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#imp-vars">Some improved variants</a>
    <ul>
      <li><a href="#double-dqn">Double deep Q-learning</a></li>
      <li><a href="#prior-rep">Prioritized replay</a></li>
      <li><a href="#duel-net">Dueling network</a></li>
      <li><a href="#rainbow">Rainbow</a></li>
      <li><a href="#c-dqn">C-DQN</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the note <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s’$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{t+1}(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_t(s’)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_t\}$, will eventually converge to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banach’s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_t(s)$ to $V_{t+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-11-18/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\sum_{a’}\pi(a’\vert s’)Q_\pi(s’,a’)\right] \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q^*(s’,a’)\right]\label{eq:qvi.2} \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{t+1}(s,a)=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q_t(s’,a’)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converge to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-11-18/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{t+1}(s,a)=\mathbb{E}_{s’\sim P(s’\vert s,a)}\left[R(s,a,s’)+\gamma\max_{a’}Q_t(s’,a’)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(s’\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \eqref{eq:ql.1} can be approximated by sampling, as</p>
<ul id="number-list">
	<li>
		At a state, taking (sampling) action $a$ (e.g. due to an $\varepsilon$-greedy policy), we get the next state:
		\begin{equation}
		s'\sim P(s'\vert s,a)
		\end{equation}
	</li>
	<li>Consider the old estimate $Q_t(s,a)$.</li>
	<li>
		Consider the new sample estimate (target):
		\begin{equation}
		Q_\text{target}=R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\label{eq:ql.2}
		\end{equation}
	</li>
	<li>
		Append the new estimate into a running average to iteratively update Q-values:
		\begin{align}
		Q_{t+1}(s,a)&amp;=(1-\alpha)Q_t(s,a)+\alpha Q_\text{target} \\ &amp;=(1-\alpha)Q_t(s,a)+\alpha\left[R(s,a,s')+\gamma\max_{a'}Q_t(s',a')\right]
		\end{align}
	</li>
</ul>

<p>This update rule is in form of a <strong>stochastic process</strong>, and thus, is <a href="#q-learning-td-convergence">guaranteed to converge</a> to the optimal $Q^*$, under the <a href="/2022/01/31/td-learning.html#stochastic-approx-condition">stochastic approximation conditions</a> for the learning rate $\alpha$.
\begin{equation}
\sum_{t=1}^{\infty}\alpha_t(s,a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{t=1}^{\infty}\alpha_t^2(s,a)&lt;\infty,\label{eq:ql.3}
\end{equation}
for all $(s,a)\in\mathcal{S}\times\mathcal{A}$.</p>

<p>The method is so called <strong>Q-learning</strong>, with pseudocode given below.</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>As a tabular method, Q-learning will work with a small and finite state-action pair space. However, for continuous environments, the exact solution might never be found. To overcome this, we have been instead trying to find an <a href="/2022/02/11/func-approx.html">approximated solution</a>.</p>

<p>In particular, we have tried to find an approximated action-value function $Q_\boldsymbol{\theta}(s,a)$, parameterized by a learnable vector $\boldsymbol{\theta}$, of the action-value function $Q(s,a)$, as
\begin{equation}
Q_\boldsymbol{\theta}(s,a)
\end{equation}
Then, we could have applied stochastic gradient descent (SGD) to repeatedly update $\boldsymbol{\theta}$ so as to minimize the loss function
\begin{equation}
L(\boldsymbol{\theta})=\mathbb{E}_{s,a\sim\mu(\cdot)}\Big[\big(Q(s,a)-Q_\boldsymbol{\theta}(s,a)\big)^2\Big]
\end{equation}
The resulting SGD update had the form
\begin{align}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t-\frac{1}{2}\alpha\nabla_\boldsymbol{\theta}\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]^2 \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[Q(s_t,a_t)-Q_\boldsymbol{\theta}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.1}
\end{align}
However, we could not perform the exact update \eqref{eq:nql.1} since the true value $Q(s_t,a_t)$ was unknown. Fortunately, we could instead replace it by $y_t$, which can be any approximation of $Q(s_t,a_t)$<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.2}
\end{equation}</p>

<h3 id="lin-func-approx">Linear function approximation</h3>
<p>Recall that, we have applied <a href="/2022/02/11/func-approx.html#lin-func-approx">linear methods</a> as our function approximators:
\begin{equation}
Q_\boldsymbol{\theta}(s,a)=\boldsymbol{\theta}^\text{T}\mathbf{f}(s,a),
\end{equation}
where $\mathbf{f}(s,a)$ represents the <strong>feature vector</strong>, (or <strong>basis functions</strong>) of the state-action pair $(s,a)$.
Linear function approximation allowed us to rewrite \eqref{eq:nql.2} in a simplified form
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[y_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\mathbf{f}(s_t,a_t)\label{eq:nql.3}
\end{equation}
The corresponding SGD method for Q-learning and Q-learning with linear function approximation are respectively given in form of
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t)\label{eq:nql.4}
\end{equation}
and
\begin{equation}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\left[R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\right]\mathbf{f}(s_t,a_t),\label{eq:nql.5}
\end{equation}
which both replace the $Q_\text{target}$ in \eqref{eq:ql.2} by the one parameterized by $\boldsymbol{\theta}$
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)
\end{equation}
However, in updating $\boldsymbol{\theta}_
{t+1}$, these methods both use the <strong>bootstrapping target</strong>:
\begin{equation}
R(s_t,a_t,s_{t+1})+\gamma\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’), 
\end{equation}
which depends on the current value $\boldsymbol{\theta}_t$, and thus will be biased. As a consequence, \eqref{eq:nql.4} does not guarantee to converge<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<p>Such methods are known as <strong>semi-gradient</strong> since they take into account the effect of changing the weight vector $\boldsymbol{\theta}_t$ on the estimate, but ignore its effect on the target.</p>

<h3 id="dqn">Deep Q-learning</h3>
<p>On the other hands, we have already known that a <strong>neural network</strong> with particular settings for hidden layers and activation functions can approximate <a href="/2022/09/02/neural-nets.html#unv-approx">any</a> continuous functions on a compact subsets of $\mathbb{R}^n$, so how about using it with the Q-learning algorithm?</p>

<p>Specifically, we will be using neural network with weight $\boldsymbol{\theta}$ as a function approximator for Q-learning update. The network is referred as <strong>Q-network</strong>, as the whole algorithm is so-called <strong>Deep Q-learning</strong>, and the agent is known as <strong>DQN</strong> in short for <strong>Deep Q-network</strong>.</p>

<p>The Q-network can be trained by minimizing a sequence of loss function $L_t(\boldsymbol{\theta}_t)$ that changes at each iteration $t$:
\begin{equation}
L_t(\boldsymbol{\theta}_t)=\mathbb{E}_{s,a\sim\rho(\cdot)}\Big[\big(y_t-Q_{\boldsymbol{\theta}_t}(s,a)\big)^2\Big],\label{eq:dqn.1}
\end{equation}
where
\begin{equation}
y_t=\mathbb{E}_{s’\sim\mathcal{E}}\left[R(s,a,s’)+\gamma\max_{a’}Q_{\boldsymbol{\theta}_{t-1}}(s’,a’)\vert s,a\right]
\end{equation}
is the target in iteration $t$, which follows as in \eqref{eq:nql.3}; and where $\rho(s,a)$ is referred as the behavior policy.</p>

<p>The TD target $y_t$ can approximated as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\max_{a’}Q_{\boldsymbol{\theta}_t}(s_{t+1},a’)
\end{equation}
To stabilize learning, DQN applies the following mechanisms.</p>

<h4 id="exp-replay">Experience replay</h4>
<p>Along with Q-network, the authors of deep-Q learning also introduce a technique called <strong>experience replay</strong>, which utilizes data efficiency and at the same time reduces the variance of the updates.</p>

<p>In particular, at each time step $t$, the <strong>experience</strong>, $e_t$, defined as
\begin{equation}
e_t=(s_t,a_t,r_t,s_{t+1})
\end{equation}
is added into a set $\mathcal{D}$ of size $N$, which is sampled uniformly at the training time to apply Q-learning updates. This method provides some advantages:</p>
<ul>
  <li>Each experience $e_t$ can be used in many weight updates.</li>
  <li>Uniformly sampling from $\mathcal{D}$ cancels out the correlations between consecutive experiences, i.e. $e_t, e_{t+1}$.</li>
</ul>

<h4 id="target-net">Target network</h4>
<p>DQN introduces a <strong>target network</strong> $\hat{Q}$ parameterized by $\boldsymbol{\theta}^-$to generate the TD target $y_t$ in \eqref{eq:dqn.1} as
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\max_{a’}\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a’)\label{eq:tn.1}
\end{equation}
The target network $\hat{Q}$ is cloned from $Q$ every $C$ Q-learning update steps, i.e. $\boldsymbol{\theta}^-\leftarrow\boldsymbol{\theta}$.</p>

<h2 id="imp-vars">Improved variants</h2>

<h3 id="double-dqn">Double deep Q-learning</h3>
<p>As stated <a href="/2022/01/31/td-learning.html#max-bias">before</a> that the Q-learning method could lead to over optimistic value estimates. Moreover, Q-learning with function approximation, such as DQN, has also been <a href="#double-dqn-paper">proved</a> to induce maximization bias. These results are due to that in Q-learning and DQN, the $\max$ operator uses the same values to both select and evaluate an action.</p>

<p>To reduce the overoptimism effect due to overestimation in DQN, we use a double estimator version of deep Q-learning, called <strong>Double Deep-Q learning</strong>, as we have used double Q-learning to mitigate the maximization bias in Q-learning.</p>

<p>The <strong>double DQN</strong> agent is similar to DQN, except that it replaces the target \eqref{eq:tn.1}, which can be rewritten as:
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\,\hat{Q}_{\boldsymbol{\theta}_t^-}(s_{t+1},a)\right),
\end{equation}
with
\begin{equation}
y_t=R(s_t,a_t,s_{t+1})+\gamma\hat{Q}_{\boldsymbol{\theta}_t^-}\left(s_{t+1},\underset{a}{\text{argmax}}\,Q_{\boldsymbol{\theta}_t}(s_{t+1},a)\right)
\end{equation}</p>

<h3 id="prior-rep">Prioritized replay</h3>

<h3 id="duel-net">Dueling network</h3>

<h3 id="rainbow">Rainbow</h3>

<h3 id="c-dqn">C-DQN</h3>

<h2 id="references">References</h2>
<p>[1] <span id="q-learning-td-convergence">Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</span></p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p>

<p>[3] Pieter Abbeel. <a href="https://youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL Series</a>, YouTube, 2021.</p>

<p>[4] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[5] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[6] <span id="double-dqn-paper">Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</span></p>

<p>[7] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas. <a href="https://arxiv.org/abs/1511.06581">Dueling Network Architectures for Deep Reinforcement Learning</a>. arXiv:1511.06581, 2015.</p>

<p>[8] Tom Schaul, John Quan, Ioannis Antonoglou, David Silver. <a href="https://arxiv.org/abs/1511.05952">Prioritized Experience Replay</a>. arXiv:1511.05952, 2016.</p>

<p>[9] Taisuke Kobayashi, Wendyam Eric Lionel Ilboudo. <a href="https://arxiv.org/abs/2008.10861">t-Soft Update of Target Network for Deep Reinforcement Learning</a>. arXiv:2008.10861, 2020.</p>

<p>[10] Zhikang T. Wang, Masahito Ueda. <a href="https://arxiv.org/abs/2106.15419">Convergent and Efficient Deep Q Network Algorithm</a>. arXiv:2106.15419, 2022.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In <strong>Monte Carlo control</strong>, the update target $y_t$ is chosen as the <strong>full return</strong> $G_t$, i.e.
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_t-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
and in (episodic on-policy) TD control methods, we use the <strong>TD target</strong> as the choice for $y_t$, i.e. for one-step TD methods such as <strong>one-step Sarsa</strong>, the update rule for $\boldsymbol{\theta}$ is given as
\begin{align*}
\boldsymbol{\theta}_{t+1}&amp;=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+1}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t) \\ &amp;=\boldsymbol{\theta}_t+\alpha\big[R(s_t,a_t,s_{t+1})+\gamma Q_{\boldsymbol{\theta}_t}(s_{t+1},a_{t+1})-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{align*}
and for $n$-step TD method, for instance, <strong>$n$-step Sarsa</strong>, we instead have
\begin{equation*}
\boldsymbol{\theta}_{t+1}=\boldsymbol{\theta}_t+\alpha\big[G_{t:t+n}-Q_{\boldsymbol{\theta}_t}(s_t,a_t)\big]\nabla_\boldsymbol{\theta}Q_\boldsymbol{\theta}(s_t,a_t),
\end{equation*}
where
\begin{equation*}
G_{t:t+n}=R_{t+1}+\gamma R_{t+2}+\ldots+\gamma^{n-1}R_{t+n}+\gamma^n Q_{\boldsymbol{\theta}_{t+n-1}}(s_{t+n},a_{t+n}),\hspace{1cm}t+n&lt;T
\end{equation*}
with $G_{t:t+n}\doteq G_t$ if $t+n\geq T$ and where $R_{t+1}\doteq R(s_t,a_t,s_{t+1})$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The semi-gradient TD methods with linear function approximation, e.g. \eqref{eq:nql.5}, are guaranteed to converge to the <strong>TD fixed point</strong> due to the <a href="/2022/02/11/func-approx.html#td-fixed-pt-proof">result</a> we have mentioned. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="deep-reinforcement-learning" /><category term="function-approximation" /><category term="q-learning" /><category term="my-rl" /><summary type="html"><![CDATA[Deep Q-learning and variants]]></summary></entry><entry><title type="html">Inverse Reinforcement Learning</title><link href="http://localhost:4000/2022/10/17/irl.html" rel="alternate" type="text/html" title="Inverse Reinforcement Learning" /><published>2022-10-17T13:00:00+07:00</published><updated>2022-10-17T13:00:00+07:00</updated><id>http://localhost:4000/2022/10/17/irl</id><content type="html" xml:base="http://localhost:4000/2022/10/17/irl.html"><![CDATA[<blockquote>
  <p>A note on Inverse Reinforcement Learning</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#mdp">Markov decision processes</a></li>
      <li><a href="#irl">Inverse reinforcement learning</a></li>
    </ul>
  </li>
  <li><a href="#max-margin-methods">Max margin methods</a>
    <ul>
      <li><a href="#max-margin-proj">Max-margin &amp; Projection</a>
        <ul>
          <li><a href="#max-margin">Max-margin</a></li>
          <li><a href="#proj">Projection</a></li>
        </ul>
      </li>
      <li><a href="#max-margin-pln">Max margin planning</a></li>
      <li><a href="#learch">LEARCH</a></li>
    </ul>
  </li>
  <li><a href="#max-ent">Max entropy methods</a></li>
  <li><a href="#bayes">Bayesian methods</a>
    <ul>
      <li><a href="#birl">BIRL</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="mdp">Markov decision processes</h3>
<p>We begin by recalling the definition of <a href="/2021/06/27/mdp-bellman-eqn.html#mdp"><strong>Markov decision processes (MDP)</strong></a>.</p>

<p>A <strong>Markov decision process</strong>, or <strong>MDP</strong> is defined to be a tuple $(\mathcal{S},\mathcal{A},T,r,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a set of states, represents the <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a set of actions, known as the <strong>action space</strong>.</li>
  <li>$T:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the <strong>state transition probabilities</strong>, i.e., $T(s’,s,a)=p(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ denotes the <strong>reward function</strong>.</li>
  <li>$\gamma\in[0,1]$ is referred as <strong>discount factor</strong>.</li>
</ul>

<p>A <strong>policy</strong>, denoted $\pi$, is a mapping from states to probabilities of selecting each possible action, which can be either deterministic $\pi:\mathcal{S}\times\mathcal{A}\to\{0,1\}$ (or $\pi:\mathcal{S}\to\mathcal{A}$) or stochastic $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$.</p>

<p>For a policy $\pi$, the <strong>state value function</strong>, denoted as $v_\pi$, of a state $s\in\mathcal{S}$ measures how good it is for the agent to be in $s$. Specifically, $v_\pi(s)$ is defined as the expected sum of discounted rewards when starting in $s$ and following $\pi$. Specifically, for all $s\in\mathcal{S}$
\begin{equation}
v_\pi(s)=\mathbb{E}_\pi\left[\sum_{k=0}^{\infty}\gamma^k r(S_t,A_t)\Big\vert S_t=s\right]
\end{equation}
The expectation of value of start state $S_0$ presents the value of the policy it is following. In particular, the value of a policy $\pi$ can be given by
\begin{equation}
\mathbb{E}\big[v_\pi(S_0)\big]=\mathbb{E}_\pi\left[\sum_{t=0}^{\infty}\gamma^t r(S_t,A_t)\Big\vert S_0\right]\label{eq:mdp.1}
\end{equation}
Analogously, the <strong>state-action value function</strong>, denoted by $q_\pi$, of a state-action pair $(s\in\mathcal{S},a\in\mathcal{A})$ specifies how good it is to take action $a$ from state $s$. Similar to $v_\pi$, $q_\pi$ is defined as the expected sum of discounted rewards when starting from $s$, taking action $a$ and thereby following policy $\pi$.</p>

<h3 id="irl">Inverse reinforcement learning</h3>
<p>In <strong>reverse reinforcment learning</strong>, we are instead working on an interface of tuple $(\mathcal{S},\mathcal{A},T,\gamma)$, which represents an $MDP$ without a predefined reward function $r$, or MDP\R for short. Rather than an explicit reward function given, we are provided samples $\{\tau_i\}$ sampled from an optimal policy $\pi^*(\tau)$.</p>

<p>In this problem, our goal is to learn a reward function $r_\boldsymbol{\psi}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ parameterized by the weight vector $\boldsymbol{\psi}$, and then use it to learn the optimal policy $\pi^*$. Specifically, for a linear reward function, we have
\begin{equation}
r_\boldsymbol{\psi}(s,a)=\sum_{i}\psi_i f_i(s,a)=\boldsymbol{\psi}^\text{T}\mathbf{f}(s,a),
\end{equation}
where the weight vector is defined as 
\begin{equation}
\boldsymbol{\psi}=(\psi_1,\ldots,\psi_k)^\text{T},
\end{equation}
and where $\mathbf{f}(s,a)$ is called the <strong>feature vector</strong> of the state-action pair $(s,a)$, defined by
\begin{equation}
\mathbf{f}(s,a)=\big(f_1(s,a),\ldots,f_k(s,a)\big)^\text{T}
\end{equation}
By \eqref{eq:mdp.1}, the value of a policy $\pi_{r_\boldsymbol{\psi}}$ corresponding to the reward function $r_\boldsymbol{\psi}$ thus can be written as
\begin{align}
\mathbb{E}\big[v_{\pi_{r_\boldsymbol{\psi}}}(S_0)\big]&amp;=\mathbb{E}_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t r(S_t,A_t)\Big\vert S_0\right] \\ &amp;=\mathbb{E}_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\boldsymbol{\psi}^\text{T}\mathbf{f}(S_t,A_t)\Big\vert S_0\right] \\ &amp;=\boldsymbol{\psi}^\text{T}\mathbb{E}_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\mathbf{f}(S_t,A_t)\Big\vert S_0\right]
\end{align}
The <strong>feature expectation</strong> or also called the expected discounted accumulated feature value, denoted as $\boldsymbol{\mu}(\pi)$, of the policy $\pi_{r_\boldsymbol{\psi}}$ is defined as
\begin{equation}
\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})=\mathbb{E}_{\pi_{r_\boldsymbol{\psi}}}\left[\sum_{t=0}^{\infty}\gamma^t\mathbf{f}(S_t,A_t)\Big\vert S_0\right]
\end{equation}
Hence, we have that
\begin{equation}
\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})=\boldsymbol{\psi}^\text{T}\boldsymbol{\mu}(\pi_{r_\boldsymbol{\psi}})
\end{equation}</p>

<h2 id="max-margin-methods">Max margin methods</h2>

<h3 id="max-margin-proj">Max-margin &amp; Projection</h3>

<h4 id="max-margin">Max-margin</h4>

<h4 id="proj">Projection</h4>

<h3 id="max-margin-pln">Max margin planning</h3>

<h3 id="learch">LEARCH</h3>

<h2 id="max-ent">Max entropy methods</h2>

<h2 id="references">References</h2>
<p>[1] Pieter Abbeel &amp; Andrew Y. Ng. <a href="https://doi.org/10.1145/1015330.1015430">Apprenticeship Learning via Inverse Reinforcement Learning</a>. ICML ‘04: Proceedings of the twenty-first international conference on Machine learning. July 2004.</p>

<p>[2] Nathan D. Ratliff, J. Andrew Bagnell &amp; Martin A. Zinkevich. <a href="https://doi.org/10.1145/1143844.1143936">Maximum margin planning</a>. ICML ‘06: Proceedings of the 23rd international conference on Machine learning. June 2006</p>

<p>[3] Nathan Ratliff, David Silver &amp; J. Andrew (Drew) Bagnell. <a href="https://www.ri.cmu.edu/publications/learning-to-search-functional-gradient-techniques-for-imitation-learning/">Learning to search: Functional gradient techniques for imitation learning</a>. Autonomous Robots. July 2009.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="reinforcement-learning" /><category term="inverse-reinforcement-learning" /><summary type="html"><![CDATA[Inverse Reinforcement Learning]]></summary></entry><entry><title type="html">Natural Evolution Strategies</title><link href="http://localhost:4000/2022/10/07/nes.html" rel="alternate" type="text/html" title="Natural Evolution Strategies" /><published>2022-10-07T13:00:00+07:00</published><updated>2022-10-07T13:00:00+07:00</updated><id>http://localhost:4000/2022/10/07/nes</id><content type="html" xml:base="http://localhost:4000/2022/10/07/nes.html"><![CDATA[<blockquote>
  <p><strong>Natural Evolution Strategies</strong>, or <strong>NES</strong>, are referred to a family of evolution strategies that throughout its generations update a search distribution repeatedly using an estimated gradient of its distribution parameters.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#search-grad">Search gradients</a>
    <ul>
      <li><a href="#search-grad-gauss">Search gradients for MVN</a></li>
      <li><a href="#ntr-grad">Natural gradient</a></li>
    </ul>
  </li>
  <li><a href="#rbn-tchnq">Robustness techniques</a>
    <ul>
      <li><a href="#fn-shp">Fitness shaping</a></li>
      <li><a href="#adp-sampl">Adaption sampling</a></li>
    </ul>
  </li>
  <li><a href="#rot-sym-dist">Rotationally-symmetric distributions</a>
    <ul>
      <li><a href="#exp-param">Exponential parameterization</a></li>
      <li><a href="#exp-coords">Exponential local coordinates</a></li>
      <li><a href="#samp-rot-sym-dist">Sampling from rotationally symmetric distributions</a></li>
      <li><a href="#xnes">Exponential Natural Evolution Strategies</a></li>
    </ul>
  </li>
  <li><a href="#test-on-rast">Testing on Rastrigin function</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="search-grad">Search gradients</h2>
<p>Usually when working on <strong>Evolution Strategy</strong> methods, we select some candidate solutions, which generate better fitness values than the other ones, to be parents of the next generation. This means, majority of solution samples have been wasted since they may contain some useful information.</p>

<p>To utilize the use all fitness samples, the <strong>NES</strong> uses <strong>search gradients</strong> in updating the parameters for the search distribution.</p>

<p>Let $\mathbf{z}\in\mathbb{R}^n$ denote the solution sampled from the distribution $\pi(\mathbf{z},\theta)$ and let $f:\mathbb{R}^n\to\mathbb{R}$ be the fitness (or objective) function. The expected fitness value is then given by
\begin{equation}
J(\theta)=\mathbb{E}_\theta[f(\mathbf{z})]=\int f(\mathbf{z})\pi(\mathbf{z}\vert\theta)\,d\mathbf{z}\label{eq:sg.1}
\end{equation}
Taking the gradient of the above function w.r.t $\theta$ using the <strong>log-likelihood trick</strong> as in <a href="/2022/05/04/policy-gradient-theorem.html#reinforce">REINFORCE</a> gives us
\begin{align}
\nabla_\theta J(\theta)&amp;=\nabla_\theta\int f(\mathbf{z})\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\int f(\mathbf{z})\nabla_\theta\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\int f(\mathbf{z})\nabla_\theta\pi(\mathbf{z}\vert\theta)\frac{\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\,d\mathbf{z} \\ &amp;=\int\left[f(\mathbf{z})\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\right]\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\mathbb{E}_\theta\left[f(\mathbf{z})\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\right]
\end{align}
Using Monte Carlo method, given samples $\mathbf{z}_1,\ldots,\mathbf{z}_\lambda$ from the population of size $\lambda$, the search gradient is then can be approximated by
\begin{equation}
\nabla_\theta J(\theta)\approx\frac{1}{\lambda}\sum_{k=1}^{\lambda}f(\mathbf{z}_k)\nabla_\theta\log\pi(\mathbf{z}_k\vert\theta)\label{eq:sg.2}
\end{equation}
Given this gradient w.r.t $\theta$, we then can use a gradient-based method to repeatedly update the parameter $\theta$ in order to give us a more desired search distribution. In particular, we can use such as SGD method
\begin{equation}
\theta\leftarrow\theta+\alpha\nabla_\theta J(\theta),\label{eq:sg.3}
\end{equation}
where $\alpha$ is the learning rate.</p>

<h3 id="search-grad-gauss">Search gradients for MVN</h3>
<p>Consider the case that our search distribution $\pi(\mathbf{z}\vert\theta)$ is in form of a Multivariate Normal  distribution, $\mathbf{z}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, where $\boldsymbol{\mu}\in\mathbb{R}^n$ and $\boldsymbol{\Sigma}\in\mathbb{R}^{n\times n}$.</p>

<p>In this case $\theta=(\boldsymbol{\mu},\boldsymbol{\Sigma})$ denotes a tuple of parameters for the search distribution, which is given by
\begin{equation}
\pi(\mathbf{z}\vert\theta)=\frac{1}{(2\pi)^{n/1}\left\vert\boldsymbol{\Sigma}\right\vert^{1/2}}\exp\left[-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right]
\end{equation}
Taking natural logarithm of both sides then gives us
\begin{align}
\log\pi(\mathbf{z}\vert\theta)&amp;=\log\left(\frac{1}{(2\pi)^{n/1}\left\vert\boldsymbol{\Sigma}\right\vert^{1/2}}\exp\left[-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right]\right) \\ &amp;=-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)
\end{align}
We continue by differentiating the above log-likelihood w.r.t $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$. Starting with $\boldsymbol{\mu}$, the gradient is given by
\begin{align}
\nabla_\boldsymbol{\mu}\log\pi(\mathbf{z}\vert\theta)&amp;=\nabla_\boldsymbol{\mu}\left(-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right) \\ &amp;=-\frac{1}{2}\nabla_\boldsymbol{\mu}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right) \\ &amp;=\boldsymbol{\Sigma}^{-1}(\mathbf{z}-\boldsymbol{\mu})
\end{align}
And the gradient w.r.t $\boldsymbol{\Sigma}$ is computed as
\begin{align}
\nabla_\boldsymbol{\Sigma}\pi(\mathbf{z}\vert\theta)&amp;=\nabla_\boldsymbol{\Sigma}\left(-\frac{n}{2}\log(2\pi)-\frac{1}{2}\log\vert\boldsymbol{\Sigma}\vert-\frac{1}{2}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\right) \\ &amp;=-\frac{1}{2}\nabla_\boldsymbol{\Sigma}\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right) \\ &amp;=\frac{1}{2}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}-\boldsymbol{\mu}\right)\left(\mathbf{z}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}-\frac{1}{2}\boldsymbol{\Sigma}^{-1}
\end{align}
The SGD update \eqref{eq:sg.3} now is applied for each of $\boldsymbol{\mu}$ and $\boldsymbol{\Sigma}$ as
\begin{align}
\boldsymbol{\mu}&amp;\leftarrow\boldsymbol{\mu}+\alpha\nabla_\boldsymbol{\mu}J(\theta) \\ &amp;\leftarrow\boldsymbol{\mu}+\alpha\frac{1}{\lambda}\sum_{k=1}^{\lambda}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}_k-\boldsymbol{\mu}\right)f(\mathbf{z}_k)
\end{align}
and
\begin{align}
\boldsymbol{\Sigma}&amp;\leftarrow\boldsymbol{\Sigma}+\alpha\nabla_\boldsymbol{\Sigma}J(\theta) \\ &amp;\leftarrow\boldsymbol{\Sigma}+\alpha\frac{1}{\lambda}\sum_{k=1}^{\lambda}\left[\frac{1}{2}\boldsymbol{\Sigma}^{-1}\left(\mathbf{z}_k-\boldsymbol{\mu}\right)\left(\mathbf{z}_k-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}-\frac{1}{2}\boldsymbol{\Sigma}^{-1}\right]f(\mathbf{z}_k)
\end{align}</p>

<h3 id="ntr-grad">Natural gradient</h3>
<p>The <strong>natural gradient</strong> searches for the direction based on the distance between distributions $\pi(\mathbf{z}\vert\theta)$ and $\pi(\mathbf{z}\vert\theta’)$. One natural measure of distance between probability distributions is the <strong>Kullback-Leibler divergence</strong>, or <strong>KL divergence</strong>.</p>

<p>In other words, our work is to look for the direction of updating gradient, denoted as $\delta\theta$, such that
\begin{align}
\max_{\delta\theta}&amp;\,J(\theta+\delta\theta)\approx J(\theta)+\delta\theta^\text{T}\nabla_\theta J \\ \text{s.t.}&amp;\,D(\theta\Vert\theta+\delta\theta)=\varepsilon,
\end{align}
where $J(\theta)$ is given as in \eqref{eq:sg.1}; $\varepsilon$ is a small increment size; and where $D(\theta+\delta\theta\Vert\theta)$ is the KL divergence of $\pi(\mathbf{z}\vert\theta+\delta\theta)$ from $\pi(\mathbf{z}\vert\theta)$, defined as
\begin{align}
D(\theta\Vert\theta+\delta\theta)&amp;=\int\pi(\mathbf{z}\vert\theta)\log\frac{\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta+\delta\theta)}\,d\mathbf{z} \\ &amp;=\mathbb{E}_{\theta}\big[\log\pi(\mathbf{z}\vert\theta)-\log\pi(\mathbf{z}\vert\theta+\delta)\big]\label{eq:ng.1}
\end{align}
As $\delta\theta\to 0$, or in other words, consider the Taylor expansion of \eqref{eq:ng.1} about $\delta\theta=0$, we have
\begin{align}
&amp;\hspace{-1cm}D(\theta+\delta\theta\Vert\theta)\nonumber \\ &amp;\hspace{-0.8cm}=\mathbb{E}_{\theta}\big[\log\pi(\mathbf{z}\vert\theta)-\log\pi(\mathbf{z}\vert\theta+\delta\theta)\big] \\ &amp;\hspace{-0.8cm}\approx\mathbb{E}_\theta\left[\log\pi(\mathbf{z}\vert\theta)-\left(\log\pi(\mathbf{z}\vert\theta)+\delta\theta^\text{T}\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}+\frac{1}{2}\delta\theta^\text{T}\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\left(\frac{\nabla_\theta\pi(\mathbf{z}\vert\theta)}{\pi(\mathbf{z}\vert\theta)}\right)^\text{T}\delta\theta\right)\right] \\ &amp;\hspace{-0.8cm}=-\mathbb{E}_\theta\left[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)+\frac{1}{2}\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\right] \\ &amp;\hspace{-0.8cm}=-\mathbb{E}_\theta\Big[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\Big]-\mathbb{E}_\theta\left[\frac{1}{2}\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\right] \\ &amp;\hspace{-0.8cm}=-\frac{1}{2}\int\pi(\mathbf{z}\vert\theta)\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\delta\theta\,d\mathbf{z} \\ &amp;\hspace{-0.8cm}=-\frac{1}{2}\delta\theta^\text{T}\mathbf{F}\delta\theta\label{eq:ng.2}
\end{align}
where in the fifth step, we have used that
\begin{align}
\mathbb{E}_\theta\Big[\delta\theta^\text{T}\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\Big]&amp;=\delta\theta^\text{T}\int\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\int\pi(\mathbf{z}\vert\theta)\frac{1}{\pi(\mathbf{z}\vert\theta)}\nabla_\theta\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\nabla_\theta\int\pi(\mathbf{z}\vert\theta)\,d\mathbf{z} \\ &amp;=\delta\theta^\text{T}\nabla_\theta 1=0
\end{align}</p>

<p>The matrix $\mathbf{F}$ in \eqref{eq:ng.2} is known as the <strong>Fisher information matrix</strong> of the given parametric family of search distributions, defined as
\begin{align}
\mathbf{F}&amp;=\int\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\,d\mathbf{z} \\ &amp;=\mathbb{E}_\theta\big[\nabla_\theta\log\pi(\mathbf{z}\vert\theta)\nabla_\theta\log\pi(\mathbf{z}\vert\theta)^\text{T}\big]
\end{align}
Hence, we have the Lagrangian of our constrained optimization problem is
\begin{align}
\mathcal{L}(\theta,\delta\theta,\lambda)&amp;=J(\theta)+\delta\theta^\text{T}\nabla_\theta J(\theta)+\lambda\big(D(\theta+\delta\theta\Vert\theta)-\varepsilon\big) \\ &amp;=J(\theta)+\delta\theta^\text{T}\nabla_\theta J(\theta)-\lambda\left(\frac{1}{2}\delta\theta^\text{T}\mathbf{F}\delta\theta+\varepsilon\right),
\end{align}
where $\lambda&gt;0$ is the Lagrange multiplier.</p>

<p>It is easily seen that $\mathbf{F}$ is symmetric, thus taking the gradient of the Lagrangian w.r.t $\delta\theta$ and setting it to zero gives us
\begin{equation}
\lambda\mathbf{F}\delta\theta=\nabla_\theta J(\theta)
\end{equation}
If the Fisher information matrix $\mathbf{F}$ is invertible, the solution for $\delta\theta$ that maximizes $\mathcal{L}$ then can be computed as
\begin{equation}
\delta\theta=\frac{1}{\lambda}\mathbf{F}^{-1}\nabla_\theta J(\theta),\label{eq:ng.3}
\end{equation}
which defines the direction of the natural gradient $\tilde{\nabla}_\theta J(\theta)$. Since $\lambda&gt;0$ we therefore obtain
\begin{equation}
\tilde{\nabla}_\theta J(\theta)=\mathbf{F}^{-1}\nabla_\theta J(\theta)
\end{equation}
Continue with the value of $\delta\theta$ given in \eqref{eq:ng.3}, the dual function of our optimization is given as
\begin{align}
g(\lambda)&amp;=J(\theta)+\frac{1}{\lambda}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta  J(\theta)-\frac{1}{2}\frac{\lambda}{\lambda^2}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\mathbf{F}\mathbf{F}^{-1}\nabla_\theta J(\theta)-\lambda\varepsilon \\ &amp;=J(\theta)+\frac{1}{2}\lambda^{-1}\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)-\lambda\varepsilon
\end{align}
Taking the gradient of $g$ w.r.t $\lambda$ and setting it to zero and since $\varepsilon&lt;0$ small gives us the solution for $\lambda$, which is
\begin{equation}
\lambda=\sqrt{\frac{\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)}{\varepsilon}},
\end{equation}
Hence, the SGD update for the parameter $\theta$ using natural gradient is
\begin{equation}
\theta\leftarrow\theta+\eta\tilde{\nabla}_\theta J(\theta)=\theta+\eta\mathbf{F}^{-1}\nabla_\theta J(\theta),\label{eq:ng.4}
\end{equation}
where $\eta$ is the learning rate, given as
\begin{equation}
\eta=\lambda^{-1}=\sqrt{\frac{\varepsilon}{\nabla_\theta J(\theta)^\text{T}\mathbf{F}^{-1}\nabla_\theta J(\theta)}}
\end{equation}
This learning rate can also be replaced by a more desirable one without changing the direction of our update. With this update rule for natural gradient, we obtain the general formulation of NES, as described in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-10-07/nes.png" alt="NES" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h2 id="rbn-tchnq">Robustness techniques</h2>

<h3 id="fn-shp">Fitness shaping</h3>
<p>NES uses the so-called <strong>fitness shaping</strong> technique, which helps to avoid early convergence due to the possible affection of outliers fitness value in \eqref{eq:sg.2}, e.g. there may exist an outlier whose fitness value, says $f(\mathbf{z}_i)$, is much greater than other solutions’ ones, $\{f(\mathbf{z}_k)\}_{k\neq i}$.</p>

<p>Rather than using fitness values $f(\mathbf{z}_k)$ in approximating the gradient in \eqref{eq:sg.2}, fitness shaping instead applies a rank-based transformation of $f(\mathbf{z}_k)$.</p>

<p>In particular, let $\mathbf{z}_{k:\lambda}$ denote the $k$-th best sample out of the population of size $\lambda$, $\mathbf{z}_1,\ldots,\mathbf{z}_\lambda$, i.e. $f(\mathbf{z}_{1:\lambda})\geq\ldots\geq f(\mathbf{z}_{\lambda:\lambda})$, the gradient estimate \eqref{eq:sg.2} now is rewritten as
\begin{equation}
\nabla_\theta J(\theta)=\sum_{k=1}^{\lambda}u_k\nabla_\theta\log\pi(\mathbf{z}_{k:\lambda}\vert\theta),\label{eq:fs.1}
\end{equation}
where $u_1\geq\ldots\geq u_\lambda$ are referred as <strong>utility values</strong>, which are preserved-order transformations of $f(\mathbf{z}_{1:\lambda}),\ldots,f(\mathbf{z}_{\lambda:\lambda})$.</p>

<p>The choice for utility function $u$ is a free parameter of the algorithm. In the original paper, the author proposed
\begin{equation}
u_k=\frac{\max\left(0,\log\left(\frac{\lambda}{2}+1\right)-\log k\right)}{\sum_{j=1}^{\lambda}\max\left(0,\log\left(\frac{\lambda}{2}+1\right)-\log j\right)}-\frac{1}{\lambda}
\end{equation}</p>

<h3 id="adp-sampl">Adaption sampling</h3>
<p>Beside fitness shaping, NES also applies another heuristic, called <strong>adaption sampling</strong>, to make the performance more robustly. This technique lets the algorithm determine the appropriate hyperparameters (in this case, NES chooses the learning rate $\eta$ be the one to adapt) more quickly.</p>

<p>In particular, for a successive parameter $\theta’$ of $\theta$, the corresponding learning rate $\eta$ used in its update \eqref{eq:ng.4} will be determined by comparing samples $\mathbf{z}’$ sampled from $\pi_\theta’$ with samples $\mathbf{z}$ sampled from $\pi_\theta$ according to a <strong>Mann-Whitney U-test</strong>.</p>

<h2 id="rot-sym-dist">Rotationally-symmetric distributions</h2>
<p>The <strong>rotationally-symmetric distributions</strong>, or <strong>radial distributions</strong> refer to class of distributions $p(\mathbf{x})$ such that
\begin{equation}
p(\mathbf{x})=p(\mathbf{U}\mathbf{x}),\label{eq:rsd.1}
\end{equation}
for all $\mathbf{x}\in\mathbb{R}^n$ and for all orthogonal matrices $\mathbf{U}\in\mathbb{R}^{n\times n}$.</p>

<p>Let $Q_\boldsymbol{\tau}(\mathbf{z})$ be a family of rotationally-symmetric distributions in $\mathbb{R}^n$ parameterized by $\boldsymbol{\tau}$. The property  \eqref{eq:rsd.1} allows us to represent $Q_\boldsymbol{\tau}(\mathbf{z})$ as
\begin{equation}
Q_\boldsymbol{\tau}(\mathbf{z})=q_\boldsymbol{\tau}(\Vert\mathbf{z}\Vert^2),
\end{equation}
for some family of functions $q_\boldsymbol{\tau}:\mathbb{R}_+\to\mathbb{R}_+$.</p>

<p>Consider the classes of search distributions in a form of
\begin{align}
\pi(\mathbf{z}\vert\boldsymbol{\mu},\boldsymbol{\Sigma},\boldsymbol{\tau})&amp;=\frac{1}{\vert\mathbf{A}\vert}q_\boldsymbol{\tau}\left(\left\Vert(\mathbf{A}^{-1})^\text{T}(\mathbf{z}-\boldsymbol{\mu})\right\Vert^2\right) \\ &amp;=\frac{1}{\left\vert\mathbf{A}^\text{T}\mathbf{A}\right\vert^{1/2}}q_\boldsymbol{\tau}\left((\mathbf{z}-\boldsymbol{\mu})^\text{T}(\mathbf{A}^\text{T}\mathbf{A})^{-1}(\mathbf{z}-\boldsymbol{\mu})\right),\label{eq:rsd.2}
\end{align}
with additional transformation parameters $\boldsymbol{\mu}\in\mathbb{R}^n$ and invertible matrices $\mathbf{A}\in\mathbb{R}^{n\times n}$.</p>

<p>It can be seen that Gaussian and its multivariate form, MVN, can be written in form of $\eqref{eq:rsd.2}$, and thus are members of these classes of distributions.</p>

<h3 id="exp-param">Exponential parameterization</h3>
<p>By \eqref{eq:ng.4}, the natural gradient update for a multivariate Gaussian search distribution, denoted $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, is
\begin{align}
\boldsymbol{\mu}&amp;\leftarrow\boldsymbol{\mu}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\mu} J(\boldsymbol{\mu},\boldsymbol{\Sigma}), \\ \boldsymbol{\Sigma}&amp;\leftarrow\boldsymbol{\Sigma}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\Sigma} J(\boldsymbol{\mu},\boldsymbol{\Sigma})
\end{align}
Thus, in updating the covariance matrix $\boldsymbol{\Sigma}$ as above, we have to ensure that $\boldsymbol{\Sigma}+\eta\mathbf{F}^{-1}\nabla_\boldsymbol{\Sigma} J(\boldsymbol{\mu},\boldsymbol{\Sigma})$ is symmetric positive definite.</p>

<p>To accomplish this, we may represent the covariance matrix using the <strong>exponential parameterization</strong> for symmetric matrices. In particular, let
\begin{equation}
\mathcal{S}_n\doteq\{\mathbf{M}\in\mathbb{R}^{n\times n}:\mathbf{M}=\mathbf{M}^\text{T}\}
\end{equation}
denote the set of symmetric matrices of $\mathbb{R}^{n\times n}$ and let
\begin{equation}
\mathcal{P}_n\doteq\{\mathbf{M}\in\mathcal{S}_n:\mathbf{M}\succ 0\}
\end{equation}
represent the cone of symmetric positive definite matrices of $\mathbb{R}^{n\times n}$.</p>

<p>Using Taylor expansion for the exponential function, we then have the exponential map $\exp:\mathcal{S}_n\to\mathcal{P}_n$ can be written as
\begin{equation}
\exp(\mathbf{M})=\sum_{i=0}^{\infty}\frac{\mathbf{M}^i}{i!},\label{eq:ep.1}
\end{equation}
which is <strong>diffeomorphism</strong>, i.e. the map is bijective, plus the map and its inverse map, $\log:\mathcal{P}_n\to\mathcal{S}_n$, both are differentiable.</p>

<p>Therefore, we can represent the covariance matrix $\boldsymbol{\Sigma}\in\mathcal{P}_n$ as
\begin{equation}
\boldsymbol{\Sigma}=\exp(\mathbf{M}),\hspace{2cm}\mathbf{M}\in\mathcal{S}_n
\end{equation}
This representation lets the gradient update always end up as a valid covariance matrix. However, the computation for the Fisher information matrix $\mathbf{F}$ is consequently more complicated due to require partial derivatives of matrix exponential \eqref{eq:ep.1}.</p>

<h3 id="exp-coords">Exponential local coordinates</h3>
<p>It is noticeable from \eqref{eq:rsd.2} that the dependency of the distribution on $\mathbf{A}$ is only in terms of $\mathbf{A}^\text{T}\mathbf{A}$, which is a symmetric positive semi-definite matrix since for all non-zero vector $\mathbf{x}\in\mathbb{R}^n$ we have
\begin{equation}
\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{A}\mathbf{x}=\Vert\mathbf{A}\mathbf{x}\Vert^2\geq 0
\end{equation}
In the case of MVN, this matrix corresponds to the covariance matrix.</p>

<p>Therefore, rather than using exponential mapping in updating the positive definite matrices $\mathbf{A}^\text{T}\mathbf{A}$, we repeatedly linear transform the coordinate system in each iteration to a coordinate system in which the calculation for $\mathbf{F}$ is trivial.</p>

<p>Specifically, let the current search distribution be given by $(\boldsymbol{\mu},\mathbf{A})$, we use <strong>exponential local coordinates</strong>
\begin{equation}
(\boldsymbol{\delta},\mathbf{M})\mapsto(\boldsymbol{\mu}_\text{new},\mathbf{A}_\text{new})=\left(\boldsymbol{\mu}+\mathbf{A}^\text{T}\boldsymbol{\delta},\mathbf{A}\exp\left(\frac{1}{2}\mathbf{M}\right)\right)
\end{equation}
This coordinate system is local in the sense that the coordinates $(\boldsymbol{\delta},\mathbf{M})=(\mathbf{0},\mathbf{0})$ is mapped to $(\boldsymbol{\mu},\mathbf{A})$.</p>

<p>For the case that $\tau\in\mathbb{R}^{n’}$, $\boldsymbol{\delta}\in\mathbb{R}^n$ and $\mathbf{M}\in\mathbb{R}^{n(n+1)/2}$, the Fisher information matrix $\mathbf{F}$ in this coordinate system is an $m\times m$ matrix, where
\begin{equation}
m=n+\frac{n(n+1)}{2}+n’=\frac{n(n+3)}{2}+n’,
\end{equation}
and is given as
\begin{equation}
\mathbf{F}=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{V} \\ \mathbf{V}^\text{T}&amp;\mathbf{C}\end{matrix}\right],\label{eq:ec.1}
\end{equation}
where
\begin{equation}
\mathbf{V}=\frac{\partial^2\log\pi(\mathbf{z})}{\partial(\boldsymbol{\delta},\mathbf{M})\partial\boldsymbol{\tau}}\in\mathbb{R}^{(m-n’)\times n’},\hspace{1cm}\mathbf{C}=\frac{\partial^2\log\pi(\mathbf{z})}{\partial\boldsymbol{\tau}^2}\in\mathbb{R}^{n’\times n’}
\end{equation}
Using the <strong>Woodbury identity</strong> for $\mathbf{F}$ gives us its inverse
\begin{equation}
\mathbf{F}^{-1}=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{V} \\ \mathbf{V}^\text{T}&amp;\mathbf{C}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{I}+\mathbf{H}\mathbf{V}\mathbf{V}^\text{T}&amp;-\mathbf{H}\mathbf{v} \\ -\mathbf{H}\mathbf{V}^\text{T}&amp;\mathbf{H}\end{matrix}\right],
\end{equation}
where $\mathbf{H}=(\mathbf{C}-\mathbf{V}^\text{T}\mathbf{V})^{-1}$, and thus $\mathbf{H}$ is symmetric.</p>

<p>On the other hands, the gradient w.r.t each parameter of $\log\pi(\mathbf{z})$ are given as
\begin{equation}
\nabla_{\boldsymbol{\delta},\mathbf{M},\boldsymbol{\tau}}\log\pi(\mathbf{z}\vert\boldsymbol{\mu},\mathbf{A},\boldsymbol{\tau},\boldsymbol{\delta},\mathbf{M})\big\vert_{\,\boldsymbol{\delta}=\mathbf{0},\mathbf{M}=\mathbf{0}}=\mathbf{g}=\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M} \\ \mathbf{g}_\boldsymbol{\tau}\end{matrix}\right],
\end{equation}
where
\begin{align}
\mathbf{g}_\boldsymbol{\delta}&amp;=-2\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s},\label{eq:ec.2} \\ \mathbf{g}_\mathbf{M}&amp;=-\frac{1}{2}\mathbf{I}-\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}\mathbf{s}^\text{T},\label{eq:ec.3} \\ \mathbf{g}_\boldsymbol{\tau}&amp;=\frac{1}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\nabla_\boldsymbol{\tau}q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2),
\end{align}
where
\begin{equation}
q_\boldsymbol{\tau}’=\frac{\partial}{\partial(r^2)}q_\boldsymbol{\tau}
\end{equation}
denotes the derivative of $q_\boldsymbol{\tau}$ w.r.t $r^2$, while $\nabla_\boldsymbol{\tau}q_\boldsymbol{\tau}$ represents the gradient w.r.t $\boldsymbol{\tau}$.</p>

<p>The natural gradient for a sample $\mathbf{s}$ is then can be computed as
\begin{equation}
\tilde{\nabla}J=\mathbf{F}^{-1}\mathbf{g}=\mathbf{F}^{-1}\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M} \\ \mathbf{g}_\boldsymbol{\tau}\end{matrix}\right]=\left[\begin{matrix}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{H}\mathbf{V}\left(\mathbf{V}^\text{T}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{g}_\boldsymbol{\tau}\right) \\ \mathbf{H}\left(\mathbf{V}^\text{T}\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)-\mathbf{g}_\boldsymbol{\tau}\right)\end{matrix}\right],
\end{equation}
where
\begin{equation}
\left(\mathbf{g}_\boldsymbol{\delta},\mathbf{g}_\mathbf{M}\right)=\left[\begin{matrix}\mathbf{g}_\boldsymbol{\delta} \\ \mathbf{g}_\mathbf{M}\end{matrix}\right]
\end{equation}</p>

<h3 id="samp-rot-sym-dist">Sampling from rotationally symmetric distributions</h3>
<p>To sample from this class of distributions, we first draw a sample $\mathbf{s}$ according to the standard density
\begin{equation}
\mathbf{s}\sim\pi(\mathbf{s}\vert\boldsymbol{\mu}=\mathbf{0},\mathbf{A}=\mathbf{I},\boldsymbol{\tau}),
\end{equation}
We continue to transform this sample into
\begin{equation}
\mathbf{z}=\boldsymbol{\mu}+\mathbf{A}^\text{T}\mathbf{s}\sim\pi(\mathbf{z}\vert\boldsymbol{\mu},\mathbf{A},\boldsymbol{\tau})
\end{equation}
In general, sampling $\mathbf{s}$ can be decomposed into sampling $r^2$ according to
\begin{equation}
r^2\sim\tilde{q}_\boldsymbol{\tau}(r^2)=\int_{\Vert\mathbf{z}\Vert^2=r^2}Q_\boldsymbol{\tau}\,d\mathbf{z}=\frac{2\pi^{n/2}}{\Gamma(n/2)}(r^2)^{(d-1)/2}q_\boldsymbol{\tau}(r^2)
\end{equation}
and a unit vector $\mathbf{u}\in\mathbb{R}^n$.</p>

<h3 id="xnes">Exponential Natural Evolution Strategies</h3>
<p>Recall that the Multivariate Gaussian can be expressed in form of a radial distribution \eqref{eq:ep.1}. In this case, we have that
\begin{equation}
q_\boldsymbol{\tau}(r^2)=\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right),\label{eq:xnes.1}
\end{equation}
which does not depend on $\boldsymbol{\tau}$. This lets the Fisher information matrix in \eqref{eq:ec.1} be simplified to the most trivial form, which is the identity matrix $\mathbf{I}$.</p>

<p>Differentiating \eqref{eq:xnes.1} w.r.t $r^2$ then gives us
\begin{equation}
q_\boldsymbol{\tau}’(r^2)=\frac{\partial}{\partial(r^2)}\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right)=-\frac{1}{2}\frac{1}{(2\pi)^{n/2}}\exp\left(-\frac{1}{2}r^2\right)=-\frac{1}{2}q_\boldsymbol{\tau}(r^2),
\end{equation}
which by \eqref{eq:ec.2} and \eqref{eq:ec.3} implies that
\begin{equation}
\mathbf{g}_\boldsymbol{\delta}=-2\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}=\mathbf{s}
\end{equation}
and
\begin{equation}
\mathbf{g}_\mathbf{M}=-\frac{1}{2}\mathbf{I}-\frac{q_\boldsymbol{\tau}’(\Vert\mathbf{s}\Vert^2)}{q_\boldsymbol{\tau}(\Vert\mathbf{s}\Vert^2)}\mathbf{s}\mathbf{s}^\text{T}=\frac{1}{2}(\mathbf{s}\mathbf{s}^\text{T}-\mathbf{I})
\end{equation}
Hence, the natural gradient is then given as
\begin{align}
\nabla_\boldsymbol{\delta}J&amp;=\sum_{k=1}^{\lambda}f(\mathbf{z}_k)\mathbf{s}_k \\ \nabla_\mathbf{M}J&amp;=\sum_{k=1}^{\lambda}f(\mathbf{z}_k)(\mathbf{s}_k\mathbf{s}_k^\text{T}-\mathbf{I}),
\end{align}
which can be improved with fitness shaping using the update formula \eqref{eq:fs.1} as
\begin{align}
\nabla_\boldsymbol{\delta}J&amp;=\sum_{k=1}^{\lambda}u_k\mathbf{s}_{k:\lambda}, \\ \nabla_\mathbf{M}J&amp;=\sum_{k=1}^{\lambda}u_k(\mathbf{s}_{k:\lambda}\mathbf{s}_{k:\lambda}^\text{T}-\mathbf{I}),
\end{align}
where $\mathbf{s}_{k:\lambda}$ denotes the $k$-th best sample in local coordinates. The resulting algorithm is thus known as <strong>Exponential Natural Evolution Strategies</strong>, or <strong>xNES</strong>, with the corresponding pseudocode shown below.</p>
<figure>
	<img src="/assets/images/2022-10-07/xnes.png" alt="xNES" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h2 id="test-on-rast">Testing on Rastrigin function</h2>
<p>Analogy to <a href="/2022/09/14/cma-es.html#test-on-rast">CMA-ES</a>, let us test NES on the Rastrigin function, which is, recall that, given by the formula
\begin{equation}
f(\mathbf{x})=10 n+\sum_{i=1}^{n}x_i^2-10\cos\left(2\pi x_i\right)
\end{equation}
$f(\mathbf{x})$ reaches its global minimum $0$ at $\mathbf{x}=\mathbf{0}$. The experimental setup we are going to use are provided in <a href="#nes-paper">Wierstra et al. 2014</a>. Similar to our test with CMA-ES, each function evaluation is counted as success when it reaches $f_\text{stop}=10^{-10}$.</p>

<p>The result after running our experiment is illustrated in the figure below.</p>
<figure>
	<img src="/assets/images/2022-10-07/nes-rastrigin.png" alt="NES on rastrigin" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Success rate to reach $f_\text{stop}=10^{-10}$ versus population size for Rastrigin function.<br /> The code can be found <span><a href="https://github.com/trunghng/evolution-strategies/blob/main/testing_ground.py">here</a></span></figcaption>
</figure>

<h2 id="references">References</h2>
<p>[1] Daan Wierstra, Tom Schaul, Jan Peters, Jürgen Schmidhuber. <a href="https://people.idsia.ch/~juergen/nes2008.pdf">Natural Evolution Strategies</a>. IEEE World Congress on Computational Intelligence, 2008.</p>

<p>[2] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jürgen Schmidhuber. <a href="https://arxiv.org/abs/1106.4487">Natural Evolution Strategies</a>. arXiv:1106.4487, 2011.</p>

<p><span id="nes-paper">[3] Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi Sun, Jan Peters, Jürgen Schmidhuber. <a href="https://www.jmlr.org/papers/volume15/wierstra14a/wierstra14a.pdf">Natural Evolution Strategies</a>. Journal of Machine Learning Research 15, 2014.</span></p>

<p>[4] Ha, David. <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/">A Visual Guide to Evolution Strategies</a>. blog.otoro.net, 2017.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="evolution-strategy" /><category term="neuroevolution" /><summary type="html"><![CDATA[Natural Evolution Strategy]]></summary></entry><entry><title type="html">Policy Gradient</title><link href="http://localhost:4000/2022/10/06/policy-gradient.html" rel="alternate" type="text/html" title="Policy Gradient" /><published>2022-10-06T15:26:00+07:00</published><updated>2022-10-06T15:26:00+07:00</updated><id>http://localhost:4000/2022/10/06/policy-gradient</id><content type="html" xml:base="http://localhost:4000/2022/10/06/policy-gradient.html"><![CDATA[<blockquote>
  <p>Notes on Policy gradient methods.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#vanilla-pg">(Vanilla) Policy Gradient</a>
    <ul>
      <li><a href="#var-red">Variance reduction</a>
        <ul>
          <li><a href="#reward-to-go">Reward-to-go</a></li>
          <li><a href="#baseline">Baseline</a>
            <ul>
              <li><a href="#unbiased">Unbiased estimator</a></li>
              <li><a href="#how-baseline-red">How can a baseline reduce variance?</a></li>
              <li><a href="#baseline-types">Types of baseline</a></li>
            </ul>
          </li>
          <li><a href="#discount-factor">Discount factor</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>Consider an finite-horizon undiscounted Markov Decision Process (MDP), which is a tuple of $(\mathcal{S},\mathcal{A},P,r,\rho_0,H)$ where</p>
<ul>
  <li>$\mathcal{S}$ is the state space.</li>
  <li>$\mathcal{A}$ is the action space.</li>
  <li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the transition probability, i.e. $P(s’\vert s,a)$ denotes the probability of being at state $s’$ by taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{\mathcal{S}\times\mathcal{A}\times\mathcal{S}}\to\mathbb{R}$ is the reward function.</li>
  <li>$\rho_0$ is the distribution of the start state $s_0$.</li>
  <li>$H$ is the horizon time.</li>
</ul>

<p>To start an episode, the agent is given an initial state $s_0$, which is sampled from $\rho_0$, i.e. $s_0\sim\rho_0$. At each time step $t$, from state $s_t$, until reaching the terminal state, the agent takes action $a_t$, according to a policy $\pi$, where $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$, which lets agent end up at state $s_{t+1}$ due to the dynamics $P$, and is given a corresponding reward $r_t=r(s_t,a_t,s_{t+1})$. The process gives rise to a sequence, called a <strong>trajectory</strong>, defined by:
\begin{equation}
\tau=(s_0,a_0,s_1,a_1,s_2,a_2\ldots)
\end{equation}
For a policy $\pi$, let $V_\pi:\mathcal{S}\to\mathbb{R}$ denote the state value function, $Q_\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ represent the state-action value function and let $A_\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ be the advantage function:
\begin{align}
V_\pi(s_t)&amp;\doteq\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}\left[\sum_{k=0}^{H-1}r_{t+k}\right] \\ Q_\pi(s_t,a_t)&amp;\doteq\mathbb{E}_{s_{t+1:H-1},a_{t+1:H-1}}\left[\sum_{k=0}^{H-1}r_{t+k}\right] \\ A_\pi(s_t,a_t)&amp;\doteq Q_\pi(s_t,a_t)-V_\pi(s_t),
\end{align}
where the expectation notation $\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}$ denotes that the expected value is computed by integrated over $s_{t+1}\sim P(s_{t+1}\vert s_t,a_t),a_t\sim\pi(a_t\vert s_t)$.</p>

<p>As in <strong>DQN</strong>, here we will be working with a policy $\pi_\theta$ parameterized by a vector $\theta$.  Let $R(\tau)\doteq\sum_{t=0}^{H-1}r_t$ denote the return, or the total reward along trajectory $\tau$. Our goal is to maximize the expected return:
\begin{equation}
\eta(\pi_\theta)\doteq\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big]=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}r_t\right]\label{eq:pre.1}
\end{equation}</p>

<h2 id="vanilla-pg">(Vanilla) Policy Gradient</h2>
<p>In <strong>(vanilla) policy gradient</strong> method, we are trying to optimize the expected total reward \eqref{eq:pre.1} by repeatedly estimating the gradient
\begin{equation}
\nabla_\theta\eta(\pi_\theta)=\nabla_\theta\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big]\label{eq:vpg.1}
\end{equation}
To continue our derivation, we will be using the probability of a trajectory $\tau\sim\pi_\theta$, computed by
\begin{equation}
P(\tau;\theta)=\rho_0(s_0)\prod_{t=0}^{H-1}P(s_{t+1}\vert s_t,a_t)\pi_\theta(a_t\vert s_t),
\end{equation}
Given this definition, \eqref{eq:vpg.1} can be written in a form that does not require a dynamics model:
\begin{align}
\hspace{-0.8cm}\nabla_\theta\eta(\pi_\theta)&amp;=\nabla_\theta\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big] \\ &amp;=\nabla_\theta\sum_\tau P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau P(\theta;\tau)\nabla_\theta\log P(\tau;\theta)R(\tau) \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[\nabla_\theta\log P(\tau;\theta)R(\tau)\big] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\nabla_\theta\left(\sum_{t=0}^{H-1}\log\rho_0(s_0)+\log P(s_{t+1}\vert s_t,a_t)+\log\pi_\theta(a_t\vert s_t)\right)R(\tau)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)R(\tau)\right]
\end{align}
Since the gradient is now an expectation, we can approximate it with the empirical estimate from $m$ sample trajectories, as
\begin{equation}
\nabla_\theta\eta(\pi_\theta)=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})R(\tau^{(i)})
\end{equation}</p>

<h3 id="var-red">Variance reduction</h3>

<h4 id="reward-to-go">Reward-to-go</h4>
<p>To reduce the variance, we first notice that the total reward along a trajectory $\tau$, $R(\tau)$, can be expressed as sum of total reward from step $t$, called <strong>reward-to-go</strong> from $t$, and total preceding rewards w.r.t $t$, which has the expected value of zero but non-zero variance. In particular, we can simplify the policy gradient to be indepedent to the reward-to-go only, as:
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)R(\tau)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\sum_{k=0}^{t-1}r_k+\sum_{k=t}^{H-1}r_k\right)\right] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t}}\left[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=0}^{t-1}r_k\right]\nonumber \\ &amp;\hspace{2cm}+\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right] \\ &amp;\overset{\text{(i)}}{=}\sum_{t=0}^{H-1}\left(\mathbb{E}_{s_{0:t},a_{0:t-1}}\left[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]\cdot\sum_{k=0}^{t-1}r_k\right]\right)\nonumber \\ &amp;\hspace{2cm}+\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right]\label{eq:vr.1} \\ &amp;\overset{\text{(ii)}}{=}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right] \\ &amp;\overset{\text{(iii)}}{=}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\hat{r}_t\right],\label{eq:vr.2}
\end{align}
where</p>
<ul>
  <li>The (i) step is due to that the total past reward is independent of the current action $t$.</li>
  <li>In the (ii) step, we have used
\begin{align}
\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]&amp;=\sum_{a_t}\pi_\theta(a_t\vert s_t)\nabla_\theta\log\pi_\theta(a_t\vert s_t) \\ &amp;=\sum_{a_t}\nabla_\theta 1=\mathbf{0}\label{eq:vr.3}
\end{align}</li>
  <li>In the (iii) step, the $\hat{r}_t\doteq\sum_{k=t}^{H-1}r_k$ is referred as the <strong>reward-to-go</strong> from step $t$.</li>
</ul>

<h4 id="baseline">Baseline</h4>
<p>It is worth remarking that we can furtherly reduce the variance of the estimator by adding an baseline, denoted $b$, as
\begin{equation}
\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\right)\right]\label{eq:vrb.1}
\end{equation}</p>

<h5 id="unbiased">Unbiased estimator</h5>
<p>First we will prove that the estimator with baseline \eqref{eq:vrb.1} is still an unbiased with \eqref{eq:vr.2}. Specifically
\begin{align}
&amp;\hspace{-1cm}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\right)\right]\nonumber \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\hat{r}_t\right]\nonumber \\ &amp;\hspace{0.5cm}-\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\right],
\end{align}
which makes our claim follow if the latter expectation of the RHS is zero. In fact, using the logic as in \eqref{eq:vr.1} and the result \eqref{eq:vr.3}, we have
\begin{align}
&amp;\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\right]\nonumber \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\big]\Big] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]\cdot b_t(s_{0:t},a_{0:t-1})\Big] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbf{0}\cdot b_t(s_{0:t},a_{0:t-1})\Big]=\mathbf{0}
\end{align}</p>

<h5 id="how-baseline-red">How can a baseline reduce variance?</h5>
<p>By definition of the variance of a r.v $X$
\begin{equation}
\text{Var}(X)=\mathbb{E}\big[X^2\big]-\mathbb{E}\big[X\big]^2,
\end{equation}
combined with the claim that \eqref{eq:vrb.1} is an unbiased estimator of the policy gradient $\nabla_\theta\eta(\pi_\theta)$, and letting $b_t\doteq b_t(s_{0:t},a_{0:t-1})$ to simplify the notation, we have 
\begin{align}
\text{Var}&amp;=\text{Var}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\left(\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right)^2\right]\nonumber \\ &amp;\hspace{1.5cm}-\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right]^2 \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\left(\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right)^2\right]-\big(\nabla_\theta\eta(\pi_\theta)\big)^2,
\end{align}
which suggests us that for each $t$, by finding $b_t$ that minimizes the former expectation, we can also reduce the variance $\text{Var}$.</p>

<p>Differentiating the variance w.r.t $b_t$ gives us
\begin{equation}
\frac{\partial\text{Var}}{\partial b_t}=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\Big[\big(\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big)^2(2b_t-2\hat{r}_t)\Big]
\end{equation}
Set the derivative to zero and solve for $b_t$, we obtain the optimal baseline
\begin{equation}
b_t=\frac{\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\Big[\big(\nabla_\theta\pi_\theta(a_t\vert s_t)\big)^2\hat{r}_t\Big]}{\mathbb{E}_{a_t}\Big[\big(\nabla_\theta\pi_\theta(a_t\vert s_t)\big)^2\Big]}
\end{equation}</p>

<h5 id="baseline-types">Types of baseline</h5>

<h4 id="discount-factor">Discount factor</h4>

<h2 id="preferences">Preferences</h2>
<p>[1] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel. <a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>. ICLR 2016.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="deep-reinforcement-learning" /><category term="policy-gradient" /><category term="my-rl" /><summary type="html"><![CDATA[Policy Gradient methods]]></summary></entry><entry><title type="html">CMA Evolution Strategy</title><link href="http://localhost:4000/2022/09/14/cma-es.html" rel="alternate" type="text/html" title="CMA Evolution Strategy" /><published>2022-09-14T13:00:00+07:00</published><updated>2022-09-14T13:00:00+07:00</updated><id>http://localhost:4000/2022/09/14/cma-es</id><content type="html" xml:base="http://localhost:4000/2022/09/14/cma-es.html"><![CDATA[<blockquote>
  <p>A note on CMA - Evolution Strategy
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#bsc-eqn">Basic equation</a></li>
  <li><a href="#upd-mean">Updating the mean</a></li>
  <li><a href="#adp-cov">Adapting the covariance matrix</a>
    <ul>
      <li><a href="#est-scratch">Estimating from scratch</a></li>
      <li><a href="#rank-lambda-mu-update">Rank-$\gamma$ update</a></li>
      <li><a href="#rank-one-update">Rank-one update</a></li>
      <li><a href="#final-update">Final update</a></li>
    </ul>
  </li>
  <li><a href="#ctrl-sigma">Controlling the step-size</a></li>
  <li><a href="#test-on-rast">Testing on Rastrigin function</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>The <strong>condition number</strong> of a matrix $\mathbf{A}$ is defined by
\begin{equation}
\kappa(\mathbf{A})\doteq\Vert\mathbf{A}\Vert\Vert\mathbf{A}^{-1}\Vert,
\end{equation}
where $\Vert\mathbf{A}\Vert=\sup_{\Vert\mathbf{x}\Vert=1}\Vert\mathbf{Ax}\Vert$.</p>

<p>For $\mathbf{A}$ that is non-singular, $\kappa(\mathbf{A})=\infty$.</p>

<p>For $\mathbf{A}$ which is positive definite, we thus have $\Vert\mathbf{A}\Vert=\lambda_\text{max}$ where $\lambda_\text{max}$ denotes the largest eigenvalue of $\mathbf{A}$, correspondingly $\lambda_\text{min}$ denotes the smallest eigenvalue of $\mathbf{A}$. The condition number of $\mathbf{A}$ therefore can be written as
\begin{equation}
\kappa(\mathbf{A})=\frac{\lambda_\text{max}}{\lambda_\text{min}}\geq 1,
\end{equation}
since corresponding to each eigenvalue $\lambda$ of $\mathbf{A}$, the inverse matrix $\mathbf{A}^{-1}$ takes $1/\lambda$ as its eigenvalue.</p>

<h2 id="bsc-eqn">Basic equation</h2>
<p>In the CMA-ES, a population of new search points is generated by sampling an MVN, in which at generation $t+1$, for $t=0,1,2,\ldots$
\begin{equation}
\mathbf{x}_k^{(t+1)}\sim\boldsymbol{\mu}^{(t)}+\sigma^{(t)}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})\sim\mathcal{N}\left(\boldsymbol{\mu}^{(t)},{\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}\right),\hspace{1cm}k=1,\ldots,\lambda\label{eq:be.1}
\end{equation}
where</p>
<ul>
  <li>$\mathbf{x}_k^{(t+1)}\in\mathbb{R}^n$: the $k$-th sample at generation $t+1$.</li>
  <li>$\boldsymbol{\mu}^{(t)}\in\mathbb{R}^n$: mean of the search distribution at generation $t$.</li>
  <li>$\sigma^{(t)}\in\mathbb{R}$: step-size at generation $t$.</li>
  <li>$\boldsymbol{\Sigma}^{(t)}$: covariance matrix at generation $t$.</li>
  <li>${\sigma^{(t)}}^2\boldsymbol{\Sigma}^{(t)}$: covariance matrix of the search distribution at generation $t$.</li>
  <li>$\lambda\geq 2$: sample size.</li>
</ul>

<h2 id="update-mean">Updating the mean</h2>
<p>The mean $\boldsymbol{\mu}^{(t+1)}$ of the search distribution is defined as the weighted average of $\gamma$ selected points from the sample $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$:
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)},\label{eq:um.1}
\end{equation}
where</p>
<ul>
  <li>$\sum_{i=1}^{\gamma}w_i=1$ with $w_1\geq w_2\geq\ldots\geq w_{\gamma}&gt;0$.</li>
  <li>$\gamma\leq\lambda$: number of selected points.</li>
  <li>$\mathbf{x}_{i:\lambda}^{(t+1)}$: $i$-th best sample out of $\mathbf{x}_1^{(t+1)},\ldots,\mathbf{x}_\lambda^{(t+1)}$ from \eqref{eq:be.1}, i.e. with $f$ is the objective function to be minimized, we have
\begin{equation}
f(\mathbf{x}_{1:\lambda}^{(t+1)})\geq f(\mathbf{x}_{2:\lambda}^{(t+1)})\geq\ldots\geq f(\mathbf{x}_{\lambda:\lambda}^{(t+1)})
\end{equation}</li>
</ul>

<p>We can rewrite \eqref{eq:um.1} as an update rule for the mean $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}^{(t+1)}=\boldsymbol{\mu}^{(t)}+\alpha_\boldsymbol{\mu}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right),
\end{equation}
where $\alpha_\boldsymbol{\mu}\leq 1$ is the learning rate, which is usually set to $1$.</p>

<p>When choosing the weight values $w_i$ and population size $\gamma$ for recombination, we take into account the <strong>variance effective selection mass</strong>, denoted as $\gamma_\text{eff}$, given by
\begin{equation}
\gamma_\text{eff}\doteq\left(\frac{\Vert\mathbf{w}\Vert_1}{\Vert\mathbf{w}\Vert_2}\right)=\frac{\Vert\mathbf{w}\Vert_1^2}{\Vert\mathbf{w}\Vert_2^2}=\frac{1}{\sum_{i=1}^{\gamma}w_i^2}
\end{equation}
where $\mathbf{w}$ is defined as the weight vector
\begin{equation}
\mathbf{w}=(w_1,\ldots,w_\gamma)^\text{T}
\end{equation}</p>

<h2 id="adp-cov">Adapting the covariance matrix</h2>
<p>The covariance matrix can be estimated from scratch using the population of the current generation or can be estimated with covariance matrix from previous generations.</p>

<h3 id="est-scratch">Estimating from scratch</h3>
<p>Rather than using the empirical covariance matrix as an estimator for $\boldsymbol{\Sigma}^{(t)}$, in the CMA-ES, we consider the following estimation
\begin{equation}
\boldsymbol{\Sigma}_\lambda^{(t+1)}=\frac{1}{\lambda{\sigma^{(t)}}^2}\sum_{i=1}^{\lambda}\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_i^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T}\label{eq:es.1}
\end{equation}
Notice that in the above estimation \eqref{eq:es.1}, we have used all of the $\lambda$ samples. We thus can estimate a better covariance matrix by select some of the best individual out of $\lambda$ samples, which is analogous to how we update the mean $\boldsymbol{\mu}$.</p>

<p>In particular, we instead consider the estimation
\begin{equation}
\boldsymbol{\Sigma}_{\gamma}^{(t+1)}=\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T},\label{eq:es.2}
\end{equation}
where $\gamma\leq\lambda$ is the number of selected points; the weights $w_i$ and selected points $\mathbf{x}_{i:\lambda}^{(t+1)}$ are defined as given in the update for $\boldsymbol{\mu}$.</p>

<h3 id="rank-lambda-mu-update">Rank-$\gamma$ update</h3>
<p>In order to ensure that \eqref{eq:es.2} is a reliable estimator, the selected population must be large enough. However, to get a fast search, the population size $\lambda$ must be small, which lets the selected sample size consequently small also. Thus, we can not get a reliable estimator for a good covariance matrix from \eqref{eq:es.2}. However, we can use the history as a helping hand.</p>

<p>In particular, if we have experienced a sufficient number of generations, the mean of the $\boldsymbol{\Sigma}_\gamma$ from all previous generations
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\frac{1}{t+1}\sum_{i=0}^{t}\boldsymbol{\Sigma}_\gamma^{(i+1)}\label{eq:rlmu.1}
\end{equation}
would be a reliable estimator.</p>

<p>In addition, it is reasonable that the recent generations will have more affection to the current generation than the distant ones. Hence, rather than assigning estimated covariance matrices $\boldsymbol{\Sigma}_\gamma$ from preceding generations the same weight as in \eqref{eq:rlmu.1}, it would be a better choice to give the more recent generations the higher weight.</p>

<p>Specifically, starting with an initial $\boldsymbol{\Sigma}^{(0)}=\mathbf{I}$, we consider the update, called <strong>rank-$\gamma$ update</strong>, for the covariance matrix at generation $t+1$ using <strong>exponential smoothing</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\boldsymbol{\Sigma}_\gamma^{(t+1)} \\ &amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\frac{1}{{\sigma^{(t)}}^2}\sum_{i=1}^{\gamma}w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)^\text{T} \\ &amp;=(1-\alpha_\gamma)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},\label{eq:rlmu.2}
\end{align}
where</p>
<ul>
  <li>$\alpha_\gamma\leq 1$: learning rate.</li>
  <li>$w_1,\ldots,w_\gamma$ and $\mathbf{x}_{1:\lambda}^{(t+1)},\ldots,\mathbf{x}_{\lambda:\lambda}^{(g+1)}$ are defined as usual.</li>
  <li>$\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$.</li>
</ul>

<p>The update \eqref{eq:rlmu.2} can be generalized to $\lambda$ weights values which neither necessarily sum to $1$, nor be non-negative anymore, as
\begin{align}
\boldsymbol{\Sigma}^{(t+1)}&amp;=\left(1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T}\label{eq:rlmu.3} \\ &amp;={\boldsymbol{\Sigma}^{(t)}}^{1/2}\left[\mathbf{I}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\left(\mathbf{z}_{i:\lambda}^{(t+1)}{\mathbf{z}_{i:\lambda}^{(t+1)}}^\text{T}-\mathbf{I}\right)\right]{\boldsymbol{\Sigma}^{(t)}}^{1/2},
\end{align}
where</p>
<ul>
  <li>$w_1\geq\ldots\geq w_\gamma&gt;0\geq w_{\gamma+1}\geq\ldots\geq w_\lambda\in\mathbb{R}$, and usually $\sum_{i=1}^{\gamma}w_i=1$ and $\sum_{i=1}^{\lambda}w_i\approx 0$.</li>
  <li>$\mathbf{z}_{i:\lambda}^{(t+1)}={\boldsymbol{\Sigma}^{(t)}}^{1/2}\mathbf{y}_{i:\lambda}^{(t+1)}$ is the mutation vector.</li>
</ul>

<h3 id="rank-one-update">Rank-one-update</h3>
<p>We first consider a method that produces an $n$-dimensional normal distribution with zero mean. Specifically, let $\mathbf{y}_1,\ldots,\mathbf{y}_{t_0}\in\mathbb{R}^n$, for $t_0\geq n$ be vectors span $\mathbb{R}^n$. We thus have that
\begin{align}
\mathcal{N}(0,1)\mathbf{y}_1+\ldots+\mathcal{N}(0,1)\mathbf{y}_{t_0}&amp;\sim\mathcal{N}(\mathbf{0},\mathbf{y}_1\mathbf{y}_1^\text{T})+\ldots+\mathcal{N}(\mathbf{0},\mathbf{y}_{t_0}\mathbf{y}_{t_0}^\text{T}) \\ &amp;\sim\mathcal{N}\left(\mathbf{0},\sum_{i=1}^{t_0}\mathbf{y}_i\mathbf{y}_i^\text{T}\right)
\end{align}
The covariance matrix $\mathbf{y}_i\mathbf{y}_i^\text{T}$ has rank one, with only one eigenvalue $\Vert\mathbf{y}_i\Vert^2$ and a corresponding eigenvector within the form $\alpha\mathbf{y}_i$ for $\alpha\in\mathbb{R}$. Using the above equation, we can generate any MVN distribution.</p>

<p>Consider the update \eqref{eq:rlmu.3} with $\gamma=1$ and let $\mathbf{y}_{t+1}=\left(\mathbf{x}_{1:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$, the <strong>rank-one update</strong> for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ is given by
\begin{equation}
\boldsymbol{\Sigma}^{t+1}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{y}_{t+1}\mathbf{y}_{t+1}^\text{T}
\end{equation}
The latter summand in the RHS has rank one and adds the maximum likelihood term for $\mathbf{y}_{t+1}$ into the covariance matrix $\boldsymbol{\Sigma}^{(t)}$, which makes the probability of generating $\mathbf{y}_{t+1}$ in the generation $t+1$ increase.</p>

<p>We continue by noticing that to update the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$, in \eqref{eq:rlmu.3}, we have used the selected steps
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}=\frac{\mathbf{x}_{i:\lambda}^{(g+1)}-\boldsymbol{\mu}^{(g)}}{\sigma^{(g)}}
\end{equation}
However, since
\begin{equation}
\mathbf{y}_{i:\lambda}^{(g+1)}{\mathbf{y}_{i:\lambda}^{(g+1)}}^\text{T}=-\mathbf{y}_{i:\lambda}^{(g+1)}\left(-\mathbf{y}_{i:\lambda}^{(g+1)}\right)^\text{T},
\end{equation}
which means the sign information is lost when computing the covariance matrix. To track the sign information to the update rule of $\boldsymbol{\Sigma}^{(t+1)}$, we use <strong>evolution path</strong>, which defined as a sequence of successive steps over number of generations.</p>

<p>In particular, analogy to \eqref{eq:rlmu.3}, we use exponential smoothing to establish the evolution path, $\mathbf{p}_c\in\mathbb{R}^n$, which starting with an initial value $\mathbf{p}_c^{(0)}=\mathbf{0}$ and being updated with
\begin{align}
\mathbf{p}_c^{(t+1)}&amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{(1-(1-\alpha_c)^2)\mu_\text{eff}}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\sum_{i=1}^{\gamma}\frac{w_i\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)}{\sigma^{(t)}} \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;=(1-\alpha_c)\mathbf{p}_c^{(t)}+\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where</p>
<ul>
  <li>$\mathbf{p}_c^{(t)}\in\mathbb{R}^n$ is the evolution path at generation $t$.</li>
  <li>$\alpha_c\leq 1$ is the learning rate.</li>
  <li>$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_c^{(t+1)}$ such that
\begin{equation}
\mathbf{p}_c^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),
\end{equation}
since by $\mathbf{y}_{i:\lambda}^{(t+1)}=(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)})/\sigma^{(t)}$ we have that
\begin{equation}
\mathbf{p}_c^{(t)}\sim\mathbf{y}_{i:\lambda}^{(t+1)}\sim\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}),\hspace{1cm}\forall i=1,\ldots,\gamma
\end{equation}
which by $\gamma_\text{eff}=\left(\sum_{i=1}^{\gamma}w_i^2\right)^{-1}$ implies that
\begin{equation}
\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)}\sim\frac{1}{\sqrt{\gamma_\text{eff}}}\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma})
\end{equation}</li>
</ul>

<p>The <strong>rank-one update</strong> for the covariance matrix $\boldsymbol{\Sigma}^{(t)}$ via the evolution path $\mathbf{p}_c^{(t+1)}$ then given as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=(1-\alpha_1)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T},\label{eq:rou.1}
\end{equation}
An empirical validated choice for the learning rate $\alpha_1$ is $\alpha_1\approx 2/n^2$.</p>

<h3 id="final-update">Final update</h3>
<p>Combining rank-$\gamma$ update \eqref{eq:rlmu.3} and rank-one update \eqref{eq:rou.1} together, we obtain the final update for the covariance matrix $\boldsymbol{\Sigma}^{(t+1)}$ as
\begin{equation}
\boldsymbol{\Sigma}^{(t+1)}=\left(1-\alpha_1-\alpha_\gamma\sum_{i=1}^{\lambda}w_i\right)\boldsymbol{\Sigma}^{(t)}+\alpha_1\mathbf{p}_c^{(t+1)}{\mathbf{p}_c^{(t+1)}}^\text{T}+\alpha_\gamma\sum_{i=1}^{\lambda}w_i\mathbf{y}_{i:\lambda}^{(t+1)}{\mathbf{y}_{i:\lambda}^{(t+1)}}^\text{T},
\end{equation}
where</p>
<ul>
  <li>$\alpha_1\approx 2/n^2$.</li>
  <li>$\alpha_\gamma\approx\min(\gamma_\text{eff}/n^2,1-\alpha_1)$.</li>
  <li>$\mathbf{y}_{i:\lambda}^{(t+1)}=\left(\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}\right)/\sigma^{(t)}$.</li>
  <li>$\sum_{i=1}^{\lambda}w_i\approx-\alpha_1/\alpha_\gamma$.</li>
</ul>

<h2 id="ctrl-sigma">Controlling the step-size</h2>
<p>To control the step-size $\sigma^{(t)}$, similar to how we cumulatively update the covariance matrix by rank-one covariance matrices, we also use an evolution path, which is defined as sum of successive steps $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$.</p>

<p>However, in this step-size adaption, we utilize a conjugate evolution path $\mathbf{p}_\sigma$, which begins with an initial value $\mathbf{p}_\sigma^{(0)}=\mathbf{0}$ and is repeatedly updated by
\begin{align}
\mathbf{p}_\sigma^{(t+1)}&amp;=(1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{(1-(1-\alpha_\sigma)^2)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\mathbf{y}_{i:\lambda}^{(t+1)} \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\sum_{i=1}^{\gamma}w_i\frac{\mathbf{x}_{i:\lambda}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}} \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{1}{\sigma^{(t)}}\left[\left(\sum_{i=1}^{\gamma}w_i\mathbf{x}_{i:\lambda}^{(t+1)}\right)-\boldsymbol{\mu}^{(t)}\sum_{i=1}^{\gamma}w_i\right] \\ &amp;=1-\alpha_\sigma)\mathbf{p}_\sigma^{(t)}+\sqrt{\alpha_\sigma(2-\alpha_\sigma)\gamma_\text{eff}}{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\frac{\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}}{\sigma^{(t)}},
\end{align}
where</p>
<ul>
  <li>$\mathbf{p}_\sigma^{(t)}\in\mathbb{R}^n$ is the conjugate evolution path at generation $t$.</li>
  <li>$\alpha_\sigma&lt;1$ is the learning rate.</li>
  <li>$\sqrt{\alpha_c(2-\alpha_c)\gamma_\text{eff}}$ is a normalization factor for $\mathbf{p}_\sigma^{(t+1)}$, which analogously to the covariance matrix adaption, lets
\begin{equation}
\mathbf{p}_\sigma^{(t+1)}\sim\mathcal{N}(\mathbf{0},\mathbf{I})
\end{equation}</li>
  <li>The covariance matrix ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$ is defined as
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1/2}\doteq\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T},\label{eq:cs.1}
\end{equation}
where
\begin{equation}
\hspace{-0.8cm}\boldsymbol{\Sigma}^{(t)}=\mathbf{Q}^{(t)}\boldsymbol{\Lambda}^{(t)}{\mathbf{Q}^{(t)}}^\text{T}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{q}_1^{(t)}&amp;\ldots&amp;\mathbf{q}_n^{(t)} \\ \vert&amp;&amp;\vert\end{matrix}\right]\left[\begin{matrix}\lambda_1^{(t)}&amp;&amp; \\ &amp;\ddots&amp; \\ &amp;&amp; \lambda_n^{(t)}\end{matrix}\right]\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{q}_1^{(t)}&amp;\ldots&amp;\mathbf{q}_n^{(t)} \\ \vert&amp;&amp;\vert\end{matrix}\right]^\text{T}
\end{equation}
is an eigendecomposition of the positive definite covariance matrix $\boldsymbol{\Sigma}^{(t)}$, where $\mathbf{Q}^{(t)}\in\mathbb{R}^{n\times n}$ is an orthonormal matrix whose columns are unit eigenvectors $\mathbf{q}_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$ and $\boldsymbol{\Lambda}^{(t)}\in\mathbb{R}^{n\times n}$ is a diagonal matrix whose diagonal entries are eigenvalues $\lambda_i^{(t)}$ of $\boldsymbol{\Sigma}^{(t)}$.<br />
Moreover, for each eigenvalue, eigenvector pair $(\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ of $\boldsymbol{\Sigma}^{(t)}$ we have
\begin{equation}
\lambda_i^{(t)}{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}={\boldsymbol{\Sigma}^{(t)}}^{-1}\boldsymbol{\Sigma}^{(t)}\mathbf{q}_i^{(t)}=\mathbf{q}_i^{(t)},
\end{equation}
or
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}\mathbf{q}_i^{(t)}=\frac{1}{\lambda_i^{(t)}}\mathbf{q}_i^{(t)},
\end{equation}
or in other words, $(1/\lambda_i^{(t)},\mathbf{q}_i^{(t)})$ is an eigenvalue, eigenvector pair of ${\boldsymbol{\Sigma}^{(t)}}^{-1}$. Therefore, the inverse of $\boldsymbol{\Sigma}^{(t)}$, which is also positive definite can be written by
\begin{equation}
{\boldsymbol{\Sigma}^{(t)}}^{-1}=\mathbf{Q}^{(t)}\left[\begin{matrix}1/\lambda_1^{(t)}&amp;&amp; \\ &amp;\ddots&amp; \\ &amp;&amp; 1/\lambda_n^{(t)}\end{matrix}\right]{\mathbf{Q}^{(t)}}^\text{T}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1}{\mathbf{Q}^{(t)}}^\text{T},
\end{equation}
which allows us to obtain the representation \eqref{eq:cs.1} of ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}$.</li>
</ul>

<p>The transformation ${\boldsymbol{\Sigma}^{(t)}}^{-1/2}=\mathbf{Q}^{(t)}{\boldsymbol{\Lambda}^{(t)}}^{-1/2}{\mathbf{Q}^{(t)}}^\text{T}$ re-scales length of the step $\boldsymbol{\mu}^{(t+1)}-\boldsymbol{\mu}^{(t)}$ without changing its direction. In more specific:</p>
<ul id="number-list">
	<li>
		${\mathbf{Q}^{(t)}}^\text{T}$ transform the original space into the coordinate space with columns of $\mathbf{Q}^{(t)}$, which is also the eigenvectors of $\boldsymbol{\Sigma}^{(t)}$ or the principle axes of $\mathcal{N}(\mathbf{0},\boldsymbol{\Sigma}^{(t)})$, as its principle axes.
	</li>
	<li>
		${\boldsymbol{\Lambda}^{(t)}}^{-1/2}$ re-scales the principle axes to have the same length.
	</li>
	<li>
		$\mathbf{Q}^{(t)}$ transforms the coordinate system back to the original space.
	</li>
</ul>

<p>It means that this transformation makes the expected length of $\mathbf{p}_\sigma^{(t+1)}$ independent of its direction.</p>

<p>We then update $\sigma^{(t)}$ by according to the ratio of its length with its expected length $\Vert\mathbf{p}_\sigma^{(t+1)}\Vert/\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert$, given by
\begin{equation}
\log\sigma^{(t+1)}=\log\sigma^{(t)}+\frac{\alpha_\sigma}{d_\sigma}\left(\frac{\Vert\mathbf{p}_\sigma^{(t+1)}\Vert}{\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert}-1\right),
\end{equation}
where $d_\sigma\approx 1$ is the <strong>damping parameter</strong>, which controls the update size. Therefore, since $\sigma^{(t)}&gt;0$, we have the update rule for $\sigma^{(t)}$ is given by
\begin{equation}
\sigma^{(t+1)}=\sigma^{(t)}\exp\left(\frac{\alpha_\sigma}{d_\sigma}\left(\frac{\Vert\mathbf{p}_\sigma^{(t+1)}\Vert}{\mathbb{E}\Vert\mathcal{N}(\mathbf{0},\mathbf{I})\Vert}-1\right)\right)
\end{equation}</p>

<h2 id="test-on-rast">Testing on Rastrigin function</h2>
<p>Let us give the CMA-ES algorithm a try on the <a href="https://en.wikipedia.org/wiki/Rastrigin_function"><strong>Rastrigin function</strong></a>, $f:\mathbb{R}^n\to\mathbb{R}$, which is given by
\begin{equation}
f(\mathbf{x})=10 n+\sum_{i=1}^{n}x_i^2-10\cos\left(2\pi x_i\right)
\end{equation}
The global minimum of $f(\mathbf{x})$ is $0$ at $\mathbf{x}=\mathbf{0}$. We will be using the experimental settings given in this <a href="#cmaes-exp">paper</a> proposed by CMA-ES’s original author. Each time we end up with a result less than $f_\text{stop}=10^{-10}$, we count it a success run.</p>

<p>The result obtained is illustrated in the following figure.</p>
<figure>
	<img src="/assets/images/2022-09-14/cmaes-rastrigin.png" alt="CMA-ES on rastrigin" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Success rate to reach $f_\text{stop}=10^{-10}$ versus population size for Rastrigin function.<br /> The code can be found <span><a href="https://github.com/trunghng/evolution-strategies/blob/main/testing_ground.py">here</a></span></figcaption>
</figure>

<h2 id="references">References</h2>
<p>[1] Nikolaus Hansen. <a href="https://arxiv.org/abs/1604.00772">The CMA Evolution Strategy: A Tutorial</a>. 	arXiv:1604.00772, 2016.</p>

<p>[2] Nikolaus Hansen, Youhei Akimoto &amp; Petr Baudis. <a href="https://github.com/CMA-ES/pycma">CMA-ES/pycma on Github</a>. Zenodo, <a href="https://doi.org/10.5281/zenodo.2559634">DOI:10.5281/zenodo.2559634</a>, February 2019.</p>

<p><span id="cmaes-exp">[3] Nikolaus Hansen, Stefan Kern. <a href="https://doi.org/10.1007/978-3-540-30217-9_29">Evaluating the CMA Evolution Strategy on Multimodal Test Functions</a>. Parallel Problem Solving from Nature - PPSN VIII. PPSN 2004.</span></p>

<p>[4] Ha, David. <a href="https://blog.otoro.net/2017/10/29/visual-evolution-strategies/">A Visual Guide to Evolution Strategies</a>. blog.otoro.net, 2017.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The simplest form of <strong>exponential smoothing</strong> is given by the formula
\begin{align*}
s_0&amp;=x_0 \\ s_t&amp;=\alpha x_t+(1-\alpha)s_{t-1},\hspace{1cm}t&gt;0
\end{align*}
where $0&lt;\alpha&lt;1$ is referred as the <strong>smoothing factor</strong>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="evolution-strategy" /><category term="neuroevolution" /><summary type="html"><![CDATA[A note CMA-ES]]></summary></entry><entry><title type="html">Neural networks</title><link href="http://localhost:4000/2022/09/02/neural-nets.html" rel="alternate" type="text/html" title="Neural networks" /><published>2022-09-02T13:00:00+07:00</published><updated>2022-09-02T13:00:00+07:00</updated><id>http://localhost:4000/2022/09/02/neural-nets</id><content type="html" xml:base="http://localhost:4000/2022/09/02/neural-nets.html"><![CDATA[<blockquote>
  <p>A note on Neural networks.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#ff-func">Feed-forward network functions</a>
    <ul>
      <li><a href="#unv-approx">Universal approximation property</a></li>
      <li><a href="#w-s-sym">Weight-space symmetries</a></li>
    </ul>
  </li>
  <li><a href="#net-training">Network training</a>
    <ul>
      <li><a href="#output-prob-itp">Network outputs probabilistic interpretation</a>
        <ul>
          <li><a href="#univ-output">Univariate regression</a></li>
          <li><a href="#mult-output">Multivariate regression</a></li>
          <li><a href="#bi-clf">Binary classification</a></li>
          <li><a href="#mult-clf">Multi-class classification</a></li>
        </ul>
      </li>
      <li><a href="#param-opt">Parameter optimization</a></li>
      <li><a href="#backprop">Backpropagation</a>
        <ul>
          <li><a href="#erf-drv">Error-function derivatives</a></li>
          <li><a href="#jacobian-mtx">Jacobian matrix</a></li>
          <li><a href="#hessian-mtx">Hessian matrix</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#bayes-nn">Bayesian neural networks</a>
    <ul>
      <li><a href="#posterior-param-dist">Posterior parameter distribution</a></li>
    </ul>
  </li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="ff-func">Feed-forward network functions</h2>
<p>Recall that the <a href="/2022/08/13/glm.html">linear models</a> used in regression and classification are based on linear combination of fixed nonlinear basis function $\phi_j(\mathbf{x})$ and take the form
\begin{equation}
y(\mathbf{x},\mathbf{w})=f\left(\sum_{j=1}^{M}w_j\phi_j(\mathbf{x})\right),\label{1}
\end{equation}
where in the case of regression, $f$ is the function $f(x)=x$, while in the classification case, $f$ takes the form of a nonlinear activation function (e.g., the <a href="/2022/08/13/glm.html#logistic-sigmoid-func">sigmoid function</a>).</p>

<p><strong>Neural networks</strong> extend this model \eqref{1} by letting each basis functions $\phi_j(\mathbf{x})$ be a nonlinear function of a linear combination of the inputs, where the coefficients in the combination are the adaptive parameters.</p>

<p>More formally, neural networks is a series of layers, in which each layer represents a functional transformation. Let us consider the first layer by constructing $M$ linear combinations of the input variable $x_1,\ldots,x_D$ in the form
\begin{equation}
a_j=\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)},\label{2}
\end{equation}
where</p>
<ul>
  <li>$j=1,\ldots,M$;</li>
  <li>the superscript $^{(1)}$ indicates that we are working with parameters of the first layer;</li>
  <li>$w_{ji}^{(1)}$’s are called the <strong>weights</strong>;</li>
  <li>$w_{j0}^{(1)}$’s are known as the <strong>biases</strong>;</li>
  <li>$a_j$’s are referred as <strong>activations</strong>.</li>
</ul>

<p>The activations $a_j$’s are then transformed using a differentiable, nonlinear <strong>activation function</strong> $h(\cdot)$, which correspond to $f(\cdot)$ in \eqref{1} to give
\begin{equation}
z_j=h(a_j),\label{3}
\end{equation}
where $z_j$ are called the <strong>hidden units</strong>. Repeating the same procedure as \eqref{2}, which was following \eqref{1}, $z_j$’s are taken as the inputs of the second layer to give $K$ outputs
\begin{equation}
a_k=\sum_{j=1}^{M}w_{kj}^{(2)}z_j+w_{k0}^{(2)},\label{4}
\end{equation}
where $k=1,\ldots,K$.</p>

<p>This process will be repeated in $L$ times with $L$ is the number of layers. At the last layer, for instance, the second layer of our example network, the outputs, also called <strong>output unit activations</strong>, $a_k$’s are transformed using an appropriate activation function to give a set of network output $y_k$. For example, in multiple binary classification problems, we can choose the logistic sigmoid as our activation function that
\begin{equation}
y_k=\sigma(a_k)\label{5}
\end{equation}
Combining all these steps \eqref{2}, \eqref{3}, \eqref{4} and \eqref{5} together, our neural network with sigmoidal output unit activation functions can be defined as
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=1}^{M}w_{kj}^{(2)}h\left(\sum_{i=1}^{D}w_{ji}^{(1)}x_i+w_{j0}^{(1)}\right)+w_{k0}^{(2)}\right),\label{6}
\end{equation}
where all of the weights and biases are comprises together into a parameter vector $\mathbf{w}$. As suggested in <a href="/2022/08/13/glm.html#dummy-coeff">linear regression</a>, we can also let the bias $w_{j0}^{(1)}$ be coefficient of a dummy input variable $x_0=1$ that makes \eqref{2} can be written as
\begin{equation}
a_j=\sum_{i=0}^{D}w_{ji}^{(1)}x_i
\end{equation}
This results that our subsequent layers are also able to be written in a more convenient form, which lets the entire network \eqref{6} take the form
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\sigma\left(\sum_{j=0}^{M}w_{kj}^{(2)}h\left(\sum_{i=0}^{D}w_{ji}^{(1)}x_i\right)\right)
\end{equation}
Our network is also an example of a <strong>multilayer perception</strong>, or <strong>MLP</strong>, which is a combination of <a href="/2022/08/13/glm.html#perceptron">perceptron models</a>. The key difference is that while the neural network uses continuous sigmoidal nonlinearities in the hidden units, which is differentiable w.r.t the parameters, the perceptron algorithm uses step-function nonlinearities, which is in contrast non-differentiable.</p>

<p>The network network we have been considering so far is <strong>feed-forward neural network</strong>, whose outputs are deterministic functions of the inputs. Each (hidden or output) unit in such a network computes a function given by
\begin{equation}
z_k=h\left(\sum_{j}w_{kj}z_j\right),
\end{equation}
where the sum runs all over units sending connections to unit $k$ (bias included).</p>

<h3 id="unv-approx">Universal approximation property</h3>
<p>Feed-forward networks with <strong>hidden layers</strong> (i.e., the layers in which the training data does not show the desired output, e.g., the first layer of our network, the second layer on the other hands is called the <strong>output layer</strong>) provide <strong>universal approximation</strong> property.</p>

<p>In concrete, the universal approximation theorem states that a feedforward network with a linear output layer and at least one hidden layer with any <strong>squashing</strong> activation function (e.g., the logistic sigmoid function) an approximate any continuous function on a compact subsets of $\mathbb{R}^n$.</p>

<h3 id="w-s-sym">Weight-space symmetries</h3>

<h2 id="net-training">Network training</h2>

<h3 id="output-prob-itp">Network outputs probabilistic interpretation</h3>

<h4 id="univ-output">Univariate regression</h4>
<p>Consider the <a href="/2022/08/13/glm.html#least-squares-reg">regression problem</a> in which the target variable $t$ has Gaussian distribution with an $\mathbf{x}$ dependent mean
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=\mathcal{N}(t\vert y(\mathbf{x},\mathbf{w}),\beta^{-1}),
\end{equation}
For the conditional distribution above, it is sufficient to take the output unit activation function to be the function $h(x)=x$, because such a network can approximate any continuous function from $\mathbf{x}$ to $y$.</p>

<p>Given the data set $(\mathbf{X},\mathbf{t})=\{\mathbf{x}_n,t_n\}$, where $\mathbf{x}_n$’s are i.i.d for $n=1,\ldots,N$, and where
\begin{align}
\mathbf{X}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{x}_1&amp;\ldots&amp;\mathbf{x}_N \\ \vert&amp;&amp;\vert\end{matrix}\right],\hspace{1cm}\mathbf{t}=\left[\begin{matrix}t_1 \\ \vdots \\ t_N\end{matrix}\right]
\end{align}
The likelihood function therefore can be given by
\begin{align}
p(t\vert\mathbf{X},\mathbf{w},\beta)&amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w},\beta) \\ &amp;=\prod_{n=1}^{N}\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1})
\end{align}
With a minor change as usual that taking negative natural logarithm of both sides gives us
\begin{align}
-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w},\beta)&amp;=-\sum_{n=1}^{N}\log\mathcal{N}(t_n\vert y(\mathbf{x}_n,\mathbf{w}),\beta^{-1}) \\ &amp;=\frac{\beta}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2-\frac{N}{2}\log\beta+\frac{N}{2}\log 2\pi
\end{align}
Therefore, maximizing the likelihood function $p(\mathbf{t}\vert\mathbf{X},\mathbf{x},\beta)$ is equivalent to minimizing the sum-of-squares error function given as
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w})-t_n\big)^2,
\end{equation}
This also means the value of $\mathbf{w}$ that minimizes $E(\mathbf{w})$ will be $\mathbf{w}_\text{ML}$, which implies that the corresponding solution for $\beta$ will be given by
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{N}\sum_{n=1}^{N}\big(y(\mathbf{x}_n,\mathbf{w}_\text{ML})-t_n\big)^2
\end{equation}</p>

<h4 id="mult-output">Multivariate regression</h4>
<p>Similarly, we consider the multiple target variables case, in which the conditional distribution of the target therefore takes the form
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}\vert\mathbf{y}(\mathbf{x},\mathbf{w}),\beta^{-1}\mathbf{I})
\end{equation}
Repeating the same procedure as the univariate case, maximizing likelihood function is also equivalent to minimizing the sum-of-squares error function given by
\begin{equation}
E(\mathbf{w})=\frac{1}{2}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w})-\mathbf{t}_n\big\Vert^2,
\end{equation}
which gives us the solution for the noise precision $\beta$ in the multivariate case as
\begin{equation}
\frac{1}{\beta_\text{ML}}=\frac{1}{NK}\sum_{n=1}^{N}\big\Vert\mathbf{y}(\mathbf{x}_n,\mathbf{w}_\text{ML})-\mathbf{t}_n\big\Vert^2,
\end{equation}
where $K$ is the number of target variables.</p>

<h4 id="bi-clf">Binary classification</h4>
<p>Consider the problem of binary classification which outputs $t=1$ to denote class $\mathcal{C}_1$ and otherwise to denote class $\mathcal{C}_2$.</p>

<p>In particular, we consider a network having a single output whose activation function is a logistic sigmoid
\begin{equation}
y=\sigma(a)\doteq\frac{1}{1+\exp(-a)},
\end{equation}
which follows immediately that $0\leq y(\mathbf{x},\mathbf{w})\leq 1$.</p>

<p>This suggests us interpreting $y(\mathbf{x},\mathbf{w})$ as the conditional probability for class $\mathcal{C}_1$, $p(\mathcal{C}_1\vert\mathbf{x})$, and hence the corresponding conditional probability for class $\mathcal{C}_2$ will be $p(\mathcal{C}_2\vert\mathbf{x})=1-y(\mathbf{x},\mathbf{w})$. Or in other words, the conditional distribution $p(t\vert\mathbf{x},\mathbf{w})$ of targets $t$ given inputs $\mathbf{x}$ is then a Bernoulli distribution of the form
\begin{equation}
p(t\vert\mathbf{x},\mathbf{w})=y(\mathbf{x},\mathbf{w})^t\big(1-y(\mathbf{x},\mathbf{w})\big)^{1-t}
\end{equation}
If we consider a training set of $N$ independent observations as in the two regression tasks above, the likelihood function of our classification task will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;=\prod_{n=1}^{N}p(t_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n}
\end{align}
Taking the negative natural logarithm of the likelihood as above gives us the cross-entropy error function
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{t}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}y(\mathbf{x}_n,\mathbf{w})^{t_n}\big(1-y(\mathbf{x}_n,\mathbf{w})\big)^{1-t_n} \\ &amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n),
\end{align}
where $y_n=y(\mathbf{x}_n,\mathbf{w})$.</p>

<p>Moreover, consider the partial derivative of this error function w.r.t the activation $a_i$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_i}&amp;=\frac{\partial}{\partial a_i}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;=-\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_i}-\frac{1-t_i}{1-y_i}\frac{\partial(1-y_i)}{\partial a_i} \\ &amp;=\frac{\partial y_i}{\partial a_i}\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;=y_i(1-y_i)\left(\frac{1-t_i}{1-y_i}-\frac{t_i}{y_i}\right) \\ &amp;=y_i-t_i,\label{eq:bin-clf-drv-error-a}
\end{align}
where in the forth step, we have use the identity of the <a href="/2022/08/13/glm.html#sigmoid-derivative">derivative of sigmoid function</a> that
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}</p>

<h4 id="mult-clf">Multi-class classification</h4>
<p>For the multi-class classification that assigns input variables to $K$ separated classes, we can use the network with $K$ outputs each of which has a logistic sigmoid activation function. Each output $t_k\in\{0,1\}$ for $k=1,\ldots,K$ indicates whether the input will be assigned to class $\mathcal{C}_k$</p>

<p>We first consider the case that the class labels are independent given the input vector, which means the conditional distributions for class $C_k$’s will be $K$ i.i.d Bernoulli distributions, in which the conditional probability for class $\mathcal{C}_k$ will take the form
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w})=y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{equation}
Therefore, the joint distribution of them, the conditional distribution of the target variables will be given as
\begin{align}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})&amp;=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\mathbf{x},\mathbf{w}) \\ &amp;=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x},\mathbf{w})\big)^{1-t_k}
\end{align}
Let $\mathbf{T}$ denote the combination of all the targets $\mathbf{t}_n$, i.e.,
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right],
\end{equation}
the likelihood function therefore takes the form of
\begin{align}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_k}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_k}\label{eq:mult-clf-llh}
\end{align}
Analogy to the binary case, taking the negative natural logarithm of the likelihood \eqref{eq:mult-clf-llh} gives us the corresponding cross-entropy error function for the multi-class case, given as
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}}\big(1-y_k(\mathbf{x}_n,\mathbf{w})\big)^{1-t_{nk}} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}),\label{eq:mult-clf-error}
\end{align}
where $y_{nk}$ is short for $y_k(\mathbf{x}_n,\mathbf{w})$.</p>

<p>Similar to the binary case, consider the partial derivative of the error function \eqref{eq:mult-clf-error} w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk}+(1-t_{nk})\log(1-y_{nk}) \\ &amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)\frac{\partial y_{ij}}{\partial a_{ij}} \\ &amp;=\left(\frac{1-t_{ij}}{1-y_{ij}}-\frac{t_{ij}}{y_{ij}}\right)y_{ij}(1-y_{ij}) \\ &amp;=y_{ij}-t_{ij}\label{eq:mult-drv-error-a}
\end{align}
which takes the same form as \eqref{eq:bin-clf-drv-error-a}</p>

<p>On the other hands, if each input is assigned only to one of $K$ classes (mutually exclusive), the conditional distributions for class $C_k$ will be instead given as
\begin{equation}
p(\mathcal{C}_k\vert\mathbf{x})=p(t_k=1\vert\mathbf{x})=y_k(\mathbf{x},\mathbf{w}),
\end{equation}
and thus the conditional distribution of the targets is
\begin{equation}
p(\mathbf{t}\vert\mathbf{x},\mathbf{w})=\prod_{k=1}^{K}p(t_k=1\vert\mathbf{x})^{t_k}=\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_k}
\end{equation}
The likelihood is therefore given as
\begin{equation}
p(\mathbf{T}\vert\mathbf{X},\mathbf{w})=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{x}_n,\mathbf{w})=\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x}_n,\mathbf{w})^{t_{nk}},
\end{equation}
which gives us the following cross-entropy error function by taking the negative natural logarithm
\begin{align}
E(\mathbf{w})=-\log p(\mathbf{T}\vert\mathbf{X},\mathbf{w})&amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}y_k(\mathbf{x},\mathbf{w})^{t_{nk}} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_k(\mathbf{x}_n,\mathbf{w})\label{eq:mult-me-clf-error}
\end{align}
As discussed in <a href="/2022/08/13/glm.html#softmax-reg">Softmax regression</a>, we see that the output unit activation function is given by the softmax function
\begin{equation}
y_k(\mathbf{x},\mathbf{w})=\frac{\exp\big[a_k(\mathbf{x},\mathbf{w})\big]}{\sum_{j=1}^{K}\exp\big[a_j(\mathbf{x},\mathbf{w})\big]}
\end{equation}
Taking the derivative of the error function \eqref{eq:mult-me-clf-error}  w.r.t to the activation for a particular output unit $a_{ij}$, corresponding to a particular data point $i$, we have
\begin{align}
\frac{\partial E(\mathbf{w})}{\partial a_{ij}}&amp;=\frac{\partial}{\partial a_{ij}}-\sum_{n=1}^{N}\sum_{k=1}^{K}t_{nk}\log y_{nk} \\ &amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}\frac{\partial y_{ik}}{\partial a_{ij}} \\ &amp;=-\sum_{k=1}^{K}\frac{t_{ik}}{y_{ik}}y_{ik}(1\{j=k\}-y_{ij})\label{53} \\ &amp;=y_{ij}\sum_{k=1}^{K}t_{ik}-\sum_{k=1}^{K}t_{ik}1\{j=k\} \\ &amp;=y_{ij}-t_{ij}
\end{align}
where we have used the identity of the <a href="/2022/08/13/glm.html#softmax-derivative">derivative of the softmax function</a> in the forth step to obtain \eqref{53}.</p>

<h3 id="param-opt">Parameter optimization</h3>
<p>In training neural network to find a value of $\mathbf{w}$ to minimize the error function $E(\mathbf{w})$, we usually start with some initial value $\mathbf{w}_0$ and iteratively update the weight vector $\mathbf{w}$, in which the weight at time step $\tau+1$ is given as
\begin{equation}
\mathbf{w}^{(t+1)}=\mathbf{w}^{(\tau)}+\Delta\mathbf{w}^{(\tau)},
\end{equation}
where $\Delta\mathbf{w}^{(\tau)}$ is some update rule.</p>

<p>At each time step $\tau$, there are two distinct stages:</p>
<ul>
  <li>Stage 1 refers to evaluating the derivatives of the error function w.r.t the weights, which can be accomplished efficiently using <strong>backpropagation</strong> that will be discussed in the next section.</li>
  <li>Stage 2 relates to using those computed derivatives to calculate the adjustments to be made to the weights $\mathbf{w}$. <strong>Gradient descent</strong>, for instance, is the simplest approach in which each time step the weights take a small step in the direction of the negative gradient, as
\begin{equation}
\mathbf{w}^{(\tau+1)}=\mathbf{w}^{(\tau)}-\eta\nabla_\mathbf{w}E(\mathbf{w}^{(\tau)}),
\end{equation}
where $\eta&gt;0$ is called the <strong>learning rate</strong> of the update.</li>
</ul>

<h3 id="backprop">Backpropagation</h3>
<p>In this section, we will consider the use of <strong>backpropagation</strong> technique to evaluate the first and second derivatives of error-functions w.r.t the weights and also the derivatives of the network outputs w.r.t the inputs.</p>

<h4 id="erf-drv">Error-function derivatives</h4>
<p>We first consider the case of evaluating the first order derivative of the error function w.r.t to the weight parameter $\mathbf{w}$.</p>

<p>Consider a simple linear model where the outputs $y_k$’s are linear combinations of the input variable $x_i$’s
\begin{equation}
y_k=\sum_{i}w_{ki}x_i,
\end{equation}
together with the error function, in which the error function for the $n$ data point is defined as
\begin{equation}
E_n(\mathbf{w})=\frac{1}{2}\sum_{k}(y_{nk}-t_{nk})^2,
\end{equation}
where $y_{nk}=y_k(\mathbf{x}_n,\mathbf{w})$.</p>

<p>The gradient of this error function w.r.t to a weight $w_{ji}$ then can be computed by
\begin{equation}
\frac{\partial E_n}{\partial w_{ji}}=(y_{nj}-t_{nj})x_{ni}
\end{equation}
In a general feed-forward network, each unit is a weighted sum of its inputs
\begin{equation}
a_j=\sum_{i}w_{ji}z_i
\end{equation}</p>

<h4 id="jacobian-mtx">Jacobian matrix</h4>

<h4 id="hessian-mtx">Hessian matrix</h4>

<h2 id="bayes-nn">Bayesian neural networks</h2>

<h3 id="posterior-param-dist">Posterior parameter distribution</h3>

<h2 id="preferences">Preferences</h2>
<p>[1] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p>

<p>[2] Ian Goodfellow &amp; Yoshua Bengio &amp; Aaron Courville. <a href="https://www.deeplearningbook.org">Deep Learning</a>. MIT Press, 2016.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="neural-network" /><summary type="html"><![CDATA[A note on neural networks]]></summary></entry><entry><title type="html">Measure theory - III: the Lebesgue integral</title><link href="http://localhost:4000/2022/08/21/measure-theory-p3.html" rel="alternate" type="text/html" title="Measure theory - III: the Lebesgue integral" /><published>2022-08-21T13:00:00+07:00</published><updated>2022-08-21T13:00:00+07:00</updated><id>http://localhost:4000/2022/08/21/measure-theory-p3</id><content type="html" xml:base="http://localhost:4000/2022/08/21/measure-theory-p3.html"><![CDATA[<blockquote>
  <p>Note III of the measure theory series. Materials are mostly taken from <a href="/2022/08/21/measure-theory-p3.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/08/21/measure-theory-p3.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#int-simp-funcs">Integration of simple functions</a>
    <ul>
      <li><a href="#simp-func">Simple function</a></li>
      <li><a href="#int-unsgn-simp-func">Integral of unsigned simple functions</a></li>
      <li><a href="#well-dfn-simp-int">Well-definedness of simple integral</a></li>
      <li><a href="#alm-evwhr-spt">Almost everywhere and support</a></li>
      <li><a href="#bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</a></li>
      <li><a href="#abs-cvg-simp-int">Absolutely convergence simple integral</a></li>
      <li><a href="#bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</a></li>
    </ul>
  </li>
  <li><a href="#msr-funcs">Measurable functions</a>
    <ul>
      <li><a href="#unsgn-msr-funcs">Unsigned measurable functions</a></li>
      <li><a href="#equiv-ntn-msrb">Equivalent notions of measurability</a></li>
      <li><a href="#cmplx-msrb">Complex measurability</a></li>
      <li><a href="#equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</a></li>
      <li><a href="#eg-msr-func">Examples of measurable function</a></li>
    </ul>
  </li>
  <li><a href="#unsgn-lebesgue-int">Unsigned Lebesgue integrals</a>
    <ul>
      <li><a href="#lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</a></li>
    </ul>
  </li>
  <li><a href="#abs-intb">Absolute integrability</a></li>
  <li><a href="#littlewoods-prncpl">Littlewood’s three principles</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="int-simp-funcs">Integration of simple functions</h2>
<p>Analogy to how the <a href="/2022/06/16/measure-theory-p1.html#riemann-integrability"><strong>Riemann integral</strong></a> was established by using the integral for <a href="/2022/06/16/measure-theory-p1.html#pc-func"><strong>piecewise constant functions</strong></a>, the <strong>Lebesgue integral</strong> is set up using the integral for <strong>simple functions</strong>.</p>

<h3 id="simp-func">Simple function</h3>
<p>A (complex-valued) <strong>simple function</strong> $f:\mathbb{R}^d\to\mathbb{C}$ is a finite linear combination
\begin{equation}
f=c_1 1_{E_1}+\ldots+c_k 1_{E_k},\label{eq:sf.1}
\end{equation}
of indicator functions $1_{E_i}$ of Lebesgue measurable sets $E_i\subset\mathbb{R}^d$ for $i=1,\ldots,k$, for natural number $k\geq 0$ and where $c_1,\ldots,c_k\in\mathbb{C}$ are complex numbers.</p>

<p>An <strong>unsigned simple function</strong> $f:\mathbb{R}^d\to[0,+\infty]$ is given as \eqref{eq:sf.1} but with the $c_i$ taking values in $[0,+\infty]$ rather than $\mathbb{C}$.</p>

<h3 id="int-unsgn-simp-func">Integral of a unsigned simple function</h3>
<p>If $f=c_1 1_{E_1}+\ldots+c_k 1_{E_k}$ is an unsigned simple function, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq c_1 m(E_1)+\ldots+c_k m(E_k),
\end{equation}
which means $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\in[0,+\infty]$.</p>

<h3 id="well-dfn-simp-int">Well-definedness of simple integral</h3>
<p><strong>Lemma 1</strong><br />
<em>Let $k,k’\geq 0$ be natural, $c_1,\ldots,c_k,c_1’,\dots,c_{k’}’\in[0,+\infty]$ and $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’\subset\mathbb{R}^d$ be Lebesgue measurable sets such that the identity
\begin{equation}
c_1 1_{E_1}+\ldots+c_k 1_{E_k}=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’}\label{eq:lemma1.1}
\end{equation}
holds identically on $\mathbb{R}^d$. Then we have</em>
\begin{equation}
c_1 m(E_1)+\ldots+c_k m(E_k)=c_1’ m(E_1’)+\ldots+c_{k’}’ m(E_{k’}’)
\end{equation}</p>

<p><strong>Proof</strong><br />
The $k+k’$ sets $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ partition $\mathbb{R}^d$ into $2^{k+k’}$ disjoint sets, each of which is an intersection of some of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ and their complements<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Removing any sets that are empty, we end up with a partition of $R^d$ of $m$ non-empty disjoint sets $A_1,\ldots,A_m$ for some $0\leq m\leq 2^{k+k’}$. It easily seen that $A_1,\ldots,A_m$ are then Lebesgue measurable due to the Lebesgue measurability of $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$.</p>

<p>With this set up, each of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ are unions of some of the $A_1,\ldots,A_m$. Or in other words, we have
\begin{equation}
E_1=\bigcup_{j\in J_i}A_j,
\end{equation}
and
\begin{equation}
E_{i’}’=\bigcup_{j’\in J_{i’}’}A_j’,
\end{equation}
for all $i=1,\ldots,k$ and $i’=1,\ldots,k’$, and some subsets $J_i,J_{i’}’\subset\{1,\ldots,m\}$. By finite additivity property of Lebesgue measure, we therefore have
\begin{equation}
m(E_i)=\sum_{j\in J_i}m(A_j)
\end{equation}
and
\begin{equation}
m(E_{i’}’)=\sum_{j\in J_{i’}’}m(A_j)
\end{equation}
Hence, the problem remains to show that
\begin{equation}
\sum_{i=1}^{k}c_i\sum_{j\in J_i}m(A_j)=\sum_{i’=1}^{k’}c_{i’}’\sum_{j\in J_{i’}’}m(A_j)\label{eq:lemma1.2}
\end{equation}
Fix $1\leq j\leq m$, we have that at each point $x$ in the non-empty set $A_j$, $1_{E_i}(x)$ is equal to $1_{J_i}(j)$, and similarly $1_{E_{i’}’}(x)$ is equal to $1_{J_{i’}’}(j)$. Then from \eqref{eq:lemma1.1} we have
\begin{equation}
\sum_{i=1}^{k}c_i 1_{J_i}(j)=\sum_{i’=1}^{k’}c_{i’}’1_{J_{i’}’}(j)
\end{equation}
Multiplying both sides by $m(A_j)$ and then summing over all $j=1,\ldots,m$, we obtain \eqref{eq:lemma1.2}</p>

<h3 id="alm-evwhr-spt">Almost everywhere and support</h3>
<p>A property $P(x)$ of a point $x\in\mathbb{R}^d$ is said to hold <strong>(Lebesgue) almost everywhere</strong> in $\mathbb{R}^d$ or for <strong>(Lebesgue) almost every point</strong> $x\in\mathbb{R}^d$, if the set of $x\in\mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure of zero (i.e. $P$ is true outside of a null set).</p>

<p>Two functions $f,g:\mathbb{R}^d\to Z$ into an arbitrary range $Z$ are referred to <strong>agree almost everywhere</strong> if we have $f(x)=g(x)$ almost every $x\in\mathbb{R}^d$.</p>

<p>The <strong>support</strong> of a function $f:\mathbb{R}^d\to\mathbb{C}$ or $f:\mathbb{R}^d\to[0,+\infty]$ is defined to be the set $\{x\in\mathbb{R}^d:f(x)\neq 0\}$ where $f$ is non-zero.</p>

<p><strong>Remark 2</strong></p>
<ul>
  <li>If $P(x)$ holds for almost every $x$, and $P(x)$ implies $Q(x)$, then $Q(x)$ holds for almost every $x$.</li>
  <li>If $P_1(x),P_2(x),\ldots$ are an at most countable family of properties, each of which individually holds for almost every $x$, then they will simultaneously holds for almost every $x$, since the countable union of null sets is still a null set.</li>
</ul>

<h3 id="bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</h3>
<p>Let $f,g:\mathbb{R}^d\to[0,+\infty]$ be simple unsigned functions.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in[0,+\infty]$.
	</li>
	<li>
		<b>Finiteness</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$ iff $f$ is finite almost everywhere, and its support has finite measure.
	</li>
	<li>
		<b>Vanishing</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=0$ iff $f$ is zero almost everywhere.
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Monotonicity</b>. If $f(x)\leq g(x)$ for almost every $x\in\mathbb{R}^d$, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\leq\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
Since $f,g:\mathbb{R}^d\to[0,+\infty]$ are simple unsigned functions, we can assume that
\begin{align}
f&amp;=c_1 1_{E_1}+\ldots+c_k 1_{E_k}, \\ g&amp;=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’},
\end{align}
where $c_1,\ldots,c_k,c_1’,\ldots,c_{k’}’\in[0,+\infty]$.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b><br />
		We have
		\begin{align}
		\hspace{-1cm}\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx&amp;=c_1 m(E_1)+\ldots+c_k m(E_k)+c_1' m(E_1')+\ldots+c_{k'}' m(E_{k'}') \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For any $c\in[0,+\infty]$, we have
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=c\left(c_1 m(E_1)+\ldots+c_k m(E_k)\right) \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Finiteness</b><br />
		Given $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$, then for every $i=1,\ldots,k$ we have that
		\begin{equation}
		c_i m(E_i)&lt;\infty\label{eq:bpsui.1}
		\end{equation}
		Suppose that $f$ is not finite almost everywhere, which means that there exists $1\leq i\leq k$ such that $E_i$ is a non-null set and $c_i=\infty$, or
		\begin{equation}
		c_i m(E_i)=\infty,
		\end{equation}
		which is in contrast with \eqref{eq:bpsui.1}.<br />
		Suppose that the support of $f$ has infinite measure, or in other word
		\begin{equation}
		c_i\neq 0,\hspace{1cm}i=1,\ldots,k\label{eq:bpsui.2}
		\end{equation}
		and
		\begin{equation}
		m\left(\bigcup_{n=1}^{k}E_n\right)=\infty,
		\end{equation}
		Since any $k$ subsets $E_1,\ldots,E_k$ of $\mathbb{R}^d$ partition $\mathbb{R}^d$ into $2^k$ disjoint sets, say $F_1,\ldots,F_{2^k}$. Hence, by finite additivity property of Lebesgue measure, we have
		\begin{equation}
		\sum_{n=1}^{2^k}m(F_n)=\infty,
		\end{equation}
		which implies that there exists $1\leq n\leq 2^k$ such that $m(F_n)=\infty$. And therefore, for a particular $1\leq i\leq k$ such that $F_n\subset E_i$, by monotonicity property of Lebesgue measure
		\begin{equation}
		m(E_i)\geq m(F_n)=\infty
		\end{equation}
		Thus, combining with \eqref{eq:bpsui.2} gives us
		\begin{equation}
		c_i m(E_1)=\infty,
		\end{equation}
		which again contradicts to \eqref{eq:bpsui.1}.<br />
		Given $f$ is finite almost everywhere and its support has finite measure, suppose that its integral is infinite, or
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=\infty,
		\end{equation}
		which implies that there exists $1\leq i\leq k$ such that either<br />
		(1) $c_i=\infty$ and $E_i$ is a non-null set, or<br />
		(2) $c_i\neq 0$ and $m(E)=\infty$.<br />
		If (1) happens, we then have that
		\begin{equation}
		f\geq c_i 1_{E_i}=\infty,
		\end{equation}
		which contradicts to our hypothesis.<br />
		If (2) happens, by monotonicity of Lebesgue measure, the support of $f$ then has infinite measure, which also contradicts to our hypothesis.
	</li>
	<li>
		<b>Vanishing</b><br />
		Given $\text{Simp}\int_{\mathbf{R^d}}f(x)\,dx=0$, we then have
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=0,
		\end{equation}
		which implies that for every $1\leq i\leq k$, we have that $c_i=0$ or $E_i$ is a null set.
		Therefore, $f$ is zero almost everywhere because in this case $f$ takes the value of non-zero iff $x$ is in a particular null set $E_j$.<br />
		Given $f$ is zero almost everywhere, for every $i=1,\ldots,k$, we have that either<br />
		(1) $c_i=0$, or<br />
		(2) $c_i\neq 0$ and $x\notin E_i$ with $E_i$ is a null set, or<br />
		(3) $c_i=0$ and and $x\notin E_i$ with $E_i$ is a null set.<br />
		Therefore, the integral of $f$
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=c_1 m(E_1)+\ldots+c_k m(E_k)=0
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
		Given $f$ and $g$ agree almost everywhere, we have that at any point $x\in\mathbb{R}^d$ such that $f(x)=g(x)$, by <b>lemma 1</b>, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		For more convenient, let $K=\{E_i\cap E_{i'}':1\leq i\leq k,1\leq i'\leq k'\}$. The set $K$ then has cardinality of $kk'$. Thus, without loss of generality, we can denote $K$ as
		\begin{equation}
		K=\{K_1,\ldots,K_{kk'}\}
		\end{equation}
		With this definition of $K$, the functions $f$ and $g$ can be rewritten by
		\begin{equation}
		f=a_1 1_{K_1}+\ldots+a_{kk'}1_{K_{kk'}}\label{eq:bpsui.3}
		\end{equation}
		and
		\begin{equation}
		g=b_1 1_{K_1}+\ldots+b_{kk'}1_{K_{kk'}}\label{eq:bpsui.4}
		\end{equation}
		On the other hand, the set in which $f(x)\neq g(x)$ is a null set. Thus by \eqref{eq:bpsui.3} and \eqref{eq:bpsui.4}, we have $x\in A$, where some $A\subset K$ is a null set, and for each $i$ such that $K_i\subset A$ (thus is also a null set, or $m(K_i)=0$), $a_i\neq b_i$, otherwise if $K_i\notin A$, $a_i=b_i$. Therefore, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		which proves our claim.
	</li>
	<li>
		<b>Monotonicity</b><br />
		Using the same procedure as the proof for equivalence, our claim can be proved.
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
		This follows directly from definition
	</li>
</ul>

<h3 id="abs-cvg-simp-int">Absolutely convergence simple integral</h3>
<p>A complex valued simple function $f:\mathbb{R}^d\to\mathbb{C}$ is known as <strong>absolutely integrable</strong> if
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}\vert f(x)\vert\,dx&lt;\infty
\end{equation}
If $f$ is absolutely integrable, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined for real signed $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}f_+(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}f_-(x)\,dx,
\end{equation}
where
\begin{align}
f_+(x)&amp;\doteq\max\left(f(x),0\right), \\ f_-(x)&amp;\doteq\max\left(-f(x),0\right),
\end{align}
and for complex-valued $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}\text{Re}\,f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}\,f(x)\,dx
\end{equation}</p>

<h3 id="bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</h3>
<p>Let $f,g:\mathbb{R}^d\to\mathbb{C}$ be absolutely integrable simple functions</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in\mathbb{C}$. Also we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
We first consider the case of real-valued $f$ and $g$.</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		Using the identity
		\begin{equation}
		f+g=(f+g)_{+}-(f+g)_{-}=(f_{+}-f_{-})+(g_{+}-g_{-})
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>
<p>For complex-valued $f$ and $g$ we have:</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		By definition of complex-valued simple integral and by linearity of simple unsigned integral we have
		\begin{align}
		&amp;\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx\nonumber \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}(f(x)+g(x))\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}(f(x)+g(x))\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}\text{Re}g(x)\,dx\nonumber \\ &amp;\hspace{2cm}+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}g(x)\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For the complex conjugate $\overline{f}$, we have its integral can be written as
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx-\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{align}
		Also, for any $c\in\mathbb{C}$, using linearity of simple unsigned integrals once again gives us
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}c\,\text{Re}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}c\,\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+c i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>

<h2 id="msr-funcs">Measurable functions</h2>
<p>Just as how the piecewise constant integral can be extended to the Riemann integral, the unsigned simple integral can be extended to the unsigned Lebesgue integral, by expanding the class of unsigned simple functions to the broader class of <strong>unsigned Lebesgue measurable functions</strong>.</p>

<h3 id="unsgn-msr-funcs">Unsigned measurable functions</h3>
<p>An unsigned function $f:\mathbb{R}^d\to[0,+\infty]$ is <strong>unsigned Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise limit of unsigned simple functions, i.e. if there exists a sequence $f_1,f_2,\ldots:\mathbb{R}\to[0,+\infty]$ of unsigned simple functions such that $f_n(x)\to f(x)$ for every $x\in\mathbb{R}^d$.</p>

<h3 id="equiv-ntn-msrb">Equivalent notions of measurability</h3>
<p><strong>Lemma 3</strong><br />
Let $f:\mathbb{R}\to[0,+\infty]$ be an unsigned function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is unsigned Lebesgue measurable.
	</li>
	<li>
		$f$ is the pointwise limit of unsigned simple functions $f_n$ (hence $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for all $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of unsigned simple function $f_n$ (thus $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for almost every $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f(x)=\sup_n f_n(x)$, where $0\leq f_1\leq f_2\leq\ldots$ is an increasing sequence of unsigned simple functions, each of which are bounded with finite measure support.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&gt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\geq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&lt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\leq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every interval $I\subset[0,+\infty)$, the set $f^{-1}(I)\doteq\{x\in\mathbb{R}^d:f(x)\in I\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) open set $U\subset[0,+\infty)$, the set $f^{-1}(U)\doteq\{x\in\mathbb{R}^d:f(x)\in U\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) closed set $K\subset[0,+\infty)$, the set $f^{-1}(K)\doteq\{x\in\mathbb{R}^d:f(x)\in K\}$ is Lebesgue measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="eg-msr-func">Examples of measurable function</h3>
<ul id="roman-list">
	<li>
		Every continuous function $f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		Every unsigned simple function is measurable.
	</li>
	<li>
		The supremum, infimum, limit superior, or limit inferior of unsigned measurable functions is unsigned measurable.
	</li>
	<li>
		An unsigned function that is equal almost everywhere to an unsigned measurable function, is also measurable.
	</li>
	<li>
		If a sequence $f_n$ of unsigned measurable functions converges pointwise almost everywhere to an unsigned limit $f$, then $f$ is also measurable.
	</li>
	<li>
		If $f:\mathbb{R}^d\to[0,+\infty]$ is measurable and $\phi:[0,+\infty]\to[0,+\infty]$ is continuous, then $\phi\circ f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		If $f,g$ are unsigned measurable functions, then $f+g$ and $fg$ are measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="cmplx-msrb">Complex measurability</h3>
<p>An almost everywhere defined complex-valued function $f:\mathbb{R}^d\to\mathbb{C}$ is <strong>Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise almost everywhere limit of complex-valued simple functions.</p>

<h3 id="equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</h3>
<p>Let $f:\mathbb{R}^d\to\mathbb{C}$ be an almost everywhere defined complex-valued function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is measurable.
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of complex-valued simple functions.
	</li>
	<li>
		The (magnitudes of the) positive and negative parts of $\text{Re}(f)$ and $\text{Im}(f)$ are unsigned measurable functions.
	</li>
	<li>
		$f^{-1}(U)$ is Lebesgue measurable for every open set $U\subset\mathbb{C}$.
	</li>
	<li>
		$f^{-1}(K)$ is Lebesgue measurable for every closed set $K\subset\mathbb{C}$.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h2 id="unsgn-lebesgue-int">Unsigned Lebesgue integrals</h2>

<h3 id="lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</h3>
<p>Let $f:\mathbb{R}^d\to[0,+\infty]$ be an unsigned functions (not necessarily measurable). We define the <strong>lower unsigned Lebesgue integral</strong>, denoted as $\underline{\int_{\mathbb{R}^d}}f(x)\,dx$, to be the quantity
\begin{equation}
\underline{\int_\mathbb{R}^d}f(x)\,dx\doteq\sup_{0\leq g\leq f;g\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx,
\end{equation}
where $g$ ranges over all unsigned simple functions $g:\mathbb{R}^d\to[0,+\infty]$ that are pointwise bounded by $f$.</p>

<p>We can also define the <strong>upper unsigned Lebesgue integral</strong> as
\begin{equation}
\overline{\int_\mathbb{R}^d}f(x)\,dx\doteq\inf_{h\geq f;h\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}h(x)\,dx
\end{equation}</p>

<h2 id="abs-intb">Absolute integrability</h2>

<h2 id="littlewoods-prncpl">Littlewood’s three principles</h2>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>.</span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It should be simpler to consider the case of $k=2$, in particular with two sets $E_1,E_2\subset\mathbb{R}^d$. These two sets partition $\mathbb{R}^d$ into four disjoint sets: $E_1\cap E_2,E_1\cap E_2^c,E_1^c\cap E_2,E_1^c\cap E_2^c$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-integral" /><summary type="html"><![CDATA[Note on measure theory part 3]]></summary></entry><entry><title type="html">The Convergence of Q-learning</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence.html" rel="alternate" type="text/html" title="The Convergence of Q-learning" /><published>2022-08-21T07:00:00+07:00</published><updated>2022-08-21T07:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/08/21/q-learning-convergence.html"><![CDATA[<blockquote>
  <p>A note on convergence proofs for Q-learning by exploiting the connection with stochastic approximation and the idea of parallel asynchronous.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#q-learning-convergence">The convergence of Q-learning</a></li>
  <li><a href="#preferences">Preferences</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>In Q-learning, transition probabilities and costs are unknown but information of them is obtained either by simulation or by experimenting. Q-learning uses simulation or experimental information to estimate the expected cost-to-go. Additionally, the algorithm is recursive and each new piece of information is used for computing an additive correction term to the old estimates. As these correction terms are random, Q-learning therefore has the same general structure as the stochastic approximation algorithms.</p>

<h2 id="preliminaries">Preliminaries</h2>

<h2 id="q-learning-convergence">The convergence of Q-learning</h2>

<h2 id="preferences">Preferences</h2>
<p>[1] T. Jaakkola &amp; M. I. Jordan &amp; S. P. Singh. <a href="doi: 10.1162/neco.1994.6.6.1185">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a> in Neural Computation, vol. 6, no. 6, pp. 1185-1201, Nov. 1994.</p>

<p>[2] Dvoretzky A. <a href="https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Stochastic-Approximation/bsmsp/1200501645?tab=ArticleFirstPage">On stochastic approximation</a>. Berkeley Symposium on Mathematical Statistics and Probability, 1956: 39-55 (1956).</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="q-learning" /><category term="dynamic-programming" /><summary type="html"><![CDATA[Convergence of Q-learning, TD(lambda)]]></summary></entry><entry><title type="html">Generalized Linear Models</title><link href="http://localhost:4000/2022/08/13/glm.html" rel="alternate" type="text/html" title="Generalized Linear Models" /><published>2022-08-13T13:00:00+07:00</published><updated>2022-08-13T13:00:00+07:00</updated><id>http://localhost:4000/2022/08/13/glm</id><content type="html" xml:base="http://localhost:4000/2022/08/13/glm.html"><![CDATA[<blockquote>
  <p>Linear models for solving both regression and classification problems are members of a broader family named Generalized Linear Models.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#ind-basis">Independence, basis in vector space</a>
        <ul>
          <li><a href="#lin-ind">Linear independence</a></li>
          <li><a href="#basis">Basis of a vector space</a></li>
        </ul>
      </li>
      <li><a href="#lagrange-mult">Lagrange Multipliers</a></li>
    </ul>
  </li>
  <li><a href="#lin-models-reg">Linear models for Regression</a>
    <ul>
      <li><a href="#lin-basis-func-models">Linear basis function models</a>
        <ul>
          <li><a href="#least-squares-reg">Least squares</a></li>
          <li><a href="#geo-least-squares">Geometrical interpretation of least squares</a></li>
          <li><a href="#lms">The LMS algorithm</a></li>
          <li><a href="#reg-least-squares">Regularized least squares</a></li>
          <li><a href="#mult-outputs">Multiple outputs</a></li>
        </ul>
      </li>
      <li><a href="#bayes-lin-reg">Bayesian Linear Regression</a>
        <ul>
          <li><a href="#param-dist">Parameter distribution</a></li>
          <li><a href="#pred-dist-reg">Predictive distribution</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#lin-models-reg">Linear models for Classification</a>
    <ul>
      <li><a href="#disc-funcs">Discriminant functions</a>
        <ul>
          <li><a href="#least-squares-clf">Least squares</a></li>
          <li><a href="#fisher-ld">Fisher’s linear discriminant</a>
            <ul>
              <li><a href="#fisher-ld-bin-clf">Binary classification</a></li>
              <li><a href="#fisher-ld-clf">Multi-class classification</a></li>
            </ul>
          </li>
          <li><a href="#perceptron">The perceptron algorithm</a></li>
        </ul>
      </li>
      <li><a href="#prob-gen-models">Probabilistic Generative Models</a>
        <ul>
          <li><a href="#gauss-gen-models">Gaussian Generative Models</a>
            <ul>
              <li><a href="#max-likelihood-sols">Maximum likelihood solutions</a>
                <ul>
                  <li><a href="#ggm-bin-clf">Binary classification</a></li>
                  <li><a href="#ggm-clf">Multi-class classification</a></li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#prob-disc-models">Probabilistic Discriminative Models</a>
        <ul>
          <li><a href="#log-reg">Logistic Regression</a></li>
          <li><a href="#softmax-reg">Softmax Regression</a></li>
          <li><a href="#newtons-method">Newton’s method</a>
            <ul>
              <li><a href="#nm-lin-reg">Linear Regression</a></li>
              <li><a href="#nm-log-reg">Logistic Regression</a></li>
              <li><a href="#nm-softmax-reg">Softmax Regression</a></li>
            </ul>
          </li>
        </ul>
      </li>
      <li><a href="#bayes-log-reg">Bayesian Logistic Regression</a>
        <ul>
          <li><a href="#laplace-approx">The Laplace approximation</a></li>
          <li><a href="#approx-posterior">Approximation of the posterior</a></li>
          <li><a href="#pred-dist-clf">Predictive distribution</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#glm">Generalized linear models</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="ind-basis">Independence, basis in vector space</h3>

<h4 id="lin-ind">Linear independence</h4>
<p>The sequence of vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ is said to be <strong>linearly independent</strong> (or <strong>independent</strong>) if
\begin{equation}
c_1\mathbf{x}_1+\ldots+c_n\mathbf{x}_n=\mathbf{0}
\end{equation}
only when $c_1,\ldots,c_n$ are all zero.</p>

<p>Considering those $n$ vectors $\mathbf{x}_1,\ldots,\mathbf{x}_n$ as $n$ columns of a matrix $\mathbf{A}$
\begin{equation}
\mathbf{A}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{x}_1 &amp; \ldots &amp; \mathbf{x}_n \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
we have that the columns of $\mathbf{A}$ are independent when
\begin{equation}
\mathbf{A}\mathbf{x}=\mathbf{0}\hspace{0.5cm}\Leftrightarrow\hspace{0.5cm}\mathbf{x}=\mathbf{0},
\end{equation}
or in other words, the rank of $\mathbf{A}$ is equal to the number of columns of $\mathbf{A}$.</p>

<h4 id="basis">Basis of a vector space</h4>
<p>We say that vectors $\mathbf{v}_1,\ldots,\mathbf{v}_k$ span a space $S$ when the space consists of all combinations of those vectors. Or in other words, any vector $\mathbf{u}\in S$ can be displayed as linear combination of $\mathbf{v}_i$.<br />
In this case, $S$ is the smallest space containing those vectors.</p>

<p>A <strong>basis</strong> for a vector space $S$ is a sequence of vectors $\mathbf{v}_1,\ldots,\mathbf{v}_d$ having two properties:</p>
<ul id="roman-list">
	<li>$\mathbf{v}_1,\ldots,\mathbf{v}_d$ are independent</li>
	<li>$\mathbf{v}_1,\ldots,\mathbf{v}_d$ span $S$</li>
</ul>
<p>In $S$, every basis for that space has the same number of vectors, which is the dimension of $S$. Therefore, there are exactly $n$ vectors in every basis for $\mathbb{R}^n$.</p>

<p>With that definition of a basis $\mathbf{v}_1,\dots,\mathbf{v}_d$ of $S$, for each vector $\mathbf{u}\in S$, there exists only one sequence $c_1,\ldots,c_d$ such that
\begin{equation}
\mathbf{u}=c_1\mathbf{v}_1+\ldots+c_d\mathbf{v}_d
\end{equation}</p>

<h3 id="lagrange-mult">Lagrange Multipliers</h3>
<p>Consider the problem of finding the maximum (or minimum) of $w=f(x_1,x_2,x_3)$ subject to a constraint relating $x_1,x_2$ and $x_3$
\begin{equation}
g(x_1,x_2,x_3)=0
\end{equation}
Apart from solving $x_3$ in terms of $x_1$ and $x_2$ in the constraint and substituting into the original function, which now becomes an unconstrained, here we can also solve this problem as a constrained one.</p>

<p>The idea is we are using the observation that the gradient vector $\nabla f(\mathbf{x})$ and $\nabla g(\mathbf{x})$ are parallel, because:</p>

<p>Suppose $f(\mathbf{x})$ has a local maximum at $\mathbf{x}^*$ on the constraint surface $g(\mathbf{x})=0$.</p>

<p>Let $\mathbf{r}(t)=\langle x_1(t),x_2(t),x_3(t)\rangle$ be a parameterized curve on the constraint surface such that and $\mathbf{r}(t)$ has
\begin{equation}
(x_1(0),x_2(0),x_3(0))^\text{T}=\mathbf{x}
\end{equation}
And also, let $h(t)=f(x_1(t),x_2(t),x_3(t))$, then it implies that $h$ has a maximum at $t=0$, which lets
\begin{equation}
h’(0)=0
\end{equation}
Taking the derivative of $h$ w.r.t, we obtain
\begin{equation}
h’(t)=\nabla f(\mathbf{x})\big\vert_{\mathbf{r}(t)}\mathbf{r}’(t)
\end{equation}
Therefore,
\begin{equation}
\nabla f(\mathbf{x})\big\vert_{\mathbf{x}^*}\mathbf{r}’(0)=0,
\end{equation}
which implies that $\nabla f(\mathbf{x})$ is perpendicular to any curve in the constraint space that goes through $\mathbf{x}^*$. And since $\nabla g(\mathbf{x})$ perpendicular to the constraint surface $g(x)=0$, then $\nabla g(\mathbf{x})$ is also perpendicular to those curves. This implies that $\nabla f(\mathbf{x})$ is parallel to $\nabla g(\mathbf{x})$.</p>

<p>With this property, we can write $\nabla f(\mathbf{x})$ in terms of $\nabla g(\mathbf{x})$, as
\begin{equation}
\nabla f(\mathbf{x})=\lambda\nabla g(\mathbf{x}),
\end{equation}
where $\lambda\neq 0$ is a constant called <strong>Lagrange multiplier</strong>.</p>

<p>With this definition of Lagrange multiplier, we continue to define the <strong>Lagrangian</strong> function, given as
\begin{equation}
\mathcal{L}(\mathbf{x},\lambda)=f(\mathbf{x})+\lambda g(\mathbf{x})
\end{equation}
Then letting the partial derivative of Lagrangian w.r.t $\lambda$ be zero gives us the constraint
\begin{equation}
0=\frac{\partial \mathcal{L}(\mathbf{x},\lambda)}{\partial\lambda}=g(\mathbf{x})
\end{equation}
With Lagrangian, in order to find the maximum of $f(\mathbf{x})$ that satisfies $g(\mathbf{x})=0$, we will instead be trying to solve
\begin{equation}
\max_\mathbf{x}\min_\lambda\mathcal{L}(\mathbf{x},\lambda),
\end{equation}
This can be solved by letting derivatives of the Lagrangian $\mathcal{L}$ w.r.t $x_i$ (i.e. components of $\mathbf{x}$) and $\lambda$ be zero
\begin{equation}
\frac{\partial\mathcal{L}}{\partial x_i}=0,\hspace{1cm}\frac{\partial\mathcal{L}}{\partial\lambda}=0
\end{equation}
and solve for $x_i$ and $\lambda$.</p>

<h2 id="lin-models-reg">Linear models for Regression</h2>
<p>Regression refers to a problem of predicting the value of one or more continuous target variable $t$ given the value of a $D$-dimensional vector $\mathbf{x}$ of input variables.</p>

<h3 id="lin-basis-func-models">Linear basis function models</h3>
<p>The simplest linear model used for regression tasks is <strong>linear regression</strong>, which is defined as a linear combination of the input variables
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1x_1+\ldots+w_Dx_D,
\end{equation}
where $\mathbf{x}=(x_1,\ldots,x_D)^\text{T}$ is the input variables, while $w_i$’s are the parameters parameterizing the space of linear function mapping from the input space $\mathcal{X}$ of $\mathbf{x}$ to $\mathcal{Y}$.</p>

<p>With the idea of spanning a space by its basis vectors, we can generalize it to establishing a function space by linear combinations of simpler basis functions. Or in other words, we can extend the class of models by instead using a linear combination of fixed nonlinear functions of the input variables $\mathbf{x}$, as
\begin{equation}
y(\mathbf{x},\mathbf{w})=w_0+w_1\phi_1(\mathbf{x})+\ldots+w_{M-1}\phi_{M-1}(\mathbf{x})=w_0+\sum_{i=1}^{M-1}w_i\phi_i(\mathbf{x}),\label{eq:lbfm.1}
\end{equation}
where $\phi_i(\mathbf{x})$’s are called the <strong>basis functions</strong>; $w_0$ is called a <strong>bias parameter</strong>. By letting <span id="dummy-coeff">$w_0$</span> be a coefficient corresponding to a dummy basis function $\phi_0(\mathbf{x})=1$, \eqref{eq:lbfm.1} can be written in a more convenient way
\begin{equation}
y(\mathbf{x},\mathbf{w})=\sum_{i=0}^{M-1}w_i\phi_i(\mathbf{x})=\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\label{eq:lbfm.2}
\end{equation}
where $\mathbf{w}=(w_0,\ldots,w_{M-1})^\text{T}$ and $\boldsymbol{\phi}=(\phi_0,\ldots,\phi_{M-1})^\text{T}$, with $\phi_0(\cdot)=1$.</p>

<p>There are various choices of basis functions:</p>
<ul id="number-list">
	<li>
		<b>Polynomial basis</b>. Each basis function $\phi_i$ is a powers of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=x^i
		\end{equation}
		An example of polynomial basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/polynomial-basis.png" alt="polynomial basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Example of polynomial basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
	<li>
		<b>Gaussian basis function</b>. Each basis function $\phi_i$ is a Gaussian function of a $1$-dimensional input $x$
		\begin{equation}
		\phi_i(x)=\exp\left(-\frac{(x-\mu_i)^2}{2\sigma_i^2}\right)
		\end{equation}
		An example of Gaussian basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/gaussian-basis.png" alt="Gaussian basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: Example of Gaussian basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
	<li>
		<b>Sigmoidal basis function</b>. Each basis function $\phi_i$ is defined as
		\begin{equation}
		\phi_i(x)=\sigma\left(\frac{x-\mu_i}{\sigma_i}\right),
		\end{equation}
		where $\sigma(\cdot)$ is the logistic sigmoid function
		\begin{equation}
		\sigma(x)=\frac{1}{1+\exp(-x)}
		\end{equation}
		An example of sigmoidal basis functions is illustrated as below
		<figure>
			<img src="/assets/images/2022-08-13/sigmoidal-basis.png" alt="sigmoidal basis" style="display: block; margin-left: auto; margin-right: auto;" />
			<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: Example of sigmoidal basis functions. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/regression/basis-funcs.py">here</a></span></figcaption>
		</figure>
	</li>
</ul>

<h4 id="least-squares-reg">Least squares</h4>
<p>Assume that the target variable $t$ and the inputs $\mathbf{x}$ is related via the equation
\begin{equation}
t=y(\mathbf{x},\mathbf{w})+\epsilon,
\end{equation}
where $\epsilon$ is an error term that captures random noise such that $\epsilon\sim\mathcal{N}(0,\sigma^2)$, which means the density of $\epsilon$ can be written as
\begin{equation}
p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{\epsilon^2}{2\sigma^2}\right),
\end{equation}
which implies that
\begin{equation}
p(t|\mathbf{x};\mathbf{w},\beta)=\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t-y(\mathbf{x},\mathbf{w}))^2\beta}{2}\right),\label{eq:lsr.1}
\end{equation}
where $\beta=1/\sigma^2$ is the precision of $\epsilon$, or
\begin{equation}
t|\mathbf{x};\mathbf{w},\beta\sim\mathcal{N}(y(\mathbf{x},\mathbf{w}),\beta^{-1})
\end{equation}
Consider a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$ with corresponding target values $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$ and assume that these data points are drawn independently from the distribution above, we obtain the batch version of \eqref{eq:lsr.1}, called the <strong>likelihood function</strong>, given as
\begin{align}
L(\mathbf{w},\beta)=p(\mathbf{t}|\mathbf{X};\mathbf{w},\beta)&amp;=\prod_{i=1}^{N}p(t_i|\mathbf{x}_i;\mathbf{w},\beta) \\ &amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\label{eq:lsr.2}
\end{align}
By maximum likelihood, we will be looking for values of $\mathbf{w}$ and $\beta$ that maximize the likelihood. We do this by considering maximizing a simpler likelihood, called <strong>log likelihood</strong>, denoted as $\ell(\mathbf{w},\beta)$, defined as
\begin{align}
\ell(\mathbf{w},\beta)=\log{L(\mathbf{w},\beta)}&amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right) \\ &amp;=\sum_{i=1}^{N}\log\left[\sqrt{\frac{\beta}{2\pi}}\exp\left(-\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2}\right)\right] \\ &amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\sum_{i=1}^{N}\frac{(t_i-y(\mathbf{x}_i,\mathbf{w}))^2\beta}{2} \\ &amp;=\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\label{eq:lsr.3},
\end{align}
where $E_D(\mathbf{w})$ is the sum-of-squares error function, defined as
\begin{equation}
E_D(\mathbf{w})\doteq\frac{1}{2}\sum_{i=1}^{N}\left(t_i-y(\mathbf{x}_i,\mathbf{w})\right)^2\label{eq:lsr.4}
\end{equation}
Consider the gradient of \eqref{eq:lsr.3} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}\ell(\mathbf{w},\beta)&amp;=\nabla_\mathbf{w}\left[\frac{N}{2}\log\beta-\frac{N}{2}\log(2\pi)-\beta E_D(\mathbf{w})\right] \\ &amp;\propto\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\big(t_i-y(\mathbf{x}_i,\mathbf{w})\big)^2 \\ &amp;=\nabla_\mathbf{w}\frac{1}{2}\sum_{i=1}^{N}\left(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}\big(\mathbf{x}_i\right)\big)^2 \\ &amp;=\sum_{i=1}^{N}(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i))\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}
\end{align}
By gradient descent, letting this gradient to zero gives us
\begin{equation}
\sum_{i=1}^{N}t_i\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}-\mathbf{w}^\text{T}\sum_{i=1}^{N}\boldsymbol{\phi}(\mathbf{x}_i)\boldsymbol{\phi}(\mathbf{x}_i)^\text{T}=0,
\end{equation}
which implies that
\begin{equation}
\mathbf{w}_\text{ML}=\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},\label{eq:lsr.5}
\end{equation}
which is known as the <strong>normal equations</strong> for the least squares problem. In \eqref{eq:lsr.5}, $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M}$ is called the <strong>design matrix</strong>, whose elements are given by $\boldsymbol{\Phi}_{ij}=\phi_j(\mathbf{x}_i)$
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;\ddots&amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
and the quantity
\begin{equation}
\boldsymbol{\Phi}^\dagger\doteq\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}
\end{equation}
is called the <strong>Moore-Penrose pseudoinverse</strong> of the matrix $\boldsymbol{\Phi}$.</p>

<p>On the other hand, consider the gradient of \eqref{eq:lsr.3} w.r.t $\beta$ and set it equal to zero, we obtain
\begin{equation}
\beta=\frac{N}{\sum_{i=1}^{N}\big(t_i-\mathbf{w}_\text{ML}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2}
\end{equation}</p>

<h4 id="geo-least-squares">Geometrical interpretation of least squares</h4>
<p>As mentioned before, we have applied the idea of spanning a vector space by its basis vectors when constructing basis functions.</p>

<p>In particular, consider an $N$-dimensional space whose axes are given by $t_i$, which implies that
\begin{equation}
\mathbf{t}=(t_1,\ldots,t_N)^\text{T}
\end{equation}
is a vector contained in the space.</p>
<figure>
	<img src="/assets/images/2022-08-13/geo-least-squares.png" alt="geometry of least squares" style="display: block; margin-left: auto; margin-right: auto; width: 400px; height: 300px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: Geometrical interpretation of the least-squares solution. The figure is taken from <span><a href="#bishops-book">Bishop’s book</a></span></figcaption>
</figure>

<p>Each basis function $\phi_j(\mathbf{x}_i)$, evaluated at the $N$ data points, then can also be presented as a vector in the same space, denoted by $\boldsymbol{\varphi}_j$, as illustrated in <strong>Figure 4</strong> above. Therefore, the design matrix $\boldsymbol{\Phi}$ can be represented as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \boldsymbol{\varphi}_{0}&amp;\ldots&amp;\boldsymbol{\varphi}_{M-1} \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
When the number $M$ of basis functions is smaller than the number $N$ of data points, the $M$ vectors $\phi_j(\mathbf{x}_i)$ will span a linear subspace $\mathcal{S}$ of $M$ dimensions.</p>

<p>We define $\mathbf{y}$ to be an $N$-dimensional vector whose the $i$-th element is given by $y(\mathbf{x}_i,\mathbf{w})$
\begin{equation}
\mathbf{y}=\big(y(\mathbf{x}_1,\mathbf{w}),\ldots,y(\mathbf{x}_N,\mathbf{w})\big)^\text{T}
\end{equation}
Since $\mathbf{y}$ is a linear combination of $\boldsymbol{\varphi}_i$, then $\mathbf{y}\in\mathcal{S}$.
Then the sum-of-squares error \eqref{eq:lsr.4} is exactly (with a factor of $1/2$) the squared Euclidean distance between $\mathbf{y}$ and $\mathbf{t}$. Therefore, the least square solution to $\mathbf{w}$ is the one that makes $\mathbf{y}$ closest to $\mathbf{t}$.</p>

<p>This solution corresponds to the orthogonal projection of $t$ onto the subspace $S$ spanned by $\boldsymbol{\varphi}_i$, because we have that
\begin{align}
\mathbf{y}^\text{T}(\mathbf{t}-\mathbf{y})&amp;=\left(\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_\text{ML}\right) \\ &amp;=\left(\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right)^\text{T}\left(\mathbf{t}-\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t}\right) \\ &amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}\mathbf{t} \\ &amp;=\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t}-\mathbf{t}^\text{T}\boldsymbol{\Phi}\left(\left(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\right)^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ &amp;=0,
\end{align}</p>

<h4 id="lms">The LMS algorithm</h4>
<p>The <strong>least-means-squares</strong>, or <strong>LMS</strong> algorithm for the sum-of-squares error \eqref{eq:lsr.4}, which start with some initial vector $\mathbf{w}_0$ of $\mathbf{w}$, and repeatedly perform the update
\begin{equation}
\mathbf{w}_{t+1}=\mathbf{w}_t+\eta(t_n-\mathbf{w}_t^\text{T}\boldsymbol{\phi}_n)\boldsymbol{\phi}_n,
\end{equation}
where $\boldsymbol{\phi}_n$ denotes $\boldsymbol{\phi}(\mathbf{x}_n)$, and $\eta$ is called the <strong>learning rate</strong> which controls the update amount.</p>

<h4 id="reg-least-squares">Regularized least squares</h4>
<p>To control over-fitting, in the error function \eqref{eq:lsr.4}, we add an regularization term, which makes the total error function to be minimized take the form
\begin{equation}
E_D(\mathbf{w})+\lambda E_W(\mathbf{w}),\label{eq:rls.1}
\end{equation}
where $\lambda$ is the regularization coefficient that controls the relative importance of the data-dependent error $E_D(\mathbf{w})$ and the regularization term $E_W(\mathbf{w})$. One simple possible form of regularizer is given as
\begin{equation}
E_W(\mathbf{w})=\frac{1}{2}\mathbf{w}^\text{T}\mathbf{w}
\end{equation}
The total error function \eqref{eq:rls.1} then can be written as
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\mathbf{w}^\text{T}\mathbf{w}\label{eq:rls.2}
\end{equation}
Setting the gradient of this error to zero and solving for $\mathbf{w}$, we have the solution
\begin{equation}
\mathbf{w}= (\lambda\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}\label{eq:rls.3}
\end{equation}
This particular choice of regularizer is called <strong>weight decay</strong> because it encourages weight values to decay towards zero in sequential learning.</p>

<p>Another choice of regularizer which is more general lets the regularized error have the form
\begin{equation}
E_D(\mathbf{w})+E_W(\mathbf{w})=\frac{1}{2}\sum_{i=1}^{N}\big(t_i-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^2+\frac{\lambda}{2}\sum_{j=1}^{M}\vert w_j\vert^q,
\end{equation}
where $q=2$ corresponds to the regularizer \eqref{eq:rls.2}.</p>

<h4 id="mult-outputs">Multiple outputs</h4>
<p>When the target of our model is instead in multiple-dimensional form, denoted as $\mathbf{t}$, we can generalize our model to be
\begin{equation}
\mathbf{y}(\mathbf{x},\mathbf{w})=\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),
\end{equation}
where $\mathbf{y}\in\mathbb{R}^K, \mathbf{W}\in\mathbb{R}^{M\times K}$ is the matrix of parameters, $\boldsymbol{\phi}\in\mathbb{R}^M$ with $\phi_i(\mathbf{x})$ as the $i$-th element, and with $\phi_0(\mathbf{x})=1$.</p>

<p>With this generalization, \eqref{eq:lsr.1} can be also be rewritten as
\begin{equation}
p(\mathbf{t}|\mathbf{x};\mathbf{W},\beta)=\sqrt{\frac{\beta}{2\pi\vert\mathbf{I}\vert}}\exp\left[-\frac{1}{2}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)^\text{T}\left(\mathbf{t}-\mathbf{W}^\text{T}\boldsymbol{\phi}\left(\mathbf{x}\right)\right)\beta\mathbf{I}^{-1}\right],
\end{equation}
or in other words
\begin{equation}
\mathbf{t}|\mathbf{x};\mathbf{W},\beta\sim\mathcal{N}(\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1}\mathbf{I})
\end{equation}
With a data set of inputs $\mathbf{X}=\{\mathbf{x}_1,\ldots,\mathbf{x}_N\}$, our target values can also be vectorized into $\mathbf{T}\in\mathbb{R}^{N\times K}$ given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.1cm}\mathbf{t}_1^\text{T}\hspace{0.1cm}- \\ \vdots \\ -\hspace{0.1cm}\mathbf{t}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right],
\end{equation}
and likewise with the input matrix $\mathbf{X}$ vectorized from input vectors $\mathbf{x}_1,\ldots,\mathbf{x}_N$. With these definitions, the multi-dimensional likelihood can be defined as
\begin{align}
L(\mathbf{W},\beta)&amp;=p(\mathbf{T}|\mathbf{X};\mathbf{W},\beta) \\ &amp;=\prod_{i=1}^{N}p(\mathbf{t}_i|\mathbf{x}_i;\mathbf{W},\beta) \\ &amp;=\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right]
\end{align}
And thus the log likelihood now becomes
\begin{align}
\ell(\mathbf{W},\beta)&amp;=\log L(\mathbf{W},\beta) \\ &amp;=\log\prod_{i=1}^{N}\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;=\sum_{i=1}^{N}\log\sqrt{\frac{\beta}{2\pi}}\exp\left[-\frac{\beta}{2}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)\right] \\ &amp;=\frac{N}{2}\log\frac{\beta}{2\pi}-\frac{\beta}{2}\sum_{i=1}^{N}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)^\text{T}\big(\mathbf{t}_i-\mathbf{W}^\text{T}\boldsymbol{\phi}(\mathbf{x}_i)\big)
\end{align}
Taking the gradient of the log likelihood w.r.t $\mathbf{W}$, setting it to zero and solving for $\mathbf{W}$ gives us
\begin{equation}
\mathbf{W}_\text{ML}=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{T}
\end{equation}</p>

<h3 id="bayes-lin-reg">Bayesian linear regression</h3>

<h4 id="param-dist">Parameter distribution</h4>
<p>Consider the noise precision parameter $\beta$ as a constant. From the equation \eqref{eq:lsr.2}, we see that the likelihood function $L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})$ takes the form of an exponential of a quadratic form in $\mathbf{w}$. Thus, if we choose the prior $p(\mathbf{w})$ as a Gaussian, the corresponding posterior will also become a Gaussian due to being computed as a product of two exponentials of quadratic forms of $\mathbf{w}$. This makes the prior be a conjugate distribution for the likelihood function, and hence be given by
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ is the mean vector and $\mathbf{S}_0$ is the covariance matrix.</p>

<p>By the <a href="/2021/11/22/normal-dist.html#marg-cond-gaussian">result</a>, we have that the corresponding posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, which is a conditional Gaussian distribution, is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N),\label{eq:pd.1}
\end{equation}
where the mean $\mathbf{m}_N$ and the precision matrix $\mathbf{S}_N^{-1}$ are defined as
\begin{align}
\mathbf{m}_N&amp;=\mathbf{S}_N(\mathbf{S}_0^{-1}\mathbf{m}_0+\beta\boldsymbol{\Phi}^\text{T}\mathbf{t}), \\ \mathbf{S}_N^{-1}&amp;=\mathbf{S}_0^{-1}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Therefore, by MAP, we have
\begin{align}
\mathbf{w}_\text{MAP}&amp;=\underset{\mathbf{w}}{\text{argmax}}\,\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\Big] \\ &amp;=\underset{\mathbf{w}}{\text{argmin}}\,(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)
\end{align}
By this <a href="/2021/11/22/normal-dist.html#precision-eigenvalue">property</a> of the covariance matrix, we have that the precision matrix $\mathbf{S}_N^{-1}$ and the covariance matrix $\mathbf{S}_N$ have the same set of eigenvalues, which are non-negative due to the fact that $\mathbf{S}_N$ is positive semi-definite. This also means that $\mathbf{S}_N$ is positive semi-definite, and thus
\begin{equation}
(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)\geq0
\end{equation}
Therefore, the maximum posterior weight vector is also the mean vector
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N\label{eq:pd.2}
\end{equation}
Consider an infinite broad prior $\mathbf{S}_0=\alpha^{-1}\mathbf{I}$ with $\alpha\to 0$, in this case the mean $\mathbf{m}_N$ reduces to the maximum likelihood value $\mathbf{w}_\text{ML}$ given by \eqref{eq:lsr.5}. And if $N=0$, then the posterior distribution reverts to the prior.</p>

<p>Furthermore, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$, the posterior given in \eqref{eq:pd.1} can be regarded as the prior distribution for that data point. If the model is given as \eqref{eq:lsr.1}, the likelihood function of the newly added data point is then given in form
\begin{equation}
p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})=\left(\frac{\beta}{2\pi}\right)^{1/2}\exp\left(-\frac{(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})\beta}{2}\right),
\end{equation}
where $\boldsymbol{\phi}_{N+1}=\boldsymbol{\phi}(\mathbf{x}_{N+1})$.
Therefore, the posterior distribution of the data point $(\mathbf{x}_{N+1},t_{N+1})$ can be computed as
\begin{align}
&amp;\hspace{0.7cm}p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t}) \\ &amp;\propto p(t_{N+1}\vert\mathbf{x}_{N+1},\mathbf{w})p(\mathbf{w}\vert\mathbf{t}) \\ &amp;=\exp\Big[-\frac{1}{2}(\mathbf{w}-\mathbf{m}_N)^\text{T}\mathbf{S}_N^{-1}(\mathbf{w}-\mathbf{m}_N)-\frac{1}{2}(t_{N+1}-\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1})^2\beta\Big] \\ &amp;=\exp\Big[-\frac{1}{2}\big(\mathbf{w}^\text{T}\mathbf{S}_N^{-1}\mathbf{w}+\beta\mathbf{w}^\text{T}\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{w}\big)+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big] \\ &amp;=\exp\Big[-\frac{1}{2}\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)\mathbf{w}+\mathbf{w}^\text{T}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)+c\Big],
\end{align}
where $c$ is a constant w.r.t $\mathbf{w}$, i.e., $c$ is independent of $\mathbf{w}$, which claims that the posterior distribution is also a Gaussian, given by
\begin{equation}
p(\mathbf{w}\vert t_{N+1},\mathbf{x}_{N+1},\mathbf{t})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1})\label{eq:pd.3}
\end{equation}
where the precision matrix $\mathbf{S}_{N+1}$ is defined as
\begin{equation}
\mathbf{S}_{N+1}^{-1}\doteq\mathbf{S}_N^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T},
\end{equation}
and the mean $\mathbf{m}_{N+1}$ is given by
\begin{equation}
\mathbf{m}_{N+1}\doteq\mathbf{S}_{N+1}\big(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}\big)
\end{equation}
Consider the prior as a Gaussian, defined by
\begin{equation}
p(\mathbf{w}\vert\alpha)=\mathcal{N}(\mathbf{w}\vert\mathbf{0},\alpha^{-1}\mathbf{I}),
\end{equation}
Therefore, the corresponding posterior over $\mathbf{w}$, $p(\mathbf{w}\vert\mathbf{t})$, will be given as \eqref{eq:pd.1} with
\begin{align}
\mathbf{m}_N&amp;=\beta\mathbf{S}_N\boldsymbol{\Phi}^\text{T}\mathbf{t} \\ \mathbf{S}_N^{-1}&amp;=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}
\end{align}
Taking the natural logarithm of the posterior distribution gives us the sum of the log likelihood and the log of the prior, as a function of $\mathbf{w}$, given by
\begin{equation}
\log p(\mathbf{w}\vert\mathbf{t})=-\frac{\beta}{2}\sum_{n=1}^{N}\big(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}_n)\big)^2-\frac{\alpha}{2}\mathbf{w}^\text{T}\mathbf{w}+c
\end{equation}
Therefore, maximizing this posterior is equivalent to minimizing the sum of the sum-of-squares error function with addition of a quadratic regularization term, which is exactly the equation \eqref{eq:rls.2} with $\lambda=\alpha/\beta$.</p>

<p>In addition, by $\eqref{eq:pd.2}$, we have that
\begin{equation}
\mathbf{w}_\text{MAP}=\mathbf{m}_N=\beta\left(\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t}=\left(\frac{\alpha}{\beta}\mathbf{I}+\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\right)^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{equation}
which for setting $\lambda=\alpha/\beta$ gives us exactly the solution \eqref{eq:rls.3} for the regularized least squares \eqref{eq:rls.2}.</p>

<h4 id="pred-dist-reg">Predictive distribution</h4>
<p>The <strong>predictive distribution</strong> that gives us the information to make predictions $t$ for new values $\mathbf{x}$ is defined as
\begin{equation}
p(t\vert\mathbf{x},\mathbf{t},\alpha,\beta)=\int p(t\vert\mathbf{x},\mathbf{w},\beta)p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)\,d\mathbf{w}\label{eq:pdr.1}
\end{equation}
in which $\mathbf{t}$ is the vector of target values from the training set.</p>

<p>The conditional distribution $p(t\vert\mathbf{x},\mathbf{w},\beta)$ of the target variable is given by \eqref{eq:lsr.1}, and the posterior weight distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$ is given by \eqref{eq:pd.1}. Thus, as a <a href="/2021/11/22/normal-dist.html#marg-cond-gaussian">marginal Gaussian distribution</a>, the distribution \eqref{eq:pdr.1} can be rewritten as
\begin{align}
p(t\vert\mathbf{x},\mathbf{t},\mathbf{w},\beta)&amp;=\int\mathcal{N}(t\vert\mathbf{w}^\text{T}\boldsymbol{\phi}(\mathbf{x}),\beta^{-1})\mathcal{N}(\mathbf{w}\vert\mathbf{m}_N,\mathbf{S}_N)\,d\mathbf{w} \\ &amp;=\mathcal{N}(t\vert\mathbf{m}_N^\text{T}\boldsymbol{\phi}(\mathbf{x}),\sigma_N^2(\mathbf{x})),
\end{align}
where the variance $\sigma_N^2(\mathbf{x})$ of the predictive distribution is defined as
\begin{equation}
\sigma_N^2(\mathbf{x})\doteq\beta^{-1}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.2}
\end{equation}
The first term in \eqref{eq:pdr.2} represents the noise on the data, while the second term reflects the uncertainty associated with the parameters $\mathbf{w}$.</p>

<p>It is worth noting that as additional data points are observed, the posterior distribution becomes narrower. In particular, consider an additional data point $(\mathbf{x}_{N+1},t_{N+1})$. Therefore, as given by the result \eqref{eq:pd.3}, its posterior distribution is
\begin{equation}
p(\mathbf{w}\vert\mathbf{m}_{N+1},\mathbf{S}_{N+1}),
\end{equation}
where
\begin{align}
\mathbf{m}_{N+1}&amp;=\mathbf{S}_{N+1}(\mathbf{S}_N^{-1}\mathbf{m}_N+t_{N+1}\beta\boldsymbol{\phi}_{N+1}), \\ \mathbf{S}_{N+1}^{-1}&amp;=\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}
\end{align}
Therefore, the variance of the corresponding predictive distribution for the newly added data point is then given as
\begin{equation}
\sigma_{N+1}^2(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_{N+1}\boldsymbol{\phi}(\mathbf{x})=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\big(\mathbf{S}_{N}^{-1}+\beta\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\big)^{-1}\boldsymbol{\phi}(\mathbf{x})\label{eq:pdr.3}
\end{equation}
Using the matrix identity
\begin{equation}
(\mathbf{M}+\mathbf{v}\mathbf{v}^\text{T})^{-1}=\mathbf{M}^{-1}-\frac{(\mathbf{M}^{-1}\mathbf{v})(\mathbf{v}^\text{T}\mathbf{M}^{-1})}{1+\mathbf{v}^\text{T}\mathbf{M}^{-1}\mathbf{v}},
\end{equation}
in the equation \eqref{eq:pdr.3} gives us
\begin{align}
\sigma_{N+1}^2(\mathbf{x})&amp;=\frac{1}{\beta}+\boldsymbol{\phi}(\mathbf{x})^\text{T}\left(\mathbf{S}_N-\frac{\beta\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\right)\boldsymbol{\phi}(\mathbf{x}) \\ &amp;=\sigma_N^2(\mathbf{x})-\beta\frac{\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})}{1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}}\leq\sigma_N^2(\mathbf{x}),
\end{align}
since
\begin{equation}
\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}(\mathbf{x})=\left\Vert\boldsymbol{\phi}(\mathbf{x})^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}\right\Vert_2^2\geq0,
\end{equation}
and since
\begin{equation}
1+\beta\boldsymbol{\phi}_{N+1}^\text{T}\mathbf{S}_N\boldsymbol{\phi}_{N+1}&gt;0
\end{equation}
due to $\mathbf{S}_N$ is the covariance matrix of the posterior distribution $p(\mathbf{w}\vert\mathbf{x},\mathbf{t},\alpha,\beta)$, which implies that it is positive semi-definite.</p>

<p>In other words, as $N\to\infty$, the second term in \eqref{eq:pdr.2} goes to zero, and the variance of the predictive distribution solely depends on $\beta$.</p>

<h2 id="linear-models-for-classification">Linear models for Classification</h2>
<p>In Machine Learning literature, <strong>classification</strong> refers the to task of taking an input vector $\mathbf{x}$ and assigning it to one of $K$ classes $\mathcal{C}_k$ for $k=1,\ldots,K$. Usually, each input will be assigned only to a single class. In this case, the input space is divided by the <strong>decision boundaries</strong> (or <strong>decision surfaces</strong>) into <strong>decision regions</strong>.</p>

<p>Taking an input space of $D$ dimensions, linear models are defined to be linear functions of the input vector $x$, and thus are a $(D-1)$-dimensional hyperplane.</p>

<h3 id="disc-funcs">Discriminant functions</h3>
<p>A discriminant is a function that takes an input vector $x$ and assigns it to one of $K$ class, denoted as $\mathcal{C}_k$</p>

<p>The simplest discriminant function is a linear function of the input vector
\begin{equation}
y(\mathbf{x})=\mathbf{w}^\text{T}\mathbf{x}+w_0,
\end{equation}
where $\mathbf{w}$ is called the <strong>weight vector</strong>, and $w_0$ is the <strong>bias</strong>.</p>

<p>In the case of binary classification, an input $\mathbf{x}$ is assigned to class $\mathcal{C}_1$ if $y(\mathbf{x})\geq 0$ and otherwise $y(\mathbf{x})\lt 0$, it belongs to class $\mathcal{C}_2$, thus the decision boundary is defined by
\begin{equation}
y(\mathbf{x})=0,
\end{equation}
which corresponds to a $(D-1)$-dimensional hyperplane with an $D$-dimensional input space.</p>

<p>Consider $\mathbf{x}_A$ and $\mathbf{x}_B$ lying on the hyperplane, thus $y(\mathbf{x}_A)=y(\mathbf{x}_B)=0$, which gives us that
\begin{equation}
0=y(\mathbf{x}_A)-y(\mathbf{x}_B)=\mathbf{w}^\text{T}\mathbf{x}_A-\mathbf{w}^\text{T}\mathbf{x}_B=\mathbf{w}^\text{T}(\mathbf{x}_A-\mathbf{x}_B)
\end{equation}
This claims that $\mathbf{w}$ is perpendicular to any vector within the decision boundary, and thus $\mathbf{w}$ is a normal vector of the decision boundary itself.</p>

<p>Hence, projecting a point $\mathbf{x}_0$ into the hyperplane, we have that the distant of $\mathbf{x}_0$ to the hyperplane is given by
\begin{equation}
\text{dist}(\mathbf{x}_0,y(\mathbf{x}))=\frac{y(\mathbf{x}_0)}{\Vert\mathbf{w}\Vert},
\end{equation}
which implies that
\begin{equation}
\text{dist}(\mathbf{0},y(\mathbf{x}))=\frac{w_0}{\Vert\mathbf{w}\Vert}
\end{equation}
To generalize the binary classification problem into multiple-class ones, we consider a $K$-class discriminant comprising $K$ linear functions of the form
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}
\end{equation}
Then for each input $\mathbf{x}$, it will be assigned to class $\mathcal{C}_k$ if $y_k(\mathbf{x})&gt;y_i(\mathbf{x}),\forall i\neq k$, or in other words $\mathbf{x}$ is assigned to a class $C_k$ that
\begin{equation}
k=\underset{i=1,\ldots,K}{\text{argmax}}\,y_i(\mathbf{x})
\end{equation}
The boundary between two class $\mathcal{C}_i$ and $\mathcal{C}_j$ is therefore given by
\begin{equation}
y_i(\mathbf{x})=y_j(\mathbf{x}),
\end{equation}
or
\begin{equation}
(\mathbf{w}_i-\mathbf{w}_j)^\text{T}\mathbf{x}+w_{i,0}-w_{j,0}=0,
\end{equation}
which is an $(D-1)$-dimensional hyperplane.</p>

<h4 id="least-squares-clf">Least squares</h4>
<p>Recall that in the regression task, we used least squares to find the models in form of linear functions of the parameters. We can also apply least squares approach to classification problems.</p>

<p>To begin, we have that for $k=1,\ldots,K$, each class $\mathcal{C}_k$ is represented the model
\begin{equation}
y_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0}\label{eq:lsc.1}
\end{equation}
By giving the bias parameter $w_{k,0}$ a dummy input variable $x_0=0$, we can rewrite \eqref{eq:lsc.1} in a more convenient form
\begin{equation}
y_k(\mathbf{x})=\widetilde{\mathbf{w}}_k^\text{T}\widetilde{\mathbf{x}},
\end{equation}
where
\begin{equation}
\widetilde{\mathbf{w}}_k=\left(w_{k,0},\mathbf{w}_k^\text{T}\right)^\text{T};\hspace{1cm}\widetilde{\mathbf{x}}=\left(1,\mathbf{x}^\text{T}\right)^\text{T}
\end{equation}
Thus, we can vectorize the $K$ linear models into
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}},\label{eq:lsc.2}
\end{equation}
where $\widetilde{\mathbf{W}}$ is the parameter matrix whose $k$-th column is the $(D+1)$-dimensional vector $\widetilde{\mathbf{w}}_k$
\begin{equation}
\widetilde{\mathbf{W}}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \widetilde{\mathbf{w}}_1&amp;\ldots&amp;\widetilde{\mathbf{w}}_K \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
Consider a training set $\{\mathbf{x}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$, analogy to the parameter matrix $\widetilde{\mathbf{W}}$, we can vectorize those input variables and target values into
\begin{equation}
\widetilde{\mathbf{X}}=\left[\begin{matrix}-\hspace{0.15cm}\widetilde{\mathbf{x}}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\widetilde{\mathbf{x}}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
and
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
With these definition, the sum-of-squares error function then can be written as
\begin{equation}
E_D(\widetilde{\mathbf{W}})=\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big]
\end{equation}
Taking the derivative of $E_D(\widetilde{\mathbf{W}})$ w.r.t $\widetilde{\mathbf{W}}$, we obtain
\begin{align}
\nabla_\widetilde{\mathbf{W}}E_D(\widetilde{\mathbf{W}})&amp;=\nabla_\widetilde{\mathbf{W}}\frac{1}{2}\text{Tr}\Big[(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^\text{T}(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\Big] \\ &amp;=\frac{1}{2}\nabla_\widetilde{\mathbf{W}}\text{Tr}\Big[\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\mathbf{T}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}+\mathbf{T}^\text{T}\mathbf{T}\Big] \\ &amp;=\frac{1}{2}\Big[2\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}\Big] \\ &amp;=\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\widetilde{\mathbf{X}}^\text{T}\mathbf{T}
\end{align}
Setting this derivative equal to zero, we obtain the least squares solution for $\widetilde{\mathbf{W}}$ as
\begin{equation}
\widetilde{\mathbf{W}}=(\widetilde{\mathbf{X}}^\text{T}\widetilde{\mathbf{X}})^{-1}\widetilde{\mathbf{X}}^\text{T}\mathbf{T}=\widetilde{\mathbf{X}}^\dagger\mathbf{T}
\end{equation}
Therefore, the discriminant function \eqref{eq:lsc.2} can be rewritten as
\begin{equation}
\mathbf{y}(\mathbf{x})=\widetilde{\mathbf{W}}^\text{T}\widetilde{\mathbf{x}}=\mathbf{T}^\text{T}\big(\widetilde{\mathbf{X}}^\dagger\big)^\text{T}\widetilde{\mathbf{x}}
\end{equation}</p>

<h4 id="fisher-lin-disc">Fisher’s linear discriminant</h4>
<p>One way to view a linear classification model is in terms of dimensional reduction. In particular, given an $D$-dimensional input $\mathbf{x}$, we project it down to one dimension using
\begin{equation}
y=\mathbf{w}^\text{T}\mathbf{x}
\end{equation}</p>

<h5 id="fisher-ld-bin-clf">Binary classification</h5>
<p>Consider a binary classification in which there are $N_1$ points of class $\mathcal{C}_1$ and $N_2$ points of class $\mathcal{C}_2$, thus the mean vectors of those two classes are given by
\begin{align}
\mathbf{m}_1&amp;=\frac{1}{N_1}\sum_{n\in\mathcal{C}_1}\mathbf{x}_n, \\ \mathbf{m}_2&amp;=\frac{1}{N_2}\sum_{n\in\mathcal{C}_2}\mathbf{x}_n
\end{align}
The simplest measure of the separation of the classes, when projected onto $\mathbf{w}$, is the separation of the projected class means, which suggests us choosing $\mathbf{w}$ in order to maximize
\begin{equation}
m_2-m_1=\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1),
\end{equation}
where for $k=1,\ldots,K$
\begin{equation}
m_k=\mathbf{w}^\text{T}\mathbf{m}_k
\end{equation}
is the mean of the projected data from class $\mathcal{C}_k$.</p>

<p>To make the computation simpler, we normalize the projection simply by making a constraint of $\mathbf{w}$ being a unit vector, which means
\begin{equation}
\big\Vert\mathbf{w}\big\Vert_2=\sum_{i}w_i=1
\end{equation}
Therefore, by Lagrange multiplier, in order to maximize $m_2-m_1$, we have that
\begin{equation}
\mathbf{w}\propto(\mathbf{m}_2-\mathbf{m}_1)
\end{equation}
To solve this problem, we use the Fisher’s LD approach to minimize the class overlap by maximizing the ratio of the <strong>between-class variance</strong> to the <strong>within-class variance</strong>.</p>

<p>The within-class variance of projected data from class $\mathbf{w}_k$ is defined as
\begin{equation}
s_k^2\doteq\sum_{n\in\mathcal{C}_k}(y_n-m_k)^2,
\end{equation}
where $y_n=\mathbf{w}^\text{T}\mathbf{x}_n$ is the projected of $\mathbf{x}_n$. Thus the total within-class variance for the whole data set is $s_1^2+s_2^2$.</p>

<p>The between-class variance is simply defined to be the squared of the difference of means, given as
\begin{equation}
(m_2-m_1)^2
\end{equation}
Hence, the ratio of the between-class variance to the within-class variance, called the <strong>Fisher criterion</strong>, can be defined as
\begin{align}
J(\mathbf{w})&amp;=\frac{(m_2-m_1)^2}{s_1^2+s_2^2} \\ &amp;=\frac{\big\Vert\mathbf{w}^\text{T}(\mathbf{m}_2-\mathbf{m}_1)\big\Vert_2^2}{\sum_{n\in\mathcal{C}_1}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_1)\big\Vert_2^2+\sum_{n\in\mathcal{C}_2}\big\Vert\mathbf{w}^\text{T}(\mathbf{x}_n-\mathbf{m}_2)\big\Vert_2^2} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}},\label{eq:flbc.1}
\end{align}
where
\begin{equation}
\mathbf{S}_\text{B}\doteq(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T},
\end{equation}
is called the <strong>between-class covariance matrix</strong> and
\begin{equation}
\mathbf{S}_\text{W}\doteq\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\mathbf{m}_1)(\mathbf{x}_n-\mathbf{m}_1)^\text{T}+\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\mathbf{m}_2)(\mathbf{x}_n-\mathbf{m}_2)^\text{T},
\end{equation}
is called the <strong>total within-class covariance matrix</strong>.</p>

<p>As usual, taking the gradient of \eqref{eq:flbc.1} w.r.t $\mathbf{w}$, we have
\begin{align}
\nabla_\mathbf{w}J(\mathbf{w})&amp;=\nabla_\mathbf{w}\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}}{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}(\mathbf{S}_\text{B}+\mathbf{S}_\text{B}^\text{T})\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}(\mathbf{S}_\text{W}+\mathbf{S}_\text{W}^\text{T})\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;=\frac{\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}}{\big\Vert\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\big\Vert_2^2} \\ &amp;\propto\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}-\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{align}
Setting the gradient equal to zero and solving for $\mathbf{w}$, we obtain that $\mathbf{w}$ satisfies
\begin{equation}
\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}\mathbf{S}_\text{B}\mathbf{w}=\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}\mathbf{S}_\text{W}\mathbf{w}
\end{equation}
Since $\mathbf{w}^\text{T}\mathbf{S}_\text{W}\mathbf{w}$ and $\mathbf{w}^\text{T}\mathbf{S}_\text{B}\mathbf{w}$ are two scalars, we then have
\begin{equation}
\mathbf{S}_\text{W}\mathbf{w}\propto\mathbf{S}_\text{B}\mathbf{w}
\end{equation}
Multiply both side by $\mathbf{S}_\text{W}^{-1}$, we obtain
\begin{align}
\mathbf{w}&amp;\propto\mathbf{S}_\text{W}^{-1}\mathbf{S}_\text{B}\mathbf{w} \\ &amp;=\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1)(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w} \\ &amp;\propto\mathbf{S}_\text{W}^{-1}(\mathbf{m}_2-\mathbf{m}_1),\label{eq:flbc.2}
\end{align}
since $(\mathbf{m}_2-\mathbf{m}_1)^\text{T}\mathbf{w}$ is a scalar.</p>

<p>If the within-class covariance matrix $\mathbf{S}_\text{W}$ is isotropic<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, we then have
\begin{equation}
\mathbf{w}\propto\mathbf{m}_2-\mathbf{m}_1
\end{equation}
The result \eqref{eq:flbc.2} is called <strong>Fisher’s linear discriminant</strong>.</p>

<p>With this $\mathbf{w}$, we can project our data down into one dimension and from projected data, we construct a discriminant by selecting a threshold $y_0$ such that $\mathbf{x}$ belongs to class $\mathcal{C}_1$ if $y(\mathbf{x})\gg y_0$ and otherwise it belongs to $\mathcal{C}_1$.</p>

<h5 id="fisher-ld-clf">Multi-class classification</h5>
<p>To generalize the Fisher discriminant to the case of $K&gt;2$, we first assume that $D&gt;K$ and consider the $D’&gt;1$ linear features
\begin{equation}
y=\mathbf{w}_k^\text{T}\mathbf{x},
\end{equation}
where $k=1,\ldots,D’$. Thus, as usual we can vectorize these feature values as
\begin{equation}
\mathbf{y}=\mathbf{W}^\text{T}\mathbf{x},\label{eq:flc.1}
\end{equation}
where
\begin{equation}
\mathbf{y}=(y_1,\ldots,y_k)^\text{T},\hspace{2cm}\mathbf{W}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{w}_1&amp;\ldots&amp;\mathbf{w}_{D’} \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
The mean vector for each class is unchanged, which is given as
\begin{equation}
\mathbf{m}_k=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n,
\end{equation}
where $N_k$ is the number of points in class $\mathcal{C}_k$  for $k=1,\ldots,K$.</p>

<p>The within-class variance covariance matrix $\mathbf{S}_\text{W}$ now can be simply generalized as
\begin{equation}
\mathbf{S}_\text{W}=\sum_{k=1}^{K}\mathbf{S}_k,\label{eq:flc.2}
\end{equation}
where
\begin{equation}
\mathbf{S}_k=\sum_{n\in\mathcal{C}_k}(\mathbf{x}_n-\mathbf{m}_k)(\mathbf{x}_n-\mathbf{m}_k)^\text{T}
\end{equation}
To find the generalization of the between-class covariance matrix $\mathbf{S}_\text{B}$, we first consider the total covariance matrix
\begin{equation}
\mathbf{S}_T=\sum_{n=1}^{N}(\mathbf{x}_n-\mathbf{m})(\mathbf{x}_n-\mathbf{m})^\text{T},
\end{equation}
where
\begin{equation}
\mathbf{m}=\frac{1}{N}\sum_{n=1}^{N}\mathbf{x}_n=\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k
\end{equation}
is the mean of the whole data set, where $N=\sum_{k=1}^{K}N_k$ is the number of the data points. The total covariance matrix can be decomposed into the sum of the within-class covariance matrix $\mathbf{S}_\text{W}$, as given in \eqref{eq:flc.2} with a matrix $\mathbf{S}_\text{B}$, defined as a measure of the between-class covariance
\begin{equation}
\mathbf{S}_\text{T}=\mathbf{S}_\text{W}+\mathbf{S}_\text{B},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{B}=\sum_{k=1}^{K}N_k(\mathbf{m}_k-\mathbf{m})(\mathbf{m}_k-\mathbf{m})^\text{T}
\end{equation}
Using \eqref{eq:flc.1}, we project the whole data set into the $D’$-dimensional space of $\mathbf{y}$, the corresponding within-class covariance matrix of the transformed data are given as
\begin{align}
\mathbf{s}_\text{W}&amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)\left(\mathbf{W}^\text{T}\mathbf{x}_n-\mathbf{W}^\text{T}\mathbf{m}_k\right)^\text{T} \\ &amp;=\sum_{k=1}^{K}\sum_{n\in\mathcal{C}_k}(\mathbf{y}_n-\boldsymbol{\mu}_k)(\mathbf{y}_n-\boldsymbol{\mu}_k)^\text{T} \\ &amp;=\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T}
\end{align}
and also the transformed between-class covariance matrix
\begin{align}
\mathbf{s}_\text{B}&amp;=\sum_{k=1}^{K}N_k(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})(\mathbf{W}^\text{T}\mathbf{m}_k-\mathbf{W}^\text{T}\mathbf{m})^\text{T} \\ &amp;=\sum_{k=1}^{K}(\boldsymbol{\mu}_k-\boldsymbol{\mu})(\boldsymbol{\mu}_k-\boldsymbol{\mu})^\text{T} \\ &amp;=\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T},
\end{align}
where
\begin{align}
\boldsymbol{\mu}_k&amp;=\mathbf{W}^\text{T}\mathbf{m}_k=\mathbf{W}^\text{T}\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{x}_n=\frac{1}{N_k}\sum_{n\in\mathcal{C}_k}\mathbf{y}_n, \\ \boldsymbol{\mu}&amp;=\mathbf{W}^\text{T}\mathbf{m}=\mathbf{W}^\text{T}\frac{1}{N}\sum_{k=1}^{K}N_k\mathbf{m}_k=\frac{1}{N}\sum_{k=1}^{K}N_k\boldsymbol{\mu}_k
\end{align}
Analogy to the case of binary classification with Fisher’s criterion \eqref{eq:flbc.1}, we need a new measure that is large when the between-class covariance is large and when the within-class covariance is small. A simple choice of criterion is given as
\begin{equation}
J(\mathbf{W})=\text{Tr}\left(\mathbf{s}_\text{W}^{-1}\mathbf{s}_\text{B}\right)
\end{equation}
or
\begin{equation}
J(\mathbf{w})=\text{Tr}\big[(\mathbf{W}\mathbf{S}_\text{W}\mathbf{W}^\text{T})^{-1}(\mathbf{W}\mathbf{S}_\text{B}\mathbf{W}^\text{T})\big]
\end{equation}
for which the linear basis model follow the same rule as the above</p>

<h4 id="perceptron">The perceptron algorithm</h4>
<p>Another example of linear discriminant model is the perceptron algorithm</p>

<h3 id="prob-gen-models">Probabilistic Generative Models</h3>
<p>When solving the classification problems, we divide the strategy into two stage</p>
<ul id="number-list">
	<li>
		<b>Inference stage</b>. In this stage we use training data to learn a model for $p(\mathcal{C}_k|\mathbf{x})$ 
	</li>
	<li>
		<b>Decision stage</b>. In this stage we use those posterior probabilities to make optimal class assignments.
	</li>
</ul>
<p>We can solve both inference and decision problems at the same time by learning a function, which is the discriminant function, maps inputs $\mathbf{x}$ directly into decisions.</p>

<p>When using the generative approach to solve the problem of classification, we first model the class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ and the class priors $p(\mathcal{C}_k)$ then apply Bayes’ theorem to compute the posterior probabilities $p(\mathcal{C}_k\vert\mathbf{x})$.</p>

<p>Consider the binary case, in which specifically the posterior probability for class $\mathcal{C}_1$ can be computed as
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)+p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)} \\ &amp;=\frac{1}{1+\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}} \\ &amp;=\frac{1}{1+\exp(-a)}=\sigma(a)\label{eq:pgm.1}
\end{align}
where
\begin{equation}
a=\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}
\end{equation}
and where $\sigma(\cdot)$ is the <span id="logistic-sigmoid-func">logistic sigmoid function</span>, defined before as
\begin{equation}
\sigma(a)\doteq\frac{1}{1+\exp(-a)}
\end{equation}
For the case of multi-class, $K&gt;2$, the posterior probability for class $\mathcal{C}_k$ can be generalized as
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;=\frac{p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)}{\sum_{i=1}^{K}p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)}=\dfrac{\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big)\Big]}{\sum_{i=1}^{K}\exp\Big[\log\big(p(\mathbf{x}\vert\mathcal{C}_i)p(\mathcal{C}_i)\big)\Big]} \\ &amp;=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)}=\sigma(\mathbf{a})_k\label{eq:pgm.2}
\end{align}
where
\begin{align}
a_k&amp;=\log\big(p(\mathbf{x}\vert\mathcal{C}_k)p(\mathcal{C}_k)\big), \\ \mathbf{a}&amp;=(a_1,\ldots,a_K)^\text{T},
\end{align}
and the function $\sigma:\mathbb{R}^K\to(0,1)^K$, known as the <strong>normalized exponential</strong> or <strong>softmax function</strong> - a generalization of sigmoid into multi-dimensional, in which the $k$-th element is defined as
\begin{equation}
\sigma(\mathbf{a})_k\doteq\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
for $k=1,\ldots,K$ and $\mathbf{a}=(a_1,\ldots,a_K)^\text{T}$.</p>

<h4 id="gauss-gen-models">Gaussian Generative models</h4>
<p>If the class-conditional probabilities are Gaussian, or specifically Multivariate Normal and share the same covariance matrix $\boldsymbol{\Sigma}$, then for $k=1,\ldots,K$,
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Thus, the density for class $\mathcal{C}_k$ is defined as
\begin{equation}
p(\mathbf{x}\vert\mathcal{C}_k)=\frac{1}{(2\pi)^{D/2}\big\vert\boldsymbol{\Sigma}\big\vert^{1/2}}\exp\left[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)\right]
\end{equation}
In the binary case, in which the densities above become Bivariate Normal, by \eqref{eq:pgm.1} we have that
\begin{align}
p(\mathcal{C}_1\vert\mathbf{x})&amp;=\sigma\left(\log\frac{p(\mathbf{x}\vert\mathcal{C}_2)p(\mathcal{C}_2)}{p(\mathbf{x}\vert\mathcal{C}_1)p(\mathcal{C}_1)}\right) \\ &amp;=\sigma\left(\log\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_2)\Big]p(\mathcal{C}_2)}{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_1)\Big]p(\mathcal{C}_1)}\right) \\ &amp;=\sigma\Bigg(-\frac{1}{2}\Big[-\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2-\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\nonumber \\ &amp;\hspace{2cm}+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\Big]-\log\frac{p(\mathcal{C}_2)}{p(\mathcal{C}_1)}\Bigg) \\ &amp;=\sigma\left(\boldsymbol{\Sigma}^{-1}\left(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2\right)^\text{T}\mathbf{x}-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)}\right)\label{eq:ggm.1}
\end{align}
Let
\begin{align}
\mathbf{w}&amp;=\boldsymbol{\Sigma}^{-1}(\boldsymbol{\mu}_1-\boldsymbol{\mu}_2), \\ w_0&amp;=-\frac{1}{2}\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1+\frac{1}{2}\boldsymbol{\mu}_2^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_2+\log\frac{p(\mathcal{C}_1)}{p(\mathcal{C}_2)},
\end{align}
we have \eqref{eq:ggm.1} can be rewritten in more convenient form as
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
From the derivation, we see that by making an assumption of having the same covariance matrix $\boldsymbol{\Sigma}$ across the densities helped us remove out the quadratic terms of $\mathbf{x}$, which leads us to ending up with a logistic sigmoid of a linear function of $\mathbf{x}$.</p>

<p>For the multi-dimensional case, $K&gt;2$, by \eqref{eq:pgm.2}, we have that the density for class $\mathcal{C}_k$ is
\begin{align}
p(\mathcal{C}_k\vert\mathbf{x})&amp;=\frac{\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_k)+\log p(\mathcal{C}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu}_i)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}_i)+\log p(\mathcal{C}_i)\Big]} \\ &amp;=\frac{\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_k+\log p(\mathbf{w}_k)\Big]}{\sum_{i=1}^{K}\exp\Big[\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_i-\frac{1}{2}\boldsymbol{\mu}_i^\text{T}\boldsymbol{\Sigma}\boldsymbol{\mu}_i+\log p(\mathbf{w}_i)\Big]}
\end{align}
Or in other words, we can simplify each element of $\mathbf{a}$ into a linear function as
\begin{equation}
a_k\doteq a_k(\mathbf{x})=\mathbf{w}_k^\text{T}\mathbf{x}+w_{k,0},
\end{equation}
where
\begin{align}
\mathbf{w}_k&amp;=\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k, \\ w_{k,0}&amp;=-\frac{1}{2}\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k+\log p(\mathcal{C}_k)
\end{align}
The simplification we can make also come from the assumption of sharing the same covariance matrix between densities, which is analogous to the binary case that cancelled out the quadratic terms.</p>

<h5 id="max-likelihood-sols">Maximum likelihood solutions</h5>
<p>Once we have specified a parametric functional form of $p(\mathbf{x}\vert\mathcal{C}_k)$, using maximum likelihood, we can solve for the values of the parameters and also the prior probabilities $p(\mathcal{C}_k)$.</p>

<h6 id="ggm-bin-clf">Binary classification</h6>
<p>In particular, first off for the binary case, in which each class-conditional densities $p(\mathbf{x}\vert\mathcal{C}_k)$ is a Bivariate Normal, with a shared covariance matrix, as
\begin{equation}
\mathbf{x}\vert\mathcal{C}_k\sim\mathcal{N}(\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Consider the data set $\{\mathbf{x}_n,t_n\}$ for $n=1,\ldots,N$, i.e., $t_n=1$ denotes class $\mathcal{C}_1$ and $t_n=0$ denotes class $\mathcal{C}_2$. Let the class prior probability $p(\mathcal{C}_1)=\pi$, thus $p(\mathcal{C}_2)=1-\pi$. Or
\begin{align}
p(\mathbf{x}_n,\mathcal{C}_1)&amp;=p(\mathcal{C}_1)p(\mathbf{x}_n\vert\mathcal{C}_1)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}), \\ p(\mathbf{x}_n,\mathcal{C}_2)&amp;=p(\mathcal{C}_2)p(\mathbf{x}_n\vert\mathcal{C}_2)=\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})
\end{align}
We also have that
\begin{equation}
p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n}
\end{equation}
Therefore, the likelihood can be defined as
\begin{align}
L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})&amp;=p(\mathbf{t}\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\prod_{n=1}^{N}p(t_n\vert\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{x}_n,\mathcal{C}_1)^{t_n}p(\mathbf{x}_n,\mathcal{C}_2)^{1-t_n} \\ &amp;=\prod_{n=1}^{N}\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]^{t_n}\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]^{1-t_n},
\end{align}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$. As usual, we continue to consider the log likelihood $\ell(\cdot)$
\begin{align}
&amp;\hspace{0.7cm}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\log L(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big]\label{eq:gbc.1}
\end{align}
Taking the gradient of the log likelihood w.r.t $\pi$ we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_\pi\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\pi\sum_{n=1}^{N}t_n\log\pi+(1-t_n)\log(1-\pi) \\ &amp;=\sum_{n=1}^{N}\left[\frac{t_n}{\pi}-\frac{1-t_n}{1-\pi}\right]
\end{align}
Setting the derivative to zero and solve for $\pi$ as usual, we have
\begin{equation}
\sum_{n=1}^{N}t_n-\pi=0
\end{equation}
Thus, we obtain the solution
\begin{equation}
\pi=\frac{1}{N}\sum_{n=1}^{N}t_n=\frac{N_1}{N}=\frac{N_1}{N_1+N_2},
\end{equation}
where $N_1,N_2$ denote the total number of data points in class $\mathcal{C}_1$ and $\mathcal{C}_2$ respectively.</p>

<p>On the other hand, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\mu}_1$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_{\boldsymbol{\mu}_1}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right] \\ &amp;\propto\nabla_{\boldsymbol{\mu}_1}\sum_{n=1}^{N}t_n\big(-\boldsymbol{x}_n^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1-\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}_n+\boldsymbol{\mu}_1^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_1\big) \\ &amp;=\sum_{n=1}^{N}t_n\Big[\big(\boldsymbol{\Sigma}^{-1}+(\boldsymbol{\Sigma}^{-1})^\text{T}\big)\big(\boldsymbol{\mu}_1-\mathbf{x}_n\big)\Big] \\ &amp;\propto\sum_{n=1}^{N}t_n(\boldsymbol{\mu}_1-\mathbf{x}_n)
\end{align}
Setting the above gradient to zero and solve for $\boldsymbol{\mu}_1$, we obtain the solution
\begin{equation}
\boldsymbol{\mu}_1=\frac{1}{N_1}\sum_{n=1}^{N}t_n\mathbf{x}_n,
\end{equation}
which is simply the mean of all input vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_1$.</p>

<p>Similarly, with the same procedure, we have that the maximum likelihood solution for $\boldsymbol{\mu}_2$ is the mean of all inputs vectors $\mathbf{x}_n$ assigned to class $\mathcal{C}_2$, as
\begin{equation}
\boldsymbol{\mu}_2=\frac{1}{N_2}\sum_{n=1}^{N}(1-t_n)\mathbf{x}_n
\end{equation}
Lastly, taking the gradient of the log likelihood \eqref{eq:gbc.1} w.r.t $\boldsymbol{\Sigma}$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\Big[\pi\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})\Big]+(1-t_n)\log\Big[(1-\pi)\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_1,\boldsymbol{\Sigma})+(1-t_n)\log\mathcal{N}(\mathbf{x}_n\vert\boldsymbol{\mu}_2,\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}t_n\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\right]\nonumber \\ &amp;\hspace{2cm}+(1-t_n)\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+t_n\left[-\frac{1}{2}(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2)\right] \\ &amp;\propto\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\log\big\vert\boldsymbol{\Sigma}\big\vert+t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_1)\nonumber \\ &amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}_n-\boldsymbol{\mu}_2) \\ &amp;=N\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}\nonumber \\ &amp;\hspace{2cm}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gbc.2}
\end{align}
The first term of the gradient can be computed as
\begin{align}
\frac{\partial\log\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\frac{\partial\big\vert\boldsymbol{\Sigma}\big\vert}{\partial\boldsymbol{\Sigma}_{ij}}=\frac{1}{\big\vert\boldsymbol{\Sigma}\big\vert}\text{adj}(\boldsymbol{\Sigma})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ji}=(\boldsymbol{\Sigma}^{-1})_{ij},
\end{align}
since $\boldsymbol{\Sigma}$ is symmetric and so is its inverse. This implies that
\begin{equation}
\nabla_\boldsymbol{\Sigma}\log\big\vert\boldsymbol{\Sigma}\big\vert=\boldsymbol{\Sigma}^{-1}\label{eq:gbc.3}
\end{equation}
Let $\mathbf{S}$ be a matrix defined as
\begin{equation}
\mathbf{S}=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T}
\end{equation}
Therefore, the derivative \eqref{eq:gbc.2} can be rewritten as
\begin{equation}
\nabla_\boldsymbol{\Sigma}\ell(\pi,\boldsymbol{\mu}_1,\boldsymbol{\mu}_2,\boldsymbol{\Sigma})=N\boldsymbol{\Sigma}^{-1}-N\boldsymbol{\Sigma}^{-1}\mathbf{S}\boldsymbol{\Sigma}^{-1}
\end{equation}
Setting this gradient to zero and solve for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{S},
\end{equation}
where $\mathbf{S}$ can be continued to derive as
\begin{align}
\mathbf{S}&amp;=\frac{1}{N}\sum_{n=1}^{N}t_n(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+(1-t_n)(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T} \\ &amp;=\frac{N_1}{N}\sum_{n\in\mathcal{C}_1}(\mathbf{x}_n-\boldsymbol{\mu}_1)(\mathbf{x}_n-\boldsymbol{\mu}_1)^\text{T}+\frac{N_2}{N}\sum_{n\in\mathcal{C}_2}(\mathbf{x}_n-\boldsymbol{\mu}_2)(\mathbf{x}_n-\boldsymbol{\mu}_2)^\text{T},
\end{align}
which is the weighted average of the covariance matrices corresponded to each of the two classes $\mathcal{C}_1,\mathcal{C}_2$.</p>

<h6 id="ggm-clf">Multi-class classification</h6>
<p>To generalize the Gaussian generative binary classification, we consider a model for $K&gt;2$ classes defined by prior class probabilities $p(\mathcal{C}_k)=\pi_k$ and Multivariate Normal class-conditional densities with shared covariance matrix, given as
\begin{equation}
p({\boldsymbol{\phi}}\vert\mathcal{C}_k)=\mathcal{N}(\boldsymbol{\phi}\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}),
\end{equation}
where $\boldsymbol{\phi}$ is the input feature vector.</p>

<p>Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Therefore, we have that
\begin{equation}
p(\boldsymbol{\phi}_n,\mathcal{C}_k)=p(\mathcal{C}_k)p(\boldsymbol{\phi}_n\vert\mathcal{C}_k)=\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})
\end{equation}
Analogy to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma})=\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k}
\end{equation}
To simplify the notation, we let $\mathbf{w}$ denote
\begin{equation}
\pi_1,\ldots,\pi_K,\boldsymbol{\phi}_1,\ldots,\boldsymbol{\phi}_K,\boldsymbol{\Sigma}
\end{equation}
And let $\mathbf{T}$ be a matrix that associates those targets $\mathbf{t}_n$’s together, given as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Thus, the likelihood is given as
\begin{align}
L(\mathbf{w})&amp;=p(\mathbf{T}\vert\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}p(\boldsymbol{\phi}_n,\mathcal{C}_k)^{(\mathbf{t}_n)_k} \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k}
\end{align}
And thus, the log likelihood $\ell(\cdot)$ can be computed as
\begin{align}
\ell(\mathbf{w})&amp;=\log L(\mathbf{w}) \\ &amp;=\log\prod_{n=1}^{N}\prod_{k=1}^{K}\Big[\pi_k\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]^{(\mathbf{t}_n)_k} \\ &amp;=\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma})\Big]\label{eq:gc.1}
\end{align}
As usual, we continue by using maximum likelihood, which begins by taking gradient of the log likelihood w.r.t to the parameters. However, when maximizing the likelihood w.r.t $\pi_k$, we have to compute subject to a constraint that
\begin{equation}
\sum_{k=1}^{K}\pi_k=1
\end{equation}
Therefore, using a Lagrange multiplier $\lambda$, we instead maximize the Lagrangian w.r.t $\pi_k$, which is
\begin{equation}
\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda)=\ell(\mathbf{w})+\lambda\left(\sum_{k=1}^{K}\pi_k-1\right)
\end{equation}
Differentiating $\mathcal{L}$ w.r.t $\pi_k$, we have
\begin{align}
&amp;\hspace{0.7cm}\nabla_{\pi_k}\mathcal{L}(\pi_1,\ldots,\pi_K,\lambda) \\ &amp;=\nabla_{\pi_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big]+\nabla_{\pi_k}\lambda\left(\sum_{i=1}^{K}\pi_i-1\right) \\ &amp;=\lambda+\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\pi_k}\log\pi_k \\ &amp;=\lambda+\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\pi_k}
\end{align}
Setting the derivative equal to zero and solve for $\pi_k$, we have
\begin{equation}
\pi_k=-\frac{\sum_{n=1}^{N}(\mathbf{t}_n)_k}{\lambda}=\frac{N_k}{\lambda},
\end{equation}
where $N_k$ denotes the number of data points in class $\mathcal{C}_k$. Moreover, since $\sum_{k=1}^{K}\pi_k=1$, we have
\begin{equation}
1=-\sum_{k=1}^{K}\frac{N_k}{\lambda}=\frac{-N}{\lambda},
\end{equation}
which implies that
\begin{equation}
\lambda=-N
\end{equation}
Hence, the maximum likelihood solution for $\pi_k$ is
\begin{equation}
\pi_k=-\frac{N_k}{\lambda}=\frac{N_k}{N}
\end{equation}
We continue by taking the gradient of the log likelihood \eqref{eq:gc.1} w.r.t $\boldsymbol{\mu}_k$, as
\begin{align}
\nabla_{\boldsymbol{\mu}_k}\ell(\mathbf{w})&amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\Big[\log\pi_i+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_i,\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k,\boldsymbol{\Sigma}) \\ &amp;=\nabla_{\boldsymbol{\mu}_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;=-\frac{1}{2}\sum_{n=1}^{N}(\mathbf{t}_n)_k\nabla_{\boldsymbol{\mu}_k}\Big[\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-2\boldsymbol{\mu}_k^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big] \\ &amp;=\sum_{n=1}^{N}(\mathbf{t}_n)_k\Big[\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}_k-\boldsymbol{\Sigma}^{-1}\boldsymbol{\phi}_n\Big]
\end{align}
Setting the above gradient equal to zero and solve for $\boldsymbol{\mu}_k$ we obtain the solution
\begin{equation}
\boldsymbol{\mu}_k=\frac{1}{\sum_{n=1}^{N}(\mathbf{t}_n)_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k\boldsymbol{\phi}_n,
\end{equation}
which is the mean of feature vectors assigned to class $\mathcal{C}_k$.</p>

<p>Finally, consider the gradient of \eqref{eq:gc.1} w.r.t $\boldsymbol{\Sigma}$, combined with the result \eqref{eq:gbc.3} we have
\begin{align}
\nabla_\boldsymbol{\Sigma}\ell(\mathbf{w})&amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\Big[\log\pi_k+\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma})\Big] \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\mathcal{N}(\boldsymbol{\phi}_n\vert\boldsymbol{\mu}_k\boldsymbol{\Sigma}) \\ &amp;=\nabla_\boldsymbol{\Sigma}\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log\big\vert\boldsymbol{\Sigma}\big\vert^{-1/2}+(\mathbf{t}_n)_k\Big[-\frac{1}{2}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\boldsymbol{\Sigma}^{-1}(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)\Big] \\ &amp;=-\frac{N}{2}\boldsymbol{\Sigma}^{-1}+\frac{1}{2}\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1} \\ &amp;\propto N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}\Big]\boldsymbol{\Sigma}^{-1}\label{eq:gc.2}
\end{align}
Let $\mathbf{S}_k$ be the covariance of the data associated with class $\mathcal{C}_k$, defined as
\begin{equation}
\mathbf{S}_k=\frac{1}{N_k}\sum_{n=1}^{N}(\mathbf{t}_n)_k(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)(\boldsymbol{\phi}_n-\boldsymbol{\mu}_k)^\text{T}
\end{equation}
Therefore, letting the derivative \eqref{eq:gc.2} equal to zero, we have
\begin{equation}
N\boldsymbol{\Sigma}^{-1}-\boldsymbol{\Sigma}^{-1}\Big[\sum_{k=1}^{K}N_k\mathbf{S}_k\Big]\boldsymbol{\Sigma}^{-1}=0
\end{equation}
Solving this equation for $\boldsymbol{\Sigma}$, we obtain the solution
\begin{equation}
\boldsymbol{\Sigma}=\sum_{k=1}^{K}\frac{N_k}{N}\mathbf{S}_k
\end{equation}</p>

<h3 id="prob-disc-models">Probabilistic Discriminative Models</h3>

<h4 id="log-reg">Logistic Regression</h4>
<p>Recall that in the previous section of generative approach, in particular for the binary case we knew that the posterior probability for class $\mathcal{C}_1$ can be defined as the logistic sigmoid of a linear function of the input vector $\mathbf{x}$
\begin{equation}
p(\mathcal{C}_1\vert\mathbf{x})=\sigma\big(\mathbf{w}^\text{T}\mathbf{x}+w_0\big)
\end{equation}
In general, the posterior probabilities can be written as the logistic sigmoid of a linear function of instead feature vector $\boldsymbol{\phi}$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi})=y(\boldsymbol{\phi})=\sigma\big(\mathbf{w}^\text{T}\boldsymbol{\phi}+w_0\big)
\end{equation}
This model is called <strong>logistic regression</strong>, although it is applied for classification tasks.
Consider a data set $\{\boldsymbol{\phi}_n,t_n\}$, where $\boldsymbol{\phi}_n=\boldsymbol{\phi}(\mathbf{x}_n)$ and $t_n\in\{0,1\}$, with $n=1,\ldots,N$. Therefore,
\begin{equation}
p(t_n\vert\mathbf{w})=y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $y_n=p(\mathcal{C}_1\vert\boldsymbol{\phi}_n)$.</p>

<p>Comprise $t_n$’s into $\mathbf{t}\doteq(t_1,\ldots,t_N)^\text{T}$, then we have that the likelihood function can be defined as
\begin{equation}
L(\mathbf{w})=p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n}\tag{35}\label{eq:lr.1}
\end{equation}
Taking the negative logarithm of the likelihood gives us the <strong>cross-entropy</strong> error function, as
\begin{align}
E(\mathbf{w})=-\log L(\mathbf{w})&amp;=-\log\prod_{n=1}^{N}p(t_n\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n} \\ &amp;=-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n)\label{eq:lr.2}
\end{align}
Differentiating the error function $E(\mathbf{w})$ w.r.t $\mathbf{w}$ we have that
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}-\sum_{n=1}^{N}t_n\log y_n+(1-t_n)\log(1-y_n) \\ &amp;=\sum_{n=1}^{N}\frac{(1-t_n)\nabla_\mathbf{w}y_n}{1-y_n}-\frac{t_n\nabla_\mathbf{w}y_n}{y_n} \\ &amp;=\sum_{n=1}^{N}\frac{(1-t_n)y_n(1-y_n)\boldsymbol{\phi}_n}{1-y_n}-\frac{t_n y_n(1-y_n)\boldsymbol{\phi}_n}{y_n} \\ &amp;=\sum_{n=1}^{N}(1-t_n)y_n\boldsymbol{\phi}_n-t_n(1-y_n)\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n,\label{eq:lr.3}
\end{align}
where in the third step, we have used the identity of the <span id="sigmoid-derivative">derivative of the logistic sigmoid function</span>
\begin{equation}
\frac{d\sigma}{d a}=\sigma(1-\sigma)
\end{equation}
and the chain rule to compute the gradient of $y_n$ w.r.t $\mathbf{w}$ as
\begin{align}
\nabla_\mathbf{w}y_n&amp;=\nabla_\mathbf{w}\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;=\frac{d\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}{d(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)}\nabla_\mathbf{w}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0) \\ &amp;=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big(1-\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n+w_0)\big)\boldsymbol{\phi}_n \\ &amp;=y_n(1-y_n)\boldsymbol{\phi}_n
\end{align}</p>

<h4 id="softmax-reg">Softmax Regression</h4>
<p>Analogy to the generalization of the binary case into logistic regression, for the multi-class case, the posterior probability for class $\mathcal{C}_k$ can be written as the softmax function of a linear function of feature vectors $\boldsymbol{\phi}$ as
\begin{equation}
p(\mathcal{C}_k\vert\boldsymbol{\phi})=y_k(\boldsymbol{\phi})=\frac{\exp(a_k)}{\sum_{i=1}^{K}\exp(a_i)},
\end{equation}
where $a_k$’s is called the <strong>activations</strong>, defined as
\begin{equation}
a_k=\mathbf{w}_k^\text{T}\boldsymbol{\phi}
\end{equation}
Given a data set $\{\boldsymbol{\phi}_n,\mathbf{t}_n\}$ for $n=1,\ldots,N$ where $\mathbf{t}_n$ is the target vector of length $K$ using the 1-of-$K$ scheme, i.e., $(\mathbf{t}_n)_k=1$ denotes class $\mathcal{C}_k$ and $(\mathbf{t}_n)_i=0$ for all $i\neq k$. Similar to the binary case, we also have that
\begin{equation}
p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K)=\prod_{k=1}^{K}p(\mathcal{C}_k\vert\boldsymbol{\phi}_n)^{(\mathbf{t}_n)_k}=\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k},
\end{equation}
where $(y_{n})_k=y_k(\boldsymbol{\phi}_n)$.</p>

<p>Let $\mathbf{T}$ be a $N\times K$ matrix comprising $\mathbf{t}_n$’s together as
\begin{equation}
\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the likelihood function can be written by
\begin{align}
L(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=p(\mathbf{T}\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=\prod_{n=1}^{N}p(\mathbf{t}_n\vert\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k}
\end{align}
We also obtain the cross-entropy error function by taking the negative logarithm of the likelihood, as
\begin{align}
E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=-\log L(\mathbf{w}_1,\ldots,\mathbf{w}_K) \\ &amp;=-\log\prod_{n=1}^{N}\prod_{k=1}^{K}(y_{n})_k^{(\mathbf{t}_n)_k} \\ &amp;=-\sum_{n=1}^{N}\sum_{k=1}^{K}(\mathbf{t}_n)_k\log(y_{n})_k\label{eq:sr.1}
\end{align}
As usual, taking the gradient of the error function $E(\mathbf{w}_1,\ldots,\mathbf{w}_K)$ w.r.t $\mathbf{w}_k$ we have
\begin{align}
\nabla_{\mathbf{w}_k}E(\mathbf{w}_1,\ldots,\mathbf{w}_K)&amp;=\nabla_{\mathbf{w}_k}-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\log(y_{n})_i \\ &amp;=-\sum_{n=1}^{N}\sum_{i=1}^{K}(\mathbf{t}_n)_i\frac{(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n}{(y_n)_i} \\ &amp;=\sum_{n=1}^{N}\Big[(y_n)_k\sum_{i=1}^{K}(\mathbf{t}_n)_i-\sum_{i=1}^{K}(\mathbf{t}_n)_i 1\{i=k\}\Big]\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n\label{eq:sr.2}
\end{align}
where in the second step, we have used the <span id="softmax-derivative">identity</span>
\begin{align}
\frac{\partial y_k}{\partial a_j}&amp;=\frac{\big(\partial\exp(a_k)/\partial\exp(a_j)\big)\sum_{i=1}^{K}\exp(a_i)-\exp(a_j)\exp(a_k)}{\big(\sum_{i=1}^{K}\exp(a_i)\big)^2} \\ &amp;=\frac{\exp(a_k)1\{k=j\}}{\sum_{i=1}^{K}\exp(a_i)}-y_k y_j \\ &amp;=y_k(1\{k=j\}-y_j)
\end{align}
where $1\{k=j\}$ is the indicator function, which returns $1$ if $k=j$ and returns $0$ otherwise. Hence, by chain rule, we obtain the gradient of $(y_n)_i$ w.r.t $\mathbf{w}_k$ given by
\begin{align}
\nabla_{\mathbf{w}_k}(y_n)_i&amp;=\frac{\partial(y_n)_i}{\partial a_k}\frac{\partial a_k(\mathbf{w}_k,\boldsymbol{\phi}_n)}{\partial\mathbf{w}_k} \\ &amp;=(y_n)_i(1\{i=k\}-(y_n)_k)\boldsymbol{\phi}_n
\end{align}</p>

<h4 id="newtons-method">Newton’s method</h4>
<figure>
	<img src="/assets/images/2022-08-13/newtons-method.gif" alt="Newton's method)" style="display: block; margin-left: auto; margin-right: auto; width: 450px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 5</b>: Illustration of the Newton's method. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/pattern-recognition-and-machine-learning-book/linear-models/classification/newtons-method.py">here</a></span></figcaption>
</figure>

<p>\begin{equation}
\mathbf{w}^{(\text{new})}=\mathbf{w}^{(\text{old})}-\mathbf{H}^{-1}\nabla_\mathbf{w}E(\mathbf{w})
\end{equation}</p>

<h5 id="nm-lin-reg">Linear Regression</h5>
<p>Consider applying the Newton’s method to the sum-of-squares error function \eqref{eq:lsr.4} for the linear regression model \eqref{eq:lbfm.2}. The gradient and Hessian of this error function are
\begin{align}
\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}\left(t_n-\mathbf{w}^\text{T}\boldsymbol{\phi}_n)\right)^2 \\ &amp;=\sum_{n=1}^{N}(\mathbf{w}^\text{T}\boldsymbol{\phi}_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
and
\begin{equation}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})=\nabla_\mathbf{w}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big)=\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi},
\end{equation}
where $\boldsymbol{\Phi}$, as defined before, is the $N\times M$ design matrix
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_1)^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}(\mathbf{x}_N)^\text{T}\hspace{0.1cm}-\end{matrix}\right]=\left[\begin{matrix}\phi_0(\mathbf{x}_1)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_1) \\ \vdots&amp;\ddots&amp;\vdots \\ \phi_0(\mathbf{x}_N)&amp;\ldots&amp;\phi_{M-1}(\mathbf{x}_N)\end{matrix}\right],
\end{equation}
Hence, we have that the Newton’s update of the model is given by
\begin{align}
\mathbf{w}^{(\text{new})}&amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\big(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}\mathbf{t}\big) \\ &amp;=(\boldsymbol{\Phi}^\text{T}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{t},
\end{align}
which is exactly the standard least-squares solution.</p>

<h5 id="nm-log-reg">Logistic Regression</h5>
<p>Consider using the Newton’s method to the logistic regression model with the cross-entropy error function \eqref{eq:lr.2}. By the result \eqref{eq:lr.3}, we have the gradient and Hessian of this error function are given as
\begin{equation}
\nabla_\mathbf{w}E(\mathbf{w})=\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n=\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t})
\end{equation}
and
\begin{align}
\mathbf{H}=\nabla_\mathbf{w}\nabla_\mathbf{w}E(\mathbf{w})&amp;=\nabla_\mathbf{w}\sum_{n=1}^{N}(y_n-t_n)\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\label{eq:nlr.1} \\ &amp;=\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi},
\end{align}
where $\mathbf{R}$ is the $N\times N$ diagonal matrix with diagonal elements
\begin{equation}
\mathbf{R}_{n n}=y_n(1-y_n)
\end{equation}
It is noticeable that hessian matrix $\mathbf{H}$ is positive definite because for any vector $\mathbf{v}$
\begin{equation}
\mathbf{v}^\text{T}\mathbf{H}\mathbf{v}=\mathbf{v}^\text{T}\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{v}&gt;0,
\end{equation}
since $\mathbf{R}$ is positive definite due to $y_n\in(0,1)$ letting all the diagonal elements of $\mathbf{R}$ are positive. This positive definiteness claims that the cross-entropy error function is a concave function of $\mathbf{w}$ and thus has a unique minimum.</p>

<p>Back to our main attention, the Newton’s update of the model then takes the form
\begin{align}
\mathbf{w}^{(\text{new})}&amp;=\mathbf{w}^{(\text{old})}-(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}(\mathbf{y}-\mathbf{t}) \\ &amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\Big[\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\boldsymbol{\Phi}^\text{T}(\mathbf{y}-\mathbf{t})\Big] \\ &amp;=(\boldsymbol{\Phi}^\text{T}\mathbf{R}\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
where $\mathbf{z}$ is an $N$-dimensional vector given by
\begin{equation}
\mathbf{z}=\boldsymbol{\Phi}\mathbf{w}^{(\text{old})}-\mathbf{R}^{-1}(\mathbf{y}-\mathbf{t})
\end{equation}
This algorithm is known as <strong>iterative reweighted least squares</strong>, or <strong>IRLS</strong>.</p>

<h5 id="nm-softmax-reg">Softmax Regression</h5>
<p>Consider applying the Newton’s method to the cross-entropy error function \eqref{eq:sr.1} for the softmax regression model.</p>

<p>First, let $\mathbf{W}$ be the $M\times K$ matrix that comprises $\mathbf{w}_1,\ldots,\mathbf{w}_K$ together, as
\begin{equation}
\mathbf{W}=\left[\begin{matrix}\vert&amp;&amp;\vert \\ \mathbf{w}_1&amp;\ldots&amp;\mathbf{w}_K \\ \vert&amp;&amp;\vert\end{matrix}\right]
\end{equation}
By the result \eqref{eq:sr.2}, we have that the $k$-th column of the gradient of this error function is given by
\begin{equation}
\nabla_{\mathbf{w}_k}E(\mathbf{W})=\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}_k-\mathbf{T}_k),
\end{equation}
where $\boldsymbol{\Phi}$ be the $N\times M$ design matrix, given as
\begin{equation}
\boldsymbol{\Phi}=\left[\begin{matrix}-\hspace{0.1cm}\boldsymbol{\phi}_1^\text{T}\hspace{0.1cm}- \\ \hspace{0.1cm}\vdots\hspace{0.1cm} \\ -\hspace{0.1cm}\boldsymbol{\phi}_N^\text{T}\hspace{0.1cm}-\end{matrix}\right]
\end{equation}
and where $\mathbf{Y}_k,\mathbf{T}_k$ are the $k$th columns of the $N\times K$ matrices
\begin{equation}
\mathbf{Y}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{y}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{y}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right];\hspace{2cm}\mathbf{T}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{t}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{t}_N^\text{T}\hspace{0.15cm}-\end{matrix}\right]
\end{equation}
Therefore, the gradient of the error function w.r.t $\mathbf{W}$ can be written as
\begin{equation}
\nabla_\mathbf{W}E(\mathbf{W})=\boldsymbol{\Phi}^\text{T}(\mathbf{Y}-\mathbf{T})
\end{equation}
Now we consider the hessian matrix $\mathbf{H}$ of the error function, whose block $(k,j)$ is given by
\begin{align}
\mathbf{H}_{k j}&amp;=\nabla_{\mathbf{w}_j}\nabla_{\mathbf{w}_k} E(\mathbf{W}) \\ &amp;=\nabla_{\mathbf{w}_j}\sum_{n=1}^{N}\big[(y_n)_k-(\mathbf{t}_n)_k\big]\boldsymbol{\phi}_n \\ &amp;=\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}
\end{align}
Analogous to the binary case, the hessian $\mathbf{H}$ for the multi-class logistic regression model is positive semi-definite. To prove it, since $\mathbf{H}$ is an $MK\times MK$ matrix, consider an $MK$-dimensional vector $\mathbf{u}$. Thus, $\mathbf{u}$ can be represented as
\begin{equation}
\mathbf{u}=\left[\begin{matrix}\mathbf{u}_1^\text{T}&amp;\ldots&amp;\mathbf{u}_K^\text{T}\end{matrix}\right]^\text{T},
\end{equation}
where each $\mathbf{u}_k$ is a vector of length $M$, for $k=1,\ldots,K$. Therefore, we have
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\mathbf{H}_{k j}\mathbf{u}_j \\ &amp;=\sum_{k=1}^{K}\sum_{j=1}^{K}\mathbf{u}_k^\text{T}\sum_{n=1}^{N}(y_n)_k\big(1\{j=k\}-(y_n)_j\big)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}\sum_{j=1}^{K}(y_n)_k(y_n)_j\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_j\right] \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right]\label{eq:nsr.1}
\end{align}
Consider $f:\mathbb{R}^M\to\mathbb{R}$, defined as
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}
\end{equation}
Thus, it follows immediately from the definition of $f$ that $f$ is convex since
\begin{equation}
f(\mathbf{x})=\mathbf{x}^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{x}=\Vert\mathbf{x}^\text{T}\boldsymbol{\phi}_n\Vert_2^2\geq 0
\end{equation}
Let us apply <strong>Jensen’s inequality</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> for $f$ with observing that $\sum_{k=1}^{K}(y_n)_k=\sum_{j=1}^{K}(y_n)_j=1$, then \eqref{eq:nsr.1} can be continued to derive as
\begin{align}
\mathbf{u}^\text{T}\mathbf{H}\mathbf{u}&amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\mathbf{u}_k-\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k^\text{T}\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T}\sum_{j=1}^{K}(y_n)_j\mathbf{u}_j\right] \\ &amp;=\sum_{n=1}^{N}\left[\sum_{k=1}^{K}(y_n)_k f\left(\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;\geq\sum_{n=1}^{N}\left[f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)-f\left(\sum_{k=1}^{K}(y_n)_k\mathbf{u}_k\right)\right] \\ &amp;=0,
\end{align}
which claims the positive semi-definiteness of $\mathbf{H}$. Therefore, the error function $E(\mathbf{w})$ is concave and thus has a unique minimum.</p>

<h3 id="bayes-log-reg">Bayesian Logistic Regression</h3>
<p>When using Bayesian approach for logistic regression model, unlike the case of linear regression \eqref{eq:pd.1}, the posterior distribution now is no longer Gaussian. This makes the evaluation of posterior be intractable when integrating over the parameter $\mathbf{w}$.</p>

<p>Therefore, it is necessary to use some approximation methods.</p>

<h4 id="laplace-approx">The Laplace approximation</h4>
<p>The goal of <strong>Laplace approximation</strong> is to fit a Gaussian distribution to a probability density defined over a set of continuous variables</p>

<p>We begin by consider applying Laplace method to one-dimensional variables $z$ with the density function $p(z)$ is defined as
\begin{equation}
p(z)=\frac{1}{Z}f(z),
\end{equation}
where $Z=\int f(z)\,dz$ is the normalization coefficient, and is unknown.</p>

<p>The idea behind Laplace method is to place a Gaussian $q(z)$ on a mode of the distribution $p(z)$. A mode $z_0$ of $p(z)$ is where the distribution reaches its global maximum, which also means the derivative of $p(z)$ at $z_0$ is zero
\begin{equation}
\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}=0
\end{equation}
Therefore, the Taylor expansion of $\log f(z)$ about $z=z_0$ can be written by
\begin{align}
\log f(z)&amp;\simeq\log f(z_0)+\log f(z)\left.\frac{d f(z)}{dz}\right\vert_{z=z_0}(z-z_0)+\frac{1}{2}\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}(z-z_0)^2 \\ &amp;=\log f(z_0)-\frac{A}{2}(z-z_0)^2,
\end{align}
where
\begin{equation}
A=-\left.\frac{d^2\log f(z)}{d^2 z}\right\vert_{z=z_0}
\end{equation}
Thus, taking the exponential gives us
\begin{equation}
f(z)\simeq f(z_0)\exp\left(-\frac{A}{2}(z-z_0)^2\right),
\end{equation}
which is in a form of an unnormalized Gaussian distribution. Hence, we can obtain a Gaussian approximation $q(z)$ of $p(z)$ by adding a normalization parameter to form a Normal distribution, as
\begin{equation}
q(z)=\left(\frac{A}{2\pi}\right)^{1/2}\exp\left(-\frac{A}{2}(z-z_0)^2\right)=\mathcal{N}(\mathbf{z}\vert z_0,A^{-1})
\end{equation}
We can extend the Laplace approximation into multi-dimensional variable $\mathbf{z}$, which is finding an Gaussian approximation of distribution
\begin{equation}
p(\mathbf{z})=\frac{1}{Z}f(\mathbf{z}),
\end{equation}
where $z$ is a vector of length $M\geq 2$.</p>

<p>Analogy to the univariate case, the first step is to consider the Taylor expansion of $\log f(\mathbf{z})$ about its stationary point $\mathbf{z}_0$, which means $\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}=0$. We have
\begin{align}
\log f(\mathbf{z})&amp;\simeq f(\mathbf{z}_0)+\log f(\mathbf{z})\nabla_\mathbf{z}f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}+\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(\mathbf{z})\vert_{\mathbf{z}=\mathbf{z}_0}(\mathbf{z}-\mathbf{z}_0) \\ &amp;=\log f(\mathbf{z}_0)-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0),
\end{align}
where
\begin{equation}
\mathbf{A}=-\nabla_\mathbf{z}\nabla_\mathbf{z}\log f(z)\vert_{\mathbf{z}=\mathbf{z}_0}
\end{equation}
Taking the exponentials of both sides lets us obtain
\begin{equation}
f(\mathbf{z})\simeq f(\mathbf{z}_0)\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right),
\end{equation}
which is in form of an unnormalized multivariate Gaussian. Adding a normalization parameter gives us the Gaussian approximation $q(\mathbf{z})$ of $p(\mathbf{z})$
\begin{equation}
q(\mathbf{z})=\frac{\vert\mathbf{A}\vert^{1/2}}{(2\pi)^{M/2}}\exp\left(-\frac{1}{2}(\mathbf{z}-\mathbf{z}_0)^\text{T}\mathbf{A}(\mathbf{z}-\mathbf{z}_0)\right)=\mathcal{N}(\mathbf{z}\vert\mathbf{z}_0,\mathbf{A}^{-1})
\end{equation}</p>

<h4 id="approx-posterior">Approximation of the posterior</h4>
<p>Consider the prior to be a Gaussian, which is
\begin{equation}
p(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{m}_0,\mathbf{S}_0),
\end{equation}
where $\mathbf{m}_0$ and $\mathbf{S}_0$ are known. Along with this is the likelihood function, which is defined by \eqref{eq:lr.1}, as
\begin{equation}
p(\mathbf{t}\vert\mathbf{w})=\prod_{n=1}^{N}y_n^{t_n}(1-y_n)^{1-t_n},
\end{equation}
where $\mathbf{t}=(t_1,\ldots,t_N)^\text{T}$, and $y_n=\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi}_n)$. Therefore, by Bayes’ theorem, the posterior is given by
\begin{equation}
p(\mathbf{w}\vert\mathbf{t})\propto p(\mathbf{w})p(\mathbf{t}\vert\mathbf{w}),
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{w}\vert\mathbf{t})&amp;=-\frac{1}{2}(\mathbf{w}-\mathbf{m}_0)^\text{T}\mathbf{S}_0^{-1}(\mathbf{w}-\mathbf{m}_0)\nonumber \\ &amp;\hspace{2cm}+\sum_{n=1}^{N}\big[t_n\log y_n+(1-t_n)\log(1-y_n)\big]+c,
\end{align}
where $c$ is independent of $\mathbf{w}$.</p>

<p>By Laplace approximation, to find a Gaussian approximation of the posterior, the first step is looking for the point which maximizes the posterior, which is the $\mathbf{w}_\text{MAP}$. This point also defines the mean of the approximation. The corresponding covariance matrix $\mathbf{S}_N$ of the Gaussian is given by
\begin{align}
\mathbf{S}_N&amp;=-\nabla_\mathbf{w}\nabla_\mathbf{w}\log p(\mathbf{w}\vert\mathbf{t}) \\ &amp;=\mathbf{S}_0^{-1}+\sum_{n=1}^{N}y_n(1-y_n)\boldsymbol{\phi}_n\boldsymbol{\phi}_n^\text{T},
\end{align}
where the second step is obtained by using the result \eqref{eq:nlr.1}. Therefore, the Gaussian approximation $q(\mathbf{w})$ for the posterior distribution is given by
\begin{equation}
q(\mathbf{w})=\mathcal{N}(\mathbf{w}\vert\mathbf{w}_\text{MAP},\mathbf{S}_N)\label{eq:ap.1}
\end{equation}</p>

<h4 id="pred-dist-clf">Predictive distribution</h4>
<p>With the Gaussian approximation \eqref{eq:ap.1}, the predict distribution for class $\mathcal{C}_1$, given a new feature vector $\boldsymbol{\phi}(\mathbf{x})$, is then given by marginalizing w.r.t the posterior distribution $p(\mathbf{w}\vert\mathbf{t})$, as
\begin{equation}
p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})=\int p(\mathcal{C}_1\vert\boldsymbol{\phi}\mathbf{w})p(\mathbf{w}\vert\mathbf{t})\,d\mathbf{w}\simeq\int\sigma(\mathbf{w}^\text{T}\boldsymbol{\phi})q(\mathbf{w})\,d\mathbf{w}
\end{equation}
And thus, the predictive distribution for class $\mathcal{C}_2$ is given by
\begin{equation}
p(\mathcal{C}_2\vert\boldsymbol{\phi},\mathbf{t})=1-p(\mathcal{C}_1\vert\boldsymbol{\phi},\mathbf{t})
\end{equation}</p>

<h2 id="glm">Generalized linear models</h2>

<h2 id="references">References</h2>
<p>[1] <span id="bishops-book">Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</span></p>

<p>[2] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra, 5th edition</a>, 2016.</p>

<p>[3] MIT 18.06. <a href="https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/">Linear Algebra</a>.</p>

<p>[4] MIT 18.02. <a href="https://ocw.mit.edu/courses/18-02-multivariable-calculus-fall-2007/">Multivariable Calculus</a>.</p>

<p>[5] <a href="https://stats.stackexchange.com/users/28666/amoeba">amoeba</a>. <a href="https://stats.stackexchange.com/q/204599">What is an isotropic (spherical) covariance matrix?</a>. Cross Validated.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A covariance matrix $\mathbf{C}$ is <strong>isotropic</strong> (or <strong>spherical</strong>) if it is proportional to the identity matrix $\mathbf{I}$
\begin{equation*}
\mathbf{C}=\lambda\mathbf{I},
\end{equation*}
where $\lambda\in\mathbb{R}$ is a constant. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>For positive numbers $p_1,\ldots,p_n$ such that $\sum_{i=1}^{n}p_i=1$ and $f$ is a continuous function, if $f$ is <strong>convex</strong>, then
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\leq\sum_{i=1}^{n}p_if(x_i),
\end{equation*}
and if $f$ is <strong>concave</strong>, we instead have
\begin{equation*}
f\left(\sum_{i=1}^{n}p_ix_i\right)\geq\sum_{i=1}^{n}p_if(x_i),
\end{equation*} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="machine-learning" /><category term="linear-regression" /><category term="logistic-regression" /><category term="linear-discriminant-analysis" /><summary type="html"><![CDATA[A note on linear models]]></summary></entry></feed>