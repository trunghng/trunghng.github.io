<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-07-05T13:53:01+07:00</updated><id>/feed.xml</id><title type="html">Trung’s blog</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Measures</title><link href="/random-stuffs/measure-theory/2021/07/03/measure.html" rel="alternate" type="text/html" title="Measures" /><published>2021-07-03T07:00:00+07:00</published><updated>2021-07-03T07:00:00+07:00</updated><id>/random-stuffs/measure-theory/2021/07/03/measure</id><content type="html" xml:base="/random-stuffs/measure-theory/2021/07/03/measure.html">&lt;p&gt;&lt;img src=&quot;/assets/images/lego.jpg&quot; alt=&quot;Lego&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;
When talking about &lt;em&gt;measure&lt;/em&gt;, you might associate it with the idea of &lt;em&gt;length&lt;/em&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;em&gt;area&lt;/em&gt;, or even three dimensions with &lt;em&gt;volume&lt;/em&gt;.
Despite of having different number of dimensions, all &lt;em&gt;length&lt;/em&gt;, &lt;em&gt;area&lt;/em&gt;, &lt;em&gt;volume&lt;/em&gt; share the same properties:&lt;br /&gt;
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Non-negative&lt;/em&gt;: In principle, length, area, and volume can be any positive value. But negative length has no meaning. Same thing happens with negative area and negative volume.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Additivity&lt;/em&gt;: To get from Hanoi to Singapore by air, you have to transit at Ho Chi Minh city (HCMC). If we cut that path into two non-overlapping pieces, say Hanoi - HCMC, and HCMC - Singapore, then the total length of the two pieces must be equal to the length of original path. If we divide a rectangular into non-overlapping pieces, the area of pieces combined must be the same as the original one. The same is true for volume as well.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Empty Set&lt;/em&gt;: An empty cup of water has volume zero.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Other Null Sets&lt;/em&gt;: The length of a point is 0. The area of a line, or a curve is 0. The volumn of a plane or a surface is 0.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Translation Invariance&lt;/em&gt;: Length, area and volume are unchanged (&lt;em&gt;invariant&lt;/em&gt;) under shifts (&lt;em&gt;translation&lt;/em&gt;) in space.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Hyper-rectangles&lt;/em&gt;: An interval of form $[a, b]\subset\mathbb{R}^3$ has length $b-a$. The area of a rectangle $[a_1,b_1]\times[a_2,b_2]$ is $(b_1-a_1)(b_2-a_2)$. And the volume of a rectangular $[a_1,b_1]\times[a_2,b_2]\times[a_3,b_3]$ is $(b_1-a_1)(b_2-a_2)(b_3-a_3)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;lebesgue-measure&quot;&gt;Lebesgue measure&lt;/h3&gt;
&lt;p&gt;Is an extension of the classical notion of length in $\mathbb{R}$, area in $\mathbb{R}^2$ to any $\mathbb{R}^k$ using k-dimensional hyper-rectangles.&lt;/p&gt;

&lt;h4 id=&quot;definition&quot;&gt;Definition&lt;/h4&gt;
&lt;p&gt;Given an open set $S=\sum_k(a_k,b_k)$ cointaining disjoint invervals, the &lt;strong&gt;Lebesgue measure&lt;/strong&gt; is defined by:
\begin{equation}
\mu_L(S)=\sum_{k}(b_k-a_k)
\end{equation}
Given a closed set $S’=[a,b]-\sum_k(a_k,b_k)$,
\begin{equation}
\mu_L(S’)=(b-a)-\sum_k(b_k-a_k)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;measures&quot;&gt;Measures&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;br /&gt;
Let $\mathcal{X}$ be any set. A &lt;em&gt;measure&lt;/em&gt; on $\mathcal{X}$ is a function $\mu$ that maps the set of subsets on $\mathcal{X}$ to $[0,\infty]$ ($\mu:2^\mathcal{X}\rightarrow[0,\infty]$) that satisfies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mu(\emptyset)=0$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Countable additivity property&lt;/em&gt;: for any countable and pairwise disjoint collection of subsets of $\mathcal{X},\mathcal{A_1},\mathcal{A_2},\dots$,
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)=\sum_i\mu(\mathcal{A_i})
\end{equation}
$\mu(\mathcal{A})$ is called &lt;em&gt;measure of the set $\mathcal{A}$&lt;/em&gt;, or &lt;em&gt;measure of $\mathcal{A}$&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;: If $\mathcal{A}\subset\mathcal{B}$, then $\mu(\mathcal{A})\leq\mu(\mathcal{B})$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Subadditivity&lt;/em&gt;: If $\mathcal{A_1},\mathcal{A_2},\dots$ is a countable collection of sets, not necessarily disjoint, then
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)\leq\sum_i\mu(\mathcal{A_i})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Cardinality of a set&lt;/em&gt; \(\#\mathcal{A}\)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;A point mass at 0&lt;/em&gt;. Consider a measure \(\delta_{\{0\}}\) on $\mathbb{R}$ defined to give measure 1 to any set that contains 0 and measure 0 to any set that does not
\begin{equation}\delta_{{0}}(\mathcal{A})=\#\left(A\cap\{0\}\right)=\begin{cases}
1\quad\textsf{if }0\in\mathcal{A} \\ 0\quad\textsf{otherwise}
\end{cases}\end{equation}
for $\mathcal{A}\subset\mathbb{R}$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Counting measure on the integers&lt;/em&gt;. Consider a measure $\mu_\mathbb{Z}$ that assigns to each set $\mathcal{A}$ the number of integers contained in $\mathcal{A}$
\begin{equation}
\delta_\mathbb{Z}(\mathcal{A})=\#\left(\mathcal{A}\cap\mathbb{Z}\right)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Geometric measure&lt;/em&gt;. Suppose that $0&amp;lt;r&amp;lt;1$. We define a measure on $\mathbb{R}$ that assigns to a set $\mathcal{A}$ a geometrically weighted sum over non-negative integers in $\mathcal{A}$
\begin{equation}
\mu(\mathcal{A})=\sum_{i\in\mathcal{A}\cap\mathbb{Z}^+}r^i
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Binomial measure&lt;/em&gt;. Let $n\in\mathbb{N}^+$ and let $0&amp;lt;p&amp;lt;1$. We define $\mu$ as:
\begin{equation}
\mu(\mathcal{A})=\sum_{k\in\mathcal{A}\cap\{0,1,\dots,n\}}{n\choose k}p^k(1-p)^{n-k}
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bivariate Gaussian&lt;/em&gt;. We define a measure on $\mathbb{R}^2$ by:
\begin{equation}
\mu({\mathcal{A}})=\int_\mathcal{A}\dfrac{1}{2\pi}\exp\left({\dfrac{-1}{2}(x^2+y^2)}\right)\,dx\,dy
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Uniform on a Ball in $\mathbb{R}^3$&lt;/em&gt;. Let $\mathcal{B}$ be the set of points in $\mathbb{R}^3$ that are within a distance 1 from the origin (unit ball in $\mathbb{R}^3$). We define a measure on $\mathbb{R}^3$ as:
\begin{equation}
\mu(\mathcal{A})=\dfrac{3}{4\pi}\mu_L(\mathcal{A}\cap\mathcal{B})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;integration-with-respect-to-a-measure-the-idea&quot;&gt;Integration with respect to a Measure: The Idea&lt;/h3&gt;
&lt;p&gt;Consider $f:\mathcal{X}\rightarrow\mathbb{R}$, where $\mathcal{X}$ is any set and a measure $\mu$ on $\mathcal{X}$ and compute the integral of $f$ w.r.t $\mu$: $\int f(x)\,\mu(dx)$. We have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\mu_L(dx)=\int g(x)\,dx
\end{equation}
Because $\mu_L(dx)\equiv\mu_L([x,x+dx[)=dx$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_{\{\alpha\}}(dx)=g(\alpha)
\end{equation}
Consider the infinitesimal $\delta_{\{\alpha\}}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\neq\alpha$, then the infinitesimal interval $[x,x+dx[$ does not contain $\alpha$, so
\begin{equation}
\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=0
\end{equation}
If $x=\alpha,\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=1$. Thus, when we add up all of the infinitesimals, we get $g(\alpha)\cdot1=g(\alpha)$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathbb{Z}(dx)=\sum_{i\in\mathbb{Z}}g(i)
\end{equation}
Similarly, consider the infinitesimal $\delta_\mathbb{Z}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\notin\mathbb{Z}$, then $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=0$. And otherwise if $x\in\mathbb{Z}$, $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=1$ since an infinitesimal interval can contain at most one integer. Hence, $g(x)\,\delta_\mathbb{Z}=g(x)$ if $x\in\mathbb{Z}$ and $=0$ otherwise. When we add up all of the infinitesimals over $x$, we get the sum above.&lt;/li&gt;
  &lt;li&gt;Suppose $\mathcal{C}$ is a countable set. We can define &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{C}$ to map $\mathcal{A}\rightarrow\#(\mathcal{A}\cap\mathcal{C})$ (recall that $\delta_\mathcal{C}(\mathcal{A})=\#(\mathcal{A}\cap\mathcal{C})$). For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v),
\end{equation}
using the same basic argument as in the above example.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above examples, we have that &lt;em&gt;integrals w.r.t to Lebesgue measure are just ordinary integrals, and that integrals w.r.t Counting measure are just ordinary summation&lt;/em&gt;.&lt;br /&gt;
Consider measures built from Lebesgue and Counting measure, we have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=f(x)\,\mu_L(dx)$, then for any function $g$,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,f(x)\,\mu_L(dx)=\int g(x)\,f(x)\,dx
\end{equation}
We say that $f$ is the density of $\mu$ w.r.t Lebesgue measure in this case.&lt;/li&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=p(x)\delta_\mathcal{C}(dx)$ for a countable set $\mathcal{C}$, then for any function g,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,p(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v)\,f(v)
\end{equation}
We say that $p$ is the density of $\mu$ w.r.t Counting measure on $\mathcal{C}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;properties-of-the-integral&quot;&gt;Properties of the Integral&lt;/h3&gt;
&lt;p&gt;A function is said to be &lt;em&gt;integrable&lt;/em&gt; w.r.t $\mu$ if $\int|f(x)|\,\mu(dx)&amp;lt;\infty$. An integrable function has a well-defined and finite integral. If $f(x)\geq0$, the integral is always well-defined but may be $\infty$.&lt;br /&gt;
Suppose $\mu$ is a measure on $\mathcal{X},\mathcal{A}\subset\mathcal{X}$, and $g$ is a real-valued function on $\mathcal{X}$. We define the integral of $g$ over the set $\mathcal{A}$, denoted by $\int_\mathcal{A}g(x)\,\mu(dx)$, as
\begin{equation}
\int_\mathcal{A}g(x)\,\mu(dx)=\int g(x)\,𝟙_\mathcal{A}(x)\,\mu(dx),
\end{equation}
where \(𝟙_\mathcal{A}\) is an &lt;em&gt;indicator function&lt;/em&gt; (\(𝟙_\mathcal{A}(x)=1\) if $x\in\mathcal{A}$, and $=0$ otherwise).&lt;/p&gt;

&lt;p&gt;Let $\mu$ is a measure on $\mathcal{X},\mathcal{A},\mathcal{B}\subset\mathcal{X},c\in\mathbb{R}$ and $f,g$ are integrable functions. The following properties hold for every $\mu$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Constant Functions&lt;/em&gt;
\begin{equation}
\int_\mathcal{A}c\,\mu(dx)=c\cdot\mu(\mathcal{A})
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity&lt;/em&gt;
\begin{align}
\int_\mathcal{A}cf(x)\,\mu(dx)&amp;amp;=c\int_\mathcal{A}f(x)\,\mu(dx) \\\int_\mathcal{A}\left(f(x)+g(x)\right)\,\mu(dx)&amp;amp;=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{A}g(x)\,\mu(dx)
\end{align}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;. If $f\leq g$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{A}g(x)\,\mu(dx),\forall\mathcal{A}$. This implies:
    &lt;ul&gt;
      &lt;li&gt;If $f\geq0$, then $\int f(x)\,\mu(dx)\geq0$.&lt;/li&gt;
      &lt;li&gt;If $f\geq0$ and $\mathcal{A}\subset\mathcal{B}$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{B}f(x)\,\mu(dx)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Null Sets&lt;/em&gt;. If $\mu(\mathcal{A})=0$, then $\int_\mathcal{A}f(x)\,\mu(dx)=0$.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Absolute Values&lt;/em&gt;
\begin{equation}
\left|\int f(x)\,\mu(dx)\right|\leq\int\left|f(x)\right|\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotone Convergence&lt;/em&gt;. If $0\leq f_1\leq f_2\leq\dots$ is an increasing sequence of integrable functions that converge to $f$, then
\begin{equation}
\lim_{k\to\infty}\int f_k(x)\,\mu(dx)=\int f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity in region of integration&lt;/em&gt;. If $\mathcal{A}\cap\mathcal{B}=\emptyset$,
\begin{equation}
\int_{\mathcal{A}\cup\mathcal{B}}f(x)\,\mu(dx)=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{B}f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;integration-with-respect-to-a-measure-the-details&quot;&gt;Integration with respect to a Measure: The Details&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Step 1&lt;/em&gt;. Define the integral for simple functions.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simple function&lt;/em&gt;: is a function that takes only a finite number of different values.
        &lt;ul&gt;
          &lt;li&gt;All constant functions are simple functions.&lt;/li&gt;
          &lt;li&gt;The indicator function ($𝟙_\mathcal{A}$) of a set $\mathcal{A}\subset\mathcal{X}$ is a simple function (taking values in $\{0,1\}$).&lt;/li&gt;
          &lt;li&gt;Any constant times an indicator ($c𝟙_\mathcal{A}$) is also a simple function (taking values in $\{0,c\}$).&lt;/li&gt;
          &lt;li&gt;Similarly, given disjoint sets $\mathcal{A_1},\mathcal{A_2}$, the linear combination \(c_1𝟙_\mathcal{A_1}+c_2𝟙_\mathcal{A_2}\) is a simple function (taking values in $\{0,c_1,c_2\}$)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
          &lt;li&gt;In fact, any simple function can be expressed as a linear combination of a finite number of indicator funtions. That is, if $f$ is &lt;em&gt;any&lt;/em&gt; simple function on $\mathcal{X}$, then there exists some finite integer $n$, non-zero constants $c_1,\dots,c_n$ and &lt;em&gt;disjoint&lt;/em&gt; sets $\mathcal{A_1},\dots\mathcal{A_n}\subset\mathcal{X}$ such that
   \begin{equation}
   f=c_1𝟙_\mathcal{A_1}+\dots+c_n𝟙_\mathcal{A_n}
   \end{equation}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;So, if $f:\mathcal{X}\to\mathbb{R}$ is a simple function as just defined, we have that
\begin{equation}
\int \mu(dx)=c_1\mu(\mathcal{A_1})+\dots+c_n\mu(\mathcal{A_n})
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Step 2&lt;/em&gt;. Define the integral for general non-negative functions, approximating the general function by simple functions.
    &lt;ul&gt;
      &lt;li&gt;The idea is that we can approximate any general non-negative function $f:\mathcal{X}\to[0,\infty[$ well by some non-negative simple functions that $\leq f$ &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
      &lt;li&gt;If $f:\mathcal{X}\to[0,\infty[$ is a general function and $0\leq s\leq f$ is a simple function (then $\int s(x)\,\mu(dx)\leq\int f(x)\,\mu(dx)$). The closer that $s$ approximates $f$, the closer we expect $\int s(x)\,\mu(dx)$ and $\int f(x)\,\mu(x)$ to be.&lt;/li&gt;
      &lt;li&gt;To be more precise, we define the integral $\int f(x)\,\mu(dx)$ to be the smallest value $I$ such that $\int s(x)\,\mu(x)\leq I$, for all simple functions $0\leq s\leq f$.
\begin{equation}
\int f(x)\,\mu(dx)\approx\sup\left\{\int s(x)\,\mu(dx)\right\}
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Step 3&lt;/em&gt;. Define the integral for general real-valued functions by separately integrating the positive and negative parts of the function.
    &lt;ul&gt;
      &lt;li&gt;If $f:\mathcal{X}\to\mathbb{R}$ is a general function, we can dfeine its &lt;em&gt;positive part&lt;/em&gt; $f^+$ and its &lt;em&gt;negative part&lt;/em&gt; $f^-$ by
\begin{align}
f^+(x)&amp;amp;=\max\left(f(x),0\right) \\ f^-(x)&amp;amp;=\max\left(-f(x),0\right)
\end{align}&lt;/li&gt;
      &lt;li&gt;Since both $f^+$ and $f^-$ are non-negative functions and $f=f^+-f^-$, we have
\begin{equation}
\int f(x)\,\mu(dx)=\int f^+(x)\,\mu(dx)-\int f^-(x)\,\mu(dx)
\end{equation}
This is a well-defined number (possibly infinite) if and only if at least one of $f^+$ and $f^-$ has a finite integral.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;constructing-measures-from-old-ones&quot;&gt;Constructing Measures from old ones&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Sums and multiples&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Consider the point mass measures at 0 and 1, \(\delta_{\\{0\\}},\delta_1\), and construct a two new measures on $\mathbb{R}$, \(\mu=\delta_{\\{0\\}}+\delta_{\\{1\\}}\) and \(v=4\delta_{\\{0\\}}\), defined by
\begin{align}
\mu(\mathcal{A})&amp;amp;=\delta_{\{0\}}(\mathcal{A})+\delta_{\{0\}}(\mathcal{A}) \\ v(\mathcal{A})&amp;amp;=4\delta_{\{0\}}(\mathcal{A})
\end{align}&lt;/li&gt;
      &lt;li&gt;The measure $\mu$ counts how mnay elements of \(\\{0,1\\}\) are in its argument. Thus, the counting measure of the integers can be re-expressed as
\begin{equation}
\delta_\mathbb{Z}=\sum_{i=-\infty}^{\infty}\delta_{\{i\}}
\end{equation}&lt;/li&gt;
      &lt;li&gt;By combining the operations of summation and multiplication, we can write the Geometric measure in the above example as \(\sum_{i=0}^{\infty}r^i\delta_{\\{i\\}}\)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Restriction to a Subset&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $\mathcal{B}\subset\mathcal{X}$. We can define a new measure on $\mathcal{B}$ which maps $\mathcal{A}\subset\mathcal{B}\to\mu(\mathcal{A})$. This is called the restriction of $\mu$ to the set $\mathcal{B}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Measure Induced by a Function&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $g:\mathcal{X}\to\mathcal{Y}$. We can use $\mu$ and $g$ to define a new measure $v$ on $\mathcal{Y}$ by
\begin{equation}
v(\mathcal{A})=\mu(g^{-1}(\mathcal{A})),
\end{equation}
for $\mathcal{A}\subset\mathcal{Y}$. This is called the &lt;em&gt;measure induced from $\mu$ by $g$&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Therefore, for any $f:\mathcal{Y}\to\mathbb{R}$,
\begin{equation}
\int f(y)\,v(dy)=\int f(g(x))\,\mu(dx)
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integrating a Density&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $f:\mathcal{X}\to\mathbb{R}$. We can define a new measure $v$ on $\mathcal{X}$ as
\begin{equation}
v(\mathcal{A})=\int_\mathcal{A}f(x)\,\mu(dx)\tag{1}\label{1}
\end{equation}
We say that $f$ is the &lt;em&gt;density&lt;/em&gt; of the measure $v$ w.r.t $\mu$.&lt;/li&gt;
      &lt;li&gt;If $v,\mu$ are measures for which the equation \eqref{1} holds for every $\mathcal{A}\subset\mathcal{X}$, we say that $v$ has a density $f$ w.r.t $\mu$. This implies two useful results:
        &lt;ul&gt;
          &lt;li&gt;$\mu(\mathcal{A})=0$ implies $v(\mathcal{A})=0$.&lt;/li&gt;
          &lt;li&gt;$v(dx)=f(x)\,\mu(dx)$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;other-types-of-measures&quot;&gt;Other types of Measures&lt;/h3&gt;
&lt;p&gt;Suppose that $\mu$ is a measure on $\mathcal{X}$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If $\mu(\mathcal{X})=\infty$, we say that $\mu$ is an &lt;em&gt;infinite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;\infty)$, we say that $\mu$ is a &lt;em&gt;finite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;1)$, we say that $\mu$ is a &lt;em&gt;probability measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If there exists a countable set $\mathcal{S}$ such taht $\mu(\mathcal{X}-\mathcal{S})=0$, we say that $\mu$ is a &lt;em&gt;discrete measure&lt;/em&gt;. Equivalently, $\mu$ has a density w.r.t &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{S}$.&lt;/li&gt;
  &lt;li&gt;If $\mu$ has a density w.r.t Lebesgue measure, we say that $\mu$ is a &lt;em&gt;continuous measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu$ is neither &lt;em&gt;continuous&lt;/em&gt; nor &lt;em&gt;discrete&lt;/em&gt;, we say that $\mu$ is a &lt;em&gt;mixed measure&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Litterally, this post is mainly written from a source that I’ve lost the reference :(. Hope that I can update this line soon.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mathworld.wolfram.com/LebesgueMeasure.html&quot;&gt;Lebesgue Measure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.countbayesie.com/blog/2015/8/17/a-very-brief-and-non-mathematical-introduction-to-measure-theory-for-probability&quot;&gt;Measure Theory for Probability: A Very Brief Introduction&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;other-resources&quot;&gt;Other resources&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cyW5z-M2yzw&quot;&gt;Music and Measure Theory - 3Blue1Brown&lt;/a&gt; - this is one of my favourite Youtube channels.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If $\mathcal{A_1},\mathcal{A_2}$ were not disjoint, we could define $\mathcal{B_1}=\mathcal{A_1}-\mathcal{A_2}$, $\mathcal{B_2}=\mathcal{A_2}-\mathcal{A_1}$, and $\mathcal{B_3}=\mathcal{A_1}\cap\mathcal{A_2}$. Then the function is equal to \(c_1𝟙_\mathcal{B_1}+c_2𝟙_\mathcal{B_2}+(c_1+c_2)𝟙_\mathcal{B_3}\). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="measure-theory" /><summary type="html">When talking about measure, you might associate it with the idea of length, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with area, or even three dimensions with volume. Despite of having different number of dimensions, all length, area, volume share the same properties:</summary></entry><entry><title type="html">Markov Decision Process, Bellman equations</title><link href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html" rel="alternate" type="text/html" title="Markov Decision Process, Bellman equations" /><published>2021-06-27T08:00:00+07:00</published><updated>2021-06-27T08:00:00+07:00</updated><id>/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn</id><content type="html" xml:base="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html">&lt;p&gt;You may have known or heard vaguely about a computer program called &lt;a href=&quot;https://deepmind.com/research/case-studies/alphago-the-story-so-far&quot;&gt;AlphaGo&lt;/a&gt; - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with &lt;strong&gt;Reinforcement Learning&lt;/strong&gt;.&lt;br /&gt;
&lt;!-- excerpt-end --&gt;&lt;/p&gt;

&lt;h3 id=&quot;what-is-reinforcement-learning&quot;&gt;What is Reinforcement Learning?&lt;/h3&gt;
&lt;p&gt;Say, there is an unknown &lt;strong&gt;environment&lt;/strong&gt; that we’re trying to put an &lt;strong&gt;agent&lt;/strong&gt; on. By interacting with the &lt;strong&gt;agent&lt;/strong&gt; through taking &lt;strong&gt;actions&lt;/strong&gt; that gives rise to &lt;strong&gt;rewards&lt;/strong&gt; continually, the &lt;strong&gt;agent&lt;/strong&gt; learns a &lt;strong&gt;policy&lt;/strong&gt; that maximize the cumulative &lt;strong&gt;rewards&lt;/strong&gt;.&lt;br /&gt;
&lt;strong&gt;Reinforcement Learning (RL)&lt;/strong&gt;, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called &lt;strong&gt;policy&lt;/strong&gt;) for the &lt;strong&gt;agent&lt;/strong&gt; from experimental trials and relative simple feedback received. With the optimal &lt;strong&gt;policy&lt;/strong&gt;, the &lt;strong&gt;agent&lt;/strong&gt; is capable to actively adapt to the environment to maximize future &lt;strong&gt;rewards&lt;/strong&gt;.
&lt;img src=&quot;/assets/images/robot.png&quot; alt=&quot;RL&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;markov-decision-process-mdp&quot;&gt;Markov Decision Process (MDP)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Markov decision processes (MDPs)&lt;/strong&gt; formally describe an environment for &lt;strong&gt;RL&lt;/strong&gt;. And almost all &lt;strong&gt;RL&lt;/strong&gt; problems can be formalised as &lt;strong&gt;MDPs&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition (MDP)&lt;/strong&gt;&lt;br /&gt;
A &lt;strong&gt;Markov Decision Process&lt;/strong&gt; is a tuple $⟨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma⟩$&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\mathcal{S}$ is a set of states called &lt;em&gt;state space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal{A}$ is a set of actions called &lt;em&gt;action space&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;$\mathcal{P}$ is a state transition probability matrix&lt;br /&gt;
  \(\mathcal{P}^a_{ss&apos;}=P(S_{t+1}=s&apos;|S_t=s,A_t=a)\)&lt;/li&gt;
  &lt;li&gt;$\mathcal{R}$ is a reward function&lt;br /&gt;
  \(\mathcal{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]\)&lt;/li&gt;
  &lt;li&gt;$\gamma\in[0, 1]$ is a discount factor for future reward&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;MDP&lt;/strong&gt; is an extension of &lt;a href=&quot;/random-stuffs/probability-statistics/2021/06/19/markov-chain.html&quot;&gt;Markov chain&lt;/a&gt;. If only one action exists for each state, and all rewards are the same, an &lt;strong&gt;MDP&lt;/strong&gt; reduces to a &lt;em&gt;Markov chain&lt;/em&gt;. All states in &lt;strong&gt;MDP&lt;/strong&gt; has &lt;em&gt;Markov property&lt;/em&gt;, referring to the fact that the current state captures all relevant information from the history
\begin{equation}
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\dots,S_t)
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;return&quot;&gt;Return&lt;/h4&gt;
&lt;p&gt;In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the &lt;strong&gt;expected return&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Return&lt;/em&gt;)&lt;br /&gt;
The &lt;strong&gt;return&lt;/strong&gt; $G_t$ is the total discounted reward from t
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called &lt;em&gt;discount rate&lt;/em&gt; (or &lt;em&gt;discount factor&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;discount rate&lt;/em&gt; $\gamma$ determines the present value of future rewards: a reward received
k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.&lt;/p&gt;

&lt;h4 id=&quot;policy&quot;&gt;Policy&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;, which is denoted as $\pi$, is the behaviour function of the agent. $\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either &lt;em&gt;deterministic&lt;/em&gt; or &lt;em&gt;stochastic&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Deterministic policy&lt;/em&gt;:	$\quad\pi(s)=a$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Stochastic policy&lt;/em&gt;: $\quad\pi(a|s)=P(A_t=a|S_t=s)$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;value-function&quot;&gt;Value Function&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Value function&lt;/strong&gt; measures &lt;em&gt;how good&lt;/em&gt; a particular state is (or &lt;em&gt;how good&lt;/em&gt; it is to perform a given action in a given state).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;state-value function&lt;/em&gt;)&lt;br /&gt;
The &lt;strong&gt;state-value function&lt;/strong&gt; of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=E_\pi[G_t|S_t=s]
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;action-value function&lt;/em&gt;)&lt;br /&gt;
Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=E_\pi[G_t|S_t=s,A_t=a]
\end{equation}&lt;/p&gt;

&lt;p&gt;Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}&lt;/p&gt;

&lt;h4 id=&quot;optimal-policy-and-optimal-value-function&quot;&gt;Optimal Policy and Optimal Value Function&lt;/h4&gt;
&lt;p&gt;For finite MDPs (finte state and action space), we can precisely define an &lt;strong&gt;optimal policy&lt;/strong&gt;. &lt;em&gt;Value functions&lt;/em&gt; define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words,
\begin{equation}
\pi\geq\pi’\iff v_\pi(s)\geq v_{\pi’} \forall s\in\mathcal{S}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Optimal policy&lt;/em&gt;)&lt;br /&gt;
For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal to all other policies,
\begin{equation}
\pi_*\geq\pi,\forall\pi
\end{equation}&lt;/p&gt;

&lt;p&gt;The proof of the above theorem is gonna be provided in another post since we need some additional tools to do that.&lt;/p&gt;

&lt;p&gt;There may be more than one &lt;strong&gt;optimal policy&lt;/strong&gt;, they share the same &lt;em&gt;state-value function&lt;/em&gt;, called &lt;strong&gt;optimal state-value function&lt;/strong&gt; though.
\begin{equation}
v_*(s)=\max_{\pi}v_\pi(s)
\end{equation}
&lt;strong&gt;Optimal policies&lt;/strong&gt; also share the same &lt;em&gt;action-value function&lt;/em&gt;, call &lt;strong&gt;optimal action-value function&lt;/strong&gt;
\begin{equation}
q_*(s,a)=\max_{\pi}q_\pi(s,a)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;bellman-equations&quot;&gt;Bellman Equations&lt;/h3&gt;
&lt;p&gt;A fundamental property of &lt;em&gt;value functions&lt;/em&gt; used throughout RL is that they satisfy recursive relationships
\begin{align}
v_\pi(s)&amp;amp;=E_\pi[G_t|S_t=s] \\&amp;amp;=E_\pi[R_t+\gamma G_{t+1}|S_t=s] \\&amp;amp;=\sum_{s’,r,g’,a}p(s’,r,g’,a|s)(r+\gamma g’) \\&amp;amp;=\sum_{a}p(a|s)\sum_{s’,r,g’}p(s’,r,g’|a,s)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r,g’}p(s’,r|a,s)p(g’|s’,r,a,s)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\sum_{g’}p(g’|s’)(r+\gamma g’) \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma\sum_{g’}p(g’|s’)g’\right] \\&amp;amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma v_\pi(s’)\right],
\end{align}
where $p(s’,r|s,a)=P(S_{t+1}=s’,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the &lt;em&gt;Bellman equation for&lt;/em&gt; $v_\pi(s)$. It expresses a relationship between the value state $s$, $v_\pi(s)$ and the values of its successor states $s’$, $v_\pi(s’)$.&lt;/p&gt;

&lt;p&gt;Similarly, we define the &lt;em&gt;Bellman equation for&lt;/em&gt; $q_\pi(s,a)$
\begin{align}
q_\pi(s,a)&amp;amp;=E_\pi[G_t|S_t=s,A_t=a] \\&amp;amp;=E_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\sum_{a’}\pi(a’|s’)q_\pi(s’,a’)\right]
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;bellman-backup-diagram&quot;&gt;Bellman backup diagram&lt;/h4&gt;
&lt;p&gt;Backup diagram of &lt;em&gt;state-value function&lt;/em&gt; and &lt;em&gt;action-value function&lt;/em&gt; respectively&lt;/p&gt;
&lt;p float=&quot;left&quot;&gt;
  &lt;img src=&quot;/assets/images/state.png&quot; width=&quot;350&quot; /&gt;
  &lt;img src=&quot;/assets/images/action.png&quot; width=&quot;350&quot; /&gt; 
&lt;/p&gt;

&lt;h4 id=&quot;bellman-optimality-equations&quot;&gt;Bellman Optimality Equations&lt;/h4&gt;
&lt;p&gt;Since $v_*$ is the value function for a policy, it must satisfy the &lt;em&gt;Bellman equation for state-values&lt;/em&gt;. Moreover, it is also the optimal value function, then we have
\begin{align}
v_*(s)&amp;amp;=\max_{a\in\mathcal{A(s)}}q_{\pi_*}(s,a) \\&amp;amp;=\max_{a}E_{\pi_*}[G_t|S_t=s,A_t=a] \\&amp;amp;=\max_{a}E_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;amp;=\max_{a}E[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\&amp;amp;=\max_{a}\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_*(s’)]
\end{align}
The last two equations are two forms of the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $v_*$. Similarly, we have the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $q_*$
\begin{align}
q_*(s,a)&amp;amp;=E\left[R_{t+1}+\gamma\max_{a’}q_*(S_{t+1},a’)|S_t=s,A_t=a\right] \\&amp;amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\max_{a’}q_*(s’,a’)\right]
\end{align}&lt;/p&gt;

&lt;h4 id=&quot;backup-diagram-for-v_-and-q_&quot;&gt;Backup diagram for $v_*$ and $q_*$&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/opt.png&quot; alt=&quot;backup diagram for optimal value func&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Reinforcement Learning: An Introduction - Richard S. Sutton &amp;amp; Andrew G. Barto&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;UCL course on RL&lt;/a&gt; - David Silver&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html&quot;&gt;A (Long) Peek into Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><summary type="html">You may have known or heard vaguely about a computer program called AlphaGo - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called self-play against its other instances, with Reinforcement Learning.</summary></entry><entry><title type="html">Markov Chain</title><link href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html" rel="alternate" type="text/html" title="Markov Chain" /><published>2021-06-19T22:27:00+07:00</published><updated>2021-06-19T22:27:00+07:00</updated><id>/random-stuffs/probability-statistics/2021/06/19/markov-chain</id><content type="html" xml:base="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">&lt;p&gt;Since I have no idea how to begin with this post, why not just dive straight into details :P&lt;/p&gt;

&lt;p&gt;Markov chain is a stochastic process in which the random variables follow a special property called &lt;em&gt;Markov&lt;/em&gt;.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;

&lt;h3 id=&quot;markov-property&quot;&gt;Markov property&lt;/h3&gt;
&lt;p&gt;A sequence of random variables $X_0, X_1, X_2, \dots$ taking values in the &lt;em&gt;state space&lt;/em&gt; $S=${$1, 2,\dots, M$}. For all $n\geq0$,
\begin{equation}
P(X_{n+1}=j|X_n=i)=P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},X_{n-2}=i_{n-2},\dots,X_0=i_0)
\end{equation}
In other words, knowledge of the preceding state is all we need to determine the probability distribution of the current state.&lt;/p&gt;

&lt;h3 id=&quot;transition-matrix&quot;&gt;Transition matrix&lt;/h3&gt;
&lt;p&gt;The quantity $P(X_{n+1}=j|X_n=i)$ is &lt;em&gt;transition probability&lt;/em&gt; from state $i$ to $j$.&lt;br /&gt;
If we denote that $q_{ij}=P(X_{n+1}=j|X_n=i)$ and let $Q=(q_{ij})$, which is a $M\times M$ matrix, there we have the &lt;em&gt;transition matrix&lt;/em&gt; $Q$ of the chain.&lt;br /&gt;
Therefore, each row of $Q$ is a conditional probability mass function (PMF) of $X_{n+1}$ given $X_n$. And hence, sum of its entries is 1.&lt;/p&gt;

&lt;h4 id=&quot;n-step-transition-probability&quot;&gt;n-step transition probability&lt;/h4&gt;
&lt;p&gt;The n-step &lt;em&gt;transition probability&lt;/em&gt; from $i$ to $j$ is the probability of being at $i$ and $n$ steps later being at $j$, and be denoted as $q_{ij}^{(n)}$,
\begin{equation}
q_{ij}^{(n)}=P(X_n=j|X_0=i)
\end{equation}
We have that
\begin{equation}
q_{ij}^{(2)}=\sum_{k}^{}q_{ik}q_{kj}
\end{equation}
since it has to go through an intermediary step $k$ to reach $j$ in 2 steps from $i$. It’s easily seen that the right hand side is $Q_{ij}^2$. And by induction, we have that:
\begin{equation}
q_{ij}^{(n)}=Q_{ij}^{n}
\end{equation}
$Q^n$ is also called the &lt;em&gt;n-step transition matrix&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;marginal-distribution-of-x_n&quot;&gt;Marginal distribution of $X_n$&lt;/h4&gt;
&lt;p&gt;Let $t=(t_1,\dots,t_M)^T$, where $t_i=P(X_0=i)$. By the law of total probability (LOTP), we have that:
\begin{align}
P(X_n=j)&amp;amp;=\sum_{i=1}^{M}P(X_0=i)P(X_n=j|X_0=i) \\&amp;amp;=\sum_{i=1}^{M}t_iq_{ij}^{(n)}
\end{align}
or the marginal distribution of $X_n$ is given by $tQ^n$.&lt;/p&gt;

&lt;h3 id=&quot;properties&quot;&gt;Properties&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;State $i$ of a Markov chain is defined as &lt;em&gt;recurrent&lt;/em&gt; or &lt;em&gt;transient&lt;/em&gt; depending upon whether or not the Markov chain will eventually return to it. Starting with &lt;em&gt;recurrent&lt;/em&gt; state i, the chain will return to it with the probability of 1. Otherwise, it is &lt;em&gt;transient&lt;/em&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: Number of returns to &lt;em&gt;transient&lt;/em&gt; state is distributed by &lt;em&gt;Geom($p$)&lt;/em&gt;, with $p&amp;gt;0$ is the probability of never returning to $i$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A Markov chain is defined as &lt;em&gt;irreducible&lt;/em&gt; if there exists a chain of steps between any $i,j$ that has positive probability. That is for any $i,j$, there is some $n&amp;gt;0,\in\mathbb{N}$ such that $Q^n_{ij}&amp;gt;0$. If not &lt;em&gt;irreducible&lt;/em&gt;, it’s called &lt;em&gt;reducible&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: &lt;em&gt;Irreducible&lt;/em&gt; implies all states &lt;em&gt;recurrent&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A state $i$ has &lt;em&gt;period&lt;/em&gt; $k&amp;gt;0$ if $k$ is the greatest common divisor (gcd) of the possible numbers of steps it can take to return to $i$ when starting at $i$.
And thus, $k=gcd(n)$ such that $Q^n_{ii}&amp;gt;0$. $i$ is called &lt;em&gt;aperiodic&lt;/em&gt; if $k_i=1$, and &lt;em&gt;periodic&lt;/em&gt; otherwise. The chain itself is called &lt;em&gt;aperiodic&lt;/em&gt; if all its states are &lt;em&gt;aperiodic&lt;/em&gt;, and &lt;em&gt;periodic&lt;/em&gt; otherwise.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;stationary-distribution&quot;&gt;Stationary distribution&lt;/h3&gt;
&lt;p&gt;A vector $s=(s_1,\dots,s_M)^T$ such that $s_i\geq0$ and $\sum_{i}s_i=1$ is a &lt;em&gt;stationary distribution&lt;/em&gt; for a Markov chain if
\begin{equation}
\sum_{i}s_iq_{ij}=s_j
\end{equation}
for all $j$, or equivalently $sQ=s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Existence and uniqueness of stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Any &lt;em&gt;irreducible&lt;/em&gt; Markov chain has a unique &lt;em&gt;stationary distribution&lt;/em&gt;. In this distribution, every state has positive probability.&lt;/p&gt;

&lt;p&gt;The theorem is a consequence of a result from &lt;a href=&quot;https://en.wikipedia.org/wiki/Perron–Frobenius_theorem&quot;&gt;&lt;em&gt;Perron-Frobenius theorem&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Convergence to stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be a Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$ and &lt;em&gt;transition matrix&lt;/em&gt; $Q$, such that some power $Q^m$ has all entries positive (or in the other words, the chain is &lt;em&gt;irreducible&lt;/em&gt; and &lt;em&gt;aperiodic&lt;/em&gt;). Then $P(X_n=i)$ converges to $s_i$ as $n\rightarrow\infty$ (or $Q^n$ converges to a matrix in which each row is $s$).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Expected time to run&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be an &lt;em&gt;irreducible&lt;/em&gt; Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$. Let $r_i$ be the expected time it takes the chain to return to $i$, given that it starts at $i$. Then $s_i=1/r_i$&lt;/p&gt;

&lt;h3 id=&quot;reversibility&quot;&gt;Reversibility&lt;/h3&gt;
&lt;p&gt;Let $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain. Suppose there is an $s=(s_1,\dots,s_M)^T$ with $s_i\geq0,\sum_{i}s_i=1$, such that
\begin{equation}
s_iq_{ij}=s_jq_{ji}
\end{equation}
for all states $i,j$. This equation is called &lt;em&gt;reversibility&lt;/em&gt; or &lt;em&gt;detailed balance&lt;/em&gt; condition. And if the condition holds, we say that the chain is &lt;em&gt;reversible&lt;/em&gt; w.r.t $s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt; (&lt;em&gt;Reversible implies stationary&lt;/em&gt;)&lt;br /&gt;
    Suppose that $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain that is &lt;em&gt;reversible&lt;/em&gt; w.r.t to an $s=(s_1,\dots,s_M)^T$ with with $s_i\geq0,\sum_{i}s_i=1$. Then $s$ is a &lt;em&gt;stationary distribution&lt;/em&gt; of the chain. (&lt;em&gt;proof&lt;/em&gt;:$\sum_{j}s_jq_{ji}=\sum_{j}s_iq_{ij}=s_i\sum_{j}q_{ij}=s_i$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt;&lt;br /&gt;
    If each column of $Q$ sum to 1, then the &lt;em&gt;uniform distribution&lt;/em&gt; over all states $(1/M,\dots,1/M)$, is a &lt;em&gt;stationary distribution&lt;/em&gt;. (This kind of matrix is called &lt;em&gt;doubly stochastic matrix&lt;/em&gt;).&lt;/p&gt;

&lt;h3 id=&quot;examples-and-application&quot;&gt;Examples and application&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;&lt;em&gt;Finite-state machines&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;&lt;em&gt;random walks&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Diced board games such as Ludo, Monopoly,…&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/PageRank&quot;&gt;&lt;em&gt;Google PageRank&lt;/em&gt;&lt;/a&gt; - the heart of Google search&lt;/li&gt;
  &lt;li&gt;Markov Decision Process (MDP), which is gonna be the content of next &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;post&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;And various other applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;footnote&quot;&gt;Footnote:&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;The Markov chain here is &lt;em&gt;time-homogeneous&lt;/em&gt; Markov chain, in which the probability of any state transition is independent of time.&lt;/li&gt;
  &lt;li&gt;This is more like intuitive and less formal definition of Markov chain, we will have more concrete definition with the help of &lt;em&gt;Measure theory&lt;/em&gt; after the post about it.&lt;/li&gt;
  &lt;li&gt;Well, it only matters where you are, not where you’ve been.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;references&quot;&gt;References:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Introduction to Probability - Joseph K. Blitzstein &amp;amp; Jessica Hwang&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brilliant.org/wiki/markov-chains/&quot;&gt;Brillant’s Markov chain&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="probability-statistics" /><summary type="html">Since I have no idea how to begin with this post, why not just dive straight into details :P Markov chain is a stochastic process in which the random variables follow a special property called Markov.</summary></entry><entry><title type="html">My very first post</title><link href="/random-stuffs/2021/06/05/fibonacci-generator.html" rel="alternate" type="text/html" title="My very first post" /><published>2021-06-05T17:00:00+07:00</published><updated>2021-06-05T17:00:00+07:00</updated><id>/random-stuffs/2021/06/05/fibonacci-generator</id><content type="html" xml:base="/random-stuffs/2021/06/05/fibonacci-generator.html">&lt;p&gt;Enjoy my index-zero-ed post while staying tuned for next ones!
&lt;!-- excerpt-end --&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fibonacci&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	generate i-th number of the Fibonacci sequence, python code obvs :p
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Why did numbers \(\frac{1+\sqrt{5}}{2}\) and \(\frac{1-\sqrt{5}}{2}\) come out of nowhere?&lt;/p&gt;

&lt;p&gt;In fact, these two numbers are eigenvalues of matrix \(A=\big(\begin{smallmatrix}1 &amp;amp; 1\\1 &amp;amp; 0\end{smallmatrix}\big)\), which is retrieved from
\begin{equation}
u_{k+1}=Au_k,
\end{equation}
where \(u_k=\big(\begin{smallmatrix}F_{k+1}\\F_k\end{smallmatrix}\big)\).
And thus, \(u_k=A^k u_0\).&lt;/p&gt;

&lt;p&gt;Then, the thing is, how can we compute \(A^k\) quickly? This is where diagonalizing plays its role. Diagonalizing produces a factorization:
\begin{equation}
A=S\Lambda S^{-1},
\end{equation}
where \(S=\big(\begin{smallmatrix}x_1\dots x_n\end{smallmatrix}\big)\) is eigenvector matrix, \(\Lambda=\big(\begin{smallmatrix}\lambda_1&amp;amp;&amp;amp;\\&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;\lambda_n\end{smallmatrix}\big)\) is a diagonal matrix established from eigenvalues of \(A\).&lt;/p&gt;

&lt;p&gt;When taking the power of \(A\),
\begin{equation}
A^k u_0=(S\Lambda S^{-1})\dots(S\Lambda S^{-1})u_0=S\Lambda^k S^{-1} u_0
\end{equation}
Writing \(u_0\) as a combination \(c_1x_1+\dots+c_nx_n\) of the eigenvectors, we have that \(c=S^{-1}u_0\). Hence:
\begin{equation}
u_k=A^ku_0=c_1{\lambda_1}^kx_1+\dots+c_n{\lambda_n}^kx_n
\end{equation}
&lt;em&gt;Fact&lt;/em&gt;: The \(\frac{1+\sqrt{5}}{2}\approx 1.618\) is so-called “&lt;em&gt;golden ratio&lt;/em&gt;”. And &lt;em&gt;for some reason a rectangle with sides 1.618 and 1 looks especially graceful&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Introduction to Linear Algebra - Gilbert Strang&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><summary type="html">Enjoy my index-zero-ed post while staying tuned for next ones!</summary></entry></feed>