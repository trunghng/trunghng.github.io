<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed/by_tag/uct.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-30T16:20:23+07:00</updated><id>http://localhost:4000/feed/by_tag/uct.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Monte Carlo Tree Search</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html" rel="alternate" type="text/html" title="Monte Carlo Tree Search" /><published>2022-05-25T13:00:00+07:00</published><updated>2022-05-25T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html"><![CDATA[<blockquote>
  <p><strong>Monte Carlo Tree Search (MCTS)</strong> is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#mcts-vanilla">(Vanilla) Monte Carlo Tree Search</a></li>
  <li><a href="#uct">Upper Confidence Bound for Trees (UCT)</a></li>
  <li><a href="#example">Example</a></li>
  <li><a href="#alphazero">AlphaZero</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="vanilla-mcts">(Vanilla) Monte Carlo Tree Search</h2>

<h2 id="uct">Upper Confidence Bound for Trees (UCT)</h2>

<h2 id="example">Example</h2>

<h2 id="alphazero">AlphaZero</h2>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[2] C. B. Browne et al. <a href="https://ieeexplore.ieee.org/document/6145622">A Survey of Monte Carlo Tree Search Methods</a>, in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012.</p>

<p>[3] Kocsis, L. &amp; Szepesvári, C. (2006). <a href="https://doi.org/10.1007/11871842_29">Bandit Based Monte-Carlo Planning</a>. In: Fürnkranz, J., Scheffer, T., Spiliopoulou, M. (eds) Machine Learning: ECML 2006. ECML 2006. Lecture Notes in Computer Science, vol 4212. Springer, Berlin, Heidelberg.</p>

<p>[4] David Silver &amp; Julian Schrittwieser &amp; Karen Simonyan et al. <a href="https://doi.org/10.1038/nature24270">Mastering the game of Go without human knowledge</a>. Nature 550, 354–359 (2017).</p>

<p>[5] David Silver &amp; Thomas Hubert &amp; Julian Schrittwieser et al. <a href="https://arxiv.org/abs/1712.01815">Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm</a>. arXiv.</p>

<p>[6] Shangtong Zhang. <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">Reinforcement Learning: An Introduction implementation</a>.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="monte-carlo" /><category term="mcts" /><category term="uct" /><category term="planning" /><summary type="html"><![CDATA[A note on Monte Carlo Tree Search]]></summary></entry></feed>