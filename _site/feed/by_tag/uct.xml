<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed/by_tag/uct.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-09-12T17:16:41+07:00</updated><id>http://localhost:4000/feed/by_tag/uct.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Monte Carlo Tree Search</title><link href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html" rel="alternate" type="text/html" title="Monte Carlo Tree Search" /><published>2022-05-25T13:00:00+07:00</published><updated>2022-05-25T13:00:00+07:00</updated><id>http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts</id><content type="html" xml:base="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/05/25/mcts.html">&lt;blockquote&gt;
  &lt;p&gt;&lt;strong&gt;Monte Carlo Tree Search (MCTS)&lt;/strong&gt; is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.
&lt;!-- excerpt-end --&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#mcts-vanilla&quot;&gt;(Vanilla) Monte Carlo Tree Search&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uct&quot;&gt;Upper Confidence Bound for Trees (UCT)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#example&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#alphazero&quot;&gt;AlphaZero&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;vanilla-mcts&quot;&gt;(Vanilla) Monte Carlo Tree Search&lt;/h2&gt;

&lt;h2 id=&quot;uct&quot;&gt;Upper Confidence Bound for Trees (UCT)&lt;/h2&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;h2 id=&quot;alphazero&quot;&gt;AlphaZero&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] C. B. Browne et al. &lt;a href=&quot;https://ieeexplore.ieee.org/document/6145622&quot;&gt;A Survey of Monte Carlo Tree Search Methods&lt;/a&gt;, in IEEE Transactions on Computational Intelligence and AI in Games, vol. 4, no. 1, pp. 1-43, March 2012.&lt;/p&gt;

&lt;p&gt;[3] Kocsis, L. &amp;amp; Szepesvári, C. (2006). &lt;a href=&quot;https://doi.org/10.1007/11871842_29&quot;&gt;Bandit Based Monte-Carlo Planning&lt;/a&gt;. In: Fürnkranz, J., Scheffer, T., Spiliopoulou, M. (eds) Machine Learning: ECML 2006. ECML 2006. Lecture Notes in Computer Science, vol 4212. Springer, Berlin, Heidelberg.&lt;/p&gt;

&lt;p&gt;[4] David Silver &amp;amp; Julian Schrittwieser &amp;amp; Karen Simonyan et al. &lt;a href=&quot;https://doi.org/10.1038/nature24270&quot;&gt;Mastering the game of Go without human knowledge&lt;/a&gt;. Nature 550, 354–359 (2017).&lt;/p&gt;

&lt;p&gt;[5] David Silver &amp;amp; Thomas Hubert &amp;amp; Julian Schrittwieser et al. &lt;a href=&quot;https://arxiv.org/abs/1712.01815&quot;&gt;Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm&lt;/a&gt;. arXiv.&lt;/p&gt;

&lt;p&gt;[6] Shangtong Zhang. &lt;a href=&quot;https://github.com/ShangtongZhang/reinforcement-learning-an-introduction&quot;&gt;Reinforcement Learning: An Introduction implementation&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="monte-carlo" /><category term="mcts" /><category term="uct" /><category term="planning" /><summary type="html">Monte Carlo Tree Search (MCTS) is a method for finding optimal decisions in a given domain by taking random samples in the decision space and building a search tree according to the results.</summary></entry></feed>