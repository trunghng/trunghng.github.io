<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed/by_tag/mathematics.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-09-08T17:36:42+07:00</updated><id>/feed/by_tag/mathematics.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Infinite Series of Constants</title><link href="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html" rel="alternate" type="text/html" title="Infinite Series of Constants" /><published>2021-09-06T11:20:00+07:00</published><updated>2021-09-06T11:20:00+07:00</updated><id>/mathematics/calculus/2021/09/06/infinite-series-of-constants</id><content type="html" xml:base="/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">&lt;blockquote&gt;
  &lt;p&gt;No idea what to say yet :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#infinite-series&quot;&gt;Infinite Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#examples&quot;&gt;Examples&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convergent-sequences&quot;&gt;Convergent Sequences&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sequences&quot;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lim-seq&quot;&gt;Limits of Sequences&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison Tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;infinite-series&quot;&gt;Infinite Series&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;infinite series&lt;/strong&gt;, or simply a &lt;strong&gt;series&lt;/strong&gt;, is an expression of the form
\begin{equation}
a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{\infty}a_n
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Infinite decimal&lt;/em&gt;
\begin{equation}
.a_1a_2\ldots a_n\ldots=\dfrac{a_1}{10}+\dfrac{a_2}{10^2}+\ldots+\dfrac{a_n}{10^n}+\ldots,
\end{equation}
where $a_i\in\{0,1,\dots,9\}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Power series expansion&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;We have
\begin{equation}
\dfrac{1}{1-x}=1+x+x^2+\dots\tag{1}\label{1}
\end{equation}&lt;/li&gt;
      &lt;li&gt;If we replace $x$ by $-x$ in \eqref{1} we have
\begin{equation}
\dfrac{1}{1+x}=1-x+x^2-x^3+\dots\tag{2}\label{2}
\end{equation}
And if we replace $x$ by $x^2$ in \eqref{2} we obtain
\begin{equation}
\dfrac{1}{1+x^2}=1-x^2+x^4-x^6+\dots\tag{3}\label{3}
\end{equation}
Moreover, if we take the integral of the left side of \eqref{3} we get
\begin{equation}
\int\dfrac{dx}{1+x^2}=\tan^{-1}x,
\end{equation}
which leads to the result that
\begin{equation}
\tan^{-1}x=x-\dfrac{x^3}{3}+\dfrac{x^5}{5}-\dfrac{x^7}{7}+\dots\tag{4}\label{4}
\end{equation}
Let $x=1$, \eqref{4} gives us an interesting result
\begin{equation}
\dfrac{\pi}{4}=1-\dfrac{1}{3}+\dfrac{1}{5}-\dfrac{1}{7}+\dots
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convergent-sequences&quot;&gt;Convergent Sequences&lt;/h2&gt;

&lt;h3 id=&quot;sequences&quot;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$’s are said to form a &lt;strong&gt;sequence&lt;/strong&gt; (denoted as $\{x_n\}$)
\begin{equation}
x_1,x_2,\dots,x_n,\dots
\end{equation}
We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.&lt;/p&gt;

&lt;p&gt;A sequence $\{x_n\}$ is said to be &lt;em&gt;bounded&lt;/em&gt; if there exists $A, B$ such that $A\leq x_n\leq B, \forall n$. $A, B$ respectively are called &lt;em&gt;lower bound&lt;/em&gt;, &lt;em&gt;upper bound&lt;/em&gt; of the sequence. A sequence that is not bounded is said to be &lt;em&gt;unbounded&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lim-seq&quot;&gt;Limits of Sequences&lt;/h3&gt;
&lt;p&gt;A sequence $\{x_n\}$ is said to have a number $L$ as &lt;strong&gt;limit&lt;/strong&gt; if for each $\epsilon&amp;gt;0$, there exists a positive integer $n_0$ that
\begin{equation}
\vert x_n-L\vert&amp;lt;\epsilon\hspace{1cm}n\geq n_0
\end{equation}
We say that $x_n$ &lt;em&gt;converges to&lt;/em&gt; $L$ &lt;em&gt;as&lt;/em&gt; $n$ &lt;em&gt;approaches infinite&lt;/em&gt; ($x_n\to L$ as $n\to\infty$) and denote this as
\begin{equation}
\lim_{n\to\infty}x_n=L
\end{equation}&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A sequence is said to &lt;strong&gt;converge&lt;/strong&gt; or to be &lt;strong&gt;convergent&lt;/strong&gt; if it has a limit.&lt;/li&gt;
  &lt;li&gt;A convergent sequence is bounded, but not all bounded sequences are convergent.&lt;/li&gt;
  &lt;li&gt;If $x_n\to L,y_n\to M$, then
\begin{align}
&amp;amp;\lim(x_n+y_n)=L+M \\ &amp;amp;\lim(x_n-y_n)=L-M \\ &amp;amp;\lim x_n y_n=LM \\ &amp;amp;\lim\dfrac{x_n}{y_n}=\dfrac{L}{M}\hspace{1cm}M\neq0
\end{align}&lt;/li&gt;
  &lt;li&gt;An &lt;em&gt;increasing&lt;/em&gt; (or &lt;em&gt;decreasing&lt;/em&gt;) sequence converges if and only if it is bounded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/h2&gt;
&lt;p&gt;Recall from the previous sections that if $a_1,a_2,\dots,a_n,\dots$ is a &lt;em&gt;sequence&lt;/em&gt; of numbers, then
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots\tag{5}\label{5}
\end{equation}
is called an &lt;em&gt;infinite series&lt;/em&gt;. We begin by establishing the sequence of &lt;em&gt;partial sums&lt;/em&gt;
\begin{align}
s_1&amp;amp;=a_1 \\ s_2&amp;amp;=a_1+a_2 \\ &amp;amp;\,\vdots \\ s_n&amp;amp;=a_1+a_2+\dots+a_n \\ &amp;amp;\,\vdots
\end{align}
The series \eqref{5} is said to be &lt;strong&gt;convergent&lt;/strong&gt; if the sequences $\{s_n\}$ converges. And if $\lim s_n=s$, then we say that \eqref{5} converges to $s$, or that $s$ is the sum of the series.
\begin{equation}
\sum_{n=1}^{\infty}a_n=s
\end{equation}
If the series does not converge, we say that it &lt;strong&gt;diverges&lt;/strong&gt; or is &lt;strong&gt;divergent&lt;/strong&gt;, and no sum is assigned to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (&lt;em&gt;harmonic series&lt;/em&gt;)&lt;br /&gt;
Let’s consider the convergence of &lt;em&gt;harmonic series&lt;/em&gt;
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots\tag{6}\label{6}
\end{equation}
Let $m$ be a positive integer and choose $n&amp;gt;2^{m+1}$. We have
\begin{align}
s_n&amp;amp;&amp;gt;1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\dots+\frac{1}{2^{m+1}} \\ &amp;amp;=\left(1+\frac{1}{2}\right)+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\ldots+\frac{1}{8}\right)+\ldots+\left(\frac{1}{2^m+1}+\ldots+\frac{1}{2^{m+1}}\right) \\ &amp;amp;&amp;gt;\frac{1}{2}+2.\frac{1}{4}+4.\frac{1}{8}+\ldots+2^m.\frac{1}{2^{m+1}} \\ &amp;amp;=(m+1)\frac{1}{2}
\end{align}
This proves that $s_n$ can be made larger than the sum of any number of $\frac{1}{2}$’s and therefore as large as we please, by taking $n$ large enough, so the $\{s_n\}$ are unbounded, which leads to that \eqref{6} is a divergent series.
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots=\infty
\end{equation}&lt;/p&gt;

&lt;p&gt;The simplest general principle that is useful to study the convergence of a series is the &lt;strong&gt;$\mathbf{n}$-th term test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;nth-term-test&quot;&gt;$\mathbf{n}$-th term test&lt;/h3&gt;
&lt;p&gt;If the series $\{a_n\}$ converges, then $a_n\to0$ as $n\to\infty$; or equivalently, if $\neg(a_n\to0)$ as $n\to\infty$, then the series must necessarily diverge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
When $\{a_n\}$ converges, as $n\to\infty$ we have
\begin{equation}
a_n=s_n-s_{n-1}\to s-s=0
\end{equation}
This result shows that $a_n\to0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e., it does not imply the convergence of the series when $a_n\to0$ as $n\to\infty$.&lt;/p&gt;

&lt;h2 id=&quot;gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Any finite number of 0’s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges).&lt;/li&gt;
  &lt;li&gt;When two convergent series are added term by term, the resulting series converges to the expected sum; i.e., if $\sum_{n=1}^{\infty}a_n=s$ and $\sum_{n=1}^{\infty}b_n=t$, then
\begin{equation}
\sum_{n=1}^{\infty}(a_n+b_n)=s+t
\end{equation}
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  Let $\{s_n\}$ and $\{t_n\}$ respectively be the sequences of partial sums of $\sum_{n=1}^{\infty}a_n$ and $\sum_{n=1}^{\infty}b_n$. As $n\to\infty$ we have
  \begin{align}
  (a_1+b_1)+(a_2+b_2)+\dots+(a_n+b_n)&amp;amp;=\sum_{i=1}^{n}a_i+\sum_{i=1}^{n}b_i \\ &amp;amp;=s_n+t_n\to s+t
  \end{align}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Similarly, $\sum_{n=1}^{\infty}(a_n-b_n)=s-t$ and $\sum_{n=1}^{\infty}ca_n=cs$ for any constant $c$.&lt;/li&gt;
  &lt;li&gt;Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  If $\sum_{n=1}^{\infty}a_n=s$, then
  \begin{equation}
  \lim_{n\to\infty}(a_0+a_1+a_2+\dots+a_n)=\lim_{n\to\infty} a_0+\lim_{n\to\infty}(a_1+a_2+\dots+a_n)=a_0+s
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison Tests&lt;/h2&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] George F.Simmons. &lt;a href=&quot;https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424&quot;&gt;Calculus With Analytic Geometry - 2nd Edition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] MIT 18.01. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&quot;&gt;Single Variable Calculus&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;More information about power series are gonna be written in another post. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="random-stuffs" /><summary type="html">No idea what to say yet :D</summary></entry><entry><title type="html">Optimal Policy Existence</title><link href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html" rel="alternate" type="text/html" title="Optimal Policy Existence" /><published>2021-07-10T13:03:00+07:00</published><updated>2021-07-10T13:03:00+07:00</updated><id>/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence</id><content type="html" xml:base="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html">&lt;blockquote&gt;
  &lt;p&gt;In the previous post about &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;&lt;strong&gt;Markov Decision Processes, Bellman equations&lt;/strong&gt;&lt;/a&gt;, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#norms-contractions-banach-fixed-pts&quot;&gt;Norms, Contractions and Banach’s Fixed-point Theorem&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#norms&quot;&gt;Norms&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#contractions&quot;&gt;Contractions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#banach-fixed-pts&quot;&gt;Banach’s Fixed-point Theorem&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bellman-operator&quot;&gt;Bellman Operator&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#proof&quot;&gt;Proof&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Before catching the pokémon, we need to prepare ourselves some pokéball.&lt;/p&gt;

&lt;h4 id=&quot;norms-contractions-banach-fixed-pts&quot;&gt;Norms, Contractions and Banach’s Fixed-point Theorem&lt;/h4&gt;

&lt;h5 id=&quot;norms&quot;&gt;Norms&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Norm&lt;/em&gt;)&lt;br /&gt;
Given a vector space $\mathcal{V}\subseteq\mathbb{R}^d$, a function $f:\mathcal{V}\to\mathbb{R}^+_0$ is a &lt;em&gt;norm&lt;/em&gt; if and only if&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If $f(v)=0$ for some $v\in\mathcal{V}$, then $v=0$&lt;/li&gt;
  &lt;li&gt;For any $\lambda\in\mathbb{R},v\in\mathcal{V},f(\lambda v)=|\lambda|v$&lt;/li&gt;
  &lt;li&gt;For any $u,v\in\mathbb{R}, f(u+v)\leq f(u)+f(v)$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (&lt;em&gt;Norm&lt;/em&gt;)&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\ell^p$ norms: for $p\geq 1$,
\begin{equation}
\Vert v\Vert_p=\left(\sum_{i=1}^{d}|v_i|^p\right)^{1/p}
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^\infty$ norms:
\begin{equation}
\Vert v\Vert_\infty=\max_{1\leq i\leq d}|v_i|
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^{\mu,p}$: the weighted variants of these norm are defined as
\begin{equation}
\Vert v\Vert_p=\begin{cases}\left(\sum_{i=1}^{d}\frac{|v_i|^p}{w_i}\right)^{1/p}&amp;amp;\text{if }1\leq p&amp;lt;\infty\\ \max_{1\leq i\leq d}\frac{|v_i|}{w_i}&amp;amp;\text{if }p=\infty\end{cases}
\end{equation}&lt;/li&gt;
  &lt;li&gt;$\ell^{2,P}$: the matrix-weighted 2-norm is defined as
\begin{equation}
\Vert v\Vert^2_P=v^TPv
\end{equation}
Similarly, we can define norms over spaces of functions. For example, if $\mathcal{V}$ is the vector space of functions over domain $\mathcal{X}$ which are &lt;em&gt;uniformly bounded&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, then
\begin{equation}
\Vert f\Vert_\infty=\sup_{x\in\mathcal{X}}\vert f(x)\vert
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Convergence in norm&lt;/em&gt;)&lt;br /&gt;
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a &lt;em&gt;normed vector space&lt;/em&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. Let $v_n\in\mathcal{V}$ is a sequence of vectors ($n\in\mathbb{N}$). The sequence ($v_n,n\geq 0$) is said to &lt;em&gt;converge to&lt;/em&gt; $v\in\mathcal{V}$ in the norm $\Vert\cdot\Vert$, denoted as $v_n\to_{\Vert\cdot\Vert}v$ if
\begin{equation}
\lim_{n\to\infty}\Vert v_n-v\Vert=0,
\end{equation}
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Cauchy sequence&lt;/em&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;)&lt;br /&gt;
Let ($v_n;n\geq 0$) be a sequence of vectors of a normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$. Then $v_n$ is called a &lt;em&gt;Cauchy sequence&lt;/em&gt; if
\begin{equation}
\lim_{n\to\infty}\sup_{m\geq n}\Vert v_n-v_m\Vert=0
\end{equation}
Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Completeness&lt;/em&gt;)&lt;br /&gt;
A normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ is called &lt;em&gt;complete&lt;/em&gt; if every Cauchy sequence in $\mathcal{V}$ is convergent in the norm of the vector space.&lt;/p&gt;

&lt;h5 id=&quot;contractions&quot;&gt;Contractions&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Lipschitzian&lt;/em&gt;) &lt;br /&gt;
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a normed vector space. A mapping $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ is called &lt;em&gt;L-Lipschitz&lt;/em&gt; if for any $u,v\in\mathcal{V}$,
\begin{equation}
\Vert\mathcal{T}u-\mathcal{T}v\Vert\leq L\Vert u-v\Vert
\end{equation}
A mapping $\mathcal{T}$ is called a &lt;em&gt;non-expansion&lt;/em&gt; if it is &lt;em&gt;Lipschitzian&lt;/em&gt; with $L\leq 1$. It is called a &lt;em&gt;contraction&lt;/em&gt; if it is &lt;em&gt;Lipschitzian&lt;/em&gt; with $L&amp;lt;1$. In this case, $L$ is called the &lt;em&gt;contraction factor of&lt;/em&gt; $\mathcal{T}$ and $\mathcal{T}$ is called an &lt;em&gt;L-contraction&lt;/em&gt;.&lt;br /&gt;
&lt;ins&gt;Note&lt;/ins&gt;: If $\mathcal{T}$ is &lt;em&gt;Lipschitz&lt;/em&gt;, it is also continuous in the sense that if $v_n\to_{\Vert\cdot\Vert}v$, then also $\mathcal{T}v_n\to_{\Vert\cdot\Vert}\mathcal{T}v$. This is because $\Vert\mathcal{T}v_n-\mathcal{T}v\Vert\leq L\Vert v_n-v\Vert\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;h5 id=&quot;banach-fixed-pts&quot;&gt;Banach’s Fixed-point Theorem&lt;/h5&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Banach space&lt;/em&gt;)&lt;br /&gt;
A complete, normed vector space is called a &lt;em&gt;Banach space&lt;/em&gt;.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Fixed point&lt;/em&gt;)&lt;br /&gt;
Let $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be some mapping. The vector $v\in\mathcal{V}$ is called a &lt;em&gt;fixed point of&lt;/em&gt; $\mathcal{T}$ if $\mathcal{T}v=v$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Banach’s fixed-point&lt;/em&gt;)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;    &lt;br /&gt;
Let $\mathcal{V}$ be a Banach space and $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be a $\gamma$-contraction mapping. Then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mathcal{T}$ admits a &lt;em&gt;unique fixed point&lt;/em&gt; $v$.&lt;/li&gt;
  &lt;li&gt;For any $v_0\in\mathcal{V}$, if $v_{n+1}=\mathcal{T}v_n$, then $v_n\to_{\Vert\cdot\Vert}v$ with a &lt;em&gt;geometric convergence rate&lt;/em&gt;&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;:
\begin{equation}
\Vert v_n-v\Vert\leq\gamma^n\Vert v_0-v\Vert
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;bellman-operator&quot;&gt;Bellman Operator&lt;/h4&gt;
&lt;p&gt;Previously, we defined Bellman equation for state-value function $v_\pi(s)$ as:
\begin{align}
v_\pi(s)&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right] \\\text{or}\quad v_\pi(s)&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_\pi(s’)\right)\tag{1}\label{1}
\end{align}
If we let
\begin{align}
\mathcal{P}^\pi_{ss’}&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}^a_{ss’}; \\\mathcal{R}^\pi_s&amp;amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s
\end{align}
then we can rewrite \eqref{1} in another form as
\begin{equation}
v_\pi(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v_\pi(s’)\tag{2}\label{2}
\end{equation}
&lt;br /&gt;
&lt;strong&gt;Definition&lt;/strong&gt; (&lt;em&gt;Bellman operator&lt;/em&gt;)&lt;br /&gt;
We define the &lt;em&gt;Bellman operator&lt;/em&gt; underlying $\pi,\mathcal{T}:\mathbb{R}^\mathcal{S}\to\mathbb{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^\pi v)(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v(s’)
\end{equation}
&lt;br /&gt;
With the help of $\mathcal{T}^\pi$, equation \eqref{2} can be rewrite as:
\begin{equation}
\mathcal{T}^\pi v_\pi=v_\pi\tag{3}\label{3}
\end{equation}
Similarly, we can rewrite the &lt;em&gt;Bellman optimality equation for&lt;/em&gt; $v_*$
\begin{align}
v_*(s)&amp;amp;=\max_{a\in\mathcal{A}}\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_*(s’)\right] \\ &amp;amp;=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_*(s’)\right)\tag{4}\label{4}
\end{align}
and thus, we can define the &lt;em&gt;Bellman optimality operator&lt;/em&gt; $\mathcal{T}^*:\mathcal{R}^\mathcal{S}\to\mathcal{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^* v)(s)=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v(s’)\right)
\end{equation}
And thus, with the help of $\mathcal{T}^*$, we can rewrite the equation \eqref{4} as:
\begin{equation}
\mathcal{T}^*v_*=v_*\tag{5}\label{5}
\end{equation}
&lt;br /&gt;
Now everything is all set, we can move on to the next step.&lt;/p&gt;

&lt;h4 id=&quot;proof&quot;&gt;Proof&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Let $B(\mathcal{S})$ be the space of &lt;em&gt;uniformly bounded functions&lt;/em&gt; with domain $\mathcal{S}$:
\begin{equation}
B(\mathcal{S})=\{v:\mathcal{S}\to\mathbb{R}:\Vert v\Vert_\infty&amp;lt;+\infty\}
\end{equation}&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;We will view $B(\mathcal{S})$ as a normed vector space with the norm $\Vert\cdot\Vert_\infty$. It is easily seen that $(B(\mathcal{S}),\Vert\cdot\Vert_\infty)$ is complete: If ($v_n;n\geq0$) is a Cauchy sequence in it then for any $s\in\mathcal{S}$, ($v_n(s);n\geq0$) is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of ($v_n(s)$), we can show that $\Vert v_n-v\Vert_\infty\to0$. Vaguely speaking, this holds because ($v_n;n\geq0$) is a Cauchy sequence in the norm $\Vert\cdot\Vert_\infty$  so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Pick any stationary policy $\pi$.&lt;/li&gt;
  &lt;li&gt;We have that $\mathcal{T}^\pi$ is &lt;em&gt;well-defined&lt;/em&gt; since: if $u\in B(\mathcal{S})$, then also $\mathcal{T}^\pi u\in B(S)$.&lt;/li&gt;
  &lt;li&gt;From equation \eqref{3}, we have that $v_\pi$ is a fixed point to $\mathcal{T}^\pi$.&lt;br /&gt;
We also have that $\mathcal{T}^\pi$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$ since for any $u, v\in B(\mathcal{S})$,
\begin{align}
\Vert\mathcal{T}^\pi u-\mathcal{T}^\pi v\Vert_\infty&amp;amp;=\gamma\max_{s\in\mathcal{S}}\left|\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left(u(s’)-v(s’)\right)\right| \\ &amp;amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\Vert u-v\Vert_\infty \\ &amp;amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
where the last line follows from $\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}=1$.&lt;/li&gt;
  &lt;li&gt;It follows that in order to find $v_\pi$, we can construct the sequence $v_0,\mathcal{T}^\pi v_0,(\mathcal{T}^\pi)^2 v_0,\dots$, which, by Banach’s fixed-point theorem will converge to $v_\pi$ at a geometric rate.&lt;/li&gt;
  &lt;li&gt;From the definition \eqref{5} of $\mathcal{T}^*$, we have that $\mathcal{T}^*$ is well-defined.&lt;/li&gt;
  &lt;li&gt;Using the fact that $\left|\max_{a\in\mathcal{A}}f(a)-\max_{a\in\mathcal{A}}g(a)\right|\leq\max_{a\in\mathcal{A}}\left|f(a)-g(a)\right|$, similarly, we have:
\begin{align}
\Vert\mathcal{T}^*u-\mathcal{T}^*v\Vert_\infty&amp;amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\Vert u-v\Vert_\infty \\ &amp;amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
which tells us that $\mathcal{T}^*$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt;&lt;br /&gt;
Let $v$ be the fixed point of $\mathcal{T}^*$ and assume that there is policy $\pi$ which is greedy w.r.t $v:\mathcal{T}^\pi v=\mathcal{T}^* v$. Then $v=v_*$ and $\pi$ is an optimal policy.&lt;br /&gt;
&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
Pick any stationary policy $\pi$. Then $\mathcal{T}^\pi\leq\mathcal{T}^*$ in the sense that for any function $v\in B(\mathcal{S})$, $\mathcal{T}^\pi v\leq\mathcal{T}^* v$ holds ($u\leq v$ means that $u(s)\leq v(s),\forall s\in\mathcal{S}$).&lt;br /&gt;
Hence, for all $n\geq0$,
\begin{equation}
v_\pi=\mathcal{T}^\pi v_\pi\leq\mathcal{T}^*v_\pi\leq(\mathcal{T}^*)^2 v_\pi\leq\dots\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
or
\begin{equation}
v_\pi\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
Since $\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\mathcal{T}^*$. Thus, $v_\pi\leq v$. And since $\pi$ was arbitrary, we obtain that $v_*\leq v$.&lt;br /&gt;
Pick a policy $\pi$ such that $\mathcal{T}^\pi v=\mathcal{T}^*v$, then $v$ is also a fixed point of $\mathcal{V}^\pi$. Since $v_\pi$ is the unique fixed point of $\mathcal{T}^\pi$, we have that $v=v_\pi$, which shows that $v_*=v$ and that $\pi$ is an optimal policy.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Csaba Szepesvári. &lt;a href=&quot;https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924&quot;&gt;Algorithms for Reinforcement Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] A. Lazaric. &lt;a href=&quot;http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf&quot;&gt;Markov Decision Processes and Dynamic Programming&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://ai.stackexchange.com/a/11133&quot;&gt;What is the Bellman operator in reinforcement learning?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[4] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[5] &lt;a href=&quot;https://en.wikipedia.org/wiki/Normed_vector_space&quot;&gt;Normed vector space&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A function is called &lt;em&gt;uniformly bounded&lt;/em&gt; exactly when $\Vert f\Vert_\infty&amp;lt;+\infty$. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;A &lt;em&gt;normed vector space&lt;/em&gt; is a vector space over the real or complex number, on which a norm is defined. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We are gonna talk further about &lt;em&gt;sequences&lt;/em&gt; in another &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#convergent-sequences&quot;&gt;post&lt;/a&gt;. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;&lt;em&gt;Proof&lt;/em&gt;&lt;/strong&gt;&lt;br /&gt;
Pick any $v_0\in\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\mathcal{T}$. c. Finally, we show that $\mathcal{T}$ has a single fixed point. Assume that $\mathcal{T}$ is a $\gamma$-contraction.&lt;br /&gt;
a. To show that $(v_n)$ converges, it suffices  to show that $(v_n)$ is a Cauchy sequence. We have:
\begin{align}
\Vert v_{n+1}-v_n\Vert&amp;amp;=\Vert\mathcal{T}v_{n}-\mathcal{T}v_{n-1}\Vert \\ &amp;amp;\leq\gamma\Vert v_{n}-v_{n-1}\Vert \\ &amp;amp;\quad\vdots \\ &amp;amp;\leq\gamma^n\Vert v_1-v_0\Vert
\end{align}
From the properties of norms, we have:
\begin{align}
\Vert v_{n+k}-v_n\Vert&amp;amp;\leq\Vert v_{n+1}-v_n\Vert+\dots+\Vert v_{n+k}-v_{n+k-1}\Vert \\ &amp;amp;\leq\left(\gamma^n+\dots+\gamma^{n+k-1}\right)\Vert v_1-v_0\Vert \\ &amp;amp;=\gamma^n\dfrac{1-\gamma^{k}}{1-\gamma}\Vert v_1-v_0\Vert
\end{align}
and so
\begin{equation}
\lim_{n\to\infty}\sup_{k\geq0}\Vert v_{n+k}-v_n\Vert=0,
\end{equation}
shows us that $(v_n;n\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.&lt;br /&gt;
b. Recall that the definition of the sequence $(v_n;n\geq0)$
\begin{equation}
v_{n+1}=\mathcal{T}v_n
\end{equation}
Taking the limes as $n\to\infty$ of both sides, one the one hand, we get that $v_{n+1}\to _{\Vert\cdot\Vert}v$. On the other hand, $\mathcal{T}v_n\to _{\Vert\cdot\Vert}\mathcal{T}v$, since $\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\mathcal{T}v$, which tells us that $v$ is a fixed point of $\mathcal{T}$.&lt;br /&gt;
c. Let us assume that $v,v’$ are both fixed points of $\mathcal{T}$. Then,
\begin{align}
\Vert v-v’\Vert&amp;amp;=\Vert\mathcal{T}v-\mathcal{v’}\Vert \\ &amp;amp;\leq\gamma\Vert v-v’\Vert \\ \text{or}\quad(1-\gamma)\Vert v-v’\Vert&amp;amp;\leq0
\end{align}
Thus, we must have that $\Vert v-v’\Vert=0$. Therefore, $v-v’=0$ or $v=v’$.&lt;br /&gt;
And finally,
\begin{align}
\Vert v_n-v\Vert&amp;amp;=\Vert\mathcal{T}v_{n-1}-\mathcal{T}v\Vert \\ &amp;amp;\leq\gamma\Vert v_{n-1}-v\Vert \\ &amp;amp;\quad\vdots \\ &amp;amp;\leq\gamma^n\Vert v_0-v\Vert
\end{align} &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Also, there’s gonna be a post about &lt;em&gt;rate of convergence&lt;/em&gt;. &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="mathematics" /><category term="my-rl" /><summary type="html">In the previous post about Markov Decision Processes, Bellman equations, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.</summary></entry><entry><title type="html">Measures</title><link href="/random-stuffs/measure-theory/2021/07/03/measure.html" rel="alternate" type="text/html" title="Measures" /><published>2021-07-03T07:00:00+07:00</published><updated>2021-07-03T07:00:00+07:00</updated><id>/random-stuffs/measure-theory/2021/07/03/measure</id><content type="html" xml:base="/random-stuffs/measure-theory/2021/07/03/measure.html">&lt;blockquote&gt;
  &lt;p&gt;When talking about &lt;em&gt;measure&lt;/em&gt;, you might associate it with the idea of &lt;em&gt;length&lt;/em&gt;, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with &lt;em&gt;area&lt;/em&gt;, or even three dimensions with &lt;em&gt;volume&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;p&gt;Despite of having different number of dimensions, all &lt;em&gt;length&lt;/em&gt;, &lt;em&gt;area&lt;/em&gt;, &lt;em&gt;volume&lt;/em&gt; share the same properties:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Non-negative&lt;/em&gt;: In principle, length, area, and volume can be any positive value. But negative length has no meaning. Same thing happens with negative area and negative volume.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Additivity&lt;/em&gt;: To get from Hanoi to Singapore by air, you have to transit at Ho Chi Minh city (HCMC). If we cut that path into two non-overlapping pieces, say Hanoi - HCMC, and HCMC - Singapore, then the total length of the two pieces must be equal to the length of original path. If we divide a rectangular into non-overlapping pieces, the area of pieces combined must be the same as the original one. The same is true for volume as well.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Empty Set&lt;/em&gt;: An empty cup of water has volume zero.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Other Null Sets&lt;/em&gt;: The length of a point is 0. The area of a line, or a curve is 0. The volume of a plane or a surface is 0.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Translation Invariance&lt;/em&gt;: Length, area and volume are unchanged (&lt;em&gt;invariant&lt;/em&gt;) under shifts (&lt;em&gt;translation&lt;/em&gt;) in space.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Hyper-rectangles&lt;/em&gt;: An interval of form $[a, b]\subset\mathbb{R}^3$ has length $b-a$. The area of a rectangle $[a_1,b_1]\times[a_2,b_2]$ is $(b_1-a_1)(b_2-a_2)$. And the volume of a rectangular $[a_1,b_1]\times[a_2,b_2]\times[a_3,b_3]$ is $(b_1-a_1)(b_2-a_2)(b_3-a_3)$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/lego.jpg&quot; alt=&quot;Lego&quot; /&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#lebesgue-measure&quot;&gt;Lebesgue Measure&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#measures&quot;&gt;Measures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-measure-idea&quot;&gt;Integration with respect to a Measure: The Idea&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#prop-int&quot;&gt;Properties of the Integral&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-measure-detail&quot;&gt;Integration with respect to a Measure: The Details&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#construct-measure&quot;&gt;Constructing Measures from old ones&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-types&quot;&gt;Other types of Measures&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-resources&quot;&gt;Other Resources&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;lebesgue-measure&quot;&gt;Lebesgue Measure&lt;/h2&gt;
&lt;p&gt;Is an extension of the classical notion of length in $\mathbb{R}$, area in $\mathbb{R}^2$ to any $\mathbb{R}^k$ using k-dimensional hyper-rectangles.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;br /&gt;
Given an open set $S\equiv\sum_k(a_k,b_k)$ containing disjoint intervals, the &lt;strong&gt;Lebesgue measure&lt;/strong&gt; is defined by:
\begin{equation}
\mu_L(S)\equiv\sum_{k}(b_k-a_k)
\end{equation}
Given a closed set $S’\equiv[a,b]-\sum_k(a_k,b_k)$,
\begin{equation}
\mu_L(S’)\equiv(b-a)-\sum_k(b_k-a_k)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;measures&quot;&gt;Measures&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;&lt;br /&gt;
Let $\mathcal{X}$ be any set. A &lt;em&gt;measure&lt;/em&gt; on $\mathcal{X}$ is a function $\mu$ that maps the set of subsets on $\mathcal{X}$ to $[0,\infty]$ ($\mu:2^\mathcal{X}\rightarrow[0,\infty]$) that satisfies:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$\mu(\emptyset)=0$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Countable additivity property&lt;/em&gt;: for any countable and pairwise disjoint collection of subsets of $\mathcal{X},\mathcal{A_1},\mathcal{A_2},\dots$,
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)=\sum_i\mu(\mathcal{A_i})
\end{equation}
$\mu(\mathcal{A})$ is called &lt;em&gt;measure of the set $\mathcal{A}$&lt;/em&gt;, or &lt;em&gt;measure of $\mathcal{A}$&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Properties&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;: If $\mathcal{A}\subset\mathcal{B}$, then $\mu(\mathcal{A})\leq\mu(\mathcal{B})$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Subadditivity&lt;/em&gt;: If $\mathcal{A_1},\mathcal{A_2},\dots$ is a countable collection of sets, not necessarily disjoint, then
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)\leq\sum_i\mu(\mathcal{A_i})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Cardinality of a set&lt;/em&gt; \(\#\mathcal{A}\)&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;A point mass at 0&lt;/em&gt;. Consider a measure \(\delta_{\{0\}}\) on $\mathbb{R}$ defined to give measure 1 to any set that contains 0 and measure 0 to any set that does not
\begin{equation}\delta_{{0}}(\mathcal{A})=\#\left(A\cap\{0\}\right)=\begin{cases}
1\quad\textsf{if }0\in\mathcal{A} \\ 0\quad\textsf{otherwise}
\end{cases}\end{equation}
for $\mathcal{A}\subset\mathbb{R}$&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Counting measure on the integers&lt;/em&gt;. Consider a measure $\mu_\mathbb{Z}$ that assigns to each set $\mathcal{A}$ the number of integers contained in $\mathcal{A}$
\begin{equation}
\delta_\mathbb{Z}(\mathcal{A})=\#\left(\mathcal{A}\cap\mathbb{Z}\right)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Geometric measure&lt;/em&gt;. Suppose that $0&amp;lt;r&amp;lt;1$. We define a measure on $\mathbb{R}$ that assigns to a set $\mathcal{A}$ a geometrically weighted sum over non-negative integers in $\mathcal{A}$
\begin{equation}
\mu(\mathcal{A})=\sum_{i\in\mathcal{A}\cap\mathbb{Z}^+}r^i
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Binomial measure&lt;/em&gt;. Let $n\in\mathbb{N}^+$ and let $0&amp;lt;p&amp;lt;1$. We define $\mu$ as:
\begin{equation}
\mu(\mathcal{A})=\sum_{k\in\mathcal{A}\cap\{0,1,\dots,n\}}{n\choose k}p^k(1-p)^{n-k}
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Bivariate Gaussian&lt;/em&gt;. We define a measure on $\mathbb{R}^2$ by:
\begin{equation}
\mu({\mathcal{A}})=\int_\mathcal{A}\dfrac{1}{2\pi}\exp\left({\dfrac{-1}{2}(x^2+y^2)}\right)\,dx\,dy
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Uniform on a Ball in $\mathbb{R}^3$&lt;/em&gt;. Let $\mathcal{B}$ be the set of points in $\mathbb{R}^3$ that are within a distance 1 from the origin (unit ball in $\mathbb{R}^3$). We define a measure on $\mathbb{R}^3$ as:
\begin{equation}
\mu(\mathcal{A})=\dfrac{3}{4\pi}\mu_L(\mathcal{A}\cap\mathcal{B})
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;int-measure-idea&quot;&gt;Integration with respect to a Measure: The Idea&lt;/h2&gt;
&lt;p&gt;Consider $f:\mathcal{X}\rightarrow\mathbb{R}$, where $\mathcal{X}$ is any set and a measure $\mu$ on $\mathcal{X}$ and compute the integral of $f$ w.r.t $\mu$: $\int f(x)\,\mu(dx)$. We have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\mu_L(dx)=\int g(x)\,dx
\end{equation}
Because $\mu_L(dx)\equiv\mu_L([x,x+dx[)=dx$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_{\{\alpha\}}(dx)=g(\alpha)
\end{equation}
Consider the infinitesimal $\delta_{\{\alpha\}}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\neq\alpha$, then the infinitesimal interval $[x,x+dx[$ does not contain $\alpha$, so
\begin{equation}
\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=0
\end{equation}
If $x=\alpha,\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=1$. Thus, when we add up all of the infinitesimals, we get $g(\alpha)\cdot1=g(\alpha)$&lt;/li&gt;
  &lt;li&gt;For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathbb{Z}(dx)=\sum_{i\in\mathbb{Z}}g(i)
\end{equation}
Similarly, consider the infinitesimal $\delta_\mathbb{Z}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\notin\mathbb{Z}$, then $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=0$. And otherwise if $x\in\mathbb{Z}$, $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=1$ since an infinitesimal interval can contain at most one integer. Hence, $g(x)\,\delta_\mathbb{Z}=g(x)$ if $x\in\mathbb{Z}$ and $=0$ otherwise. When we add up all of the infinitesimals over $x$, we get the sum above.&lt;/li&gt;
  &lt;li&gt;Suppose $\mathcal{C}$ is a countable set. We can define &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{C}$ to map $\mathcal{A}\rightarrow\#(\mathcal{A}\cap\mathcal{C})$ (recall that $\delta_\mathcal{C}(\mathcal{A})=\#(\mathcal{A}\cap\mathcal{C})$). For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v),
\end{equation}
using the same basic argument as in the above example.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;From the above examples, we have that &lt;em&gt;integrals w.r.t to Lebesgue measure are just ordinary integrals, and that integrals w.r.t Counting measure are just ordinary summation&lt;/em&gt;.&lt;br /&gt;
Consider measures built from Lebesgue and Counting measure, we have:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=f(x)\,\mu_L(dx)$, then for any function $g$,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,f(x)\,\mu_L(dx)=\int g(x)\,f(x)\,dx
\end{equation}
We say that $f$ is the density of $\mu$ w.r.t Lebesgue measure in this case.&lt;/li&gt;
  &lt;li&gt;Suppose $\mu$ is a measure that satisfies $\mu(dx)=p(x)\delta_\mathcal{C}(dx)$ for a countable set $\mathcal{C}$, then for any function g,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,p(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v)\,f(v)
\end{equation}
We say that $p$ is the density of $\mu$ w.r.t Counting measure on $\mathcal{C}$.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;prop-int&quot;&gt;Properties of the Integral&lt;/h2&gt;
&lt;p&gt;A function is said to be &lt;em&gt;integrable&lt;/em&gt; w.r.t $\mu$ if $\int|f(x)|\,\mu(dx)&amp;lt;\infty$. An integrable function has a well-defined and finite integral. If $f(x)\geq0$, the integral is always well-defined but may be $\infty$.&lt;br /&gt;
Suppose $\mu$ is a measure on $\mathcal{X},\mathcal{A}\subset\mathcal{X}$, and $g$ is a real-valued function on $\mathcal{X}$. We define the integral of $g$ over the set $\mathcal{A}$, denoted by $\int_\mathcal{A}g(x)\,\mu(dx)$, as
\begin{equation}
\int_\mathcal{A}g(x)\,\mu(dx)=\int g(x)\,𝟙_\mathcal{A}(x)\,\mu(dx),
\end{equation}
where \(𝟙_\mathcal{A}\) is an &lt;em&gt;indicator function&lt;/em&gt; (\(𝟙_\mathcal{A}(x)=1\) if $x\in\mathcal{A}$, and $=0$ otherwise).&lt;/p&gt;

&lt;p&gt;Let $\mu$ is a measure on $\mathcal{X},\mathcal{A},\mathcal{B}\subset\mathcal{X},c\in\mathbb{R}$ and $f,g$ are integrable functions. The following properties hold for every $\mu$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Constant Functions&lt;/em&gt;
\begin{equation}
\int_\mathcal{A}c\,\mu(dx)=c\cdot\mu(\mathcal{A})
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity&lt;/em&gt;
\begin{align}
\int_\mathcal{A}cf(x)\,\mu(dx)&amp;amp;=c\int_\mathcal{A}f(x)\,\mu(dx) \\\int_\mathcal{A}\left(f(x)+g(x)\right)\,\mu(dx)&amp;amp;=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{A}g(x)\,\mu(dx)
\end{align}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotonicity&lt;/em&gt;. If $f\leq g$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{A}g(x)\,\mu(dx),\forall\mathcal{A}$. This implies:
    &lt;ul&gt;
      &lt;li&gt;If $f\geq0$, then $\int f(x)\,\mu(dx)\geq0$.&lt;/li&gt;
      &lt;li&gt;If $f\geq0$ and $\mathcal{A}\subset\mathcal{B}$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{B}f(x)\,\mu(dx)$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Null Sets&lt;/em&gt;. If $\mu(\mathcal{A})=0$, then $\int_\mathcal{A}f(x)\,\mu(dx)=0$.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Absolute Values&lt;/em&gt;
\begin{equation}
\left|\int f(x)\,\mu(dx)\right|\leq\int\left|f(x)\right|\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Monotone Convergence&lt;/em&gt;. If $0\leq f_1\leq f_2\leq\dots$ is an increasing sequence of integrable functions that converge to $f$, then
\begin{equation}
\lim_{k\to\infty}\int f_k(x)\,\mu(dx)=\int f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Linearity in region of integration&lt;/em&gt;. If $\mathcal{A}\cap\mathcal{B}=\emptyset$,
\begin{equation}
\int_{\mathcal{A}\cup\mathcal{B}}f(x)\,\mu(dx)=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{B}f(x)\,\mu(dx)
\end{equation}&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;int-measure-detail&quot;&gt;Integration with respect to a Measure: The Details&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;Step 1&lt;/strong&gt;. Define the integral for simple functions.
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;Simple function&lt;/em&gt;: is a function that takes only a finite number of different values.
        &lt;ul&gt;
          &lt;li&gt;All constant functions are simple functions.&lt;/li&gt;
          &lt;li&gt;The indicator function ($𝟙_\mathcal{A}$) of a set $\mathcal{A}\subset\mathcal{X}$ is a simple function (taking values in $\{0,1\}$).&lt;/li&gt;
          &lt;li&gt;Any constant times an indicator ($c𝟙_\mathcal{A}$) is also a simple function (taking values in $\{0,c\}$).&lt;/li&gt;
          &lt;li&gt;Similarly, given disjoint sets $\mathcal{A_1},\mathcal{A_2}$, the linear combination \(c_1𝟙_\mathcal{A_1}+c_2𝟙_\mathcal{A_2}\) is a simple function (taking values in $\{0,c_1,c_2\}$)&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
          &lt;li&gt;In fact, any simple function can be expressed as a linear combination of a finite number of indicator functions. That is, if $f$ is &lt;em&gt;any&lt;/em&gt; simple function on $\mathcal{X}$, then there exists some finite integer $n$, non-zero constants $c_1,\dots,c_n$ and &lt;em&gt;disjoint&lt;/em&gt; sets $\mathcal{A_1},\dots\mathcal{A_n}\subset\mathcal{X}$ such that
   \begin{equation}
   f=c_1𝟙_\mathcal{A_1}+\dots+c_n𝟙_\mathcal{A_n}
   \end{equation}&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;So, if $f:\mathcal{X}\to\mathbb{R}$ is a simple function as just defined, we have that
\begin{equation}
\int \mu(dx)=c_1\mu(\mathcal{A_1})+\dots+c_n\mu(\mathcal{A_n})
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 2&lt;/strong&gt;. Define the integral for general non-negative functions, approximating the general function by simple functions.
    &lt;ul&gt;
      &lt;li&gt;The idea is that we can approximate any general non-negative function $f:\mathcal{X}\to[0,\infty[$ well by some non-negative simple functions that $\leq f$ &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
      &lt;li&gt;If $f:\mathcal{X}\to[0,\infty[$ is a general function and $0\leq s\leq f$ is a simple function (then $\int s(x)\,\mu(dx)\leq\int f(x)\,\mu(dx)$). The closer that $s$ approximates $f$, the closer we expect $\int s(x)\,\mu(dx)$ and $\int f(x)\,\mu(x)$ to be.&lt;/li&gt;
      &lt;li&gt;To be more precise, we define the integral $\int f(x)\,\mu(dx)$ to be the smallest value $I$ such that $\int s(x)\,\mu(x)\leq I$, for all simple functions $0\leq s\leq f$.
\begin{equation}
\int f(x)\,\mu(dx)\approx\sup\left\{\int s(x)\,\mu(dx)\right\}
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step 3&lt;/strong&gt;. Define the integral for general real-valued functions by separately integrating the positive and negative parts of the function.
    &lt;ul&gt;
      &lt;li&gt;If $f:\mathcal{X}\to\mathbb{R}$ is a general function, we can define its &lt;em&gt;positive part&lt;/em&gt; $f^+$ and its &lt;em&gt;negative part&lt;/em&gt; $f^-$ by
\begin{align}
f^+(x)&amp;amp;=\max\left(f(x),0\right) \\ f^-(x)&amp;amp;=\max\left(-f(x),0\right)
\end{align}&lt;/li&gt;
      &lt;li&gt;Since both $f^+$ and $f^-$ are non-negative functions and $f=f^+-f^-$, we have
\begin{equation}
\int f(x)\,\mu(dx)=\int f^+(x)\,\mu(dx)-\int f^-(x)\,\mu(dx)
\end{equation}
This is a well-defined number (possibly infinite) if and only if at least one of $f^+$ and $f^-$ has a finite integral.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;construct-measure&quot;&gt;Constructing Measures from old ones&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;em&gt;Sums and multiples&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Consider the point mass measures at 0 and 1, \(\delta_{\\{0\\}},\delta_1\), and construct a two new measures on $\mathbb{R}$, \(\mu=\delta_{\\{0\\}}+\delta_{\\{1\\}}\) and \(v=4\delta_{\\{0\\}}\), defined by
\begin{align}
\mu(\mathcal{A})&amp;amp;=\delta_{\{0\}}(\mathcal{A})+\delta_{\{0\}}(\mathcal{A}) \\ v(\mathcal{A})&amp;amp;=4\delta_{\{0\}}(\mathcal{A})
\end{align}&lt;/li&gt;
      &lt;li&gt;The measure $\mu$ counts how many elements of \(\\{0,1\\}\) are in its argument. Thus, the counting measure of the integers can be re-expressed as
\begin{equation}
\delta_\mathbb{Z}=\sum_{i=-\infty}^{\infty}\delta_{\{i\}}
\end{equation}&lt;/li&gt;
      &lt;li&gt;By combining the operations of summation and multiplication, we can write the Geometric measure in the above example 
\begin{equation}
\sum_{i=0}^{\infty}r^i\delta_{\{i\}}
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Restriction to a Subset&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $\mathcal{B}\subset\mathcal{X}$. We can define a new measure on $\mathcal{B}$ which maps $\mathcal{A}\subset\mathcal{B}\to\mu(\mathcal{A})$. This is called the restriction of $\mu$ to the set $\mathcal{B}$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Measure Induced by a Function&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $g:\mathcal{X}\to\mathcal{Y}$. We can use $\mu$ and $g$ to define a new measure $v$ on $\mathcal{Y}$ by
\begin{equation}
v(\mathcal{A})=\mu(g^{-1}(\mathcal{A})),
\end{equation}
for $\mathcal{A}\subset\mathcal{Y}$. This is called the &lt;em&gt;measure induced from $\mu$ by $g$&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;Therefore, for any $f:\mathcal{Y}\to\mathbb{R}$,
\begin{equation}
\int f(y)\,v(dy)=\int f(g(x))\,\mu(dx)
\end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Integrating a Density&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;Suppose $\mu$ is a measure on $\mathcal{X}$ and $f:\mathcal{X}\to\mathbb{R}$. We can define a new measure $v$ on $\mathcal{X}$ as
\begin{equation}
v(\mathcal{A})=\int_\mathcal{A}f(x)\,\mu(dx)\tag{1}\label{1}
\end{equation}
We say that $f$ is the &lt;em&gt;density&lt;/em&gt; of the measure $v$ w.r.t $\mu$.&lt;/li&gt;
      &lt;li&gt;If $v,\mu$ are measures for which the equation \eqref{1} holds for every $\mathcal{A}\subset\mathcal{X}$, we say that $v$ has a density $f$ w.r.t $\mu$. This implies two useful results:
        &lt;ul&gt;
          &lt;li&gt;$\mu(\mathcal{A})=0$ implies $v(\mathcal{A})=0$.&lt;/li&gt;
          &lt;li&gt;$v(dx)=f(x)\,\mu(dx)$.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-types&quot;&gt;Other types of Measures&lt;/h2&gt;
&lt;p&gt;Suppose that $\mu$ is a measure on $\mathcal{X}$&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;If $\mu(\mathcal{X})=\infty$, we say that $\mu$ is an &lt;em&gt;infinite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;\infty)$, we say that $\mu$ is a &lt;em&gt;finite measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu(\mathcal{X}&amp;lt;1)$, we say that $\mu$ is a &lt;em&gt;probability measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If there exists a countable set $\mathcal{S}$ such that $\mu(\mathcal{X}-\mathcal{S})=0$, we say that $\mu$ is a &lt;em&gt;discrete measure&lt;/em&gt;. Equivalently, $\mu$ has a density w.r.t &lt;em&gt;counting measure&lt;/em&gt; on $\mathcal{S}$.&lt;/li&gt;
  &lt;li&gt;If $\mu$ has a density w.r.t Lebesgue measure, we say that $\mu$ is a &lt;em&gt;continuous measure&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;If $\mu$ is neither &lt;em&gt;continuous&lt;/em&gt; nor &lt;em&gt;discrete&lt;/em&gt;, we say that $\mu$ is a &lt;em&gt;mixed measure&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Literally, this post is mainly written from a source that I’ve lost the reference :(. Hope that I can update this line soon.&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://mathworld.wolfram.com/LebesgueMeasure.html&quot;&gt;Lebesgue Measure&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://www.countbayesie.com/blog/2015/8/17/a-very-brief-and-non-mathematical-introduction-to-measure-theory-for-probability&quot;&gt;Measure Theory for Probability: A Very Brief Introduction&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;other-resources&quot;&gt;Other Resources&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cyW5z-M2yzw&quot;&gt;Music and Measure Theory - 3Blue1Brown&lt;/a&gt; - this is one of my favourite Youtube channels.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If $\mathcal{A_1},\mathcal{A_2}$ were not disjoint, we could define $\mathcal{B_1}=\mathcal{A_1}-\mathcal{A_2}$, $\mathcal{B_2}=\mathcal{A_2}-\mathcal{A_1}$, and $\mathcal{B_3}=\mathcal{A_1}\cap\mathcal{A_2}$. Then the function is equal to \(c_1𝟙_\mathcal{B_1}+c_2𝟙_\mathcal{B_2}+(c_1+c_2)𝟙_\mathcal{B_3}\). &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;

      &lt;p&gt;&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="measure-theory" /><category term="mathematics" /><category term="measure-theory" /><category term="random-stuffs" /><summary type="html">When talking about measure, you might associate it with the idea of length, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with area, or even three dimensions with volume.</summary></entry><entry><title type="html">Markov Chain</title><link href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html" rel="alternate" type="text/html" title="Markov Chain" /><published>2021-06-19T22:27:00+07:00</published><updated>2021-06-19T22:27:00+07:00</updated><id>/random-stuffs/probability-statistics/2021/06/19/markov-chain</id><content type="html" xml:base="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">&lt;blockquote&gt;
  &lt;p&gt;Since I have no idea how to begin with this post, why not just dive straight into details :P&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#markov-property&quot;&gt;Markov Property&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#transition-matrix&quot;&gt;Transition Matrix&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nstep-trans-prob&quot;&gt;n-step Transition Probability&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#marginal-dist-xn&quot;&gt;Marginal Distribution of $X_n$&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#properties&quot;&gt;Properties&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#stationary-distribution&quot;&gt;Stationary Distribution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#reversibility&quot;&gt;Reversibility&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exp-app&quot;&gt;Examples and Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Markov chain&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; is a stochastic process in which the random variables follow a special property called &lt;em&gt;Markov&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;markov-property&quot;&gt;Markov Property&lt;/h4&gt;
&lt;p&gt;A sequence of random variables $X_0, X_1, X_2, \dots$ taking values in the &lt;em&gt;state space&lt;/em&gt; $S=${$1, 2,\dots, M$}. For all $n\geq0$,
\begin{equation}
P(X_{n+1}=j|X_n=i)=P(X_{n+1}=j|X_n=i,X_{n-1}=i_{n-1},X_{n-2}=i_{n-2},\dots,X_0=i_0)
\end{equation}
In other words, knowledge of the preceding state is all we need to determine the probability distribution of the current state&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h4 id=&quot;transition-matrix&quot;&gt;Transition Matrix&lt;/h4&gt;
&lt;p&gt;The quantity $P(X_{n+1}=j|X_n=i)$ is &lt;em&gt;transition probability&lt;/em&gt; from state $i$ to $j$.&lt;br /&gt;
If we denote that $q_{ij}=P(X_{n+1}=j|X_n=i)$ and let $Q=(q_{ij})$, which is a $M\times M$ matrix, there we have the &lt;em&gt;transition matrix&lt;/em&gt; $Q$ of the chain.&lt;br /&gt;
Therefore, each row of $Q$ is a conditional probability mass function (PMF) of $X_{n+1}$ given $X_n$. And hence, sum of its entries is 1.&lt;/p&gt;

&lt;h5 id=&quot;nstep-trans-prob&quot;&gt;n-step Transition Probability&lt;/h5&gt;
&lt;p&gt;The n-step &lt;em&gt;transition probability&lt;/em&gt; from $i$ to $j$ is the probability of being at $i$ and $n$ steps later being at $j$, and be denoted as $q_{ij}^{(n)}$,
\begin{equation}
q_{ij}^{(n)}=P(X_n=j|X_0=i)
\end{equation}
We have that
\begin{equation}
q_{ij}^{(2)}=\sum_{k}^{}q_{ik}q_{kj}
\end{equation}
since it has to go through an intermediary step $k$ to reach $j$ in 2 steps from $i$. It’s easily seen that the right hand side is $Q_{ij}^2$. And by induction, we have that:
\begin{equation}
q_{ij}^{(n)}=Q_{ij}^{n}
\end{equation}
$Q^n$ is also called the &lt;em&gt;n-step transition matrix&lt;/em&gt;.&lt;/p&gt;

&lt;h5 id=&quot;marginal-dist-xn&quot;&gt;Marginal Distribution of $X_n$&lt;/h5&gt;
&lt;p&gt;Let $t=(t_1,\dots,t_M)^T$, where $t_i=P(X_0=i)$. By the law of total probability (LOTP), we have that:
\begin{align}
P(X_n=j)&amp;amp;=\sum_{i=1}^{M}P(X_0=i)P(X_n=j|X_0=i) \\&amp;amp;=\sum_{i=1}^{M}t_iq_{ij}^{(n)}
\end{align}
or the marginal distribution of $X_n$ is given by $tQ^n$.&lt;/p&gt;

&lt;h4 id=&quot;properties&quot;&gt;Properties&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;State $i$ of a Markov chain is defined as &lt;em&gt;recurrent&lt;/em&gt; or &lt;em&gt;transient&lt;/em&gt; depending upon whether or not the Markov chain will eventually return to it. Starting with &lt;em&gt;recurrent&lt;/em&gt; state i, the chain will return to it with the probability of 1. Otherwise, it is &lt;em&gt;transient&lt;/em&gt;.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: Number of returns to &lt;em&gt;transient&lt;/em&gt; state is distributed by &lt;em&gt;Geom($p$)&lt;/em&gt;, with $p&amp;gt;0$ is the probability of never returning to $i$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A Markov chain is defined as &lt;em&gt;irreducible&lt;/em&gt; if there exists a chain of steps between any $i,j$ that has positive probability. That is for any $i,j$, there is some $n&amp;gt;0,\in\mathbb{N}$ such that $Q^n_{ij}&amp;gt;0$. If not &lt;em&gt;irreducible&lt;/em&gt;, it’s called &lt;em&gt;reducible&lt;/em&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proposition&lt;/strong&gt;: &lt;em&gt;Irreducible&lt;/em&gt; implies all states &lt;em&gt;recurrent&lt;/em&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A state $i$ has &lt;em&gt;period&lt;/em&gt; $k&amp;gt;0$ if $k$ is the greatest common divisor (gcd) of the possible numbers of steps it can take to return to $i$ when starting at $i$.
And thus, $k=gcd(n)$ such that $Q^n_{ii}&amp;gt;0$. $i$ is called &lt;em&gt;aperiodic&lt;/em&gt; if $k_i=1$, and &lt;em&gt;periodic&lt;/em&gt; otherwise. The chain itself is called &lt;em&gt;aperiodic&lt;/em&gt; if all its states are &lt;em&gt;aperiodic&lt;/em&gt;, and &lt;em&gt;periodic&lt;/em&gt; otherwise.&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;stationary-distribution&quot;&gt;Stationary Distribution&lt;/h4&gt;
&lt;p&gt;A vector $s=(s_1,\dots,s_M)^T$ such that $s_i\geq0$ and $\sum_{i}s_i=1$ is a &lt;em&gt;stationary distribution&lt;/em&gt; for a Markov chain if
\begin{equation}
\sum_{i}s_iq_{ij}=s_j
\end{equation}
for all $j$, or equivalently $sQ=s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Existence and uniqueness of stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Any &lt;em&gt;irreducible&lt;/em&gt; Markov chain has a unique &lt;em&gt;stationary distribution&lt;/em&gt;. In this distribution, every state has positive probability.&lt;/p&gt;

&lt;p&gt;The theorem is a consequence of a result from &lt;em&gt;Perron-Frobenius theorem&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Convergence to stationary distribution&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be a Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$ and &lt;em&gt;transition matrix&lt;/em&gt; $Q$, such that some power $Q^m$ has all entries positive (or in the other words, the chain is &lt;em&gt;irreducible&lt;/em&gt; and &lt;em&gt;aperiodic&lt;/em&gt;). Then $P(X_n=i)$ converges to $s_i$ as $n\rightarrow\infty$ (or $Q^n$ converges to a matrix in which each row is $s$).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Expected time to run&lt;/em&gt;)&lt;br /&gt;
    Let $X_0,X_1,\dots$ be an &lt;em&gt;irreducible&lt;/em&gt; Markov chain with &lt;em&gt;stationary distribution&lt;/em&gt; $s$. Let $r_i$ be the expected time it takes the chain to return to $i$, given that it starts at $i$. Then $s_i=1/r_i$&lt;/p&gt;

&lt;h4 id=&quot;reversibility&quot;&gt;Reversibility&lt;/h4&gt;
&lt;p&gt;Let $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain. Suppose there is an $s=(s_1,\dots,s_M)^T$ with $s_i\geq0,\sum_{i}s_i=1$, such that
\begin{equation}
s_iq_{ij}=s_jq_{ji}
\end{equation}
for all states $i,j$. This equation is called &lt;em&gt;reversibility&lt;/em&gt; or &lt;em&gt;detailed balance&lt;/em&gt; condition. And if the condition holds, we say that the chain is &lt;em&gt;reversible&lt;/em&gt; w.r.t $s$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt; (&lt;em&gt;Reversible implies stationary&lt;/em&gt;)&lt;br /&gt;
    Suppose that $Q=(q_{ij})$ be the &lt;em&gt;transition matrix&lt;/em&gt; of a Markov chain that is &lt;em&gt;reversible&lt;/em&gt; w.r.t to an $s=(s_1,\dots,s_M)^T$ with with $s_i\geq0,\sum_{i}s_i=1$. Then $s$ is a &lt;em&gt;stationary distribution&lt;/em&gt; of the chain. (&lt;em&gt;proof&lt;/em&gt;:$\sum_{j}s_jq_{ji}=\sum_{j}s_iq_{ij}=s_i\sum_{j}q_{ij}=s_i$)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proposition&lt;/strong&gt;&lt;br /&gt;
    If each column of $Q$ sum to 1, then the &lt;em&gt;uniform distribution&lt;/em&gt; over all states $(1/M,\dots,1/M)$, is a &lt;em&gt;stationary distribution&lt;/em&gt;. (This kind of matrix is called &lt;em&gt;doubly stochastic matrix&lt;/em&gt;).&lt;/p&gt;

&lt;h4 id=&quot;exp-app&quot;&gt;Examples and Applications&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Finite-state_machine&quot;&gt;&lt;em&gt;Finite-state machines&lt;/em&gt;&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Random_walk&quot;&gt;&lt;em&gt;random walks&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Diced board games such as Ludo, Monopoly,…&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/PageRank&quot;&gt;&lt;em&gt;Google PageRank&lt;/em&gt;&lt;/a&gt; - the heart of Google search&lt;/li&gt;
  &lt;li&gt;Markov Decision Process (MDP), which is gonna be the content of next &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html&quot;&gt;post&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;And various other applications.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Joseph K. Blitzstein &amp;amp; Jessica Hwang. &lt;a href=&quot;https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573&quot;&gt;Introduction to Probability&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] &lt;a href=&quot;https://brilliant.org/wiki/markov-chains/&quot;&gt;Brillant’s Markov chain&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] &lt;a href=&quot;https://en.wikipedia.org/wiki/Perron–Frobenius_theorem&quot;&gt;Perron-Frobenius theorem&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;This is more like intuitive and less formal definition of Markov chain, we will have a more concrete definition with the help of &lt;em&gt;Measure theory&lt;/em&gt; after the post about it. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The Markov chain here is &lt;em&gt;time-homogeneous&lt;/em&gt; Markov chain, in which the probability of any state transition is independent of time. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Well, it only matters where you are, not where you’ve been. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="random-stuffs" /><category term="probability-statistics" /><category term="mathematics" /><category term="probability-statistics" /><summary type="html">Since I have no idea how to begin with this post, why not just dive straight into details :P</summary></entry><entry><title type="html">My very first post</title><link href="/mathematics/linear-algebra/2021/06/05/fibonacci-generator.html" rel="alternate" type="text/html" title="My very first post" /><published>2021-06-05T17:00:00+07:00</published><updated>2021-06-05T17:00:00+07:00</updated><id>/mathematics/linear-algebra/2021/06/05/fibonacci-generator</id><content type="html" xml:base="/mathematics/linear-algebra/2021/06/05/fibonacci-generator.html">&lt;blockquote&gt;
  &lt;p&gt;Enjoy my index-zero-ed post while staying tuned for next ones!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fibonacci&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
	generate i-th number of the Fibonacci sequence, python code obvs :p
	&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Why did numbers \(\frac{1+\sqrt{5}}{2}\) and \(\frac{1-\sqrt{5}}{2}\) come out of nowhere?&lt;/p&gt;

&lt;p&gt;In fact, these two numbers are eigenvalues of matrix \(A=\left(\begin{smallmatrix}1 &amp;amp; 1\\1 &amp;amp; 0\end{smallmatrix}\right)\), which is retrieved from
\begin{equation}
u_{k+1}=Au_k,
\end{equation}
where \(u_k=\left(\begin{smallmatrix}F_{k+1}\\F_k\end{smallmatrix}\right)\).
And thus, \(u_k=A^k u_0\).&lt;/p&gt;

&lt;p&gt;Then, the thing is, how can we compute \(A^k\) quickly? This is where diagonalizing plays its role. Diagonalizing produces a factorization:
\begin{equation}
A=S\Lambda S^{-1},
\end{equation}
where \(S=\left(\begin{smallmatrix}x_1 &amp;amp; \dots &amp;amp; x_n\end{smallmatrix}\right)\) is eigenvector matrix, \(\Lambda=\left(\begin{smallmatrix}\lambda_1&amp;amp;&amp;amp;\\&amp;amp;\ddots&amp;amp;\\&amp;amp;&amp;amp;\lambda_n\end{smallmatrix}\right)\) is a diagonal matrix established from eigenvalues of \(A\).&lt;/p&gt;

&lt;p&gt;When taking the power of \(A\),
\begin{equation}
A^k u_0=(S\Lambda S^{-1})\dots(S\Lambda S^{-1})u_0=S\Lambda^k S^{-1} u_0,
\end{equation}
writing \(u_0\) as a combination \(c_1x_1+\dots+c_nx_n\) of the eigenvectors, we have that \(c=S^{-1}u_0\). Hence:
\begin{equation}
u_k=A^ku_0=c_1{\lambda_1}^kx_1+\dots+c_n{\lambda_n}^kx_n
\end{equation}
&lt;em&gt;Fact&lt;/em&gt;: The \(\frac{1+\sqrt{5}}{2}\approx 1.618\) is so-called “&lt;strong&gt;golden ratio&lt;/strong&gt;”. And &lt;em&gt;for some reason a rectangle with sides 1.618 and 1 looks especially graceful&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Gilbert Strang. &lt;a href=&quot;http://math.mit.edu/~gs/linearalgebra/&quot;&gt;Introduction to Linear Algebra&lt;/a&gt;&lt;/p&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="linear-algebra" /><category term="mathematics" /><category term="linear-algebra" /><category term="random-stuffs" /><summary type="html">Enjoy my index-zero-ed post while staying tuned for next ones!</summary></entry></feed>