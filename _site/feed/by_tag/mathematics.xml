<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed/by_tag/mathematics.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-23T20:28:23+07:00</updated><id>http://localhost:4000/feed/by_tag/mathematics.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Measure theory - III: the Lebesgue integral</title><link href="http://localhost:4000/2022/08/21/measure-theory-p3.html" rel="alternate" type="text/html" title="Measure theory - III: the Lebesgue integral" /><published>2022-08-21T13:00:00+07:00</published><updated>2022-08-21T13:00:00+07:00</updated><id>http://localhost:4000/2022/08/21/measure-theory-p3</id><content type="html" xml:base="http://localhost:4000/2022/08/21/measure-theory-p3.html"><![CDATA[<blockquote>
  <p>Part III of the measure theory series. Materials are mostly taken from <a href="/2022/08/21/measure-theory-p3.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/08/21/measure-theory-p3.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#int-simp-funcs">Integration of simple functions</a>
    <ul>
      <li><a href="#simp-func">Simple function</a></li>
      <li><a href="#int-unsgn-simp-func">Integral of unsigned simple functions</a></li>
      <li><a href="#well-dfn-simp-int">Well-definedness of simple integral</a></li>
      <li><a href="#alm-evwhr-spt">Almost everywhere and support</a></li>
      <li><a href="#bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</a></li>
      <li><a href="#abs-cvg-simp-int">Absolutely convergence simple integral</a></li>
      <li><a href="#bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</a></li>
    </ul>
  </li>
  <li><a href="#msr-funcs">Measurable functions</a>
    <ul>
      <li><a href="#unsgn-msr-funcs">Unsigned measurable functions</a></li>
      <li><a href="#equiv-ntn-msrb">Equivalent notions of measurability</a></li>
      <li><a href="#cmplx-msrb">Complex measurability</a></li>
      <li><a href="#equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</a></li>
      <li><a href="#eg-msr-func">Examples of measurable function</a></li>
    </ul>
  </li>
  <li><a href="#unsgn-lebesgue-int">Unsigned Lebesgue integrals</a>
    <ul>
      <li><a href="#lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</a></li>
    </ul>
  </li>
  <li><a href="#abs-intb">Absolute integrability</a></li>
  <li><a href="#littlewoods-prncpl">Littlewood’s three principles</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="int-simp-funcs">Integration of simple functions</h2>
<p>Analogy to how the <a href="/2022/06/16/measure-theory-p1.html#riemann-integrability"><strong>Riemann integral</strong></a> was established by using the integral for <a href="/2022/06/16/measure-theory-p1.html#pc-func"><strong>piecewise constant functions</strong></a>, the <strong>Lebesgue integral</strong> is set up using the integral for <strong>simple functions</strong>.</p>

<h3 id="simp-func">Simple function</h3>
<p>A (complex-valued) <strong>simple function</strong> $f:\mathbb{R}^d\to\mathbb{C}$ is a finite linear combination
\begin{equation}
f=c_1 1_{E_1}+\ldots+c_k 1_{E_k},\label{eq:sf.1}
\end{equation}
of indicator functions $1_{E_i}$ of Lebesgue measurable sets $E_i\subset\mathbb{R}^d$ for $i=1,\ldots,k$, for natural number $k\geq 0$ and where $c_1,\ldots,c_k\in\mathbb{C}$ are complex numbers.</p>

<p>An <strong>unsigned simple function</strong> $f:\mathbb{R}^d\to[0,+\infty]$ is given as \eqref{eq:sf.1} but with the $c_i$ taking values in $[0,+\infty]$ rather than $\mathbb{C}$.</p>

<h3 id="int-unsgn-simp-func">Integral of a unsigned simple function</h3>
<p>If $f=c_1 1_{E_1}+\ldots+c_k 1_{E_k}$ is an unsigned simple function, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq c_1 m(E_1)+\ldots+c_k m(E_k),
\end{equation}
which means $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\in[0,+\infty]$.</p>

<h3 id="well-dfn-simp-int">Well-definedness of simple integral</h3>
<p><strong>Lemma 1</strong><br />
<em>Let $k,k’\geq 0$ be natural, $c_1,\ldots,c_k,c_1’,\dots,c_{k’}’\in[0,+\infty]$ and $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’\subset\mathbb{R}^d$ be Lebesgue measurable sets such that the identity
\begin{equation}
c_1 1_{E_1}+\ldots+c_k 1_{E_k}=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’}\label{eq:lemma1.1}
\end{equation}
holds identically on $\mathbb{R}^d$. Then we have</em>
\begin{equation}
c_1 m(E_1)+\ldots+c_k m(E_k)=c_1’ m(E_1’)+\ldots+c_{k’}’ m(E_{k’}’)
\end{equation}</p>

<p><strong>Proof</strong><br />
The $k+k’$ sets $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ partition $\mathbb{R}^d$ into $2^{k+k’}$ disjoint sets, each of which is an intersection of some of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ and their complements<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Removing any sets that are empty, we end up with a partition of $R^d$ of $m$ non-empty disjoint sets $A_1,\ldots,A_m$ for some $0\leq m\leq 2^{k+k’}$. It easily seen that $A_1,\ldots,A_m$ are then Lebesgue measurable due to the Lebesgue measurability of $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$.</p>

<p>With this set up, each of the $E_1,\ldots,E_k,E_1’,\ldots,E_{k’}’$ are unions of some of the $A_1,\ldots,A_m$. Or in other words, we have
\begin{equation}
E_1=\bigcup_{j\in J_i}A_j,
\end{equation}
and
\begin{equation}
E_{i’}’=\bigcup_{j’\in J_{i’}’}A_j’,
\end{equation}
for all $i=1,\ldots,k$ and $i’=1,\ldots,k’$, and some subsets $J_i,J_{i’}’\subset\{1,\ldots,m\}$. By finite additivity property of Lebesgue measure, we therefore have
\begin{equation}
m(E_i)=\sum_{j\in J_i}m(A_j)
\end{equation}
and
\begin{equation}
m(E_{i’}’)=\sum_{j\in J_{i’}’}m(A_j)
\end{equation}
Hence, the problem remains to show that
\begin{equation}
\sum_{i=1}^{k}c_i\sum_{j\in J_i}m(A_j)=\sum_{i’=1}^{k’}c_{i’}’\sum_{j\in J_{i’}’}m(A_j)\label{eq:lemma1.2}
\end{equation}
Fix $1\leq j\leq m$, we have that at each point $x$ in the non-empty set $A_j$, $1_{E_i}(x)$ is equal to $1_{J_i}(j)$, and similarly $1_{E_{i’}’}(x)$ is equal to $1_{J_{i’}’}(j)$. Then from \eqref{eq:lemma1.1} we have
\begin{equation}
\sum_{i=1}^{k}c_i 1_{J_i}(j)=\sum_{i’=1}^{k’}c_{i’}’1_{J_{i’}’}(j)
\end{equation}
Multiplying both sides by $m(A_j)$ and then summing over all $j=1,\ldots,m$, we obtain \eqref{eq:lemma1.2}</p>

<h3 id="alm-evwhr-spt">Almost everywhere and support</h3>
<p>A property $P(x)$ of a point $x\in\mathbb{R}^d$ is said to hold <strong>(Lebesgue) almost everywhere</strong> in $\mathbb{R}^d$ or for <strong>(Lebesgue) almost every point</strong> $x\in\mathbb{R}^d$, if the set of $x\in\mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure of zero (i.e. $P$ is true outside of a null set).</p>

<p>Two functions $f,g:\mathbb{R}^d\to Z$ into an arbitrary range $Z$ are referred to <strong>agree almost everywhere</strong> if we have $f(x)=g(x)$ almost every $x\in\mathbb{R}^d$.</p>

<p>The <strong>support</strong> of a function $f:\mathbb{R}^d\to\mathbb{C}$ or $f:\mathbb{R}^d\to[0,+\infty]$ is defined to be the set $\{x\in\mathbb{R}^d:f(x)\neq 0\}$ where $f$ is non-zero.</p>

<p><strong>Remark 2</strong></p>
<ul>
  <li>If $P(x)$ holds for almost every $x$, and $P(x)$ implies $Q(x)$, then $Q(x)$ holds for almost every $x$.</li>
  <li>If $P_1(x),P_2(x),\ldots$ are an at most countable family of properties, each of which individually holds for almost every $x$, then they will simultaneously holds for almost every $x$, since the countable union of null sets is still a null set.</li>
</ul>

<h3 id="bsc-prop-simp-unsgn-int">Basic properties of the simple unsigned integral</h3>
<p>Let $f,g:\mathbb{R}^d\to[0,+\infty]$ be simple unsigned functions.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in[0,+\infty]$.
	</li>
	<li>
		<b>Finiteness</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$ iff $f$ is finite almost everywhere, and its support has finite measure.
	</li>
	<li>
		<b>Vanishing</b>. We have $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=0$ iff $f$ is zero almost everywhere.
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Monotonicity</b>. If $f(x)\leq g(x)$ for almost every $x\in\mathbb{R}^d$, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\leq\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
Since $f,g:\mathbb{R}^d\to[0,+\infty]$ are simple unsigned functions, we can assume that
\begin{align}
f&amp;=c_1 1_{E_1}+\ldots+c_k 1_{E_k}, \\ g&amp;=c_1’ 1_{E_1’}+\ldots+c_{k’}’ 1_{E_{k’}’},
\end{align}
where $c_1,\ldots,c_k,c_1’,\ldots,c_{k’}’\in[0,+\infty]$.</p>
<ul id="roman-list">
	<li>
		<b>Unsigned linearity</b><br />
		We have
		\begin{align}
		\hspace{-1cm}\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx&amp;=c_1 m(E_1)+\ldots+c_k m(E_k)+c_1' m(E_1')+\ldots+c_{k'}' m(E_{k'}') \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For any $c\in[0,+\infty]$, we have
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=c\left(c_1 m(E_1)+\ldots+c_k m(E_k)\right) \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Finiteness</b><br />
		Given $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx&lt;\infty$, then for every $i=1,\ldots,k$ we have that
		\begin{equation}
		c_i m(E_i)&lt;\infty\label{eq:bpsui.1}
		\end{equation}
		Suppose that $f$ is not finite almost everywhere, which means that there exists $1\leq i\leq k$ such that $E_i$ is a non-null set and $c_i=\infty$, or
		\begin{equation}
		c_i m(E_i)=\infty,
		\end{equation}
		which is in contrast with \eqref{eq:bpsui.1}.<br />
		Suppose that the support of $f$ has infinite measure, or in other word
		\begin{equation}
		c_i\neq 0,\hspace{1cm}i=1,\ldots,k\label{eq:bpsui.2}
		\end{equation}
		and
		\begin{equation}
		m\left(\bigcup_{n=1}^{k}E_n\right)=\infty,
		\end{equation}
		Since any $k$ subsets $E_1,\ldots,E_k$ of $\mathbb{R}^d$ partition $\mathbb{R}^d$ into $2^k$ disjoint sets, say $F_1,\ldots,F_{2^k}$. Hence, by finite additivity property of Lebesgue measure, we have
		\begin{equation}
		\sum_{n=1}^{2^k}m(F_n)=\infty,
		\end{equation}
		which implies that there exists $1\leq n\leq 2^k$ such that $m(F_n)=\infty$. And therefore, for a particular $1\leq i\leq k$ such that $F_n\subset E_i$, by monotonicity property of Lebesgue measure
		\begin{equation}
		m(E_i)\geq m(F_n)=\infty
		\end{equation}
		Thus, combining with \eqref{eq:bpsui.2} gives us
		\begin{equation}
		c_i m(E_1)=\infty,
		\end{equation}
		which again contradicts to \eqref{eq:bpsui.1}.<br />
		Given $f$ is finite almost everywhere and its support has finite measure, suppose that its integral is infinite, or
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=\infty,
		\end{equation}
		which implies that there exists $1\leq i\leq k$ such that either<br />
		(1) $c_i=\infty$ and $E_i$ is a non-null set, or<br />
		(2) $c_i\neq 0$ and $m(E)=\infty$.<br />
		If (1) happens, we then have that
		\begin{equation}
		f\geq c_i 1_{E_i}=\infty,
		\end{equation}
		which contradicts to our hypothesis.<br />
		If (2) happens, by monotonicity of Lebesgue measure, the support of $f$ then has infinite measure, which also contradicts to our hypothesis.
	</li>
	<li>
		<b>Vanishing</b><br />
		Given $\text{Simp}\int_{\mathbf{R^d}}f(x)\,dx=0$, we then have
		\begin{equation}
		c_1 m(E_1)+\ldots+c_k m(E_k)=0,
		\end{equation}
		which implies that for every $1\leq i\leq k$, we have that $c_i=0$ or $E_i$ is a null set.
		Therefore, $f$ is zero almost everywhere because in this case $f$ takes the value of non-zero iff $x$ is in a particular null set $E_j$.<br />
		Given $f$ is zero almost everywhere, for every $i=1,\ldots,k$, we have that either<br />
		(1) $c_i=0$, or<br />
		(2) $c_i\neq 0$ and $x\notin E_i$ with $E_i$ is a null set, or<br />
		(3) $c_i=0$ and and $x\notin E_i$ with $E_i$ is a null set.<br />
		Therefore, the integral of $f$
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=c_1 m(E_1)+\ldots+c_k m(E_k)=0
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
		Given $f$ and $g$ agree almost everywhere, we have that at any point $x\in\mathbb{R}^d$ such that $f(x)=g(x)$, by <b>lemma 1</b>, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		For more convenient, let $K=\{E_i\cap E_{i'}':1\leq i\leq k,1\leq i'\leq k'\}$. The set $K$ then has cardinality of $kk'$. Thus, without loss of generality, we can denote $K$ as
		\begin{equation}
		K=\{K_1,\ldots,K_{kk'}\}
		\end{equation}
		With this definition of $K$, the functions $f$ and $g$ can be rewritten by
		\begin{equation}
		f=a_1 1_{K_1}+\ldots+a_{kk'}1_{K_{kk'}}\label{eq:bpsui.3}
		\end{equation}
		and
		\begin{equation}
		g=b_1 1_{K_1}+\ldots+b_{kk'}1_{K_{kk'}}\label{eq:bpsui.4}
		\end{equation}
		On the other hand, the set in which $f(x)\neq g(x)$ is a null set. Thus by \eqref{eq:bpsui.3} and \eqref{eq:bpsui.4}, we have $x\in A$, where some $A\subset K$ is a null set, and for each $i$ such that $K_i\subset A$ (thus is also a null set, or $m(K_i)=0$), $a_i\neq b_i$, otherwise if $K_i\notin A$, $a_i=b_i$. Therefore, we obtain
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx=\sum_{i,K_i\notin A}c_i m(K_i)
		\end{equation}
		which proves our claim.
	</li>
	<li>
		<b>Monotonicity</b><br />
		Using the same procedure as the proof for equivalence, our claim can be proved.
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
		This follows directly from definition
	</li>
</ul>

<h3 id="abs-cvg-simp-int">Absolutely convergence simple integral</h3>
<p>A complex valued simple function $f:\mathbb{R}^d\to\mathbb{C}$ is known as <strong>absolutely integrable</strong> if
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}\vert f(x)\vert\,dx&lt;\infty
\end{equation}
If $f$ is absolutely integrable, the integral $\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx$ is defined for real signed $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}f_+(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}f_-(x)\,dx,
\end{equation}
where
\begin{align}
f_+(x)&amp;\doteq\max\left(f(x),0\right), \\ f_-(x)&amp;\doteq\max\left(-f(x),0\right),
\end{align}
and for complex-valued $f$ by the formula
\begin{equation}
\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx\doteq\text{Simp}\int_{\mathbb{R}^d}\text{Re}\,f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}\,f(x)\,dx
\end{equation}</p>

<h3 id="bsc-prop-cmplx-simp-int">Basic properties of the complex-valued simple integral</h3>
<p>Let $f,g:\mathbb{R}^d\to\mathbb{C}$ be absolutely integrable simple functions</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b>. We have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
		and
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx,
		\end{equation}
		for all $c\in\mathbb{C}$. Also we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b>. If $f$ and $g$ agree almost everywhere, then
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx=\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b>. For any Lebesgue measurable $E$, we have
		\begin{equation}
		\text{Simp}\int_{\mathbb{R}^d}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong><br />
We first consider the case of real-valued $f$ and $g$.</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		Using the identity
		\begin{equation}
		f+g=(f+g)_{+}-(f+g)_{-}=(f_{+}-f_{-})+(g_{+}-g_{-})
		\end{equation}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>
<p>For complex-valued $f$ and $g$ we have:</p>
<ul id="roman-list">
	<li>
		<b>*-linearity</b><br />
		By definition of complex-valued simple integral and by linearity of simple unsigned integral we have
		\begin{align}
		&amp;\text{Simp}\int_{\mathbb{R}^d}f(x)+g(x)\,dx\nonumber \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}(f(x)+g(x))\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}(f(x)+g(x))\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}\text{Re}g(x)\,dx\nonumber \\ &amp;\hspace{2cm}+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}g(x)\,dx \\ &amp;=\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx+\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx
		\end{align}
		For the complex conjugate $\overline{f}$, we have its integral can be written as
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}\overline{f}(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx-\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=\overline{\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx}
		\end{align}
		Also, for any $c\in\mathbb{C}$, using linearity of simple unsigned integrals once again gives us
		\begin{align}
		\text{Simp}\int_{\mathbb{R}^d}cf(x)\,dx&amp;=\text{Simp}\int_{\mathbb{R}^d}c\,\text{Re}f(x)\,dx+i\,\text{Simp}\int_{\mathbb{R}^d}c\,\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}\text{Re}f(x)\,dx+c i\,\text{Simp}\int_{\mathbb{R}^d}\text{Im}f(x)\,dx \\ &amp;=c\,\text{Simp}\int_{\mathbb{R}^d}f(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Equivalence</b><br />
	</li>
	<li>
		<b>Compatibility with Lebesgue measure</b><br />
	</li>
</ul>

<h2 id="msr-funcs">Measurable functions</h2>
<p>Just as how the piecewise constant integral can be extended to the Riemann integral, the unsigned simple integral can be extended to the unsigned Lebesgue integral, by expanding the class of unsigned simple functions to the broader class of <strong>unsigned Lebesgue measurable functions</strong>.</p>

<h3 id="unsgn-msr-funcs">Unsigned measurable functions</h3>
<p>An unsigned function $f:\mathbb{R}^d\to[0,+\infty]$ is <strong>unsigned Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise limit of unsigned simple functions, i.e. if there exists a sequence $f_1,f_2,\ldots:\mathbb{R}\to[0,+\infty]$ of unsigned simple functions such that $f_n(x)\to f(x)$ for every $x\in\mathbb{R}^d$.</p>

<h3 id="equiv-ntn-msrb">Equivalent notions of measurability</h3>
<p><strong>Lemma 3</strong><br />
Let $f:\mathbb{R}\to[0,+\infty]$ be an unsigned function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is unsigned Lebesgue measurable.
	</li>
	<li>
		$f$ is the pointwise limit of unsigned simple functions $f_n$ (hence $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for all $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of unsigned simple function $f_n$ (thus $\lim_{n\to\infty}f_n(x)$ exists and is equal to $f(x)$ for almost every $x\in\mathbb{R}^d$).
	</li>
	<li>
		$f(x)=\sup_n f_n(x)$, where $0\leq f_1\leq f_2\leq\ldots$ is an increasing sequence of unsigned simple functions, each of which are bounded with finite measure support.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&gt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\geq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)&lt;\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every $\lambda\in[0,+\infty]$, the set $\{x\in\mathbb{R}^d:f(x)\leq\lambda\}$ is Lebesgue measurable.
	</li>
	<li>
		For every interval $I\subset[0,+\infty)$, the set $f^{-1}(I)\doteq\{x\in\mathbb{R}^d:f(x)\in I\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) open set $U\subset[0,+\infty)$, the set $f^{-1}(U)\doteq\{x\in\mathbb{R}^d:f(x)\in U\}$ is Lebesgue measurable.
	</li>
	<li>
		For every (relatively) closed set $K\subset[0,+\infty)$, the set $f^{-1}(K)\doteq\{x\in\mathbb{R}^d:f(x)\in K\}$ is Lebesgue measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="eg-msr-func">Examples of measurable function</h3>
<ul id="roman-list">
	<li>
		Every continuous function $f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		Every unsigned simple function is measurable.
	</li>
	<li>
		The supremum, infimum, limit superior, or limit inferior of unsigned measurable functions is unsigned measurable.
	</li>
	<li>
		An unsigned function that is equal almost everywhere to an unsigned measurable function, is also measurable.
	</li>
	<li>
		If a sequence $f_n$ of unsigned measurable functions converges pointwise almost everywhere to an unsigned limit $f$, then $f$ is also measurable.
	</li>
	<li>
		If $f:\mathbb{R}^d\to[0,+\infty]$ is measurable and $\phi:[0,+\infty]\to[0,+\infty]$ is continuous, then $\phi\circ f:\mathbb{R}^d\to[0,+\infty]$ is measurable.
	</li>
	<li>
		If $f,g$ are unsigned measurable functions, then $f+g$ and $fg$ are measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="cmplx-msrb">Complex measurability</h3>
<p>An almost everywhere defined complex-valued function $f:\mathbb{R}^d\to\mathbb{C}$ is <strong>Lebesgue measurable</strong>, or <strong>measurable</strong>, if it is the pointwise almost everywhere limit of complex-valued simple functions.</p>

<h3 id="equiv-ntn-cmplx-msrb">Equivalent notions of complex measurability</h3>
<p>Let $f:\mathbb{R}^d\to\mathbb{C}$ be an almost everywhere defined complex-valued function. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$f$ is measurable.
	</li>
	<li>
		$f$ is the pointwise almost everywhere limit of complex-valued simple functions.
	</li>
	<li>
		The (magnitudes of the) positive and negative parts of $\text{Re}(f)$ and $\text{Im}(f)$ are unsigned measurable functions.
	</li>
	<li>
		$f^{-1}(U)$ is Lebesgue measurable for every open set $U\subset\mathbb{C}$.
	</li>
	<li>
		$f^{-1}(K)$ is Lebesgue measurable for every closed set $K\subset\mathbb{C}$.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h2 id="unsgn-lebesgue-int">Unsigned Lebesgue integrals</h2>

<h3 id="lwr-unsgn-lebesgue-int">Lower unsigned Lebesgue integral</h3>
<p>Let $f:\mathbb{R}^d\to[0,+\infty]$ be an unsigned functions (not necessarily measurable). We define the <strong>lower unsigned Lebesgue integral</strong>, denoted as $\underline{\int_{\mathbb{R}^d}}f(x)\,dx$, to be the quantity
\begin{equation}
\underline{\int_\mathbb{R}^d}f(x)\,dx\doteq\sup_{0\leq g\leq f;g\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}g(x)\,dx,
\end{equation}
where $g$ ranges over all unsigned simple functions $g:\mathbb{R}^d\to[0,+\infty]$ that are pointwise bounded by $f$.</p>

<p>We can also define the <strong>upper unsigned Lebesgue integral</strong> as
\begin{equation}
\overline{\int_\mathbb{R}^d}f(x)\,dx\doteq\inf_{h\geq f;h\text{ simple}}\text{Simp}\int_{\mathbb{R}^d}h(x)\,dx
\end{equation}</p>

<h2 id="abs-intb">Absolute integrability</h2>

<h2 id="littlewoods-prncpl">Littlewood’s three principles</h2>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>.</span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>It should be simpler to consider the case of $k=2$, in particular with two sets $E_1,E_2\subset\mathbb{R}^d$. These two sets partition $\mathbb{R}^d$ into four disjoint sets: $E_1\cap E_2,E_1\cap E_2^c,E_1^c\cap E_2,E_1^c\cap E_2^c$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-integral" /><summary type="html"><![CDATA[Note on measure theory part 3]]></summary></entry><entry><title type="html">Measure theory - II: Lebesgue measure</title><link href="http://localhost:4000/2022/07/03/measure-theory-p2.html" rel="alternate" type="text/html" title="Measure theory - II: Lebesgue measure" /><published>2022-07-03T13:00:00+07:00</published><updated>2022-07-03T13:00:00+07:00</updated><id>http://localhost:4000/2022/07/03/measure-theory-p2</id><content type="html" xml:base="http://localhost:4000/2022/07/03/measure-theory-p2.html"><![CDATA[<blockquote>
  <p>Part II of the measure theory series. Materials are mostly taken from <a href="/2022/07/03/measure-theory-p2.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/07/03/measure-theory-p2.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#lebesgue-measure">Lebesgue measure</a>
    <ul>
      <li><a href="#lebesgue-outer-measure-properties">Properties of Lebesgue outer measure</a>
        <ul>
          <li><a href="#fnt-add-spt-sets">Finite additivity for separated sets</a></li>
          <li><a href="#outer-measure-elem-sets">Outer measure of elementary sets</a></li>
          <li><a href="#fnt-add-alm-dsjnt-boxes">Finite additivity for almost disjoint boxes</a></li>
          <li><a href="#outer-msr-cntbl-uni-alm-dsjnt-boxes">Outer measure of countable unions of almost disjoint boxes</a></li>
          <li><a href="#open-sets-cntbl-uni-alm-dsjnt-boxes">Open sets as countable unions of almost disjoint boxes</a></li>
          <li><a href="#outer-msr-open-sets">Outer measure of open sets</a></li>
          <li><a href="#outer-msr-arb-sets">Outer measure of arbitrary sets - Outer regularity</a></li>
        </ul>
      </li>
      <li><a href="#lebesgue-measurability">Lebesgue measurability</a>
        <ul>
          <li><a href="#exist-lebesgue-msr-sets">Existence of Lebesgue measurable sets</a></li>
          <li><a href="#crt-msrb">Criteria for measurability</a></li>
          <li><a href="#msr-axiom">The measure axioms</a></li>
          <li><a href="#mnt-cvg-theorem-msr-sets">Monotone convergence theorem for measurable sets</a></li>
          <li><a href="#dmnt-cvg-theorem-msr-sets">Dominated convergence theorem for measurable sets</a></li>
          <li><a href="#inn-rglr">Inner regularity</a></li>
          <li><a href="#crt-fnt-msr">Criteria for finite measure</a></li>
          <li><a href="#caratheodory-crt">Carathéodory criterion, one direction</a></li>
          <li><a href="#inn-msr">Inner measure</a></li>
          <li><a href="#trans-inv">Translation invariance</a></li>
          <li><a href="#change-vars">Change of variables</a></li>
          <li><a href="#uniq-lebesgue-msr">Uniqueness of Lebesgue measure</a></li>
        </ul>
      </li>
      <li><a href="#non-measurable-sets">Non-measurable sets</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lebesgue-measure">Lebesgue measure</h2>
<p>Recall that the Jordan outer measure of a set $E\subset\mathbb{R}^d$ has been defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
From the finite additivity and subadditivity of elementary measure, we can also write the Jordan outer measure as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B_1\cup\dots\cup B_k\supset E;B_1,\dots,B_k\text{ boxes}}\vert B_1\vert+\dots+\vert B_k\vert,
\end{equation}
which means the Jordan outer measure is the infimal cost required to cover $E$ by a finite union of boxes. By replacing the finite union of boxes by a countable union of boxes, we obtain the <strong>Lebesgue outer measure</strong> $m^{*}(E)$ of $E$:
\begin{equation}
m^{*}(E)\doteq\inf_{\bigcup_{n=1}^{\infty}B_n\supset E;B_1,B_2,\dots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
which is be seen as the infimal cost required to cover $E$ by a countable union of boxes.</p>

<p>A set $E\subset\mathbb{R}^d$ is said to be <strong>Lebesgue measurable</strong> if, for every $\varepsilon&gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $E$ such that $m^{*}(U\backslash E)\leq\varepsilon$. If $E$ is Lebesgue measurable, we refer to
\begin{equation}
m(E)\doteq m^{*}(E)
\end{equation}
as the <strong>Lebesgue measure</strong> of $E$.</p>

<h3 id="lebesgue-outer-measure-properties">Properties of Lebesgue outer measure</h3>
<p><strong>Remark 1</strong>. (<strong>The outer measure axioms</strong>)</p>
<ul id="roman-list">
	<li><b>Empty set</b>. $m^*(\emptyset)=0$.</li>
	<li><b>Monotonicity</b>. If $E\subset F\subset\mathbb{R}^d$, then $m^*(E)\leq m^*(F)$.</li>
	<li><b>Countable subadditivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of sets, then $m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{\infty}m^*(E_n)$.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>This follows from the definition of Lebesgue outer measure.</li>
	<li>
		Since $E\subset F\subset\mathbb{R}^d$, then any set containing $F$ also includes $E$, but not every set having $E$ contains $F$. That means
		\begin{equation}
		\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\supset\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		Thus,
		\begin{equation}
		\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:E\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}\leq\inf\left\{\sum_{n=1}^{\infty}\vert B_n\vert:F\subset\bigcup_{n=1}^{\infty}B_n;B_n\text{ boxes}\right\}
		\end{equation}
		or
		\begin{equation}
		m^*(E)&lt; m^*(F)
		\end{equation}
	</li>
	<li>
		By the definition of Lebesgue outer measure, for any positive integer $i$, we have
		\begin{equation}
		m^*(E_i)=\inf_{\bigcup_{n=1}^{\infty}B_n\supset E_i;B_1,B_2,\ldots\text{ boxes}}\sum_{n=1}^{\infty}\vert B_n\vert
		\end{equation}
		Thus, by definition of infimum and by <span><a href="/2022/06/16/measure-theory-p1.html#countable-choice-axiom">axiom of countable choice</a></span>, for each $E_i$ in the sequence $(E_n)_{n\in\mathbb{N}}$, there exists a family of boxes $B_{i,1},B_{i,2},\ldots$ in the doubly sequence $(B_{i,j})_{(i,j)\in\mathbb{N}^2}$ covering $E_i$ such that
		\begin{equation}
		\sum_{j=1}^{\infty}\vert B_{i,j}\vert\lt m^*(E_i)+\frac{\varepsilon}{i},
		\end{equation}
		for any $\varepsilon&gt;0$, and for $i=1,2,\ldots$. Plus, we also have
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}
		\end{equation}
		Moreover, by the <span><a href="/2022/06/16/measure-theory-p1.html#tonelli-theorem">Tonelli’s theorem for series</a></span>, we have
		\begin{equation}
		\bigcup_{i=1}^{\infty}\bigcup_{j=1}^{\infty}B_{i,j}=\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}
		\end{equation}
		Therefore once again, by definition of outer measure and definition of infimum, we obtain
		\begin{align}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;=\inf_{\bigcup_{(i,j)\in\mathbb{N}^2}B_{i,j}}\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert\leq\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}\vert B_{i,j}\vert \\\\ &amp;\lt\sum_{i=1}^{\infty}m^*(E_i)+\frac{\varepsilon}{2^i}=\sum_{i=1}^{\infty}m^*(E_i)+\varepsilon
		\end{align}
		And since $\varepsilon&gt;0$ was arbitrary, we can conclude that
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{i=n}^{\infty}m^*(E_n)
		\end{equation}
	</li>
</ul>

<p><strong>Corollary 2</strong><br />
Combining empty set with countable subadditivity axiom gives us the <strong>finite subadditivity</strong> property
\begin{equation}
m^{*}\left(E_1\cup\ldots\cup E_k\right)\leq m^{*}(E_1)+\ldots+m^{*}(E_k),\hspace{1cm}\forall k\geq 0
\end{equation}</p>

<h4 id="fnt-add-spt-sets">Finite additivity for separated sets</h4>
<p><strong>Lemma 3</strong>  <br />
<em>Let $E,F\subset\mathbb{R}^d$ be such that $\text{dist}(E,F)&gt;0$, where
\begin{equation}
\text{dist}(E,F)\doteq\inf\left\{\vert x-y\vert:x\in E,y\in F\right\}
\end{equation}
is the distance between $E$ and $F$. Then $m^*(E\cup F)=m^*(E)+m^*(F)$.</em></p>

<p><strong>Proof</strong><br />
From subadditivity property, we have $m^*(E\cup F)\leq m^*(E)+m^*(F)$. Then it suffices to prove the inverse, that
\begin{equation}
m^*(E\cup F)\geq m^*(E)+m^*(F)
\end{equation}
Let $\varepsilon&gt;0$. By definition of Lebesgue outer measure, we can cover $E\cup F$ by a countable family $B_1,B_2,\ldots$ of boxes such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E\cup F)+\varepsilon
\end{equation}
Suppose it was the case that each box intersected at most one of $E$ and $F$. Then we could divide this family into two subfamilies $B_1’,B_2’,\ldots$ and $B_1'',B_2'',B_3'',\ldots$, the first of which covered $E$, while the second of which covered $F$. From definition of Lebesgue outer measure, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}
and
\begin{equation}
m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n''\vert
\end{equation}
Summing up these two equation, we obtain
\begin{equation}
m^*(E)+m^*(F)\leq\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
and thus
\begin{equation}
m^*(E)+m^*(F)\leq m^*(E\cup F)+\varepsilon
\end{equation}
Since $\varepsilon$ was arbitrary, this gives $m^*(E)+m^*(F)\leq m^*(E\cup F)$ as required.</p>

<p>Now we consider the case that some of the boxes $B_n$ intersect both $E$ and $F$.</p>

<p>Since given any $r&gt;0$, we can always partition a box $B_n$ into a finite number of smaller boxes, each of which has diameter<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> at most $r$, with the total volume of these sub-boxes equal to the volume of the original box $B_n$. Therefore, given any $r&gt;0$, we may assume without loss of generality that the boxes $B_1,B_2,\ldots$ covering $E\cup F$ have diameter at most $r$. Or in particular, we may assume that all such boxes have diameter strictly less than $\text{dist}(E,f)$.</p>

<p>Once we do this, then it is no longer possible for any box to intersect both $E$ and $F$, which allows the previous argument be applicable.</p>

<p><strong>Example 1</strong><br />
Let $E,F\subset\mathbb{R}^d$ be disjoint closed sets, with at least one of $E,F$ being compact. Then $\text{dist}(E,F)&gt;0$.</p>

<p><strong>Proof</strong></p>

<h4 id="outer-measure-elem-sets">Outer measure of elementary sets</h4>
<p><strong>Lemma 4</strong>  <br />
<em>Let $E$ be an elementary set. Then the Lebesgue outer measure of $E$ is equal to the elementary measure of $E$:</em>
\begin{equation}
m^*(E)=m(E)
\end{equation}</p>

<p><strong>Proof</strong><br />
Since
\begin{equation}
m^*(E)\leq m^{*,(J)}(E)=m(E),
\end{equation}
then it suffices to show that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
We first consider the case that $E$ is closed. Since $E$ is elementary, $E$ is also bounded, which implies that $E$ is compact.</p>

<p>Let $\varepsilon&gt;0$ be arbitrary, then we can find a countable family $B_1,B_2,\ldots$ of boxes that cover $E$
\begin{equation}
E\subset\bigcup_{n=1}^{\infty}B_n,
\end{equation}
such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We have that for each box $B_n$, we can find an open box $B_n’$ containing $B_n$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n}
\end{equation}
The $B_n’$ still cover $E$ and we have
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq\sum_{n=1}^{\infty}\left(\vert B_n\vert+\frac{\varepsilon}{2^n}\right)=\left(\sum_{n=1}^{\infty}\vert B_n\vert\right)+\varepsilon\leq m^*(E)+2\varepsilon\label{eq:lemma5.1}
\end{equation}
As the $B_n’$ are open, apply the <span><a href="/2022/06/16/measure-theory-p1.html#heine-borel-theorem"><strong>Heine-Borel theorem</strong></a>, we obtain
\begin{equation}
E\subset\bigcup_{n=1}^{N}B_n’,
\end{equation}
for some finite $N$. Thus, using the finite subadditivity property of elementary measure, combined with the result \eqref{eq:lemma5.1}, we obtain
\begin{equation}
m(E)\leq\sum_{n=1}^{N}m(B_n’)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&gt;0$ was arbitrary, we can conclude that
\begin{equation}
m(E)\leq m^*(E)
\end{equation}
Now we turn to considering the case that $E$ is not closed. Then we can write $E$ as the finite union of disjoint boxes
\begin{equation}
E=Q_1\cup\ldots\cup Q_k,
\end{equation}
which need not be closed.</span></p>

<p>Analogy to before, we have that for every $\varepsilon&gt;0$ and every $1\leq j\leq k$, we can find a closed sub-box $Q_j’$ of $Q_j$ such that
\begin{equation}
\vert Q_j’\vert\geq\vert Q_j\vert-\frac{\varepsilon}{k}
\end{equation}
Then $E$ now contains the finite union of $Q_1’\cup\ldots\cup Q_k’$ disjoint closed boxes, which is a closed elementary set. By the finite additivity property of elementary measure, the monotonicity property of Lebesgue measure, combined with the result we have proved in the first case, we have
\begin{align}
m^*(E)&amp;\geq m^*(Q_1’\cup\ldots\cup Q_k’) \\ &amp;=m(Q_1’\cup\ldots\cup Q_k’) \\ &amp;=m(Q_1’)+\ldots+m(Q_k’) \\ &amp;\geq m(Q_1)+\ldots+m(Q_k)-\varepsilon \\ &amp;= m(E)-\varepsilon,
\end{align}
for every $\varepsilon&gt;0$. And since $\varepsilon&gt;0$ was arbitrary, our claim has been proved.</p>

<p><strong>Corollary 6</strong><br />
From the lemma above and the monotonicity property, 
for every $E\in\mathbb{R}^d$, we have
\begin{equation}
m_{*,(J)}(E)\leq m^{*}(E)\leq m^{*,(J)}(E)\label{eq:cor6.1}
\end{equation}</p>

<p><strong>Corollary 7</strong><br />
Not every bounded open set or compact set (bounded closed) is Jordan measurable.</p>

<p><strong>Proof</strong><br />
Consider the countable set $\mathbf{Q}\cap[0,1]$, which we enumerate as $\{q_1,q_2,\ldots\}$. Let $\varepsilon&gt;0$ be a small number, and consider that
\begin{equation}
U\doteq\bigcup_{n=1}^{\infty}(q_n-\varepsilon/2^n,q_n+\varepsilon/2^n),
\end{equation}
which is a union of open sets and thus is open. On the other hand, by countable subadditivity property of Lebesgue outer measure, we have
\begin{align}
m^{*}(U)&amp;=m^{*}\left(\sum_{n=1}^{\infty}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right)\right) \\ &amp;\leq\sum_{n=1}^{\infty}m^{*}\left(q_n-\frac{\varepsilon}{2^n},q_n+\frac{\varepsilon}{2^n}\right) \\ &amp;=\sum_{n=1}^{\infty}\frac{2\varepsilon}{2^n}=2\varepsilon
\end{align}
As $U$ dense in $[0,1]$ (i.e.,$\overline{U}$ contains $[0,1]$), we have
\begin{equation}
m^{*}(U)=m^{*,(J)}(\overline{U})\geq m^{*,(J)}([0,1])=1
\end{equation}
Then for $\varepsilon\lt 1$, we have that
\begin{equation}
m^{*}(U)\lt 1\leq m^{*,(J)}(U)
\end{equation}
Combining with \eqref{eq:cor6.1}, we obtain that the bounded open set $U$ is not Jordan measurable.</p>

<h4 id="fnt-add-alm-dsjnt-boxes">Finite additivity for almost disjoint boxes</h4>
<p>Two boxes are <strong>almost disjoint</strong> if their interiors are disjoint, e.g., $[0,1]$ and $[1,2]$ are almost disjoint. If a box has the same elementary as its interior, we see that the finite additivity property
\begin{equation}
m(B_1\cup\ldots\cup B_n)=\vert B_1\vert+\ldots+\vert B_n\vert\label{eq:faadb.1}
\end{equation}
also holds for almost disjoint boxes $B_1,\ldots,B_n$.</p>

<h4 id="outer-msr-cntbl-uni-alm-dsjnt-boxes">Outer measure of countable unions of almost disjoint boxes</h4>
<p><strong>Lemma 8</strong><br />
<em>Let $E=\bigcup_{n=1}^{\infty}B_n$ be a countable union of almost disjoint boxes $B_1,B_2,\ldots$. Then</em>
\begin{equation}
m^*(E)=\sum_{n=1}^{\infty}\vert B_n\vert
\end{equation}
Thus, for example, $\mathbb{R}^d$ has an infinite outer measure.</p>

<p><strong>Proof</strong><br />
From countable subadditivity property of Lebesgue measure and <strong>Lemma 5</strong>, we have
\begin{equation}
m^*(E)\leq\sum_{n=1}^{\infty}m^*(B_n)=\sum_{n=1}^{\infty}\vert B_n\vert,
\end{equation}
so it suffices to show that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)
\end{equation}
Since for each integer $N$, $E$ contains the elementary set $B_1\cup\ldots\cup B_N$, then by monotonicity property and <strong>Lemma 5</strong>
\begin{align}
m^*(E)&amp;\geq m^*(B_1\cup\ldots\cup B_N)=m(B_1\cup\ldots\cup B_N)
\end{align}
And thus by \eqref{eq:faadb.1}, we have
\begin{equation}
\sum_{n=1}^{N}\vert B_n\vert\leq m^*(E)
\end{equation}
Letting $N\to\infty$ we obtain the claim.</p>

<p><strong>Corollary 9</strong><br />
If $E=\bigcup_{n=1}^{\infty}B_n=\bigcup_{n=1}^{\infty}B_n’$ can be decomposed in two different ways as the countable union of almost disjoint boxes, then
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert=\sum_{n=1}^{\infty}\vert B_n’\vert
\end{equation}</p>

<p><strong>Example 2</strong><br />
If a set $E\subset\mathbb{R}^{d}$ is expressible as the countable union of almost disjoint boxes, then
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}</p>

<p><strong>Proof</strong><br />
For $B_n$’s are disjoint boxes, we begin by express $E$ as 
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n\label{eq:eg2.1}
\end{equation}
Hence, by <strong>Lemma 8</strong>, we have
\begin{equation}
m^{*}(E)=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.2}
\end{equation}
Moreover, \eqref{eq:eg2.1} can be continued to derive as
\begin{equation}
E=\bigcup_{n=1}^{\infty}B_n=\left(\bigcup_{n=1}^{N}B_n\right)\cup\left(\bigcup_{n=N+1}^{\infty}B_n\right)=\left(\bigcup_{n=1}^{N}B_n\right)\cup B,
\end{equation}
where we have defined $B=\bigcup_{n=N+1}^{\infty}B_n$. And thus, we also have that $B_1,\ldots,B_N,B$ are almost disjoint boxes, which claims that $E$ is an elementary set. Therefore, $E$ is also Jordan measurable. Using finite additivity property of Jordan measurability yields
\begin{equation}
m_{*,(J)}(E)=m(E)=\left(\sum_{n=1}^{N}\vert B_n\vert\right)+\vert B\vert=\sum_{n=1}^{\infty}\vert B_n\vert\label{eq:eg2.3}
\end{equation}
Combining \eqref{eq:eg2.2} and \eqref{eq:eg2.3} together gives us
\begin{equation}
m^{*}(E)=m_{*,(J)}(E)
\end{equation}</p>

<h4 id="open-sets-cntbl-uni-alm-dsjnt-boxes">Open sets as countable unions of almost disjoint boxes</h4>
<p><strong>Lemma 10</strong><br />
<em>Let $E\subset\mathbb{R}^d$ be an open set. Then $E$ can be expressed as the countable union of almost disjoint boxes (and, in fact, as the countable union of almost disjoint closed cubes)</em>.</p>

<p><strong>Proof</strong><br />
We begin by defining a <strong>closed dyadic cube</strong> to be a cube $Q$ of the form
\begin{equation}
Q=\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right]\times\ldots\times\left[\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d;n\geq 0$.</p>

<p>We have that such closed dyadic cubes of a fixed sidelength $2^{-n}$ are almost disjoint and cover all of $\mathbb{R}^d$. And also, each dyadic cube of sidelength $2^{-n}$ is contained in exactly one “parent” of sidelength $2^{-n+1}$ (which, conversely, has $2^d$ “children” of sidelength $2^{-n}$), giving the dyadic cubes a structure analogous to that of a binary tree.</p>

<p>As a consequence of these facts, we also obtain the <strong>dyadic nesting property</strong>: given any two closed dyadic cubes (not necessarily same sidelength), then either they are almost disjoint, or one of them is contained in the other.</p>

<p>If $E$ is open, and $x\in E$, then by definition there is an open ball centered at $x$ that is contained in $E$. Also, it is easily seen that there is also a closed dyadic cube containing $x$ that is contained in $E$. Hence, if we let $\mathcal{Q}$ be the collection of all the dyadic cubes $Q$ that are contained in $E$, we see that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}}Q
\end{equation}
Let $\mathcal{Q}^*$ denote cubes in $\mathcal{Q}$ such that they are not contained in any other cube in $\mathcal{Q}$. From the nesting property, we see that every cube in $\mathcal{Q}$ is contained in exactly one maximal cube in $\mathcal{Q}^*$, and that any two such maximal cubes in $\mathcal{Q}^*$ are almost disjoint. Thus, we have that
\begin{equation}
E=\bigcup_{Q\in\mathcal{Q}^*}Q,
\end{equation}
which is union of almost disjoint cubes. As $\mathcal{Q}^*$ is at most countable, the claim follows.</p>

<h4 id="outer-msr-open-sets">Outer measure of open sets</h4>
<p><strong>Corollary 11</strong><br />
The Lebesgue outer measure of any open set is equal to the Jordan inner measure of that set, or of the total volume of any partitioning of that set into almost disjoint boxes.</p>

<h4 id="outer-msr-arb-sets">Outer measure of arbitrary sets - Outer regularity</h4>
<p><strong>Lemma 12</strong>.<br />
<em>Let $E\subset\mathbb{R}^d$ be an arbitrary set. Then we have</em>
\begin{equation}
m^*(E)=\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}</p>

<p><strong>Proof</strong><br />
From monotonicity property, we have
\begin{equation}
m^*(E)\leq\inf_{E\subset U,U\text{ open}}m^*(U)
\end{equation}
Then, it suffices to show that
\begin{equation}
m^*(E)\geq\inf_{E\subset U,U\text{ open}}m^*(U),
\end{equation}
which is obvious in the case that $m^*(E)$ is infinite. Thus, we now assume that $m^*(E)$ is finite.</p>

<p>Let $\varepsilon&gt;0$. By the definition of Lebesgue outer measure, there exists a countable family $B_1,B_2,\ldots$ of boxes covering $E$ such that
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n\vert\leq m^*(E)+\varepsilon
\end{equation}
We can enlarge each of these boxes $B_n$ to an open box $B_n’$ such that
\begin{equation}
\vert B_n’\vert\leq\vert B_n\vert+\frac{\varepsilon}{2^n},
\end{equation}
for any $\varepsilon&gt;0$. Then the set $\bigcup_{n=1}^{\infty}B_n’$, being a union of open sets, is itself open, and contains $E$, and
\begin{equation}
\sum_{n=1}^{\infty}\vert B_n’\vert\leq m^*(E)+\varepsilon+\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=m^*(E)+2\varepsilon
\end{equation}
By countable subadditivity property, it implies that
\begin{equation}
m^*\left(\bigcup_{n=1}^{\infty}B_n’\right)\leq m^*(E)+2\varepsilon
\end{equation}
and thus
\begin{equation}
\inf_{E\subset U,U\text{ open}}m^*(U)\leq m^*(E)+2\varepsilon
\end{equation}
And since $\varepsilon&gt;0$ was arbitrary, the claim follows.</p>

<h3 id="lebesgue-measurability">Lebesgue measurability</h3>

<h4 id="exist-lebesgue-msr-sets">Existence of Lebesgue measurable sets</h4>
<p><strong>Lemma 13</strong>.</p>
<ul id="roman-list" style="font-style: italic;">
	<li>Every open set is Lebesgue measurable.</li>
	<li>Every closed set is Lebesgue measurable.</li>
	<li>Every set of Lebesgue outer measure zero is measurable. (Such sets are called <b>null sets</b>.)</li>
	<li>The empty set $\emptyset$ is Lebesgue measurable.</li>
	<li>If $E\subset\mathbb{R}^d$ is Lebesgue measurable, then so its complement $\mathbb{R}^d\backslash E$.</li>
	<li>If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the union $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.</li>
	<li>If $E_1,E_2,\ldots\subset\mathbb{R}^d$ are a sequence of Lebesgue measurable sets, then the intersection $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>This follows from definition.</li>
	<li>
		We have that every closed set is a the countable union of closed and bounded set, so by (vi), if suffices to verify the claim when $E$ is bounded and closed.<br />
		Let $U\supset E$ be an open set, we thus have that $U\backslash E$ is also open due to the compactness of $E$. By <b>lemma 10</b>, we can represent the open set $U\backslash E$ as a countable union of almost disjoint boxes as
		\begin{equation}
		U\backslash E=\bigcup_{n=1}^{\infty}B_n
		\end{equation}
		The problem remains to prove that for any $\varepsilon&gt;0$
		\begin{equation}
		\sum_{n=1}^{\infty}\vert B_n\vert&lt;\varepsilon
		\end{equation}
	</li>
	<li>This follows from definition.</li>
	<li>This follows from definition.</li>
	<li>
		Given $E$ is Lebesgue measurable, for each positive integer $n$, we can find an open set $U_n$ containing $E$ such that
		\begin{equation}
		m^*(U_n\backslash E)\leq\frac{1}{n}
		\end{equation}
		Let $F_n=U_n^c=\mathbb{R}^d\backslash U_n$. Thus, we have $F_n\subset\mathbb{R}^d\backslash E$ and
		\begin{equation}
		m^*\big((\mathbb{R}^d\backslash E)\backslash F_n\big)=m^*\big((\mathbb{R}^d\backslash E)\backslash(\mathbb{R}^d\backslash U_n)\big)=m^*(U_n\backslash E)\leq\frac{1}{n}\label{eq:lemma13.1}
		\end{equation}
		In addition, since $F_n\subset\mathbb{R}^d\backslash E$, the countable union of them, denoted as $F$, is also a subset of $\mathbb{R}^d\backslash E$
		\begin{equation}
		F=\bigcup_{n=1}^{\infty}F_n\subset\mathbb{R}^d\backslash E
		\end{equation}
		Moreover, from \eqref{eq:lemma13.2}, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{N}F_n\right)=m^*\left(\bigcap_{n=1}^{N}(\mathbb{R}^d\backslash E)\backslash F_n\right)\leq\frac{1}{N}
		\end{equation}
		Let $N$ approaches $\infty$, we have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=m^*\left((\mathbb{R}^d\backslash E)\backslash\bigcup_{n=1}^{\infty}F_n\right)\leq 0
		\end{equation}
		By non-negativity property, we then have
		\begin{equation}
		m^*\left((\mathbb{R}^d\backslash E)\backslash F\right)=0,
		\end{equation}
		Hence, $\mathbb{R}^d\backslash E$ is a union of $F$ with a set of Lebesgue outer measure of zero. The set $F$, in the other hand, is a countable union of closed set $F_n$'s (since each $U_n$ is an open set). Therefore, by (ii), (iii) and (vi), we have that $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.
	</li>
	<li>
		For each Lebesgue measurable set $E_n$, for any $\varepsilon&gt;0$ and for $U_n$ is an open set containing $E_n$ we have 
		\begin{equation}
		m^{*}(U_n\backslash E_n)\leq\frac{\varepsilon}{2^n}\label{eq:lemma13.2}
		\end{equation}
		Moreover, since $E_n\subset U_n$, then
		\begin{equation}
		\bigcup_{n=1}^{\infty}E_n\subset\bigcup_{n=1}^{\infty}U_n,
		\end{equation}
		which is also an open set. Therefore, from \eqref{eq:lemma13.2} and by countable subadditivity, we have
		\begin{equation}
		m^*\left(\left(\bigcup_{n=1}^{\infty}U_n\right)\backslash\left(\bigcup_{n=1}^{\infty}E_n\right)\right)\leq\sum_{n=1}^{\infty}m^*(U_n\backslash E_n)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon,
		\end{equation}
		which proves that $\bigcup_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	</li>
	<li>
		Given $E_1,E_2,E_3,\ldots\subset\mathbb{R}^d$ are Lebesgue measurable, by (v), the complement of them,
		\begin{equation}
		E_1^c,E_2^c,E_3^c,\ldots\subset\mathbb{R}^d,
		\end{equation}
		are also Lebesgue measurable. By <b>De Morgan's laws</b>, we have
		\begin{equation}
		\left(\bigcap_{n=1}^{\infty}E_n\right)^c=\bigcup_{n=1}^{\infty}E_n^c,
		\end{equation}
		which is Lebesgue measurable by (vi). Thus, $\left(\bigcap_{n=1}^{\infty}E_n\right)^c$ is also Lebesgue measurable. This means, using (v) once again, we obtain that $\bigcap_{n=1}^{\infty}E_n$ is Lebesgue measurable.
	</li>
</ul>

<h4 id="crt-msrb">Criteria for measurability</h4>
<p>Let $E\subset\mathbb{R}^d$, then the following are equivalent:</p>
<ul id="roman-list">
	<li>$E$ is Lebesgue measurable.</li>
	<li><b>Outer approximation by open</b>. For every $\varepsilon&gt;0$, $E$ can be contained in an open set $U$ with $m^*(U\backslash E)\leq\varepsilon$.</li>
	<li><b>Almost open</b>. For every $\varepsilon&gt;0$, we can find an open set $U$ such that $m^*(U\Delta E)\leq\varepsilon$. ($E$ differs from an open set by a set of outer measure at most $\varepsilon$.)</li>
	<li><b>Inner approximation by closed</b>. For every $\varepsilon&gt;0$, we can find a closed set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.</li>
	<li><b>Almost closed</b>. For every $\varepsilon&gt;0$, we can find a closed set $F$ such that $m^*(F\Delta E)\leq\varepsilon$. ($E$ differs from a closed set by a set of outer measure at most $\varepsilon$.)</li>
	<li><b>Almost measurable</b>. For every $\varepsilon&gt;0$, we can find a Lebesgue measurable set $E_\varepsilon$ such that $m^*(E_\varepsilon\Delta E)\leq\varepsilon$. ($E$ differs from a measurable set by a set of outer measure at most $\varepsilon$.)</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		This follows from definition
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
		Given $E$ is Lebesgue measurable, for any $\varepsilon&gt;0$, we can find an open set $U$ containing $E$ such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		And since $E\subset U$, we have that
		\begin{equation}
		m^(E\backslash U)=m^*(\emptyset)=0,
		\end{equation}
		which implies that for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(U\Delta E)=m^*(U\backslash E)+m^*(E\backslash U)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (iv)<br />
		By the claim (v) in <b>lemma 13</b>, given Lebesgue measurable set $E\subset\mathbb{R}^d$, we have that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable. Therefore, there exists an open set $U$ containing $\mathbb{R}^d\backslash E$ such that for any $\varepsilon&gt;0$ we have
		\begin{equation}
		m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.1}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}\backslash U$, thus $F$ is a closed set contained in $E$. Moreover, from \eqref{eq:cm.1} we also have for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(E\backslash F)=m^*\left(E\backslash(\mathbb{R}^d\backslash U)\right)=m^*\left(U\backslash(\mathbb{R}^d\backslash E)\right)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (v)<br />
		Given Lebesgue measurable set $E\subset\mathbb{R}^d$, using the claim (v) in <b>lemma 13</b> gives us that its complement $\mathbb{R}^d\backslash E$ is also Lebesgue measurable.<br />
		From claim (iii), for any $\varepsilon&gt;0$, we can find an open set $U$ such that
		\begin{equation}
		m^*\left(U\Delta(\mathbb{R}^d\backslash E)\right)\leq\varepsilon\label{eq:cm.2}
		\end{equation}
		Let $F$ denote the complement of $U$, $F=\mathbb{R}^d\backslash$. We then have that $F$ is a closed set. In addition, $U\Delta(\mathbb{R}^d\backslash E)$ can be rewritten by
		\begin{align}
		U\Delta(\mathbb{R}^d\backslash E)&amp;=\left(U\backslash(\mathbb{R}^d\backslash E)\right)\cup\left((\mathbb{R}^d\backslash E)\backslash U\right) \\ &amp;=\left(E\backslash(\mathbb{R}^d\backslash U)\right)\cup\left((\mathbb{R}^d\backslash U)\backslash E\right) \\ &amp;=(\mathbb{R}^d\backslash U)\backslash E \\ &amp;=F\Delta E,
		\end{align}
		which lets \eqref{eq:cm.2} can be written as, for any $\varepsilon&gt;0$
		\begin{equation}
		m^*(F\Delta E)\leq\varepsilon
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (vi)<br />
		Given $E$ is Lebesgue measurable, by claim (v), for any $\varepsilon&gt;0$ we can find a closed set $E_\varepsilon$ such that
		\begin{equation}
		m^*(E_\varepsilon\Delta E)\leq\varepsilon
		\end{equation}
		While by property (ii) of <b>lemma 13</b>, we have that $E_\varepsilon$ is Lebesgue measurable, which proves our claim.
	</li>
	<li>
		(vi) $\Rightarrow$ (i)<br />
		Given (vi), for any $\varepsilon&gt;0$, we can find a Lebesgue measurable set $E_\varepsilon^{(n)}$ such that
		\begin{equation}
		m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\frac{\varepsilon}{2^n}
		\end{equation}
		Therefore, by countable subadditivity property of Lebesgue outer measurability
		\begin{equation}
		m^*\left(\bigcup_{n=1}^{\infty}E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}m^*\left(E_\varepsilon^{(n)}\Delta E\right)\leq\sum_{n=1}^{\infty}\frac{\varepsilon}{2^n}=\varepsilon
		\end{equation}
	</li>
</ul>

<p><strong>Remark 14</strong>  <br />
Every Jordan measurable set is Lebesgue measurable.</p>

<p><strong>Proof</strong><br />
This follows directly from <strong>corollary 6</strong>.</p>

<p><strong>Remark 15</strong>  <br />
The <a href="/2022/06/16/measure-theory-p1.html#cantor-set"><strong>Cantor set</strong></a> is compact, uncountable, and a null set.</p>

<p><strong>Proof</strong></p>
<ul>
  <li>Since $\mathcal{C}\subseteq[0,1]$ is closed and bounded, by the <a href="/2022/06/16/measure-theory-p1.html#heine-borel-theorem">Heine-Borel theorem</a>, $\mathcal{C}$ is then compact.</li>
  <li></li>
</ul>

<h4 id="msr-axiom">The measure axioms</h4>
<p><strong>Lemma 16</strong></p>
<ul id="roman-list" style="font-style: italic;">
	<li><b>Empty set</b>. $m(\emptyset)=0$.</li>
	<li><b>Countable additivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then</li>
	\begin{equation}
	m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
	\end{equation}
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		<b>Empty set</b><br />
		We have that empty set $\emptyset$ is Lebesgue measurable since for every $\varepsilon&gt;0$, there exists an open set $U\subset\mathbb{R}^d$ containing $\emptyset$ such that $m^*(U\backslash\emptyset)\leq\varepsilon$. Thus,
		\begin{equation}
		m(\emptyset)=m^*(\emptyset)=0
		\end{equation}
	</li>
	<li>
		<b>Countable additivity</b><br />
		We begin by considering the case that $E_n$ are all compact sets.
		<br />
		By repeated use of <b>Lemma 12</b> and <b>Example ?</b>, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Thus, using monotonicity property, we have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Let $N\to\infty$, we obtain
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\geq\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
		On the other hand, by countable subadditivity, we also have
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)\leq\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Therefore, we can conclude that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{N}m(E_n)
		\end{equation}
		Next, we consider the case that $E_n$ are bounded but not necessarily compact.
		<br />
		Let $\varepsilon&gt;0$. By criteria for measurability, we know that each $E_n$ is the union of a compact set $K_n$ and a set of outer measure at most $\varepsilon/2^n$. Thus
		\begin{equation}
		m(E_n)\leq m(K_n)+\frac{\varepsilon}{2^n}
		\end{equation}
		And hence
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq\left(\sum_{n=1}^{\infty}m(K_n)\right)+\varepsilon
		\end{equation}
		From the first case, we know that
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)=\sum_{n=1}^{\infty}m(K_n)
		\end{equation}
		while from monotonicity property of Lebesgue measure
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}K_n\right)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Putting these results together we obtain
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)+\varepsilon,
		\end{equation}
		for every $\varepsilon&gt;0$. And since $\varepsilon$ was arbitrary, we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\leq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		while from countable subadditivity property we have
		\begin{equation}
		\sum_{n=1}^{\infty}m(E_n)\geq m\left(\bigcup_{n=1}^{\infty}E_n\right)
		\end{equation}
		Therefore, the claim follows.
		<br />
		Finally, we consider the case that $E_n$ are not bounded or closed with the idea of decomposing each $E_n$ as a countable disjoint union of bounded Lebesgue measurable sets.
		<br />
	</li>
</ul>

<p><strong>Remark 17</strong><br />
The countable additivity also implies the <strong>finite additivity</strong> property of Lebesgue  measure
\begin{equation}
m\left(\bigcup_{n=1}^{N}E_n\right)=\sum_{n=1}^{N}m(E_n),
\end{equation}
where $E_1,\ldots,E_N$ are Lebesgue measurable.</p>

<h4 id="mnt-cvg-theorem-msr-sets">Monotone convergence theorem for measurable sets</h4>
<ul id="roman-list">
	<li>
		<b>Upward monotone convergence</b>. Let $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ be a countable non-decreasing sequence of Lebesgue measurable sets. Then
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		<b>Downward monotone convergence</b>. Let $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ be a countable non-increasing sequence of Lebesgue measurable sets. If at least one of the $m(E_n)$ is finite, then
		\begin{equation}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)=\lim_{n\to\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		The hypothesis that at least one of the $m(E_n)$ is finite in the downward monotone convergence theorem cannot be dropped.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		<b>Upward monotone convergence</b><br />
		Since $E_1\subset E_2\subset\ldots\subset\mathbb{R}^d$ is a countable non-decreasing sequence of Lebesgue measurable sets, by countable additivity, we have
		\begin{align}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)&amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash\bigcup_{n'=1}^{n-1}E_{n'}\right) \\ &amp;=m\left(\bigcup_{n=1}^{\infty}E_n\backslash E_{n-1}\right) \\ &amp;=\left(\sum_{n=2}^{\infty}m(E_n)-m(E_{n-1})\right)+m(E_1) \\ &amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	</li>
	<li>
		<b>Downward monotone convergence</b><br />
		Since $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ is a countable non-increasing sequence of Lebesgue measurable sets, the sequence of their complement $E_1^c\subset E_2^c\subset\ldots\subset\mathbb{R}^d$ is therefore a countable non-decreasing sequence of Lebesgue measurable sets. Using the claim (i) and by De Morgan's laws, we have
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m(\mathbb{R}^d)-m\left(\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m(\mathbb{R}^d)-\lim_{n\to\infty}m(E_n^c) \\ &amp;=m(\mathbb{R}^d)-m(\mathbb{R}^d)+\lim_{n\to\infty}m(E_n) \\ &amp;=\lim_{n\to\infty}m(E_n)
		\end{align}
	</li>
	<li>
		Consider sequence $\mathbb{R}^d\supset E_1\supset E_2\supset\ldots$ of non-increasing Lebesgue measurable sets where each $E_n$ is given by
		\begin{equation}
		E_n\doteq[n,+\infty)
		\end{equation}
		Therefore, by De Morgan's laws, the Lebesgue measure of their countable intersection is
		\begin{align}
		m\left(\bigcap_{n=1}^{\infty}E_n\right)&amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}E_n^c\right) \\ &amp;=m\left(\mathbb{R}^d\backslash\bigcup_{n=1}^{\infty}(-\infty,n)\right) \\ &amp;=m(\mathbb{R}^d\backslash\mathbb{R}^d) \\ &amp;=m(\emptyset)=0,
		\end{align}
		while for every $n$, we have
		\begin{equation}
		m(E_n)=m\left([n,+\infty)\right)=\infty
		\end{equation}
	</li>
</ul>

<h4 id="dmnt-cvg-theorem-msr-sets">Dominated convergence theorem for measurable sets</h4>
<p>We say that a sequence $E_n$ of sets in $\mathbb{R}^d$ <strong>converges pointwise</strong> to another set $E$ in $\mathbb{R}^d$ if the indicator function $1_{E_n}$ converges pointwise to $1_E$.</p>
<ul id="roman-list">
	<li>
		If the $E_n$ are all Lebesgue measurable, and converge pointwise to $E$, then $E$ is Lebesgue measurable also.
	</li>
	<li>
		<b>Dominated convergence theorem</b>. Suppose that the $E_n$ are all contained in another Lebesgue measurable set $F$ of finite measure. Then $m(E_n)$ converges to $m(E)$.
	</li>
	<li>
		The dominated convergence theorem fails if the $E_n$'s are not contained in a set of finite measure, even if we assume that the $m(E_n)$ are all uniformly bounded.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		We have
	</li>
</ul>

<p><strong>Remark 18</strong><br />
Let $E\subset\mathbb{R}^d$, then $E$ is contained in a Lebesgue measurable set of measure exactly equal to $m^*(E)$.</p>

<p><strong>Proof</strong></p>

<h4 id="inn-rglr">Inner regularity</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable. Then
\begin{equation}
m(E)=\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}</p>

<p><strong>Proof</strong><br />
By monotonic we have that
\begin{equation}
m(E)\geq\sup_{K\subset E,K\text{ compact}}m(K),
\end{equation}
thus it suffices to show that
\begin{equation}
m(E)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Consider the case that $E$ is bounded. By the <strong>criteria for Lebesgue measurability</strong>, we have that for any $\varepsilon&gt;0$, there exist a bounded and closed, and thus compact by the Heine-Borel theorem, set $K’$ contained in $E$ such that
\begin{equation}
m(E\backslash K’)\leq\varepsilon
\end{equation}
Moreover, by claim (ii) of <strong>lemma 13</strong>, we have that $K’$ is Lebesgue measurable. Using finite additivity property of Lebesgue measure gives us
\begin{equation}
\varepsilon\geq m(E\backslash K’)=m(E)-m(K’),
\end{equation}
which means
\begin{equation}
m(E)\leq m(K’)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Now consider {the case that $E$ is an unbounded set. Let $(K_r)_{r=1,2,\ldots}$ be the sequence sets in which each $K_r$ is defined as
\begin{equation}
K_r\doteq E\cap B(\mathbf{0},r),\label{eq:ir.1}
\end{equation}
where $B(\mathbf{0},r)$ is a closed ball centered at $\mathbf{0}\in\mathbb{R}^d$ with radius $r$
\begin{equation}
B(\mathbf{0},r)=\{\mathbf{x}:\vert\mathbf{x}\vert\leq r\}
\end{equation}
which means $K_1\subset K_2\subset\ldots\subset E$ is an increasing sequence of compact set (since \eqref{eq:ir.1} also implies that $K_r\subset B(\mathbf{0},r)$, and hence bounded and closed, then using the Heine-Borel theorem to obtain the compactness of $K_r$). By the <strong>monotone convergence theorem</strong>, we have
\begin{equation}
m\left(\bigcup_{r=1}^{\infty}K_r\right)=\lim_{r\to\infty}m(K_r)
\end{equation}
On the other hand, the countable union of $K_r$ can be written as
\begin{equation}
\bigcup_{r=1}^{\infty}K_r=\bigcup_{r=1}^{\infty}E\cap B(\mathbf{0},r)=E\cap\bigcup_{r=1}^{\infty}B(\mathbf{0},r)=E\cap\mathbb{R}^d=E,
\end{equation}
which therefore gives us
\begin{equation}
m(E)=\lim_{r\to\infty}m(K_r)\label{eq:ir.2}
\end{equation}
Moreover, by monotonicity property $m(E)\geq m(K_r),\forall r$. Hence, \eqref{eq:ir.2} implies that for any $\varepsilon&gt;0$, there exists $r’$ such that for all $r\geq r’$
\begin{equation}
\varepsilon&gt;\vert m(E)-m(K_{r’})\vert=m(E)-m(K_{r’})
\end{equation}
This means that
\begin{equation}
m(A)\leq\sup_{K\subset E,K\text{ compact}}m(K)
\end{equation}
Our claim then follows.</p>

<h4 id="crt-fnt-msr">Criteria for finite measure</h4>
<p>Let $E\subset\mathbb{R}^d$, then the following are equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable with finite measure.
	</li>
	<li>
		<b>Outer approximation by open</b>. For every $\varepsilon&gt;0$, we can contain $E$ in an open set $U$ of finite measure with $m^*(U\backslash E)\leq\varepsilon$.
	</li>
	<li>
		<b>Almost open bounded</b>. For every $\varepsilon&gt;0$, there exists a bounded open set $U$ such that $m^*(E\Delta U)\leq\varepsilon$. (In other words, $E$ differs from a bounded set by a set of arbitrarily small Lebesgue outer measure.)
	</li>
	<li>
		<b>Inner approximation by compact</b>. For every $\varepsilon&gt;0$, we can find a compact set $F$ contained in $E$ with $m^*(E\backslash F)\leq\varepsilon$.
	</li>
	<li>
		<b>Almost compact</b>. $E$ differs from a compact set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost bounded measurable</b>. $E$ differs from a bounded Lebesgue measurable set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost finite measure</b>. $E$ differs from a Lebesgue measurable set with finite measure by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost elementary</b>. $E$ differs from an elementary set by a set of arbitrarily small Lebesgue outer measure.
	</li>
	<li>
		<b>Almost dyadically elementary</b>. For every $\varepsilon&gt;0$, there exists an integer $n$ and a finite union $F$ of closed dyadic cubes of sidelength $2^{-n}$ such that $m^*(E\Delta F)\leq\varepsilon$.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		Given $E$ is Lebesgue measurable with finite measure, by definition, for any $\varepsilon&gt;0$, there exists an open set $U$ o such that
		\begin{equation}
		m^*(U\backslash E)\leq\varepsilon
		\end{equation}
		Then, by finite subadditivity property of Lebesgue outer measure
		\begin{equation}
		m^*(U)\leq m^*(E)+\varepsilon,
		\end{equation}
		which implies that $m^*(U)$ finite due to finiteness of $m^*(E)$ and $\varepsilon$, and hence $U$ has finite measure since $m(U)\leq m^*(U)$.
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
	</li>
</ul>

<h4 id="caratheodory-crt">Carathéodory criterion, one direction</h4>
<p>Let $E\subset\mathbb{R}^d$, the following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable.
	</li>
	<li>
		For every elementary set $A$
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	</li>
	<li>
		For every box $B$, we have
		\begin{equation}
		\vert B\vert=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		(i) $\Rightarrow$ (ii)<br />
		We begin with an observation that, by finite additivity property of Lebesgue measure
		\begin{equation}
		m(A)=m(A\cap E)+m(A\backslash E)\leq m^*(A\cap E)+m^*(A\backslash E)\label{eq:cc.1}
		\end{equation}
		Given $A$ is elementary, by <span><a href="/2022/06/16/measure-theory-p1.html#measure-elementary-set"><b>lemma 10</b></a></span>, we can express $A$ as a finite union of disjoint boxes
		\begin{equation}
		A=\bigcup_{n=1}^{N}B_n
		\end{equation}
		Continuing using finite subadditivity of Lebesgue outer measure and finite additivity of Lebesgue measure, \eqref{eq:cc.1} then can be continued to derive as
		\begin{align}
		m(A)&amp;\leq m^*(A\cap E)+m^*(A\backslash E) \\ &amp;=m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\cap E\right)+m^*\left(\left(\bigcup_{n=1}^{N}B_n\right)\backslash E\right) \\ &amp;=m^*\left(\bigcup_{n=1}^{N}B_n\cap E\right)+m^*\left(\bigcup_{n=1}^{N}B_n\backslash E\right) \\ &amp;\leq\sum_{n=1}^{N}m^*(B_n\cap E)+m^*(B_n\backslash E) \\ &amp;=\sum_{n=1}^{N}m^*(B_n)=\sum_{n=1}^{N}m(B_n)=m\left(\bigcup_{n=1}^{N}B_n\right)=m(A),
		\end{align}
		which implies that
		\begin{equation}
		m(A)=m^*(A\cap E)+m^*(A\backslash E)
		\end{equation}
	</li>
	<li>
		(i) $\Rightarrow$ (iii)<br />
		Since every box $B$ is Lebesgue measurable, then given $E$ is also Lebesgue measurable, by <b>lemma 13</b>, their difference and intersection are also Lebesgue measurable, which means by additivity property of Lebesgue measure we have
		\begin{equation}
		\vert B\vert=m(B)=m(B\cap E)+m(B\backslash E)=m^*(B\cap E)+m^*(B\backslash E)
		\end{equation}
	</li>
	<li>
		(ii) $\Rightarrow$ (i)<br />
	</li>
</ul>

<h4 id="inn-msr">Inner measure</h4>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set. The <strong>Lebesgue inner measure</strong> $m_*(E)$ of $E$ is defined by
\begin{equation}
m_*(E)\doteq m(A)-m^*(A\backslash E),
\end{equation}
for any elementary set $A$ containing $E$. Then</p>
<ul id="roman-list">
	<li>
		If $A,A'$ are two elementary sets containing $E$, then
		\begin{equation}
		m(A)-m^*(A\backslash E)=m(A')-m^*(A'\backslash E)
		\end{equation}
	</li>
	<li>
		We have that $m_*(E)\leq m^*(E)$, and that equality holds iff $E$ is Lebesgue measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>

<p><strong>Example 3</strong><br />
Let $E\subset \mathbb{R}^d$, and define a $G_\delta$ <em>set</em> to be a countable intersection $\bigcap_{n=1}^{\infty}U_n$ of open sets, and define an $F_\delta$ <em>set</em> to be a countable union $\bigcup_{n=1}^{\infty}F_n$ of closed sets. The following are then equivalent:</p>
<ul id="roman-list">
	<li>
		$E$ is Lebesgue measurable.
	</li>
	<li>
		$E$ is a $G_\delta$ set with a null set removed.
	</li>
	<li>
		$E$ is the union of an $F_\delta$ set and a null set.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h4 id="trans-inv">Translation invariance</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, then $E+x$ is also Lebesgue measurable for any $x\in\mathbb{R}^d$, and $m(E+x)=m(E)$.</p>

<p><strong>Proof</strong></p>

<h4 id="change-vars">Change of variables</h4>
<p>Let $E\subset\mathbb{R}^d$ be Lebesgue measurable, and $T:\mathbb{R}^d\to\mathbb{R}^d$ be a linear transformation, then $T(E)$ is Lebesgue measurable, and $m(T(E))=\vert\text{det}(T)\vert m(E)$.</p>

<p><strong>Note</strong><br />
If $T:\mathbb{R}^d\to\mathbb{R}^{d’}$ is a linear map to a space $\mathbb{R}^{d’}$ of strictly smaller dimension than $\mathbb{R}^d$, then $T(E)$ need not be Lebesgue measurable.</p>

<p><strong>Proof</strong></p>

<p><strong>Remark 19</strong><br />
Let $d,d’\geq 1$ be natural numbers</p>
<ul id="roman-list">
	<li>
		If $E\subset\mathbb{R}^d$ and $F\subset\mathbb{R}^{d'}$, then
		\begin{equation}
		(m^{d+d'})^*(E\times F)\leq(m^d)^*(E)(m^{d'})^*(F)
		\end{equation}
	</li>
	<li>
		Let $E\subset\mathbb{R}^d,F\subset\mathbb{R}^{d'}$ be Lebesgue measurable sets. Then $E\times F\subset\mathbb{R}^{d+d'}$ is Lebesgue measurable, with \begin{equation}
		m^{d+d'}(E\times F)=m^d(E).m^{d'}(F)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>

<h4 id="uniq-lebesgue-msr">Uniqueness of Lebesgue measure</h4>
<p>Lebesgue measure $E\mapsto m(E)$ is the only map from Lebesgue measurable sets to $[0,+\infty]$ that obeys the following axioms:</p>
<ul id="roman-list">
	<li>
		<b>Empty set</b>. $m(\emptyset)=0$.
	</li>
	<li>
		<b>Countable additivity</b>. If $E_1,E_2,\ldots\subset\mathbb{R}^d$ is a countable sequence of disjoint Lebesgue measurable sets, then 
		\begin{equation}
		m\left(\bigcup_{n=1}^{\infty}E_n\right)=\sum_{n=1}^{\infty}m(E_n)
		\end{equation}
	</li>
	<li>
		<b>Translation invariance</b>. If $E$ is Lebesgue measurable and $x\in\mathbb{R}^d$, then $m(E+x)=m(E)$.
	</li>
	<li>
		<b>Normalisation</b>. $m([0,1]^d)=1$.
	</li>
</ul>

<p><strong>Proof</strong></p>

<h3 id="non-measurable-sets">Non-measurable sets</h3>
<p><strong>Remark 20</strong><br />
There exists a subset $E\subset[0,1]$ which is not Lebesgue measurable.</p>

<p><strong>Remark 21</strong> (Outer measure is not finitely additive)<br />
There exists disjoint bounded subsets $E,F\subset\mathbb{R}$ such that
\begin{equation}
m^*(E\cap F)\neq m^*(E)+m^*(F)
\end{equation}</p>

<p><strong>Remark 22</strong><br />
Let $\pi:\mathbb{R}^2\to\mathbb{R}$ be the coordinate projection $\pi(x,y)\doteq x$. Then there exists a measurable $E\subset\mathbb{R}^2$ such that $\pi(E)$ is not measurable.</p>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="#http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. </span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The <strong>diameter</strong> of a set $B$ is defined as
\begin{equation*}
\text{dia}(B)\doteq\sup\{\vert x-y\vert:x,y\in B\}
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="lebesgue-measure" /><summary type="html"><![CDATA[Note on measure theory part 2]]></summary></entry><entry><title type="html">Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral</title><link href="http://localhost:4000/2022/06/16/measure-theory-p1.html" rel="alternate" type="text/html" title="Measure theory - I: Elementary measure, Jordan measure &amp;amp; the Riemann integral" /><published>2022-06-16T13:00:00+07:00</published><updated>2022-06-16T13:00:00+07:00</updated><id>http://localhost:4000/2022/06/16/measure-theory-p1</id><content type="html" xml:base="http://localhost:4000/2022/06/16/measure-theory-p1.html"><![CDATA[<blockquote>
  <p>Part I of the measure theory series. Materials are mostly taken from <a href="/2022/06/16/measure-theory-p1.html#taos-book">Tao’s book</a>, except for some needed notations extracted from <a href="/2022/06/16/measure-theory-p1.html#steins-book">Stein’s book</a>.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#pts-sets">Points, sets</a></li>
      <li><a href="#open-closed-compact-sets">Open, closed, compact sets</a></li>
      <li><a href="#rects-cubes">Rectangles, cubes</a></li>
      <li><a href="#cantor-set">The Cantor set</a></li>
      <li><a href="#others">Others</a></li>
    </ul>
  </li>
  <li><a href="#elementary-measure">Elementary measure</a>
    <ul>
      <li><a href="#intervals-boxes-elementary-sets">Intervals, boxes, elementary sets</a></li>
      <li><a href="#measure-elementary-set">Measure of an elementary set</a></li>
      <li><a href="#elementary-measure-properties">Properties of elementary measure</a></li>
      <li><a href="#uniqueness-elementary-measure">Uniqueness of elementary measure</a></li>
    </ul>
  </li>
  <li><a href="#jordan-measure">Jordan measure</a>
    <ul>
      <li><a href="#jordan-measurability-characterisation">Characterisation of Jordan measurability</a></li>
      <li><a href="#jordan-measurability-properties">Properties of Jordan measurability</a></li>
      <li><a href="#jordan-null-sets">Jordan null sets</a></li>
      <li><a href="#metric-formula-jordan-measurability">Metric entropy formulation of Jordan measurability</a></li>
      <li><a href="#uniqueness-jordan-measure">Uniqueness of Jordan measure</a></li>
      <li><a href="#topo-jordan-measurability">Topological of Jordan measurability</a></li>
      <li><a href="#caratheodory-type-property">Carathéodory type property</a></li>
    </ul>
  </li>
  <li><a href="#connect-riemann-int">Connection with the Riemann integral</a>
    <ul>
      <li><a href="#riemann-integrability">Riemann integrability</a></li>
      <li><a href="#pc-func">Piecewise constant functions</a>
        <ul>
          <li><a href="#pc-int-properties">Basic properties of piecewise constant integral</a></li>
        </ul>
      </li>
      <li><a href="#darboux-int">Darboux integral</a>
        <ul>
          <li><a href="#equiv-riemann-darboux-int">Equivalence of Riemann integral and Darboux integral</a></li>
        </ul>
      </li>
      <li><a href="#riemann-int-properties">Basic properties of the Riemann integral</a></li>
      <li><a href="#riemann-int-area-interpret">Area interpretation of the Riemann integral</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="pts-sets">Points, sets</h3>
<p>A <strong>point</strong> $x\in\mathbb{R}^d$ consists of a $d$-tuple of real numbers
\begin{equation}
x=\left(x_1,x_2,\dots,x_d\right),\hspace{1cm}x_i\in\mathbb{R}, i=1,\dots,d
\end{equation}
Addition between points and multiplication of a point by a real scalar is elementwise.</p>

<p>The <strong>norm</strong> of $x$ is denoted by $\vert x\vert$ and is defined to be the standard <strong>Euclidean norm</strong> given by
\begin{equation}
\vert x\vert=\left(x_1^2+\dots+x_d^2\right)^{1/2}
\end{equation}
We can then calculate the <strong>distance</strong> between two points $x$ and $y$, which is
\begin{equation}
\text{dist}(x,y)=\vert x-y\vert
\end{equation}
The <strong>complement</strong> of a set $E$ in $\mathbb{R}^d$ is denoted as $E^c$, and defined by
\begin{equation}
E^c=\{x\in\mathbb{R}^d:x\notin E\}
\end{equation}
If $E$ and $F$ are two subsets of $\mathbb{R}^d$, we denote the complement of $F$ in $E$ by
\begin{equation}
E-F=\{x\in\mathbb{R}^d:x\in E;\,x\notin F\}
\end{equation}
The <strong>distance</strong> between two sets $E$ and $F$ is defined by
\begin{equation}
\text{dist}(E,F)=\inf_{x\in E,\,y\in F}\vert x-y\vert
\end{equation}</p>

<h3 id="open-closed-compact-sets">Open, closed and compact sets</h3>
<p>The <strong>open ball</strong> in $\mathbb{R}^d$ centered at $x$ and of radius $r$ is defined by
\begin{equation}
B(x,r)=\{y\in\mathbb{R}^d:\vert y-x\vert&lt; r\}
\end{equation}
A subset $E\subset\mathbb{R}^d$ is <strong>open</strong> if for every $x\in E$ there exists $r&gt;0$ with $B(x,r)\subset E$. And a set is <strong>closed</strong> if its complement is open.<br />
Any (not necessarily countable) union of open sets is open, while in general, the intersection of only finitely many open sets is open. A similar statement holds for the class of closed sets, if we interchange the roles of unions and intersections.</p>

<p>A set $E$ is <strong>bounded</strong> if it is contained in some ball of finite radius. A set is <strong>compact</strong> if it is bounded and is also closed. Compact sets enjoy the <strong>Heine-Borel</strong> covering property:</p>

<p><span id="heine-borel-theorem"><strong>Theorem 1</strong>. (<strong>Heine-Borel theorem</strong>)</span><br />
<em>Assume $E$ is compact, $E\subset\bigcup_\alpha\mathcal{O}_\alpha$, and each $\mathcal{O}_\alpha$ is open. Then there are finitely many of the open sets $\mathcal{O}_{\alpha_1},\mathcal{O}_{\alpha_2},\dots,\mathcal{O}_{\alpha_N}$, such that $E\subset\bigcup_{j=1}^{N}\mathcal{O}_{\alpha_j}$.</em></p>

<p>In words, <em>any</em> covering of a compact set by a collection of open sets contains a <em>finite</em> subcovering.</p>

<p>A point $x\in\mathbb{R}^d$ is a <strong>limit point</strong> of the set $E$ if for every $r&gt;0$, the ball $B(x,r)$ contains points of $E$. This means that there are points in $E$ which are arbitrarily close to $x$. An <strong>isolated point</strong> of $E$ is a point $x\in E$ such that there exists an $r&gt;0$ where $B(x,r)\cap E=\{x\}$.</p>

<p>A point $x\in E$ is an <strong>interior point</strong> of $E$ if there exists $r&gt;0$ such that $B(x,r)\subset E$. The set of all interior points of $E$ is called the <strong>interior</strong> of $E$.</p>

<p>The <strong>closure</strong> of $E$, denoted as $\bar{E}$, consists the union of $E$ and all its limit points. The <strong>boundary</strong> of $E$, denoted as $\partial E$, is the set of points which are in the closure of $E$ but not in the interior of $E$.</p>

<p>A closed set $E$ is <strong>perfect</strong> if $E$ does not have any isolated point.</p>

<p>The <strong>boundary</strong> of $E$, denoted by $\partial E$, is the set of points in $\bar{E}$ not belonging to the interior of $E$.</p>

<p><strong>Remark</strong>:</p>
<ul>
  <li>The closure of a set is a closed set.</li>
  <li>Every point in $E$ is a limit point of $E$.</li>
  <li>A set is closed iff it contains all its limit points.</li>
</ul>

<h3 id="rects-cubes">Rectangles, cubes</h3>
<p>A (closed) <strong>rectangle</strong> $R$ in $\mathbb{R}^d$ is given by the product of $d$ one-dimensional closed and bounded intervals
\begin{equation}
R\doteq[a_1,b_1]\times[a_2,b_2]\times\ldots\times[a_d,b_d],
\end{equation}
where $a_j\leq b_j$, for $j=1,\ldots,d$, are real numbers. In other words, we have
\begin{equation}
R=\left\{\left(x_1,\ldots,x_d\right)\in\mathbb{R}^d:a_j\leq x_j\leq b_j,\forall j=1,\ldots,d\right\}
\end{equation}
With this definition, a rectangle is closed and has sides parallel to the coordinate axis. In $\mathbb{R}$, the rectangles are the closed and bounded intervals; they becomes the usual rectangles as we usually see in $\mathbb{R}^2$; while in $\mathbb{R}^3$, they are the closed parallelepipeds.</p>
<figure>
	<img src="/assets/images/2022-06-16/rectangles.png" alt="Rectangles in R^d" style="display: block; margin-left: auto; margin-right: auto; width: 500px; height: 370px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Rectangles in $\mathbb{R}^d,d=1,2,3$</figcaption>
</figure>

<p>The lengths of the sides of the rectangle $R$ in $\mathbb{R}^d$ are $b_1-a_1,\ldots,b_d-a_d$. The <strong>volume</strong> of its, denoted as $\vert R\vert$, is defined as
\begin{equation}
\vert R\vert\doteq(b_1-a_1)\dots(b_d-a_d)
\end{equation}
An open rectangle is the product of open intervals, and the interior of the rectangle $R$ is then
\begin{equation}
(a_1,b_1)\times\ldots\times(a_d,b_d)
\end{equation}
A <strong>cube</strong> is a rectangle for which $b_1-a_1=\ldots=b_d-a_d$.</p>

<p>A union of rectangles is said to be <strong>almost disjoint</strong> if the interiors of the rectangles are disjoint.</p>

<p><strong>Lemma 2</strong><br />
<em>If a rectangle is the almost disjoint union of finitely many other rectangles, say $R=\bigcup_{k=1}^{N}R_k$, then</em>
\begin{equation}
\vert R\vert=\sum_{k=1}^{N}\vert R_k\vert
\end{equation}</p>

<p><strong>Lemma 3</strong><br />
<em>If $R,R_1,\ldots,R_N$ are rectangles, and $R\subset\bigcup_{k=1}^{N}R_k$, then</em>
\begin{equation}
\vert R\vert\leq\sum_{k=1}^{N}\vert R_k\vert
\end{equation}</p>

<p><strong>Theorem 4</strong><br />
<em>Every open $\mathcal{O}\subset\mathbb{R}$ can be written uniquely as a countable union of disjoint open intervals</em>.</p>

<p><strong>Theorem 5</strong><br />
<em>Every open $\mathcal{O}\subset\mathbb{R}^d,d\geq 1$, can be written as countable union of almost disjoint closed cubes</em>.</p>

<h3 id="cantor-set">The Cantor set</h3>
<p>Let $C_0=[0,1]$ denote the closed unit interval and let $C_1$ represent the set obtained from deleting the middle third open interval from $[0,1]$, as
\begin{equation}
C_1=[0,1/3]\cup[2/3,1]
\end{equation}
We repeat this procedure of deleting the middle third open interval for each subinterval of $C_1$. In the second stage we obtain
\begin{equation}
C_1=[0,1/9]\cup[2/9,1/3]\cup[2/3,7/9]\cup[8/9,1]
\end{equation}
We continue to repeat this process for each subinterval of $C_2$, and so on. The result of this process is a sequence $(C_k)_{k=0,1,\ldots}$ of compact sets with
\begin{equation}
C_0\supset C_1\supset C_2\supset\ldots\supset C_k\supset C_{k+1}\supset\ldots
\end{equation}
The <strong>Cantor set</strong> $\mathcal{C}$ is defined as the intersection of all $C_k$’s
\begin{equation}
\mathcal{C}=\bigcap_{k=0}^{\infty}C_k
\end{equation}
The set $\mathcal{C}$ is not empty, since all end-points of the intervals in $C_k$ (all $k$) belong to $\mathcal{C}$.</p>

<h3 id="others">Others</h3>
<p>Given any sequence $x_1,x_2,\ldots\in[0,+\infty]$. We can always form the sum
\begin{equation}
\sum_{n=1}^{n}x_n\in[0,+\infty]
\end{equation}
as the limit of the partial sums $\sum_{n=1}^{N}x_n$, which may be either finite or infinite. An equivalence definition of this infinite sum is as the supremum of all finite subsums:
\begin{equation}
\sum_{n=1}^{\infty}x_n=\sup_{F\subset\mathbb{N},F\text{ finite}}\sum_{n\in F}x_n
\end{equation}
From this equation, given any collection $(x_\alpha)_{\alpha\in A}$ of numbers $x_\alpha\in[0,+\infty]$ indexed by an arbitrary set $A$, we can define the sum $\sum_{\alpha\in A}x_\alpha$ as
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sup_{F\subset A,F\text{ finite}}\sum_{\alpha\in F}x_\alpha\label{eq:others.1}
\end{equation}
Or moreover, given any bijection $\phi:B\to A$, we has the change of variables formula
\begin{equation}
\sum_{\alpha\in A}x_\alpha=\sum_{\beta\in B}x_{\phi(\beta)}
\end{equation}</p>

<p><span id="tonelli-theorem"><strong>Theorem 6</strong>. (<strong>Tonelli’s theorem for series</strong>)</span><br />
<em>Let $(x_{n,m})_{n,m\in\mathbb{N}}$ be a doubly infinite sequence of extended nonnegative reals $x_{n,m}\in[0,+\infty]$. Then</em>
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}x_{n,m}
\end{equation}</p>

<p><strong>Proof</strong><br />
We will prove the equality between the first and second expression, the proof for the equality between the first and the third one is similar.</p>

<p>We begin by showing that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Let $F\subset\mathbb{N}^2$ be any finite set. Then $F\subset\{1,\ldots,N\}\times\{1,\ldots,N\}$ for some finite $N$. Since $x_{n,m}$ are nonnegative, we have
\begin{align}
\sum_{(n,m)\in F}x_{n,m}&amp;\leq\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,\ldots,N\}}x_{n,m} \\ &amp;=\sum_{n=1}^{N}\sum_{m=1}^{N}x_{n,m} \\ &amp;\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{align}
for any finite subset $F$ of $\mathbb{R}^2$. Then by \eqref{eq:others.1}, we have
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}=\sup_{F\subset\mathbb{N}^2,F\text{ finite}}x_{n,m}\leq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
The problem now remains to prove that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{\infty}\sum_{m=1}^{\infty}x_{n,m},
\end{equation}
which will be proved if we can show that
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{\infty}x_{n,m}
\end{equation}
Fix $N$, we have since each $\sum_{m=1}^{\infty}$ is the limit of $\sum_{m=1}^{M}x_{n,m}$, LHS is the limit of $\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}$ as $M\to\infty$. Thus, it suffices to show that for each finite $M$
\begin{equation}
\sum_{(n,m)\in\mathbb{N}^2}x_{n,m}\geq\sum_{n=1}^{N}\sum_{m=1}^{M}x_{n,m}=\sum_{(n,m)\in\{1,\ldots,N\}\times\{1,ldots,M\}}x_{n,m}
\end{equation}
which is true for all finite $M,N$. And it concludes our proof.</p>

<p><strong>Axiom 7</strong>. (<strong>Axiom of choice</strong>)<br />
<em>Let $(E_\alpha)_{\alpha\in A}$ be a family of non-empty set $E_\alpha$, indexed by an index set $A$. Then we can find a family $(x_\alpha)_{\alpha\in A}$ of elements $x_\alpha$ of $E_\alpha$, indexed by the same set $A$.</em></p>

<p><span id="countable-choice-axiom"><strong>Corollary 8</strong>. (<strong>Axiom of countable choice</strong>)</span><br />
<em>Let $E_1,E_2,\ldots$ be a sequence of non-empty sets. Then we can find a sequence $x_1,x_2,\ldots$ such that $x_n\in E_n,\forall n=1,2,\ldots$.</em></p>

<h2 id="elementary-measure">Elementary measure</h2>

<h3 id="intervals-boxes-elementary-sets">Intervals, boxes, elementary sets</h3>
<p>An <strong>interval</strong> is a subset of $\mathbb{R}$ having one of the forms
\begin{align}
[a,b]&amp;\doteq\{x\in\mathbb{R}:a\leq x\leq b\}, \\ [a,b)&amp;\doteq\{x\in\mathbb{R}:a\leq x\lt b\}, \\ (a,b]&amp;\doteq\{x\in\mathbb{R}:a\lt x\leq b\}, \\ (a,b)&amp;\doteq\{x\in\mathbb{R}:a\lt x\lt b\},
\end{align}
where $a\leq b$ are real numbers.<br />
The <strong>length</strong> of an interval $I=[a,b],[a,b),(a,b],(a,b)$ is denoted as $\vert I\vert$ and is defined by
\begin{equation}
\vert I\vert\doteq b-a
\end{equation}
A <strong>box</strong> in $\mathbb{R}^d$ is a Cartesian product $B\doteq I_1\times\ldots\times I_d$ of $d$ intervals $I_1,\ldots,I_d$ (not necessarily the same length). The <strong>volume</strong> $\vert B\vert$ of such a box $B$ is defined as
\begin{equation}
\vert B\vert\doteq \vert I_1\vert\times\ldots\times\vert I_d\vert
\end{equation}
An <strong>elementary set</strong> is any subset of $\mathbb{R}^d$ which is the union of a finite number of boxes.</p>

<p><strong>Remark 9</strong> (<strong>Boolean closure</strong>)<br />
If $E,F\subset\mathbb{R}^d$ are elementary sets, then</p>
<ul>
  <li>the union $E\cup F$,</li>
  <li>the intersection $E\cap F$,</li>
  <li>the set theoretic difference $E\backslash F\doteq\{x\in E:x\notin F\}$,</li>
  <li>the symmetric difference $E\Delta F\doteq(E\backslash F)\cup(F\backslash E)$ 
are also elementary,</li>
  <li>if $x\in\mathbb{R}^d$, then the translate $E+x\doteq\{y+x:y\in E\}$ is also an elementary set.</li>
</ul>

<p><strong>Proof</strong><br />
With their definitions as elementary sets, we can assume that
\begin{align}
E&amp;=B_1\cup\ldots\cup B_k, \\ F&amp;=B_1’\cup\ldots\cup B_{k’}’,
\end{align}
where each $B_i$ and $B_i’$ is a $d$-dimensional box. By set theory, we have that</p>
<ul>
  <li>The union of $E$ and $F$ can be written as
\begin{equation}
E\cup F=B_1\cup\ldots\cup B_k\cup B_1’\cup\ldots\cup B_{k’}’,
\end{equation}
which is an elementary set.</li>
  <li>The intersection of $E$ and $F$ can be written as
\begin{align}
E\cap F&amp;=\left(B_1\cup\ldots\cup B_k\right)\cup\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\cap B_j’\right),
\end{align}
which is also an elementary set.</li>
  <li>The set theoretic difference of $E$ and $F$ can be written as
\begin{align}
E\backslash F&amp;=\left(B_1\cup\ldots\cup B_k\right)\backslash\left(B_1’\cup\ldots\cup B_{k’}’\right) \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right),
\end{align}
which is, once again, an elementary set.</li>
  <li>With this display, the symmetric difference of $E$ and $F$ can be written as
\begin{align}
E\Delta F&amp;=\left(E\backslash F\right)\cup\left(F\backslash E\right) \\ &amp;=\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_i\backslash B_j’\right)\Bigg]\cup\Bigg[\bigcup_{i=1}^{k}\bigcup_{j=1}^{k’}\left(B_j’\backslash B_i\right)\Bigg],
\end{align}
which satisfies conditions of an elementary set.</li>
  <li>Since $B_i$’s are $d$-dimensional boxes, we can express them as
\begin{equation}
B_i=I_{i,1}\times\ldots I_{i,d},
\end{equation}
where each $I_{i,j}$ is an interval in $\mathbb{R}^d$. Without loss of generality, we assume that they are all closed. In particular, for $j=1,\ldots,d$
\begin{equation}
I_{i,j}=(a_{i,j},b_{i,j})
\end{equation}
Thus, for any $x\in\mathbb{R}^d$, we have that
\begin{align}
E+x&amp;=\left\{y+x:y\in E\right\} \\ &amp;=\Big\{y+x:y\in B_1\cup\ldots\cup B_k\Big\} \\ &amp;=\Big\{y+x:y\in\bigcup_{i=1}^{k}B_i\Big\} \\ &amp;=\left\{y+x:y\in\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j},b_{i,j})\right\} \\ &amp;=\bigcup_{i=1}^{k}\bigcup_{j=1}^{d}(a_{i,j}+x,b_{i,j}+x),
\end{align}
which is an elementary set.</li>
</ul>

<h3 id="measure-elementary-set">Measure of an elementary set</h3>
<p><strong>Lemma 10</strong><br />
<em>Let $E\subset\mathbb{R}^d$ be an elementary set</em>.</p>
<ul id="roman-list" style="font-style: italic;">
	<li>$E$ <i>can be expressed as the finite union of disjoint boxes.</i></li>
	<li>If $E$ is partitioned as the finite union $B_1\cup\ldots\cup B_k$ of disjoint boxes, then the quantity $m(E)\doteq\vert B_1\vert+\ldots+\vert B_k\vert$ is independent of the partition. In other words, given any other partition $B_1'\cup\ldots\cup B_{k'}'$ of $E$, we have</li>
	\begin{equation}
	\vert B_1\vert+\ldots+\vert B_k\vert=\vert B_1'\vert+\ldots+\vert B_{k'}'\vert
	\end{equation}
</ul>

<p>We refer to $m(E)$ as the <strong>elementary measure</strong> of $E$.</p>

<p><strong>Proof</strong></p>
<ul id="roman-list">
	<li>Consider the one-dimensional case, with these $k$ intervals, we can put their $2k$ endpoints into an increasing-order list (discarding repetitions). By looking at the open intervals between these end points, together with the endpoints themselves (viewed as intervals of length zero), we see that there exists a finite collection of disjoint intervals $J_1,\dots,J_{k'}$, such that each of the $I_1,\dots,I_k$ are union of some collection of the $J_1,\dots,J_{k'}$. And since each interval is a one-dimensional box, our statement has been proved with $d=1$.<br />
	In order to prove the multi-dimensional case, we begin by expressing $E$ as
	\begin{equation}
	E=\bigcap_{i=1}^{k}B_i,
	\end{equation}
	where each box $B_i=I_{i,1}\times\dots\times I_{i,d}$. For each $j=1,\dots,d$, since we has proved the one-dimensional case, we can express $I_{1,j},\dots I_{k,j}$ as the union of subcollections of collections $J_{1,j},\dots,J_{k',j}$ of disjoint intervals. Taking Cartesian product, we can express the $B_1,\dots,B_k$ as finite unions of box $J_{i_1,1}\times\dots\times J_{i_d,d}$, where $1\leq i_j\leq k_j'$ for all $1\leq j\leq d$. Moreover such boxes are disjoint, which proved our argument.</li>
	<li> We have that the length for an interval $I$ can be computed as
	\begin{equation}
	\vert I\vert=\lim_{N\to\infty}\frac{1}{N}\#\left(I\cap\frac{1}{N}\mathbb{Z}\right),
	\end{equation}
	where $\#A$ represents the cardinality of a finite set $A$ and 
	\begin{equation}
	\frac{1}{N}\mathbb{Z}\doteq\left\{\frac{x}{N}:x\in\mathbb{Z}\right\}
	\end{equation}
	Thus, volume of the box, say $B$, established from $d$ intervals $I_1,\dots,I_d$ by taking Cartesian product of them can be written as
	\begin{equation}
	\vert B\vert=\lim_{N\to\infty}\frac{1}{N^d}\#\left(B\cap\frac{1}{N}\mathbb{Z}^d\right)
	\end{equation}
	Therefore, with $k$ disjoint boxes $B_1,\dots,B_k$, we have that
	\begin{align}
	\vert B_1\vert+\dots+\vert B_k\vert&amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k}B_i\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cap\frac{1}{N}\mathbb{Z}^d\right) \\\\ &amp;=\lim_{N\to\infty}\frac{1}{N^d}\#\left[\left(\bigcup_{i=1}^{k'}B_i'\right)\cap\frac{1}{N}\mathbb{Z}^d\right] \\\\ &amp;=\vert B_1'\vert+\dots+\vert B_{k'}'\vert
	\end{align}
	</li>
</ul>

<h3 id="elementary-measure-properties">Properties of elementary measure</h3>
<p>From the definition of elementary measure, it is easily seen that, for any elementary sets $E$ and $F$ (not necessarily disjoint),</p>
<ul id="number-list">
	<li>
		$m(E)$ is a nonnegative real number (<b>non-negativity</b>), and has <b>finite additivity property</b>:
		\begin{equation}
		m(E\cup F)=m(E)+m(F)
		\end{equation}
		And by induction, it also implies that
		\begin{equation}
		m(E_1\cup\dots\cup E_k)=m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,E_k$ are disjoint elementary sets.
	</li>
	<li>
		$m(\emptyset)=0$.
	</li>
	<li>
		$m(B)=\vert B\vert$ for all box $B$.
	</li>
	<li>
		From non-negativity, finite additivity and <b>Remark 9</b>, we conclude the <b>monotonicity</b> property, i.e., $E\subset F$ implies that
		\begin{equation}
		m(E)\leq m(F)
		\end{equation}
	</li>
	<li>
		From the above and finite additivity, we also obtain the <b>finite subadditivity</b> property
		\begin{equation}
		m(E\cup F)\leq m(E)+m(F)
		\end{equation}
		And by induction, we then have
		\begin{equation}
		m(E_1\cup\dots\cup E_k)\leq m(E_1)+\dots+m(E_k),
		\end{equation}
		whenever $E_1,\dots,
		E_k$ are elementary sets (not necessarily disjoint).
	</li>
	<li>
		We also have the <b>translation invariance</b> property
		\begin{equation}
		m(E+x)=m(E),\hspace{1cm}\forall x\in\mathbb{R}^d
		\end{equation}
	</li>
</ul>

<h3 id="uniqueness-elementary-measure">Uniqueness of elementary measure</h3>
<p>Let $d\geq 1$ and let $m’:\mathcal{E}(\mathbb{R}^d)\to\mathbb{R}^+$ be a map from the collection $\mathcal{E}(\mathbb{R}^d)$ of elementary subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity, and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all elementary sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.</p>

<p><strong>Proof</strong><br />
Set $c\doteq m’([0,1)^d)$, we then have that $c\in\mathbb{R}^+$ by the non-negativity property. Using the translation invariance property, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)=\dots=m’\left(\left[\frac{n-1}{n},1\right)^d\right)
\end{equation}
On other hand, using the finite additivity property, for any positive integer $n$, we obtain that
\begin{align}
m’([0,1)^d)&amp;=m’\left(\left[0,\frac{1}{n}\right)^d\cup\left[\frac{1}{n},\frac{2}{n}\right)^d\cup\dots\cup\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;=m’\left(\left[0,\frac{1}{n}\right)^d\right)+m’\left(\left[\frac{1}{n},\frac{2}{n}\right)^d\right)+\dots+m’\left(\left[\frac{n-1}{n},1\right)^d\right) \\ &amp;=n m’\left(\left[0,\frac{1}{n}\right)^d\right)
\end{align}
Thus,
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{c}{n},\hspace{1cm}\forall n\in\mathbb{Z}^+
\end{equation}
Moreover, since $m\left(\left[0,\frac{1}{n}\right)^d\right)=\frac{1}{n}$, we have that for any positive integer $n$
\begin{equation}
m’\left(\left[0,\frac{1}{n}\right)^d\right)=cm\left(\left[0,\frac{1}{n}\right)^d\right)
\end{equation}
It then follows by induction that
\begin{equation}
m’(E)=cm(E)
\end{equation}</p>

<p><strong>Remark 11</strong><br />
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be elementary sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also elementary, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.</p>

<p><strong>Proof</strong><br />
Without loss of generality, assume that $d_1\leq d_2$. With their definitions as elementary sets, we can assume that
\begin{align}
E_1&amp;=B_1\cup\dots\cup B_{k_1}, \\ E_2&amp;=B_1’\cup\dots\cup B_{k_2}’,
\end{align}
where each $B_i$ is a $d_1$-dimensional box while each $B_i’$ is a $d_2$-dimensional box. And using <strong>Lemma 5</strong>, without loss of generality, we can assume that $B_i$ are disjoint boxes and $B_i’$ are also disjoint, which implies that
\begin{align}
m^{d_1}(E_1)&amp;=m^{d_1}(B_1)+\dots+m^{d_1}(B_{k_1}),\label{eq:remark11.1} \\ m^{d_2}(E_2)&amp;=m^{d_2}(B_1’)+\dots+m^{d_2}(B_{k_2}’)\label{eq:remark11.2}
\end{align}
By set theory, we have that
\begin{align}
E_1\times E_2&amp;=\Big(B_1\cup\dots\cup B_{k_1}\Big)\times\Big(B_1’\cup\dots\cup B_{k_2}’\Big) \\ &amp;=\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right),\label{eq:remark11.3}
\end{align}
which is an elementary set.</p>

<p>Since $B_1,\dots,B_{k_1}$ are disjoint and $B_1’,\dots,B_{k_2}’$ are disjoint, the Cartesian products $B_i\times B_j’$ for $i=1,\dots,k_1$ and $j=1,\dots,k_2$ are also disjoint. From \eqref{eq:remark11.3} and using the finite additivity property, we have that
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;=m^{d_1+d_2}\Bigg(\bigcup_{i=1}^{k_1}\bigcup_{j=1}^{k_2}\left(B_i\times B_j’\right)\Bigg) \\ &amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right)\label{eq:remark11.4}
\end{align}
On the one hand, using the definition of boxes, and without loss of generality we can express, for each $i=1,\dots,k_1$, that:
\begin{equation}
B_i=(a_{i,1},b_{i,1})\times\dots\times(a_{i,d_1},b_{i,d_1}),
\end{equation}
where $a_{i,j},b_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_1$. Hence,
\begin{equation}
m^{d_1}(B_i)=\prod_{j=1}^{d_1}(b_{i,j}-a_{i,j}),\hspace{1cm}i=1,\dots,k_1\label{eq:remark11.5}
\end{equation}
Similarly, we also have that
\begin{equation}
m^{d_2}(B_i’)=\prod_{j=1}^{d_2}(d_{i,j}-c_{i,j}),\hspace{1cm}i=1,\dots,k_2\label{eq:remark11.6}
\end{equation}
where $c_{i,j},d_{i,j}\in\mathbb{R}$ for all $j=1,\dots,d_2$.</p>

<p>Moreover, on the other hand, we also have that the $(d_1+d_2)$-dimensional box $B_i\times B_j’$ can be expressed as
\begin{equation}
B_i\times B_j’=(e_1,f_1)\times\dots\times(e_{d_1+d_2},f_{d_1+d_2}),\label{eq:remark11.7}
\end{equation}
where $e_k=a_{i,k};f_k=b_{i,k}$ for all $k=1,\dots,d_1$ and $e_k=c_{j,k-d_1};f_k=d_{j,k-d_1}$ for all $k=d_1+1,\dots,d_2$.</p>

<p>From \eqref{eq:remark11.5}, \eqref{eq:remark11.6} and \eqref{eq:remark11.5}, for any $i=1,\dots,k_1$ and for any $j=1,\dots,k_2$, we have
\begin{align}
m^{d_1+d_2}(B_i\times B_j’)&amp;=\prod_{k=1}^{d_1+d_2}(f_k-e_k) \\ &amp;=\Bigg(\prod_{k=1}^{d_1}(b_{i,k}-a_{i,k})\Bigg)\Bigg(\prod_{k=1}^{d_2}(d_{j,k}-c_{j,k})\Bigg) \\ &amp;=m^{d_1}(B_i)\times m^{d_2}(B_j’)
\end{align}
With this result, combined with \eqref{eq:remark11.1} and \eqref{eq:remark11.2}, equation \eqref{eq:remark11.4} can be written as
\begin{align}
m^{d_1+d_2}(E_1\times E_2)&amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1+d_2}\left(B_i\times B_j’\right) \\ &amp;=\sum_{i=1}^{k_1}\sum_{j=1}^{k_2}m^{d_1}(B_i)\times m^{d_2}(B_j’) \\ &amp;=m^{d_1}(E_1)\times m^{d_2}(E_2),
\end{align}
which concludes our proof.</p>

<h2 id="jordan-measure">Jordan measure</h2>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set.</p>
<ul>
  <li>The <strong>Jordan inner measure</strong> $m_{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m_{*,(J)}(E)\doteq\sup_{A\subset E,A\text{ elementary}}m(A)
\end{equation}</li>
  <li>The <strong>Jordan outer measure</strong> $m^{*,(J)}(E)$ of $E$ is defined as
\begin{equation}
m^{*,(J)}(E)\doteq\inf_{B\supset E,B\text{ elementary}}m(B)
\end{equation}</li>
  <li>If $m_{*,(J)}(E)=m^{*,(J)}(E)$, then we say that $E$ is <strong>Jordan measurable</strong>, and call
\begin{equation}
m(E)\doteq m_{*,(J)}(E)=m^{*,(J)}(E)
\end{equation}
the <strong>Jordan measure</strong> of $E$.</li>
</ul>

<h3 id="jordan-measurability-characterisation">Characterisation of Jordan measurability</h3>
<p>Let $E\subset\mathbb{R}^d$ be bounded. These following statements are equivalence</p>
<ul id="number-list">
	<li>$E$ is Jordan measurable.</li>
	<li>For every $\varepsilon&gt;0$, there exists elementary sets $A\subset E\subset B$ such that $m(B\backslash A)\leq\varepsilon$.</li>
	<li>For every $\varepsilon&gt;0$, there exists an elementary set $A$ such that $m^{*,(J)}(A\Delta E)\leq\varepsilon$.</li>
</ul>

<p><strong>Proof</strong><br />
In order to prove these three statements are equivalence, we will be proving that (1) implies (2); (2) implies (3); and that (2) implies (1).</p>
<ul>
  <li>(1) implies (2).<br />
Since $E$ is Jordan measurable, we have that
\begin{equation}
m(E)=\sup_{A\subset E;A\text{ elementary}}m(A)=\inf_{B\supset E;B\text{ elementary}}m(B)
\end{equation}
By the definition of supremum, there exists an elementary set $A\subset E$ such that for any $\varepsilon&gt;0$ 
\begin{equation}
m(A)\geq m(E)-\frac{\varepsilon}{2}\label{eq:jmc.1}
\end{equation}
In addition, by the definition of infimum, there also exists an elementary set $B\supset E$ such that for any $\varepsilon&gt;0$
\begin{equation}
m(B)\leq m(E)+\frac{\varepsilon}{2}\label{eq:jmc.2}
\end{equation}
From \eqref{eq:jmc.1} and \eqref{eq:jmc.2}, we have that for any $\varepsilon&gt;0$
\begin{equation}
m(B\backslash A)=m(B)-m(A)\leq\varepsilon
\end{equation}</li>
  <li>(2) implies (3).<br />
With (2) satisfied, we have that we can find elementary sets $A\subset E\subset B$ such that
\begin{equation}
m(B\backslash A)\leq\varepsilon,\hspace{1cm}\forall\varepsilon&gt;0
\end{equation}
Since $A\subset E\subset B$ and by the definition of symmetric difference, we have
\begin{equation}
A\Delta E=(A\backslash E)\cup(E\backslash A)=(E\backslash A)\subset(B\backslash A)
\end{equation}
Hence
\begin{equation}
m^{*,(J)}(A\Delta E)\leq m(B\backslash A)\leq\varepsilon
\end{equation}</li>
  <li>(2) implies (1).<br />
Let $(A_n)_{n\in\mathbb{N}}$ and $(B_n)_{n\in\mathbb{N}}$ be sequences of elementary sets such that $A_n\subset E\subset B_n$ for all $n\in\mathbb{N}$. Statement (2) says that for all $\varepsilon&gt;0$, there exists $i,j\in\mathbb{N}$ such that
\begin{equation}
m(B_j\backslash A_i)\leq\varepsilon
\end{equation}
or
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon\label{eq:jmc.3}
\end{equation}
Let $A_\text{sup}$ and $B_\text{inf}$ be two sets in the two sequences above with
\begin{align}
m(A_\text{sup})&amp;=\sup_{n\in\mathbb{N}}m(A_n), \\ m(B_\text{inf})&amp;=\inf_{n\in\mathbb{N}}m(B_n),
\end{align}
which means
\begin{align}
m_{*,(J)}(E)&amp;=m(A_\text{sup}) \\ m^{*,(J)}(E)&amp;=m(B_\text{inf})
\end{align}
Using the monotonicity property of elementary measure, we have that
\begin{equation}
m(A_\text{sup})\leq m(B_\text{inf})
\end{equation}
Assume that $m(B_\text{inf})&gt;m(A_\text{sup})$, and consider an $\varepsilon&gt;0$ such that $\varepsilon&lt; m(B_\text{inf})-m(A_\text{sup})$. We can continue to derive \eqref{eq:jmc.3} as
\begin{equation}
m(B_j)\leq m(A_i)+\varepsilon&lt; m(A_i)+m(B_\text{inf})-m(A_\text{sup})&lt; m(B_\text{inf}),
\end{equation}
which is false with the definition of $B_\text{inf}$. Therefore, our assumption is also false, which means
\begin{equation}
m(A_\text{sup})=m(B_\text{inf})
\end{equation}
or
\begin{equation}
m_{*,(J)}(E)=m^{*,(J)}(E),
\end{equation}
or in other words, $E$ is Jordan measurable.</li>
</ul>

<p><strong>Corollary 12</strong></p>
<ul>
  <li>Every elementary set $E$ is Jordan measurable.</li>
  <li>On elementary sets, Jordan measure is elementary measure.</li>
</ul>

<p>Jordan measurability also inherits many of the properties of elementary measure.</p>

<h3 id="jordan-measurability-properties">Properties of Jordan measurability</h3>
<p>Let $E,F\in\mathbb{R}^d$ be Jordan measurable sets. Then</p>
<ul id="number-list">
	<li>
		<b>Boolean closure</b>. $E\cup F,E\cap F,E\backslash F,E\Delta F$ are also Jordan measurable sets.
	</li>
	<li>
		<b>Non-negativity</b>. $m(E)\geq 0$.
	</li>
	<li>
		<b>Finite additivity</b>. If $E,F$ are disjoint, then $m(E\cup F)=m(E)+m(F)$.
	</li>
	<li>
		<b>Monotonicity</b>. If $E\subset F$, then $m(E)\leq m(F)$.
	</li>
	<li>
		<b>Finite subadditivity</b>. $m(E\cup F)\leq m(E)+m(F)$.
	</li>
	<li>
		<b>Translation invariance</b>. For any $x\in\mathbb{R}^d$, $E+x$ is Jordan measurable, and $m(E+x)=m(E)$.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ol id="number-list">
	<li>
		<b>Boolean closure</b>.
		<ul>
			<li>
				By characterisation of Jordan measurability, we can find elementary sets $A_1\subset E\subset B_1$ and $A_2\subset F\subset B_2$ such that for any $\varepsilon&gt;0$
				\begin{align}
				m(B_1\backslash A_1)&amp;\leq\frac{\varepsilon}{2}, \\ m(B_2\backslash A_2)&amp;\leq\frac{\varepsilon}{2}
				\end{align}
				Thus, we have that
				\begin{equation}
				\left(A_1\cap A_2\right)\subset\left(E\cap F\right)\subset\left(B_1\cap B_2\right)
				\end{equation}
				and
				\begin{equation}
				\left(A_1\cup A_2\right)\subset\left(E\cup F\right)\subset\left(B_1\cup B_2\right)
				\end{equation}
				Moreover, for any $\varepsilon&gt;0$, we have that
				\begin{align*}
				m\big((B_1\cup B_2)\backslash(A_1\cup A_2)\big)&amp;=m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;=m(B_1)+m(B_2\backslash B_1)-m(A_1\cup A_2) \\ &amp;\leq m(B_1)+m(B_2\backslash A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1)-m(A_1)+m(B_2\backslash A_1)+m(A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1)-m(A_1)+m(B_2\cup A_1)-m(A_1\cup A_2) \\ &amp;=m(B_1\backslash A_1)+m\big((B_2\cup A_1)\backslash(A_1\cup A_2)\big) \\ &amp;=m(B_1\backslash A_1)+m(B_2\backslash A_2) \\ &amp;\leq\varepsilon/2+\varepsilon/2 \\ &amp;=\varepsilon,
				\end{align*}
				which implies that $E\cup F$ is Jordan measurable.
			</li>
			<li>
				From the result above, and by monotonicity, finite additivity, finite subadditivity properties of elementary measure, for any $\varepsilon&gt;0$, we also have that
				\begin{align*}
				m\big((B_1\cap B_2)\backslash(A_1\cap A_2)\big)&amp;=m(B_1\cap B_2)-m(A_1\cap A_2) \\ &amp;=m\Big(\big(B_1\cup B_2\big)\backslash\big((B_1\backslash B_2)\cup(B_2\backslash B_1)\big)\Big) \\ &amp;\hspace{1cm}-m\Big(\big(A_1\cup A_2\big)\backslash\big((A_1\backslash A_2)\cup(A_2\backslash A_1)\big)\Big) \\ &amp;=m(B_1\cup B_2)-m(B_1\backslash B_2)-m(B_2\backslash B_1) \\ &amp;\hspace{1cm}-m(A_1\cup A_2)+m(A_1\backslash A_2)+m(A_2\backslash A_1) \\ &amp;=m(B_1\cup B_2)-m(A_1\cup A_2)+m(A_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;\hspace{1cm}+m(A_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2)+m(B_1\backslash A_2)-m(B_1\backslash B_2) \\ &amp;\hspace{1cm}+m(B_2\backslash A_1)-m(B_2\backslash B_1) \\ &amp;\leq m(B_1\cup B_2)-m(A_1\cup A_2) \\ &amp;\leq\varepsilon,
				\end{align*}
				which also implies that $E\cap F$ is Jordan measurable.
			</li>
			<li></li>
		</ul>
	</li>
	<li>
		<b>Non-negativity</b>.<br />
		Given $E$ being Jordan measurable set, we have
		\begin{equation}
		m(E)=\sup_{A\subset E,A\text{ elementary}}m(A)\geq m(\emptyset)=0,
		\end{equation}
		by the monotonicity property of elementary measure.
	</li>
	<li>
		<b>Finite additivity</b>.<br />
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite additivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset F,A_2\text{ elementary}}m(A_2) \\ &amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1)+m(A_2) \\ &amp;=\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2)=m(E\cup F)
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>.<br />
		Given $E\subset F$ are Jordan measurable sets, the we have
		\begin{equation}
		m(E)\leq\sup_{A\subset F,A\text{ elementary}}m(A)=m(F)
		\end{equation}
	</li>
	<li>
		<b>Finite subadditivity</b>.<br />
		Since given $E,F$ being Jordan measurable sets, $E\cup F$ is also Jordan measurable set. And by the finite subadditivity property of elementary measure, we have
		\begin{align}
		m(E)+m(F)&amp;=\sup_{A_1\subset E,A_1\text{ elementary}}m(A_1)+\sup_{A_2\subset E,A_2\text{ elementary}}m(A_2) \\ &amp;\geq\sup_{A_1\subset E,A_2\subset F;A_1,A_2\text{ elementary}}m(A_1\cup A_2) \\ &amp;=m(E\cup F)=m(E\cup F)
		\end{align}
	</li>
	<li>
		<b>Translation invariance</b>.<br />
		By the translation invariance property of elementary measure, for any $x\in\mathbb{R}^d$, the Jordan inner measure of $E+x$ can be written as
		\begin{align}
		m_{*,(J)}(E+x)&amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A) \\ &amp;=\sup_{A\subset E+x,A\text{ elementary}}m(A-x) \\ &amp;=\sup_{A-x\subset E,A-x\text{ elementary}}m(A-x)=m(E)
		\end{align}
		Similarly, we also have the Jordan outer measure of $E+x$ is also equal to the Jordan measure of $E$
		\begin{equation}
		m^{*,(J)}(E+x)=m(E)
		\end{equation}
		Hence,
		\begin{equation}
		m_{*,(J)}(E+x)=m^{*,(J)}(E+x)=m(E),
		\end{equation}
		or in other words, $E+x$ is Jordan measurable with $m(E+x)=m(E)$.
	</li>
</ol>

<p><strong>Remark 13</strong> (Regions under graphs are Jordan measurable)<br />
Let $B$ be a closed box in $\mathbb{R}^d$, and let $f:B\to\mathbb{R}$ be a continuous function. Then</p>
<ul id="number-list">
	<li>
		The graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ is Jordan measurable in $\mathbb{R}^{d+1}$ with Jordan measure zero.
	</li>
	<li>
		The set $\{(x,t):x\in B;0\leq t\leq f(x)\}\subset\mathbb{R}^{d+1}$ is Jordan measurable.
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		For any closed box $C\in\mathbb{R}^d$, we have $\{(x,f(x)):x\in C\}\subset\mathbb{R}^{d+1}$ with $f:C\to\mathbb{R}$ is a compact set. And when $f$ continuous in a compact set we also have $f$ is <span>uniformly continuous<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></span>, which means for any $\varepsilon&gt;0$, there exists $\delta$ such that for every $x,y\in C$
		\begin{equation}
		\vert f(x)-f(y)\vert&lt;\varepsilon,
		\end{equation}
		with $\vert x-y\vert&lt;\delta$. Therefore, we can divide $C$ into finitely many almost disjoint boxes $C_1,\ldots,C_n$ such that $\vert x_i-y_i\vert&lt;\delta$ for every $x_i,y_i\in C_i$ and for any $\varepsilon&gt;0$
		\begin{equation}
		\vert f(x_i)-f(y_i)\vert&lt;\varepsilon
		\end{equation}
		Moreover, for each such box $C_i$ with center of the box $x_i$ we also have
		\begin{equation}
		\left\{(x,f(x)):x\in C_i\right\}\subset C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		Therefore
		\begin{equation}
		\left\{(x,f(x)):x\in C\right\}=\bigcup_{i=1}^{n}\left\{(x,f(x)):x\in C_i\right\}\subset\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)
		\end{equation}
		With this result, and by the monotonicity, finite additivity of elementary measure, we have the Jordan outer measure of the graph $\{(x,f(x)):x\in B\}\subset\mathbb{R}^{d+1}$ can be written as
		\begin{align}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)&amp;=\inf_{C\supset B,C\text{ closed box}}m\left(\left\{(x,f(x)):x\in C\right\}\right) \\ &amp;\leq m^{d+1}\left(\bigcup_{i=1}^{n}C_i\times\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;=\sum_{i=1}^{n}m^d(C_i)\times m^1\left(\left(f(x_i)-\varepsilon,f(x_i)+\varepsilon\right)\right) \\ &amp;=2n\varepsilon m^d(C)&lt;2n\varepsilon\delta
		\end{align}
		And since $\varepsilon&gt;0$ arbitrarily, we finally obtain
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)=0
		\end{equation}
		Plus that, since
		\begin{equation}
		m^{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq m_{*,(J)}\left(\{(x,f(x)):x\in B\}\right)\geq 0,
		\end{equation}
		we have that
		\begin{equation}
		m^{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=m_{*,(J)}\Big(\big\{(x,f(x)):x\in B\big\}\Big)=0,
		\end{equation}
		or in other words, the graph $\left(\{(x,f(x)):x\in B\}\right)$ is Jordan measurable on $\mathbb{R}^{d+1}$ with Jordan measure zero.
	</li>
	<li>
		Let $E=\big\{(x,t):x\in B;0\leq t\leq f(x)\big\}$ and let $I$, $O$ be sets defined as for an arbitrary $\varepsilon&gt;0$
		\begin{align}
		I&amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)-\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)-\frac{\varepsilon}{2}\right], \\ O&amp;=\left\{(x,t):x\in B,0\leq t\leq f(x)+\frac{\varepsilon}{2}\right\}=B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]
		\end{align}
		Therefore, it follows immediately that $I\subset E\subset O$ and moreover
		\begin{align}
		m^{d+1}(O\backslash I)&amp;=m^{d+1}\left(B\times\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash B\times\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;=m^d(B)\times m^1\left(\left[0,f(x)+\frac{\varepsilon}{2}\right]\backslash\left[0,f(x)-\frac{\varepsilon}{2}\right]\right) \\ &amp;=m^d(B)\times\varepsilon
		\end{align}
		And since $\varepsilon&gt;0$ arbitrarily, we can claim that $E$ is Jordan measurable.
	</li>
</ul>

<p><strong>Remark 14</strong></p>
<ul>
	<li>
		All open and closed Euclidean balls, $B(x,r)\doteq\{y\in\mathbb{R}^d:\vert y-x\vert&lt; r\}$ and $\overline{B(x,r)}\doteq\{y\in\mathbb{R}^d:\vert y-x\vert\leq r\}$, in $\mathbb{R}^d$ are Jordan measurable, with Jordan measure $c_dr^d$ for some constant $c_d$ depending only on $d$.
	</li>
	<li>
		Establish the crude bounds
		\begin{equation}
		\left(\frac{2}{\sqrt{d}}\leq c_d\leq 2^d\right)
		\end{equation} 
	</li>
</ul>

<h3 id="jordan-null-sets">Jordan null sets</h3>
<p>A <strong>Jordan null set</strong> is a Jordan measurable set of Jordan measure zero. We have that any subset of a Jordan null set is also a Jordan null set.</p>

<p><strong>Proof</strong><br />
Let $E\subset F$ where F is a Jordan null set. Also let $A\subset E$, it follows that $A\subset F$, and hence
\begin{equation}
m(A)\leq m_{*,(J)}(F)=0
\end{equation}
Since $m(E)=0$, we can choose a set $B\supset F$ such that $m(B)\leq\varepsilon$ for $\varepsilon&gt;0$ arbitrarily. Thus, $E\subset B$ and moreover
\begin{equation}
m(B\backslash A)\leq\varepsilon,
\end{equation}
which claims that $E$ is Jordan measurable with measurable of zero since $m(E)\leq m(F)=0$. Or in other words, $E$ is also a Jordan null set.</p>

<p><strong>Remark 15</strong><br />
For any Jordan measurable set $E\subset\mathbb{R}^d$, its Jordan measure can be written as
\begin{equation}
m(E)\doteq\lim_{N\to\infty}\frac{1}{N^d}\#\left(E\cup\frac{1}{N}\mathbb{Z}^d\right)
\end{equation}</p>

<p><strong>Proof</strong></p>

<h3 id="metric-formula-jordan-measurability">Metric entropy formulation of Jordan measurability</h3>
<p>A <strong>dyadic cube</strong> is defined to be a half-open box of the form
\begin{equation}
\left[\frac{i_1}{2^n},\frac{i_1+1}{2^n}\right)\times\ldots\times\left(\frac{i_d}{2^n},\frac{i_d+1}{2^n}\right],
\end{equation}
for some integers $n,i_1,\ldots,i_d$. Let $E\subset\mathbb{R}^d$ be a bounded set. For each integer $n$, let $\mathcal{E}_*(E,2^{-n})$ denote the number of dyadic cubes of sidelength $2^{-n}$ that are contained in $E$, and let $\mathcal{E}^*(E,2^{-n})$ be the number of dyadic cubes of sidelength $2^{-n}$ that intersect $E$. Then $E$ is Jordan measurable iff
\begin{equation}
\lim_{n\to\infty}2^{-dn}(\mathcal{E}^*(E,2^{-n}))-\mathcal{E}_*(E, 2^{-n})=0,
\end{equation}
in which case we have
\begin{equation}
m(E)=\lim_{n\to\infty}2^{-dn}\mathcal{E}_*(E,2^{-n})=\lim_{n\to\infty}2^{-dn}\mathcal{E}^*(E,2^{-n})
\end{equation}</p>

<h3 id="uniqueness-jordan-measure">Uniqueness of Jordan measure</h3>
<p>Let $d\geq 1$ and let $m’:\mathcal{J}(\mathbb{R}^d)\to\mathbb{R}^+$  be a map from the collection of Jordan measurable subsets of $\mathbb{R}^d$ to the nonnegative reals that obeys the non-negativity, finite additivity and translation invariance properties. Then there exists a constant $c\in\mathbb{R}^+$ such that
\begin{equation}
m’(E)=cm(E),
\end{equation}
for all Jordan measurable sets $E$. In particular, if we impose the additional normalization $m’([0,1)^d)=1$, then $m’\equiv m$.</p>

<p><strong>Proof</strong><br />
Follow the same steps as the proof of the uniqueness of elementary measure, the argument above can easily be proved.</p>

<p><strong>Remark 16</strong><br />
Let $d_1,d_2\geq 1$, and let $E_1\subset\mathbb{R}^{d_1},E_2\subset\mathbb{R}^{d_2}$ be Jordan measurable sets. Then $E_1\times E_2\subset\mathbb{R}^{d_1+d_2}$ is also Jordan measurable, and $m^{d_1+d_2}(E_1\times E_2)=m^{d_1}(E_1)\times m^{d_2}(E_2)$.</p>

<p><strong>Proof</strong><br />
Let $A_1\subset E_1$ such that $A_1$ is elementary and
\begin{equation}
m^{d_1}(A_1)=\sup_{A\subset E_1,A\text{ elementary}}m(A)=m_{*,(J)}(E_1)=m^{d_1}(E_1)
\end{equation}
Let $B_1\supset E_1$ such that $B_1$ is elementary and
\begin{equation}
m^{d_1}(B_1)=\inf_{B\supset E_1,B\text{ elementary}}m(B)=m^{*,(J)}(E_1)=m^{d_1}(E_1),
\end{equation}
which implies that
\begin{equation}
m^{d_1}(A_1)=m^{d_1}(B_1)=m^{d_1}(E_1)
\end{equation}
Analogously, we define $A_2\subset E_2\subset B_2$ such that
\begin{align}
m^{d_2}(A_2)&amp;=\sup_{A\subset E_2,A\text{ elementary}}m(A)=m_{*,(J)}(E_2)=m^{d_2}(E_2) \\ m^{d_2}(B_2)&amp;=\inf_{B\supset E_2,B\text{ elementary}}m(B)=m^{*,(J)}(E_2)=m^{d_1}(E_2)
\end{align}
And thus, we also have
\begin{equation}
m^{d_2}(A_2)=m^{d_2}(B_2)=m^{d_2}(E_2)
\end{equation}
On the one hand, with these definitions, we have
\begin{equation}
m^{d_1+d_2}(A_1\times A_2)=\sup_{A\subset E_1\times E_2,A\text{ elementary}}=m_{*,(J)}(E_1\times E_2)\label{eq:remark15.1}
\end{equation}
and
\begin{equation}
m^{d_1\times d_2}(B_1\times B_2)=\sup_{B\supset E_1\times E_2,A\text{ elementary}}=m^{*,(J)}(E_1\times E_2)\label{eq:remark15.2}
\end{equation}
On the other hands, By <strong>remark 11</strong>, we have that $A_1\times A_2$ and $B_1\times B_2$ are also elementary sets and
\begin{align}
m^{d_1}(A_1)\times m^{d_2}(A_2)&amp;=m^{d_1+d_2}(A_1\times A_2)\label{eq:remark15.3} \\ m^{d_1}(B_1)\times m^{d_2}(B_2)&amp;=m^{d_1+d_2}(B_1\times B_2)\label{eq:remark15.4}
\end{align}
From \eqref{eq:remark15.1}, \eqref{eq:remark15.2}, \eqref{eq:remark15.3} and \eqref{eq:remark15.4}, we can claim that $E_1\times E_2$ is Jordan measurable and
\begin{equation}
m^{d_1}(E_1)\times m^{d_2}(E_2)=m^{d_1+d_2}(E_1\times E_2)
\end{equation}</p>

<h3 id="topo-jordan-measurability">Topological of Jordan measurability</h3>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set</p>
<ul id="number-list">
	<li>$E$ and the closure $\bar{E}$ of $E$ have the same Jordan outer measure.</li>
	<li>$E$ and the interior $E^\circ$ of $E$ have the same Jordan inner measure.</li>
	<li>$E$ is Jordan measurable iff the <b>topological boundary</b> $\partial E$ of $E$ has Jordan outer measure zero.</li>
	<li>The <b>bullet-riddled square</b> $[0,1]^2\backslash\mathbf{Q}^2$, and set of bullets $[0,1]^2\cup Q^2$, both have Jordan inner measure zero and Jordan outer measure one. In particular, both sets are not Jordan measurable.</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		Since $E\subset\overline{E}$, it is easily seen that
		\begin{equation}
		m^{*,(J)}(E)\leq m^{*,(J)}(\overline{E})
		\end{equation}
		Thus, the problem remains to prove that
		\begin{equation}
		m^{*,(J)}(E)\geq m^{*,(J)(\overline{E})}
		\end{equation}
		Let $B_1,\ldots,B_N$ be $N$ disjoint boxes such that
	</li>
</ul>

<h3 id="caratheodory-type-property">Carathéodory type property</h3>
<p>Let $E\subset\mathbb{R}^d$ be a bounded set, and $F\subset\mathbb{R}^d$ be an elementary set. Then we have that
\begin{equation}
m^{*,(J)}(E)=m^{*,(J)}(E\cap F)+m^{*,(J)}(E\backslash F)
\end{equation}</p>

<h2 id="connect-riemann-int">Connection with the Riemann integral</h2>
<p>We then consider the relationship between Jordan measure and the <strong>Rieman integral</strong>, or the equivalent <strong>Darboux integral</strong>.</p>

<h3 id="riemann-integrability">Riemann integrability</h3>
<p>Let $[a,b]$ be an interval of positive length, and $f:[a,b]\to\mathbb{R}$ be a function. A <strong>tagged partition</strong>
\begin{equation}
\mathcal{P}=\left(\left(x_0,x_1,\dots,x_n\right),\left(x_1^{*},\dots,x_n^{*}\right)\right)
\end{equation}
of $[a,b]$ is a finite sequence of real numbers $a=x_0&lt; x_1&lt;\dots&lt; x_n=b$, together with additional numbers $x_{i-1}\leq x_i^{*}\leq x_i$ for each $i=1,\dots,n$. Let $\delta x_i\doteq x_i-x_{i-1}$, the quantity
\begin{equation}
\Delta(\mathcal{P})\doteq\sup_{1\leq i\leq n}\delta x_i
\end{equation}
is called the <strong>norm</strong> of the tagged partition. The <strong>Riemann sum</strong> $\mathcal{R}(f,\mathcal{P})$ of $f$ w.r.t the tagged partition $\mathcal{P}$ is defined as
\begin{equation}
\mathcal{R}(f,\mathcal{P})\doteq\sum_{i=1}^{n}f(x_i^{*})\delta x_i
\end{equation}
we say that $f$ is <strong>Riemann integrable</strong> on $[a,b]$ if there exists a real number, denoted as $\int_{a}^{b}f(x)\,dx$ and referred to as the <strong>Riemann integral</strong> on $[a,b]$, for which we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=\lim_{\Delta\mathcal{P}\to 0}\mathcal{R}(f,\mathcal{P}),
\end{equation}
by which we mean that for every $\varepsilon&gt;0$ there exists $\delta&gt;0$ such that
\begin{equation}
\left\vert\mathcal{R}(f,\mathcal{P})-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon,
\end{equation}
for every tagged partition $\mathcal{P}$ with $\Delta(\mathcal{P})\leq\delta$.</p>

<h3 id="pc-func">Piecewise constant functions</h3>
<p>Let $[a,b]$ be an interval. a <strong>piecewise constant function</strong> $f:[a,b]\to\mathbb{R}$ is a function for which there exists a partition of $[a,b]$ into infinitely many intervals $I_1,\dots,I_n$ such that $f$ is equal to a constant $c_i$ on each of the intervals $I_i$. Then, the expression
\begin{equation}
\sum_{i=1}^{n}c_i\vert I_i\vert
\end{equation}
is independent of the choice of partition used to demonstrate the piecewise constant nature of $f$. We denote this quantity as $\text{p.c.}\int_{a}^{b}f(x)\,dx$, and refer it to as <strong>piecewise constant integral</strong> of $f$ on $[a,b]$.</p>

<p><strong>Proof</strong><br />
Consider two partitions of the interval $[a,b]$ into finitely many intervals $(I_i)_{i=1,\ldots,n}=I_1,\ldots,I_n$ and $(J_i)_{i=1,\ldots,m}=J_1,\ldots,J_m$ such that:
\begin{align}
f(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ f(x)&amp;=d_i,\hspace{1cm}\forall x\in J_i
\end{align}
Thus, we have that:
\begin{equation}
c_i=d_j,\hspace{1cm}\forall x\in\left(I_i\cap J_j\right)
\end{equation}
With this result, we have:
\begin{align}
\sum_{i=1}^{n}c_i\vert I_i\vert&amp;=\sum_{i=1}^{n}c_i\left\vert\bigcup_{j=1}^{m}\left(I_i\cap J_j\right)\right\vert \\ &amp;=\sum_{i=1}^{n}\sum_{j=1}^{m}c_i\left\vert I_i\cap J_j\right\vert \\ &amp;=\sum_{j=1}^{m}\sum_{i=1}^{n}d_j\left\vert I_i\cap J_j\right\vert \\ &amp;=\sum_{j=1}^{m}d_j\left\vert\bigcup_{i=1}^{n}\left(J_j\cap I_i\right)\right\vert \\ &amp;=\sum_{j=1}^{m}d_j\vert J_j\vert,
\end{align}
which claims the independence of the choices of partition of $f$.</p>

<h4 id="pc-int-properties">Basic properties of piecewise constant integral</h4>
<p>Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be piecewise constant functions. Then</p>
<ul id="number-list">
	<li>
		<b>Linearity</b>. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are piecewise constant functions, with
		\begin{align}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx&amp;=c\text{p.c.}\int_{a}^{b}f(x)\,dx \\\\ \text{p.c.}\int_{a}^{b}\left(f(x)+g(x)\right)\,dx&amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>. If $f\leq g$ pointwise, i.e., $f(x)\leq g(x),\forall x\in[a,b]$, then
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx\leq\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>. If $E$ is an elementary subset of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ (defined by setting $1_E(x)\doteq 1$ if $x\in E$ and 0 otherwise) is piecewise constant, and
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		<b>Linearity</b><br />
		For any $c\in\mathbb{R}$, we have:
		\begin{equation}
		\text{p.c.}\int_{a}^{b}cf(x)\,dx=\sum_{i=1}^{n}cc_i\vert I_i\vert=c\sum_{i=1}^{n}c_i\vert I_i\vert=c\text{p.c.}\int_{a}^{b}f(x)\,dx
		\end{equation}
		From the partitioning independence of piecewise constant functions, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{equation}
		f(x)=c_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		and
		\begin{equation}
		g(x)=d_i,\hspace{1cm}\forall x\in I_i,
		\end{equation}
		Thus, we have
		\begin{align}
		\text{p.c.}\int_{a}^{b}f(x)+g(x)\,dx&amp;=\sum_{i=1}^{n}\left(c_i+d_i\right)\vert I_i\vert \\ &amp;=\sum_{i=1}^{n}c_i\vert I_i\vert+\sum_{i=1}^{n}d_i\vert I_i\vert \\ &amp;=\text{p.c.}\int_{a}^{b}f(x)\,dx+\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b><br />
		Analogy to the above proof, there exists a partition of the interval $[a,b]$ into finitely many intervals, $I_1,\ldots,I_n$, such that
		\begin{align}
		f(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ g(x)&amp;=d_i,\hspace{1cm}\forall x\in I_i,
		\end{align}
		Since $f\leq g$ pointwise, in any interval $I_i$, we also have that $c_i=f(x)\leq g(x)=d_i$. Therefore,
		\begin{equation}
		\text{p.c.}\int_{a}^{b}f(x)\,dx=\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert=\text{p.c.}\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b><br />
		Since $E\subset[a,b]\subset\mathbb{R}$ is an elementary set, we can represent the elementary measure $m(E)$ of set $E$ as
		\begin{equation}
		m(E)=\sum_{i=1}^{n}\vert I_i\vert
		\end{equation}
		Therefore, for any $x\in I_i$ for $i=1,\ldots n$, we have that $1_E(x)=1$; and for any $x\in[b-a]\backslash E=\bigcup_{j=1}^{m}J_j$, we get that $1_E(x)=0$, which lets $1_E$ satisfy the condition of a piecewise constant function.<br />
		Moreover, we have that
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_E(x)\,dx=\sum_{i=1}^{n}1\vert I_i\vert+\sum_{j=1}^{m}0\vert J_j\vert=\sum_{i=1}^{n}\vert I_i\vert=m(E)
		\end{equation}
	</li>
</ul>

<h3 id="darboux-int">Darboux integral</h3>
<p>Let $[a,b]$ be an integral, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. The <strong>lower Darboux integral</strong> of $f$ on $[a,b]$, denoted as $\underline{\int_{a}^{b}}f(x)\,dx$, is defined as
\begin{equation}
\underline{\int_a^b}f(x)\,dx\doteq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx,
\end{equation}
where $g$ ranges over all piecewise constant functions that are pointwise bounded above by $f$ (the hypothesis that $f$ is bounded ensures that the supremum is over a non-empty set).</p>

<p>Similarly, we can define the <strong>upper Darboux integral</strong> of $f$ on $[a,b]$, denoted as $\overline{\int_a^b}f(x)\,dx$, as
\begin{equation}
\overline{\int_a^b}f(x)\,dx\doteq\inf_{h\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx
\end{equation}
It is easily seen that $\underline{\int_a^b}f(x)\,dx\leq\overline{\int_a^b}f(x)\,dx$. The equality holds when $f$ is <strong>Darboux integrable</strong>, and we refer to this quantity as <strong>Darboux integral</strong> of $f$ on $[a,b]$.</p>

<p>Note that the upper and lower Darboux integrals are related by
\begin{equation}
\overline{\int_a^b}-f(x)\,dx=-\underline{\int_a^b}f(x)\,dx
\end{equation}</p>

<h4 id="equiv-riemann-darboux-int">Equivalence of Riemann integral and Darboux integral</h4>
<p>Let $[a,b]$ be an interval, and $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff it is Darboux integrable, in which case the Riemann integrals and Darboux integrals are the same.</p>

<p><strong>Proof</strong></p>
<ul>
  <li>Given $f$ is Riemann integrable on $[a,b]$, we have that for any $\varepsilon&gt;0$, there exists a tagged partition $((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*))$ of $[a,b]$ with $x_i^*\in I_i$ such that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
\end{equation}
For each interval $I_i$, there exist an $x_i^{(1)}$ such that for any $\varepsilon&gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i^{(1)})&lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Thus, for any $\varepsilon&gt;0$ we obtain
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert&lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that for any $\varepsilon&gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon\label{eq:erdi.1}
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(1)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Combining with \eqref{eq:erdi.1}, we have that as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Moreover, we also have that
\begin{equation}
\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sup_{g\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the lower Darboux integral of $f$ on $[a,b]$. Thus,
\begin{equation}
\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx\label{eq:erdi.2}
\end{equation}
Similarly, applying the same procedure as above, we also have that on each $I_i$ there exists an $x_i^{(2)}$ such that for any $\varepsilon&gt;0$
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert-\sum_{n=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon
\end{equation}
Since $f$ is Riemann integrable on $[a,b]$, as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$, we have
\begin{equation}
\sum_{i=1}^{n}f(x_i^{(2)})\vert I_i\vert\to\int_{a}^{b}f(x)\,dx
\end{equation}
Therefore,
\begin{equation}
\sum_{n=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\to\int_{a}^{b}f(x)\,dx,
\end{equation}
as $\sup_{i=1,\ldots,n}\vert I_i\vert\to 0$. Additionally, we also have
\begin{equation}
\sum_{i=1}^{n}\sup_{x\in I_i}f(x)\vert I_i\vert\geq\inf_{h\geq f, \text{ piecewise constant}}\text{p.c.}\int_{a}^{b}h(x)\,dx=\overline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which is the upper Darboux integral of $f$ on $[a,b]$. And hence
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\label{eq:erdi.3}
\end{equation}
From \eqref{eq:erdi.2} and \eqref{eq:erdi.3}, we end up with
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx\leq\int_{a}^{b}f(x)\,dx\leq\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which happens iff
\begin{equation}
\overline{\int_{a}^{b}}f(x)\,dx=\int_{a}^{b}f(x)\,dx=\underline{\int_{a}^{b}}f(x)\,dx,
\end{equation}
which claims that $f$ is Darboux integrable on $[a,b]$, with the Darboux integral is exactly the Riemann integral $\int_{a}^{b}f(x)\,dx$.</li>
  <li>Given $f$ is Darboux integrable on $[a,b]$, we have that the upper and lower Darboux integrals are equal, and are equal to the Darboux integral of $f$ on $[a,b]$ which we denote as $\text{d.}\int_{a}^{b}f(x)\,dx\in\mathbb{R}$.
\begin{equation}
\underline{\int_a^b}f(x)\,dx=\overline{\int_a^b}f(x)\,dx=\text{d.}\int_{a}^{b}f(x)\,dx
\end{equation}
By definition of the lower Darboux integral, there exists a piecewise constant function $g(x)$ bounded above by $f$ (i.e., $g\leq f$ piecewise), such that for any $\varepsilon&gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}g(x)\,dx&gt;\underline{\int_{a}^{b}}f(x)\,dx-\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon\label{eq:erdi.4}
\end{equation}
Likewise, by definition of the upper Darboux integral, there exists a piecewise constant function $h(x)$ bounded below by $f$ (i.e., $h\geq f$ piecewise), such that for any $\varepsilon&gt;0$
\begin{equation}
\text{p.c.}\int_{a}^{b}h(x)\,dx&lt;\overline{\int_{a}^{b}}f(x)\,dx+\varepsilon=\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon\label{eq:erdi.5}
\end{equation}
From the independence of choice of partition of piecewise constant functions $g$ and $h$, there exists a partition $I_1,\ldots,I_n$ such that
\begin{align}
g(x)&amp;=c_i,\hspace{1cm}\forall x\in I_i, \\ h(x)&amp;=d_i,\hspace{1cm}\forall x\in I_i
\end{align}
and
\begin{align}
\text{p.c.}\int_{a}^{b}g(x)\,dx&amp;=\sum_{i=1}^{n}c_i\vert I_i\vert,\label{eq:erdi.6} \\ \text{p.c.}\int_{a}^{b}h(x)\,dx&amp;=\sum_{i=1}^{n}d_i\vert I_i\vert,\label{eq:erdi.7}
\end{align}
then it follows immediately that $c_i\leq d_i$. And since $g\leq f\leq h$ piecewise, on each interval $I_i$, we can find a $x_i^*$ such that $c_i\leq f(x_i^*)\leq d_i$. Additionally, combining with \eqref{eq:erdi.4}, \eqref{eq:erdi.5}, \eqref{eq:erdi.6} and \eqref{eq:erdi.7}, we have that for any $\varepsilon&gt;0$
\begin{equation}
\text{d.}\int_{a}^{b}f(x)\,dx-\varepsilon&lt;\sum_{i=1}^{n}c_i\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert\leq\sum_{i=1}^{n}d_i\vert I_i\vert&lt;\text{d.}\int_{a}^{b}f(x)\,dx+\varepsilon
\end{equation}
Therefore, for any $\varepsilon&gt;0$, we have
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\text{d.}\int_{a}^{b}f(x)\,dx\right\vert&lt;\varepsilon,
\end{equation}
which claims that $f$ is Riemann integrable on $[a,b]$ with $\text{d.}\int_{a}^{b}f(x)\,dx$ is the Riemann integral of $f$.</li>
</ul>

<p><strong>Example</strong><br />
Any continuous function $f:[a,b]\to\mathbb{R}$ is Riemann integrable. More generally, any bounded, <strong>piecewise continuous function</strong><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> $f:[a,b]\to\mathbb{R}$ is Riemann integrable.</p>

<p><strong>Solution</strong><br />
Consider a partition of piecewise continuous f on $[a,b]$ into finitely many intervals $I_1,\ldots,I_n$. Using the procedure that we used for the above proof, we have that on each interval $I_i$, there exists an $x_i$ such that for any $\varepsilon&gt;0$
\begin{equation}
\inf_{x\in I_i}f(x)\leq f(x_i)&lt;\inf_{x\in I_i}f(x)+\frac{\varepsilon}{n}
\end{equation}
Hence,
\begin{equation}
\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\leq\sum_{i=1}^{n}f(x_i)\vert I_i\vert&lt;\sum_{i=1}^{n}\inf_{x\in I_i}f(x)+\varepsilon,
\end{equation}
which implies that
\begin{equation}
\left\vert\sum_{i=1}^{n}f(x_i)\vert I_i\vert-\sum_{i=1}^{n}\inf_{x\in I_i}f(x)\vert I_i\vert\right\vert&lt;\varepsilon,
\end{equation}
which implies that $f$ is Riemann integrable on $[a,b]$.</p>

<h3 id="riemann-int-properties">Basic properties of Riemann integral</h3>
<p>Let $[a,b]$ be an interval, and let $f,g:[a,b]\to\mathbb{R}$ be Riemann integrable. We then have that</p>
<ul id="number-list">
	<li>
		<b>Linearity</b>. For any $c\in\mathbb{R}$, $cf$ and $f+g$ are Riemann integrable, with
		\begin{align}
		\int_{a}^{b}cf(x)\,dx&amp;=c\int_{a}^{b}f(x)\,dx \\\\ \int_{a}^{b}\big(f(x)+g(x)\big)\,dx&amp;=\int_{a}^{b}f(x)\,dx+\int_{a}^{b}g(x)\,dx
		\end{align}
	</li>
	<li>
		<b>Monotonicity</b>. If $f\leq g$ pointwise, then
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>. If $E$ is a Jordan measurable of $[a,b]$, then the indicator function $1_E:[a,b]\to\mathbb{R}$ is Riemann integrable, and
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>

<p><strong>Proof</strong></p>
<ul id="number-list">
	<li>
		<b>Linearity</b>.
		<ul>
			<li>
				Given $f$ Riemann integrable on $[a,b]$, we have that there exists a tagged partition $\mathcal{P}=((I_1,\ldots,I_n),(x_1^*,\ldots,x_n^*));(x_i^*\in I_i)$ of $[a,b]$ such that for any $\varepsilon&gt;0$, we have
				\begin{equation}
				\left\vert\sum_{i=1}^{n}f(x_i^*)\vert I_i\vert-\int_{a}^{b}f(x)\,dx\right\vert\leq\varepsilon
				\end{equation}
				Thus, for any $c\in\mathbb{R}$
				\begin{equation}
				\left\vert\sum_{i=1}^{n}cf(x_i^*)\vert I_i\vert-\int_{a}^{b}cf(x)\,dx\right\vert\leq\vert c\vert\varepsilon=\varepsilon',
				\end{equation}
				where $\varepsilon'&gt;0$ arbitrarily. This implies that $cf$ is Riemann integrable on $[a,b]$ with Riemann integral $\int_{a}^{b}cf(x)\,dx=c\int_{a}^{b}f(x)\,dx$.
			</li>
			<li>
				Given $f$ Riemann integrable on $[a,b]$, then $f$ is also Darboux integrable on $[a,b]$, which means
				\begin{align}
				\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx&amp;=\inf_{f_2\geq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)\,dx \\ &amp;\hspace{1cm}=\int_{a}^{b}f(x)\,dx\label{eq:rip.1}
				\end{align}
				Similarly, $g$ Riemann integrable on $[a,b]$ implies that $g$ is also Darboux integrable, or in particular
				\begin{align}
				\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx&amp;=\inf_{g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_2(x)\,dx \\ &amp;\hspace{1cm}=\int_{a}^{b}g(x)\,dx\label{eq:rip.2}
				\end{align}
				By the linearity property of piecewise constant functions, combined with \eqref{eq:rip.1} and \eqref{eq:rip.2}, we obtain
				\begin{align}
				&amp;\sup_{f_1\leq f,g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)+g_1(x)\,dx \\ &amp;\hspace{2cm}=\inf_{f_2\geq f,g_2\geq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_2(x)+g_2(x)\,dx \\ &amp;\hspace{2cm}=\int_{a}^{b}f(x)+g(x)\,dx,
				\end{align}
				which claims the Riemann integrability of $f+g$ on $[a,b]$.
			</li>
		</ul>
	</li>
	<li>
		<b>Monotonicity</b>.<br />
		Given $f$ and $g$, we obtain two consequential equations \eqref{eq:rip.1} and \eqref{eq:rip.2}. And since $f\leq g$ pointwise we have that
		\begin{equation}
		\sup_{f_1\leq f,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}f_1(x)\,dx\leq\sup_{g_1\leq g,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}g_1(x)\,dx
		\end{equation}
		or
		\begin{equation}
		\int_{a}^{b}f(x)\,dx\leq\int_{a}^{b}g(x)\,dx
		\end{equation}
	</li>
	<li>
		<b>Indicator</b>.<br />
		Given $E\subset [a,b]$ is Jordan measurable, we have
		\begin{equation}
		\sup_{A\subset E,A\text{ elementary}}m(A)=\inf_{B\supset E,B\text{ elementary}}m(B)=m(E)\label{eq:rip.3}
		\end{equation}
		Recall that we have proved that for any elementary set $E'\subset[a,b]$, the indicator function $1_{E'}:[a,b]\to\mathbb{R}$ is also piecewise constant with
		\begin{equation}
		\text{p.c.}\int_{a}^{b}1_{E'}(x)\,dx=m(E')
		\end{equation}
		Moreover for any $A\subset E$, we have $1_A(x)\leq 1_E(x)$; and for any $B\supset E$, we have $1_B(x)\geq 1_E(x)$. Therefore the lower Darboux integral of $1_E$ on $[a,b]$ can be defined as
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\sup_{1_A\leq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_A(x)\,dx=\sup_{A\subset E,A\text{ elementary}}m(A)\label{eq:rip.4}
		\end{equation}
		And the upper Darboux integral of $1_E$ on $[a,b]$ can also be defined as
		\begin{equation}
		\overline{\int_{a}^{b}}1_E(x)\,dx=\inf_{1_B\geq 1_E,\text{ piecewise constant}}\text{p.c.}\int_{a}^{b}1_B(x)\,dx=\inf_{B\supset E,B\text{ elementary}}m(B)\label{eq:rip.5}
		\end{equation}
		Combine \eqref{eq:rip.3}, \eqref{eq:rip.4} and \eqref{eq:rip.5}, we have
		\begin{equation}
		\underline{\int_{a}^{b}}1_E(x)\,dx=\overline{\int_{a}^{b}}1_E(x)\,dx=m(E),
		\end{equation}
		which means $1_E$ is Darboux integrable on $[a,b]$ with the Darboux integrable $m(E)$. By the equivalence of Riemann and Darboux integral, $1_E$ is also Riemann integrable on $[a,b]$ with the Riemann integral
		\begin{equation}
		\int_{a}^{b}1_E(x)\,dx=m(E)
		\end{equation}
	</li>
</ul>
<p>These properties uniquely define the Riemann integral, in the sense that the functional $f\mapsto\int_{a}^{b}f(x)\,dx$ is the only map from the space of Riemann integrable functions on $[a,b]$ to $\mathbb{R}$ which obeys all of these above properties.</p>

<h3 id="riemann-int-area-interpret">Area interpretation of the Riemann integral</h3>
<p>Let $[a,b]$ be an interval, and let $f:[a,b]\to\mathbb{R}$ be a bounded function. Then $f$ is Riemann integrable iff the sets $E_+\doteq\{(x,t):x\in[a,b];0\leq t\leq f(x)\}$ and $E_-\doteq\{(x,t):x\in[a,b];f(x)\leq t\leq 0\}$ are both Jordan measurable in $R^2$, in which case we have
\begin{equation}
\int_{a}^{b}f(x)\,dx=m^2(E_+)-m^2(E_-),
\end{equation}
where $m^2$ denotes two-dimensional Jordan measure.</p>

<p><strong>Proof</strong></p>

<h2 id="references">References</h2>
<p>[1] <span id="taos-book">Terence Tao. <a href="https://terrytao.wordpress.com/books/an-introduction-to-measure-theory/">An introduction to measure theory</a>. Graduate Studies in Mathematics, vol. 126, 2011.</span></p>

<p>[2] <span id="steins-book">Elias M. Stein &amp; Rami Shakarchi. <a href="http://www.cmat.edu.uy/~mordecki/courses/medida2013/book.pdf">Real Analysis: Measure Theory, Integration, and Hilbert Spaces</a>. 2007</span></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A function $f$ is said to be <strong>uniformly continuous</strong> if for any real $\varepsilon&gt;0$, there exists a real number $\delta&gt;0$ such that for any $x, y$ with $d_1(x, y)&lt;\delta$, we also have
\begin{equation*}
d_2(f(x),f(y))&lt;\varepsilon
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A function $f:[a,b]\to\mathbb{R}$ is <strong>piecewise continuous</strong> if we can partition $[a,b]$ into finitely many intervals, such that $f$ is continuous on each interval. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="jordan-measure" /><category term="riemann-integral" /><summary type="html"><![CDATA[Note on measure theory part 1]]></summary></entry><entry><title type="html">The exponential family</title><link href="http://localhost:4000/2022/04/04/exponential-family.html" rel="alternate" type="text/html" title="The exponential family" /><published>2022-04-04T14:00:00+07:00</published><updated>2022-04-04T14:00:00+07:00</updated><id>http://localhost:4000/2022/04/04/exponential-family</id><content type="html" xml:base="http://localhost:4000/2022/04/04/exponential-family.html"><![CDATA[<blockquote>
  <p>A note on the exponential family.</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#exp-fam">The exponential family</a></li>
  <li><a href="#examples">Examples</a>
    <ul>
      <li><a href="#bern">Bernoulli distribution</a></li>
      <li><a href="#bin">Binomial distribution</a></li>
      <li><a href="#mult">Multinomial distribution</a></li>
      <li><a href="#pois">Poisson distribution</a></li>
      <li><a href="#gauss">Gaussian distribution</a></li>
      <li><a href="#mvn">Multivariate Normal distribution</a></li>
    </ul>
  </li>
  <li><a href="#cvxt">Convexity</a></li>
  <li><a href="mmt-suff-stat">Moments of sufficient statistic</a>
    <ul>
      <li><a href="#mean-var">Means, variances</a></li>
      <li><a href="#mgf">Moment generating functions</a></li>
      <li><a href="#cgf">Cumulant generating functions</a></li>
      <li><a href="#cumulants">Cumulants</a></li>
    </ul>
  </li>
  <li><a href="#sufficiency">Sufficiency</a></li>
  <li><a href="#mle">Maximum likelihood estimates</a></li>
  <li><a href="conj-prior">Conjugate priors</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="exp-fam">The exponential family</h2>
<p>The <strong>exponential family</strong> of distributions is defined as family of distributions of form
\begin{equation}
p(x;\eta)=h(x)\exp\Big[\eta^\text{T}T(x)-A(\eta)\Big],\label{eq:ef.1}
\end{equation}
where</p>
<ul>
  <li>$\eta$ is known as the <strong>natural parameter</strong>, or <strong>canonical parameter</strong>,</li>
  <li>$T(X)$ is referred to as a <strong>sufficient statistic</strong>,</li>
  <li>$A(\eta)$ is called the <strong>cumulant function</strong>, which can be view as the logarithm of a normalization factor since integrating \eqref{eq:ef.1} w.r.t the measure $\nu$ gives us
\begin{equation}
A(\eta)=\log\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx),\label{eq:ef.2}
\end{equation}
This also implies that $A(\eta)$ will be determined once we have specified $\nu,T(x)$ and $h(x)$.</li>
</ul>

<p>The set of parameters $\eta$ for which the integral in \eqref{eq:ef.2} is finite is known as the <strong>natural parameter space</strong>
\begin{equation}
N=\left\{\eta:\int h(x)\exp\left(\eta^\text{T}T(x)\right)\nu(dx)&lt;\infty\right\}
\end{equation}
which explains why $\eta$ is also referred as <strong>natural parameter</strong>. If $N$ is an non-empty open set, the exponential families are said to be <strong>regular</strong>.</p>

<p>An exponential family is known as <strong>minimal</strong> if there are no linear constraints among the components of $\eta$ nor are there linear constraints among the components of $T(x)$.</p>

<h2 id="examples">Examples</h2>
<p>Each particular choice of $\nu$, $T$ and $h$ defines a family (or set) of distributions that is parameterized by $\eta$. As we vary $\eta$, we then get different distributions within this family.</p>

<h3 id="bern">Bernoulli distribution</h3>
<p>The probability mass function (i.e., the density function w.r.t counting measure) of a Bernoulli random variable $X$, denoted as $X\sim\text{Bern}(\pi)$, is given by
\begin{align}
p(x;\pi)&amp;=\pi^x(1-\pi)^{1-x} \\ &amp;=\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &amp;=\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which can be written in the form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;=\frac{\pi}{1-\pi} \\ T(x)&amp;=x \\ A(\eta)&amp;=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&amp;=1
\end{align}
Notice that the relationship between $\eta$ and $\pi$ is invertible since
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}},
\end{equation}
which is the <strong>sigmoid function</strong>.</p>

<h3 id="bin">Binomial distribution</h3>
<p>The probability mass function of a Binomial random variable $X$, denoted as $X\sim\text{Bin}(N,\pi)$, is defined as
\begin{align}
p(x;N,\pi)&amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\pi^{x}(1-\pi)^{1-x} \\ &amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\exp\big[x\log\pi+(1-x)\log(1-\pi)\big] \\ &amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)\exp\left[\log\left(\frac{\pi}{1-\pi}\right)x+\log(1-\pi)\right],
\end{align}
which is in form of an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;=\frac{\pi}{1-\pi} \\ T(x)&amp;=x \\ A(\eta)&amp;=-\log(1-\pi)=\log(1+e^{\eta}) \\ h(x)&amp;=\left(\begin{matrix}N \\ x\end{matrix}\right)
\end{align}
Similar to the Bernoulli case, we also have the invertible relationship between $\eta$ and $\pi$ as
\begin{equation}
\pi=\frac{1}{1+e^{-\eta}}
\end{equation}</p>

<h3 id="pois">Poisson distribution</h3>
<p>The probability mass function of a Poisson random variable $X$, denoted as $X\sim\text{Pois}(\lambda)$, is given as
\begin{align}
p(x;\lambda)&amp;=\frac{\lambda^x e^{-\lambda}}{x!} \\ &amp;=\frac{1}{x!}\exp\left(x\log\lambda-\lambda\right),
\end{align}
which is also able to be written as an exponential family distribution \eqref{eq:ef.1} with
\begin{align}
\eta&amp;=\log\lambda \\ T(x)&amp;=x \\ A(\eta)&amp;=\lambda=e^{\eta} \\ h(x)&amp;=\frac{1}{x!}
\end{align}
Analogy to Bernoulli distribution, we also have that
\begin{equation}
\lambda=e^{\eta}
\end{equation}</p>

<h3 id="gauss">Gaussian distribution</h3>
<p>The (univariate) Gaussian density of a random variable $X$, denoted as $X\sim\mathcal{N}(\mu,\sigma^2)$, is given by
\begin{align}
p(x;\mu,\sigma^2)&amp;=\frac{1}{\sqrt{2\pi}\sigma}\exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] \\ &amp;=\frac{1}{\sqrt{2\pi}}\exp\left[\frac{\mu}{\sigma^2}x-\frac{1}{2\sigma^2}x^2-\frac{1}{2\sigma^2}\mu^2-\log\sigma\right],
\end{align}
which allows us to write it as an instance of the exponential family with
\begin{align}
\eta&amp;=\left[\begin{matrix}\mu/\sigma^2 \\ -1/2\sigma^2\end{matrix}\right] \\ T(x)&amp;=\left[\begin{matrix}x\\ x^2\end{matrix}\right] \\ A(\eta)&amp;=\frac{\mu^2}{2\sigma^2}+\log\sigma=-\frac{\eta_1^2}{4\eta_2}-\frac{1}{2}\log(-2\eta_2) \\ h(x)&amp;=\frac{1}{\sqrt{2\pi}}
\end{align}</p>

<h3 id="mult">Multinomial distribution</h3>
<p>Let $\mathbf{X}=(X_1,\ldots,X_K)$ be the collection of $K$ random variable in which $X_k$ denotes the number of times the $k$-th event occurs in a set of $N$ independent trials. And let $\mathbf{\pi}=(\pi_1,\ldots,\pi_K)$ with $\sum_{k=1}^{K}\pi_k=1$ correspondingly represents the probability of occurring of each event within each trials.</p>

<p>Then $\mathbf{X}$ is said to have Multinomial distribution, denoted as $\mathbf{X}\sim\text{Mult}_K(N,\boldsymbol{\pi})$, if its probability mass function is given as with $\sum_{k=1}^{K}x_k=1$
\begin{align}
p(\mathbf{x};\boldsymbol{\pi},N,K)&amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\pi_1^{x_1}\pi_2^{x_2}\ldots\pi_n^{x_n} \\ &amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right)\label{eq:m.1}
\end{align}
It is noticeable that the above equation is not minimal, since there exists a linear constraint between the components of $T(\mathbf{x})$, which is
\begin{equation}
\sum_{k=1}^{K}x_k=1
\end{equation}
In order to remove this constraint, we substitute $1-\sum_{k=1}^{K-1}x_k$ to $x_K$ , which lets \eqref{eq:m.1} be written by
\begin{align}
\hspace{-0.5cm}p(\mathbf{x};\boldsymbol{\pi},N,K)&amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left(\sum_{k=1}^{K}x_k\log\pi_k\right) \\ &amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{k=1}^{K-1}x_k\log\pi_k+\left(1-\sum_{k=1}^{K-1}x_k\right)\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right] \\ &amp;=\frac{N!}{x_1!x_2!\ldots x_K!}\exp\left[\sum_{i=1}^{K-1}\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)x_i+\log\left(1-\sum_{k=1}^{K-1}\pi_k\right)\right]\label{eq:m.2}
\end{align}
With this representation, and also for convenience, for $i=1,\ldots,K$ we continue by letting
\begin{equation}
\eta_i=\log\left(\frac{\pi_i}{1-\sum_{k=1}^{K-1}\pi_k}\right)=\log\left(\frac{\pi_i}{\pi_K}\right)\label{eq:m.3}
\end{equation}
Take the exponential of both sides and summing over $K$, we have
\begin{equation}
\sum_{i=1}^{K}e^{\eta_i}=\frac{\sum_{i=1}^{K}\pi_i}{\pi_K}=\frac{1}{\pi_K}\label{eq:m.4}
\end{equation}
From this result, we have that the Multinomial distribution \eqref{eq:m.2} is therefore also a member of the exponential family with
\begin{align}
\eta&amp;=\left[\begin{matrix}\log\left(\pi_1/\pi_K\right) \\ \vdots \\ \log\left(\pi_K/\pi_K\right)\end{matrix}\right] \\ T(\mathbf{x})&amp;=\left[\begin{matrix}x_1,\ldots,x_K\end{matrix}\right]^\text{T} \\ A(\eta)&amp;=-\log\left(1-\sum_{i=1}^{K-1}\pi_i\right)=-\log(\pi_K)=\log\left(\sum_{k=1}^{K}e^{\eta_k}\right) \\ h(\mathbf{x})&amp;=\frac{N!}{x_1!x_2!\ldots x_K!}
\end{align}
Additionally, substituting the result \eqref{eq:m.4} into \eqref{eq:m.3} gives us for $i=1,\ldots,K$
\begin{equation}
\eta_i=\log\left(\pi_i\sum_{k=1}^{K}e^{\eta_k}\right),
\end{equation}
or we can express $\boldsymbol{\pi}$ in terms of $\eta$ by
\begin{equation}
\pi_i=\frac{e^{\eta_i}}{\sum_{k=1}^{K}e^{\eta_k}},
\end{equation}
which is the <strong>softmax function</strong>.</p>

<h3 id="mvn">Multivariate Normal distribution</h3>

<h2 id="cvxt">Convexity</h2>
<p><strong>Theorem</strong><br />
The natural space $N$ is a convex set and the cumulant function $A(\eta)$ is a convex function. If the family is minimal, then $A(\eta)$ is strictly convex.</p>

<p><strong>Proof</strong><br />
Let $\eta_1,\eta_2\in N$, thus from \eqref{eq:ef.2}, we have that
\begin{align}
\exp\big(A(\eta_1)\big)&amp;=A_1, \\ \exp\big(A(\eta_2)\big)&amp;=A_2
\end{align}
where $A_1,A_2$ are finite.</p>

<p>To prove that $N$ is convex, we need to show that for any $\eta=\lambda\eta_1+(1-\lambda)\eta_2$ for $0\lt\lambda\lt 1$, we also have $\eta\in N$. From \eqref{eq:ef.2}, and by <strong>Hölder’s inequality</strong><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, we have
\begin{align}
\exp\big(A(\eta)\big)&amp;=\int h(x)\exp\big(\eta^\text{T}T(x)\big)\nu(dx) \\ &amp;=\int h(x)\exp\Big[\big(\lambda\eta_1+(1-\lambda)\eta_2\big)^\text{T}T(x)\Big]\nu(dx) \\ &amp;=\int \Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda}\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}\nu(dx) \\ &amp;\leq\Bigg[\int h(x)\exp\big(\eta_1^\text{T}T(x)\big)\nu(dx)\Bigg]^\lambda\Bigg[\int h(x)\exp\big(\eta_2^\text{T}T(x)\big)\nu(dx)\Bigg]^{1-\lambda} \\ &amp;=\Big[\exp\big(A(\eta_1)\big)\Big]^\lambda\Big[\exp\big(A(\eta_2)\big)\Big]^{1-\lambda} \\ &amp;=A_1^\lambda A_2^{1-\lambda},\label{eq:c.1}
\end{align}
which proves that $A(\eta)$ is finite, or $\eta\in N$.</p>

<p>Moreover, taking logarithm of both sides of \eqref{eq:c.1} gives us
\begin{equation}
\lambda A(\eta_1)+(1-\lambda)A(\eta_2)\geq A(\eta)=A\big(\lambda\eta_1+(1-\lambda)\eta_2\big),
\end{equation}
which also claims the convexity of $A(\eta)$.</p>

<p>By Hölder’s inequality, the equality in \eqref{eq:c.1} holds when
\begin{equation}
\Big[h(x)\exp\big(\eta_2^\text{T}T(x)\big)\Big]^{1-\lambda}=c\Big[h(x)\exp\big(\eta_1^\text{T}T(x)\big)\Big]^{\lambda(1/\lambda-1)}
\end{equation}
or
\begin{equation}
\exp\big(\eta_2^\text{T}T(x)\big)=c\exp\big(\eta_1^\text{T}T(x)\big),
\end{equation}
and therefore
\begin{equation}
(\eta_2-\eta_1)^\text{T}T(x)=\log c,
\end{equation}
which is not minimal since $\eta_1,\eta_2$ are taken arbitrarily.</p>

<h2 id="mmt-suff-stat">Moments of sufficient statistic</h2>
<p>In this section, we will see how the moments of the sufficient statistic $T(X)$ can be calculated from the cumulant function $A(\eta)$. In more specifically, the first moment (mean) and the second central moment (variance) of $T(X)$ are exactly the first and the second <strong>cumulants</strong>.</p>

<h3 id="mean-var">Means, variances</h3>
<p>Let us first consider the first derivative of the cumulant function $A(\eta)$. By the <strong>dominated convergence theorem</strong>, we have
\begin{align}
\frac{\partial A(\eta)}{\partial\eta^\text{T}}&amp;=\frac{\partial}{\partial\eta^\text{T}}\log\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx) \\ &amp;=\frac{\int T(x)\exp\big(\eta^\text{T}(x)\big)h(x)\nu(dx)}{\int\exp\big(\eta^\text{T}T(x)\big)h(x)\nu(dx)} \\ &amp;=\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx)\label{eq:mv.1} \\ &amp;=\int T(x)p(x;\eta)\nu(dx) \\ &amp;=\mathbb{E}[T(X)],
\end{align}
which is the mean of the sufficient statistic $T(x)$.</p>

<p>Moreover, taking the second derivative of cumulant function by continuing with the result \eqref{eq:mv.1}, we have
\begin{align}
\frac{\partial^2 A(\eta)}{\partial\eta\partial\eta^\text{T}}&amp;=\frac{\partial}{\partial\eta^\text{T}}\int T(x)\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;=\int T(x)\left(T(x)-\frac{\partial}{\partial\eta^\text{T}}A(\eta)\right)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;=\int T(x)\big(T(x)-E(T(X))\big)^\text{T}\exp\big(\eta^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;=\mathbb{E}\left[T(X)T(X)^\text{T}\right]-\mathbb{E}[T(X)]\mathbb{E}[T(X)]^\text{T} \\ &amp;=\text{Var}[T(X)],
\end{align}
which is the variance (or the covariance matrix in the multivariate case) of the sufficient statistic $T(X)$.</p>

<h3 id="mgf">Moment generating functions</h3>
<p>The <strong>moment generating function</strong> (or <strong>MGF</strong>) of a random variable $X$, denoted as $M(t)$, is given by
\begin{equation}
M(t)=\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.</p>

<p>The MGF of the sufficient statistic $T(X)$ then can be computed as
\begin{align}
M_{T(X)}(t)&amp;=\mathbb{E}(e^{t^\text{T}T(X)}) \\ &amp;=\int \exp\big((\eta+t)^\text{T}T(x)-A(\eta)\big)h(x)\nu(dx) \\ &amp;=\exp\big(A(\eta+t)-A(\eta)\big)\label{eq:mgf.1}
\end{align}</p>

<h3 id="cgf">Cumulant generating functions</h3>
<p>The <strong>cumulant generating function</strong> (or <strong>CGF</strong>) of a random variable $X$, denoted by $K(t)$, is given as
\begin{equation}
K(t)=\log M(t)=\log\mathbb{E}(e^{t^\text{T}X}),
\end{equation}
for all values of $t$ for which the expectation exists.</p>

<p>From the MGF of $T(X)$ in \eqref{eq:mgf.1}, the CGF of the sufficient statistic $T(X)$ therefore can be calculated by
\begin{equation}
K_{T(X)}(t)=\log M_{T(X)}(t)=A(\eta+t)-A(\eta)
\end{equation}</p>

<h3 id="cumulants">Cumulants</h3>
<p>The $k$-th <strong>cumulant</strong> of a random variable $X$ is defined to be the $k$-th derivative of $K_{X}(t)$ at $0$, i.e.,
\begin{equation}
c_k=K^{(k)}(0)
\end{equation}</p>

<p>Thus, the mean of $T(X)$ is exactly the first cumulant, while the variance is the second cumulant of $T(X)$.</p>

<h2 id="sufficiency">Sufficiency</h2>

<h2 id="mle">Maximum likelihood estimates</h2>
<p>Consider an i.i.d data set $\mathcal{D}=\{x_1,\ldots,x_N\}$, the likelihood function is then given by
\begin{align}
L(\eta)=p(\mathbf{X}\vert\eta)&amp;=\prod_{n=1}^{N}p(x_n\vert\eta) \\ &amp;=\prod_{n=1}^{N}h(x_n)\exp\big[\eta^\text{T}T(x_n)-A(\eta)\big] \\ &amp;=\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right]\label{eq:mle.1}
\end{align}
Taking the logarithm of both sides gives us the log likelihood as
\begin{equation}
\ell(\eta)=\log L(\eta)=\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)
\end{equation}
Consider the gradient of the log likelihood w.r.t $\eta$, we have
\begin{align}
\nabla_\eta\ell(\eta)&amp;=\nabla_\eta\left[\log\left(\prod_{n=1}^{N}h(x_n)\right)+\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &amp;=\sum_{n=1}^{N}T(x_n)-N\nabla_\eta A(\eta)
\end{align}
Setting the gradient to zero, we have the value of $\eta$ that maximizes the likelihood, or maximum likelihood estimation for $\eta$, denoted as $\eta_\text{ML}$ satisfies
\begin{equation}
\nabla_{\eta}A(\eta_\text{ML})=\frac{1}{N}\sum_{n=1}^{N}T(x_n)
\end{equation}</p>

<h2 id="conj-prior">Conjugate priors</h2>
<p>Given a probability distribution $p(x\vert\eta)$, its prior $p(\eta)$ is said to be <strong>conjugate</strong> to the likelihood function if the prior and the posterior has the same functional form. The prior distribution in this case is also referred as <strong>conjugate prior</strong>.</p>

<p>For any member of the exponential family, there exists a conjugate prior that can be written in form
\begin{equation}
p(\eta\vert\mathcal{X},\theta)=f(\mathcal{X},\theta)\exp(\eta^\text{T}\mathcal{X}-\theta A(\eta)),\label{eq:cp.1}
\end{equation}
where $\theta&gt;0$ and $\mathcal{X}$ are hyperparameters.</p>

<p>By Bayes’ rule, and with the likelihood function as given in \eqref{eq:mle.1}, the posterior distribution can be computed as
\begin{align}
&amp;\hspace{0.7cm}p(\eta\vert\mathbf{X},\mathcal{X},\theta) \\ &amp;\propto p(\eta\vert\mathcal{X},\theta)p(\mathbf{X}\vert\eta) \\ &amp;=f(\mathcal{X},\theta)\exp\big(\eta^\text{T}\mathcal{X}-\theta A(\eta)\big)\left(\prod_{n=1}^{N}h(x_n)\right)\exp\left[\eta^\text{T}\left(\sum_{n=1}^{N}T(x_n)\right)-N A(\eta)\right] \\ &amp;\propto\exp\left[\eta^\text{T}\left(\mathcal{X}+\sum_{n=1}^{N}T(x_n)\right)-(\theta+N)A(\eta)\right],
\end{align}
which is in the same form as \eqref{eq:cp.1} and therefore claims the conjugacy.</p>

<h2 id="references">References</h2>
<p>[1] M. Jordan. <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter8.pdf">The Exponential Family: Basics</a>. 2009.</p>

<p>[2] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a>.</p>

<p>[3] Weisstein, Eric W. <a href="https://mathworld.wolfram.com/HoeldersInequalities.html">Hölder’s Inequalities</a> From MathWorld–A Wolfram Web Resource.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Let $p,q&gt;1$ such that
\begin{equation*}
\frac{1}{p}+\frac{1}{q}=1
\end{equation*}
The <strong>Hölder’s inequality</strong> for integrals states that
\begin{equation*}
\int_a^b\vert f(x)g(x)\vert\,dx\leq\left(\int_a^b\vert f(x)\vert\,dx\right)^{1/p}\left(\int_a^b\vert g(x)\vert\,dx\right)^{1/q}
\end{equation*}
The equality holds with
\begin{equation*}
\vert g(x)\vert=c\vert f(x)\vert^{p-1}
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="probability-statistics" /><category term="exponential-family" /><summary type="html"><![CDATA[Note on exponential family]]></summary></entry><entry><title type="html">Convex sets, convex functions</title><link href="http://localhost:4000/2021/12/02/cvx-sets-funcs.html" rel="alternate" type="text/html" title="Convex sets, convex functions" /><published>2021-12-02T13:03:00+07:00</published><updated>2021-12-02T13:03:00+07:00</updated><id>http://localhost:4000/2021/12/02/cvx-sets-funcs</id><content type="html" xml:base="http://localhost:4000/2021/12/02/cvx-sets-funcs.html"><![CDATA[<blockquote>
  <p>A note on convex sets, convex functions</p>
</blockquote>

<!-- excerpt-end -->

<ul>
  <li><a href="#cvx-sets">Convex sets</a>
    <ul>
      <li><a href="#aff-cvx-sets">Affine &amp; convex sets</a>
        <ul>
          <li><a href="#aff-sets">Affine sets</a></li>
          <li><a href="#aff-dim-rel-int">Affine dimension, relative interior</a></li>
          <li><a href="#cvx-sets-def">Convex sets</a></li>
          <li><a href="#cones">Cones</a></li>
        </ul>
      </li>
      <li><a href="#cvx-sets-eg">Examples</a>
        <ul>
          <li><a href="#hyperplane-halfspaces">Hyperplanes, halfspaces</a></li>
          <li><a href="#balls-ellips-cones">Balls, ellipsoids, norm cones</a>
            <ul>
              <li><a href="#balls">Balls</a></li>
              <li><a href="#ellips">Ellipsoids</a></li>
              <li><a href="#norm-cones">Norm cones</a></li>
            </ul>
          </li>
          <li><a href="#polyhedra">Polyhedra</a>
            <ul>
              <li><a href="#non-neg-orthant">Nonnegative orthant</a></li>
              <li><a href="#simplex">Simplex</a></li>
            </ul>
          </li>
          <li><a href="#psd-cone">Positive semi-definite cone</a></li>
        </ul>
      </li>
      <li><a href="#operations-sets">Operations that preserve convexity</a>
        <ul>
          <li><a href="#intersect">Intersection</a></li>
          <li><a href="#aff-funcs">Affine functions</a></li>
          <li><a href="#lin-frac-persp-funcs">Linear-fractional, perspective functions</a>
            <ul>
              <li><a href="#persp-funcs">Perspective functions</a></li>
              <li><a href="#lin-frac-funcs">Linear-fractional functions</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#cvx-funcs">Convex functions</a>
    <ul>
      <li><a href="#props">Properties</a>
        <ul>
          <li><a href="#st-order-conds">First-order conditions</a></li>
          <li><a href="#nd-order-conds">Second-order conditions</a></li>
        </ul>
      </li>
      <li><a href="#cvx-funcs-eg">Examples</a>
        <ul>
          <li><a href="#func-on-r">Functions on $\mathbb{R}$</a></li>
          <li><a href="#func-on-rn">Functions on $\mathbb{R}^n$</a></li>
        </ul>
      </li>
      <li><a href="#sub-lvl-sets">Sub-level sets</a></li>
      <li><a href="#inequalities">Inequalities</a>
        <ul>
          <li><a href="#jensens-inequality">Jensen’s inequality</a></li>
        </ul>
      </li>
      <li><a href="#operations-funcs">Operations that preserve convexity</a></li>
      <li><a href="#conjugate-func">The conjugate function</a></li>
      <li><a href="#quasi-cvx-funcs">Quasiconvex functions</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="cvx-sets">Convex sets</h2>

<h3 id="aff-cvx-sets">Affine &amp; convex sets</h3>

<h4 id="aff-sets">Affine sets</h4>
<p>A set $C\subset\mathbb{R}^n$ is <strong>affine</strong> if the line through any two distinct points in $C$ lies in $C$, i.e. for any $x_1,x_2\in C$ and for any $\theta\in\mathbb{R}$ we have
\begin{equation}
\theta x_1+(1-\theta)x_2\in C
\end{equation}
A point of the form $\theta_1 x_1+\ldots+\theta_k x_k$, where $\theta_1+\ldots+\theta_k=1$ is known as an <strong>affine combination</strong> of the points $x_1,\ldots,x_k$.</p>

<p>Hence, if $C$ is an affine set, and $x_1,\ldots,x_k\in C$, and $\theta_1+\ldots+\theta_k=1$, then the point
\begin{equation}
\theta_1 x_1+\ldots+\theta_k x_k\in C,
\end{equation}
or in other words, an affine set contains every affine combination of its points.</p>

<p>If $C$ is an affine set and $x_0\in C$, then the set
\begin{equation}
V=C-x_0\{x-x_0:x\in C\}
\end{equation}
is a subspace.</p>

<p>The set of all affine combinations of points in some set $C\subset\mathbb{R}^n$ is referred as <strong>affine hull</strong> of $C$, denoted as $\text{aff}\,C$:
\begin{equation}
\text{aff}\,C=\{\theta_1 x_1+\ldots+\theta_k x_k:x_1,\ldots,x_k\in C;\theta_1+\ldots+\theta_k=1\}
\end{equation}
The affine hull is the <em>smallest</em> affine set containing $C$.</p>

<h4 id="aff-dim-rel-int">Affine dimension, relative interior</h4>
<p>The <strong>affine dimension</strong> of a set $C$ is defined as the dimension of $\text{aff}\,C$.</p>

<p>If the affine dimension of $C\subset\mathbb{R}^n$ is less than $n$, then the set lies in $\text{aff}\,C\neq\mathbb{R}^n$. The <strong>relative interior</strong> of the set $C$, denoted as $\text{relint}\,C$, is defined as its interior relative to $\text{aff}\,C$:
\begin{equation}
\text{relint}\,C=\{x\in C:B(x,r)\cap\text{aff}\,C\in C\text{ for some }r&gt;0\},
\end{equation}
where $B(x,r)$ is the ball centered at $x$ with radius $r$ in the norm $\Vert\cdot\Vert$ (here $\Vert\cdot\Vert$ could be any norm; all norms define the same relative interior).</p>

<p>The <strong>relative boundary</strong> of $C$ is defined as $\overline{C}\backslash\text{relint}\,C$, where $\overline{C}$ is the closure of $C$.</p>

<h4 id="cvx-sets-def">Convex sets</h4>
<p>A set $C$ is <strong>convex</strong> if the line segment between any points in $C$ also lies in $C$, i.e. for any $x_1,x_2\in C$ and for any $0\leq\theta\leq 1$, we have
\begin{equation}
\theta x_1+(1-\theta)x_2\in C
\end{equation}
It is then easily seen that every affine sets is also convex.</p>

<p>Analogy to affine sets, we also refer a point of the form $\theta_1 x_1+\ldots+\theta_k x_k$, where $\theta_1+\ldots+\theta_k=1$ and $\theta_i\geq 0,\forall i=1,\ldots,k$, a <strong>convex combination</strong> of the points $x_1,\ldots,x_k$. And a set is convex iff it contains every convex combination of its points.</p>

<p>The <strong>convex hull</strong> of $C$, denoted by $\text{conv}\,C$, is defined as the set of all convex combinations of points in $C$:
\begin{equation}
\text{conv}\,C=\{\theta_1 x_1+\ldots+\theta_k x_k:x_1,\ldots,x_k\in C;\theta_1+\ldots+\theta_k=1;\theta_1,\ldots,\theta_k\geq 0\}
\end{equation}
Thus, $\text{conv}\,C$ is convex and is the smallest convex set containing $C$.</p>

<p>We can generalize the definition of convex combination into: let $x_1,x_2\ldots\in C$ where $C\subset\mathbb{R}^n$ and let $\{\theta_n\}_{n=1,2,\ldots}$ be a countable sequence such that
\begin{equation}
\sum_{i=1}^{\infty}\theta_i=1;\hspace{2cm}\theta_i\geq 0,\hspace{0.5cm}\forall i=1,2,\ldots
\end{equation}
Then the series
\begin{equation}
\sum_{i=1}^{\infty}\theta_i x_i\in C,
\end{equation}
if it converges.</p>

<p>More generally, suppose $p:\mathbb{R}^n\to\mathbb{R}$ satisfies $p(x)\geq 0$ forall $x\in C$ and $\int_C p(x)\,dx=1$, where $C\subset\mathbb{R}^n$ is a convex set. Then the integral
\begin{equation}
\int_C p(x)x\,dx\in C
\end{equation}
if it exists.</p>

<p>In the most general form, suppose $C\subset\mathbb{R}^n$ is convex and $x$ is a random vector with $x\in C$ with probability one. Then we also have that
\begin{equation}
\mathbb{E}x\in C
\end{equation}</p>

<h4 id="cones">Cones</h4>
<p>A set $C$ is called a <strong>cone</strong>, or <strong>nonnegative homogeneous</strong>, if for every $x\in C$ and for any $\theta\geq 0$, we also have $\theta x\in C$.</p>

<p>A <strong>convex cone</strong> $C$ is both convex and a cone, i.e. for any $x_1,x_2\in C$ and for any $\theta_1,\theta_2\geq 0$, we have
\begin{equation}
\theta_1 x_1+\theta_2 x_2\in C
\end{equation}
since by definition of a cone, we can add a normalization factor $\alpha$ into the point above
\begin{equation}
\alpha\theta_1 x_1+\alpha\theta_2 x_2
\end{equation}
such that $\alpha\theta_1+\alpha\theta_2=1$ (in this particular case, $\alpha=1/(\theta_1+\theta_2)$).</p>

<p>A point of the form $\theta_1 x_1+\ldots+\theta_k x_k$ with $\theta_1,\ldots,\theta_k\geq 0$ is called a <strong>conic combination</strong>. It is easily seen that a set $C$ is a convex cone iff it contains all conic combinations of its points.
Like convex and affine combinations, we can generalize the definition of conic combination into infinite series and integrals.</p>

<p>We define the <strong>conic hull</strong> of a set $C$ as the set of all conic combinations of elements in $C$
\begin{equation}
\{\theta_1 x_1+\ldots+\theta_k x_k:x_i\in C;\theta_i\geq 0,\forall i=1,\ldots,k\}
\end{equation}
Also, the conic hull of $C$ is the smallest convex cone containing $C$.</p>

<h3 id="cvx-sets-eg">Examples</h3>

<h4 id="hyperplane-halfspaces">Hyperplanes, halfspaces</h4>
<p>A <strong>hyperplane</strong> $P$ is a set of form
\begin{equation}
P=\{x\in\mathbb{R}^n:a^\text{T}x=b\},
\end{equation}
where $a\in\mathbb{R}^n$, $a\neq 0$ and $b\in\mathbb{R}$. We have that $P$ is convex.</p>

<p>To prove this, for $x_1,x_2\in P$, and for any $0\leq\theta\leq 1$, we have
\begin{equation}
a^\text{T}\big(\theta x_1+(1-\theta)x_2\big)=\theta a^\text{T}x_1+(1-\theta)a^\text{T}x_2=\theta b+(1-\theta)b=b
\end{equation}</p>

<p>A hyperplane separates $\mathbb{R}^n$ into two <strong>halfspaces</strong>. A (closed) halfspace is a set of of the form
\begin{equation}
\{x\in\mathbb{R}^n:a^\text{T}x\leq b\},
\end{equation}
where $a\in\mathbb{R}^n$, $a\neq 0$ and $b\in\mathbb{R}$. It is also easily seen that halfspaces are also convex.</p>

<h4 id="balls-ellips-cones">Balls, ellipsoids, norm cones</h4>

<h5 id="balls">Balls</h5>
<p>A (closed) <strong>ball</strong> in $\mathbb{R}^n$ centered at $x_c$ and with radius $r$ and with $\Vert\cdot\Vert$ is any norm in $\mathbb{R}^n$
\begin{equation}
B(x_c,r)=\{x\in\mathbb{R}^n:\Vert x-x_c\Vert\leq r\}
\end{equation}
is a convex set.</p>

<p>To see this, for any $x_1,x_2\in B(x_c,r)$ and for any $0\leq\theta\leq 1$, by triangle inequality of norm, we have
\begin{align}
\Vert\theta x_1+(1-\theta)x_2-x_c\Vert&amp;=\Vert\theta(x_1-x_c)+(1-\theta)(x_2-x_c)\Vert \\ &amp;\leq\theta\Vert x_1-x_c\Vert+(1-\theta)\Vert x_2-x_c\Vert \\ &amp;\leq\theta r+(1-\theta)r \\ &amp;=r
\end{align}</p>

<h5 id="ellips">Ellipsoids</h5>
<p>An <strong>ellipsoid</strong> $\mathcal{E}$ in $\mathbb{R}^n$ centered at $x_c\in\mathbb{R}^n$ is defined as
\begin{equation}
\mathcal{E}=\{x:(x-x_c)^\text{T}P^{-1}(x-x_c)\leq 1\},
\end{equation}
where $P\in\mathbb{R}^{n\times n}$ is symmetric and positive definite. The matrix $P$ determines how far $\mathcal{E}$ extends in every direction from $x_c$; the lengths of the semi-axes of $\mathcal{E}$ are $\sqrt{\lambda_i}$, where $\lambda_i$ for $i=1,\ldots,n$ are the eigenvalues of $P$. A ball of radius $r$ is an ellipsoid with
\begin{equation}
P=r^2 I
\end{equation}
We then have $\mathcal{E}$ is convex.</p>

<p>To prove this claim, as usual, for $x_1,x_2\in\mathcal{E}$ and for $0\leq\theta\leq 1$, we have
\begin{align}
&amp;\hspace{0.7cm}\big(\theta x_1+(1-\theta)x_2-x_c\big)^\text{T}P^{-1}\big(\theta x_1+(1-\theta)x_2-x_c\big) \\ &amp;=\big(\theta x_1-\theta x_c+(1-\theta)x_2-(1-\theta)x_c\big)^\text{T}P^{-1}\big(\theta x_1-\theta x_c+(1-\theta)x_2-(1-\theta)x_c\big) \\ &amp;=(a+b)^\text{T}P^{-1}(a+b) \\ &amp;=a^\text{T}P^{-1}a+b^\text{T}P^{-1}b+2a^\text{T}P^{-1}b \\ &amp;\leq\theta^2+(1-\theta)^2+2\theta(1-\theta) \\ &amp;=1
\end{align}
where in the second step, we have let
\begin{equation}
a=\theta x_1-\theta x_c,\hspace{2cm}b=(1-\theta)x_2-(1-\theta)x_c,
\end{equation}
which implies that
\begin{equation}
a^\text{T}P^{-1}a\leq 1,\hspace{2cm}b^\text{T}P^{-1}b\leq 1
\end{equation}
and thus
\begin{equation}
a^\text{T}P^{-1/2}\leq 1,\hspace{2cm}b^\text{T}P^{-1/2}\leq 1
\end{equation}</p>

<h5 id="norm-cones">Norm cones</h5>
<p>A <strong>norm cone</strong> $C$ associated with the norm $\Vert\cdot\Vert$ is defined as
\begin{equation}
C=\{(x,t):\Vert x\Vert\leq t\}\subset\mathbb{R}^{n+1}
\end{equation}
is also convex</p>

<h4 id="polyhedra">Polyhedra</h4>
<p>A <strong>polyhedron</strong> $\mathcal{P}$ is defined as
\begin{equation}
\mathcal{P}=\{x:a_i^\text{T}\leq b_i,i=1,\ldots,m;c_j^\text{T}=d_j,j=1,\ldots,p\}
\end{equation}
Then $\mathcal{P}$ can be seen as the intersection of a finite number of halfspaces and hyperplanes. Another representation of $\mathcal{P}$ is
\begin{equation}
\mathcal{P}=\{x:Ax\preceq b,Cx=d\},
\end{equation}
where
\begin{equation}
A=\left[\begin{matrix}a_1^\text{T} \\ \vdots \\ a_m^\text{T}\end{matrix}\right],\hspace{2cm}C=\left[\begin{matrix}c_1^\text{T} \\ \vdots \\ c_p^\text{T}\end{matrix}\right]
\end{equation}
And thus, we also have that $\mathcal{P}$ is convex, which can be proved easily since $Ax$ and $Cx$ are both linear functions.</p>

<h5 id="non-neg-orthant">Nonnegative orthant</h5>
<p>The <strong>nonnegative orthant</strong> in $\mathbb{R}^n$, denoted $\mathbb{R}_+^{n}$, is the set of points with nonnegative components, i.e.
\begin{equation}
\mathbb{R}_+^n=\{x\in\mathbb{R}^n:x\succeq 0\}
\end{equation}
We have that $\mathbb{R}_+^n$ is both a polyhedron and a cone, or a <strong>polyhedral cone</strong>, and hence is also convex.</p>

<h5 id="simplex">Simplex</h5>
<p>Suppose the $v_0,\ldots,v_k\in\mathbb{R}^n$ are <strong>affinely independent</strong>, i.e. $v_1-v_0,\ldots,v_k-v_0$ are linearly independent. The <strong>simplex</strong> determined by them is given as
\begin{equation}
C=\text{conv}\{v_0,\ldots,v_k\}=\{\theta_0 v_0+\ldots+\theta_k v_k:\theta\succeq 0,\mathbf{1}^\text{T}\theta=1\}
\end{equation}
As an instance of polyhedra, $C$ is thus convex.</p>

<h4 id="psd-cone">Positive semi-definite cone</h4>
<p>Let $\mathbb{S}^n$ denote the set of symmetric $n\times n$ matrices
\begin{equation}
\mathbb{S}^n=\{X\in\mathbb{R}^{n\times n}:X=X^\text{T}\},
\end{equation}
and let $\mathbb{S}_+^n$ represent the set of symmetric positive semi-definite matrices
\begin{equation}
\mathbb{S}_+^n=\{X\in\mathbb{S}^n:X\succeq 0\},
\end{equation}
and finally, let us assign the set of symmetric positive definite matrices to $\mathbb{S}_{++}^n$
\begin{equation}
\mathbb{S}_{++}^n=\{X\in\mathbb{S}^n:X\succ 0\}
\end{equation}
We have that $\mathbb{S}_+^n$ is a convex cone, since for any matrices $A_1,A_2\in\mathbb{S}_+^n$, for any $\theta_1,\theta_2\geq 0$ and for any $x\in\mathbb{R}^n$, we have
\begin{equation}
x^\text{T}(\theta_1 A_1+\theta_2 A_2)x=\theta_1 x^\text{T}A_1 x+\theta_2 x^\text{T}A_2 x\geq 0
\end{equation}
The same argument can be applied to prove that $\mathbb{S}_{++}^n$ or even the set of symmetric negative definite matrices and the set of symmetric negative semi-definite matrices are convex.</p>

<h3 id="operations-sets">Operations that preserve convexity</h3>

<h4 id="intersect">Intersection</h4>
<p>Let $S_1,S_2$ be convex sets and let $x_1,x_2$ are two points containing in both sets, thus $x_1,x_2\in S_1\cap S_2$.</p>

<p>Since $x_1,x_2\in S_1$ which is convex, for $0\leq\theta\leq 1$, we have the point
\begin{equation}
\theta x_1+(1-\theta)x_2\in S_1
\end{equation}
Analogously, we also have
\begin{equation}
\theta x_1+(1-\theta)x_2\in S_2,
\end{equation}
which implies that
\begin{equation}
\theta x_1+(1-\theta)x_2\in S_1\cap S_2
\end{equation}
Or in other words, $S_1\cap S_2$ is also convex.</p>

<p>By induction, we can extend this property to: if $S_\alpha$ is convex for every $\alpha\in\mathcal{A}$, then their intersection
\begin{equation}
\bigcap_{\alpha\in\mathcal{A}}S_\alpha
\end{equation}
is also convex.</p>

<h4 id="aff-funcs">Affine functions</h4>
<p>A function $f:\mathbb{R}^n\to\mathbb{R}^m$ is <strong>affine</strong> if it is a sum of linear function and a constant, i.e. it can be written as
\begin{equation}
f(x)=Ax+b,
\end{equation}
where $A\in\mathbb{R}^{m\times n}$ and $b\in\mathbb{R}^m$.</p>

<p>Let $S\subset\mathbb{R}^n$ be a convex set and let $f:\mathbb{R}^n\to\mathbb{R}^m$ be an affine function. Then the image of $S$ under $f$
\begin{equation}
f(S)=\{f(x):x\in S\}
\end{equation}
is convex.</p>

<p>Analogously, the inverse image of $S$ under an affine function $g:\mathbb{R}^k\to\mathbb{R}^n$
\begin{equation}
g^{-1}(S)=\{x:g(x)\in S\}
\end{equation}
is convex.</p>

<p>The <strong>projection</strong> of a convex set $S\subset\mathbb{R}^m\times\mathbb{R}^n$ onto some of its coordinates
\begin{equation}
T=\{x_1\in\mathbb{R}^m:(x_1,x_2)\in S,\text{ for some }x_2\in\mathbb{R}^n\}
\end{equation}
is convex.</p>

<p>If $S_1,S_2$ are convex then so is their sum
\begin{equation}
S_1+S_2=\{x_1+x_2:x_1\in S_1,x_2\in S_2\}
\end{equation}
This is due to its reverse image under the linear function $f(x_1,x_2)=x_1+x_2$, which is the <strong>Cartesian product</strong>
\begin{equation}
S_1\times S_2=\{(x_1,x_2):x_1\in S_1,x_2\in S_2\}
\end{equation}
is convex.</p>

<h4 id="lin-frac-persp-funcs">Linear-fractional, perspective functions</h4>

<h5 id="persp-funcs">Perspective functions</h5>
<p>The <strong>perspective function</strong> $P:\mathbb{R}^{n+1}\to\mathbb{R}^n$, with domain $\text{dom}\,P=\mathbb{R}^n\times\mathbb{R}_{++}$ is defined as
\begin{equation}
P(z,t)=\frac{z}{t}
\end{equation}
Suppose that $x=(\tilde{x},x_{n+1}),y=(\tilde{y},y_{n+1})\in\mathbb{R}^{n+1}$ with $x_{n+1},y_{n+1}\gt 0$. Then for $0\leq\theta\leq 1$, we have
\begin{equation}
P(\theta x+(1-\theta)y)=\frac{\theta\tilde{x}+(1-\theta)\tilde{y}}{\theta x_{1+1}+(1-\theta)y_{n+1}}=\mu P(x)+(1-\mu)P(y),
\end{equation}
where
\begin{equation}
\mu=\frac{\theta x_{n+1}}{\theta x_{n+1}+(1-\theta)y_{n+1}}\in[0,1],
\end{equation}
which implies that
\begin{equation}
P([x,y])=[P(x),P(y)]\label{eq:pf.1}
\end{equation}
Let $C$ be convex with $C\subset\text{dom}\,P$, and let $x,y\in C$. By \eqref{eq:pf.1}, we have that the line segment $[P(x),P(y)]$ is the image of the line segment $[x,y]$ under $P$, $P([x,y])$, and so lies in $P(C)$, which also claims the convexity of $P(C)$.</p>

<p>The inverse image of a convex set under the perspective function is also convex: if $C\subset\mathbb{R}^n$ is convex, then
\begin{equation}
P^{-1}(C)=\{(x,t)\in\mathbb{R}^{n+1}:x/t\in C,t&gt;0\}
\end{equation}
is convex.</p>

<p>To prove this, for any $(x_1,t_1),(x_2,t_2)\in P^{-1}(C)$ and for any $0\leq t\leq 1$, by the result \eqref{eq:pf.1}, we have
\begin{equation}
P^{-1}\big(\theta(x_1,t_1)+(1-\theta)(x_2,t_2)\big)=\frac{\theta x_1+(1-\theta x_2)}{\theta t_1+(1-\theta)t_2}=\mu\frac{x_1}{t_1}+(1-\mu)\frac{x_2}{t_2},
\end{equation}
where
\begin{equation}
\mu=\frac{\theta t_1}{\theta t_1+(1-\theta)t_2}\in[0,1]
\end{equation}</p>

<h5 id="lin-frac-funcs">Linear-fractional functions</h5>
<p>We define the <strong>linear-fractional function</strong> to be the composite function of a perspective function with an affine function. Specifically, let $g:\mathbb{R}^n\to\mathbb{R}^{m+1}$ be affine
\begin{equation}
g(x)=\left[\begin{matrix}A \\ c^\text{T}\end{matrix}\right]x+\left[\begin{matrix}b \\ d\end{matrix}\right],
\end{equation}
where $A\in\mathbb{R}^{m\times n},b\in\mathbb{R}^m,c\in\mathbb{R}^n$ and $d\in\mathbb{R}$. The function $f:\mathbb{R}^n\to\mathbb{R}^m$ given by
\begin{equation}
f(x)=(P\circ g)(x)=\frac{Ax+b}{c^\text{T}x+d},
\end{equation}
for $\text{dom}\,f\{x:c^\text{T}x+d&gt;0\}$, is called a <strong>linear-fractional function</strong>.</p>

<p>It is convenient to represent a linear-fractional function as a matrix
\begin{equation}
Q=\left[\begin{matrix}A&amp;b \\ c^\text{T}&amp;d\end{matrix}\right]\in\mathbb{R}^{(m+1)\times(n+1)},
\end{equation}
which lets
\begin{equation}
Q\left[\begin{matrix}x \\ 1\end{matrix}\right]=\left[\begin{matrix}A&amp;b \\ c^\text{T}&amp;d\end{matrix}\right]\left[\begin{matrix}x \\ 1\end{matrix}\right]=\left[\begin{matrix}Ax+b \\ c^\text{T}x+d\end{matrix}\right]
\end{equation}</p>

<h2 id="cvx-funcs">Convex functions</h2>
<p>A function $f:\mathbb{R}^n\to\mathbb{R}$ is called <strong>convex</strong> if $\text{dom}\,f$ is a convex set and if for all $x,y\in\text{dom}\,f$ and for any $0\leq\theta\leq 1$, we have
\begin{equation}
f\big(\theta x+(1-\theta)y\big)\leq\theta f(x)+(1-\theta)f(y)\label{eq:cf.1}
\end{equation}
Intuitively, we can think of the above inequality as the line segment between $(x,f(x))$ and $(y,f(y))$ lies above the graph of $f$.</p>

<p>We call $f$ a <strong>strictly convex</strong> function if strict inequality hold in \eqref{eq:cf.1} for every $x\neq y$ and $0\lt\theta\lt 1$. And $f$ is referred as <strong>concave</strong> if $-f$ is convex, or is known as <strong>strictly concave</strong> if $-f$ is strictly convex.</p>

<p>A function is convex iff it is convex when restricted to any line that intersects its domain. In other words, $f$ is convex iff for all $x\in\text{dom}\,f$ and for all $v$, the function
\begin{equation}
g(t)=f(x+tv)
\end{equation}
is convex on its domain, $\text{dom}\,g=\{t:x+tv\in\text{dom}\,f\}$.</p>

<p>All linear and affine functions are either convex or concave.</p>

<h3 id="props">Properties</h3>

<h4 id="st-order-conds">First-order conditions</h4>

<p>Let $f$ be differentible, i.e. $\nabla f$ exists at each point in $\text{dom}\,f$, which is open. Then $f$ is convex iff $\text{dom}\,f$ is convex and
\begin{equation}
f(y)\geq f(x)+\nabla f(x)^\text{T}(y-x)
\end{equation}
holds for all $x,y\in\text{dom}\,f$.</p>

<p>Similarly, we can also have that $f$ is strictly convex iff $\text{dom}\,f$ is convex and for $x,y\in\text{dom}\,f$ such that $x\neq y$, we have
\begin{equation}
f(y)&gt;f(x)+\nabla f(x)^\text{T}(y-x)
\end{equation}
And hence, $f$ is concave iff $\text{dom}\,f$ is convex and for all $x,y\in\text{dom}\,f$, we have
\begin{equation}
f(y)\leq f(x)+\nabla f(x)^\text{T}(y-x)
\end{equation}</p>

<p><strong>Proof</strong><br />
We first consider the case that $n=1$, i.e. $f:\mathbb{R}\to\mathbb{R}$ is convex iff for all $x,y\in$, we have
\begin{equation}
f(y)\geq f(x)+f’(x)(y-x)\label{eq:soc.1}
\end{equation}
Suppose that $f$ is convex, thus $\text{dom}\,f$ is convex.</p>

<p>Let $x,y$ be two points in $\text{dom}\,f$. We therefore have that for any $0\lt\theta\leq 1$, $(1-\theta)x+\theta y\in\text{dom}\,f$ and
\begin{equation}
f\big((1-\theta)x+\theta y\big)\leq(1-\theta)f(x)+\theta f(y),
\end{equation}
which give us
\begin{equation}
f(y)\geq f(x)+\frac{f\big(x+\theta(y-x)\big)-f(x)}{\theta}
\end{equation}
Let $t\to 0$, we obtain
\begin{equation}
f(y)\geq f(x)+f’(x)(y-x)
\end{equation}
Given $f$ such that \eqref{eq:soc.1} satisfies for all $x,y$ in $\text{dom}\,f$ and $\text{dom}\,f$ is convex.</p>

<p>Choose any $x\neq y$ and let $z=\theta x+(1-\theta)y$, for some $0\leq\theta\leq 1$, we then have $z\in\text{dom}\,f$, which implies that
\begin{equation}
f(x)\geq f(z)+f’(z)(x-z)
\end{equation}
and
\begin{equation}
f(y)\geq f(z)+f’(z)(y-z)
\end{equation}
Since $0\leq\theta\leq 1$, these two results above give us
\begin{equation}
\theta f(x)+(1-\theta)f(y)\geq f(z)=\theta x+(1-\theta)y,
\end{equation}
which proves our claim.</p>

<p>For the general case of $f:\mathbb{R}^n\to\mathbb{R}$. Let $x,y\in\mathbb{R}^n$ and consider $f$ restricted to the line passing through $x,y$, i.e. the function
\begin{equation}
g(t)=f(ty+(1-t)x),
\end{equation}
whose derivative is given as
\begin{equation}
g’(t)=\nabla f(ty+(1-t)x)^\text{T}(y-x)
\end{equation}
First suppose that $f$ is convex, and thus so is $g$. Using the above argument for the case of $n=1$, we have that
\begin{equation}
g(1)\geq g(0)+g’(0)
\end{equation}
or
\begin{equation}
f(y)\geq f(x)+\nabla f(x)^\text{T}(y-x)\label{eq:soc.2}
\end{equation}
We continue with assuming that \eqref{eq:soc.2} holds for any $x,y\in\text{dom}\,f$ and $\text{dom}\,f$ is convex.</p>

<p>The convexity of $\text{dom}\,f$ implies that for any $x,y\in\text{dom}\,f$ and for any $0\leq t_1,t_2\leq 1$, we have
\begin{equation}
t_1 y+(1-t_1)x,t_2 y+(1-t_2)x\in\text{dom}\,f
\end{equation}
By \eqref{eq:soc.2} we have
\begin{equation}
f(t_1 y+(1-t_1)x)\geq f(t_2 y+(1-t_2)x)+\nabla f(t_2 y+(1-t_2)x)^\text{T}(y-x)(t_1-t_2)
\end{equation}
or
\begin{equation}
g(t_1)\geq g(t_2)+g’(t_2)(t_1-t_2),
\end{equation}
which implies that $g$ is convex, and hence so is $f$.</p>

<h4 id="nd-order-conds">Second-order conditions</h4>
<p>Consider a function $f$ such that $f$ is twice differentiable. Then $f$ is convex iff $\text{dom}\,f$ is convex and its Hessian is positive semidefinite, i.e. for all $x\in\text{dom}\,f$ we have
\begin{equation}
\nabla^2 f(x)\succeq 0
\end{equation}
Analogously, we have that $f$ is concave iff $\text{dom}\,f$ is convex and for all $x\in\text{dom}\,f$, $\nabla^2 f(x)\preceq 0$.</p>

<p>On the other hands, $f$ is stricly convex (or concave) if $\text{dom}\,f$ is convex and $\nabla^2 f(x)\succ 0$ (or $\nabla^2 f(x)\prec 0$) for all $x\in\text{dom}\,f$. The reverse order is not true.</p>

<h3 id="cvx-funcs-eg">Examples</h3>

<h4 id="func-on-r">Functions on $\mathbb{R}$</h4>
<ul id="number-list">
	<li>
		<b>Exponential</b>. $\,e^{ax}$ is convex on $\mathbb{R}$, for any $a\in\mathbb{R}$. Since
		\begin{equation}
		(e^{ax})''=(a e^{ax})'=a^2 e^{ax}\geq 0,
		\end{equation}
		for $a\in\mathbb{R}$.
	</li>
	<li>
		<b>Powers</b>. $\,x^a$ is convex on $\mathbb{R}_{++}$ when $a\geq 1$ or $a\leq 0$, and concave for $0\leq a\leq 1$.<br />
		For $x\in\mathbb{R}_{++}$ and for $a\geq 1$ or $a\leq 0$, we have
		\begin{equation}
		(x^a)''=(a x^{a-1})'=a(a-1)x^{a-2}\geq 0
		\end{equation}
		On the other hands, for $x\in\mathbb{R}_{++}$ and for $0\leq a\leq 1$, we have
		\begin{equation}
		(x^a)''=a(a-1)x^{a-2}\leq 0
		\end{equation}
	</li>
	<li>
		<b>Powers of absolute value</b>. $\,\vert x\vert^p$ is convex on $\mathbb{R}$ for $p\geq 1$.<br />
		For any $0\leq\theta\leq 1$, by triangle inequality we have
		\begin{align}
		f(\theta x+(1-\theta)y)&amp;=\vert\theta x+(1-\theta)y\vert^p \\ &amp;\leq\left(\theta\vert x\vert+(1-\theta)\vert y\vert\right)^p
		\end{align}
	</li>
	<li>
		<b>Logarithm</b>. $\,\log x$ is concave on $\mathbb{R}_{++}$.<br />
		For $x\in\mathbb{R}_{++}$ we have
		\begin{equation}
		(\log x)''=\left(\frac{1}{x}\right)'=\frac{-1}{x^2}\lt 0
		\end{equation}
	</li>
	<li>
		<b>Negative entropy</b>. $\,x\log x$ (either on $\mathbb{R}_{++}$ or on $\mathbb{R}_+$ and defined as $0$ for $x=0$) is convex.<br />
		We have for all $x\in\mathbb{R}_{++}$
		\begin{equation}
		(x\log x)''=(1+\log x)'=\frac{1}{x}&gt;0
		\end{equation}
	</li>
</ul>

<h4 id="func-on-rn">Functions on $\mathbb{R}^n$</h4>
<ul id="number-list">
	<li>
		<b>Norms</b>. Every norm on $\mathbb{R}^n$ is convex.<br />
		For $f:\mathbb{R}^n\to\mathbb{R}$ is a norm and for any $0\leq\theta\leq 1$, by triangle inequality, we have:
		\begin{equation}
		f\big(\theta x+(1-\theta)y\big)\leq f(\theta x)+f\big((1-\theta)y\big)=\theta f(\theta)+(1-\theta)f(y)
		\end{equation}
	</li>
	<li>
		<b>Max function</b>. $\,f(x)=\max{x_1,\ldots,x_n}=\max_{i=1,\ldots,n}x_i$ is convex on $\mathbb{R}^n$.<br />
		For any $0\leq\theta\leq 1$, we have
		\begin{align}
		f(\theta x+(1-\theta)y)&amp;=\max_i(\theta x+(1-\theta)y) \\ &amp;\leq\theta\max_i x_i+(1-\theta)\max_i y_i \\ &amp;=\theta f(x)+(1-\theta)f(y)
		\end{align}
	</li>
	<li>
		<b>Quadratic-over-linear function</b>. The function $f(x,y)=x^2/y$ with
		\begin{equation}
		\text{dom}\,f=\{(x,y)\in\mathbb{R}^2:y&gt;0\}
		\end{equation}
		is convex, since we have its Hessian:
		\begin{equation}
		\nabla^2 f(x,y)=\nabla^2\frac{x^2}{y}=\left[\begin{matrix}2/y&amp;-2x/y^2 \\ -2x/y^2&amp;2x^2/y^3\end{matrix}\right]=\frac{2}{y^3}\left[\begin{matrix}y \\ -x\end{matrix}\right]\left[\begin{matrix}y \\ -x\end{matrix}\right]^\text{T}\succeq 0
		\end{equation}
	</li>
	<li>
		<b>Log-sum-exp</b>. $\,f(x)=\log(e^{x_1}+\ldots+e^{x_n})$ is convex on $\mathbb{R}^n$.<br />
		We have the Hessian of $f(x)$ is
		\begin{equation}
		\nabla^2 f(x)=\nabla^2\log(e^{x_1}+\ldots+e^{x_n})
		\end{equation}
	</li>
</ul>

<h3 id="sub-lvl-sets">Sub-level sets</h3>

<h3 id="inequalities">Inequalities</h3>

<h4 id="jensens-inequality">Jensen’s inequality</h4>

<h3 id="operations-funcs">Operations that preserve convexity</h3>

<h3 id="conjugate-func">The conjugate function</h3>

<h3 id="quasi-cvx-funcs">Quasiconvex functions</h3>

<h2 id="references">References</h2>
<p>[1] Stephen Boyd &amp; Lieven Vandenberghe. <a href="http://www.stanford.edu/∼boyd/cvxbook/">Convex Optimization</a>. Cambridge UP, 2004.</p>

<h2 id="footnotes">Footnotes</h2>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="convex-optimization" /><summary type="html"><![CDATA[convex sets, convex functions]]></summary></entry><entry><title type="html">Gaussian Distribution</title><link href="http://localhost:4000/2021/11/22/normal-dist.html" rel="alternate" type="text/html" title="Gaussian Distribution" /><published>2021-11-22T14:46:00+07:00</published><updated>2021-11-22T14:46:00+07:00</updated><id>http://localhost:4000/2021/11/22/normal-dist</id><content type="html" xml:base="http://localhost:4000/2021/11/22/normal-dist.html"><![CDATA[<blockquote>
  <p>The <strong>Gaussian (Normal) distribution</strong> is a continuous distribution with a bell-shaped PDF used widely in statistics due to the <strong>Central Limit theorem</strong>. The theorem states that under very weak assumptions, the sum of a large number of i.i.d. random variables has an approximately Normal distribution, regardless of the distribution of the individual r.v.s. This means we can start with independent r.v.s from almost any distribution, discrete or continuous, but once we add up a bunch of them, the distribution of the resulting random variable looks like a Gaussian distribution.
<!-- excerpt-end --></p>
</blockquote>

<ul>
  <li><a href="#gauss-dist">Gaussian (Normal) distribution</a>
    <ul>
      <li><a href="#std-norm">Standard Normal</a></li>
    </ul>
  </li>
  <li><a href="#mvn">Multivariate Normal distribution</a>
    <ul>
      <li><a href="#bvn">Bivariate Normal</a></li>
    </ul>
  </li>
  <li><a href="#prop-cov">Properties of the covariance matrix</a>
    <ul>
      <li><a href="#sym-cov">Symmetric</a></li>
      <li><a href="#re-cov">Real eigenvalues</a></li>
      <li><a href="#proj-ev-cov">Projection onto eigenvectors</a></li>
    </ul>
  </li>
  <li><a href="#geo-int">Geometrical interpretation</a></li>
  <li><a href="#cond-gauss-dist">Conditional Gaussian distribution</a></li>
  <li><a href="#marg-gauss-dist">Marginal Gaussian distribution</a></li>
  <li><a href="#bayes-theorem-gauss">Bayes’ theorem for Gaussian variables</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p>
<h2 id="gauss-dist">Gaussian (Normal) Distribution</h2>
<p>A random variable $X$ is said to be <strong>Gaussian</strong> or to have the <strong>Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$.</p>

<h3 id="std-normal">Standard Normal</h3>
<p>When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution <strong>Standard Normal</strong>.
\begin{equation}
X\sim\mathcal{N}(0,1)
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2},
\end{equation}
and
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\,dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt
\end{equation}
Below are some illustrations of Normal distribution.</p>
<figure>
	<img src="/assets/images/2021-11-22/normal.png" alt="normal distribution" style="display: block; margin-left: auto; margin-right: auto; width:  900px; height: 380px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/gauss-dist.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h2 id="mvn">Multivariate Normal Distribution</h2>
<p>A $k$-dimensional random vector $\mathbf{X}=\left(X_1,\dots,X_D\right)^\text{T}$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_DX_D
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_D$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})
\end{equation}
where
\begin{equation}
\boldsymbol{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^\text{T}=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^\text{T}
\end{equation}
is the $D$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{D\times D}$ with
\begin{equation}
\boldsymbol{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)\label{eq:mvn.1}
\end{equation}
We also have that $\boldsymbol{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Thus, the PDF of an MVN is defined as
\begin{equation}
f_\mathbf{X}(x_1,\ldots,x_D)=\dfrac{1}{(2\pi)^{D/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right]\label{eq:mvn.2}
\end{equation}
With this idea, <em>Standard Normal</em> distribution in multi-dimensional case can be defined as a Gaussian with mean $\boldsymbol{\mu}=0$ (here $0$ is an $D$-dimensional vector) and identity covariance matrix $\boldsymbol{\Sigma}=\mathbf{I}_{D\times D}$.</p>

<h3 id="bvn">Bivariate Normal</h3>
<p>When the number of dimensions in $\mathbf{X}$, $D=2$, this special case of MVN is called the <strong>Bivariate Normal (BVN)</strong>.</p>

<p>An example of an BVN, $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$, is shown as following.</p>
<figure>
	<img src="/assets/images/2021-11-22/bvn.png" alt="monte carlo method" style="display: block; margin-left: auto; margin-right: auto; width: 750px; height: 350px" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/bayes-optimization/mvn.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h2 id="prop-cov">Properties of the covariance matrix</h2>

<h3 id="sym-cov">Symmetric</h3>
<p>With the definition \eqref{eq:mvn.1} of the covariance matrix $\boldsymbol{\Sigma}$, we can easily see that it is symmetric. However, notice that in the illustration of BVN, we gave the distribution a non-symmetric covariance matrix. The reason why we could do that is without loss of generality, we can assume that $\boldsymbol{\Sigma}$ is symmetric.</p>

<p>To prove this property, first off consider a square matrix $\mathbf{S}$, we have it can be written by
\begin{equation}
\mathbf{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2}+\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}=\mathbf{S}_\text{S}+\mathbf{S}_\text{A},
\end{equation}
where
\begin{equation}
\mathbf{S}_\text{S}=\frac{\mathbf{S}+\mathbf{S}^\text{T}}{2},\hspace{2cm}\mathbf{S}_\text{A}=\frac{\mathbf{S}-\mathbf{S}^\text{T}}{2}
\end{equation}
It is easily seen that $\mathbf{S}_\text{S}$ is symmetric because the $\{i,j\}$ element of its equal to the $\{j,i\}$ element due to
\begin{equation}
(\mathbf{S}_\text{S})_{ij}=\frac{(\mathbf{S})_{ij}+(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}+(\mathbf{S})_{ji}}{2}=(\mathbf{S}_\text{S})_{ji}
\end{equation}
On the other hand, the matrix $\mathbf{S}_\text{A}$ is anti-symmetric since
\begin{equation}
(\mathbf{S}_\text{A})_{ij}=\frac{(\mathbf{S})_{ij}-(\mathbf{S}^\text{T})_{ij}}{2}=\frac{(\mathbf{S}^\text{T})_{ji}-(\mathbf{S})_{ji}}{2}=-(\mathbf{S}_\text{A})_{ji}
\end{equation}
Consider the density of a distribution $\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, we have that $\boldsymbol{\Sigma}$ is square and so is its inverse $\boldsymbol{\Sigma}^{-1}$. Therefore we can express $\boldsymbol{\Sigma}^{-1}$ as a sum of a symmetric matrix $\boldsymbol{\Sigma}_\text{S}$ with an anti-symmetric matrix $\boldsymbol{\Sigma}_\text{A}$
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A}
\end{equation}
We have that the density of the distribution is given by
\begin{align}
f(\mathbf{x})&amp;=\frac{1}{(2\pi)^{D/2}\vert\boldsymbol{\Sigma}\vert^{1/2}}\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;\propto\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;=\exp\left[-\dfrac{1}{2}\left(\mathbf{x}-\boldsymbol{\mu}\right)^\text{T}(\boldsymbol{\Sigma}_\text{S}+\boldsymbol{\Sigma}_\text{A})(\mathbf{x}-\boldsymbol{\mu})\right] \\ &amp;\propto\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}+\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}\right] \\ &amp;=\exp\left[\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{S}\mathbf{v}\right]
\end{align}
where in the forth step, we have defined $\mathbf{v}\doteq\mathbf{x}-\boldsymbol{\mu}$, and where in the fifth-step, the result obtained was due to
\begin{align}
\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}&amp;=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i(\boldsymbol{\Sigma}_\text{A})_{ij}\mathbf{v}_j \\ &amp;=\sum_{i=1}^{D}\sum_{j=1}^{D}\mathbf{v}_i-(\boldsymbol{\Sigma}_\text{A})_{ji}\mathbf{v}_j \\ &amp;=-\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}
\end{align}
which implies that $\mathbf{v}^\text{T}\boldsymbol{\Sigma}_\text{A}\mathbf{v}=0$.</p>

<p>Thus, when computing the density, the symmetric part of $\boldsymbol{\Sigma}^{-1}$ is the only one matters. Or in other words, without loss of generality, we can assume that $\boldsymbol{\Sigma}^{-1}$ is symmetric, which means that $\boldsymbol{\Sigma}$ is also symmetric.</p>

<p>With this assumption of symmetry, the covariance matrix $\boldsymbol{\Sigma}$ now has all the properties of a symmetric matrix, as following in the next two sections.</p>

<h3 id="re-cov">Real eigenvalues</h3>
<p>Consider an eigenvector, eigenvalue pair $(\mathbf{v},\lambda)$ of covariance matrix $\boldsymbol{\Sigma}$, we have
\begin{equation}
\boldsymbol{\Sigma}\mathbf{v}=\lambda\mathbf{v}\label{eq:rc.1}
\end{equation}
Since $\boldsymbol{\Sigma}\in\mathbb{R}^{D\times D}$, we have $\boldsymbol{\Sigma}=\overline{\boldsymbol{\Sigma}}$. Conjugate both sides of the equation above we have
\begin{equation}
\boldsymbol{\Sigma}\overline{\mathbf{v}}=\overline{\lambda}\overline{\mathbf{v}},\label{eq:rc.2}
\end{equation}
Since $\boldsymbol{\Sigma}$ is symmetric, we have $\boldsymbol{\Sigma}=\boldsymbol{\Sigma}^\text{T}$. Taking the transpose of both sides of \eqref{eq:rc.2} gives us
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\label{eq:rc.3}
\end{equation}
Continuing by taking dot product of both sides of \eqref{eq:rc.3} with $\mathbf{v}$ lets us obtain
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}\label{eq:rc.4}
\end{equation}
On the other hand, take dot product of $\overline{\mathbf{v}}^\text{T}$ with both sides of \eqref{eq:rc.1}, we have
\begin{equation}
\overline{\mathbf{v}}^\text{T}\boldsymbol{\Sigma}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v}
\end{equation}
which by \eqref{eq:rc.4} implies that
\begin{equation}
\overline{\lambda}\overline{\mathbf{v}}^\text{T}\mathbf{v}=\lambda\overline{\mathbf{v}}^\text{T}\mathbf{v},
\end{equation}
or
\begin{equation}
(\lambda-\overline{\lambda})\overline{\mathbf{v}}^\text{T}\mathbf{v}=0\label{eq:rc.5}
\end{equation}
Moreover, we have that
\begin{equation}
\overline{\mathbf{v}}^\text{T}\mathbf{v}=\sum_{k=1}^{D}(a_k-i b_k)(a_k+i b_k)=\sum_{k=1}^{D}a^2+b^2&gt;0
\end{equation}
where we have denoted the complex eigenvector $\mathbf{v}\neq\mathbf{0}$ as
\begin{equation}
\mathbf{v}=(a_1+i b_1,\ldots,a_D+i b_D)^\text{T},
\end{equation}
which implies that its complex conjugate $\overline{\mathbf{v}}$ can be written by
\begin{equation}
\overline{\mathbf{v}}=(a_1-i b_1,\ldots,a_D-i b_D)^\text{T}
\end{equation}
Therefore, by \eqref{eq:rc.5}, we can claim that
\begin{equation}
\lambda=\overline{\lambda}
\end{equation}
or in other words, the eigenvalue $\lambda$ of $\boldsymbol{\Sigma}$ is real.</p>

<h3 id="projection-onto-eigenvectors">Projection onto eigenvectors</h3>
<p>First, we have that eigenvectors $\mathbf{v}_i$ and $\mathbf{v}_j$ corresponding to different eigenvalues $\lambda_i$ and $\lambda_j$ of $\boldsymbol{\Sigma}$ are perpendicular, because
\begin{align}
\lambda_i\mathbf{v}_i^\text{T}\mathbf{v}_j&amp;=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}^\text{T}\mathbf{v}_j \\ &amp;=\mathbf{v}_i^\text{T}\boldsymbol{\Sigma}\mathbf{v}_j=\mathbf{v}_i^\text{T}\lambda_j\mathbf{v}_j,
\end{align}
which implies that
\begin{equation}
(\lambda_i-\lambda_j)\mathbf{v}_i^\text{T}\mathbf{v}_j=0
\end{equation}
Therefore, $\mathbf{v}_i^\text{T}\mathbf{v}_j=0$ since $\lambda_i\neq\lambda_j$.</p>

<p>Hence, for any unit eigenvectors $\mathbf{q}_i,\mathbf{q}_j$ of $\boldsymbol{\Sigma}$, we have
\begin{equation}
\mathbf{q}_i^\text{T}\mathbf{q}_j\begin{cases}1,&amp;\hspace{0.5cm}\text{if }i=j \\ 0,&amp;\hspace{0.5cm}\text{if }i\neq j\end{cases}
\end{equation}
This allows us to write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\mathbf{Q}^\text{T}\boldsymbol{\Lambda}\mathbf{Q},
\end{equation}
where $\mathbf{Q}$ is the orthonormal matrix whose $i$-th row is $\mathbf{q}_i^\text{T}$ and $\boldsymbol{\Lambda}$ is the diagonal matrix whose $\{i,i\}$ element is $\lambda_i$, as
\begin{equation}
\mathbf{Q}=\left[\begin{matrix}-\hspace{0.15cm}\mathbf{q}_1^\text{T}\hspace{0.15cm}- \\ \vdots \\ -\hspace{0.15cm}\mathbf{q}_D^\text{T}\hspace{0.15cm}-\end{matrix}\right],\hspace{2cm}\boldsymbol{\Lambda}=\left[\begin{matrix}\lambda_1&amp;&amp; \\ &amp;\ddots&amp; \\ &amp;&amp;\lambda_D\end{matrix}\right]
\end{equation}
Therefore, we can also write $\boldsymbol{\Sigma}$ as
\begin{equation}
\boldsymbol{\Sigma}=\sum_{i=1}^{D}\lambda_i\mathbf{q}_i\mathbf{q}_i^\text{T}
\end{equation}
Each matrix $\mathbf{q}_i\mathbf{q}_i^\text{T}$ is the projection matrix onto $\mathbf{q}_i$, then $\boldsymbol{\Sigma}$ can be express as a combination of perpendicular projection matrices.</p>

<p>Other than that, for any eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of the matrix $\boldsymbol{\Sigma}$, we have
\begin{align}
\lambda_i\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\boldsymbol{\Sigma}^{-1}\boldsymbol{\Sigma}\mathbf{q}_i=\mathbf{q}_i
\end{align}
or
\begin{equation}
\boldsymbol{\Sigma}^{-1}\mathbf{q}_i=\frac{1}{\lambda_i}\mathbf{q}_i,
\end{equation}
<span id="precision-eigenvalue">which implies that each eigenvector, eigenvalue pair $(\mathbf{q_i},\lambda_i)$ of $\boldsymbol{\Sigma}$ corresponds to an eigenvector, eigenvalue pair $(\mathbf{q}_i,1/\lambda_i)$ of $\boldsymbol{\Sigma}^{-1}$. Therefore, $\boldsymbol{\Sigma}^{-1}$ can also be written by</span>
\begin{equation}
\boldsymbol{\Sigma}^{-1}=\sum_{i=1}^{D}\frac{1}{\lambda_i}\mathbf{q}_i\mathbf{q}_i^\text{T}\label{eq:pec.1}
\end{equation}</p>

<h2 id="geo-int">Geometrical interpretation</h2>
<p>Consider the probability density function of the Gaussian \eqref{eq:mvn.2}, by the result \eqref{eq:pec.1}, we have that the functional dependence of the Gaussian on $\mathbf{x}$ is through the quadratic form
\begin{align}
\Delta^2&amp;=(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) \\ &amp;=\sum_{i=1}^{D}\frac{y_i^2}{\lambda_i},
\end{align}
where we have defined
\begin{equation}
y_i=\mathbf{q}_i^\text{T}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}
Let $\mathbf{y}=(y_1,\ldots,y_D)^\text{T}$ be the vector comprising $y_i$’s together, then we have
\begin{equation}
\mathbf{y}=\mathbf{Q}(\mathbf{x}-\boldsymbol{\mu})
\end{equation}
Consider the form of the Gaussian distribution in the new coordinate system defined by $y_i$. When changing variable from $\mathbf{x}$ to $\mathbf{y}$, firstly we define the <strong>Jacobian matrix</strong> $\mathbf{J}$, whose elements are given by
\begin{equation}
\mathbf{J}_{ij}=\frac{\partial x_i}{\partial y_j}=\mathbf{Q}_{ji},
\end{equation}
which implies that
\begin{equation}
\mathbf{J}=\mathbf{Q}^\text{T}
\end{equation}
Thus, $\vert\mathbf{J}\vert=\vert\mathbf{Q}^\text{T}\vert=1$ since
\begin{equation}
1=\vert\mathbf{I}\vert=\vert\mathbf{Q}^\text{T}\mathbf{Q}\vert=\vert\mathbf{Q}^\text{T}\vert\vert\mathbf{Q}\vert=\vert\mathbf{Q}^\text{T}\vert
\end{equation}
Additionally, by \eqref{eq:pec.1}, we also have
\begin{equation}
\vert\boldsymbol{\Sigma}\vert^{1/2}=\left\vert\mathbf{Q}^\text{T}\boldsymbol{\Lambda}\mathbf{Q}\right\vert^{1/2}=\left(\vert\mathbf{Q}^\text{T}\vert\vert\boldsymbol{\Lambda}\vert\vert\mathbf{Q}\vert\right)^{1/2}=\prod_{i=1}^{D}\lambda_i^{1/2}
\end{equation}
Therefore, in the $y_j$ coordinate system, the Gaussian distribution takes the form
\begin{equation}
p(\mathbf{y})=\mathbf{x}\vert\mathbf{J}\vert=\prod_{j=1}^{D}\frac{1}{(2\pi\lambda_j)^{1/2}}\exp\left(-\frac{y_j^2}{2\lambda_j}\right),
\end{equation}
which is the product of $D$ independent univariate Gaussian distributions.</p>

<h2 id="cond-gauss-dist">Conditional Gaussian distribution</h2>
<p>Let $\mathbf{x}$ be a $D$-dimensional random vector such that $\mathbf{x}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})$, and that we partition $\mathbf{x}$ into two disjoint subsets $\mathbf{x}_a$ and $\mathbf{x}_b$ with $\mathbf{x}_a$ is an $M$-dimensional vector and $\mathbf{x}_b$ is a $(D-M)$-dimensional vector.
\begin{equation}
\mathbf{x}=\left[\begin{matrix}\mathbf{x}_a \\ \mathbf{x}_b\end{matrix}\right]
\end{equation}
Along with them, we also define their corresponding means, as a partition of $\boldsymbol{\mu}$
\begin{equation}
\boldsymbol{\mu}=\left[\begin{matrix}\boldsymbol{\mu}_a \\ \boldsymbol{\mu}_b\end{matrix}\right]
\end{equation}
and their corresponding covariance matrices
\begin{equation}
\boldsymbol{\Sigma}=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&amp;\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&amp;\boldsymbol{\Sigma}_{bb}\end{matrix}\right],
\end{equation}
which implies that $\boldsymbol{\Sigma}_{ab}=\boldsymbol{\Sigma}_{b a}^\text{T}$.</p>

<p>Analogously, we also define the partitioned form of the precision matrix $\boldsymbol{\Sigma}^{-1}$
\begin{equation}
\boldsymbol{\Lambda}\doteq\boldsymbol{\Sigma}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&amp;\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&amp;\boldsymbol{\Lambda}_{bb}\end{matrix}\right],
\end{equation}
Thus, we also have that $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$ since $\boldsymbol{\Sigma}^{-1}$ or in other words, $\boldsymbol{\Lambda}$ is symmetric due to the symmetry of $\boldsymbol{\Sigma}$.
With these partitions, we can rewrite the functional dependence of the Gaussian \eqref{eq:mvn.2} on $\mathbf{x}$ as
\begin{align}
\hspace{-1cm}-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})&amp;=-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{aa}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_a-\boldsymbol{\mu}_a)^\text{T}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b) \\ &amp;\hspace{0.5cm}-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{ba}(\mathbf{x}_a-\boldsymbol{\mu}_a)-\frac{1}{2}(\mathbf{x}_b-\boldsymbol{\mu}_b)^\text{T}\boldsymbol{\Lambda}_{bb}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.1}
\end{align}
Consider the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$, which is the distribution of $\mathbf{x}_a$ given $\mathbf{x}_b$. Viewing $\mathbf{x}_b$ as a constant, \eqref{eq:cgd.1} will be the functional dependence of the conditional probability $p(\mathbf{x}_a\vert\mathbf{x}_b)$ on $\mathbf{x}_a$, which can be continued to derive as 
\begin{align}
&amp;-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\frac{1}{2}\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a+\boldsymbol{\Lambda}_{aa}^\text{T}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}\mathbf{x}_b+\boldsymbol{\Lambda}_{ab}\boldsymbol{\mu}_b-\boldsymbol{\Lambda}_{ba}^\text{T}\mathbf{x}_b+\boldsymbol{\Lambda}_{ba}\boldsymbol{\mu}_b\big)+c \\ &amp;\hspace{3cm}=-\frac{1}{2}\mathbf{x}_a^\text{T}\boldsymbol{\Lambda}_{aa}\mathbf{x}_a+\mathbf{x}_a^\text{T}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big)+c,\label{eq:cgd.2}
\end{align}
where $c$ is a constant, and we have used the $\boldsymbol{\Lambda}_{aa}=\boldsymbol{\Lambda}_{aa}^\text{T}$ and $\boldsymbol{\Lambda}_{ab}=\boldsymbol{\Lambda}_{ba}^\text{T}$.</p>

<p>Moreover, we have that the variation part which depends on $\mathbf{x}$ for any Gaussian $\mathbf{X}\sim\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Sigma})$ can be written as a quadratic function of $\mathbf{x}$
\begin{equation}
-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})=-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\mathbf{x}+\mathbf{x}^\text{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{\mu}+c,\label{eq:cgd.3}
\end{equation}
where $c$ is a constant. With this observation, and by \eqref{eq:cgd.2} we have that the conditional distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ is a Gaussian, with the corresponding covariance matrix, denoted as $\boldsymbol{\Sigma}_{a\vert b}$, given by
\begin{equation}
\boldsymbol{\Sigma}_{a\vert b}=\boldsymbol{\Lambda}_{aa}^{-1},\label{eq:cgd.4}
\end{equation}
and with the corresponding mean vector, denoted as $\boldsymbol{\mu}_{a\vert b}$, given by
\begin{align}
\boldsymbol{\mu}_{a\vert b}&amp;=\boldsymbol{\Sigma}_{a\vert b}\big(\boldsymbol{\Lambda}_{aa}\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\big) \\ &amp;=\boldsymbol{\mu}_a-\boldsymbol{\Lambda}_{aa}^{-1}\boldsymbol{\Lambda}_{ab}(\mathbf{x}_b-\boldsymbol{\mu}_b)\label{eq:cgd.5}
\end{align}
To express the mean $\boldsymbol{\mu}_{a\vert b}$ and the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ of $p(\mathbf{x}_a\vert\mathbf{x}_b)$ in terms of partition of the covariance matrix $\boldsymbol{\Sigma}$ instead of the precision matrix $\boldsymbol{\Lambda}$’s, we will be using the identity for the inverse of a partitioned matrix
\begin{align}
\left[\begin{matrix}\mathbf{A}&amp;\mathbf{B} \\ \mathbf{C}&amp;\mathbf{D}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}&amp;-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&amp;\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right],\label{eq:cgd.6}
\end{align}
where we have defined
\begin{equation}
\mathbf{M}\doteq(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})^{-1},
\end{equation}
whose inverse $\mathbf{M}^{-1}$ is called the <strong>Schur complement</strong> of the matrix $\left[\begin{matrix}\mathbf{A}&amp;\mathbf{B} \\ \mathbf{C}&amp;\mathbf{D}\end{matrix}\right]^{-1}$. This identity can be proved by multiplying both sides of \eqref{eq:cgd.6} with $\left[\begin{matrix}\mathbf{A}&amp;\mathbf{B} \\ \mathbf{C}&amp;\mathbf{D}\end{matrix}\right]$ to give
\begin{align}
\mathbf{I}&amp;=\left[\begin{matrix}\mathbf{M}&amp;-\mathbf{M}\mathbf{B}\mathbf{D}^{-1} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}&amp;\mathbf{D}^{-1}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\end{matrix}\right]\left[\begin{matrix}\mathbf{A}&amp;\mathbf{B} \\ \mathbf{C}&amp;\mathbf{D}\end{matrix}\right] \\ &amp;=\left[\begin{matrix}\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})&amp;\mathbf{M}\mathbf{B}-\mathbf{M}\mathbf{B} \\ -\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{A}+\mathbf{D}^{-1}\mathbf{C}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\mathbf{D}^{-1}\mathbf{C}&amp;-\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}+\mathbf{I}+\mathbf{D}^{-1}\mathbf{C}\mathbf{M}\mathbf{B}\end{matrix}\right] \\ &amp;=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{0} \\ \mathbf{D}^{-1}\mathbf{C}\big(\mathbf{I}-\mathbf{M}(\mathbf{A}-\mathbf{B}\mathbf{D}^{-1}\mathbf{C})\big)&amp;\mathbf{I}\end{matrix}\right] \\ &amp;=\left[\begin{matrix}\mathbf{I}&amp;\mathbf{0} \\ \mathbf{0}&amp;\mathbf{I}\end{matrix}\right]=\mathbf{I},
\end{align}
which claims our argument.</p>

<p>Applying the identity \eqref{eq:cgd.6} into the precision matrix $\boldsymbol{\Lambda}=\boldsymbol{\Sigma}^{-1}$ gives us
\begin{equation}
\hspace{-0.5cm}\left[\begin{matrix}\boldsymbol{\Lambda}_{aa}&amp;\boldsymbol{\Lambda}_{ab} \\ \boldsymbol{\Lambda}_{ba}&amp;\boldsymbol{\Lambda}_{bb}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\Sigma}_{aa}&amp;\boldsymbol{\Sigma}_{ab} \\ \boldsymbol{\Sigma}_{b a}&amp;\boldsymbol{\Sigma}_{bb}\end{matrix}\right]^{-1}=\left[\begin{matrix}\mathbf{M}_\boldsymbol{\Sigma}&amp;-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1} \\ -\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}&amp;\boldsymbol{\Sigma}_{bb}^{-1}+\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\end{matrix}\right],
\end{equation}
where the Schur complement of $\mathbf{\Sigma}^{-1}$ is given by
\begin{equation}
\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}
\end{equation}
Hence, we obtain
\begin{align}
\boldsymbol{\Lambda}_{aa}&amp;=\mathbf{M}_\boldsymbol{\Sigma}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}, \\ \boldsymbol{\Lambda}_{ab}&amp;=-\mathbf{M}_\boldsymbol{\Sigma}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}=\big(\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}\big)^{-1}\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}
\end{align}
Substitute these results into \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we have the mean and the covariance matrix of the conditional Gaussian distribution $p(\mathbf{x}_a\vert\mathbf{x}_b)$ can be rewritten as
\begin{align}
\boldsymbol{\mu}_{a\vert b}&amp;=\boldsymbol{\mu}_a+\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}(\mathbf{x}_b-\boldsymbol{\mu}_b), \\ \boldsymbol{\Sigma}_{a\vert b}&amp;=\boldsymbol{\Sigma}_{aa}-\boldsymbol{\Sigma}_{ab}\boldsymbol{\Sigma}_{bb}^{-1}\boldsymbol{\Sigma}_{ba}
\end{align}
It is worth noticing that the mean $\boldsymbol{\mu}_{a\vert b}$ given above is a linear function of $\mathbf{x}_b$, while the covariance matrix $\boldsymbol{\Sigma}_{a\vert b}$ is independent of $\mathbf{x}_b$. This is an example of a <strong>linear-Gaussian model</strong>.</p>

<h2 id="marg-gauss-dist">Marginal Gaussian distribution</h2>

<h2 id="bayes-theorem-gauss">Bayes’ theorem for Gaussian variables</h2>
<p>In this section, we will apply the Bayes’ theorem to find the marginal distribution of $p(\mathbf{y})$ and conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ with supposing that we are given a Gaussian distribution $p(\mathbf{x})$ and a conditional Gaussian distribution $p(\mathbf{y}\vert\mathbf{x})$ in which $p(\mathbf{y}\vert\mathbf{x})$ has a mean that is a linear function of $\mathbf{x}$, and a covariance matrix which is independent of $\mathbf{x}$, as
\begin{align}
p(\mathbf{x})&amp;=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&amp;=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
where $\mathbf{A},\mathbf{b}$ are two parameters controlling the means, and $\boldsymbol{\Lambda},\boldsymbol{L}$ are precision matrices.</p>

<p>In order to find the marginal and conditional distribution, first we will be looking for the joint distribution $p(\mathbf{x},\mathbf{y})$ by considering the augmented vector
\begin{equation}
\mathbf{z}=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]
\end{equation}
Therefore, we have
\begin{equation}
p(\mathbf{z})=p(\mathbf{x},\mathbf{y})=p(\mathbf{x})p(\mathbf{y}\vert\mathbf{x})
\end{equation}
Taking the natural logarithm of both sides gives us
\begin{align}
\log p(\mathbf{z})&amp;=\log p(\mathbf{x})+\log p(\mathbf{y}\vert\mathbf{x}) \\ &amp;=\log\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1})+\log\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}) \\ &amp;=-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\text{T}\boldsymbol{\Lambda}(\mathbf{x}-\boldsymbol{\mu})-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}-\mathbf{b})+c\label{eq:btg.1}
\end{align}
where $c$ is a constant in terms of $\mathbf{x}$ and $\mathbf{y}$, i.e., $c$ is independent of $\mathbf{x},\mathbf{y}$.</p>

<p>It is easily to notice that \eqref{eq:btg.1} is a quadratic function of the components of $\mathbf{z}$, which implies that $p(\mathbf{z})$ is a Gaussian. By \eqref{eq:cgd.3}, in order to find the covariance matrix of $\mathbf{z}$, we consider the quadratic terms in \eqref{eq:btg.1}, which are given by
\begin{align}
&amp;-\frac{1}{2}\mathbf{x}^\text{T}\boldsymbol{\Lambda}\mathbf{x}-\frac{1}{2}(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \\ &amp;=-\frac{1}{2}\Big[\mathbf{x}^\text{T}\big(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}\big)\mathbf{x}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{y}-\mathbf{y}^\text{T}\mathbf{L}\mathbf{A}\mathbf{x}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{y}\Big] \\ &amp;=-\frac{1}{2}\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&amp;-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&amp;\mathbf{L}\end{matrix}\right]\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right] \\ &amp;=-\frac{1}{2}\mathbf{z}^\text{T}\mathbf{R}\mathbf{z},
\end{align}
which implies that the precision matrix of $\mathbf{z}$ is $\mathbf{R}$, defined as
\begin{equation}
\mathbf{R}=\left[\begin{matrix}\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A}&amp;-\mathbf{A}^\text{T}\mathbf{L} \\ -\mathbf{L}\mathbf{A}&amp;\mathbf{L}\end{matrix}\right]
\end{equation}
Thus, using the identity \eqref{eq:cgd.6}, we obtain the covariance matrix of the joint distribution
\begin{equation}
\boldsymbol{\Sigma}_\mathbf{z}=\mathbf{R}^{-1}=\left[\begin{matrix}\boldsymbol{\Lambda}^{-1}&amp;\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T} \\ \mathbf{A}\boldsymbol{\Lambda}^{-1}&amp;\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}\end{matrix}\right]
\end{equation}
Analogously, by \eqref{eq:cgd.3}, we can find the mean of the joint distribution by considering the linear terms of \eqref{eq:btg.1}, which are
\begin{align}
\hspace{-0.7cm}\frac{1}{2}\Big[\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}+\boldsymbol{\mu}^\text{T}\boldsymbol{\Lambda}\mathbf{x}+(\mathbf{y}-\mathbf{A}\mathbf{x})^\text{T}\mathbf{L}\mathbf{b}+\mathbf{b}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{A}\mathbf{x}) \Big]&amp;=\mathbf{x}^\text{T}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{x}^\text{T}\mathbf{A}^\text{T}\mathbf{L}\mathbf{b}+\mathbf{y}^\text{T}\mathbf{L}\mathbf{b} \\ &amp;=\left[\begin{matrix}\mathbf{x} \\ \mathbf{y}\end{matrix}\right]^\text{T}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]
\end{align}
Thus, by \eqref{eq:cgd.3}, we have that the mean of the joint distribution is then given by
\begin{equation}
\boldsymbol{\mu}_\mathbf{z}=\boldsymbol{\Sigma}_\mathbf{z}\left[\begin{matrix}\boldsymbol{\Lambda}\boldsymbol{\mu}-\mathbf{A}^\text{T}\mathbf{L}\mathbf{b} \\ \mathbf{L}\mathbf{b}\end{matrix}\right]=\left[\begin{matrix}\boldsymbol{\mu} \\ \mathbf{A}\boldsymbol{\mu}+\mathbf{b}\end{matrix}\right]
\end{equation}
Given the mean $\boldsymbol{\mu}_\mathbf{z}$ and the covariance matrix $\boldsymbol{\Sigma}_\mathbf{z}$ of the joint distribution of $\mathbf{x},\mathbf{y}$, by \eqref{22} and \eqref{23}, we then can obtain the mean of the covariance matrix of the marginal distribution $p(\mathbf{y})$, which are
\begin{align}
\boldsymbol{\mu}_\mathbf{y}&amp;=\mathbf{A}\boldsymbol{\mu}+\mathbf{b}, \\ \boldsymbol{\Sigma}_\mathbf{y}&amp;=\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T},
\end{align}
and also, by \eqref{eq:cgd.4} and \eqref{eq:cgd.5}, we can easily get mean and covariance matrix of the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$, which are given by
\begin{align}
\boldsymbol{\mu}_{\mathbf{x}\vert\mathbf{y}}&amp;=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}\big(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}\big) \\ \boldsymbol{\Sigma}_{\mathbf{x}\vert\mathbf{y}}&amp;=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{align}
In Bayesian approach, we can consider $p(\mathbf{x})$ as a prior distribution over $\mathbf{x}$, and if $\mathbf{y}$ is observed, the conditional distribution $p(\mathbf{x}\vert\mathbf{y})$ will represents the corresponding posterior distribution over $\mathbf{x}$.</p>

<p><span id="marg-cond-gaussian"><strong>Remark</strong>.<br />
Given a marginal Gaussian distribution for $\mathbf{x}$ and a conditional Gaussian distribution for $\mathbf{y}$ given $\mathbf{x}$ in the form
\begin{align}
p(\mathbf{x})&amp;=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\mu},\boldsymbol{\Lambda}^{-1}), \\ p(\mathbf{y}\vert\mathbf{x})&amp;=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\mathbf{x}+\mathbf{b},\mathbf{L}^{-1}),
\end{align}
the marginal distribution of $\mathbf{y}$ and the conditional distribution of $\mathbf{x}$ given $\mathbf{y}$ are then given by
\begin{align}
p(\mathbf{y})&amp;=\mathcal{N}(\mathbf{y}\vert\mathbf{A}\boldsymbol{\mu}+\mathbf{b},\mathbf{L}^{-1}+\mathbf{A}\boldsymbol{\Lambda}^{-1}\mathbf{A}^\text{T}), \\ p(\mathbf{x}\vert\mathbf{y})&amp;=\mathcal{N}(\mathbf{x}\vert\boldsymbol{\Sigma}(\mathbf{A}^\text{T}\mathbf{L}(\mathbf{y}-\mathbf{b})+\boldsymbol{\Lambda}\boldsymbol{\mu}),\boldsymbol{\Sigma})
\end{align}
where
\begin{equation}
\boldsymbol{\Sigma}=(\boldsymbol{\Lambda}+\mathbf{A}^\text{T}\mathbf{L}\mathbf{A})^{-1}
\end{equation}</span></p>

<h2 id="references">References</h2>
<p>[1] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a>.</p>

<p>[2] Christopher M. Bishop. <a href="https://link.springer.com/book/9780387310732">Pattern Recognition and Machine Learning</a>. Springer New York, NY, 2006.</p>

<p>[3] Gilbert Strang. <a href="http://math.mit.edu/~gs/linearalgebra/">Introduction to Linear Algebra, 5th edition</a>, 2016.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The definition of covariance matrix $\boldsymbol{\Sigma}$ can be rewritten as
\begin{equation}
\boldsymbol{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation}
Let $\mathbf{z}\in\mathbb{R}^D$, we have
\begin{equation}
\Var(\mathbf{z}^\text{T}\mathbf{X})=\mathbf{z}^\text{T}\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^\text{T}\boldsymbol{\Sigma}\mathbf{z}
\end{equation}
And since $\Var(\mathbf{z}^\text{T}\mathbf{X})\geq0$, we also have that $\mathbf{z}^\text{T}\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\boldsymbol{\Sigma}$ is a positive semi-definite matrix. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="probability-statistics" /><category term="normal-distribution" /><summary type="html"><![CDATA[Gaussian Distribution]]></summary></entry><entry><title type="html">Power Series</title><link href="http://localhost:4000/2021/09/21/power-series.html" rel="alternate" type="text/html" title="Power Series" /><published>2021-09-21T15:40:00+07:00</published><updated>2021-09-21T15:40:00+07:00</updated><id>http://localhost:4000/2021/09/21/power-series</id><content type="html" xml:base="http://localhost:4000/2021/09/21/power-series.html"><![CDATA[<blockquote>
  <p>Recall that in the previous post, <a href="/2021/09/06/infinite-series-of-constants.html">Infinite Series of Constants</a>, we mentioned a type of series called <strong>power series</strong> a lot. In the content of this post, we will be diving deeper into details of that series.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#power-series">Power Series</a></li>
  <li><a href="#int-conv">The Interval of Convergence</a>
    <ul>
      <li><a href="#eg1">Example</a></li>
    </ul>
  </li>
  <li><a href="#dif-int-power-series">Differentiation and Integration of Power Series</a>
    <ul>
      <li><a href="#dif-power-series">Differentiation of Power Series</a></li>
      <li><a href="#int-power-series">Integration of Power Series</a></li>
      <li><a href="#eg2">Example</a></li>
    </ul>
  </li>
  <li><a href="#taylor-series-formula">Taylor Series, Taylor’s Formula</a>
    <ul>
      <li><a href="#taylor-series">Taylor Series</a></li>
      <li><a href="#taylors-formula">Taylor’s Formula</a></li>
    </ul>
  </li>
  <li><a href="#op-power-series">Operations on Power Series</a>
    <ul>
      <li><a href="#mult">Multiplication</a></li>
      <li><a href="#div">Division</a></li>
      <li><a href="#sub">Substitution</a></li>
      <li><a href="#even-odd-funcs">Even and Odd Functions</a></li>
    </ul>
  </li>
  <li><a href="#uni-conv-power-series">Uniform Convergence for Power Series</a>
    <ul>
      <li><a href="#cont-sum">Continuity of the Sum</a></li>
      <li><a href="#int">Integrating term by term</a></li>
      <li><a href="#dif">Differentiating term by term</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="power-series">Power Series</h2>
<p>A <strong>power series</strong> is a series of the form
\begin{equation}
\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots,
\end{equation}
where the coefficient $a_n$ are constants and $x$ is a variable.</p>

<h2 id="int-conv">The Interval of Convergence</h2>
<p>Similar to what we have done in the post of <a href="/2021/09/06/infinite-series-of-constants.html">infinite series of constants</a>, we begin studying properties of power series by considering their convergence behavior.</p>

<p><strong>Lemma 1</strong><br />
<em>If a power series $\sum a_nx^n$ converges at $x_1$, with $x_1\neq 0$, then it converges <a href="/2021/09/06/infinite-series-of-constants.html#abs-conv">absolutely</a> at all $x$ with $\vert x\vert&lt;\vert x_1\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\vert x\vert&gt;\vert x_1\vert$.</em></p>

<p><strong>Proof</strong><br />
By the <a href="/2021/09/06/infinite-series-of-constants.html#nth-term-test">$n$-th term test</a>, we have that if $\sum a_nx^n$ converges, then $a_nx^n\to 0$. In particular, if $n$ is sufficiently large, then $\vert a_n{x_1}^n\vert&lt;1$, and therefore
\begin{equation}
\vert a_nx^n\vert=\vert a_n{x_1}^n\vert\left\vert\dfrac{x}{x_1}\right\vert^n&lt;r^n,\tag{1}\label{1}
\end{equation}
where $r=\vert\frac{x}{x_1}\vert$. Suppose that $\vert x\vert&lt;\vert x_1\vert$, we have
\begin{equation}
r=\left\vert\dfrac{x}{x_1}\right\vert&lt;1,
\end{equation}
which leads to the result that geometric series $\sum r^n$ converges (with the sum $\frac{1}{1-r}$). And hence, from \eqref{1} and by the <a href="/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, the series $\sum\vert a_nx^n\vert$ also converges.</p>

<p>Moreover, if $\sum a_n{x_1}^n$ diverges, then $\sum\vert a_n{x_1}^n\vert$ also diverges. By the <a href="/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, for any $x$ such that $\vert x\vert&gt;\vert x_1\vert$, we also have that $\sum\vert a_nx^n\vert$ diverges. This leads to the divergence of $\sum a_nx^n$, because if the series $\sum a_nx^n$ converges, so does $\sum\vert a_nx^n\vert$, which contradicts to our result.</p>

<p>These are some main facts about the convergence behavior of an arbitrary power series and some properties of its:</p>
<ul>
  <li>Given a power series $\sum a_nx^n$, precisely one of the following is true:
    <ul>
      <li>The series converges only for $x=0$.</li>
      <li>The series is absolutely convergent for all $x$.</li>
      <li>There exists a positive real number $R$ such that the series is absolutely convergent for $\vert x\vert&lt;R$ and divergent for $\vert x\vert&gt;R$.</li>
    </ul>
  </li>
  <li>The positive real number $R$ is called <strong>radius of convergence</strong> of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$.</li>
  <li>The set of all $x$’s for which a power series converges is called its <strong>interval of convergence</strong>.</li>
  <li>When the series converges only for $x=0$, we define $R=0$; and we define $R=\infty$ when the series converges for all $x$.</li>
  <li>Every power series $\sum a_nx^n$ has a radius of convergence $R$, where $0\leq R\leq\infty$, with the property that the series converges absolutely if $\vert x\vert&lt;R$ and diverges if $\vert x\vert&gt;R$.</li>
</ul>

<h3 id="eg1">Example</h3>
<p>Find the interval of convergence of the series
\begin{equation}
\sum_{n=0}^{\infty}\dfrac{x^n}{n+1}=1+\dfrac{x}{2}+\dfrac{x^2}{3}+\ldots
\end{equation}</p>

<p><strong>Solution</strong><br />
In order to find the interval of convergence of a series, we begin by identifying its radius of convergence.</p>

<p>Consider a power series $\sum a_nx^n$. Suppose that this limit exists, and has $\infty$ as an allowed value, we have
\begin{equation}
\lim_{n\to\infty}\dfrac{\vert a_{n+1}x^{n+1}\vert}{a_nx^n}=\lim_{n\to\infty}\left\vert\dfrac{a_{n+1}}{a_n}\right\vert.\vert x\vert=\dfrac{\vert x\vert}{\lim_{n\to\infty}\left\vert\frac{a_n}{a_{n+1}}\right\vert}=L
\end{equation}
By the <a href="/2021/09/06/infinite-series-of-constants.html#ratio-test">ratio test</a>, we have $\sum a_nx^n$ converges absolutely if $L&lt;1$ and diverges in case of $L&gt;1$. Or in other words, the series converges absolutely if
\begin{equation}
\vert x\vert&lt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert,
\end{equation}
or diverges if
\begin{equation}
\vert x\vert&gt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}
From the definition of radius of convergence, we can choose the radius of converge of $\sum a_nx^n$ as
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}</p>

<p>Back to our problem, for the series $\sum\frac{x^n}{n+1}$, we have its radius of convergence is
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert=\lim_{n\to\infty}\dfrac{\frac{1}{n+1}}{\frac{1}{n+2}}=\lim_{n\to\infty}\dfrac{n+2}{n+1}=1
\end{equation}
At $x=1$, the series becomes the <em>harmonic series</em> $1+\frac{1}{2}+\frac{1}{3}+\ldots$, which diverges; and at $x=-1$, it is the <em>alternating harmonic series</em> $1-\frac{1}{2}+\frac{1}{3}-\ldots$, which converges. Hence, the interval of convergence of the series is $[-1,1)$.</p>

<h2 id="dif-int-power-series">Differentiation and Integration of Power Series</h2>

<p>It is easily seen that the sum of the series $\sum_{n=0}^{\infty}a_nx^n$  is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots\tag{2}\label{2}
\end{equation}
This relation between the series and the function is also expressed by saying that $\sum a_nx^n$ is a <strong>power series expansion</strong> of $f(x)$.</p>

<p>These are some crucial facts about that relation.</p>
<ul>
  <li>(i) The function $f(x)$ defined by \eqref{2} is continuous on the open interval $(-R,R)$.</li>
  <li>(ii) The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula
\begin{equation}
f’(x)=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots\tag{3}\label{3}
\end{equation}</li>
  <li>(iii) If $x$ is any point in $(-R,R)$, then
\begin{equation}
\int_{0}^{x}f(t)\,dt=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{4}\label{4}
\end{equation}</li>
</ul>

<p><strong>Remark</strong><br />
We have that series \eqref{3} and \eqref{4} converge on the interval $(-R,R)$.</p>

<p><strong>Proof</strong></p>
<ol>
  <li>
    <p>We begin by proving the convergence on $(-R,R)$ of \eqref{3}.<br />
Let $x$ be a point in the interval $(-R,R)$ and choose $\epsilon&gt;0$ so that $\vert x\vert+\epsilon&lt;R$. Since $\vert x\vert+\epsilon$ is in the interval, $\sum\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert$ converges.<br />
We continue by proving the inequality
\begin{equation}
\vert nx^{n-1}\vert\leq\left(\vert x\vert+\epsilon\right)^n\hspace{1cm}\forall n\geq n_0,
\end{equation}
where $\epsilon&gt;0$, $n_0$ is a positive integer.<br />
We have
\begin{align}
\lim_{n\to\infty}n^{1/n}&amp;=\lim_{n\to\infty} \\ &amp;=\lim_{n\to\infty}\exp\left(\frac{\ln n}{n}\right) \\ &amp;=\exp\left(\lim_{n\to\infty}\frac{\ln n}{n}\right) \\ &amp;={\rm e}^0=1,
\end{align}
where in the fourth step, we use the <em>L’Hospital theorem</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Therefore, as $n\to\infty$
\begin{equation}
n^{1/n}\vert x\vert^{1-1/n}\to\vert x\vert
\end{equation}
Then for all sufficiently large $n$’s
\begin{align}
n^{1/n}\vert x\vert^{1-1/n}&amp;\leq\vert x\vert+\epsilon \\ \vert nx^{n-1}\vert&amp;\leq\left(\vert x\vert+\epsilon\right)^n
\end{align}
This implies that
\begin{equation}
\vert na_nx^{n-1}\vert\leq\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert
\end{equation}
By the <a href="/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a>, we have that the series $\sum\vert na_nx^{n-1}\vert$ converges, and so does $\sum na_nx^{n-1}$.</p>
  </li>
  <li>
    <p>Since $\sum\vert a_nx^n\vert$ converges and
\begin{equation}
\left\vert\dfrac{a_nx^n}{n+1}\right\vert\leq\vert a_nx^n\vert,
\end{equation}
the <a href="/2021/09/06/infinite-series-of-constants.html#comparison-test">comparison test</a> implies that $\sum\left\vert\frac{a_nx^n}{n+1}\right\vert$ converges, and therefore
\begin{equation}
x\sum\frac{a_nx^n}{n+1}=\sum\frac{1}{n+1}a_nx^{n+1}
\end{equation}
also converges.</p>
  </li>
</ol>

<h3 id="dif-power-series">Differentiation of Power Series</h3>

<p>If we instead apply (ii) to the function $f’(x)$ in \eqref{3}, then it follows that $f’(x)$ is also differentiable. Doing the exact same process to $f’'(x)$, we also have that $f’'(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:</p>

<p><em>In the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term</em>.
\begin{equation}
\dfrac{d}{dx}\left(\sum a_nx^n\right)=\sum\dfrac{d}{dx}(a_nx^n)
\end{equation}</p>

<h3 id="int-power-series">Integration of Power Series</h3>

<p>Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \eqref{4} as
\begin{equation}
\int\left(\sum a_nx^n\right)\,dx=\sum\left(\int a_nx^n\,dx\right)
\end{equation}</p>

<h3 id="eg2">Example</h3>

<p>Find a power series expansion of ${\rm e}^x$.</p>

<p><strong>Solution</strong><br />
Since ${\rm e}^x$ is the only function that equals its own derivatives<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We therefore want each term to be the derivative of the one that follows it.</p>

<p>Starting with $1$ as the constant term, the next should be $x$, then $\frac{1}{2}x^2$, then $\frac{1}{2.3}x^3$, and so on. This produces the series
\begin{equation}
1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots,\tag{5}\label{5}
\end{equation}
which converges for all $x$ because
\begin{equation}
R=\lim_{n\to\infty}\dfrac{\frac{1}{n!}}{\frac{1}{(n+1)!}}=\lim_{n\to\infty}(n+1)=\infty
\end{equation}
We have constructed the series \eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$,
\begin{equation}
{\rm e}^x=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots
\end{equation}</p>

<h2 id="taylor-series-formula">Taylor Series, Taylor’s Formula</h2>

<h3 id="taylor-series">Taylor Series</h3>
<p>Assume that $f(x)$ is the sum of a power series with positive radius of convergence
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots,\hspace{1cm}R&gt;0\tag{6}\label{6}
\end{equation}
By the results obtained from previous section, differentiating \eqref{6} term by term we have
\begin{align}
f^{(1)}(x)&amp;=a_1+2a_2x+3a_3x^2+\ldots \\ f^{(2)}(x)&amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\ldots \\ f^{(3)}(x)&amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\ldots
\end{align}
and in general,
\begin{equation}
f^{(n)}(x)=n!a_n+A(x),\tag{7}\label{7}
\end{equation}
where $A(x)$ contains $x$ as a factor.</p>

<p>Since these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \eqref{7} we obtain
\begin{equation}
f^{(n)}(0)=n!a_n
\end{equation}
so
\begin{equation}
a_n=\dfrac{f^{(n)}(0)}{n!}
\end{equation}
Putting this result in \eqref{6}, our series becomes
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\ldots\tag{8}\label{8}
\end{equation}
This power series is called <strong>Taylor series</strong> of $f(x)$ [at $x=0$], which is named after the person who introduced it, Brook Taylor.</p>

<p>If we use the convention that $0!=1$, then \eqref{8} can be written as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(0)}{n!}x^n
\end{equation}
The numbers $a_n=\frac{f^{(n)}(0)}{n!}$ are called the <strong>Taylor coefficients</strong> of $f(x)$.</p>

<p><strong>Remark</strong><br />
Given a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?<br />
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\ldots
\end{equation}
Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is
\begin{align}
f(x)&amp;=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\ldots\tag{9}\label{9}
\end{align}</p>

<h3 id="taylors-formula">Taylor’s Formula</h3>
<p>If we break off the Taylor series on the right side of \eqref{8} at the term containing $x^n$ and define the <em>remainder</em> $R_n(x)$ by the equation
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\tag{10}\label{10}
\end{equation}
then the Taylor series on the right side of \eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when
\begin{equation}
\lim_{n\to\infty}R_n(x)=0
\end{equation}
Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing
\begin{equation}
R_n(x)=S_n(x)x^{n+1}
\end{equation}
for $x\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for $0\leq t\leq x$ (or $x\leq t\leq 0$) by writing
\begin{multline}
F(t)=f(x)-f(t)-f^{(1)}(t)(x-t)-\dfrac{f^{(2)}(t)}{2!}(x-t)^2-\ldots \\ -\dfrac{f^{(n)}(t)}{n!}(x-t)^n-S_n(x)(x-t)^{n+1}
\end{multline}
It is easily seen that $F(x)=0$. Also, from equation \eqref{10}, we have that $F(0)=0$. Then by the <em>Mean Value Theorem</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>, $F’(c)=0$ for some constant $c$ between $0$ and $x$.</p>

<p>By differentiating $F(t)$ w.r.t $t$, and evaluate it at $t=c$, we have
\begin{equation}
F’(c)=-\dfrac{f^{(n+1)}(c)}{n!}(x-c)^n+S_n(x)(n+1)(x-c)^n=0
\end{equation}
so
\begin{equation}
S_n(x)=\dfrac{f^{(n+1)}(c)}{(n+1)!}
\end{equation}
and
\begin{equation}
R_n(x)=S_n(x)x^{n+1}=\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}
\end{equation}
which makes \eqref{10} become
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1},
\end{equation}
where $c$ is some number between $0$ and $x$. This equation is called <strong>Taylor’s formula with derivative remainder</strong>.</p>

<p>Moreover, with this formula we can rewrite \eqref{9} as
\begin{multline}
f(x)=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots \\ +\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1},\tag{11}\label{11}
\end{multline}
where $c$ is some number between $a$ and $x$.</p>

<p>The polynomial part of \eqref{11}
\begin{multline}
\sum_{j=0}^{n}\dfrac{f^{(j)}(a)}{j!}(x-a)^j=f(a)+f^{(1)}(a)(x-a) \\ +\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n
\end{multline}
is called the <strong>nth-degree Taylor polynomial at</strong> $x=a$.</p>

<p>On the other hand, the remainder part of \eqref{11}
\begin{equation}
R_n(x)=\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1}
\end{equation}
is often called <strong>Lagrange’s remainder formula</strong>.</p>

<p><strong>Remark</strong><br />
It is worth remarking that power series expansions are <em>unique</em>. This means that if a function $f(x)$ can be expressed as a sum of a power series by <em>any method</em>, then this series must be the Taylor series of $f(x)$.</p>

<h2 id="op-power-series">Operations on Power Series</h2>

<h3 id="mult">Multiplication</h3>
<p>Suppose we are given two power series expansions
\begin{align}
f(x)&amp;=\sum a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+\ldots\tag{12}\label{12} \\ g(x)&amp;=\sum b_nx^n=b_0+b_1x+b_2x^2+b_3x^3+\ldots\tag{13}\label{13}
\end{align}
both valid on $(-R,R)$. If we multiply these two series term by term, we obtain the power series
\begin{multline}
a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2 \\ +(a_0b_3+a_1b_2+a_2b_1+a_3b_0)x^3+\ldots
\end{multline}
Briefly, we have multiplied \eqref{12} and \eqref{13} to obtain
\begin{equation}
f(x)g(x)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n}a_kb_{n-k}\right)x^n\tag{14}\label{14}
\end{equation}
By the <strong>Theorem 10</strong> from <a href="/2021/09/06/infinite-series-of-constants.html#abs-vs-cond">Absolute vs Conditionally Convergence</a>, we have that this product of the series \eqref{12} and \eqref{13} actually converges on the interval $(-R,R)$ to the product of the functions $f(x)$ and $g(x)$, as indicated by \eqref{14}.</p>

<h3 id="div">Division</h3>
<p>With the two series \eqref{12} and \eqref{13}, we have
\begin{equation}
\dfrac{\sum a_nx^n}{\sum b_nx^n}=\left(\sum a_nx^n\right).\left(\dfrac{1}{\sum b_nx^n}\right)
\end{equation}
This suggests us that if we can expand $\frac{1}{\sum b_nx^n}$ in a power series with positive radius of convergence $\sum c_nx^n$, and multiply this series by $\sum a_nx^n$, we can compute the division of our two series $\sum a_nx^n$ and $\sum b_nx^n$.</p>

<p>To do the division properly, it is necessary to assume that $b_0\neq0$ (for the case $x=0$). Moreover, without any loss of generality, we may assume that $b_0=1$, because with the assumption that $b_0\neq0$, we simply factor it out
\begin{equation}
\dfrac{1}{b_0+b_1x+b_2x^2+\ldots}=\dfrac{1}{b_0}.\dfrac{1}{1+\frac{b_1}{b_0}x+\frac{b_2}{b_0}x^2+\ldots}
\end{equation}</p>

<p>We begin by determining the $c_n$’s. Since $\frac{1}{\sum b_nx^n}=\sum c_nx^n$, then $(\sum b_nx^n)(\sum c_nx^n)=1$, so
\begin{multline}
b_0c_0+(b_0c_1+b_1c_0)x+(b_0c_2+b_1c_1+b_2c_0)x^2+\ldots \\ +(b_0c_n+b_1c_{n-1}+\ldots+b_nc_0)x^n+\ldots=1,
\end{multline}
and since $b_0=1$, we can determine the $c_n$’s recursively
\begin{align}
c_0&amp;=1 \\ c_1&amp;=-b_1c_0 \\ c_2&amp;=-b_1c_1-b_2c_0 \\ &amp;\vdots \\ c_n&amp;=-b_1c_{n-1}-b_2c_{n-2}-\ldots-b_nc_0 \\ &amp;\vdots
\end{align}
Now our work reduces to proving that the power series $\sum c_nx^n$ with these coefficients has positive radius of convergence, and for this it suffices to show that the series converges for at least one nonzero $x$.</p>

<p>Let $r$ be any number such that $0&lt;r&lt;R$, so that $\sum b_nr^n$ converges. Then there exists a constant $K\geq 1$ with the property that $\vert b_nr^n\vert\leq K$ or $\vert b_n\vert\leq\frac{K}{r^n}$ for all $n$. Therefore,
\begin{align}
\vert c_0\vert&amp;=1\leq K, \\ \vert c_1\vert&amp;=\vert b_1c_0\vert=\vert b_1\vert\leq \dfrac{K}{r}, \\ \vert c_2\vert&amp;\leq\vert b_1c_1\vert+\vert b_2c_0\vert\leq\dfrac{K}{r}.\dfrac{K}{r}+\dfrac{K}{r^2}.K=\dfrac{2K^2}{r^2}, \\ \vert c_3\vert&amp;\leq\vert b_1c_2\vert+\vert b_2c_1\vert+\vert b_3c_0\vert\leq\dfrac{K}{r}.\dfrac{2K^2}{r^2}+\dfrac{K}{r^2}.\dfrac{K}{r}+\dfrac{K}{r^3}.K \\ &amp;\hspace{5.3cm}\leq(2+1+1)\dfrac{K^3}{r^3}=\dfrac{4K^3}{r^3}=\dfrac{2^2K^3}{r^3},
\end{align}
since $K^2\leq K^3$ since $K\geq1$. In general,
\begin{align}
\vert c_n\vert&amp;\leq\vert c_1b_{n-1}\vert+\vert c_2b_{n-2}\vert+\ldots+\vert b_nc_0\vert \\ &amp;\leq\dfrac{K}{r}.\dfrac{2^{n-2}K^{n-1}}{r^{n-1}}+\dfrac{K}{r^2}.\dfrac{2^{n-3}K^{n-2}}{r^{n-2}}+\ldots+\dfrac{K}{r^n}.K \\ &amp;\leq(2^{n-2}+2^{n-3}+\ldots+1+1)\dfrac{K^n}{r^n}=\dfrac{2^{n-1}K^n}{r^n}\leq\dfrac{2^nK^n}{r^n}
\end{align}
Hence, for any $x$ such that $\vert x\vert&lt;\frac{r}{2K}$, we have that the series $\sum c_nx^n$ converges absolutely, and therefore converges, or in other words, $\sum c_nx^n$ has nonzero radius of convergence.</p>

<h3 id="sub">Substitution</h3>
<p>If a power series
\begin{equation}
f(X)=a_0+a_1x+a_2x^2+\ldots\tag{15}\label{15}
\end{equation}
converges for $\vert x\vert&lt;R$ and if $\vert g(x)\vert&lt;R$, then we can find $f(g(x))$ by substituting $g(x)$ for $x$ in \eqref{15}.</p>

<p>Suppose $g(x)$ is given by a power series,
\begin{equation}
g(x)=b_0+b_1x+b_2x^2+\ldots,\tag{16}\label{16}
\end{equation}
therefore,
\begin{align}
f(g(x))&amp;=a_0+a_1g(x)+a_2g(x)^2+\ldots \\ &amp;=a_0+a_1(b+0+b_1x+\ldots)+a_2(b_0+b_1x+\ldots)^2+\ldots
\end{align}
The power series formed in this way converges to $f(g(x))$ whenever \eqref{16} is absolutely convergent and $\vert g(x)\vert&lt;R$.</p>

<h3 id="even-odd-funcs">Even and Odd Functions</h3>
<p>A function $f(x)$ defined on $(-R,R)$ is said to be <strong>even</strong> if
\begin{equation}
f(-x)=f(x),
\end{equation}
and <strong>odd</strong> if
\begin{equation}
f(-x)=-f(x)
\end{equation}
Then if $f(x)$ is an even function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n}x^{2n}=a_0+a_2x^2+a_4x^4+\ldots
\end{equation}
and if $f(x)$ is an odd function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n+1}x^{2n+1}=a_1x+a_3x^3+a_5x^5+\ldots
\end{equation}
since if $f(x)=\sum_{n=0}^{\infty}a_nx^n$ is even, then $\sum_{n=0}^{\infty}a_nx^n=\sum_{n=0}^{\infty}(-1)^na_nx^n$, so by the uniqueness of the Taylor series expansion, we have that $a_n=(-1)^na_n$; similarly, $a_n=(-1)^{n+1}a_n$ if $f(x)$ is an odd function.</p>

<h2 id="uni-conv-power-series">Uniform Convergence for Power Series</h2>
<p>Consider a power series $\sum a_nx^n$ with positive radius of convergence $R$, and let $f(x)$ be its sum.</p>

<p>In the <a href="#dif-int-power-series">section</a> above, we stated that $f(x)$ is continuous and differentiable on $(-R,R)$, and we can differentiate and integrate it term by term. So let’s prove these statements!</p>

<p>Let $S_n(x)$ be the $n$-th partial sum of the series, so that
\begin{equation}
S_n(x)=\sum_{i=0}^{n}a_ix^i=a_0+a_1x+a_2x^2+\ldots+a_nx^n
\end{equation}
Similar to what we did in <a href="#taylors-formula">Taylor’s formula</a>, we write
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
Thus, the remainder
\begin{equation}
R_n(x)=a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots
\end{equation}</p>

<p>For each $x$ in the interval of convergence, we know that $R_n(x)\to0$ as $n\to\infty$; that is, for any given $\epsilon&gt;0$, and for an integer $n_0$ large enough, we have
\begin{equation}
\vert R_n(x)\vert&lt;\epsilon\hspace{1cm}n\geq n_0,\tag{17}\label{17}
\end{equation}
This is true for each $x$ individually, and is an equivalent way of expressing the fact that $\sum a_nx^n$ converges to $f(x)$.</p>

<p>Moreover, for every $x$ in the given a closed interval $\vert x\vert\leq\vert x_1\vert&lt;R$, we have
\begin{align}
\vert R_n(x)\vert&amp;=\left\vert a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots\right\vert \\ &amp;\leq\left\vert a_{n+1}x^{n+1}\right\vert+\left\vert a_{n+2}x^{n+2}\right\vert+\ldots \\ &amp;\leq\left\vert a_{n+1}{x_1}^{n+1}\right\vert+\left\vert a_{n+2}{x_1}^{n+2}\right\vert+\ldots
\end{align}
Because of the <a href="/2021/09/06/infinite-series-of-constants.html#abs-conv">absolute convergence</a> of $\sum a_n{x_1}^n$, the last sum can be made $&lt;\epsilon$ by taking $n$ large enough, $n\geq n_0$. Therefore, we have that \eqref{17} holds for all $x$ inside the closed interval $\vert x\vert\leq\vert x_1\vert$ inside the interval of convergence $(-R,R)$.</p>

<p>Or in other words, $R_n(x)$ can be made small <em>independently of $x$ in the given closed interval</em> $\vert x\vert\leq\vert x_1\vert$, which is equivalent to saying that the series $\sum a_nx^n$ is <strong>uniformly convergent</strong> in this interval<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>.</p>

<h3 id="cont-sum">Continuity of the Sum</h3>
<p>In order to prove that $f(x)$ is continuous on $(-R,R)$, it suffices to prove that $f(x)$ is continuous at each point $x_0$ in the interval of convergence.</p>

<p>Consider a closed subinterval $\vert x\vert\leq\vert x_1\vert&lt;R$ containing $x_0$ in its interior. If $\epsilon&gt;0$ is given, then by uniform convergence we can find an $n$ such that $\vert R_n(x)\vert&lt;\epsilon$ for all $x$’s in the subinterval.</p>

<p>Since the polynomial $S_n(x)$ is continuous at $x_0$, we can find $\delta&gt;0$ small that $\vert x-x_0\vert&lt;\delta$ implies $x$ lies in the subinterval and $\vert S_n(x)-S_n(x_0)\vert&lt;\epsilon$. Putting these conditions together we find that $\vert x-x_0\vert&lt;\delta$ implies
\begin{align}
\vert f(x)-f(x_0)\vert&amp;=\left\vert S_n(x)+R_n(x)-\left(S_n(x_0)+R_n(x_0)\right)\right\vert \\ &amp;=\left\vert\left(S_n(x)-S_n(x_0)\right)+R_n(x)-R_n(x_0)\right\vert \\ &amp;\leq\left\vert S_n(x)-S_n(x_0)\right\vert+\left\vert R_n(x)\right\vert+\left\vert R_n(x_0)\right\vert \\ &amp;&lt;\epsilon+\epsilon+\epsilon=3\epsilon
\end{align}
which proves the continuity of $f(x)$ at $x_0$.</p>

<h3 id="int">Integrating term by term</h3>
<p>With what we have just proved that $f(x)=\sum a_nx^n$ is continuous on $(-R,R)$, we can therefore integrate this function between $a$ and $b$ that lie inside the interval
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx
\end{equation}
We need to prove that the right side of this equation can be integrated term by term, which is
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx=\sum\int_{a}^{b}a_nx^n\,dx\tag{18}\label{18}
\end{equation}
In order to prove this, we begin by observing that $S_n(x)$ is a polynomial, and for that reason it is continuous. Thus, all there of the functions in
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
are continuous on $(-R,R)$. This allows us to write
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}S_n(x)\,dx+\int_{a}^{b}R_n(x)\,dx
\end{equation}
Moreover, we can integrate $S_n(x)$ term by term
\begin{align}
\int_{a}^{b}S_n(x)\,dx&amp;=\int_{a}^{b}\left(a_0+a_1x+a_2x^2+\ldots+a_nx^n\right)\,dx \\ &amp;=\int_{a}^{b}a_0\,dx+\int_{a}^{b}a_1x\,dx+\int_{a}^{b}a_2x^2\,dx+\ldots+\int_{a}^{b}a_nx^n\,dx
\end{align}
To prove \eqref{18}, it therefore suffices to show that as $n\to\infty$
\begin{equation}
\int_{a}^{b}R_n(x)\,dx\to 0
\end{equation}
By uniform convergence, if $\epsilon&gt;0$ is given and $\vert x\vert\leq\vert x_1\vert&lt;R$ is a closed subinterval of $(-R,R)$ that contains both $a,b$, then $\vert R_n(x)\vert&lt;\epsilon$ for all $x$ in the subinterval and $n$ large enough. Hence,
\begin{equation}
\left\vert\int_{a}^{b}R_n(x)\,dx\right\vert\leq\int_{a}^{b}\left\vert R_n(x)\right\vert\,dx&lt;\epsilon\vert b-a\vert
\end{equation}
for any $n$ large enough, which proves our statement.</p>

<p>As a special case of \eqref{18}, we take the limits $0$ and $x$ instead of $a$ and $b$, and obtain
\begin{align}
\int_{a}^{b}f(t)\,dt&amp;=\sum\dfrac{1}{n+1}a_nx^{n+1} \\ &amp;=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{19}\label{19}
\end{align}</p>

<h3 id="dif">Differentiating term by term</h3>
<p>We now prove that the function $f(x)$ is not only continuous but also differentiable on $(-R,R)$, and that its derivative can be calculated by differentiating term by term
\begin{equation}
f’(x)=\sum na_nx^{n-1}
\end{equation}
It is easily seen that the series on right side of this equation is exact the series on the right side of \eqref{3}, which is convergent on $(-R,R)$ as we proved. If we denote its sum by $g(x)$
\begin{equation}
g(x)=\sum na_nx^{n-1}=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots,
\end{equation}
then \eqref{19} tells us that
\begin{align}
\int_{0}^{x}g(t)\,dt&amp;=a_1x+a_2x^2+a_3x^3+\ldots \\ &amp;=f(x)-a_0
\end{align}
Since the left side of this has a derivative, so does the right side, and by differentiating we obtain
\begin{equation}
f’(x)=g(x)=\sum na_nx^{n-1}
\end{equation}</p>

<h2 id="references">References</h2>
<p>[1] George F.Simmons. <a href="https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424">Calculus With Analytic Geometry - 2nd Edition</a>.</p>

<p>[2] Marian M. <a href="https://www.springer.com/gp/book/9780387789323">A Concrete Approach to Classical Analysis</a>.</p>

<p>[3] MIT 18.01. <a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/">Single Variable Calculus</a>.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><strong>Theorem</strong> (<em>L’Hospital</em>)<br />
<em>Assume $f$ and $g$ are real and differentiable on $]a,b[$ and $g’(x)\neq 0$ for all $x\in]a,b[$, where $-\infty\leq a&lt;b\leq\infty$. Suppose as $x\to a$,
\begin{equation}
\dfrac{f’(x)}{g’(x)}\to A\,(\in[-\infty,\infty])
\end{equation}
If as $x\to a$, $f(x)\to 0$ and $g(x)\to 0$ or if $g(x)\to+\infty$ as $x\to a$, then
\begin{equation}
\dfrac{f(x)}{g(x)}\to A
\end{equation}
as $x\to a$.</em> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><strong>Proof</strong><br />
Consider the function $f(x)=a^x$.<br />
Using the definition of the derivative, we have
\begin{align}
\dfrac{d}{dx}f(x)&amp;=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h} \\ &amp;=\lim_{h\to 0}\dfrac{a^{x+h}-a^x}{h} \\ &amp;=a^x\lim_{h\to 0}\dfrac{a^h-1}{h}
\end{align}
Therefore,
\begin{equation}
\lim_{h\to 0}\dfrac{a^h-1}{h}=1
\end{equation}
then, let $n=\frac{1}{h}$, we have
\begin{equation}
a=\lim_{h\to 0}\left(1+\dfrac{1}{h}\right)^{1/h}=\lim_{n\to\infty}\left(1+\dfrac{1}{n}\right)^n={\rm e}
\end{equation}
Thus, $f(x)=a^x={\rm e}^x$. Every function $y=c{\rm e}^x$ also satisfies the differential equation $\frac{dy}{dx}=y$, because
\begin{equation}
\dfrac{dy}{dx}=\dfrac{d}{dx}c{\rm e}^x=c\dfrac{d}{dx}{\rm e}^x=c{\rm e}^x=y
\end{equation}<br />
The rest of our proof is to prove that these are only functions that are unchanged by differentiation.<br />
To prove this, suppose $f(x)$ is any function with that property. By the quotient rule,
\begin{equation}
\dfrac{d}{dx}\dfrac{f(x)}{e^x}=\dfrac{f’(x)e^x-e^x f(x)}{e^{2x}}=\dfrac{e^x f(x)-e^x f(x)}{e^{2x}}=0
\end{equation}
which implies that
\begin{equation}
\dfrac{f(x)}{e^x}=c,
\end{equation}
for some constant $c$, and so $f(x)=ce^x$. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><strong>Theorem</strong> (<em>Mean Value Theorem</em>)<br />
<em>If a function $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable in the open interval $(a,b)$, then there exists at least one number $c$ between $a$ and $b$ with the property that</em>
\begin{equation}
f’(c)=\frac{f(b)-f(a)}{b-a}
\end{equation} <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>We will talk more about uniform convergence in the post of sequences. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="power-series" /><category term="taylor-series" /><category term="random-stuffs" /><summary type="html"><![CDATA[A note on Power Series]]></summary></entry><entry><title type="html">Infinite Series of Constants</title><link href="http://localhost:4000/2021/09/06/infinite-series-of-constants.html" rel="alternate" type="text/html" title="Infinite Series of Constants" /><published>2021-09-06T11:20:00+07:00</published><updated>2021-09-06T11:20:00+07:00</updated><id>http://localhost:4000/2021/09/06/infinite-series-of-constants</id><content type="html" xml:base="http://localhost:4000/2021/09/06/infinite-series-of-constants.html"><![CDATA[<blockquote>
  <p>A note on infinite series of constants.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#infinite-series">Infinite Series</a>
    <ul>
      <li><a href="#examples">Examples</a></li>
    </ul>
  </li>
  <li><a href="#convergent-sequences">Convergent Sequences</a>
    <ul>
      <li><a href="#sequences">Sequences</a></li>
      <li><a href="#lim-seq">Limits of Sequences</a></li>
    </ul>
  </li>
  <li><a href="#conv-div-series">Convergent and Divergent Series</a>
    <ul>
      <li><a href="#nth-term-test">$n$-th term test</a></li>
    </ul>
  </li>
  <li><a href="#gen-props-conv-series">General Properties of Convergent Series</a></li>
  <li><a href="#series-nonneg-ct">Series of Nonnegative terms. Comparison tests</a>
    <ul>
      <li><a href="#comparison-test">Comparison test</a></li>
      <li><a href="#limit-comparison-test">Limit comparison test</a></li>
    </ul>
  </li>
  <li><a href="#int-test-euler-c">The Integral test. Euler’s constant</a>
    <ul>
      <li><a href="#integral-test">Integral test</a></li>
      <li><a href="#euler-c">Euler’s constant</a></li>
    </ul>
  </li>
  <li><a href="#ratio-root">The Ratio test. Root test</a>
    <ul>
      <li><a href="#ratio-test">Ratio test</a></li>
      <li><a href="#root-test">Root test</a></li>
      <li><a href="#extended-ratio-test">The Extended Ratio tests of Raabe and Gauss</a>
        <ul>
          <li><a href="#kummers-theorem">Kummer’s theorem</a></li>
          <li><a href="#raabes-test">Raabe’s test</a></li>
          <li><a href="#gausss-test">Gauss’s test</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#alt-test-abs-conv">The Alternating Series test. Absolute Convergence</a>
    <ul>
      <li><a href="#alt-series">Alternating Series</a></li>
      <li><a href="#alt-series-test">Alternating Series test</a></li>
      <li><a href="#abs-conv">Absolute Convergence</a></li>
    </ul>
  </li>
  <li><a href="#abs-vs-cond">Absolute vs. Conditionally Convergence</a></li>
  <li><a href="#dirichlets-test">Dirichlet’s test</a>
    <ul>
      <li><a href="#abel-part-sum">Abel’s partial summation formula</a></li>
      <li><a href="#d-test">Dirichlet’s test</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="infinite-series">Infinite Series</h2>
<p>An <strong>infinite series</strong>, or simply a <strong>series</strong>, is an expression of the form
\begin{equation}
a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{\infty}a_n
\end{equation}</p>

<h3 id="examples">Examples</h3>
<ol>
  <li>
    <p><em>Infinite decimal</em>
\begin{equation}
.a_1a_2\ldots a_n\ldots=\dfrac{a_1}{10}+\dfrac{a_2}{10^2}+\ldots+\dfrac{a_n}{10^n}+\ldots,
\end{equation}
where $a_i\in\{0,1,\dots,9\}$.</p>
  </li>
  <li>
    <p><em>Power series expansion</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>
    <ul>
      <li>Geometric series
\begin{equation}
\dfrac{1}{1-x}=\sum_{n=0}^{\infty}x^n=1+x+x^2+x^3+\dots,\hspace{1cm}\vert x\vert&lt;1
\end{equation}</li>
      <li>Exponential function
\begin{equation}
{\rm e}^x=\sum_{n=0}^{\infty}\dfrac{x^n}{n!}=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots
\end{equation}</li>
      <li>Sine and cosine formulas
\begin{align}
\sin x&amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n+1}}{(2n+1)!}=x-\dfrac{x^3}{3!}+\dfrac{x^5}{5!}-\dfrac{x^7}{7!}+\ldots \\ \cos x&amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n}}{(2n)!}=1-\dfrac{x^2}{2!}+\dfrac{x^4}{4!}-\dfrac{x^6}{6!}+\ldots
\end{align}</li>
    </ul>
  </li>
</ol>

<h2 id="convergent-sequences">Convergent Sequences</h2>

<h3 id="sequences">Sequences</h3>
<p>If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$’s are said to form a <strong>sequence</strong> (denoted as $\{x_n\}$)
\begin{equation}
x_1,x_2,\dots,x_n,\dots
\end{equation}
We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.</p>

<p>A sequence $\{x_n\}$ is said to be <em>bounded</em> if there exists $A, B$ such that $A\leq x_n\leq B, \forall n$. $A, B$ respectively are called <em>lower bound</em>, <em>upper bound</em> of the sequence. A sequence that is not bounded is said to be <em>unbounded</em>.</p>

<h3 id="lim-seq">Limits of Sequences</h3>
<p>A sequence $\{x_n\}$ is said to have a number $L$ as <strong>limit</strong> if for each $\epsilon&gt;0$, there exists a positive integer $n_0$ that
\begin{equation}
\vert x_n-L\vert&lt;\epsilon\hspace{1cm}n\geq n_0
\end{equation}
We say that $x_n$ <em>converges to</em> $L$ <em>as</em> $n$ <em>approaches infinite</em> ($x_n\to L$ as $n\to\infty$) and denote this as
\begin{equation}
\lim_{n\to\infty}x_n=L
\end{equation}</p>
<ul>
  <li>A sequence is said to <strong>converge</strong> or to be <strong>convergent</strong> if it has a limit.</li>
  <li>A convergent sequence is bounded, but not all bounded sequences are convergent.</li>
  <li>If $x_n\to L,y_n\to M$, then
\begin{align}
&amp;\lim(x_n+y_n)=L+M \\ &amp;\lim(x_n-y_n)=L-M \\ &amp;\lim x_n y_n=LM \\ &amp;\lim\dfrac{x_n}{y_n}=\dfrac{L}{M}\hspace{1cm}M\neq0
\end{align}</li>
  <li>An <em>increasing</em> (or <em>decreasing</em>) sequence converges if and only if it is bounded.</li>
</ul>

<h2 id="conv-div-series">Convergent and Divergent Series</h2>
<p>Recall from the previous sections that if $a_1,a_2,\dots,a_n,\dots$ is a <em>sequence</em> of numbers, then
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots\tag{1}\label{1}
\end{equation}
is called an <em>infinite series</em>. We begin by establishing the sequence of <em>partial sums</em>
\begin{align}
s_1&amp;=a_1 \\ s_2&amp;=a_1+a_2 \\ &amp;\,\vdots \\ s_n&amp;=a_1+a_2+\dots+a_n \\ &amp;\,\vdots
\end{align}
The series \eqref{1} is said to be <strong>convergent</strong> if the sequences $\{s_n\}$ converges. And if $\lim s_n=s$, then we say that \eqref{1} converges to $s$, or that $s$ is the sum of the series.
\begin{equation}
\sum_{n=1}^{\infty}a_n=s
\end{equation}
If the series does not converge, we say that it <strong>diverges</strong> or is <strong>divergent</strong>, and no sum is assigned to it.</p>

<p><strong>Examples</strong> (<em>harmonic series</em>)<br />
Let’s consider the convergence of <em>harmonic series</em>
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots\tag{2}\label{2}
\end{equation}
Let $m$ be a positive integer and choose $n&gt;2^{m+1}$. We have
\begin{align}
s_n&amp;&gt;1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\dots+\frac{1}{2^{m+1}} \\ &amp;=\left(1+\frac{1}{2}\right)+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\ldots+\frac{1}{8}\right)+\ldots+\left(\frac{1}{2^m+1}+\ldots+\frac{1}{2^{m+1}}\right) \\ &amp;&gt;\frac{1}{2}+2.\frac{1}{4}+4.\frac{1}{8}+\ldots+2^m.\frac{1}{2^{m+1}} \\ &amp;=(m+1)\frac{1}{2}
\end{align}
This proves that $s_n$ can be made larger than the sum of any number of $\frac{1}{2}$’s and therefore as large as we please, by taking $n$ large enough, so the $\{s_n\}$ are unbounded, which leads to that \eqref{2} is a divergent series.
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots=\infty
\end{equation}</p>

<p>The simplest general principle that is useful to study the convergence of a series is the <strong>$\mathbf{n}$-th term test</strong>.</p>

<h3 id="nth-term-test">$\mathbf{n}$-th term test</h3>
<p>If the series $\{a_n\}$ converges, then $a_n\to 0$ as $n\to\infty$; or equivalently, if $\neg(a_n\to0)$ as $n\to\infty$, then the series must necessarily diverge.</p>

<p><strong>Proof</strong><br />
When $\{a_n\}$ converges, as $n\to\infty$ we have
\begin{equation}
a_n=s_n-s_{n-1}\to s-s=0
\end{equation}
This result shows that $a_n\to 0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e., it does not imply the convergence of the series when $a_n\to 0$ as $n\to\infty$.</p>

<h2 id="gen-props-conv-series">General Properties of Convergent Series</h2>
<ul>
  <li>Any finite number of 0’s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges).</li>
  <li>When two convergent series are added term by term, the resulting series converges to the expected sum; i.e., if $\sum_{n=1}^{\infty}a_n=s$ and $\sum_{n=1}^{\infty}b_n=t$, then
\begin{equation}
\sum_{n=1}^{\infty}(a_n+b_n)=s+t
\end{equation}
    <ul>
      <li><strong>Proof</strong><br />
  Let $\{s_n\}$ and $\{t_n\}$ respectively be the sequences of partial sums of $\sum_{n=1}^{\infty}a_n$ and $\sum_{n=1}^{\infty}b_n$. As $n\to\infty$ we have
  \begin{align}
  (a_1+b_1)+(a_2+b_2)+\dots+(a_n+b_n)&amp;=\sum_{i=1}^{n}a_i+\sum_{i=1}^{n}b_i \\ &amp;=s_n+t_n\to s+t
  \end{align}</li>
    </ul>
  </li>
  <li>Similarly, $\sum_{n=1}^{\infty}(a_n-b_n)=s-t$ and $\sum_{n=1}^{\infty}ca_n=cs$ for any constant $c$.</li>
  <li>Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way.
    <ul>
      <li><strong>Proof</strong><br />
  If $\sum_{n=1}^{\infty}a_n=s$, then
  \begin{equation}
  \lim_{n\to\infty}(a_0+a_1+a_2+\dots+a_n)=\lim_{n\to\infty} a_0+\lim_{n\to\infty}(a_1+a_2+\dots+a_n)=a_0+s
  \end{equation}</li>
    </ul>
  </li>
</ul>

<h2 id="series-nonneg-ct">Series of Nonnegative terms. Comparison Tests</h2>
<p>The easiest infinite series to work with are those whose terms are all nonnegative numbers. The reason, as we saw in the above <a href="#conv-div-series">section</a>, is that if $a_n\geq0$, then the series $\sum a_n$ converges if and only if its sequence $\{s_n\}$ of partial sums is bounded (since $s_{n+1}=s_n+a_{n+1}$).</p>

<p>Thus, in order to establish the convergence of a series of nonnegative terms, it suffices to show that its terms approach zero fast enough, or at least as fast as the terms of a known convergent series of nonnegative terms to keep the partial sums bounded.</p>

<h3 id="comparison-test">Comparison test</h3>
<p>If $0\leq a_n\leq b_n$, then</p>
<ul>
  <li>$\sum a_n$ converges if $\sum b_n$ converges.</li>
  <li>$\sum b_n$ diverges if $\sum a_n$ diverges.</li>
</ul>

<p><strong>Proof</strong><br />
If $s_n, t_n$ respectively are the partial sums of $\sum a_n,\sum b_n$, then
\begin{equation}
0\leq s_n=\sum_{i=1}^{n}a_i\leq\sum_{i=1}^{n}b_i=t_n
\end{equation}
Then if $\{t_n\}$ is bounded, then so is $\{s_n\}$; and if $\{s_n\}$ is unbounded, then so is $\{t_n\}$.</p>

<p><strong>Example</strong><br />
Consider convergence behavior of two series
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{2^n+1};\hspace{2cm}\sum_{n=1}^{\infty}\frac{1}{\ln n}
\end{equation}
The first series converges, because
\begin{equation}
\frac{1}{2^n+1}&lt;\frac{1}{2^n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{2^n}=1$, which is a convergent series. At the same time, the second series diverges, since
\begin{equation}
\frac{1}{n}\leq\frac{1}{\ln n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges.</p>

<p>One thing worth remarking is that the condition $0\leq a_n\leq b_n$ for the comparison test need not hold for all $n$, but only for all $n$ from some point on.</p>

<p>The comparison test is simple, but in some cases where it is difficult to establish the necessary inequality between the n-th terms of the two series. And since limits are often easier to work with than inequalities, we have the following test.</p>

<h3 id="limit-comparison-test">Limit comparison test</h3>
<p>If $\sum a_n, \sum b_n$ are series with positive terms such that
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=1\tag{3}\label{3}
\end{equation}
then either both series converge or both series diverge.</p>

<p><strong>Proof</strong><br />
we observe that \eqref{3} implies that for all sufficient large $n$, we have
\begin{align}
\frac{1}{2}&amp;\leq\frac{a_n}{b_n}\leq 2 \\ \text{or}\hspace{1cm}\frac{1}{2}b_n&amp;\leq a_n\leq 2b_n
\end{align}
which leads to the fact that $\sum a_n$ and $\sum b_n$ have the same convergence behavior.</p>

<p>The condition \eqref{3} can be generalized by
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=L,
\end{equation}
where $0&lt;L&lt;\infty$.</p>

<p><strong>Example</strong> ($p$<em>-series</em>)<br />
Consider the convergence behavior of the series
\begin{equation}
\sum_{n=1}^{\infty}\dfrac{1}{n^p}=1+\dfrac{1}{2^p}+\dfrac{1}{3^p}+\dfrac{1}{4^p}+\ldots,\tag{4}\label{4}
\end{equation}
where $p$ is a positive constant.</p>

<p>If $p\leq 1$, then $n^p\leq n$ or $\frac{1}{n}\leq\frac{1}{n^p}$. Thus, by comparison with the harmonic series $\sum\frac{1}{n}$, we have that \eqref{4} diverges.</p>

<p>If $p&gt;1$, let $n$ be given and choose $m$ so that $n&lt;2^m$. Then
\begin{align}
s_n&amp;\leq s_{2^m-1} \\ &amp;=1+\left(\dfrac{1}{2^p}+\dfrac{1}{3^p}\right)+\left(\dfrac{1}{4^p}+\ldots+\dfrac{1}{7^p}\right)+\ldots+\left[\dfrac{1}{(2^{m-1})^p}+\ldots+\dfrac{1}{(2^m-1)^p}\right] \\ &amp;\leq 1+\dfrac{2}{2^p}+\dfrac{4}{4^p}+\ldots+\dfrac{2^{m-1}}{(2^{m-1})^p}
\end{align}
Let $a=\frac{1}{2^{p-1}}$, then $a&lt;1$ since $p&gt;1$, and
\begin{equation}
s_n\leq 1+a+a^2+\ldots+a^{m-1}=\dfrac{1-a^m}{1-a}&lt;\dfrac{1}{1-a}
\end{equation}
which proves that $\{s_n\}$ has an upper bound. Thus \eqref{4} converges.</p>

<p><strong>Theorem 1</strong><br />
<em>If a convergent series of nonnegative terms is rearranged in any manner, then the resulting series also converges and has the same sum.</em></p>

<p><strong>Proof</strong><br />
Consider two series $\sum a_n$ and $\sum b_n$, where $\sum a_n$ is a convergent series of nonnegative terms and $\sum b_n$ is formed form $\sum a_n$ by rearranging its terms.</p>

<p>Let $p$ be a positive integer and consider the $p$-partial sum $t_p=b_1+\ldots+b_p$ of $\sum b_n$. Since each $b$ is some $a$, then there exists an $m$ such that each term in $t_p$ is one of the terms in $s_m=a_1+\ldots+a_m$. This shows us that $t_p\leq s_m\leq s$. Thus, $\sum b_n$ converges to a sum $t\leq s$.</p>

<p>On the other hand, $\sum a_n$ is also a rearrangement of $\sum b_n$, so by the same procedure, similarly we have that $s\leq t$, and therefore $t=s$.</p>

<h2 id="int-test-euler-c">The Integral test. Euler’s constant</h2>
<p>In this section, we will be going through a more detailed class of infinite series with nonnegative terms which is those whose terms form a decreasing sequence of positive numbers.</p>

<p>We begin by considering a series
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots
\end{equation}
whose terms are positive and decreasing. Suppose $a_n=f(n)$, as shown is <strong><em>Figure 1</em></strong>.</p>
<figure>
	<img src="/assets/images/2021-09-06/integral-test.png" alt="integral test" width="500px" height="230px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b></figcaption>
</figure>

<p>On the left of this figure we see that the rectangles of areas $a_1,a_2,\dots,a_n$ have a greater combined area than the area under the curve from $x=1$ to $x=n+1$, so
\begin{equation}
a_1+a_2+\dots+a_n\geq\int_{1}^{n+1}f(x)\,dx\geq\int_{1}^{n}f(x)\,dx\tag{5}\label{5}
\end{equation}
On the right side of the figure, the rectangles lie under the curve, which makes
\begin{align}
a_2+a_3+\dots+a_n&amp;\leq\int_{1}^{n}f(x)\,dx \\ a_1+a_2+\dots+a_n&amp;\leq a_1+\int_{1}^{n}f(x)\,dx\tag{6}\label{6}
\end{align}
Putting \eqref{5} and \eqref{6} together we have
\begin{equation}
\int_{1}^{n}f(x)\,dx\leq a_1+a_2+\dots+a_n\leq a_1+\int_{1}^{n}f(x)\,dx\tag{7}\label{7}
\end{equation}
The result we obtained in \eqref{7} allows us to establish the <strong>integral test</strong>.</p>

<h3 id="integral-test">Integral test</h3>

<p>If $f(x)$ is a positive decreasing function for $x\geq1$ such that $f(n)=a_n$ for each positive integer $n$, then the series and integral
\begin{equation}
\sum_{n=1}^{\infty}a_n;\hspace{2cm}\int_{1}^{\infty}f(x)\,dx
\end{equation}
converge or diverge together.</p>

<p>The integral test holds for any interval of the form $x\geq k$, not just for $x\geq 1$.</p>

<p><strong>Example</strong> (<em>Abel’s series</em>)<br />
Let’s consider the convergence behavior of the series
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n\ln n}\tag{8}\label{8}
\end{equation}
By the integral test, we have that \eqref{8} diverges, because
\begin{equation}
\sum_{2}^{\infty}\frac{dx}{x\ln x}=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x\ln x}=\lim_{b\to\infty}\left(\ln\ln x\Big|_{2}^{b}\right)=\lim_{b\to\infty}\left(\ln\ln b-\ln\ln 2\right)=\infty
\end{equation}
More generally, if $p&gt;0$, then
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n(\ln n)^p}
\end{equation}
converges if $p&gt;1$ and diverges if $0&lt;p\leq 1$. For if $p\neq 1$, we have
\begin{align}
\int_{2}^{\infty}\frac{dx}{x(\ln x)^p}&amp;=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x(\ln x)^p} \\ &amp;=\lim_{b\to\infty}\left[\dfrac{(\ln x)^{1-p}}{1-p}\Bigg|_2^b\right] \\ &amp;=\lim_{b\to\infty}\left[\dfrac{(\ln b)^{1-p}-(\ln 2)^{1-p}}{1-p}\right]
\end{align}
exists if and only if $p&gt;1$.</p>

<h3 id="euler-c">Euler’s constant</h3>
<p>From \eqref{7} we have that
\begin{equation}
0\leq a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\leq a_1
\end{equation}
Denoting $F(n)=a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx$, the above expression becomes
\begin{equation}
0\leq F(n)\leq a_1
\end{equation}
Moreover, $\{F(n)\}$ is a decreasing sequence, because
\begin{align}
F(n)-F(n+1)&amp;=\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]-\left[a_1+a_2+\ldots+a_{n+1}-\int_{1}^{n+1}f(x)\,dx\right] \\ &amp;=\int_{n}^{n+1}f(x)\,dx-a_{n+1}\geq 0
\end{align}
where the last step can be seen by observing the right side of <strong><em>Figure 1</em></strong>.</p>

<p>Since any decreasing sequence of nonnegative numbers converges, we have that
\begin{equation}
L=\lim_{n\to\infty}F(n)=\lim_{n\to\infty}\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]\tag{9}\label{9}
\end{equation}
exists and satisfies the inequalities $0\leq L\leq a_1$.</p>

<p>Let $a_n=\frac{1}{n}$ and $f(x)=\frac{1}{x}$, the last quantity in \eqref{9} becomes
\begin{equation}
\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)\tag{10}\label{10}
\end{equation}
since
\begin{equation}
\int_{1}^{n}\dfrac{dx}{x}=\ln x\Big|_1^n=\ln n
\end{equation}
The value of the limit \eqref{10} is called <strong>Euler’s constant</strong> (denoted as $\gamma$).
\begin{equation}
\gamma=\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)
\end{equation}</p>

<h2 id="ratio-root">The Ratio test. Root test</h2>

<h3 id="ratio-test">Ratio test</h3>
<p>If $\sum a_n$ is a series of positive terms such that
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}}{a_n}=L,\tag{11}\label{11}
\end{equation}
then</p>
<ol>
  <li>if $L&lt;1$, the series <em>converges</em>.</li>
  <li>if $L&gt;1$, the series <em>diverges</em>.</li>
  <li>if $L=1$, the test is <em>inconclusive</em>.</li>
</ol>

<p><strong>Proof</strong></p>
<ol>
  <li>
    <p>Let $L&lt;1$ and choose any number $r$ such that $L&lt;r&lt;1$. From \eqref{11}, we have that there exists an $n_0$ such that
\begin{align}
\dfrac{a_{n+1}}{a_n}&amp;\leq r=\dfrac{r^{n+1}}{r_n},\hspace{1cm}\forall n\geq n_0 \\ \dfrac{a_{n+1}}{r^{n+1}}&amp;\leq\dfrac{a_n}{r^n},\hspace{2cm}\forall n\geq n_0
\end{align}
which means that $\{\frac{a_n}{r^n}\}$ is a decreasing sequence for $n\geq n_0$; in particular, $\frac{a_n}{r^n}\leq\frac{a_{n_0}}{r^{n_0}}$ for $n\geq n_0$. Thus, if we let $K=\frac{a_{n_0}}{r^{n_0}}$, then we get
\begin{equation}
a_n\leq Kr^n,\hspace{1cm}\forall n\geq n_0\tag{12}\label{12}
\end{equation}
However, $\sum Kr^n$ converges since $r&lt;1$. Hence, by the <a href="#comparison-test">comparison test</a>, \eqref{12} implies that $\sum a_n$ converges.</p>
  </li>
  <li>
    <p>When $L&gt;1$, we have that $\frac{a_{n+1}}{a_n}\geq 1$, or equivalently $a_{n+1}\geq a_n$, for all $n\geq n_0$, for some constant $n_0$. That means $\neg(a_n\to 0)$ as $n\to\infty$ (since $\sum a_n$ is a series of positive terms).<br />
By the <a href="#nth-term-test">$n$-th term test</a>, we know that the series diverges.</p>
  </li>
  <li>
    <p>Consider the $p$-series $\sum\frac{1}{n^p}$. For all values of $p$, as $n\to\infty$ we have
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^p}{(n+1)^p}=\left(\dfrac{n}{n+1}\right)^p\to 1
\end{equation}
As in the above example, we have that this series converges if $p&gt;1$ and diverges if $p\leq 1$.</p>
  </li>
</ol>

<h3 id="root-test">Root test</h3>
<p>If $\sum a_n$ is a series of nonnegative terms such that
\begin{equation}
\lim_{n\to\infty}\sqrt[n]{a_n}=L,\tag{13}\label{13}
\end{equation}
then</p>
<ol>
  <li>if $L&lt;1$, the series <em>converges</em>.</li>
  <li>if $L&gt;1$, the series <em>diverges</em>.</li>
  <li>if $L=1$, the test is <em>inconclusive</em>.</li>
</ol>

<p><strong>Proof</strong></p>
<ol>
  <li>
    <p>Let $L&lt;1$ and $r$ is any number such that $L&lt;r&lt;1$. From \eqref{13}, we have that there exist $n_0$ such that
\begin{align}
\sqrt[n]{a_n}&amp;\leq r&lt;1,\hspace{1cm}\forall n\geq n_0 \\ a_n&amp;\leq r^n&gt;1,\hspace{1cm}\forall n\geq n_0
\end{align}
And since the geometric series $\sum r^n$ converges, we clearly have that $\sum a_n$ also converges.</p>
  </li>
  <li>
    <p>If $L&gt;1$, then $\sqrt[n]{a_n}\geq 1$ for all $n\geq n_0$, for some $n_0$, so $a_n\geq 1$ for all $n\geq n_0$. That means as $n\to\infty$, $\neg(a_n\to 0)$. Therefore, by the <a href="#nth-term-test">$n$-th term test</a>, we have that the series diverges.</p>
  </li>
  <li>
    <p>For $L=1$, we provide 2 examples. One is the divergent series $\sum\frac{1}{n}$ and the other is the convergent series $\sum\frac{1}{n^2}$ (since $\sqrt[n]{n}\to 1$ as $n\to\infty$).</p>
  </li>
</ol>

<h3 id="extended-ratio-test">The Extended Ratio tests of Raabe and Gauss</h3>

<h4 id="kummers-theorem">Kummer’s theorem</h4>

<p><strong>Theorem 2</strong> (<em>Kummer’s</em>)<br />
<em>Assume that $a_n&gt;0,b_n&gt;0$ and $\sum\frac{1}{b_n}$ diverges. If
\begin{equation}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)=L,\tag{14}\label{14}
\end{equation}
then $\sum a_n$ converges if $L&gt;0$ and diverges if $L&lt;0$.</em></p>

<p><strong>Proof</strong></p>
<ul>
  <li>
    <p>If $L&gt;0$, then there exists $h$ such that $L&gt;h&gt;0$. From \eqref{14}, for some positive integer $n_0$ we have
\begin{align}
b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}&amp;\geq h&gt;0,\hspace{1cm}\forall n\geq n_0 \\ a_n b_n-a_{n+1}b_{n+1}&amp;\geq ha_n&gt;0,\hspace{1cm}\forall n\geq n_0\tag{15}\label{15}
\end{align}
Hence, $\{a_n b_n\}$ is a decreasing sequence of positive numbers for $n\geq n_0$, so $K=\lim a_n b_n$ exists.<br />
Moreover, we have that
\begin{equation}
\sum_{n=n_0}^{\infty}a_nb_n-a_{n+1}b_{n+1}=a_{n_0}b_{n_0}-\lim_{n\to\infty}a_nb_n=a_{n_0}b_{n_0}-K
\end{equation}
Therefore, by \eqref{15} and the <a href="#comparison-test">comparison test</a>, we can conclude that $\sum ha_n$ converges, which means that $\sum a_n$ also converges.</p>
  </li>
  <li>
    <p>If $L&lt;0$, for some positive integer $n_0$ we have
\begin{equation}
a_nb_n-a_{n+1}b_{n+1}\leq 0,\hspace{1cm}\forall n\geq n_0
\end{equation}
Hence, $\{a_nb_n\}$ is a increasing sequence of positive number for all $n\geq n_0$, for some positive integer $n_0$. This also means for all $n\geq n_0$,
\begin{align}
a_nb_n&amp;\geq a_{n_0}b_{n_0} \\ a_n&amp;\geq (a_{n_0}b_{n_0}).\dfrac{1}{b_n}
\end{align}
Therefore $\sum a_n$ diverges (since $\sum\frac{1}{b_n}$ diverges).</p>
  </li>
</ul>

<h4 id="raabes-test">Raabe’s test</h4>

<p><strong>Theorem 3</strong> (<em>Raabe’s test</em>)<br />
<em>If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n},
\end{equation}
where $A_n\to 0$, then $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.</em></p>

<p><strong>Proof</strong><br />
Take $n=b_n$ in <em>Kummber’s theorem</em>. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;=\lim\left[n-\left(1-\dfrac{A}{n}+\dfrac{A_n}{n}\right)(n+1)\right] \\ &amp;=\lim\left[-1+\dfrac{A(n+1)}{n}-\dfrac{A_n(n+1)}{n}\right] \\ &amp;=A-1
\end{align}
and by <em>Kummer’s theorem</em> we have that $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.</p>

<p><em>Raabe’s test</em> can be formulated as followed: If $a_n&gt;0$ and
\begin{equation}
\lim n\left(1-\dfrac{a_{n+1}}{a_n}\right)=A,
\end{equation}
then $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.</p>

<p>When $A=1$ in <em>Raabe’s test</em>, we turn to <strong>Gauss’s test</strong></p>

<h4 id="gausss-test">Gauss’s test</h4>

<p><strong>Theorem 4</strong><br />
<em>If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n^{1+c}},
\end{equation}
where $c&gt;0$ and $A_n$ is bounded as $n\to\infty$, then $\sum a_n$ converges if $A&gt;1$ and diverges if $A\leq 1$.</em></p>

<p><strong>Proof</strong></p>
<ul>
  <li>
    <p>If $A\neq 1$, the statement follows exactly from <em>Raabe’s test</em>, since $\frac{A_n}{n^c}\to 0$ as $n\to\infty$.</p>
  </li>
  <li>
    <p>If $A=1$, we begin by taking $b_n=n\ln n$ in <em>Kummer’s theorem</em>. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;=\lim\left[n\ln n-\left(1-\dfrac{1}{n}+\dfrac{A_n}{n^{1+c}}\right)(n+1)\ln(n+1)\right] \\ &amp;=\lim\left[n\ln n-\dfrac{n^2-1}{n}\ln(n+1)-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;=\lim\left[n\ln\left(\dfrac{n}{n+1}\right)+\dfrac{\ln(n+1)}{n}-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;=-1+0-0=-1&lt;0,
\end{align}
where in fourth step we use the <em>Stolz–Cesàro theorem</em><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Therefore, by <em>Kummer’s theorem</em>, we have that the series is divergent.</p>
  </li>
</ul>

<p><strong>Theorem 5</strong> (<em>Gauss’s test</em>)<br />
<em>If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^k+\alpha n^{k-1}+\ldots}{n^k+\beta n^{k-1}+\ldots},\tag{16}\label{16}
\end{equation}
then $\sum a_n$ converges if $\beta-\alpha&gt;1$ and diverges if $\beta-\alpha\leq 1$.</em></p>

<p><strong>Proof</strong><br />
If the quotient on the right of \eqref{16} is worked out by long division, we get
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{\beta-\alpha}{n}+\dfrac{A_n}{n^2},
\end{equation}
where $A_n$ is a quotient of the form
\begin{equation}
\dfrac{\gamma n^{k-2}+\ldots}{n^{k-2}+\ldots}
\end{equation}
and is therefore clearly bounded as $n\to\infty$. The statement now follows from <strong>Theorem 4</strong> with $c=1$.</p>

<h2 id="alt-test-abs-conv">The Alternating Series test. Absolute Convergence</h2>
<p>Previously, we have been working with series of positive terms and nonnegative terms. It’s time to consider series with both positive and negative terms. The simplest are those whose terms are alternatively positive and negative.</p>

<h3 id="alt-series">Alternating Series</h3>
<p><strong>Alternating series</strong> is series with the form
\begin{equation}
\sum_{n=1}^{\infty}(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+\ldots,\tag{17}\label{17}
\end{equation}
where $a_n$’s are all positive numbers.</p>

<p>From the definition of alternating series, we establish <strong>alternating series test</strong>.</p>

<h3 id="alt-series-test">Alternating Series test</h3>
<p>If the alternating series \eqref{17} has the property that</p>
<ol>
  <li>$a_1\geq a_2\geq a_3\geq\ldots$</li>
  <li>$a_n\to 0$ as $n\to\infty$</li>
</ol>

<p>then $\sum a_n$ converges.</p>

<p><strong>Proof</strong><br />
On the one hand, we have that a typical even partial sum $s_{2n}$ can be written as
\begin{equation}
s_{2n}=(a_1-a_2)+(a_3-a_4)+\ldots+(a_{2n-1}-a_{2n}),
\end{equation}
where each expression in parentheses is nonnegative since $\{a_n\}$ is a decreasing sequence. Hence, we also have that $s_{2n}\leq s_{2n+2}$, which leads to the result that the even partial sums form an increasing sequence.</p>

<p>Moreover, we can also display $s_{2n}$ as
\begin{equation}
s_{2n}=a_1-(a_2-a_3)-(a_4-a_5)-\ldots-(a_{2n-2}-a_{2n-1})-a_{2n},
\end{equation}
where each expression in parentheses once again is nonnegative. Thus, we have that $s_{2n}\leq a_1$, so ${s_{2n}}$ has an upper bound. Since every bounded increasing sequence converges, there exists a number $s$ such that
\begin{equation}
\lim_{n\to\infty}s_{2n}=s
\end{equation}</p>

<p>On the other hand, the odd partial sums approach the same limit, because
\begin{align}
s_{2n+1}&amp;=a_1-a_2+a_3-a_4+\ldots-a_{2n}+a_{2n+1} \\ &amp;=s_{2n}+a_{2n+1}
\end{align}
and therefore
\begin{equation}
\lim_{n\to\infty}s_{2n+1}=\lim_{n\to\infty}s_{2n}+\lim_{n\to\infty}a_{2n+1}=s+0=s
\end{equation}
Since both sequence of even sums and sequence of odd partial sums converges to $s$ as $n$ tends to infinity, this shows us that $\{s_n\}$ also converges to $s$, and therefore the alternating series \eqref{17} converges to the sum $s$.</p>

<h3 id="abs-conv">Absolute Convergence</h3>
<p>A series $\sum a_n$ is said to be <strong>absolutely convergent</strong> if $\sum\vert a_n\vert$ converges.</p>

<p>These are some properties of absolute convergence.</p>
<ol>
  <li>Absolute convergence implies convergence.
    <ul>
      <li><strong>Proof</strong><br />
Suppose that $\sum a_n$ is an absolutely convergent series, or $\sum\vert a_n\vert$ converges. We have that
\begin{equation}
0\leq a_n+\vert a_n\vert\leq 2\vert a_n\vert
\end{equation}
And since $\sum 2\vert a_n\vert$ converges, by <a href="#comparison-test">comparison test</a>, we also have that $\sum(a_n+\vert a_n\vert)$ converges.<br />
Since both $\sum\vert a_n\vert$ and $\sum(a_n+\vert a_n\vert)$ converge, so does their difference, which is $\sum a_n$.<br />
<br /></li>
    </ul>
  </li>
  <li>A convergent series that is not absolutely convergent is said to be <strong>conditionally convergent</strong>.
    <ul>
      <li>Any conditionally convergent series can be made to converge to any given number as its sum, or even to diverge, by <em>suitably changing the order of its terms without changing the terms themselves</em> (check out <strong>Theorem 8</strong> to see the proof).</li>
      <li>On the other hand, any absolutely convergent series can be rearranged in any manner without changing its convergence behavior or its sum (check out <strong>Theorem 7</strong> to see the proof).</li>
    </ul>
  </li>
</ol>

<h2 id="abs-vs-cond">Absolute vs Conditionally Convergence</h2>
<p><strong>Theorem 6</strong><br />
<em>Consider a series $\sum a_n$ and define $p_n$ and $q_n$ by
\begin{align}
p_n&amp;=\dfrac{\vert a_n\vert+a_n}{2} \\ q_n&amp;=\dfrac{\vert a_n\vert-a_n}{2}
\end{align}
If $\sum a_n$ converges conditionally, then both $\sum p_n$ and $\sum q_n$ diverges.<br />
If $\sum a_n$ converges absolutely, then $\sum p_n$ and $\sum q_n$ both converge and the sums of these series are related by the equation</em>
\begin{equation}
\sum a_n=\sum p_n-\sum q_n
\end{equation}</p>

<p><strong>Proof</strong><br />
From the formulas of $p_n$ and $q_n$, we have
\begin{align}
a_n&amp;=p_n-q_n\tag{18}\label{18} \\ \vert a_n\vert&amp;=p_n+q_n\tag{19}\label{19}
\end{align}</p>
<ul>
  <li>
    <p>We begin by proving the first statement.<br />
When $\sum a_n$ converges, from \eqref{18}, we have $\sum p_n$ and $\sum q_n$ both must have the same convergence behavior (i.e., converge or diverge at the same time).<br />
If they both converge, then from \eqref{19}, we have that $\sum\vert a_n\vert$ converges, contrary to the hypothesis, so $\sum p_n$ and $\sum q_n$ are both divergent.</p>
  </li>
  <li>
    <p>To prove the second statement, we assume that $\sum\vert a_n\vert$ converges. We have
\begin{equation}
p_n=\dfrac{\vert a_n\vert+a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which shows us that $\sum p_n$ converges. Similarly, for $q_n$, we have
\begin{equation}
q_n=\dfrac{\vert a_n\vert-a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which also lets us obtain that $\sum q_n$ converges.<br />
Therefore
\begin{equation}
\sum p_n-\sum q_n=\sum(p_n-q_n)=\sum a_n
\end{equation}
<br /></p>
  </li>
</ul>

<p><strong>Theorem 7</strong><br />
<em>If $\sum a_n$ is an absolutely convergent series with sum $s$, and if $a_n$’s are rearranged in any way to from a new series $\sum b_n$, then this new series is also absolutely convergent with sum $s$.</em></p>

<p><strong>Proof</strong><br />
Since $\sum\vert a_n\vert$ is a convergent series of nonnegative terms with sum $s$ and since the $b_n$’s are just the $a_n$’s in a different order, it follows from <strong>Theorem 1</strong> that $\sum\vert b_n\vert$ also converges to $s$, and therefore $\sum b_n$ is absolutely convergent with sum $t$, for some positive $t$.</p>

<p><strong>Theorem 6</strong> allows us to write
\begin{equation}
s=\sum a_n=\sum p_n-\sum q_n
\end{equation}
and
\begin{equation}
t=\sum b_n=\sum P_n-\sum Q_n
\end{equation}
where each of the series on the right is convergent and consists of nonnegative. But the $P_n$’s and $Q_n$’s are simply the $p_n$’s and $q_n$’s in a different order. Hence, by <strong>Theorem 1</strong>, we have $\sum P_n=\sum p_n$ and $\sum Q_n=\sum q_n$. And therefore, $t=s$.<br />
<br /></p>

<p><strong>Theorem 8</strong> (<em>Riemann’s rearrangement theorem</em>)<br />
<em>Let $\sum a_n$ be a conditionally convergent series. Then its terms can be rearranged to yield a convergent series whose sum is an arbitrary preassigned number, or a series that diverges to $\infty$, or a series that diverges to $-\infty$.</em></p>

<p><strong>Proof</strong><br />
Since $\sum a_n$ converges conditionally, we begin by using <strong>Theorem 6</strong> to form the two divergent series of nonnegative terms $\sum p_n$ and $\sum q_n$.</p>
<ul>
  <li>
    <p>To prove the first statement, let $s$ be any number and construct a rearrangement of the given series as follows. Start by writing down $p$’s in order until the partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}
\end{equation}
is first $\geq s$; next we continue with $-q$’s until the total partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}-q_1-q_2-\ldots-q_{m_1}
\end{equation}
is first $\leq s$; then we continue with $p$’s until the total partial sum
\begin{equation}
p_1+\ldots+p_{n_1}-q_1-\ldots-q_{m_1}+p_{n_1+1}+\ldots+p_{n_2}
\end{equation}
is first $\geq s$; and so on.<br />
The possibility of each of these steps is guaranteed by the divergence of $\sum p_n$ and $\sum q_n$; and the resulting rearrangement of $\sum a_n$ converges to $s$ because $p_n\to 0$ and $q_n\to 0$.</p>
  </li>
  <li>
    <p>In order to make the rearrangement diverge to $\infty$, it suffices to write down enough $p$’s to yield
\begin{equation}
p_1+p_2+\ldots+p_{n_1}\geq 1,
\end{equation}
then to insert $-q_1$, and then to continue with $p$’s until
\begin{equation}
p_1+\ldots+p_{n_1}-q_1+p_{n_1+1}+\ldots+p_{n_2}\geq 2,
\end{equation}
then to insert $-q_2$, and so on.<br />
We can produce divergence to $-\infty$ by a similar construction.</p>
  </li>
</ul>

<p>One of the principal application of <strong>Theorem 7</strong> relates to the <em>multiplication of series</em>.</p>

<p>If we multiply two series
\begin{align}
\sum_{n=0}^{\infty}a_n&amp;=a_0+a_1+\ldots+a_n+\ldots\tag{20}\label{20} \\ \sum_{n=0}^{\infty}b_n&amp;=b_0+b_1+\ldots+b_n+\ldots\tag{21}\label{21}
\end{align}
by forming all possible product $a_i b_j$ (as in the case of finite sums), then we obtain the following doubly infinite array</p>
<figure>
	<img src="/assets/images/2021-09-06/series-mult.png" alt="series multiplication" width="300px" height="210px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b></figcaption>
</figure>

<p>There are various ways of arranging these products into a single infinite series, of which two are important. The first one is to group them by diagonals, as indicated in the arrows in <strong>Figure 2</strong>:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1)+(a_0b_2+a_1b_1+a_2b_0)+\ldots\tag{22}\label{22}
\end{equation}
This series can be defined as $\sum_{n=0}^{\infty}c_n$, where
\begin{equation}
c_n=a_0b_n+a_1b_{n-1}+\ldots+a_nb_0
\end{equation}</p>

<p>It is called the <em>product</em> (or <em>Cauchy product</em>) of the two series $\sum a_n$ and $\sum b_n$.</p>

<p>The second crucial method of arranging these products into a series is by squares, as shown in <strong>Figure 2</strong>:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1+a_1b_0)+(a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0)+\ldots\tag{23}\label{23}
\end{equation}
The advantage of this arrangement is that the $n$-th partial sum $s_n$ of \eqref{23} is given by
\begin{equation}
s_n=(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n)\tag{24}\label{24}
\end{equation}
<br /></p>

<p><strong>Theorem 9</strong><br />
<em>If the two series \eqref{20} and \eqref{21} have nonnegative terms and converges to $s$ and $t$, then their product \eqref{22} converges to $st$.</em></p>

<p><strong>Proof</strong><br />
It is clear from \eqref{24} that \eqref{23} converges to $st$. Let’s denote the series \eqref{22} and \eqref{23} without parenthesis by $(22’)$ and $(23’)$.</p>

<p>We have the series $(23’)$ of nonnegative terms still converges to $st$ because, for if $m$ is an integer such that $n^2\leq m\leq (n+1)^2$, then the $m$-th partial sum of $(23’)$ lies between $s_{n-1}$ and $s_n$, and both of these converge to $st$.</p>

<p>By <strong>Theorem 7</strong>, the terms of $(23’)$ can be rearranged to yield $(22’)$ without changing the sum $st$; and when parentheses are suitably inserted, we see that \eqref{8} converges to $st$.</p>

<p>We now extend <strong>Theorem 9</strong> to the case of absolute convergence.<br />
<br /></p>

<p><strong>Theorem 10</strong><br />
<em>If the series $\sum_{n=0}^{\infty}a_n$ and $\sum_{n=0}^{\infty}b_n$ are absolutely convergent, with sum $s$ and $t$, then their product
\begin{multline}
\sum_{n=0}^{\infty}(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)=a_0b_0+(a_0b_1+a_1b_0)\,+ \\ (a_0b_2+a_1b_1+a_2b_0)+\ldots+(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)+\ldots\tag{25}\label{25}
\end{multline}
is absolutely convergent, with sum $st$.</em></p>

<p><strong>Proof</strong><br />
The series $\sum_{n=0}^{\infty}\vert a_n\vert$ and $\sum_{n=0}^{\infty}\vert b_n\vert$ are convergent and have nonnegative terms. So by the <strong>Theorem 9</strong> above, their product
\begin{multline}
\vert a_0\vert\vert b_0\vert+\vert a_0\vert\vert b_1\vert+\vert a_1\vert\vert b_0\vert+\ldots+\vert a_0\vert\vert b_n\vert+\vert a_1\vert\vert b_{n-1}\vert+\ldots+\vert a_n\vert\vert b_0\vert+\ldots \\ =\vert a_0b_0\vert+\vert a_0b_1\vert+\vert a_1b_0\vert+\ldots+\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert+\ldots\tag{26}\label{26}
\end{multline}
converges, and therefore the series
\begin{equation}
a_0b_0+a_0b_1+a_1b_0+\ldots+a_0b_n+\ldots+a_nb_0+\ldots\tag{27}\label{27}
\end{equation}
is absolutely convergent. It follows from <strong>Theorem 7</strong> that the sum of \eqref{27} will not change if we rearrange its terms and write it as
\begin{equation}
a_0b_0+a_0b_1+a_1b_1+a_1b_0+a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0+\ldots\tag{28}\label{28}
\end{equation}
We now observe that the sum of the first $(n+1)^2$ terms of \eqref{28} is
\begin{equation}
(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n),
\end{equation}
so it is clear that \eqref{28}, and with it \eqref{27}, converges to $st$.</p>

<p>Thus, \eqref{25} also converges to $st$, since \eqref{25} is retrieved by suitably inserted parentheses in \eqref{27}.</p>

<p>Moreover, we also have
\begin{equation}
\vert a_0b_n+a_1b_{n-1}+\ldots+a_nb_0\vert\leq\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert
\end{equation}
and the series
\begin{equation}
\vert a_0b_0\vert+(\vert a_0b_1\vert+\vert a_1b_0\vert)+\ldots+(\vert a_0b_n\vert+\ldots+\vert a_nb_0\vert)+\ldots
\end{equation}
obtained from \eqref{26} by inserting parentheses. By the <a href="#comparison-test">comparison test</a>, \eqref{25} converges absolutely.</p>

<p>Hence, we can conclude that \eqref{25} is absolutely convergent, with sum $st$.<br />
<br /></p>

<p>We have already gone through convergence tests applied only to series of positive (or nonnegative) terms. Let’s end this lengthy post with the alternating series test. ^^!</p>

<h2 id="dirichlets-test">Dirichlet’s test</h2>

<h3 id="abel-part-sum">Abel’s partial summation formula</h3>
<p>Consider series $\sum_{n=1}^{\infty}a_n$, sequence $\{b_n\}$. If $s_n=a_1+a_2+\ldots+a_n$, then
\begin{equation}
a_1b_1+a_2b_2+\ldots+a_nb_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots+s_{n-1}(b_{n-1}-b_n)+s_nb_n\tag{29}\label{29}
\end{equation}</p>

<p><strong>Proof</strong><br />
Since $a_1=s_1$ and $a_n=s_n-s_{n-1}$ for $n&gt;1$, we have
\begin{align}
a_1b_1&amp;=s_1b_1 \\ a_2b_2&amp;=s_2b_2-s_1b_2 \\ a_3b_3&amp;=s_3b_3-s_2b_3 \\ &amp;\vdots \\ a_nb_n&amp;=s_nb_n-s_{n-1}b_n
\end{align}
On adding these equations, and grouping suitably, we obtain \eqref{29}.</p>

<h3 id="d-test">Dirichlet’s test</h3>
<p><em>If the series $\sum_{n=1}^{\infty}a_n$ has bounded partial sums, and if $\{b_n\}$ is a decreasing sequence of positive numbers such that $b_n\to 0$, then the series
\begin{equation}
\sum_{n=1}^{\infty}a_nb_n=a_1b_1+a_2b_2+\ldots+a_nb_n+\ldots\tag{30}\label{30}
\end{equation}
converges</em>.</p>

<p><strong>Proof</strong><br />
Let $S_n=a_1b_1+a_2b_2+\ldots+a_nb_n$ denote the $n$-th partial sum of \eqref{30}, then \eqref{29} tells us that
\begin{equation}
S_n=T_n+s_nb_n,
\end{equation}
where
\begin{equation}
T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots
\end{equation}
Since ${s_n}$ is bounded there exists a positive constant $m$ such that $\vert s_n\vert\leq m,\forall n$, so $\vert s_nb_n\vert\leq mb_n$. And since $b_n\to 0$, we have that $s_nb_n\to 0$ as $n\to\infty$.</p>

<p>Moreover, since $\{b_n\}$ is a decreasing sequence of positive numbers, we have that
\begin{equation}
\begin{aligned}
\vert s_1(b_1-b_2)\vert+\vert s_2(b_3-b_3)\vert+\ldots&amp;\,+\vert s_{n-1}(b_{n-1}-b_n)\vert \\ &amp;\leq m(b_1-b_2)+m(b_2-b_3)+\ldots+m(b_{n-1}-b_n) \\ &amp;=m(b_1-b_n)\leq mb_1
\end{aligned}
\end{equation}
which implies that $T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots$ converges absolutely, and thus, it converges to a sum $t$. Therefore
\begin{equation}
\lim_{n\to\infty}S_n=\lim_{n\to\infty}T_n+s_nb_n=\lim_{n\to\infty}T_n+\lim_{n\to\infty}s_nb_n=t+0=t
\end{equation}
which lets us conclude that the series \eqref{30} converges.</p>

<h2 id="references">References</h2>
<p>[1] George F.Simmons. <a href="https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424">Calculus With Analytic Geometry - 2nd Edition</a>.</p>

<p>[2] Marian M. <a href="https://www.springer.com/gp/book/9780387789323">A Concrete Approach to Classical Analysis</a>.</p>

<p>[3] MIT 18.01. <a href="https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/">Single Variable Calculus</a>.</p>

<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>We will be going through power series in more detailed in another <a href="/2021/09/21/power-series.html">post</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><strong>Theorem</strong> (<em>Stolz–Cesaro</em>)<br />
<em>Let $\{a_n\}$ be a sequence of real numbers and $\{b_n\}$ be a strictly monotone and divergent sequence. Then
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}-a_n}{b_{n+1}-b_n}=L\hspace{1cm}(\in\left[-\infty,+\infty\right])
\end{equation}
implies
\begin{equation}
\lim_{n\to\infty}\dfrac{a_n}{b_n}=L
\end{equation}</em> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="random-stuffs" /><summary type="html"><![CDATA[A note on infinite series of constants]]></summary></entry><entry><title type="html">Optimal Policy Existence</title><link href="http://localhost:4000/2021/07/10/optimal-policy-existence.html" rel="alternate" type="text/html" title="Optimal Policy Existence" /><published>2021-07-10T13:03:00+07:00</published><updated>2021-07-10T13:03:00+07:00</updated><id>http://localhost:4000/2021/07/10/optimal-policy-existence</id><content type="html" xml:base="http://localhost:4000/2021/07/10/optimal-policy-existence.html"><![CDATA[<blockquote>
  <p>In the previous post about <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#norms">Norms</a></li>
      <li><a href="#contractions">Contractions</a></li>
      <li><a href="#banach-fixed-pts">Banach’s Fixed-point Theorem</a></li>
      <li><a href="#bellman-operator">Bellman Operator</a></li>
    </ul>
  </li>
  <li><a href="#proof">Proof of the existence</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>Before catching the pokémon, we need to prepare ourselves some pokéball.</p>

<h2 id="preliminaries">Preliminaries</h2>

<h3 id="norms">Norms</h3>
<p><strong>Definition</strong> (<em>Norm</em>)<br />
Given a vector space $\mathcal{V}\subseteq\mathbb{R}^d$, a function $f:\mathcal{V}\to\mathbb{R}^+_0$ is a <em>norm</em> if and only if</p>
<ol>
  <li>If $f(v)=0$ for some $v\in\mathcal{V}$, then $v=0$</li>
  <li>For any $\lambda\in\mathbb{R},v\in\mathcal{V},f(\lambda v)=|\lambda|v$</li>
  <li>For any $u,v\in\mathbb{R}, f(u+v)\leq f(u)+f(v)$</li>
</ol>

<p><strong>Examples</strong> (<em>Norm</em>)</p>
<ol>
  <li>$\ell^p$ norms: for $p\geq 1$,
\begin{equation}
\Vert v\Vert_p=\left(\sum_{i=1}^{d}|v_i|^p\right)^{1/p}
\end{equation}</li>
  <li>$\ell^\infty$ norms:
\begin{equation}
\Vert v\Vert_\infty=\max_{1\leq i\leq d}|v_i|
\end{equation}</li>
  <li>$\ell^{\mu,p}$: the weighted variants of these norm are defined as
\begin{equation}
\Vert v\Vert_p=\begin{cases}\left(\sum_{i=1}^{d}\frac{|v_i|^p}{w_i}\right)^{1/p}&amp;\text{if }1\leq p&lt;\infty\\ \max_{1\leq i\leq d}\frac{|v_i|}{w_i}&amp;\text{if }p=\infty\end{cases}
\end{equation}</li>
  <li>$\ell^{2,P}$: the matrix-weighted 2-norm is defined as
\begin{equation}
\Vert v\Vert^2_P=v^\intercal Pv
\end{equation}
Similarly, we can define norms over spaces of functions. For example, if $\mathcal{V}$ is the vector space of functions over domain $\mathcal{X}$ which are <em>uniformly bounded</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, then
\begin{equation}
\Vert f\Vert_\infty=\sup_{x\in\mathcal{X}}\vert f(x)\vert
\end{equation}</li>
</ol>

<p><strong>Definition</strong> (<em>Convergence in norm</em>)<br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a <em>normed vector space</em><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Let $v_n\in\mathcal{V}$ is a sequence of vectors ($n\in\mathbb{N}$). The sequence ($v_n,n\geq 0$) is said to <em>converge to</em> $v\in\mathcal{V}$ in the norm $\Vert\cdot\Vert$, denoted as $v_n\to_{\Vert\cdot\Vert}v$ if
\begin{equation}
\lim_{n\to\infty}\Vert v_n-v\Vert=0,
\end{equation}
<br /></p>

<p><strong>Definition</strong> (<em>Cauchy sequence</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>)<br />
Let ($v_n;n\geq 0$) be a sequence of vectors of a normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$. Then $v_n$ is called a <em>Cauchy sequence</em> if
\begin{equation}
\lim_{n\to\infty}\sup_{m\geq n}\Vert v_n-v_m\Vert=0
\end{equation}
Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Completeness</em>)<br />
A normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ is called <em>complete</em> if every Cauchy sequence in $\mathcal{V}$ is convergent in the norm of the vector space.</p>

<h3 id="contractions">Contractions</h3>
<p><strong>Definition</strong> (<em>Lipschitz function</em>, <em>Contraction</em>) <br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a normed vector space. A mapping $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ is called <em>L-Lipschitz</em> if for any $u,v\in\mathcal{V}$,
\begin{equation}
\Vert\mathcal{T}u-\mathcal{T}v\Vert\leq L\Vert u-v\Vert
\end{equation}
A mapping $\mathcal{T}$ is called a <em>non-expansion</em> if it is <em>Lipschitzian</em> with $L\leq 1$. It is called a <em>contraction</em> if it is <em>Lipschitzian</em> with $L&lt;1$. In this case, $L$ is called the <em>contraction factor of</em> $\mathcal{T}$ and $\mathcal{T}$ is called an <em>L-contraction</em>.</p>

<p><strong>Remark</strong><br />
If $\mathcal{T}$ is <em>Lipschitz</em>, it is also continuous in the sense that if $v_n\to_{\Vert\cdot\Vert}v$, then also $\mathcal{T}v_n\to_{\Vert\cdot\Vert}\mathcal{T}v$. This is because $\Vert\mathcal{T}v_n-\mathcal{T}v\Vert\leq L\Vert v_n-v\Vert\to 0$ as $n\to\infty$.</p>

<h3 id="banach-fixed-pts">Banach’s Fixed-point Theorem</h3>
<p><strong>Definition</strong> (<em>Banach space</em>)<br />
A complete, normed vector space is called a <em>Banach space</em>.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Fixed point</em>)<br />
Let $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be some mapping. The vector $v\in\mathcal{V}$ is called a <em>fixed point of</em> $\mathcal{T}$ if $\mathcal{T}v=v$.<br />
<br /></p>

<p><strong>Theorem</strong> (<em>Banach’s fixed-point</em>)<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>    <br />
Let $\mathcal{V}$ be a Banach space and $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be a $\gamma$-contraction mapping. Then</p>
<ol>
  <li>$\mathcal{T}$ admits a <em>unique fixed point</em> $v$.</li>
  <li>For any $v_0\in\mathcal{V}$, if $v_{n+1}=\mathcal{T}v_n$, then $v_n\to_{\Vert\cdot\Vert}v$ with a <em>geometric convergence rate</em><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:
\begin{equation}
\Vert v_n-v\Vert\leq\gamma^n\Vert v_0-v\Vert
\end{equation}</li>
</ol>

<h3 id="bellman-operator">Bellman Operator</h3>
<p>Previously, we defined <a href="/2021/06/27/mdp-bellman-eqn.html#bellman-equations">Bellman equation</a> for state-value function $v_\pi(s)$ as:
\begin{align}
v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right] \\\text{or}\quad v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_\pi(s’)\right)\tag{1}\label{1}
\end{align}
If we let
\begin{align}
\mathcal{P}^\pi_{ss’}&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}^a_{ss’}; \\\mathcal{R}^\pi_s&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s
\end{align}
then we can rewrite \eqref{1} in another form as
\begin{equation}
v_\pi(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v_\pi(s’)\tag{2}\label{2}
\end{equation}
<br />
<strong>Definition</strong> (<em>Bellman operator</em>)<br />
We define the <em>Bellman operator</em> underlying $\pi,\mathcal{T}:\mathbb{R}^\mathcal{S}\to\mathbb{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^\pi v)(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v(s’)
\end{equation}
<br />
With the help of $\mathcal{T}^\pi$, equation \eqref{2} can be rewrite as:
\begin{equation}
\mathcal{T}^\pi v_\pi=v_\pi\tag{3}\label{3}
\end{equation}
Similarly, we can rewrite the <em>Bellman optimality equation for</em> $v_*$
\begin{align}
v_*(s)&amp;=\max_{a\in\mathcal{A}}\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_*(s’)\right] \\ &amp;=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_*(s’)\right)\tag{4}\label{4}
\end{align}
and thus, we can define the <em>Bellman optimality operator</em> $\mathcal{T}^*:\mathcal{R}^\mathcal{S}\to\mathcal{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^* v)(s)=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v(s’)\right)
\end{equation}
And thus, with the help of $\mathcal{T}^*$, we can rewrite the equation \eqref{4} as:
\begin{equation}
\mathcal{T}^*v_*=v_*\tag{5}\label{5}
\end{equation}
<br />
Now everything is all set, we can move on to the next step.</p>

<h2 id="proof">Proof of the existence</h2>
<p>Let $B(\mathcal{S})$ be the space of <em>uniformly bounded functions</em> with domain $\mathcal{S}$:
\begin{equation}
B(\mathcal{S})=\{v:\mathcal{S}\to\mathbb{R}:\Vert v\Vert_\infty&lt;+\infty\}
\end{equation}
We will view $B(\mathcal{S})$ as a normed vector space with the norm $\Vert\cdot\Vert_\infty$.</p>

<p>It is easily seen that $(B(\mathcal{S}),\Vert\cdot\Vert_\infty)$ is complete: If $(v_n;n\geq0)$ is a Cauchy sequence in it then for any $s\in\mathcal{S}$, $(v_n(s);n\geq0)$ is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of $(v_n(s))$, we can show that $\Vert v_n-v\Vert_\infty\to0$. Vaguely speaking, this holds because $(v_n;n\geq0)$ is a Cauchy sequence in the norm $\Vert\cdot\Vert_\infty$, so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.</p>

<p>Let $\pi$ be some stationary policy. We have that $\mathcal{T}^\pi$ is <em>well-defined</em> since: if $u\in B(\mathcal{S})$, then also $\mathcal{T}^\pi u\in B(S)$.</p>

<p>From equation \eqref{3}, we have that $v_\pi$ is a fixed point to $\mathcal{T}^\pi$.</p>

<p>We also have that $\mathcal{T}^\pi$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$ since for any $u, v\in B(\mathcal{S})$,
\begin{align}
\Vert\mathcal{T}^\pi u-\mathcal{T}^\pi v\Vert_\infty&amp;=\gamma\max_{s\in\mathcal{S}}\left|\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left(u(s’)-v(s’)\right)\right| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\big|u(s’)-v(s’)\big| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\big\Vert u-v\big\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
where the last line follows from $\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}=1$.</p>

<p>It follows that in order to find $v_\pi$, we can construct the sequence $v_0,\mathcal{T}^\pi v_0,(\mathcal{T}^\pi)^2 v_0,\dots$, which, by Banach’s fixed-point theorem will converge to $v_\pi$ at a geometric rate.</p>

<p>From the definition \eqref{5} of $\mathcal{T}^*$, we have that $\mathcal{T}^*$ is well-defined.</p>

<p>Using the fact that $\left|\max_{a\in\mathcal{A}}f(a)-\max_{a\in\mathcal{A}}g(a)\right|\leq\max_{a\in\mathcal{A}}\left|f(a)-g(a)\right|$, similarly, we have:
\begin{align}
\Vert\mathcal{T}^*u-\mathcal{T}^*v\Vert_\infty&amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\Vert u-v\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
which tells us that $\mathcal{T}^*$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$.
<br /></p>

<p><strong>Theorem</strong><br />
Let $v$ be the fixed point of $\mathcal{T}^*$ and assume that there is policy $\pi$ which is greedy w.r.t $v:\mathcal{T}^\pi v=\mathcal{T}^* v$. Then $v=v_*$ and $\pi$ is an optimal policy.</p>

<p><strong><em>Proof</em></strong><br />
Pick any stationary policy $\pi$. Then $\mathcal{T}^\pi\leq\mathcal{T}^*$ in the sense that for any function $v\in B(\mathcal{S})$, $\mathcal{T}^\pi v\leq\mathcal{T}^* v$ holds ($u\leq v$ means that $u(s)\leq v(s),\forall s\in\mathcal{S}$).</p>

<p>Hence, for all $n\geq0$,
\begin{equation}
v_\pi=\mathcal{T}^\pi v_\pi\leq\mathcal{T}^*v_\pi\leq(\mathcal{T}^*)^2 v_\pi\leq\dots\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
or
\begin{equation}
v_\pi\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
Since $\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\mathcal{T}^*$. Thus, $v_\pi\leq v$. And since $\pi$ was arbitrary, we obtain that $v_*\leq v$.</p>

<p>Pick a policy $\pi$ such that $\mathcal{T}^\pi v=\mathcal{T}^*v$, then $v$ is also a fixed point of $\mathcal{V}^\pi$. Since $v_\pi$ is the unique fixed point of $\mathcal{T}^\pi$, we have that $v=v_\pi$, which shows that $v_*=v$ and that $\pi$ is an optimal policy.</p>

<h2 id="references">References</h2>
<p>[1] Csaba Szepesvári. <a href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a>.</p>

<p>[2] A. Lazaric. <a href="http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf">Markov Decision Processes and Dynamic Programming</a>.</p>

<p>[3] <a href="https://ai.stackexchange.com/a/11133">What is the Bellman operator in reinforcement learning?</a>. AI.StackExchange.</p>

<p>[4] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p>

<p>[5] <a href="https://en.wikipedia.org/wiki/Normed_vector_space">Normed vector space</a>. Wikipedia.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A function is called <em>uniformly bounded</em> exactly when $\Vert f\Vert_\infty&lt;+\infty$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A <em>normed vector space</em> is a vector space over the real or complex number, on which a norm is defined. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>We are gonna talk further about <em>sequences</em> in another <a href="/2021/09/06/infinite-series-of-constants.html#convergent-sequences">post</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><strong><em>Proof</em></strong><br />
Pick any $v_0\in\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\mathcal{T}$. c. Finally, we show that $\mathcal{T}$ has a single fixed point. Assume that $\mathcal{T}$ is a $\gamma$-contraction.<br />
a. To show that $(v_n)$ converges, it suffices  to show that $(v_n)$ is a Cauchy sequence. We have:
\begin{align}
\Vert v_{n+1}-v_n\Vert&amp;=\Vert\mathcal{T}v_{n}-\mathcal{T}v_{n-1}\Vert \\ &amp;\leq\gamma\Vert v_{n}-v_{n-1}\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_1-v_0\Vert
\end{align}
From the properties of norms, we have:
\begin{align}
\Vert v_{n+k}-v_n\Vert&amp;\leq\Vert v_{n+1}-v_n\Vert+\dots+\Vert v_{n+k}-v_{n+k-1}\Vert \\ &amp;\leq\left(\gamma^n+\dots+\gamma^{n+k-1}\right)\Vert v_1-v_0\Vert \\ &amp;=\gamma^n\dfrac{1-\gamma^{k}}{1-\gamma}\Vert v_1-v_0\Vert
\end{align}
and so
\begin{equation}
\lim_{n\to\infty}\sup_{k\geq0}\Vert v_{n+k}-v_n\Vert=0,
\end{equation}
shows us that $(v_n;n\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.<br />
b. Recall that the definition of the sequence $(v_n;n\geq0)$
\begin{equation}
v_{n+1}=\mathcal{T}v_n
\end{equation}
Taking the limes as $n\to\infty$ of both sides, one the one hand, we get that $v_{n+1}\to _{\Vert\cdot\Vert}v$. On the other hand, $\mathcal{T}v_n\to _{\Vert\cdot\Vert}\mathcal{T}v$, since $\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\mathcal{T}v$, which tells us that $v$ is a fixed point of $\mathcal{T}$.<br />
c. Let us assume that $v,v’$ are both fixed points of $\mathcal{T}$. Then,
\begin{align}
\Vert v-v’\Vert&amp;=\Vert\mathcal{T}v-\mathcal{v’}\Vert \\ &amp;\leq\gamma\Vert v-v’\Vert \\ \text{or}\quad(1-\gamma)\Vert v-v’\Vert&amp;\leq0
\end{align}
Thus, we must have that $\Vert v-v’\Vert=0$. Therefore, $v-v’=0$ or $v=v’$.<br />
And finally,
\begin{align}
\Vert v_n-v\Vert&amp;=\Vert\mathcal{T}v_{n-1}-\mathcal{T}v\Vert \\ &amp;\leq\gamma\Vert v_{n-1}-v\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_0-v\Vert
\end{align} <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Also, there’s gonna be a post about <em>rate of convergence</em>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="reinforcement-learning" /><category term="mathematics" /><category term="bellman-equation" /><category term="my-rl" /><summary type="html"><![CDATA[Proof of the existence of optimal policy in finite Markov Decision Processes (MDPs)]]></summary></entry><entry><title type="html">Measures</title><link href="http://localhost:4000/2021/07/03/measure.html" rel="alternate" type="text/html" title="Measures" /><published>2021-07-03T07:00:00+07:00</published><updated>2021-07-03T07:00:00+07:00</updated><id>http://localhost:4000/2021/07/03/measure</id><content type="html" xml:base="http://localhost:4000/2021/07/03/measure.html"><![CDATA[<blockquote>
  <p>When talking about <strong>measure</strong>, you might associate it with the idea of <strong>length</strong>, the measurement of something in one dimension. And then probably, you will extend your idea into two dimensions with <strong>area</strong>, or even three dimensions with <strong>volume</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<p>Despite of having different number of dimensions, all <strong>length</strong>, <strong>area</strong>,  and <strong>volume</strong> share the same properties:</p>
<ul id="roman-list">
	<li>
		<b>Non-negative</b>: in principle, length, area, and volume can take any positive value. But negative length has no meaning. Same thing happens with negative area and negative volume.
	</li>
	<li>
		<b>Additivity</b>: to get from Hanoi to Singapore by air, you have to transit at Ho Chi Minh city (HCMC). If we cut that path into two non-overlapping pieces, say Hanoi - HCMC, and HCMC - Singapore, then the total length of the two pieces must be equal to the length of original path. If we divide a rectangular into non-overlapping pieces, the area of pieces combined must be the same as the original one. The same is true for volume as well.
	</li>
	<li>
		<b>Empty set</b>: an empty cup of water has volume zero.
	</li>
	<li>
		<b>Other null sets</b>: the length of a point is $0$. The area of a line, or a curve is $0$. The volume of a plane or a surface is also $0$.
	</li>
	<li>
		<b>Translation invariance</b>: length, area and volume are unchanged (<b>invariant</b>) under shifts (<b>translation</b>) in space.
	</li>
	<li>
		<b>Hyper-rectangles</b>: an interval of form $[a, b]\subset\mathbb{R}^3$ has length $b-a$. The area of a rectangle $[a_1,b_1]\times[a_2,b_2]$ is $(b_1-a_1)(b_2-a_2)$. And the volume of a rectangular $[a_1,b_1]\times[a_2,b_2]\times[a_3,b_3]$ is $(b_1-a_1)(b_2-a_2)(b_3-a_3)$. 
	</li>
</ul>

<figure>
	<img src="/assets/images/2021-07-03/lego.jpg" alt="Lego" style="display: block; margin-left: auto; margin-right: auto; width: 700px;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<ul>
  <li><a href="#lebesgue-measure">Lebesgue Measure</a></li>
  <li><a href="#measures">Measures</a></li>
  <li><a href="#int-measure-idea">Integration with respect to a Measure: The Idea</a></li>
  <li><a href="#prop-int">Properties of the Integral</a></li>
  <li><a href="#int-measure-detail">Integration with respect to a Measure: The Details</a></li>
  <li><a href="#construct-measure">Constructing Measures from old ones</a></li>
  <li><a href="#other-types">Other types of Measures</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#other-resources">Other Resources</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="lebesgue-measure">Lebesgue Measure</h2>
<p>Is an extension of the classical notion of length in $\mathbb{R}$, area in $\mathbb{R}^2$ to any $\mathbb{R}^k$ using k-dimensional hyper-rectangles.</p>

<p><strong>Definition</strong><br />
Given an open set $S\equiv\sum_k(a_k,b_k)$ containing disjoint intervals, the <strong>Lebesgue measure</strong> is defined by:
\begin{equation}
\mu_L(S)\equiv\sum_{k}(b_k-a_k)
\end{equation}
Given a closed set $S’\equiv[a,b]-\sum_k(a_k,b_k)$,
\begin{equation}
\mu_L(S’)\equiv(b-a)-\sum_k(b_k-a_k)
\end{equation}</p>

<h2 id="measures">Measures</h2>
<p><strong>Definition</strong><br />
Let $\mathcal{X}$ be any set. A <em>measure</em> on $\mathcal{X}$ is a function $\mu$ that maps the set of subsets on $\mathcal{X}$ to $[0,\infty]$ ($\mu:2^\mathcal{X}\rightarrow[0,\infty]$) that satisfies:</p>
<ol>
  <li>$\mu(\emptyset)=0$</li>
  <li><em>Countable additivity property</em>: for any countable and pairwise disjoint collection of subsets of $\mathcal{X},\mathcal{A_1},\mathcal{A_2},\dots$,
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)=\sum_i\mu(\mathcal{A_i})
\end{equation}
$\mu(\mathcal{A})$ is called <em>measure of the set $\mathcal{A}$</em>, or <em>measure of $\mathcal{A}$</em>.</li>
</ol>

<p><strong>Properties</strong></p>
<ol>
  <li><em>Monotonicity</em>: If $\mathcal{A}\subset\mathcal{B}$, then $\mu(\mathcal{A})\leq\mu(\mathcal{B})$</li>
  <li><em>Subadditivity</em>: If $\mathcal{A_1},\mathcal{A_2},\dots$ is a countable collection of sets, not necessarily disjoint, then
\begin{equation}
\mu\left(\bigcup_i\mathcal{A_i}\right)\leq\sum_i\mu(\mathcal{A_i})
\end{equation}</li>
</ol>

<p><strong>Examples</strong></p>
<ol>
  <li><em>Cardinality of a set</em> \(\#\mathcal{A}\)</li>
  <li><em>A point mass at 0</em>. Consider a measure \(\delta_{\{0\}}\) on $\mathbb{R}$ defined to give measure 1 to any set that contains 0 and measure 0 to any set that does not
\begin{equation}\delta_{{0}}(\mathcal{A})=\#\left(A\cap\{0\}\right)=\begin{cases}
1\quad\textsf{if }0\in\mathcal{A} \\ 0\quad\textsf{otherwise}
\end{cases}\end{equation}
for $\mathcal{A}\subset\mathbb{R}$</li>
  <li><em>Counting measure on the integers</em>. Consider a measure $\mu_\mathbb{Z}$ that assigns to each set $\mathcal{A}$ the number of integers contained in $\mathcal{A}$
\begin{equation}
\delta_\mathbb{Z}(\mathcal{A})=\#\left(\mathcal{A}\cap\mathbb{Z}\right)
\end{equation}</li>
  <li><em>Geometric measure</em>. Suppose that $0&lt;r&lt;1$. We define a measure on $\mathbb{R}$ that assigns to a set $\mathcal{A}$ a geometrically weighted sum over non-negative integers in $\mathcal{A}$
\begin{equation}
\mu(\mathcal{A})=\sum_{i\in\mathcal{A}\cap\mathbb{Z}^+}r^i
\end{equation}</li>
  <li><em>Binomial measure</em>. Let $n\in\mathbb{N}^+$ and let $0&lt;p&lt;1$. We define $\mu$ as:
\begin{equation}
\mu(\mathcal{A})=\sum_{k\in\mathcal{A}\cap\{0,1,\dots,n\}}{n\choose k}p^k(1-p)^{n-k}
\end{equation}</li>
  <li><em>Bivariate Gaussian</em>. We define a measure on $\mathbb{R}^2$ by:
\begin{equation}
\mu({\mathcal{A}})=\int_\mathcal{A}\dfrac{1}{2\pi}\exp\left({\dfrac{-1}{2}(x^2+y^2)}\right)\,dx\,dy
\end{equation}</li>
  <li><em>Uniform on a Ball in $\mathbb{R}^3$</em>. Let $\mathcal{B}$ be the set of points in $\mathbb{R}^3$ that are within a distance 1 from the origin (unit ball in $\mathbb{R}^3$). We define a measure on $\mathbb{R}^3$ as:
\begin{equation}
\mu(\mathcal{A})=\dfrac{3}{4\pi}\mu_L(\mathcal{A}\cap\mathcal{B})
\end{equation}</li>
</ol>

<h2 id="int-measure-idea">Integration with respect to a Measure: The Idea</h2>
<p>Consider $f:\mathcal{X}\rightarrow\mathbb{R}$, where $\mathcal{X}$ is any set and a measure $\mu$ on $\mathcal{X}$ and compute the integral of $f$ w.r.t $\mu$: $\int f(x)\,\mu(dx)$. We have:</p>
<ol>
  <li>For any function $f$,
\begin{equation}
\int g(x)\,\mu_L(dx)=\int g(x)\,dx
\end{equation}
Because $\mu_L(dx)\equiv\mu_L([x,x+dx[)=dx$</li>
  <li>For any function $f$,
\begin{equation}
\int g(x)\,\delta_{\{\alpha\}}(dx)=g(\alpha)
\end{equation}
Consider the infinitesimal $\delta_{\{\alpha\}}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\neq\alpha$, then the infinitesimal interval $[x,x+dx[$ does not contain $\alpha$, so
\begin{equation}
\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=0
\end{equation}
If $x=\alpha,\delta_{\{\alpha\}}(dx)\equiv\delta_{\{\alpha\}}([x,x+dx[)=1$. Thus, when we add up all of the infinitesimals, we get $g(\alpha)\cdot1=g(\alpha)$</li>
  <li>For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathbb{Z}(dx)=\sum_{i\in\mathbb{Z}}g(i)
\end{equation}
Similarly, consider the infinitesimal $\delta_\mathbb{Z}(dx)$ as $x$ ranges over $\mathbb{R}$. If $x\notin\mathbb{Z}$, then $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=0$. And otherwise if $x\in\mathbb{Z}$, $\delta_\mathbb{Z}(dx)\equiv\delta_\mathbb{Z}([x,x+dx[)=1$ since an infinitesimal interval can contain at most one integer. Hence, $g(x)\,\delta_\mathbb{Z}=g(x)$ if $x\in\mathbb{Z}$ and $=0$ otherwise. When we add up all of the infinitesimals over $x$, we get the sum above.</li>
  <li>Suppose $\mathcal{C}$ is a countable set. We can define <em>counting measure</em> on $\mathcal{C}$ to map $\mathcal{A}\rightarrow\#(\mathcal{A}\cap\mathcal{C})$ (recall that $\delta_\mathcal{C}(\mathcal{A})=\#(\mathcal{A}\cap\mathcal{C})$). For any function $f$,
\begin{equation}
\int g(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v),
\end{equation}
using the same basic argument as in the above example.</li>
</ol>

<p>From the above examples, we have that <em>integrals w.r.t to Lebesgue measure are just ordinary integrals, and that integrals w.r.t Counting measure are just ordinary summation</em>.<br />
Consider measures built from Lebesgue and Counting measure, we have:</p>
<ol>
  <li>Suppose $\mu$ is a measure that satisfies $\mu(dx)=f(x)\,\mu_L(dx)$, then for any function $g$,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,f(x)\,\mu_L(dx)=\int g(x)\,f(x)\,dx
\end{equation}
We say that $f$ is the density of $\mu$ w.r.t Lebesgue measure in this case.</li>
  <li>Suppose $\mu$ is a measure that satisfies $\mu(dx)=p(x)\delta_\mathcal{C}(dx)$ for a countable set $\mathcal{C}$, then for any function g,
\begin{equation}
\int g(x)\,\mu(dx)=\int g(x)\,p(x)\,\delta_\mathcal{C}(dx)=\sum_{v\in\mathcal{C}}g(v)\,f(v)
\end{equation}
We say that $p$ is the density of $\mu$ w.r.t Counting measure on $\mathcal{C}$.</li>
</ol>

<h2 id="prop-int">Properties of the Integral</h2>
<p>A function is said to be <em>integrable</em> w.r.t $\mu$ if $\int|f(x)|\,\mu(dx)&lt;\infty$. An integrable function has a well-defined and finite integral. If $f(x)\geq0$, the integral is always well-defined but may be $\infty$.<br />
Suppose $\mu$ is a measure on $\mathcal{X},\mathcal{A}\subset\mathcal{X}$, and $g$ is a real-valued function on $\mathcal{X}$. We define the integral of $g$ over the set $\mathcal{A}$, denoted by $\int_\mathcal{A}g(x)\,\mu(dx)$, as
\begin{equation}
\int_\mathcal{A}g(x)\,\mu(dx)=\int g(x)\,𝟙_\mathcal{A}(x)\,\mu(dx),
\end{equation}
where \(𝟙_\mathcal{A}\) is an <em>indicator function</em> (\(𝟙_\mathcal{A}(x)=1\) if $x\in\mathcal{A}$, and $=0$ otherwise).</p>

<p>Let $\mu$ is a measure on $\mathcal{X},\mathcal{A},\mathcal{B}\subset\mathcal{X},c\in\mathbb{R}$ and $f,g$ are integrable functions. The following properties hold for every $\mu$</p>
<ol>
  <li><em>Constant Functions</em>
\begin{equation}
\int_\mathcal{A}c\,\mu(dx)=c\cdot\mu(\mathcal{A})
\end{equation}</li>
  <li><em>Linearity</em>
\begin{align}
\int_\mathcal{A}cf(x)\,\mu(dx)&amp;=c\int_\mathcal{A}f(x)\,\mu(dx) \\\int_\mathcal{A}\left(f(x)+g(x)\right)\,\mu(dx)&amp;=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{A}g(x)\,\mu(dx)
\end{align}</li>
  <li><em>Monotonicity</em>. If $f\leq g$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{A}g(x)\,\mu(dx),\forall\mathcal{A}$. This implies:
    <ul>
      <li>If $f\geq0$, then $\int f(x)\,\mu(dx)\geq0$.</li>
      <li>If $f\geq0$ and $\mathcal{A}\subset\mathcal{B}$, then $\int_\mathcal{A}f(x)\,\mu(dx)\leq\int_\mathcal{B}f(x)\,\mu(dx)$.</li>
    </ul>
  </li>
  <li><em>Null Sets</em>. If $\mu(\mathcal{A})=0$, then $\int_\mathcal{A}f(x)\,\mu(dx)=0$.</li>
  <li><em>Absolute Values</em>
\begin{equation}
\left|\int f(x)\,\mu(dx)\right|\leq\int\left|f(x)\right|\,\mu(dx)
\end{equation}</li>
  <li><em>Monotone Convergence</em>. If $0\leq f_1\leq f_2\leq\dots$ is an increasing sequence of integrable functions that converge to $f$, then
\begin{equation}
\lim_{k\to\infty}\int f_k(x)\,\mu(dx)=\int f(x)\,\mu(dx)
\end{equation}</li>
  <li><em>Linearity in region of integration</em>. If $\mathcal{A}\cap\mathcal{B}=\emptyset$,
\begin{equation}
\int_{\mathcal{A}\cup\mathcal{B}}f(x)\,\mu(dx)=\int_\mathcal{A}f(x)\,\mu(dx)+\int_\mathcal{B}f(x)\,\mu(dx)
\end{equation}</li>
</ol>

<h2 id="int-measure-detail">Integration with respect to a Measure: The Details</h2>
<ol>
  <li><strong>Step 1</strong>. Define the integral for simple functions.
    <ul>
      <li><em>Simple function</em>: is a function that takes only a finite number of different values.
        <ul>
          <li>All constant functions are simple functions.</li>
          <li>The indicator function ($𝟙_\mathcal{A}$) of a set $\mathcal{A}\subset\mathcal{X}$ is a simple function (taking values in $\{0,1\}$).</li>
          <li>Any constant times an indicator ($c𝟙_\mathcal{A}$) is also a simple function (taking values in $\{0,c\}$).</li>
          <li>Similarly, given disjoint sets $\mathcal{A_1},\mathcal{A_2}$, the linear combination \(c_1𝟙_\mathcal{A_1}+c_2𝟙_\mathcal{A_2}\) is a simple function (taking values in $\{0,c_1,c_2\}$)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</li>
          <li>In fact, any simple function can be expressed as a linear combination of a finite number of indicator functions. That is, if $f$ is <em>any</em> simple function on $\mathcal{X}$, then there exists some finite integer $n$, non-zero constants $c_1,\dots,c_n$ and <em>disjoint</em> sets $\mathcal{A_1},\dots\mathcal{A_n}\subset\mathcal{X}$ such that
   \begin{equation}
   f=c_1𝟙_\mathcal{A_1}+\dots+c_n𝟙_\mathcal{A_n}
   \end{equation}</li>
        </ul>
      </li>
      <li>So, if $f:\mathcal{X}\to\mathbb{R}$ is a simple function as just defined, we have that
\begin{equation}
\int \mu(dx)=c_1\mu(\mathcal{A_1})+\dots+c_n\mu(\mathcal{A_n})
\end{equation}</li>
    </ul>
  </li>
  <li><strong>Step 2</strong>. Define the integral for general non-negative functions, approximating the general function by simple functions.
    <ul>
      <li>The idea is that we can approximate any general non-negative function $f:\mathcal{X}\to[0,\infty[$ well by some non-negative simple functions that $\leq f$ <sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</li>
      <li>If $f:\mathcal{X}\to[0,\infty[$ is a general function and $0\leq s\leq f$ is a simple function (then $\int s(x)\,\mu(dx)\leq\int f(x)\,\mu(dx)$). The closer that $s$ approximates $f$, the closer we expect $\int s(x)\,\mu(dx)$ and $\int f(x)\,\mu(x)$ to be.</li>
      <li>To be more precise, we define the integral $\int f(x)\,\mu(dx)$ to be the smallest value $I$ such that $\int s(x)\,\mu(x)\leq I$, for all simple functions $0\leq s\leq f$.
\begin{equation}
\int f(x)\,\mu(dx)\approx\sup\left\{\int s(x)\,\mu(dx)\right\}
\end{equation}</li>
    </ul>
  </li>
  <li><strong>Step 3</strong>. Define the integral for general real-valued functions by separately integrating the positive and negative parts of the function.
    <ul>
      <li>If $f:\mathcal{X}\to\mathbb{R}$ is a general function, we can define its <em>positive part</em> $f^+$ and its <em>negative part</em> $f^-$ by
\begin{align}
f^+(x)&amp;=\max\left(f(x),0\right) \\ f^-(x)&amp;=\max\left(-f(x),0\right)
\end{align}</li>
      <li>Since both $f^+$ and $f^-$ are non-negative functions and $f=f^+-f^-$, we have
\begin{equation}
\int f(x)\,\mu(dx)=\int f^+(x)\,\mu(dx)-\int f^-(x)\,\mu(dx)
\end{equation}
This is a well-defined number (possibly infinite) if and only if at least one of $f^+$ and $f^-$ has a finite integral.</li>
    </ul>
  </li>
</ol>

<h2 id="construct-measure">Constructing Measures from old ones</h2>
<ol>
  <li><em>Sums and multiples</em>
    <ul>
      <li>Consider the point mass measures at 0 and 1, \(\delta_{\\{0\\}},\delta_1\), and construct a two new measures on $\mathbb{R}$, \(\mu=\delta_{\\{0\\}}+\delta_{\\{1\\}}\) and \(v=4\delta_{\\{0\\}}\), defined by
\begin{align}
\mu(\mathcal{A})&amp;=\delta_{\{0\}}(\mathcal{A})+\delta_{\{0\}}(\mathcal{A}) \\ v(\mathcal{A})&amp;=4\delta_{\{0\}}(\mathcal{A})
\end{align}</li>
      <li>The measure $\mu$ counts how many elements of \(\\{0,1\\}\) are in its argument. Thus, the counting measure of the integers can be re-expressed as
\begin{equation}
\delta_\mathbb{Z}=\sum_{i=-\infty}^{\infty}\delta_{\{i\}}
\end{equation}</li>
      <li>By combining the operations of summation and multiplication, we can write the Geometric measure in the above example 
\begin{equation}
\sum_{i=0}^{\infty}r^i\delta_{\{i\}}
\end{equation}</li>
    </ul>
  </li>
  <li><em>Restriction to a Subset</em>
    <ul>
      <li>Suppose $\mu$ is a measure on $\mathcal{X}$ and $\mathcal{B}\subset\mathcal{X}$. We can define a new measure on $\mathcal{B}$ which maps $\mathcal{A}\subset\mathcal{B}\to\mu(\mathcal{A})$. This is called the restriction of $\mu$ to the set $\mathcal{B}$.</li>
    </ul>
  </li>
  <li><em>Measure Induced by a Function</em>
    <ul>
      <li>Suppose $\mu$ is a measure on $\mathcal{X}$ and $g:\mathcal{X}\to\mathcal{Y}$. We can use $\mu$ and $g$ to define a new measure $v$ on $\mathcal{Y}$ by
\begin{equation}
v(\mathcal{A})=\mu(g^{-1}(\mathcal{A})),
\end{equation}
for $\mathcal{A}\subset\mathcal{Y}$. This is called the <em>measure induced from $\mu$ by $g$</em>.</li>
      <li>Therefore, for any $f:\mathcal{Y}\to\mathbb{R}$,
\begin{equation}
\int f(y)\,v(dy)=\int f(g(x))\,\mu(dx)
\end{equation}</li>
    </ul>
  </li>
  <li><em>Integrating a Density</em>
    <ul>
      <li>Suppose $\mu$ is a measure on $\mathcal{X}$ and $f:\mathcal{X}\to\mathbb{R}$. We can define a new measure $v$ on $\mathcal{X}$ as
\begin{equation}
v(\mathcal{A})=\int_\mathcal{A}f(x)\,\mu(dx)\tag{1}\label{1}
\end{equation}
We say that $f$ is the <em>density</em> of the measure $v$ w.r.t $\mu$.</li>
      <li>If $v,\mu$ are measures for which the equation \eqref{1} holds for every $\mathcal{A}\subset\mathcal{X}$, we say that $v$ has a density $f$ w.r.t $\mu$. This implies two useful results:
        <ul>
          <li>$\mu(\mathcal{A})=0$ implies $v(\mathcal{A})=0$.</li>
          <li>$v(dx)=f(x)\,\mu(dx)$.</li>
        </ul>
      </li>
    </ul>
  </li>
</ol>

<h2 id="other-types">Other types of Measures</h2>
<p>Suppose that $\mu$ is a measure on $\mathcal{X}$</p>
<ol>
  <li>If $\mu(\mathcal{X})=\infty$, we say that $\mu$ is an <strong>infinite measure</strong>.</li>
  <li>If $\mu(\mathcal{X}&lt;\infty)$, we say that $\mu$ is a <strong>finite measure</strong>.</li>
  <li>If $\mu(\mathcal{X}&lt;1)$, we say that $\mu$ is a <strong>probability measure</strong>.</li>
  <li>If there exists a countable set $\mathcal{S}$ such that $\mu(\mathcal{X}-\mathcal{S})=0$, we say that $\mu$ is a <strong>discrete measure</strong>. Equivalently, $\mu$ has a density w.r.t <strong>counting measure</strong> on $\mathcal{S}$.</li>
  <li>If $\mu$ has a density w.r.t Lebesgue measure, we say that $\mu$ is a <strong>continuous measure</strong>.</li>
  <li>If $\mu$ is neither <strong>continuous</strong> nor <strong>discrete</strong>, we say that $\mu$ is a <strong>mixed measure</strong>.</li>
</ol>

<h2 id="references">References</h2>
<p>[1] Literally, this post is mainly written from a source that I’ve lost the reference :(. Hope that I can update this line soon.</p>

<p>[2] <a href="https://mathworld.wolfram.com/LebesgueMeasure.html">Lebesgue Measure</a>.</p>

<p>[3] <a href="https://www.countbayesie.com/blog/2015/8/17/a-very-brief-and-non-mathematical-introduction-to-measure-theory-for-probability">Measure Theory for Probability: A Very Brief Introduction</a>.</p>

<h2 id="other-resources">Other Resources</h2>
<ol>
  <li><a href="https://www.youtube.com/watch?v=cyW5z-M2yzw">Music and Measure Theory - 3Blue1Brown</a> - this is one of my favourite Youtube channels.</li>
</ol>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>If $\mathcal{A_1},\mathcal{A_2}$ were not disjoint, we could define $\mathcal{B_1}=\mathcal{A_1}-\mathcal{A_2}$, $\mathcal{B_2}=\mathcal{A_2}-\mathcal{A_1}$, and $\mathcal{B_3}=\mathcal{A_1}\cap\mathcal{A_2}$. Then the function is equal to \(c_1𝟙_\mathcal{B_1}+c_2𝟙_\mathcal{B_2}+(c_1+c_2)𝟙_\mathcal{B_3}\). <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">

      <p><a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="measure-theory" /><category term="random-stuffs" /><summary type="html"><![CDATA[A brief look into Measure theory]]></summary></entry></feed>