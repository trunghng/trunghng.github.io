<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed/by_tag/dqn.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-09-14T18:49:33+07:00</updated><id>/feed/by_tag/dqn.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Temporal-Difference Learning. Deep-Q Network</title><link href="/artificial-intelligent/reinforcement-learning/2021/09/14/td-learning-dqn.html" rel="alternate" type="text/html" title="Temporal-Difference Learning. Deep-Q Network" /><published>2021-09-14T18:16:00+07:00</published><updated>2021-09-14T18:16:00+07:00</updated><id>/artificial-intelligent/reinforcement-learning/2021/09/14/td-learning-dqn</id><content type="html" xml:base="/artificial-intelligent/reinforcement-learning/2021/09/14/td-learning-dqn.html">&lt;blockquote&gt;
  &lt;p&gt;So far in this &lt;a href=&quot;/tag/my-rl&quot;&gt;series&lt;/a&gt;, we have gone through ideas of &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html&quot;&gt;&lt;strong&gt;dynamic programming&lt;/strong&gt; (DP)&lt;/a&gt; and &lt;a href=&quot;/artificial-intelligent/reinforcement-learning/2021/08/21/monte-carlo-in-rl.html&quot;&gt;&lt;strong&gt;Monte Carlo&lt;/strong&gt;&lt;/a&gt;. What will happen if we combine these ideas together? &lt;strong&gt;Temporal-deffirence (TD) learning&lt;/strong&gt; is our answer.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Richard S. Sutton &amp;amp; Andrew G. Barto. &lt;a href=&quot;https://mitpress.mit.edu/books/reinforcement-learning-second-edition&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[2] David Silver. &lt;a href=&quot;https://www.davidsilver.uk/teaching/&quot;&gt;UCL course on RL&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[3] Mnih, V., Kavukcuoglu, K., Silver, D. et al. &lt;a href=&quot;https://doi.org/10.1038/nature14236&quot;&gt;Human-level control through deep reinforcement learning&lt;/a&gt;. Nature 518, 529–533 (2015).&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="artificial-intelligent" /><category term="reinforcement-learning" /><category term="td-learning" /><category term="q-learning" /><category term="dqn" /><category term="my-rl" /><summary type="html">So far in this series, we have gone through ideas of dynamic programming (DP) and Monte Carlo. What will happen if we combine these ideas together? Temporal-deffirence (TD) learning is our answer.</summary></entry></feed>