<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed/by_tag/calculus.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-08-05T15:34:27+07:00</updated><id>http://localhost:4000/feed/by_tag/calculus.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Power Series</title><link href="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html" rel="alternate" type="text/html" title="Power Series" /><published>2021-09-21T15:40:00+07:00</published><updated>2021-09-21T15:40:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/21/power-series</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html">&gt; Recall that in the previous post, [Infinite Series of Constants]({% post_url 2021-09-06-infinite-series-of-constants %}), we mentioned a type of series called **power series** a lot. In the content of this post, we will be diving deeper into details of that series.

&lt;!-- excerpt-end --&gt;
- [Power Series](#power-series)
- [The Interval of Convergence](#int-conv)
	- [Example](#eg1)
- [Differentiation and Integration of Power Series](#dif-int-power-series)
	- [Differentiation of Power Series](#dif-power-series)
	- [Integration of Power Series](#int-power-series)
	- [Example](#eg2)
- [Taylor Series, Taylor&apos;s Formula](#taylor-series-formula)
	- [Taylor Series](#taylor-series)
	- [Taylor&apos;s Formula](#taylors-formula)
- [Operations on Power Series](#op-power-series)
	- [Multiplication](#mult)
	- [Division](#div)
	- [Substitution](#sub)
	- [Even and Odd Functions](#even-odd-funcs)
- [Uniform Convergence for Power Series](#uni-conv-power-series)
	- [Continuity of the Sum](#cont-sum)
	- [Integrating term by term](#int)
	- [Differentiating term by term](#dif)
- [References](#references)
- [Footnotes](#footnotes)

## Power Series
A **power series** is a series of the form
\begin{equation}
\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots,
\end{equation}
where the coefficient $a_n$ are constants and $x$ is a variable.  

## The Interval of Convergence
{: #int-conv}
Similar to what we have done in the post of [infinite series of constants]({% post_url 2021-09-06-infinite-series-of-constants %}), we begin studying properties of power series by considering their convergence behavior.  

**Lemma 1**  
*If a power series $\sum a_nx^n$ converges at $x_1$, with $x_1\neq 0$, then it converges [absolutely]({% post_url 2021-09-06-infinite-series-of-constants %}#abs-conv) at all $x$ with $\vert x\vert&lt;\vert x_1\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\vert x\vert&gt;\vert x_1\vert$.*  

**Proof**  
By the [$n$-th term test]({% post_url 2021-09-06-infinite-series-of-constants %}#nth-term-test), we have that if $\sum a_nx^n$ converges, then $a_nx^n\to 0$. In particular, if $n$ is sufficiently large, then $\vert a_n{x_1}^n\vert&lt;1$, and therefore
\begin{equation}
\vert a_nx^n\vert=\vert a_n{x_1}^n\vert\left\vert\dfrac{x}{x_1}\right\vert^n\&lt;r^n,\tag{1}\label{1}
\end{equation}
where $r=\vert\frac{x}{x_1}\vert$. Suppose that $\vert x\vert&lt;\vert x_1\vert$, we have
\begin{equation}
r=\left\vert\dfrac{x}{x_1}\right\vert&lt;1,
\end{equation}
which leads to the result that geometric series $\sum r^n$ converges (with the sum $\frac{1}{1-r}$). And hence, from \eqref{1} and by the [comparison test]({% post_url 2021-09-06-infinite-series-of-constants %}#comparison-test), the series $\sum\vert a_nx^n\vert$ also converges.  

Moreover, if $\sum a_n{x_1}^n$ diverges, then $\sum\vert a_n{x_1}^n\vert$ also diverges. By the [comparison test]({% post_url 2021-09-06-infinite-series-of-constants %}#comparison-test), for any $x$ such that $\vert x\vert&gt;\vert x_1\vert$, we also have that $\sum\vert a_nx^n\vert$ diverges. This leads to the divergence of $\sum a_nx^n$, because if the series $\sum a_nx^n$ converges, so does $\sum\vert a_nx^n\vert$, which contradicts to our result.  

These are some main facts about the convergence behavior of an arbitrary power series and some properties of its:
- Given a power series $\sum a_nx^n$, precisely one of the following is true:
	- The series converges only for $x=0$.
	- The series is absolutely convergent for all $x$.
	- There exists a positive real number $R$ such that the series is absolutely convergent for $\vert x\vert\&lt;R$ and divergent for $\vert x\vert&gt;R$.
- The positive real number $R$ is called **radius of convergence** of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$.
- The set of all $x$&apos;s for which a power series converges is called its **interval of convergence**.
- When the series converges only for $x=0$, we define $R=0$; and we define $R=\infty$ when the series converges for all $x$.
- Every power series $\sum a_nx^n$ has a radius of convergence $R$, where $0\leq R\leq\infty$, with the property that the series converges absolutely if $\vert x\vert\&lt;R$ and diverges if $\vert x\vert&gt;R$.  

### Example
{: #eg1} 
Find the interval of convergence of the series
\begin{equation}
\sum_{n=0}^{\infty}\dfrac{x^n}{n+1}=1+\dfrac{x}{2}+\dfrac{x^2}{3}+\ldots
\end{equation}

**Solution**  
In order to find the interval of convergence of a series, we begin by identifying its radius of convergence.  

Consider a power series $\sum a_nx^n$. Suppose that this limit exists, and has $\infty$ as an allowed value, we have
\begin{equation}
\lim_{n\to\infty}\dfrac{\vert a_{n+1}x^{n+1}\vert}{a_nx^n}=\lim_{n\to\infty}\left\vert\dfrac{a_{n+1}}{a_n}\right\vert.\vert x\vert=\dfrac{\vert x\vert}{\lim_{n\to\infty}\left\vert\frac{a_n}{a_{n+1}}\right\vert}=L
\end{equation}
By the [ratio test]({% post_url 2021-09-06-infinite-series-of-constants %}#ratio-test), we have $\sum a_nx^n$ converges absolutely if $L&lt;1$ and diverges in case of $L&gt;1$. Or in other words, the series converges absolutely if
\begin{equation}
\vert x\vert&lt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert,
\end{equation}
or diverges if
\begin{equation}
\vert x\vert&gt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}
From the definition of radius of convergence, we can choose the radius of converge of $\sum a_nx^n$ as
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}

Back to our problem, for the series $\sum\frac{x^n}{n+1}$, we have its radius of convergence is
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert=\lim_{n\to\infty}\dfrac{\frac{1}{n+1}}{\frac{1}{n+2}}=\lim_{n\to\infty}\dfrac{n+2}{n+1}=1
\end{equation}
At $x=1$, the series becomes the *harmonic series* $1+\frac{1}{2}+\frac{1}{3}+\ldots$, which diverges; and at $x=-1$, it is the *alternating harmonic series* $1-\frac{1}{2}+\frac{1}{3}-\ldots$, which converges. Hence, the interval of convergence of the series is $[-1,1)$.

## Differentiation and Integration of Power Series
{: #dif-int-power-series}

It is easily seen that the sum of the series $\sum_{n=0}^{\infty}a_nx^n$  is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots\tag{2}\label{2}
\end{equation}
This relation between the series and the function is also expressed by saying that $\sum a_nx^n$ is a **power series expansion** of $f(x)$.  

These are some crucial facts about that relation.
- (i) The function $f(x)$ defined by \eqref{2} is continuous on the open interval $(-R,R)$.  
- (ii) The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula
\begin{equation}
f&apos;(x)=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots\tag{3}\label{3}
\end{equation}
- (iii) If $x$ is any point in $(-R,R)$, then
\begin{equation}
\int_{0}^{x}f(t)\,dt=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{4}\label{4}
\end{equation}

**Remark**  
We have that series \eqref{3} and \eqref{4} converge on the interval $(-R,R)$.  

**Proof**  
1. We begin by proving the convergence on $(-R,R)$ of \eqref{3}.  
Let $x$ be a point in the interval $(-R,R)$ and choose $\epsilon&gt;0$ so that $\vert x\vert+\epsilon\&lt;R$. Since $\vert x\vert+\epsilon$ is in the interval, $\sum\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert$ converges.  
We continue by proving the inequality
\begin{equation}
\vert nx^{n-1}\vert\leq\left(\vert x\vert+\epsilon\right)^n\hspace{1cm}\forall n\geq n_0,
\end{equation}
where $\epsilon&gt;0$, $n_0$ is a positive integer.  
We have
\begin{align}
\lim_{n\to\infty}n^{1/n}&amp;=\lim_{n\to\infty} \\\\ &amp;=\lim_{n\to\infty}\exp\left(\frac{\ln n}{n}\right) \\\\ &amp;=\exp\left(\lim_{n\to\infty}\frac{\ln n}{n}\right) \\\\ &amp;={\rm e}^0=1,
\end{align}
where in the fourth step, we use the *L’Hospital theorem*[^1]. Therefore, as $n\to\infty$
\begin{equation}
n^{1/n}\vert x\vert^{1-1/n}\to\vert x\vert
\end{equation}
Then for all sufficiently large $n$&apos;s
\begin{align}
n^{1/n}\vert x\vert^{1-1/n}&amp;\leq\vert x\vert+\epsilon \\\\ \vert nx^{n-1}\vert&amp;\leq\left(\vert x\vert+\epsilon\right)^n
\end{align}
This implies that
\begin{equation}
\vert na_nx^{n-1}\vert\leq\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert
\end{equation}
By the [comparison test]({% post_url 2021-09-06-infinite-series-of-constants %}#comparison-test), we have that the series $\sum\vert na_nx^{n-1}\vert$ converges, and so does $\sum na_nx^{n-1}$.  

2. Since $\sum\vert a_nx^n\vert$ converges and
\begin{equation}
\left\vert\dfrac{a_nx^n}{n+1}\right\vert\leq\vert a_nx^n\vert,
\end{equation}
the [comparison test]({% post_url 2021-09-06-infinite-series-of-constants %}#comparison-test) implies that $\sum\left\vert\frac{a_nx^n}{n+1}\right\vert$ converges, and therefore
\begin{equation}
x\sum\frac{a_nx^n}{n+1}=\sum\frac{1}{n+1}a_nx^{n+1}
\end{equation}
also converges.

### Differentiation of Power Series
{: #dif-power-series}

If we instead apply (ii) to the function $f&apos;(x)$ in \eqref{3}, then it follows that $f&apos;(x)$ is also differentiable. Doing the exact same process to $f&apos;\&apos;(x)$, we also have that $f&apos;\&apos;(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:  

*In the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term*.
\begin{equation}
\dfrac{d}{dx}\left(\sum a_nx^n\right)=\sum\dfrac{d}{dx}(a_nx^n)
\end{equation}

### Integration of Power Series
{: #int-power-series}

Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \eqref{4} as
\begin{equation}
\int\left(\sum a_nx^n\right)\,dx=\sum\left(\int a_nx^n\,dx\right)
\end{equation}

### Example
{: #eg2}

Find a power series expansion of ${\rm e}^x$.  

**Solution**  
Since ${\rm e}^x$ is the only function that equals its own derivatives[^2] and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We therefore want each term to be the derivative of the one that follows it.  

Starting with $1$ as the constant term, the next should be $x$, then $\frac{1}{2}x^2$, then $\frac{1}{2.3}x^3$, and so on. This produces the series
\begin{equation}
1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots,\tag{5}\label{5}
\end{equation}
which converges for all $x$ because
\begin{equation}
R=\lim_{n\to\infty}\dfrac{\frac{1}{n!}}{\frac{1}{(n+1)!}}=\lim_{n\to\infty}(n+1)=\infty
\end{equation}
We have constructed the series \eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$,
\begin{equation}
{\rm e}^x=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots
\end{equation}

## Taylor Series, Taylor&apos;s Formula
{: #taylor-series-formula}

### Taylor Series
Assume that $f(x)$ is the sum of a power series with positive radius of convergence
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots,\hspace{1cm}R&gt;0\tag{6}\label{6}
\end{equation}
By the results obtained from previous section, differentiating \eqref{6} term by term we have
\begin{align}
f^{(1)}(x)&amp;=a_1+2a_2x+3a_3x^2+\ldots \\\\ f^{(2)}(x)&amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\ldots \\\\ f^{(3)}(x)&amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\ldots
\end{align}
and in general,
\begin{equation}
f^{(n)}(x)=n!a_n+A(x),\tag{7}\label{7}
\end{equation}
where $A(x)$ contains $x$ as a factor.  

Since these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \eqref{7} we obtain
\begin{equation}
f^{(n)}(0)=n!a_n
\end{equation}
so
\begin{equation}
a_n=\dfrac{f^{(n)}(0)}{n!}
\end{equation}
Putting this result in \eqref{6}, our series becomes
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\ldots\tag{8}\label{8}
\end{equation}
This power series is called **Taylor series** of $f(x)$ [at $x=0$], which is named after the person who introduced it, Brook Taylor.  

If we use the convention that $0!=1$, then \eqref{8} can be written as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(0)}{n!}x^n
\end{equation}
The numbers $a_n=\frac{f^{(n)}(0)}{n!}$ are called the **Taylor coefficients** of $f(x)$.  

**Remark**  
Given a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?  
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\ldots
\end{equation}
Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is
\begin{align}
f(x)&amp;=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\\\ &amp;=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\ldots\tag{9}\label{9}
\end{align}

### Taylor&apos;s Formula
{: #taylors-formula}
If we break off the Taylor series on the right side of \eqref{8} at the term containing $x^n$ and define the *remainder* $R_n(x)$ by the equation
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\tag{10}\label{10}
\end{equation}
then the Taylor series on the right side of \eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when
\begin{equation}
\lim_{n\to\infty}R_n(x)=0
\end{equation}
Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing
\begin{equation}
R_n(x)=S_n(x)x^{n+1}
\end{equation}
for $x\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for $0\leq t\leq x$ (or $x\leq t\leq 0$) by writing
\begin{multline}
F(t)=f(x)-f(t)-f^{(1)}(t)(x-t)-\dfrac{f^{(2)}(t)}{2!}(x-t)^2-\ldots \\\\ -\dfrac{f^{(n)}(t)}{n!}(x-t)^n-S_n(x)(x-t)^{n+1}
\end{multline}
It is easily seen that $F(x)=0$. Also, from equation \eqref{10}, we have that $F(0)=0$. Then by the *Mean Value Theorem*[^3], $F&apos;(c)=0$ for some constant $c$ between $0$ and $x$.  

By differentiating $F(t)$ w.r.t $t$, and evaluate it at $t=c$, we have
\begin{equation}
F&apos;(c)=-\dfrac{f^{(n+1)}(c)}{n!}(x-c)^n+S_n(x)(n+1)(x-c)^n=0
\end{equation}
so
\begin{equation}
S_n(x)=\dfrac{f^{(n+1)}(c)}{(n+1)!}
\end{equation}
and
\begin{equation}
R_n(x)=S_n(x)x^{n+1}=\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}
\end{equation}
which makes \eqref{10} become
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1},
\end{equation}
where $c$ is some number between $0$ and $x$. This equation is called **Taylor&apos;s formula with derivative remainder**.  

Moreover, with this formula we can rewrite \eqref{9} as
\begin{multline}
f(x)=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots \\\\ +\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1},\tag{11}\label{11}
\end{multline}
where $c$ is some number between $a$ and $x$.  

The polynomial part of \eqref{11}
\begin{multline}
\sum_{j=0}^{n}\dfrac{f^{(j)}(a)}{j!}(x-a)^j=f(a)+f^{(1)}(a)(x-a) \\\\ +\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n
\end{multline}
is called the **nth-degree Taylor polynomial at** $x=a$.  

On the other hand, the remainder part of \eqref{11}
\begin{equation}
R_n(x)=\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1}
\end{equation}
is often called **Lagrange&apos;s remainder formula**.  

**Remark**  
It is worth remarking that power series expansions are *unique*. This means that if a function $f(x)$ can be expressed as a sum of a power series by *any method*, then this series must be the Taylor series of $f(x)$.

## Operations on Power Series
{: #op-power-series}

### Multiplication
{: #mult}
Suppose we are given two power series expansions
\begin{align}
f(x)&amp;=\sum a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+\ldots\tag{12}\label{12} \\\\ g(x)&amp;=\sum b_nx^n=b_0+b_1x+b_2x^2+b_3x^3+\ldots\tag{13}\label{13}
\end{align}
both valid on $(-R,R)$. If we multiply these two series term by term, we obtain the power series
\begin{multline}
a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2 \\\\ +(a_0b_3+a_1b_2+a_2b_1+a_3b_0)x^3+\ldots
\end{multline}
Briefly, we have multiplied \eqref{12} and \eqref{13} to obtain
\begin{equation}
f(x)g(x)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n}a_kb_{n-k}\right)x^n\tag{14}\label{14}
\end{equation}
By the **Theorem 10** from [Absolute vs Conditionally Convergence]({% post_url 2021-09-06-infinite-series-of-constants %}#abs-vs-cond), we have that this product of the series \eqref{12} and \eqref{13} actually converges on the interval $(-R,R)$ to the product of the functions $f(x)$ and $g(x)$, as indicated by \eqref{14}.

### Division
{: #div}
With the two series \eqref{12} and \eqref{13}, we have
\begin{equation}
\dfrac{\sum a_nx^n}{\sum b_nx^n}=\left(\sum a_nx^n\right).\left(\dfrac{1}{\sum b_nx^n}\right)
\end{equation}
This suggests us that if we can expand $\frac{1}{\sum b_nx^n}$ in a power series with positive radius of convergence $\sum c_nx^n$, and multiply this series by $\sum a_nx^n$, we can compute the division of our two series $\sum a_nx^n$ and $\sum b_nx^n$.  

To do the division properly, it is necessary to assume that $b_0\neq0$ (for the case $x=0$). Moreover, without any loss of generality, we may assume that $b_0=1$, because with the assumption that $b_0\neq0$, we simply factor it out
\begin{equation}
\dfrac{1}{b_0+b_1x+b_2x^2+\ldots}=\dfrac{1}{b_0}.\dfrac{1}{1+\frac{b_1}{b_0}x+\frac{b_2}{b_0}x^2+\ldots}
\end{equation}

We begin by determining the $c_n$&apos;s. Since $\frac{1}{\sum b_nx^n}=\sum c_nx^n$, then $(\sum b_nx^n)(\sum c_nx^n)=1$, so
\begin{multline}
b_0c_0+(b_0c_1+b_1c_0)x+(b_0c_2+b_1c_1+b_2c_0)x^2+\ldots \\\\ +(b_0c_n+b_1c_{n-1}+\ldots+b_nc_0)x^n+\ldots=1,
\end{multline}
and since $b_0=1$, we can determine the $c_n$&apos;s recursively
\begin{align}
c_0&amp;=1 \\\\ c_1&amp;=-b_1c_0 \\\\ c_2&amp;=-b_1c_1-b_2c_0 \\\\ &amp;\vdots \\\\ c_n&amp;=-b_1c_{n-1}-b_2c_{n-2}-\ldots-b_nc_0 \\\\ &amp;\vdots
\end{align}
Now our work reduces to proving that the power series $\sum c_nx^n$ with these coefficients has positive radius of convergence, and for this it suffices to show that the series converges for at least one nonzero $x$.  

Let $r$ be any number such that $0\&lt;r\&lt;R$, so that $\sum b_nr^n$ converges. Then there exists a constant $K\geq 1$ with the property that $\vert b_nr^n\vert\leq K$ or $\vert b_n\vert\leq\frac{K}{r^n}$ for all $n$. Therefore,
\begin{align}
\vert c_0\vert&amp;=1\leq K, \\\\ \vert c_1\vert&amp;=\vert b_1c_0\vert=\vert b_1\vert\leq \dfrac{K}{r}, \\\\ \vert c_2\vert&amp;\leq\vert b_1c_1\vert+\vert b_2c_0\vert\leq\dfrac{K}{r}.\dfrac{K}{r}+\dfrac{K}{r^2}.K=\dfrac{2K^2}{r^2}, \\\\ \vert c_3\vert&amp;\leq\vert b_1c_2\vert+\vert b_2c_1\vert+\vert b_3c_0\vert\leq\dfrac{K}{r}.\dfrac{2K^2}{r^2}+\dfrac{K}{r^2}.\dfrac{K}{r}+\dfrac{K}{r^3}.K \\\\ &amp;\hspace{5.3cm}\leq(2+1+1)\dfrac{K^3}{r^3}=\dfrac{4K^3}{r^3}=\dfrac{2^2K^3}{r^3},
\end{align}
since $K^2\leq K^3$ since $K\geq1$. In general,
\begin{align}
\vert c_n\vert&amp;\leq\vert c_1b_{n-1}\vert+\vert c_2b_{n-2}\vert+\ldots+\vert b_nc_0\vert \\\\ &amp;\leq\dfrac{K}{r}.\dfrac{2^{n-2}K^{n-1}}{r^{n-1}}+\dfrac{K}{r^2}.\dfrac{2^{n-3}K^{n-2}}{r^{n-2}}+\ldots+\dfrac{K}{r^n}.K \\\\ &amp;\leq(2^{n-2}+2^{n-3}+\ldots+1+1)\dfrac{K^n}{r^n}=\dfrac{2^{n-1}K^n}{r^n}\leq\dfrac{2^nK^n}{r^n}
\end{align}
Hence, for any $x$ such that $\vert x\vert&lt;\frac{r}{2K}$, we have that the series $\sum c_nx^n$ converges absolutely, and therefore converges, or in other words, $\sum c_nx^n$ has nonzero radius of convergence.

### Substitution
{: #sub}
If a power series
\begin{equation}
f(X)=a_0+a_1x+a_2x^2+\ldots\tag{15}\label{15}
\end{equation}
converges for $\vert x\vert\&lt;R$ and if $\vert g(x)\vert\&lt;R$, then we can find $f(g(x))$ by substituting $g(x)$ for $x$ in \eqref{15}.  

Suppose $g(x)$ is given by a power series,
\begin{equation}
g(x)=b_0+b_1x+b_2x^2+\ldots,\tag{16}\label{16}
\end{equation}
therefore,
\begin{align}
f(g(x))&amp;=a_0+a_1g(x)+a_2g(x)^2+\ldots \\\\ &amp;=a_0+a_1(b+0+b_1x+\ldots)+a_2(b_0+b_1x+\ldots)^2+\ldots
\end{align}
The power series formed in this way converges to $f(g(x))$ whenever \eqref{16} is absolutely convergent and $\vert g(x)\vert\&lt;R$.

### Even and Odd Functions
{: #even-odd-funcs}
A function $f(x)$ defined on $(-R,R)$ is said to be **even** if
\begin{equation}
f(-x)=f(x),
\end{equation}
and **odd** if
\begin{equation}
f(-x)=-f(x)
\end{equation}
Then if $f(x)$ is an even function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n}x^{2n}=a_0+a_2x^2+a_4x^4+\ldots
\end{equation}
and if $f(x)$ is an odd function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n+1}x^{2n+1}=a_1x+a_3x^3+a_5x^5+\ldots
\end{equation}
since if $f(x)=\sum_{n=0}^{\infty}a_nx^n$ is even, then $\sum_{n=0}^{\infty}a_nx^n=\sum_{n=0}^{\infty}(-1)^na_nx^n$, so by the uniqueness of the Taylor series expansion, we have that $a_n=(-1)^na_n$; similarly, $a_n=(-1)^{n+1}a_n$ if $f(x)$ is an odd function.

## Uniform Convergence for Power Series
{: #uni-conv-power-series}
Consider a power series $\sum a_nx^n$ with positive radius of convergence $R$, and let $f(x)$ be its sum.  

In the [section](#dif-int-power-series) above, we stated that $f(x)$ is continuous and differentiable on $(-R,R)$, and we can differentiate and integrate it term by term. So let&apos;s prove these statements!  

Let $S_n(x)$ be the $n$-th partial sum of the series, so that
\begin{equation}
S_n(x)=\sum_{i=0}^{n}a_ix^i=a_0+a_1x+a_2x^2+\ldots+a_nx^n
\end{equation}
Similar to what we did in [Taylor&apos;s formula](#taylors-formula), we write
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
Thus, the remainder
\begin{equation}
R_n(x)=a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots
\end{equation}

For each $x$ in the interval of convergence, we know that $R_n(x)\to0$ as $n\to\infty$; that is, for any given $\epsilon&gt;0$, and for an integer $n_0$ large enough, we have
\begin{equation}
\vert R_n(x)\vert&lt;\epsilon\hspace{1cm}n\geq n_0,\tag{17}\label{17}
\end{equation}
This is true for each $x$ individually, and is an equivalent way of expressing the fact that $\sum a_nx^n$ converges to $f(x)$.  

Moreover, for every $x$ in the given a closed interval $\vert x\vert\leq\vert x_1\vert\&lt;R$, we have
\begin{align}
\vert R_n(x)\vert&amp;=\left\vert a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots\right\vert \\\\ &amp;\leq\left\vert a_{n+1}x^{n+1}\right\vert+\left\vert a_{n+2}x^{n+2}\right\vert+\ldots \\\\ &amp;\leq\left\vert a_{n+1}{x_1}^{n+1}\right\vert+\left\vert a_{n+2}{x_1}^{n+2}\right\vert+\ldots
\end{align}
Because of the [absolute convergence]({% post_url 2021-09-06-infinite-series-of-constants %}#abs-conv) of $\sum a_n{x_1}^n$, the last sum can be made $&lt;\epsilon$ by taking $n$ large enough, $n\geq n_0$. Therefore, we have that \eqref{17} holds for all $x$ inside the closed interval $\vert x\vert\leq\vert x_1\vert$ inside the interval of convergence $(-R,R)$.  

Or in other words, $R_n(x)$ can be made small *independently of $x$ in the given closed interval* $\vert x\vert\leq\vert x_1\vert$, which is equivalent to saying that the series $\sum a_nx^n$ is **uniformly convergent** in this interval[^4].

### Continuity of the Sum
{: #cont-sum}
In order to prove that $f(x)$ is continuous on $(-R,R)$, it suffices to prove that $f(x)$ is continuous at each point $x_0$ in the interval of convergence.  

Consider a closed subinterval $\vert x\vert\leq\vert x_1\vert\&lt;R$ containing $x_0$ in its interior. If $\epsilon&gt;0$ is given, then by uniform convergence we can find an $n$ such that $\vert R_n(x)\vert&lt;\epsilon$ for all $x$&apos;s in the subinterval.  

Since the polynomial $S_n(x)$ is continuous at $x_0$, we can find $\delta&gt;0$ small that $\vert x-x_0\vert&lt;\delta$ implies $x$ lies in the subinterval and $\vert S_n(x)-S_n(x_0)\vert&lt;\epsilon$. Putting these conditions together we find that $\vert x-x_0\vert&lt;\delta$ implies
\begin{align}
\vert f(x)-f(x_0)\vert&amp;=\left\vert S_n(x)+R_n(x)-\left(S_n(x_0)+R_n(x_0)\right)\right\vert \\\\ &amp;=\left\vert\left(S_n(x)-S_n(x_0)\right)+R_n(x)-R_n(x_0)\right\vert \\\\ &amp;\leq\left\vert S_n(x)-S_n(x_0)\right\vert+\left\vert R_n(x)\right\vert+\left\vert R_n(x_0)\right\vert \\\\ &amp;&lt;\epsilon+\epsilon+\epsilon=3\epsilon
\end{align}
which proves the continuity of $f(x)$ at $x_0$.

### Integrating term by term
{: #int}
With what we have just proved that $f(x)=\sum a_nx^n$ is continuous on $(-R,R)$, we can therefore integrate this function between $a$ and $b$ that lie inside the interval
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx
\end{equation}
We need to prove that the right side of this equation can be integrated term by term, which is
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx=\sum\int_{a}^{b}a_nx^n\,dx\tag{18}\label{18}
\end{equation}
In order to prove this, we begin by observing that $S_n(x)$ is a polynomial, and for that reason it is continuous. Thus, all there of the functions in
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
are continuous on $(-R,R)$. This allows us to write
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}S_n(x)\,dx+\int_{a}^{b}R_n(x)\,dx
\end{equation}
Moreover, we can integrate $S_n(x)$ term by term
\begin{align}
\int_{a}^{b}S_n(x)\,dx&amp;=\int_{a}^{b}\left(a_0+a_1x+a_2x^2+\ldots+a_nx^n\right)\,dx \\\\ &amp;=\int_{a}^{b}a_0\,dx+\int_{a}^{b}a_1x\,dx+\int_{a}^{b}a_2x^2\,dx+\ldots+\int_{a}^{b}a_nx^n\,dx
\end{align}
To prove \eqref{18}, it therefore suffices to show that as $n\to\infty$
\begin{equation}
\int_{a}^{b}R_n(x)\,dx\to 0
\end{equation}
By uniform convergence, if $\epsilon&gt;0$ is given and $\vert x\vert\leq\vert x_1\vert\&lt;R$ is a closed subinterval of $(-R,R)$ that contains both $a,b$, then $\vert R_n(x)\vert&lt;\epsilon$ for all $x$ in the subinterval and $n$ large enough. Hence,
\begin{equation}
\left\vert\int_{a}^{b}R_n(x)\,dx\right\vert\leq\int_{a}^{b}\left\vert R_n(x)\right\vert\,dx&lt;\epsilon\vert b-a\vert
\end{equation}
for any $n$ large enough, which proves our statement.  

As a special case of \eqref{18}, we take the limits $0$ and $x$ instead of $a$ and $b$, and obtain
\begin{align}
\int_{a}^{b}f(t)\,dt&amp;=\sum\dfrac{1}{n+1}a_nx^{n+1} \\\\ &amp;=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{19}\label{19}
\end{align}

### Differentiating term by term
{: #dif}
We now prove that the function $f(x)$ is not only continuous but also differentiable on $(-R,R)$, and that its derivative can be calculated by differentiating term by term
\begin{equation}
f&apos;(x)=\sum na_nx^{n-1}
\end{equation}
It is easily seen that the series on right side of this equation is exact the series on the right side of \eqref{3}, which is convergent on $(-R,R)$ as we proved. If we denote its sum by $g(x)$
\begin{equation}
g(x)=\sum na_nx^{n-1}=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots,
\end{equation}
then \eqref{19} tells us that
\begin{align}
\int_{0}^{x}g(t)\,dt&amp;=a_1x+a_2x^2+a_3x^3+\ldots \\\\ &amp;=f(x)-a_0
\end{align}
Since the left side of this has a derivative, so does the right side, and by differentiating we obtain
\begin{equation}
f&apos;(x)=g(x)=\sum na_nx^{n-1}
\end{equation}

## References
[1] George F.Simmons. [Calculus With Analytic Geometry - 2nd Edition](https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424)  

[2] Marian M. [A Concrete Approach to Classical Analysis](https://www.springer.com/gp/book/9780387789323)  

[3] MIT 18.01. [Single Variable Calculus](https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/)  


## Footnotes
[^1]: **Theorem** (*L’Hospital*)  
	*Assume $f$ and $g$ are real and differentiable on $]a,b[$ and $g&apos;(x)\neq 0$ for all $x\in]a,b[$, where $-\infty\leq a&lt;b\leq\infty$. Suppose as $x\to a$,
	\begin{equation}
	\dfrac{f&apos;(x)}{g&apos;(x)}\to A\,(\in[-\infty,\infty])
	\end{equation}
	If as $x\to a$, $f(x)\to 0$ and $g(x)\to 0$ or if $g(x)\to+\infty$ as $x\to a$, then
	\begin{equation}
	\dfrac{f(x)}{g(x)}\to A
	\end{equation}
	as $x\to a$.*  

[^2]: **Proof**  
	Consider the function $f(x)=a^x$.  
	Using the definition of the derivative, we have
	\begin{align}
	\dfrac{d}{dx}f(x)&amp;=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h} \\\\ &amp;=\lim_{h\to 0}\dfrac{a^{x+h}-a^x}{h} \\\\ &amp;=a^x\lim_{h\to 0}\dfrac{a^h-1}{h}
	\end{align}
	Therefore,
	\begin{equation}
	\lim_{h\to 0}\dfrac{a^h-1}{h}=1
	\end{equation}
	then, let $n=\frac{1}{h}$, we have
	\begin{equation}
	a=\lim_{h\to 0}\left(1+\dfrac{1}{h}\right)^{1/h}=\lim_{n\to\infty}\left(1+\dfrac{1}{n}\right)^n={\rm e}
	\end{equation}
	Thus, $f(x)=a^x={\rm e}^x$. Every function $y=c{\rm e}^x$ also satisfies the differential equation $\frac{dy}{dx}=y$, because
	\begin{equation}
	\dfrac{dy}{dx}=\dfrac{d}{dx}c{\rm e}^x=c\dfrac{d}{dx}{\rm e}^x=c{\rm e}^x=y
	\end{equation}  
	The rest of our proof is to prove that these are only functions that are unchanged by differentiation.  
	To prove this, suppose $f(x)$ is any function with that property. By the quotient rule,
	\begin{equation}
	\dfrac{d}{dx}\dfrac{f(x)}{e^x}=\dfrac{f&apos;(x)e^x-e^x f(x)}{e^{2x}}=\dfrac{e^x f(x)-e^x f(x)}{e^{2x}}=0
	\end{equation}
	which implies that
	\begin{equation}
	\dfrac{f(x)}{e^x}=c,
	\end{equation}
	for some constant $c$, and so $f(x)=ce^x$.  

[^3]: **Theorem** (*Mean Value Theorem*)  
	*If a function $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable in the open interval $(a,b)$, then there exists at least one number $c$ between $a$ and $b$ with the property that*
	\begin{equation}
	f&apos;(c)=\frac{f(b)-f(a)}{b-a}
	\end{equation}  

[^4]: We will talk more about uniform convergence in the post of sequences.</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="power-series" /><category term="taylor-series" /><category term="random-stuffs" /><summary type="html">Recall that in the previous post, Infinite Series of Constants, we mentioned a type of series called power series a lot. In the content of this post, we will be diving deeper into details of that series.</summary></entry><entry><title type="html">Infinite Series of Constants</title><link href="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html" rel="alternate" type="text/html" title="Infinite Series of Constants" /><published>2021-09-06T11:20:00+07:00</published><updated>2021-09-06T11:20:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">&gt; No idea what to say yet :D

&lt;!-- excerpt-end --&gt;
- [Infinite Series](#infinite-series)
	- [Examples](#examples)
- [Convergent Sequences](#convergent-sequences)
	- [Sequences](#sequences)
	- [Limits of Sequences](#lim-seq)
- [Convergent and Divergent Series](#conv-div-series)
	- [$n$-th term test](#nth-term-test)
- [General Properties of Convergent Series](#gen-props-conv-series)
- [Series of Nonnegative terms. Comparison tests](#series-nonneg-ct)
	- [Comparison test](#comparison-test)
	- [Limit comparison test](#limit-comparison-test)
- [The Integral test. Euler&apos;s constant](#int-test-euler-c)
	- [Integral test](#integral-test)
	- [Euler&apos;s constant](#euler-c)
- [The Ratio test. Root test](#ratio-root)
	- [Ratio test](#ratio-test)
	- [Root test](#root-test)
	- [The Extended Ratio tests of Raabe and Gauss](#extended-ratio-test)
		- [Kummer&apos;s theorem](#kummers-theorem)
		- [Raabe&apos;s test](#raabes-test)
		- [Gauss&apos;s test](#gausss-test)
- [The Alternating Series test. Absolute Convergence](#alt-test-abs-conv)
	- [Alternating Series](#alt-series)
	- [Alternating Series test](#alt-series-test)
	- [Absolute Convergence](#abs-conv)
- [Absolute vs. Conditionally Convergence](#abs-vs-cond)
- [Dirichlet&apos;s test](#dirichlets-test)
	- [Abel&apos;s partial summation formula](#abel-part-sum)
	- [Dirichlet&apos;s test](#d-test)
- [References](#references)
- [Footnotes](#footnotes)


## Infinite Series
An **infinite series**, or simply a **series**, is an expression of the form
\begin{equation}
a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{\infty}a_n
\end{equation}

### Examples
1. *Infinite decimal*
\begin{equation}
.a_1a_2\ldots a_n\ldots=\dfrac{a_1}{10}+\dfrac{a_2}{10^2}+\ldots+\dfrac{a_n}{10^n}+\ldots,
\end{equation}
where $a_i\in\\{0,1,\dots,9\\}$.  

2. *Power series expansion*[^1]
- Geometric series
\begin{equation}
\dfrac{1}{1-x}=\sum_{n=0}^{\infty}x^n=1+x+x^2+x^3+\dots,\hspace{1cm}\vert x\vert&lt;1
\end{equation}
- Exponential function
\begin{equation}
{\rm e}^x=\sum_{n=0}^{\infty}\dfrac{x^n}{n!}=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots
\end{equation}
- Sine and cosine formulas
\begin{align}
\sin x&amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n+1}}{(2n+1)!}=x-\dfrac{x^3}{3!}+\dfrac{x^5}{5!}-\dfrac{x^7}{7!}+\ldots \\\\ \cos x&amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n}}{(2n)!}=1-\dfrac{x^2}{2!}+\dfrac{x^4}{4!}-\dfrac{x^6}{6!}+\ldots
\end{align}

## Convergent Sequences

### Sequences
If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$&apos;s are said to form a **sequence** (denoted as $\\{x_n\\}$)
\begin{equation}
x_1,x_2,\dots,x_n,\dots
\end{equation}
We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.  

A sequence $\\{x_n\\}$ is said to be *bounded* if there exists $A, B$ such that $A\leq x_n\leq B, \forall n$. $A, B$ respectively are called *lower bound*, *upper bound* of the sequence. A sequence that is not bounded is said to be *unbounded*.

### Limits of Sequences
{: #lim-seq}
A sequence $\\{x_n\\}$ is said to have a number $L$ as **limit** if for each $\epsilon&gt;0$, there exists a positive integer $n_0$ that
\begin{equation}
\vert x_n-L\vert&lt;\epsilon\hspace{1cm}n\geq n_0
\end{equation}
We say that $x_n$ *converges to* $L$ *as* $n$ *approaches infinite* ($x_n\to L$ as $n\to\infty$) and denote this as
\begin{equation}
\lim_{n\to\infty}x_n=L
\end{equation}
- A sequence is said to **converge** or to be **convergent** if it has a limit.  
- A convergent sequence is bounded, but not all bounded sequences are convergent.
- If $x_n\to L,y_n\to M$, then
\begin{align}
&amp;\lim(x_n+y_n)=L+M \\\\ &amp;\lim(x_n-y_n)=L-M \\\\ &amp;\lim x_n y_n=LM \\\\ &amp;\lim\dfrac{x_n}{y_n}=\dfrac{L}{M}\hspace{1cm}M\neq0
\end{align}
- An *increasing* (or *decreasing*) sequence converges if and only if it is bounded.

## Convergent and Divergent Series
{: #conv-div-series}
Recall from the previous sections that if $a_1,a_2,\dots,a_n,\dots$ is a *sequence* of numbers, then
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots\tag{1}\label{1}
\end{equation}
is called an *infinite series*. We begin by establishing the sequence of *partial sums*
\begin{align}
s_1&amp;=a_1 \\\\ s_2&amp;=a_1+a_2 \\\\ &amp;\,\vdots \\\\ s_n&amp;=a_1+a_2+\dots+a_n \\\\ &amp;\,\vdots
\end{align}
The series \eqref{1} is said to be **convergent** if the sequences $\\{s_n\\}$ converges. And if $\lim s_n=s$, then we say that \eqref{1} converges to $s$, or that $s$ is the sum of the series.
\begin{equation}
\sum_{n=1}^{\infty}a_n=s
\end{equation}
If the series does not converge, we say that it **diverges** or is **divergent**, and no sum is assigned to it.

**Examples** (*harmonic series*)  
Let&apos;s consider the convergence of *harmonic series*
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots\tag{2}\label{2}
\end{equation}
Let $m$ be a positive integer and choose $n&gt;2^{m+1}$. We have
\begin{align}
s_n&amp;&gt;1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\dots+\frac{1}{2^{m+1}} \\\\ &amp;=\left(1+\frac{1}{2}\right)+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\ldots+\frac{1}{8}\right)+\ldots+\left(\frac{1}{2^m+1}+\ldots+\frac{1}{2^{m+1}}\right) \\\\ &amp;&gt;\frac{1}{2}+2.\frac{1}{4}+4.\frac{1}{8}+\ldots+2^m.\frac{1}{2^{m+1}} \\\\ &amp;=(m+1)\frac{1}{2}
\end{align}
This proves that $s_n$ can be made larger than the sum of any number of $\frac{1}{2}$&apos;s and therefore as large as we please, by taking $n$ large enough, so the $\\{s_n\\}$ are unbounded, which leads to that \eqref{2} is a divergent series.
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots=\infty
\end{equation}


The simplest general principle that is useful to study the convergence of a series is the **$\mathbf{n}$-th term test**.

### $\mathbf{n}$-th term test
{: #nth-term-test}
If the series $\\{a_n\\}$ converges, then $a_n\to 0$ as $n\to\infty$; or equivalently, if $\neg(a_n\to0)$ as $n\to\infty$, then the series must necessarily diverge.  

**Proof**  
When $\\{a_n\\}$ converges, as $n\to\infty$ we have
\begin{equation}
a_n=s_n-s_{n-1}\to s-s=0
\end{equation}
This result shows that $a_n\to 0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e., it does not imply the convergence of the series when $a_n\to 0$ as $n\to\infty$.

## General Properties of Convergent Series
{: #gen-props-conv-series}
- Any finite number of 0&apos;s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges).
- When two convergent series are added term by term, the resulting series converges to the expected sum; i.e., if $\sum_{n=1}^{\infty}a_n=s$ and $\sum_{n=1}^{\infty}b_n=t$, then
\begin{equation}
\sum_{n=1}^{\infty}(a_n+b_n)=s+t
\end{equation}
	- **Proof**  
	Let $\\{s_n\\}$ and $\\{t_n\\}$ respectively be the sequences of partial sums of $\sum_{n=1}^{\infty}a_n$ and $\sum_{n=1}^{\infty}b_n$. As $n\to\infty$ we have
	\begin{align}
	(a_1+b_1)+(a_2+b_2)+\dots+(a_n+b_n)&amp;=\sum_{i=1}^{n}a_i+\sum_{i=1}^{n}b_i \\\\ &amp;=s_n+t_n\to s+t
	\end{align}
- Similarly, $\sum_{n=1}^{\infty}(a_n-b_n)=s-t$ and $\sum_{n=1}^{\infty}ca_n=cs$ for any constant $c$.
- Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way.
	- **Proof**  
	If $\sum_{n=1}^{\infty}a_n=s$, then
	\begin{equation}
	\lim_{n\to\infty}(a_0+a_1+a_2+\dots+a_n)=\lim_{n\to\infty} a_0+\lim_{n\to\infty}(a_1+a_2+\dots+a_n)=a_0+s
	\end{equation}

## Series of Nonnegative terms. Comparison Tests
{: #series-nonneg-ct}
The easiest infinite series to work with are those whose terms are all nonnegative numbers. The reason, as we saw in the above [section](#conv-div-series), is that if $a_n\geq0$, then the series $\sum a_n$ converges if and only if its sequence $\\{s_n\\}$ of partial sums is bounded (since $s_{n+1}=s_n+a_{n+1}$).  

Thus, in order to establish the convergence of a series of nonnegative terms, it suffices to show that its terms approach zero fast enough, or at least as fast as the terms of a known convergent series of nonnegative terms to keep the partial sums bounded.

### Comparison test
If $0\leq a_n\leq b_n$, then
- $\sum a_n$ converges if $\sum b_n$ converges.
- $\sum b_n$ diverges if $\sum a_n$ diverges.  

**Proof**  
If $s_n, t_n$ respectively are the partial sums of $\sum a_n,\sum b_n$, then
\begin{equation}
0\leq s_n=\sum_{i=1}^{n}a_i\leq\sum_{i=1}^{n}b_i=t_n
\end{equation}
Then if $\\{t_n\\}$ is bounded, then so is $\\{s_n\\}$; and if $\\{s_n\\}$ is unbounded, then so is $\\{t_n\\}$.  

**Example**  
Consider convergence behavior of two series
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{2^n+1};\hspace{2cm}\sum_{n=1}^{\infty}\frac{1}{\ln n}
\end{equation}
The first series converges, because
\begin{equation}
\frac{1}{2^n+1}&lt;\frac{1}{2^n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{2^n}=1$, which is a convergent series. At the same time, the second series diverges, since
\begin{equation}
\frac{1}{n}\leq\frac{1}{\ln n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges.  

One thing worth remarking is that the condition $0\leq a_n\leq b_n$ for the comparison test need not hold for all $n$, but only for all $n$ from some point on.  

The comparison test is simple, but in some cases where it is difficult to establish the necessary inequality between the n-th terms of the two series. And since limits are often easier to work with than inequalities, we have the following test.

### Limit comparison test
If $\sum a_n, \sum b_n$ are series with positive terms such that
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=1\tag{3}\label{3}
\end{equation}
then either both series converge or both series diverge.  

**Proof**  
we observe that \eqref{3} implies that for all sufficient large $n$, we have
\begin{align}
\frac{1}{2}&amp;\leq\frac{a_n}{b_n}\leq 2 \\\\ \text{or}\hspace{1cm}\frac{1}{2}b_n&amp;\leq a_n\leq 2b_n
\end{align}
which leads to the fact that $\sum a_n$ and $\sum b_n$ have the same convergence behavior.  

The condition \eqref{3} can be generalized by
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=L,
\end{equation}
where $0\&lt;L&lt;\infty$.  

**Example** ($p$*-series*)  
Consider the convergence behavior of the series
\begin{equation}
\sum_{n=1}^{\infty}\dfrac{1}{n^p}=1+\dfrac{1}{2^p}+\dfrac{1}{3^p}+\dfrac{1}{4^p}+\ldots,\tag{4}\label{4}
\end{equation}
where $p$ is a positive constant.  

If $p\leq 1$, then $n^p\leq n$ or $\frac{1}{n}\leq\frac{1}{n^p}$. Thus, by comparison with the harmonic series $\sum\frac{1}{n}$, we have that \eqref{4} diverges.   

If $p&gt;1$, let $n$ be given and choose $m$ so that $n&lt;2^m$. Then
\begin{align}
s_n&amp;\leq s_{2^m-1} \\\\ &amp;=1+\left(\dfrac{1}{2^p}+\dfrac{1}{3^p}\right)+\left(\dfrac{1}{4^p}+\ldots+\dfrac{1}{7^p}\right)+\ldots+\left[\dfrac{1}{(2^{m-1})^p}+\ldots+\dfrac{1}{(2^m-1)^p}\right] \\\\ &amp;\leq 1+\dfrac{2}{2^p}+\dfrac{4}{4^p}+\ldots+\dfrac{2^{m-1}}{(2^{m-1})^p}
\end{align}
Let $a=\frac{1}{2^{p-1}}$, then $a&lt;1$ since $p&gt;1$, and
\begin{equation}
s_n\leq 1+a+a^2+\ldots+a^{m-1}=\dfrac{1-a^m}{1-a}&lt;\dfrac{1}{1-a}
\end{equation}
which proves that $\\{s_n\\}$ has an upper bound. Thus \eqref{4} converges.  

**Theorem 1**  
*If a convergent series of nonnegative terms is rearranged in any manner, then the resulting series also converges and has the same sum.*  

**Proof**  
Consider two series $\sum a_n$ and $\sum b_n$, where $\sum a_n$ is a convergent series of nonnegative terms and $\sum b_n$ is formed form $\sum a_n$ by rearranging its terms.  

Let $p$ be a positive integer and consider the $p$-partial sum $t_p=b_1+\ldots+b_p$ of $\sum b_n$. Since each $b$ is some $a$, then there exists an $m$ such that each term in $t_p$ is one of the terms in $s_m=a_1+\ldots+a_m$. This shows us that $t_p\leq s_m\leq s$. Thus, $\sum b_n$ converges to a sum $t\leq s$.  

On the other hand, $\sum a_n$ is also a rearrangement of $\sum b_n$, so by the same procedure, similarly we have that $s\leq t$, and therefore $t=s$.


## The Integral test. Euler&apos;s constant
{: #int-test-euler-c}
In this section, we will be going through a more detailed class of infinite series with nonnegative terms which is those whose terms form a decreasing sequence of positive numbers.  

We begin by considering a series
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots
\end{equation}
whose terms are positive and decreasing. Suppose $a_n=f(n)$, as shown is ***Figure 1***.
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/integral-test.png&quot; alt=&quot;integral test&quot; width=&quot;500px&quot; height=&quot;230px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot;/&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

On the left of this figure we see that the rectangles of areas $a_1,a_2,\dots,a_n$ have a greater combined area than the area under the curve from $x=1$ to $x=n+1$, so
\begin{equation}
a_1+a_2+\dots+a_n\geq\int_{1}^{n+1}f(x)\,dx\geq\int_{1}^{n}f(x)\,dx\tag{5}\label{5}
\end{equation}
On the right side of the figure, the rectangles lie under the curve, which makes
\begin{align}
a_2+a_3+\dots+a_n&amp;\leq\int_{1}^{n}f(x)\,dx \\\\ a_1+a_2+\dots+a_n&amp;\leq a_1+\int_{1}^{n}f(x)\,dx\tag{6}\label{6}
\end{align}
Putting \eqref{5} and \eqref{6} together we have
\begin{equation}
\int_{1}^{n}f(x)\,dx\leq a_1+a_2+\dots+a_n\leq a_1+\int_{1}^{n}f(x)\,dx\tag{7}\label{7}
\end{equation}
The result we obtained in \eqref{7} allows us to establish the **integral test**.

### Integral test

If $f(x)$ is a positive decreasing function for $x\geq1$ such that $f(n)=a_n$ for each positive integer $n$, then the series and integral
\begin{equation}
\sum_{n=1}^{\infty}a_n;\hspace{2cm}\int_{1}^{\infty}f(x)\,dx
\end{equation}
converge or diverge together.  

The integral test holds for any interval of the form $x\geq k$, not just for $x\geq 1$.  

**Example** (*Abel&apos;s series*)  
Let&apos;s consider the convergence behavior of the series
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n\ln n}\tag{8}\label{8}
\end{equation}
By the integral test, we have that \eqref{8} diverges, because
\begin{equation}
\sum_{2}^{\infty}\frac{dx}{x\ln x}=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x\ln x}=\lim_{b\to\infty}\left(\ln\ln x\Big|\_{2}^{b}\right)=\lim_{b\to\infty}\left(\ln\ln b-\ln\ln 2\right)=\infty
\end{equation}
More generally, if $p&gt;0$, then
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n(\ln n)^p}
\end{equation}
converges if $p&gt;1$ and diverges if $0\&lt;p\leq 1$. For if $p\neq 1$, we have
\begin{align}
\int_{2}^{\infty}\frac{dx}{x(\ln x)^p}&amp;=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x(\ln x)^p} \\\\ &amp;=\lim_{b\to\infty}\left[\dfrac{(\ln x)^{1-p}}{1-p}\Bigg|\_2^b\right] \\\\ &amp;=\lim_{b\to\infty}\left[\dfrac{(\ln b)^{1-p}-(\ln 2)^{1-p}}{1-p}\right]
\end{align}
exists if and only if $p&gt;1$.

### Euler&apos;s constant
{: #euler-c}
From \eqref{7} we have that
\begin{equation}
0\leq a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\leq a_1
\end{equation}
Denoting $F(n)=a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx$, the above expression becomes
\begin{equation}
0\leq F(n)\leq a_1
\end{equation}
Moreover, $\\{F(n)\\}$ is a decreasing sequence, because
\begin{align}
F(n)-F(n+1)&amp;=\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]-\left[a_1+a_2+\ldots+a_{n+1}-\int_{1}^{n+1}f(x)\,dx\right] \\\\ &amp;=\int_{n}^{n+1}f(x)\,dx-a_{n+1}\geq 0
\end{align}
where the last step can be seen by observing the right side of ***Figure 1***.  

Since any decreasing sequence of nonnegative numbers converges, we have that
\begin{equation}
L=\lim_{n\to\infty}F(n)=\lim_{n\to\infty}\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]\tag{9}\label{9}
\end{equation}
exists and satisfies the inequalities $0\leq L\leq a_1$.  

Let $a_n=\frac{1}{n}$ and $f(x)=\frac{1}{x}$, the last quantity in \eqref{9} becomes
\begin{equation}
\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)\tag{10}\label{10}
\end{equation}
since
\begin{equation}
\int_{1}^{n}\dfrac{dx}{x}=\ln x\Big|\_1^n=\ln n
\end{equation}
The value of the limit \eqref{10} is called **Euler&apos;s constant** (denoted as $\gamma$).
\begin{equation}
\gamma=\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)
\end{equation}

## The Ratio test. Root test
{: #ratio-root}

### Ratio test
If $\sum a_n$ is a series of positive terms such that
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}}{a_n}=L,\tag{11}\label{11}
\end{equation}
then
1. if $L&lt;1$, the series *converges*.
2. if $L&gt;1$, the series *diverges*.
3. if $L=1$, the test is *inconclusive*.

**Proof**  
1. Let $L&lt;1$ and choose any number $r$ such that $L\&lt;r&lt;1$. From \eqref{11}, we have that there exists an $n_0$ such that
\begin{align}
\dfrac{a_{n+1}}{a_n}&amp;\leq r=\dfrac{r^{n+1}}{r_n},\hspace{1cm}\forall n\geq n_0 \\\\ \dfrac{a_{n+1}}{r^{n+1}}&amp;\leq\dfrac{a_n}{r^n},\hspace{2cm}\forall n\geq n_0
\end{align}
which means that $\\{\frac{a_n}{r^n}\\}$ is a decreasing sequence for $n\geq n_0$; in particular, $\frac{a_n}{r^n}\leq\frac{a_{n_0}}{r^{n_0}}$ for $n\geq n_0$. Thus, if we let $K=\frac{a_{n_0}}{r^{n_0}}$, then we get
\begin{equation}
a_n\leq Kr^n,\hspace{1cm}\forall n\geq n_0\tag{12}\label{12}
\end{equation}
However, $\sum Kr^n$ converges since $r&lt;1$. Hence, by the [comparison test](#comparison-test), \eqref{12} implies that $\sum a_n$ converges.  

2. When $L&gt;1$, we have that $\frac{a_{n+1}}{a_n}\geq 1$, or equivalently $a_{n+1}\geq a_n$, for all $n\geq n_0$, for some constant $n_0$. That means $\neg(a_n\to 0)$ as $n\to\infty$ (since $\sum a_n$ is a series of positive terms).  
By the [$n$-th term test](#nth-term-test), we know that the series diverges.  

3. Consider the $p$-series $\sum\frac{1}{n^p}$. For all values of $p$, as $n\to\infty$ we have
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^p}{(n+1)^p}=\left(\dfrac{n}{n+1}\right)^p\to 1
\end{equation}
As in the above example, we have that this series converges if $p&gt;1$ and diverges if $p\leq 1$.

### Root test
If $\sum a_n$ is a series of nonnegative terms such that
\begin{equation}
\lim_{n\to\infty}\sqrt[n]{a_n}=L,\tag{13}\label{13}
\end{equation}
then
1. if $L&lt;1$, the series *converges*.
2. if $L&gt;1$, the series *diverges*.
3. if $L=1$, the test is *inconclusive*.

**Proof**
1. Let $L&lt;1$ and $r$ is any number such that $L\&lt;r&lt;1$. From \eqref{13}, we have that there exist $n_0$ such that
\begin{align}
\sqrt[n]{a_n}&amp;\leq r&lt;1,\hspace{1cm}\forall n\geq n_0 \\\\ a_n&amp;\leq r^n&gt;1,\hspace{1cm}\forall n\geq n_0
\end{align}
And since the geometric series $\sum r^n$ converges, we clearly have that $\sum a_n$ also converges.  

2. If $L&gt;1$, then $\sqrt[n]{a_n}\geq 1$ for all $n\geq n_0$, for some $n_0$, so $a_n\geq 1$ for all $n\geq n_0$. That means as $n\to\infty$, $\neg(a_n\to 0)$. Therefore, by the [$n$-th term test](#nth-term-test), we have that the series diverges.  

3. For $L=1$, we provide 2 examples. One is the divergent series $\sum\frac{1}{n}$ and the other is the convergent series $\sum\frac{1}{n^2}$ (since $\sqrt[n]{n}\to 1$ as $n\to\infty$).

### The Extended Ratio tests of Raabe and Gauss
{: #extended-ratio-test}

#### Kummer&apos;s theorem
{: #kummers-theorem}

**Theorem 2** (*Kummer&apos;s*)  
*Assume that $a_n&gt;0,b_n&gt;0$ and $\sum\frac{1}{b_n}$ diverges. If
\begin{equation}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)=L,\tag{14}\label{14}
\end{equation}
then $\sum a_n$ converges if $L&gt;0$ and diverges if $L&lt;0$.*  

**Proof**  
- If $L&gt;0$, then there exists $h$ such that $L&gt;h&gt;0$. From \eqref{14}, for some positive integer $n_0$ we have
\begin{align}
b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}&amp;\geq h&gt;0,\hspace{1cm}\forall n\geq n_0 \\\\ a_n b_n-a_{n+1}b_{n+1}&amp;\geq ha_n&gt;0,\hspace{1cm}\forall n\geq n_0\tag{15}\label{15}
\end{align}
Hence, $\\{a_n b_n\\}$ is a decreasing sequence of positive numbers for $n\geq n_0$, so $K=\lim a_n b_n$ exists.  
Moreover, we have that
\begin{equation}
\sum_{n=n_0}^{\infty}a_nb_n-a_{n+1}b_{n+1}=a_{n_0}b_{n_0}-\lim_{n\to\infty}a_nb_n=a_{n_0}b_{n_0}-K
\end{equation}
Therefore, by \eqref{15} and the [comparison test](#comparison-test), we can conclude that $\sum ha_n$ converges, which means that $\sum a_n$ also converges.  

- If $L&lt;0$, for some positive integer $n_0$ we have
\begin{equation}
a_nb_n-a_{n+1}b_{n+1}\leq 0,\hspace{1cm}\forall n\geq n_0
\end{equation}
Hence, $\\{a_nb_n\\}$ is a increasing sequence of positive number for all $n\geq n_0$, for some positive integer $n_0$. This also means for all $n\geq n_0$,
\begin{align}
a_nb_n&amp;\geq a_{n_0}b_{n_0} \\\\ a_n&amp;\geq (a_{n_0}b_{n_0}).\dfrac{1}{b_n}
\end{align}
Therefore $\sum a_n$ diverges (since $\sum\frac{1}{b_n}$ diverges).  

#### Raabe&apos;s test
{: #raabes-test}

**Theorem 3** (*Raabe&apos;s test*)  
*If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n},
\end{equation}
where $A_n\to 0$, then $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.*  

**Proof**  
Take $n=b_n$ in *Kummber&apos;s theorem*. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;=\lim\left[n-\left(1-\dfrac{A}{n}+\dfrac{A_n}{n}\right)(n+1)\right] \\\\ &amp;=\lim\left[-1+\dfrac{A(n+1)}{n}-\dfrac{A_n(n+1)}{n}\right] \\\\ &amp;=A-1
\end{align}
and by *Kummer&apos;s theorem* we have that $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.  

*Raabe&apos;s test* can be formulated as followed: If $a_n&gt;0$ and
\begin{equation}
\lim n\left(1-\dfrac{a_{n+1}}{a_n}\right)=A,
\end{equation}
then $\sum a_n$ converges if $A&gt;1$ and diverges if $A&lt;1$.  

When $A=1$ in *Raabe&apos;s test*, we turn to **Gauss&apos;s test**

#### Gauss&apos;s test
{: #gausss-test}

**Theorem 4**  
*If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n^{1+c}},
\end{equation}
where $c&gt;0$ and $A_n$ is bounded as $n\to\infty$, then $\sum a_n$ converges if $A&gt;1$ and diverges if $A\leq 1$.*  

**Proof**  
- If $A\neq 1$, the statement follows exactly from *Raabe&apos;s test*, since $\frac{A_n}{n^c}\to 0$ as $n\to\infty$.  

- If $A=1$, we begin by taking $b_n=n\ln n$ in *Kummer&apos;s theorem*. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;=\lim\left[n\ln n-\left(1-\dfrac{1}{n}+\dfrac{A_n}{n^{1+c}}\right)(n+1)\ln(n+1)\right] \\\\ &amp;=\lim\left[n\ln n-\dfrac{n^2-1}{n}\ln(n+1)-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\\\ &amp;=\lim\left[n\ln\left(\dfrac{n}{n+1}\right)+\dfrac{\ln(n+1)}{n}-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\\\ &amp;=-1+0-0=-1&lt;0,
\end{align}
where in fourth step we use the *Stolz–Cesàro theorem*[^2]. Therefore, by *Kummer&apos;s theorem*, we have that the series is divergent.  

**Theorem 5** (*Gauss&apos;s test*)  
*If $a_n&gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^k+\alpha n^{k-1}+\ldots}{n^k+\beta n^{k-1}+\ldots},\tag{16}\label{16}
\end{equation}
then $\sum a_n$ converges if $\beta-\alpha&gt;1$ and diverges if $\beta-\alpha\leq 1$.*  

**Proof**  
If the quotient on the right of \eqref{16} is worked out by long division, we get
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{\beta-\alpha}{n}+\dfrac{A_n}{n^2},
\end{equation}
where $A_n$ is a quotient of the form
\begin{equation}
\dfrac{\gamma n^{k-2}+\ldots}{n^{k-2}+\ldots}
\end{equation}
and is therefore clearly bounded as $n\to\infty$. The statement now follows from **Theorem 4** with $c=1$.

## The Alternating Series test. Absolute Convergence
{: #alt-test-abs-conv}
Previously, we have been working with series of positive terms and nonnegative terms. It&apos;s time to consider series with both positive and negative terms. The simplest are those whose terms are alternatively positive and negative.

### Alternating Series
{: #alt-series}
**Alternating series** is series with the form
\begin{equation}
\sum_{n=1}^{\infty}(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+\ldots,\tag{17}\label{17}
\end{equation}
where $a_n$&apos;s are all positive numbers.  

From the definition of alternating series, we establish **alternating series test**.

### Alternating Series test
{: #alt-series-test}
If the alternating series \eqref{17} has the property that
1. $a_1\geq a_2\geq a_3\geq\ldots$
2. $a_n\to 0$ as $n\to\infty$  

then $\sum a_n$ converges.  

**Proof**  
On the one hand, we have that a typical even partial sum $s_{2n}$ can be written as
\begin{equation}
s_{2n}=(a_1-a_2)+(a_3-a_4)+\ldots+(a_{2n-1}-a_{2n}),
\end{equation}
where each expression in parentheses is nonnegative since $\\{a_n\\}$ is a decreasing sequence. Hence, we also have that $s_{2n}\leq s_{2n+2}$, which leads to the result that the even partial sums form an increasing sequence.  

Moreover, we can also display $s_{2n}$ as
\begin{equation}
s_{2n}=a_1-(a_2-a_3)-(a_4-a_5)-\ldots-(a_{2n-2}-a_{2n-1})-a_{2n},
\end{equation}
where each expression in parentheses once again is nonnegative. Thus, we have that $s_{2n}\leq a_1$, so ${s_{2n}}$ has an upper bound. Since every bounded increasing sequence converges, there exists a number $s$ such that
\begin{equation}
\lim_{n\to\infty}s_{2n}=s
\end{equation}

On the other hand, the odd partial sums approach the same limit, because
\begin{align}
s_{2n+1}&amp;=a_1-a_2+a_3-a_4+\ldots-a_{2n}+a_{2n+1} \\\\ &amp;=s_{2n}+a_{2n+1}
\end{align}
and therefore
\begin{equation}
\lim_{n\to\infty}s_{2n+1}=\lim_{n\to\infty}s_{2n}+\lim_{n\to\infty}a_{2n+1}=s+0=s
\end{equation}
Since both sequence of even sums and sequence of odd partial sums converges to $s$ as $n$ tends to infinity, this shows us that $\\{s_n\\}$ also converges to $s$, and therefore the alternating series \eqref{17} converges to the sum $s$.

### Absolute Convergence
{: #abs-conv}
A series $\sum a_n$ is said to be **absolutely convergent** if $\sum\vert a_n\vert$ converges.  

These are some properties of absolute convergence.
1. Absolute convergence implies convergence.
- **Proof**  
Suppose that $\sum a_n$ is an absolutely convergent series, or $\sum\vert a_n\vert$ converges. We have that
\begin{equation}
0\leq a_n+\vert a_n\vert\leq 2\vert a_n\vert
\end{equation}
And since $\sum 2\vert a_n\vert$ converges, by [comparison test](#comparison-test), we also have that $\sum(a_n+\vert a_n\vert)$ converges.  
Since both $\sum\vert a_n\vert$ and $\sum(a_n+\vert a_n\vert)$ converge, so does their difference, which is $\sum a_n$.  
&lt;br/&gt;

2. A convergent series that is not absolutely convergent is said to be **conditionally convergent**.
- Any conditionally convergent series can be made to converge to any given number as its sum, or even to diverge, by *suitably changing the order of its terms without changing the terms themselves* (check out **Theorem 8** to see the proof).
- On the other hand, any absolutely convergent series can be rearranged in any manner without changing its convergence behavior or its sum (check out **Theorem 7** to see the proof).

## Absolute vs Conditionally Convergence
{: #abs-vs-cond}
**Theorem 6**  
*Consider a series $\sum a_n$ and define $p_n$ and $q_n$ by
\begin{align}
p_n&amp;=\dfrac{\vert a_n\vert+a_n}{2} \\\\ q_n&amp;=\dfrac{\vert a_n\vert-a_n}{2}
\end{align}
If $\sum a_n$ converges conditionally, then both $\sum p_n$ and $\sum q_n$ diverges.  
If $\sum a_n$ converges absolutely, then $\sum p_n$ and $\sum q_n$ both converge and the sums of these series are related by the equation*
\begin{equation}
\sum a_n=\sum p_n-\sum q_n
\end{equation}

**Proof**  
From the formulas of $p_n$ and $q_n$, we have
\begin{align}
a_n&amp;=p_n-q_n\tag{18}\label{18} \\\\ \vert a_n\vert&amp;=p_n+q_n\tag{19}\label{19}
\end{align}
- We begin by proving the first statement.  
When $\sum a_n$ converges, from \eqref{18}, we have $\sum p_n$ and $\sum q_n$ both must have the same convergence behavior (i.e., converge or diverge at the same time).  
If they both converge, then from \eqref{19}, we have that $\sum\vert a_n\vert$ converges, contrary to the hypothesis, so $\sum p_n$ and $\sum q_n$ are both divergent.  

- To prove the second statement, we assume that $\sum\vert a_n\vert$ converges. We have
\begin{equation}
p_n=\dfrac{\vert a_n\vert+a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which shows us that $\sum p_n$ converges. Similarly, for $q_n$, we have
\begin{equation}
q_n=\dfrac{\vert a_n\vert-a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which also lets us obtain that $\sum q_n$ converges.  
Therefore
\begin{equation}
\sum p_n-\sum q_n=\sum(p_n-q_n)=\sum a_n
\end{equation}
&lt;br/&gt;

**Theorem 7**  
*If $\sum a_n$ is an absolutely convergent series with sum $s$, and if $a_n$&apos;s are rearranged in any way to from a new series $\sum b_n$, then this new series is also absolutely convergent with sum $s$.*  

**Proof**  
Since $\sum\vert a_n\vert$ is a convergent series of nonnegative terms with sum $s$ and since the $b_n$&apos;s are just the $a_n$&apos;s in a different order, it follows from **Theorem 1** that $\sum\vert b_n\vert$ also converges to $s$, and therefore $\sum b_n$ is absolutely convergent with sum $t$, for some positive $t$.  

**Theorem 6** allows us to write
\begin{equation}
s=\sum a_n=\sum p_n-\sum q_n
\end{equation}
and
\begin{equation}
t=\sum b_n=\sum P_n-\sum Q_n
\end{equation}
where each of the series on the right is convergent and consists of nonnegative. But the $P_n$&apos;s and $Q_n$&apos;s are simply the $p_n$&apos;s and $q_n$&apos;s in a different order. Hence, by **Theorem 1**, we have $\sum P_n=\sum p_n$ and $\sum Q_n=\sum q_n$. And therefore, $t=s$.  
&lt;br/&gt;

**Theorem 8** (*Riemann&apos;s rearrangement theorem*)  
*Let $\sum a_n$ be a conditionally convergent series. Then its terms can be rearranged to yield a convergent series whose sum is an arbitrary preassigned number, or a series that diverges to $\infty$, or a series that diverges to $-\infty$.*

**Proof**  
Since $\sum a_n$ converges conditionally, we begin by using **Theorem 6** to form the two divergent series of nonnegative terms $\sum p_n$ and $\sum q_n$.
- To prove the first statement, let $s$ be any number and construct a rearrangement of the given series as follows. Start by writing down $p$&apos;s in order until the partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}
\end{equation}
is first $\geq s$; next we continue with $-q$&apos;s until the total partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}-q_1-q_2-\ldots-q_{m_1}
\end{equation}
is first $\leq s$; then we continue with $p$&apos;s until the total partial sum
\begin{equation}
p_1+\ldots+p_{n_1}-q_1-\ldots-q_{m_1}+p_{n_1+1}+\ldots+p_{n_2}
\end{equation}
is first $\geq s$; and so on.  
The possibility of each of these steps is guaranteed by the divergence of $\sum p_n$ and $\sum q_n$; and the resulting rearrangement of $\sum a_n$ converges to $s$ because $p_n\to 0$ and $q_n\to 0$.  

- In order to make the rearrangement diverge to $\infty$, it suffices to write down enough $p$&apos;s to yield
\begin{equation}
p_1+p_2+\ldots+p_{n_1}\geq 1,
\end{equation}
then to insert $-q_1$, and then to continue with $p$&apos;s until
\begin{equation}
p_1+\ldots+p_{n_1}-q_1+p_{n_1+1}+\ldots+p_{n_2}\geq 2,
\end{equation}
then to insert $-q_2$, and so on.  
We can produce divergence to $-\infty$ by a similar construction.  

One of the principal application of **Theorem 7** relates to the *multiplication of series*.  

If we multiply two series
\begin{align}
\sum_{n=0}^{\infty}a_n&amp;=a_0+a_1+\ldots+a_n+\ldots\tag{20}\label{20} \\\\ \sum_{n=0}^{\infty}b_n&amp;=b_0+b_1+\ldots+b_n+\ldots\tag{21}\label{21}
\end{align}
by forming all possible product $a_i b_j$ (as in the case of finite sums), then we obtain the following doubly infinite array
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/series-mult.png&quot; alt=&quot;series multiplication&quot; width=&quot;300px&quot; height=&quot;210px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot;/&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

There are various ways of arranging these products into a single infinite series, of which two are important. The first one is to group them by diagonals, as indicated in the arrows in **Figure 2**:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1)+(a_0b_2+a_1b_1+a_2b_0)+\ldots\tag{22}\label{22}
\end{equation}
This series can be defined as $\sum_{n=0}^{\infty}c_n$, where
\begin{equation}
c_n=a_0b_n+a_1b_{n-1}+\ldots+a_nb_0
\end{equation}

It is called the *product* (or *Cauchy product*) of the two series $\sum a_n$ and $\sum b_n$.  

The second crucial method of arranging these products into a series is by squares, as shown in **Figure 2**:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1+a_1b_0)+(a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0)+\ldots\tag{23}\label{23}
\end{equation}
The advantage of this arrangement is that the $n$-th partial sum $s_n$ of \eqref{23} is given by
\begin{equation}
s_n=(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n)\tag{24}\label{24}
\end{equation}
&lt;br/&gt;

**Theorem 9**  
*If the two series \eqref{20} and \eqref{21} have nonnegative terms and converges to $s$ and $t$, then their product \eqref{22} converges to $st$.*  

**Proof**  
It is clear from \eqref{24} that \eqref{23} converges to $st$. Let&apos;s denote the series \eqref{22} and \eqref{23} without parenthesis by $(22&apos;)$ and $(23&apos;)$.  

We have the series $(23&apos;)$ of nonnegative terms still converges to $st$ because, for if $m$ is an integer such that $n^2\leq m\leq (n+1)^2$, then the $m$-th partial sum of $(23&apos;)$ lies between $s_{n-1}$ and $s_n$, and both of these converge to $st$.  

By **Theorem 7**, the terms of $(23&apos;)$ can be rearranged to yield $(22&apos;)$ without changing the sum $st$; and when parentheses are suitably inserted, we see that \eqref{8} converges to $st$.  

We now extend **Theorem 9** to the case of absolute convergence.  
&lt;br/&gt;

**Theorem 10**  
*If the series $\sum_{n=0}^{\infty}a_n$ and $\sum_{n=0}^{\infty}b_n$ are absolutely convergent, with sum $s$ and $t$, then their product
\begin{multline}
\sum_{n=0}^{\infty}(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)=a_0b_0+(a_0b_1+a_1b_0)\,+ \\\\ (a_0b_2+a_1b_1+a_2b_0)+\ldots+(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)+\ldots\tag{25}\label{25}
\end{multline}
is absolutely convergent, with sum $st$.*  

**Proof**  
The series $\sum_{n=0}^{\infty}\vert a_n\vert$ and $\sum_{n=0}^{\infty}\vert b_n\vert$ are convergent and have nonnegative terms. So by the **Theorem 9** above, their product
\begin{multline}
\vert a_0\vert\vert b_0\vert+\vert a_0\vert\vert b_1\vert+\vert a_1\vert\vert b_0\vert+\ldots+\vert a_0\vert\vert b_n\vert+\vert a_1\vert\vert b_{n-1}\vert+\ldots+\vert a_n\vert\vert b_0\vert+\ldots \\\\ =\vert a_0b_0\vert+\vert a_0b_1\vert+\vert a_1b_0\vert+\ldots+\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert+\ldots\tag{26}\label{26}
\end{multline}
converges, and therefore the series
\begin{equation}
a_0b_0+a_0b_1+a_1b_0+\ldots+a_0b_n+\ldots+a_nb_0+\ldots\tag{27}\label{27}
\end{equation}
is absolutely convergent. It follows from **Theorem 7** that the sum of \eqref{27} will not change if we rearrange its terms and write it as
\begin{equation}
a_0b_0+a_0b_1+a_1b_1+a_1b_0+a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0+\ldots\tag{28}\label{28}
\end{equation}
We now observe that the sum of the first $(n+1)^2$ terms of \eqref{28} is
\begin{equation}
(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n),
\end{equation}
so it is clear that \eqref{28}, and with it \eqref{27}, converges to $st$.  

Thus, \eqref{25} also converges to $st$, since \eqref{25} is retrieved by suitably inserted parentheses in \eqref{27}.  

Moreover, we also have
\begin{equation}
\vert a_0b_n+a_1b_{n-1}+\ldots+a_nb_0\vert\leq\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert
\end{equation}
and the series
\begin{equation}
\vert a_0b_0\vert+(\vert a_0b_1\vert+\vert a_1b_0\vert)+\ldots+(\vert a_0b_n\vert+\ldots+\vert a_nb_0\vert)+\ldots
\end{equation}
obtained from \eqref{26} by inserting parentheses. By the [comparison test](#comparison-test), \eqref{25} converges absolutely.  

Hence, we can conclude that \eqref{25} is absolutely convergent, with sum $st$.  
&lt;br/&gt;

We have already gone through convergence tests applied only to series of positive (or nonnegative) terms. Let&apos;s end this lengthy post with the alternating series test. ^^!

## Dirichlet&apos;s test
{: #dirichlets-test}

### Abel&apos;s partial summation formula
{: #abel-part-sum}
Consider series $\sum_{n=1}^{\infty}a_n$, sequence $\\{b_n\\}$. If $s_n=a_1+a_2+\ldots+a_n$, then
\begin{equation}
a_1b_1+a_2b_2+\ldots+a_nb_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots+s_{n-1}(b_{n-1}-b_n)+s_nb_n\tag{29}\label{29}
\end{equation}

**Proof**  
Since $a_1=s_1$ and $a_n=s_n-s_{n-1}$ for $n&gt;1$, we have
\begin{align}
a_1b_1&amp;=s_1b_1 \\\\ a_2b_2&amp;=s_2b_2-s_1b_2 \\\\ a_3b_3&amp;=s_3b_3-s_2b_3 \\\\ &amp;\vdots \\\\ a_nb_n&amp;=s_nb_n-s_{n-1}b_n
\end{align}
On adding these equations, and grouping suitably, we obtain \eqref{29}.

### Dirichlet&apos;s test
{: #d-test}
*If the series $\sum_{n=1}^{\infty}a_n$ has bounded partial sums, and if $\\{b_n\\}$ is a decreasing sequence of positive numbers such that $b_n\to 0$, then the series
\begin{equation}
\sum_{n=1}^{\infty}a_nb_n=a_1b_1+a_2b_2+\ldots+a_nb_n+\ldots\tag{30}\label{30}
\end{equation}
converges*.  

**Proof**  
Let $S_n=a_1b_1+a_2b_2+\ldots+a_nb_n$ denote the $n$-th partial sum of \eqref{30}, then \eqref{29} tells us that
\begin{equation}
S_n=T_n+s_nb_n,
\end{equation}
where
\begin{equation}
T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots
\end{equation}
Since ${s_n}$ is bounded there exists a positive constant $m$ such that $\vert s_n\vert\leq m,\forall n$, so $\vert s_nb_n\vert\leq mb_n$. And since $b_n\to 0$, we have that $s_nb_n\to 0$ as $n\to\infty$.  

Moreover, since $\\{b_n\\}$ is a decreasing sequence of positive numbers, we have that
\begin{equation}
\begin{aligned}
\vert s_1(b_1-b_2)\vert+\vert s_2(b_3-b_3)\vert+\ldots&amp;\,+\vert s_{n-1}(b_{n-1}-b_n)\vert \\\\ &amp;\leq m(b_1-b_2)+m(b_2-b_3)+\ldots+m(b_{n-1}-b_n) \\\\ &amp;=m(b_1-b_n)\leq mb_1
\end{aligned}
\end{equation}
which implies that $T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots$ converges absolutely, and thus, it converges to a sum $t$. Therefore
\begin{equation}
\lim_{n\to\infty}S_n=\lim_{n\to\infty}T_n+s_nb_n=\lim_{n\to\infty}T_n+\lim_{n\to\infty}s_nb_n=t+0=t
\end{equation}
which lets us conclude that the series \eqref{30} converges.

## References
[1] George F.Simmons. [Calculus With Analytic Geometry - 2nd Edition](https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424)  

[2] Marian M. [A Concrete Approach to Classical Analysis](https://www.springer.com/gp/book/9780387789323)  

[3] MIT 18.01. [Single Variable Calculus](https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/)  

## Footnotes
[^1]: We will be going through power series in more detailed in another [post]({% post_url 2021-09-21-power-series %}).
[^2]: **Theorem** (*Stolz–Cesaro*)  
	*Let $\\{a_n\\}$ be a sequence of real numbers and $\\{b_n\\}$ be a strictly monotone and divergent sequence. Then
	\begin{equation}
	\lim_{n\to\infty}\dfrac{a_{n+1}-a_n}{b_{n+1}-b_n}=L\hspace{1cm}(\in\left[-\infty,+\infty\right])
	\end{equation}
	implies
	\begin{equation}
	\lim_{n\to\infty}\dfrac{a_n}{b_n}=L
	\end{equation}*</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="random-stuffs" /><summary type="html">No idea what to say yet :D</summary></entry></feed>