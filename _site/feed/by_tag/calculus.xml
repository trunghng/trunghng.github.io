<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed/by_tag/calculus.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-10T19:38:17+07:00</updated><id>http://localhost:4000/feed/by_tag/calculus.xml</id><title type="html">Trung’s cabin</title><subtitle>To document something I&apos;ve learned
</subtitle><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><entry><title type="html">Power Series</title><link href="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html" rel="alternate" type="text/html" title="Power Series" /><published>2021-09-21T15:40:00+07:00</published><updated>2021-09-21T15:40:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/21/power-series</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/21/power-series.html">&lt;blockquote&gt;
  &lt;p&gt;Recall that in the previous post, &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html&quot;&gt;Infinite Series of Constants&lt;/a&gt;, we mentioned a type of series called &lt;strong&gt;power series&lt;/strong&gt; a lot. In the content of this post, we will be diving deeper into details of that series.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#power-series&quot;&gt;Power Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-conv&quot;&gt;The Interval of Convergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#eg1&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dif-int-power-series&quot;&gt;Differentiation and Integration of Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dif-power-series&quot;&gt;Differentiation of Power Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#int-power-series&quot;&gt;Integration of Power Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eg2&quot;&gt;Example&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#taylor-series-formula&quot;&gt;Taylor Series, Taylor’s Formula&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#taylor-series&quot;&gt;Taylor Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#taylors-formula&quot;&gt;Taylor’s Formula&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#op-power-series&quot;&gt;Operations on Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mult&quot;&gt;Multiplication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#div&quot;&gt;Division&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sub&quot;&gt;Substitution&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#even-odd-funcs&quot;&gt;Even and Odd Functions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uni-conv-power-series&quot;&gt;Uniform Convergence for Power Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#cont-sum&quot;&gt;Continuity of the Sum&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#int&quot;&gt;Integrating term by term&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dif&quot;&gt;Differentiating term by term&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;power-series&quot;&gt;Power Series&lt;/h2&gt;
&lt;p&gt;A &lt;strong&gt;power series&lt;/strong&gt; is a series of the form
\begin{equation}
\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots,
\end{equation}
where the coefficient $a_n$ are constants and $x$ is a variable.&lt;/p&gt;

&lt;h2 id=&quot;int-conv&quot;&gt;The Interval of Convergence&lt;/h2&gt;
&lt;p&gt;Similar to what we have done in the post of &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html&quot;&gt;infinite series of constants&lt;/a&gt;, we begin studying properties of power series by considering their convergence behavior.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lemma 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a power series $\sum a_nx^n$ converges at $x_1$, with $x_1\neq 0$, then it converges &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-conv&quot;&gt;absolutely&lt;/a&gt; at all $x$ with $\vert x\vert&amp;lt;\vert x_1\vert$; and if it diverges at $x_1$, then it diverges at all $x$ with $\vert x\vert&amp;gt;\vert x_1\vert$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we have that if $\sum a_nx^n$ converges, then $a_nx^n\to 0$. In particular, if $n$ is sufficiently large, then $\vert a_n{x_1}^n\vert&amp;lt;1$, and therefore
\begin{equation}
\vert a_nx^n\vert=\vert a_n{x_1}^n\vert\left\vert\dfrac{x}{x_1}\right\vert^n&amp;lt;r^n,\tag{1}\label{1}
\end{equation}
where $r=\vert\frac{x}{x_1}\vert$. Suppose that $\vert x\vert&amp;lt;\vert x_1\vert$, we have
\begin{equation}
r=\left\vert\dfrac{x}{x_1}\right\vert&amp;lt;1,
\end{equation}
which leads to the result that geometric series $\sum r^n$ converges (with the sum $\frac{1}{1-r}$). And hence, from \eqref{1} and by the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, the series $\sum\vert a_nx^n\vert$ also converges.&lt;/p&gt;

&lt;p&gt;Moreover, if $\sum a_n{x_1}^n$ diverges, then $\sum\vert a_n{x_1}^n\vert$ also diverges. By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, for any $x$ such that $\vert x\vert&amp;gt;\vert x_1\vert$, we also have that $\sum\vert a_nx^n\vert$ diverges. This leads to the divergence of $\sum a_nx^n$, because if the series $\sum a_nx^n$ converges, so does $\sum\vert a_nx^n\vert$, which contradicts to our result.&lt;/p&gt;

&lt;p&gt;These are some main facts about the convergence behavior of an arbitrary power series and some properties of its:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given a power series $\sum a_nx^n$, precisely one of the following is true:
    &lt;ul&gt;
      &lt;li&gt;The series converges only for $x=0$.&lt;/li&gt;
      &lt;li&gt;The series is absolutely convergent for all $x$.&lt;/li&gt;
      &lt;li&gt;There exists a positive real number $R$ such that the series is absolutely convergent for $\vert x\vert&amp;lt;R$ and divergent for $\vert x\vert&amp;gt;R$.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The positive real number $R$ is called &lt;strong&gt;radius of convergence&lt;/strong&gt; of the power series: the series converges absolutely at every point of the open interval $(-R,R)$, and diverges outside the closed interval $[-R,R]$.&lt;/li&gt;
  &lt;li&gt;The set of all $x$’s for which a power series converges is called its &lt;strong&gt;interval of convergence&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;When the series converges only for $x=0$, we define $R=0$; and we define $R=\infty$ when the series converges for all $x$.&lt;/li&gt;
  &lt;li&gt;Every power series $\sum a_nx^n$ has a radius of convergence $R$, where $0\leq R\leq\infty$, with the property that the series converges absolutely if $\vert x\vert&amp;lt;R$ and diverges if $\vert x\vert&amp;gt;R$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;eg1&quot;&gt;Example&lt;/h3&gt;
&lt;p&gt;Find the interval of convergence of the series
\begin{equation}
\sum_{n=0}^{\infty}\dfrac{x^n}{n+1}=1+\dfrac{x}{2}+\dfrac{x^2}{3}+\ldots
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
In order to find the interval of convergence of a series, we begin by identifying its radius of convergence.&lt;/p&gt;

&lt;p&gt;Consider a power series $\sum a_nx^n$. Suppose that this limit exists, and has $\infty$ as an allowed value, we have
\begin{equation}
\lim_{n\to\infty}\dfrac{\vert a_{n+1}x^{n+1}\vert}{a_nx^n}=\lim_{n\to\infty}\left\vert\dfrac{a_{n+1}}{a_n}\right\vert.\vert x\vert=\dfrac{\vert x\vert}{\lim_{n\to\infty}\left\vert\frac{a_n}{a_{n+1}}\right\vert}=L
\end{equation}
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#ratio-test&quot;&gt;ratio test&lt;/a&gt;, we have $\sum a_nx^n$ converges absolutely if $L&amp;lt;1$ and diverges in case of $L&amp;gt;1$. Or in other words, the series converges absolutely if
\begin{equation}
\vert x\vert&amp;lt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert,
\end{equation}
or diverges if
\begin{equation}
\vert x\vert&amp;gt;\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}
From the definition of radius of convergence, we can choose the radius of converge of $\sum a_nx^n$ as
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert
\end{equation}&lt;/p&gt;

&lt;p&gt;Back to our problem, for the series $\sum\frac{x^n}{n+1}$, we have its radius of convergence is
\begin{equation}
R=\lim_{n\to\infty}\left\vert\dfrac{a_n}{a_{n+1}}\right\vert=\lim_{n\to\infty}\dfrac{\frac{1}{n+1}}{\frac{1}{n+2}}=\lim_{n\to\infty}\dfrac{n+2}{n+1}=1
\end{equation}
At $x=1$, the series becomes the &lt;em&gt;harmonic series&lt;/em&gt; $1+\frac{1}{2}+\frac{1}{3}+\ldots$, which diverges; and at $x=-1$, it is the &lt;em&gt;alternating harmonic series&lt;/em&gt; $1-\frac{1}{2}+\frac{1}{3}-\ldots$, which converges. Hence, the interval of convergence of the series is $[-1,1)$.&lt;/p&gt;

&lt;h2 id=&quot;dif-int-power-series&quot;&gt;Differentiation and Integration of Power Series&lt;/h2&gt;

&lt;p&gt;It is easily seen that the sum of the series $\sum_{n=0}^{\infty}a_nx^n$  is a function of $x$ since the sum depends only on $x$ for any value of $x$. Hence, we can denote this as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots+a_nx^n+\ldots\tag{2}\label{2}
\end{equation}
This relation between the series and the function is also expressed by saying that $\sum a_nx^n$ is a &lt;strong&gt;power series expansion&lt;/strong&gt; of $f(x)$.&lt;/p&gt;

&lt;p&gt;These are some crucial facts about that relation.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(i) The function $f(x)$ defined by \eqref{2} is continuous on the open interval $(-R,R)$.&lt;/li&gt;
  &lt;li&gt;(ii) The function $f(x)$ is differentiable on $(-R,R)$, and its derivative is given by the formula
\begin{equation}
f’(x)=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots\tag{3}\label{3}
\end{equation}&lt;/li&gt;
  &lt;li&gt;(iii) If $x$ is any point in $(-R,R)$, then
\begin{equation}
\int_{0}^{x}f(t)\,dt=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{4}\label{4}
\end{equation}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
We have that series \eqref{3} and \eqref{4} converge on the interval $(-R,R)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;We begin by proving the convergence on $(-R,R)$ of \eqref{3}.&lt;br /&gt;
Let $x$ be a point in the interval $(-R,R)$ and choose $\epsilon&amp;gt;0$ so that $\vert x\vert+\epsilon&amp;lt;R$. Since $\vert x\vert+\epsilon$ is in the interval, $\sum\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert$ converges.&lt;br /&gt;
We continue by proving the inequality
\begin{equation}
\vert nx^{n-1}\vert\leq\left(\vert x\vert+\epsilon\right)^n\hspace{1cm}\forall n\geq n_0,
\end{equation}
where $\epsilon&amp;gt;0$, $n_0$ is a positive integer.&lt;br /&gt;
We have
\begin{align}
\lim_{n\to\infty}n^{1/n}&amp;amp;=\lim_{n\to\infty} \\ &amp;amp;=\lim_{n\to\infty}\exp\left(\frac{\ln n}{n}\right) \\ &amp;amp;=\exp\left(\lim_{n\to\infty}\frac{\ln n}{n}\right) \\ &amp;amp;={\rm e}^0=1,
\end{align}
where in the fourth step, we use the &lt;em&gt;L’Hospital theorem&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Therefore, as $n\to\infty$
\begin{equation}
n^{1/n}\vert x\vert^{1-1/n}\to\vert x\vert
\end{equation}
Then for all sufficiently large $n$’s
\begin{align}
n^{1/n}\vert x\vert^{1-1/n}&amp;amp;\leq\vert x\vert+\epsilon \\ \vert nx^{n-1}\vert&amp;amp;\leq\left(\vert x\vert+\epsilon\right)^n
\end{align}
This implies that
\begin{equation}
\vert na_nx^{n-1}\vert\leq\vert a_n\left(\vert x\vert+\epsilon\right)^n\vert
\end{equation}
By the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we have that the series $\sum\vert na_nx^{n-1}\vert$ converges, and so does $\sum na_nx^{n-1}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Since $\sum\vert a_nx^n\vert$ converges and
\begin{equation}
\left\vert\dfrac{a_nx^n}{n+1}\right\vert\leq\vert a_nx^n\vert,
\end{equation}
the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#comparison-test&quot;&gt;comparison test&lt;/a&gt; implies that $\sum\left\vert\frac{a_nx^n}{n+1}\right\vert$ converges, and therefore
\begin{equation}
x\sum\frac{a_nx^n}{n+1}=\sum\frac{1}{n+1}a_nx^{n+1}
\end{equation}
also converges.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;dif-power-series&quot;&gt;Differentiation of Power Series&lt;/h3&gt;

&lt;p&gt;If we instead apply (ii) to the function $f’(x)$ in \eqref{3}, then it follows that $f’(x)$ is also differentiable. Doing the exact same process to $f’&apos;(x)$, we also have that $f’&apos;(x)$ is differentiable, and so on. Hence, the original $f(x)$ has derivatives of all orders, as expressed in the following statement:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;In the interior of its interval of convergence, a power series defines an finitely differentiable function whose derivatives can be calculated by differentiating the series term by term&lt;/em&gt;.
\begin{equation}
\dfrac{d}{dx}\left(\sum a_nx^n\right)=\sum\dfrac{d}{dx}(a_nx^n)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;int-power-series&quot;&gt;Integration of Power Series&lt;/h3&gt;

&lt;p&gt;Similarly, from (iii), the term-by-term integration of power series can be emphasized by writing \eqref{4} as
\begin{equation}
\int\left(\sum a_nx^n\right)\,dx=\sum\left(\int a_nx^n\,dx\right)
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;eg2&quot;&gt;Example&lt;/h3&gt;

&lt;p&gt;Find a power series expansion of ${\rm e}^x$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Solution&lt;/strong&gt;&lt;br /&gt;
Since ${\rm e}^x$ is the only function that equals its own derivatives&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; and has the value $1$ at $x=0$. To construct a power series equal to its own derivative, we use the fact that when such a series is differentiated, the degree of each term drops by $1$. We therefore want each term to be the derivative of the one that follows it.&lt;/p&gt;

&lt;p&gt;Starting with $1$ as the constant term, the next should be $x$, then $\frac{1}{2}x^2$, then $\frac{1}{2.3}x^3$, and so on. This produces the series
\begin{equation}
1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots,\tag{5}\label{5}
\end{equation}
which converges for all $x$ because
\begin{equation}
R=\lim_{n\to\infty}\dfrac{\frac{1}{n!}}{\frac{1}{(n+1)!}}=\lim_{n\to\infty}(n+1)=\infty
\end{equation}
We have constructed the series \eqref{5} so that its sum is unchanged by differentiated and has the value $1$ at $x=0$. Therefore, for all $x$,
\begin{equation}
{\rm e}^x=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots+\dfrac{x^n}{n!}+\ldots
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;taylor-series-formula&quot;&gt;Taylor Series, Taylor’s Formula&lt;/h2&gt;

&lt;h3 id=&quot;taylor-series&quot;&gt;Taylor Series&lt;/h3&gt;
&lt;p&gt;Assume that $f(x)$ is the sum of a power series with positive radius of convergence
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_nx^n=a_0+a_1x+a_2x^2+\ldots,\hspace{1cm}R&amp;gt;0\tag{6}\label{6}
\end{equation}
By the results obtained from previous section, differentiating \eqref{6} term by term we have
\begin{align}
f^{(1)}(x)&amp;amp;=a_1+2a_2x+3a_3x^2+\ldots \\ f^{(2)}(x)&amp;amp;=1.2a_2+2.3a_3x+3.4a_4x^2+\ldots \\ f^{(3)}(x)&amp;amp;=1.2.3a_3+2.3.4a_4x+3.4.5a_5x^2+\ldots
\end{align}
and in general,
\begin{equation}
f^{(n)}(x)=n!a_n+A(x),\tag{7}\label{7}
\end{equation}
where $A(x)$ contains $x$ as a factor.&lt;/p&gt;

&lt;p&gt;Since these series expansions of the derivatives are valid on the open interval $(-R,R)$, putting $x=0$ in \eqref{7} we obtain
\begin{equation}
f^{(n)}(0)=n!a_n
\end{equation}
so
\begin{equation}
a_n=\dfrac{f^{(n)}(0)}{n!}
\end{equation}
Putting this result in \eqref{6}, our series becomes
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\ldots\tag{8}\label{8}
\end{equation}
This power series is called &lt;strong&gt;Taylor series&lt;/strong&gt; of $f(x)$ [at $x=0$], which is named after the person who introduced it, Brook Taylor.&lt;/p&gt;

&lt;p&gt;If we use the convention that $0!=1$, then \eqref{8} can be written as
\begin{equation}
f(x)=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(0)}{n!}x^n
\end{equation}
The numbers $a_n=\frac{f^{(n)}(0)}{n!}$ are called the &lt;strong&gt;Taylor coefficients&lt;/strong&gt; of $f(x)$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
Given a function $f(x)$ that is infinitely differentiable in some interval containing the point $x=0$, we have already examined the possibility of expanding this function as a power series in $x$. More generally, if $f(x)$ is infinitely differentiable in some interval containing the point $x=a$, is there any possibility for the power series expansion of $f(x)$ in $x-a$ instead of $x$?&lt;br /&gt;
\begin{equation}
f(x)=\sum_{n=0}^{\infty}a_n(x-a)^n=a_0+a_1(x-a)+a_2(x-a)^2+\ldots
\end{equation}
Let $w=x-a$, and $g(w)=f(x)$, we have that $g^{(n)}(0)=f^{(n)}(a)$. Thus, the Taylor series of $f(x)$ in power of $x-a$ (or at $x=a$) is
\begin{align}
f(x)&amp;amp;=\sum_{n=0}^{\infty}\dfrac{f^{(n)}(a)}{n!}(x-a)^n \\ &amp;amp;=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\ldots\tag{9}\label{9}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;taylors-formula&quot;&gt;Taylor’s Formula&lt;/h3&gt;
&lt;p&gt;If we break off the Taylor series on the right side of \eqref{8} at the term containing $x^n$ and define the &lt;em&gt;remainder&lt;/em&gt; $R_n(x)$ by the equation
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+R_n(x),\tag{10}\label{10}
\end{equation}
then the Taylor series on the right side of \eqref{8} converges to the function $f(x)$ as $n$ tends to infinity precisely when
\begin{equation}
\lim_{n\to\infty}R_n(x)=0
\end{equation}
Since $R_n(x)$ contains $x^{n+1}$ as a factor, we can define a function $S_n(x)$ by writing
\begin{equation}
R_n(x)=S_n(x)x^{n+1}
\end{equation}
for $x\neq 0$. Next, we keep $x$ fixed and define a function $F(t)$ for $0\leq t\leq x$ (or $x\leq t\leq 0$) by writing
\begin{multline}
F(t)=f(x)-f(t)-f^{(1)}(t)(x-t)-\dfrac{f^{(2)}(t)}{2!}(x-t)^2-\ldots \\ -\dfrac{f^{(n)}(t)}{n!}(x-t)^n-S_n(x)(x-t)^{n+1}
\end{multline}
It is easily seen that $F(x)=0$. Also, from equation \eqref{10}, we have that $F(0)=0$. Then by the &lt;em&gt;Mean Value Theorem&lt;/em&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, $F’(c)=0$ for some constant $c$ between $0$ and $x$.&lt;/p&gt;

&lt;p&gt;By differentiating $F(t)$ w.r.t $t$, and evaluate it at $t=c$, we have
\begin{equation}
F’(c)=-\dfrac{f^{(n+1)}(c)}{n!}(x-c)^n+S_n(x)(n+1)(x-c)^n=0
\end{equation}
so
\begin{equation}
S_n(x)=\dfrac{f^{(n+1)}(c)}{(n+1)!}
\end{equation}
and
\begin{equation}
R_n(x)=S_n(x)x^{n+1}=\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1}
\end{equation}
which makes \eqref{10} become
\begin{equation}
f(x)=f(0)+f^{(1)}(0)x+\dfrac{f^{(2)}(0)}{2!}x^2+\ldots+\dfrac{f^{(n)}(0)}{n!}x^n+\dfrac{f^{(n+1)}(c)}{(n+1)!}x^{n+1},
\end{equation}
where $c$ is some number between $0$ and $x$. This equation is called &lt;strong&gt;Taylor’s formula with derivative remainder&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Moreover, with this formula we can rewrite \eqref{9} as
\begin{multline}
f(x)=f(a)+f^{(1)}(a)(x-a)+\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots \\ +\dfrac{f^{(n)}(a)}{n!}(x-a)^n+\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1},\tag{11}\label{11}
\end{multline}
where $c$ is some number between $a$ and $x$.&lt;/p&gt;

&lt;p&gt;The polynomial part of \eqref{11}
\begin{multline}
\sum_{j=0}^{n}\dfrac{f^{(j)}(a)}{j!}(x-a)^j=f(a)+f^{(1)}(a)(x-a) \\ +\dfrac{f^{(2)}(a)}{2!}(x-a)^2+\ldots+\dfrac{f^{(n)}(a)}{n!}(x-a)^n
\end{multline}
is called the &lt;strong&gt;nth-degree Taylor polynomial at&lt;/strong&gt; $x=a$.&lt;/p&gt;

&lt;p&gt;On the other hand, the remainder part of \eqref{11}
\begin{equation}
R_n(x)=\dfrac{f^{(n+1)}(a)}{(n+1)!}(x-a)^{n+1}
\end{equation}
is often called &lt;strong&gt;Lagrange’s remainder formula&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Remark&lt;/strong&gt;&lt;br /&gt;
It is worth remarking that power series expansions are &lt;em&gt;unique&lt;/em&gt;. This means that if a function $f(x)$ can be expressed as a sum of a power series by &lt;em&gt;any method&lt;/em&gt;, then this series must be the Taylor series of $f(x)$.&lt;/p&gt;

&lt;h2 id=&quot;op-power-series&quot;&gt;Operations on Power Series&lt;/h2&gt;

&lt;h3 id=&quot;mult&quot;&gt;Multiplication&lt;/h3&gt;
&lt;p&gt;Suppose we are given two power series expansions
\begin{align}
f(x)&amp;amp;=\sum a_nx^n=a_0+a_1x+a_2x^2+a_3x^3+\ldots\tag{12}\label{12} \\ g(x)&amp;amp;=\sum b_nx^n=b_0+b_1x+b_2x^2+b_3x^3+\ldots\tag{13}\label{13}
\end{align}
both valid on $(-R,R)$. If we multiply these two series term by term, we obtain the power series
\begin{multline}
a_0b_0+(a_0b_1+a_1b_0)x+(a_0b_2+a_1b_1+a_2b_0)x^2 \\ +(a_0b_3+a_1b_2+a_2b_1+a_3b_0)x^3+\ldots
\end{multline}
Briefly, we have multiplied \eqref{12} and \eqref{13} to obtain
\begin{equation}
f(x)g(x)=\sum_{n=0}^{\infty}\left(\sum_{k=0}^{n}a_kb_{n-k}\right)x^n\tag{14}\label{14}
\end{equation}
By the &lt;strong&gt;Theorem 10&lt;/strong&gt; from &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-vs-cond&quot;&gt;Absolute vs Conditionally Convergence&lt;/a&gt;, we have that this product of the series \eqref{12} and \eqref{13} actually converges on the interval $(-R,R)$ to the product of the functions $f(x)$ and $g(x)$, as indicated by \eqref{14}.&lt;/p&gt;

&lt;h3 id=&quot;div&quot;&gt;Division&lt;/h3&gt;
&lt;p&gt;With the two series \eqref{12} and \eqref{13}, we have
\begin{equation}
\dfrac{\sum a_nx^n}{\sum b_nx^n}=\left(\sum a_nx^n\right).\left(\dfrac{1}{\sum b_nx^n}\right)
\end{equation}
This suggests us that if we can expand $\frac{1}{\sum b_nx^n}$ in a power series with positive radius of convergence $\sum c_nx^n$, and multiply this series by $\sum a_nx^n$, we can compute the division of our two series $\sum a_nx^n$ and $\sum b_nx^n$.&lt;/p&gt;

&lt;p&gt;To do the division properly, it is necessary to assume that $b_0\neq0$ (for the case $x=0$). Moreover, without any loss of generality, we may assume that $b_0=1$, because with the assumption that $b_0\neq0$, we simply factor it out
\begin{equation}
\dfrac{1}{b_0+b_1x+b_2x^2+\ldots}=\dfrac{1}{b_0}.\dfrac{1}{1+\frac{b_1}{b_0}x+\frac{b_2}{b_0}x^2+\ldots}
\end{equation}&lt;/p&gt;

&lt;p&gt;We begin by determining the $c_n$’s. Since $\frac{1}{\sum b_nx^n}=\sum c_nx^n$, then $(\sum b_nx^n)(\sum c_nx^n)=1$, so
\begin{multline}
b_0c_0+(b_0c_1+b_1c_0)x+(b_0c_2+b_1c_1+b_2c_0)x^2+\ldots \\ +(b_0c_n+b_1c_{n-1}+\ldots+b_nc_0)x^n+\ldots=1,
\end{multline}
and since $b_0=1$, we can determine the $c_n$’s recursively
\begin{align}
c_0&amp;amp;=1 \\ c_1&amp;amp;=-b_1c_0 \\ c_2&amp;amp;=-b_1c_1-b_2c_0 \\ &amp;amp;\vdots \\ c_n&amp;amp;=-b_1c_{n-1}-b_2c_{n-2}-\ldots-b_nc_0 \\ &amp;amp;\vdots
\end{align}
Now our work reduces to proving that the power series $\sum c_nx^n$ with these coefficients has positive radius of convergence, and for this it suffices to show that the series converges for at least one nonzero $x$.&lt;/p&gt;

&lt;p&gt;Let $r$ be any number such that $0&amp;lt;r&amp;lt;R$, so that $\sum b_nr^n$ converges. Then there exists a constant $K\geq 1$ with the property that $\vert b_nr^n\vert\leq K$ or $\vert b_n\vert\leq\frac{K}{r^n}$ for all $n$. Therefore,
\begin{align}
\vert c_0\vert&amp;amp;=1\leq K, \\ \vert c_1\vert&amp;amp;=\vert b_1c_0\vert=\vert b_1\vert\leq \dfrac{K}{r}, \\ \vert c_2\vert&amp;amp;\leq\vert b_1c_1\vert+\vert b_2c_0\vert\leq\dfrac{K}{r}.\dfrac{K}{r}+\dfrac{K}{r^2}.K=\dfrac{2K^2}{r^2}, \\ \vert c_3\vert&amp;amp;\leq\vert b_1c_2\vert+\vert b_2c_1\vert+\vert b_3c_0\vert\leq\dfrac{K}{r}.\dfrac{2K^2}{r^2}+\dfrac{K}{r^2}.\dfrac{K}{r}+\dfrac{K}{r^3}.K \\ &amp;amp;\hspace{5.3cm}\leq(2+1+1)\dfrac{K^3}{r^3}=\dfrac{4K^3}{r^3}=\dfrac{2^2K^3}{r^3},
\end{align}
since $K^2\leq K^3$ since $K\geq1$. In general,
\begin{align}
\vert c_n\vert&amp;amp;\leq\vert c_1b_{n-1}\vert+\vert c_2b_{n-2}\vert+\ldots+\vert b_nc_0\vert \\ &amp;amp;\leq\dfrac{K}{r}.\dfrac{2^{n-2}K^{n-1}}{r^{n-1}}+\dfrac{K}{r^2}.\dfrac{2^{n-3}K^{n-2}}{r^{n-2}}+\ldots+\dfrac{K}{r^n}.K \\ &amp;amp;\leq(2^{n-2}+2^{n-3}+\ldots+1+1)\dfrac{K^n}{r^n}=\dfrac{2^{n-1}K^n}{r^n}\leq\dfrac{2^nK^n}{r^n}
\end{align}
Hence, for any $x$ such that $\vert x\vert&amp;lt;\frac{r}{2K}$, we have that the series $\sum c_nx^n$ converges absolutely, and therefore converges, or in other words, $\sum c_nx^n$ has nonzero radius of convergence.&lt;/p&gt;

&lt;h3 id=&quot;sub&quot;&gt;Substitution&lt;/h3&gt;
&lt;p&gt;If a power series
\begin{equation}
f(X)=a_0+a_1x+a_2x^2+\ldots\tag{15}\label{15}
\end{equation}
converges for $\vert x\vert&amp;lt;R$ and if $\vert g(x)\vert&amp;lt;R$, then we can find $f(g(x))$ by substituting $g(x)$ for $x$ in \eqref{15}.&lt;/p&gt;

&lt;p&gt;Suppose $g(x)$ is given by a power series,
\begin{equation}
g(x)=b_0+b_1x+b_2x^2+\ldots,\tag{16}\label{16}
\end{equation}
therefore,
\begin{align}
f(g(x))&amp;amp;=a_0+a_1g(x)+a_2g(x)^2+\ldots \\ &amp;amp;=a_0+a_1(b+0+b_1x+\ldots)+a_2(b_0+b_1x+\ldots)^2+\ldots
\end{align}
The power series formed in this way converges to $f(g(x))$ whenever \eqref{16} is absolutely convergent and $\vert g(x)\vert&amp;lt;R$.&lt;/p&gt;

&lt;h3 id=&quot;even-odd-funcs&quot;&gt;Even and Odd Functions&lt;/h3&gt;
&lt;p&gt;A function $f(x)$ defined on $(-R,R)$ is said to be &lt;strong&gt;even&lt;/strong&gt; if
\begin{equation}
f(-x)=f(x),
\end{equation}
and &lt;strong&gt;odd&lt;/strong&gt; if
\begin{equation}
f(-x)=-f(x)
\end{equation}
Then if $f(x)$ is an even function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n}x^{2n}=a_0+a_2x^2+a_4x^4+\ldots
\end{equation}
and if $f(x)$ is an odd function, then its Taylor series has the form
\begin{equation}
\sum_{n=0}^{\infty}a_{2n+1}x^{2n+1}=a_1x+a_3x^3+a_5x^5+\ldots
\end{equation}
since if $f(x)=\sum_{n=0}^{\infty}a_nx^n$ is even, then $\sum_{n=0}^{\infty}a_nx^n=\sum_{n=0}^{\infty}(-1)^na_nx^n$, so by the uniqueness of the Taylor series expansion, we have that $a_n=(-1)^na_n$; similarly, $a_n=(-1)^{n+1}a_n$ if $f(x)$ is an odd function.&lt;/p&gt;

&lt;h2 id=&quot;uni-conv-power-series&quot;&gt;Uniform Convergence for Power Series&lt;/h2&gt;
&lt;p&gt;Consider a power series $\sum a_nx^n$ with positive radius of convergence $R$, and let $f(x)$ be its sum.&lt;/p&gt;

&lt;p&gt;In the &lt;a href=&quot;#dif-int-power-series&quot;&gt;section&lt;/a&gt; above, we stated that $f(x)$ is continuous and differentiable on $(-R,R)$, and we can differentiate and integrate it term by term. So let’s prove these statements!&lt;/p&gt;

&lt;p&gt;Let $S_n(x)$ be the $n$-th partial sum of the series, so that
\begin{equation}
S_n(x)=\sum_{i=0}^{n}a_ix^i=a_0+a_1x+a_2x^2+\ldots+a_nx^n
\end{equation}
Similar to what we did in &lt;a href=&quot;#taylors-formula&quot;&gt;Taylor’s formula&lt;/a&gt;, we write
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
Thus, the remainder
\begin{equation}
R_n(x)=a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots
\end{equation}&lt;/p&gt;

&lt;p&gt;For each $x$ in the interval of convergence, we know that $R_n(x)\to0$ as $n\to\infty$; that is, for any given $\epsilon&amp;gt;0$, and for an integer $n_0$ large enough, we have
\begin{equation}
\vert R_n(x)\vert&amp;lt;\epsilon\hspace{1cm}n\geq n_0,\tag{17}\label{17}
\end{equation}
This is true for each $x$ individually, and is an equivalent way of expressing the fact that $\sum a_nx^n$ converges to $f(x)$.&lt;/p&gt;

&lt;p&gt;Moreover, for every $x$ in the given a closed interval $\vert x\vert\leq\vert x_1\vert&amp;lt;R$, we have
\begin{align}
\vert R_n(x)\vert&amp;amp;=\left\vert a_{n+1}x^{n+1}+a_{n+2}x^{n+2}+\ldots\right\vert \\ &amp;amp;\leq\left\vert a_{n+1}x^{n+1}\right\vert+\left\vert a_{n+2}x^{n+2}\right\vert+\ldots \\ &amp;amp;\leq\left\vert a_{n+1}{x_1}^{n+1}\right\vert+\left\vert a_{n+2}{x_1}^{n+2}\right\vert+\ldots
\end{align}
Because of the &lt;a href=&quot;/mathematics/calculus/2021/09/06/infinite-series-of-constants.html#abs-conv&quot;&gt;absolute convergence&lt;/a&gt; of $\sum a_n{x_1}^n$, the last sum can be made $&amp;lt;\epsilon$ by taking $n$ large enough, $n\geq n_0$. Therefore, we have that \eqref{17} holds for all $x$ inside the closed interval $\vert x\vert\leq\vert x_1\vert$ inside the interval of convergence $(-R,R)$.&lt;/p&gt;

&lt;p&gt;Or in other words, $R_n(x)$ can be made small &lt;em&gt;independently of $x$ in the given closed interval&lt;/em&gt; $\vert x\vert\leq\vert x_1\vert$, which is equivalent to saying that the series $\sum a_nx^n$ is &lt;strong&gt;uniformly convergent&lt;/strong&gt; in this interval&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h3 id=&quot;cont-sum&quot;&gt;Continuity of the Sum&lt;/h3&gt;
&lt;p&gt;In order to prove that $f(x)$ is continuous on $(-R,R)$, it suffices to prove that $f(x)$ is continuous at each point $x_0$ in the interval of convergence.&lt;/p&gt;

&lt;p&gt;Consider a closed subinterval $\vert x\vert\leq\vert x_1\vert&amp;lt;R$ containing $x_0$ in its interior. If $\epsilon&amp;gt;0$ is given, then by uniform convergence we can find an $n$ such that $\vert R_n(x)\vert&amp;lt;\epsilon$ for all $x$’s in the subinterval.&lt;/p&gt;

&lt;p&gt;Since the polynomial $S_n(x)$ is continuous at $x_0$, we can find $\delta&amp;gt;0$ small that $\vert x-x_0\vert&amp;lt;\delta$ implies $x$ lies in the subinterval and $\vert S_n(x)-S_n(x_0)\vert&amp;lt;\epsilon$. Putting these conditions together we find that $\vert x-x_0\vert&amp;lt;\delta$ implies
\begin{align}
\vert f(x)-f(x_0)\vert&amp;amp;=\left\vert S_n(x)+R_n(x)-\left(S_n(x_0)+R_n(x_0)\right)\right\vert \\ &amp;amp;=\left\vert\left(S_n(x)-S_n(x_0)\right)+R_n(x)-R_n(x_0)\right\vert \\ &amp;amp;\leq\left\vert S_n(x)-S_n(x_0)\right\vert+\left\vert R_n(x)\right\vert+\left\vert R_n(x_0)\right\vert \\ &amp;amp;&amp;lt;\epsilon+\epsilon+\epsilon=3\epsilon
\end{align}
which proves the continuity of $f(x)$ at $x_0$.&lt;/p&gt;

&lt;h3 id=&quot;int&quot;&gt;Integrating term by term&lt;/h3&gt;
&lt;p&gt;With what we have just proved that $f(x)=\sum a_nx^n$ is continuous on $(-R,R)$, we can therefore integrate this function between $a$ and $b$ that lie inside the interval
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx
\end{equation}
We need to prove that the right side of this equation can be integrated term by term, which is
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}\left(\sum a_nx^n\right)\,dx=\sum\int_{a}^{b}a_nx^n\,dx\tag{18}\label{18}
\end{equation}
In order to prove this, we begin by observing that $S_n(x)$ is a polynomial, and for that reason it is continuous. Thus, all there of the functions in
\begin{equation}
f(x)=S_n(x)+R_n(x)
\end{equation}
are continuous on $(-R,R)$. This allows us to write
\begin{equation}
\int_{a}^{b}f(x)\,dx=\int_{a}^{b}S_n(x)\,dx+\int_{a}^{b}R_n(x)\,dx
\end{equation}
Moreover, we can integrate $S_n(x)$ term by term
\begin{align}
\int_{a}^{b}S_n(x)\,dx&amp;amp;=\int_{a}^{b}\left(a_0+a_1x+a_2x^2+\ldots+a_nx^n\right)\,dx \\ &amp;amp;=\int_{a}^{b}a_0\,dx+\int_{a}^{b}a_1x\,dx+\int_{a}^{b}a_2x^2\,dx+\ldots+\int_{a}^{b}a_nx^n\,dx
\end{align}
To prove \eqref{18}, it therefore suffices to show that as $n\to\infty$
\begin{equation}
\int_{a}^{b}R_n(x)\,dx\to 0
\end{equation}
By uniform convergence, if $\epsilon&amp;gt;0$ is given and $\vert x\vert\leq\vert x_1\vert&amp;lt;R$ is a closed subinterval of $(-R,R)$ that contains both $a,b$, then $\vert R_n(x)\vert&amp;lt;\epsilon$ for all $x$ in the subinterval and $n$ large enough. Hence,
\begin{equation}
\left\vert\int_{a}^{b}R_n(x)\,dx\right\vert\leq\int_{a}^{b}\left\vert R_n(x)\right\vert\,dx&amp;lt;\epsilon\vert b-a\vert
\end{equation}
for any $n$ large enough, which proves our statement.&lt;/p&gt;

&lt;p&gt;As a special case of \eqref{18}, we take the limits $0$ and $x$ instead of $a$ and $b$, and obtain
\begin{align}
\int_{a}^{b}f(t)\,dt&amp;amp;=\sum\dfrac{1}{n+1}a_nx^{n+1} \\ &amp;amp;=a_0x+\dfrac{1}{2}a_1x^2+\dfrac{1}{3}a_2x^3+\ldots+\dfrac{1}{n+1}a_nx^{n+1}+\ldots\tag{19}\label{19}
\end{align}&lt;/p&gt;

&lt;h3 id=&quot;dif&quot;&gt;Differentiating term by term&lt;/h3&gt;
&lt;p&gt;We now prove that the function $f(x)$ is not only continuous but also differentiable on $(-R,R)$, and that its derivative can be calculated by differentiating term by term
\begin{equation}
f’(x)=\sum na_nx^{n-1}
\end{equation}
It is easily seen that the series on right side of this equation is exact the series on the right side of \eqref{3}, which is convergent on $(-R,R)$ as we proved. If we denote its sum by $g(x)$
\begin{equation}
g(x)=\sum na_nx^{n-1}=a_1+2a_2x+3a_3x^2+\ldots+na_nx^{n-1}+\ldots,
\end{equation}
then \eqref{19} tells us that
\begin{align}
\int_{0}^{x}g(t)\,dt&amp;amp;=a_1x+a_2x^2+a_3x^3+\ldots \\ &amp;amp;=f(x)-a_0
\end{align}
Since the left side of this has a derivative, so does the right side, and by differentiating we obtain
\begin{equation}
f’(x)=g(x)=\sum na_nx^{n-1}
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] George F.Simmons. &lt;a href=&quot;https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424&quot;&gt;Calculus With Analytic Geometry - 2nd Edition&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Marian M. &lt;a href=&quot;https://www.springer.com/gp/book/9780387789323&quot;&gt;A Concrete Approach to Classical Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] MIT 18.01. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&quot;&gt;Single Variable Calculus&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;L’Hospital&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Assume $f$ and $g$ are real and differentiable on $]a,b[$ and $g’(x)\neq 0$ for all $x\in]a,b[$, where $-\infty\leq a&amp;lt;b\leq\infty$. Suppose as $x\to a$,
\begin{equation}
\dfrac{f’(x)}{g’(x)}\to A\,(\in[-\infty,\infty])
\end{equation}
If as $x\to a$, $f(x)\to 0$ and $g(x)\to 0$ or if $g(x)\to+\infty$ as $x\to a$, then
\begin{equation}
\dfrac{f(x)}{g(x)}\to A
\end{equation}
as $x\to a$.&lt;/em&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider the function $f(x)=a^x$.&lt;br /&gt;
Using the definition of the derivative, we have
\begin{align}
\dfrac{d}{dx}f(x)&amp;amp;=\lim_{h\to 0}\dfrac{f(x+h)-f(x)}{h} \\ &amp;amp;=\lim_{h\to 0}\dfrac{a^{x+h}-a^x}{h} \\ &amp;amp;=a^x\lim_{h\to 0}\dfrac{a^h-1}{h}
\end{align}
Therefore,
\begin{equation}
\lim_{h\to 0}\dfrac{a^h-1}{h}=1
\end{equation}
then, let $n=\frac{1}{h}$, we have
\begin{equation}
a=\lim_{h\to 0}\left(1+\dfrac{1}{h}\right)^{1/h}=\lim_{n\to\infty}\left(1+\dfrac{1}{n}\right)^n={\rm e}
\end{equation}
Thus, $f(x)=a^x={\rm e}^x$. Every function $y=c{\rm e}^x$ also satisfies the differential equation $\frac{dy}{dx}=y$, because
\begin{equation}
\dfrac{dy}{dx}=\dfrac{d}{dx}c{\rm e}^x=c\dfrac{d}{dx}{\rm e}^x=c{\rm e}^x=y
\end{equation}&lt;br /&gt;
The rest of our proof is to prove that these are only functions that are unchanged by differentiation.&lt;br /&gt;
To prove this, suppose $f(x)$ is any function with that property. By the quotient rule,
\begin{equation}
\dfrac{d}{dx}\dfrac{f(x)}{e^x}=\dfrac{f’(x)e^x-e^x f(x)}{e^{2x}}=\dfrac{e^x f(x)-e^x f(x)}{e^{2x}}=0
\end{equation}
which implies that
\begin{equation}
\dfrac{f(x)}{e^x}=c,
\end{equation}
for some constant $c$, and so $f(x)=ce^x$. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Mean Value Theorem&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If a function $f(x)$ is continuous on the closed interval $[a,b]$ and differentiable in the open interval $(a,b)$, then there exists at least one number $c$ between $a$ and $b$ with the property that&lt;/em&gt;
\begin{equation}
f’(c)=\frac{f(b)-f(a)}{b-a}
\end{equation} &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We will talk more about uniform convergence in the post of sequences. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="power-series" /><category term="taylor-series" /><category term="random-stuffs" /><summary type="html">Recall that in the previous post, Infinite Series of Constants, we mentioned a type of series called power series a lot. In the content of this post, we will be diving deeper into details of that series.</summary></entry><entry><title type="html">Infinite Series of Constants</title><link href="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html" rel="alternate" type="text/html" title="Infinite Series of Constants" /><published>2021-09-06T11:20:00+07:00</published><updated>2021-09-06T11:20:00+07:00</updated><id>http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants</id><content type="html" xml:base="http://localhost:4000/mathematics/calculus/2021/09/06/infinite-series-of-constants.html">&lt;blockquote&gt;
  &lt;p&gt;A note on infinite series of constants.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!-- excerpt-end --&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#infinite-series&quot;&gt;Infinite Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#examples&quot;&gt;Examples&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#convergent-sequences&quot;&gt;Convergent Sequences&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#sequences&quot;&gt;Sequences&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lim-seq&quot;&gt;Limits of Sequences&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison tests&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#comparison-test&quot;&gt;Comparison test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#limit-comparison-test&quot;&gt;Limit comparison test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#int-test-euler-c&quot;&gt;The Integral test. Euler’s constant&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#integral-test&quot;&gt;Integral test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#euler-c&quot;&gt;Euler’s constant&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ratio-root&quot;&gt;The Ratio test. Root test&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ratio-test&quot;&gt;Ratio test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#root-test&quot;&gt;Root test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#extended-ratio-test&quot;&gt;The Extended Ratio tests of Raabe and Gauss&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#kummers-theorem&quot;&gt;Kummer’s theorem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#raabes-test&quot;&gt;Raabe’s test&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#gausss-test&quot;&gt;Gauss’s test&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#alt-test-abs-conv&quot;&gt;The Alternating Series test. Absolute Convergence&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#alt-series&quot;&gt;Alternating Series&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#alt-series-test&quot;&gt;Alternating Series test&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#abs-conv&quot;&gt;Absolute Convergence&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#abs-vs-cond&quot;&gt;Absolute vs. Conditionally Convergence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dirichlets-test&quot;&gt;Dirichlet’s test&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#abel-part-sum&quot;&gt;Abel’s partial summation formula&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#d-test&quot;&gt;Dirichlet’s test&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;infinite-series&quot;&gt;Infinite Series&lt;/h2&gt;
&lt;p&gt;An &lt;strong&gt;infinite series&lt;/strong&gt;, or simply a &lt;strong&gt;series&lt;/strong&gt;, is an expression of the form
\begin{equation}
a_1+a_2+\dots+a_n+\dots=\sum_{n=1}^{\infty}a_n
\end{equation}&lt;/p&gt;

&lt;h3 id=&quot;examples&quot;&gt;Examples&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Infinite decimal&lt;/em&gt;
\begin{equation}
.a_1a_2\ldots a_n\ldots=\dfrac{a_1}{10}+\dfrac{a_2}{10^2}+\ldots+\dfrac{a_n}{10^n}+\ldots,
\end{equation}
where $a_i\in\{0,1,\dots,9\}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Power series expansion&lt;/em&gt;&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Geometric series
\begin{equation}
\dfrac{1}{1-x}=\sum_{n=0}^{\infty}x^n=1+x+x^2+x^3+\dots,\hspace{1cm}\vert x\vert&amp;lt;1
\end{equation}&lt;/li&gt;
      &lt;li&gt;Exponential function
\begin{equation}
{\rm e}^x=\sum_{n=0}^{\infty}\dfrac{x^n}{n!}=1+x+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\ldots
\end{equation}&lt;/li&gt;
      &lt;li&gt;Sine and cosine formulas
\begin{align}
\sin x&amp;amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n+1}}{(2n+1)!}=x-\dfrac{x^3}{3!}+\dfrac{x^5}{5!}-\dfrac{x^7}{7!}+\ldots \\ \cos x&amp;amp;=\sum_{n=0}^{\infty}\dfrac{(-1)^n x^{2n}}{(2n)!}=1-\dfrac{x^2}{2!}+\dfrac{x^4}{4!}-\dfrac{x^6}{6!}+\ldots
\end{align}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;convergent-sequences&quot;&gt;Convergent Sequences&lt;/h2&gt;

&lt;h3 id=&quot;sequences&quot;&gt;Sequences&lt;/h3&gt;
&lt;p&gt;If to each positive integer $n$ there corresponds a definite number $x_n$, then the $x_n$’s are said to form a &lt;strong&gt;sequence&lt;/strong&gt; (denoted as $\{x_n\}$)
\begin{equation}
x_1,x_2,\dots,x_n,\dots
\end{equation}
We call the numbers constructing a sequence its terms, where $x_n$ is the $n$-th term.&lt;/p&gt;

&lt;p&gt;A sequence $\{x_n\}$ is said to be &lt;em&gt;bounded&lt;/em&gt; if there exists $A, B$ such that $A\leq x_n\leq B, \forall n$. $A, B$ respectively are called &lt;em&gt;lower bound&lt;/em&gt;, &lt;em&gt;upper bound&lt;/em&gt; of the sequence. A sequence that is not bounded is said to be &lt;em&gt;unbounded&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;lim-seq&quot;&gt;Limits of Sequences&lt;/h3&gt;
&lt;p&gt;A sequence $\{x_n\}$ is said to have a number $L$ as &lt;strong&gt;limit&lt;/strong&gt; if for each $\epsilon&amp;gt;0$, there exists a positive integer $n_0$ that
\begin{equation}
\vert x_n-L\vert&amp;lt;\epsilon\hspace{1cm}n\geq n_0
\end{equation}
We say that $x_n$ &lt;em&gt;converges to&lt;/em&gt; $L$ &lt;em&gt;as&lt;/em&gt; $n$ &lt;em&gt;approaches infinite&lt;/em&gt; ($x_n\to L$ as $n\to\infty$) and denote this as
\begin{equation}
\lim_{n\to\infty}x_n=L
\end{equation}&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A sequence is said to &lt;strong&gt;converge&lt;/strong&gt; or to be &lt;strong&gt;convergent&lt;/strong&gt; if it has a limit.&lt;/li&gt;
  &lt;li&gt;A convergent sequence is bounded, but not all bounded sequences are convergent.&lt;/li&gt;
  &lt;li&gt;If $x_n\to L,y_n\to M$, then
\begin{align}
&amp;amp;\lim(x_n+y_n)=L+M \\ &amp;amp;\lim(x_n-y_n)=L-M \\ &amp;amp;\lim x_n y_n=LM \\ &amp;amp;\lim\dfrac{x_n}{y_n}=\dfrac{L}{M}\hspace{1cm}M\neq0
\end{align}&lt;/li&gt;
  &lt;li&gt;An &lt;em&gt;increasing&lt;/em&gt; (or &lt;em&gt;decreasing&lt;/em&gt;) sequence converges if and only if it is bounded.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conv-div-series&quot;&gt;Convergent and Divergent Series&lt;/h2&gt;
&lt;p&gt;Recall from the previous sections that if $a_1,a_2,\dots,a_n,\dots$ is a &lt;em&gt;sequence&lt;/em&gt; of numbers, then
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots\tag{1}\label{1}
\end{equation}
is called an &lt;em&gt;infinite series&lt;/em&gt;. We begin by establishing the sequence of &lt;em&gt;partial sums&lt;/em&gt;
\begin{align}
s_1&amp;amp;=a_1 \\ s_2&amp;amp;=a_1+a_2 \\ &amp;amp;\,\vdots \\ s_n&amp;amp;=a_1+a_2+\dots+a_n \\ &amp;amp;\,\vdots
\end{align}
The series \eqref{1} is said to be &lt;strong&gt;convergent&lt;/strong&gt; if the sequences $\{s_n\}$ converges. And if $\lim s_n=s$, then we say that \eqref{1} converges to $s$, or that $s$ is the sum of the series.
\begin{equation}
\sum_{n=1}^{\infty}a_n=s
\end{equation}
If the series does not converge, we say that it &lt;strong&gt;diverges&lt;/strong&gt; or is &lt;strong&gt;divergent&lt;/strong&gt;, and no sum is assigned to it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Examples&lt;/strong&gt; (&lt;em&gt;harmonic series&lt;/em&gt;)&lt;br /&gt;
Let’s consider the convergence of &lt;em&gt;harmonic series&lt;/em&gt;
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots\tag{2}\label{2}
\end{equation}
Let $m$ be a positive integer and choose $n&amp;gt;2^{m+1}$. We have
\begin{align}
s_n&amp;amp;&amp;gt;1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\dots+\frac{1}{2^{m+1}} \\ &amp;amp;=\left(1+\frac{1}{2}\right)+\left(\frac{1}{3}+\frac{1}{4}\right)+\left(\frac{1}{5}+\ldots+\frac{1}{8}\right)+\ldots+\left(\frac{1}{2^m+1}+\ldots+\frac{1}{2^{m+1}}\right) \\ &amp;amp;&amp;gt;\frac{1}{2}+2.\frac{1}{4}+4.\frac{1}{8}+\ldots+2^m.\frac{1}{2^{m+1}} \\ &amp;amp;=(m+1)\frac{1}{2}
\end{align}
This proves that $s_n$ can be made larger than the sum of any number of $\frac{1}{2}$’s and therefore as large as we please, by taking $n$ large enough, so the $\{s_n\}$ are unbounded, which leads to that \eqref{2} is a divergent series.
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{n}=1+\frac{1}{2}+\frac{1}{3}+\ldots=\infty
\end{equation}&lt;/p&gt;

&lt;p&gt;The simplest general principle that is useful to study the convergence of a series is the &lt;strong&gt;$\mathbf{n}$-th term test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;nth-term-test&quot;&gt;$\mathbf{n}$-th term test&lt;/h3&gt;
&lt;p&gt;If the series $\{a_n\}$ converges, then $a_n\to 0$ as $n\to\infty$; or equivalently, if $\neg(a_n\to0)$ as $n\to\infty$, then the series must necessarily diverge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
When $\{a_n\}$ converges, as $n\to\infty$ we have
\begin{equation}
a_n=s_n-s_{n-1}\to s-s=0
\end{equation}
This result shows that $a_n\to 0$ is a necessary condition for convergence. However, it is not a sufficient condition; i.e., it does not imply the convergence of the series when $a_n\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;h2 id=&quot;gen-props-conv-series&quot;&gt;General Properties of Convergent Series&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Any finite number of 0’s can be inserted or removed anywhere in a series without affecting its convergence behavior or its sum (in case it converges).&lt;/li&gt;
  &lt;li&gt;When two convergent series are added term by term, the resulting series converges to the expected sum; i.e., if $\sum_{n=1}^{\infty}a_n=s$ and $\sum_{n=1}^{\infty}b_n=t$, then
\begin{equation}
\sum_{n=1}^{\infty}(a_n+b_n)=s+t
\end{equation}
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  Let $\{s_n\}$ and $\{t_n\}$ respectively be the sequences of partial sums of $\sum_{n=1}^{\infty}a_n$ and $\sum_{n=1}^{\infty}b_n$. As $n\to\infty$ we have
  \begin{align}
  (a_1+b_1)+(a_2+b_2)+\dots+(a_n+b_n)&amp;amp;=\sum_{i=1}^{n}a_i+\sum_{i=1}^{n}b_i \\ &amp;amp;=s_n+t_n\to s+t
  \end{align}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Similarly, $\sum_{n=1}^{\infty}(a_n-b_n)=s-t$ and $\sum_{n=1}^{\infty}ca_n=cs$ for any constant $c$.&lt;/li&gt;
  &lt;li&gt;Any finite number of terms can be added or subtracted at the beginning of a convergent series without disturbing its convergence, and the sum of various series are related in the expected way.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
  If $\sum_{n=1}^{\infty}a_n=s$, then
  \begin{equation}
  \lim_{n\to\infty}(a_0+a_1+a_2+\dots+a_n)=\lim_{n\to\infty} a_0+\lim_{n\to\infty}(a_1+a_2+\dots+a_n)=a_0+s
  \end{equation}&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;series-nonneg-ct&quot;&gt;Series of Nonnegative terms. Comparison Tests&lt;/h2&gt;
&lt;p&gt;The easiest infinite series to work with are those whose terms are all nonnegative numbers. The reason, as we saw in the above &lt;a href=&quot;#conv-div-series&quot;&gt;section&lt;/a&gt;, is that if $a_n\geq0$, then the series $\sum a_n$ converges if and only if its sequence $\{s_n\}$ of partial sums is bounded (since $s_{n+1}=s_n+a_{n+1}$).&lt;/p&gt;

&lt;p&gt;Thus, in order to establish the convergence of a series of nonnegative terms, it suffices to show that its terms approach zero fast enough, or at least as fast as the terms of a known convergent series of nonnegative terms to keep the partial sums bounded.&lt;/p&gt;

&lt;h3 id=&quot;comparison-test&quot;&gt;Comparison test&lt;/h3&gt;
&lt;p&gt;If $0\leq a_n\leq b_n$, then&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;$\sum a_n$ converges if $\sum b_n$ converges.&lt;/li&gt;
  &lt;li&gt;$\sum b_n$ diverges if $\sum a_n$ diverges.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
If $s_n, t_n$ respectively are the partial sums of $\sum a_n,\sum b_n$, then
\begin{equation}
0\leq s_n=\sum_{i=1}^{n}a_i\leq\sum_{i=1}^{n}b_i=t_n
\end{equation}
Then if $\{t_n\}$ is bounded, then so is $\{s_n\}$; and if $\{s_n\}$ is unbounded, then so is $\{t_n\}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;&lt;br /&gt;
Consider convergence behavior of two series
\begin{equation}
\sum_{n=1}^{\infty}\frac{1}{2^n+1};\hspace{2cm}\sum_{n=1}^{\infty}\frac{1}{\ln n}
\end{equation}
The first series converges, because
\begin{equation}
\frac{1}{2^n+1}&amp;lt;\frac{1}{2^n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{2^n}=1$, which is a convergent series. At the same time, the second series diverges, since
\begin{equation}
\frac{1}{n}\leq\frac{1}{\ln n}
\end{equation}
and $\sum_{n=1}^{\infty}\frac{1}{n}$ diverges.&lt;/p&gt;

&lt;p&gt;One thing worth remarking is that the condition $0\leq a_n\leq b_n$ for the comparison test need not hold for all $n$, but only for all $n$ from some point on.&lt;/p&gt;

&lt;p&gt;The comparison test is simple, but in some cases where it is difficult to establish the necessary inequality between the n-th terms of the two series. And since limits are often easier to work with than inequalities, we have the following test.&lt;/p&gt;

&lt;h3 id=&quot;limit-comparison-test&quot;&gt;Limit comparison test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n, \sum b_n$ are series with positive terms such that
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=1\tag{3}\label{3}
\end{equation}
then either both series converge or both series diverge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
we observe that \eqref{3} implies that for all sufficient large $n$, we have
\begin{align}
\frac{1}{2}&amp;amp;\leq\frac{a_n}{b_n}\leq 2 \\ \text{or}\hspace{1cm}\frac{1}{2}b_n&amp;amp;\leq a_n\leq 2b_n
\end{align}
which leads to the fact that $\sum a_n$ and $\sum b_n$ have the same convergence behavior.&lt;/p&gt;

&lt;p&gt;The condition \eqref{3} can be generalized by
\begin{equation}
\lim_{n\to\infty}\frac{a_n}{b_n}=L,
\end{equation}
where $0&amp;lt;L&amp;lt;\infty$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; ($p$&lt;em&gt;-series&lt;/em&gt;)&lt;br /&gt;
Consider the convergence behavior of the series
\begin{equation}
\sum_{n=1}^{\infty}\dfrac{1}{n^p}=1+\dfrac{1}{2^p}+\dfrac{1}{3^p}+\dfrac{1}{4^p}+\ldots,\tag{4}\label{4}
\end{equation}
where $p$ is a positive constant.&lt;/p&gt;

&lt;p&gt;If $p\leq 1$, then $n^p\leq n$ or $\frac{1}{n}\leq\frac{1}{n^p}$. Thus, by comparison with the harmonic series $\sum\frac{1}{n}$, we have that \eqref{4} diverges.&lt;/p&gt;

&lt;p&gt;If $p&amp;gt;1$, let $n$ be given and choose $m$ so that $n&amp;lt;2^m$. Then
\begin{align}
s_n&amp;amp;\leq s_{2^m-1} \\ &amp;amp;=1+\left(\dfrac{1}{2^p}+\dfrac{1}{3^p}\right)+\left(\dfrac{1}{4^p}+\ldots+\dfrac{1}{7^p}\right)+\ldots+\left[\dfrac{1}{(2^{m-1})^p}+\ldots+\dfrac{1}{(2^m-1)^p}\right] \\ &amp;amp;\leq 1+\dfrac{2}{2^p}+\dfrac{4}{4^p}+\ldots+\dfrac{2^{m-1}}{(2^{m-1})^p}
\end{align}
Let $a=\frac{1}{2^{p-1}}$, then $a&amp;lt;1$ since $p&amp;gt;1$, and
\begin{equation}
s_n\leq 1+a+a^2+\ldots+a^{m-1}=\dfrac{1-a^m}{1-a}&amp;lt;\dfrac{1}{1-a}
\end{equation}
which proves that $\{s_n\}$ has an upper bound. Thus \eqref{4} converges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 1&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If a convergent series of nonnegative terms is rearranged in any manner, then the resulting series also converges and has the same sum.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Consider two series $\sum a_n$ and $\sum b_n$, where $\sum a_n$ is a convergent series of nonnegative terms and $\sum b_n$ is formed form $\sum a_n$ by rearranging its terms.&lt;/p&gt;

&lt;p&gt;Let $p$ be a positive integer and consider the $p$-partial sum $t_p=b_1+\ldots+b_p$ of $\sum b_n$. Since each $b$ is some $a$, then there exists an $m$ such that each term in $t_p$ is one of the terms in $s_m=a_1+\ldots+a_m$. This shows us that $t_p\leq s_m\leq s$. Thus, $\sum b_n$ converges to a sum $t\leq s$.&lt;/p&gt;

&lt;p&gt;On the other hand, $\sum a_n$ is also a rearrangement of $\sum b_n$, so by the same procedure, similarly we have that $s\leq t$, and therefore $t=s$.&lt;/p&gt;

&lt;h2 id=&quot;int-test-euler-c&quot;&gt;The Integral test. Euler’s constant&lt;/h2&gt;
&lt;p&gt;In this section, we will be going through a more detailed class of infinite series with nonnegative terms which is those whose terms form a decreasing sequence of positive numbers.&lt;/p&gt;

&lt;p&gt;We begin by considering a series
\begin{equation}
\sum_{n=1}^{\infty}a_n=a_1+a_2+\ldots+a_n+\ldots
\end{equation}
whose terms are positive and decreasing. Suppose $a_n=f(n)$, as shown is &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/integral-test.png&quot; alt=&quot;integral test&quot; width=&quot;500px&quot; height=&quot;230px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 1&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;On the left of this figure we see that the rectangles of areas $a_1,a_2,\dots,a_n$ have a greater combined area than the area under the curve from $x=1$ to $x=n+1$, so
\begin{equation}
a_1+a_2+\dots+a_n\geq\int_{1}^{n+1}f(x)\,dx\geq\int_{1}^{n}f(x)\,dx\tag{5}\label{5}
\end{equation}
On the right side of the figure, the rectangles lie under the curve, which makes
\begin{align}
a_2+a_3+\dots+a_n&amp;amp;\leq\int_{1}^{n}f(x)\,dx \\ a_1+a_2+\dots+a_n&amp;amp;\leq a_1+\int_{1}^{n}f(x)\,dx\tag{6}\label{6}
\end{align}
Putting \eqref{5} and \eqref{6} together we have
\begin{equation}
\int_{1}^{n}f(x)\,dx\leq a_1+a_2+\dots+a_n\leq a_1+\int_{1}^{n}f(x)\,dx\tag{7}\label{7}
\end{equation}
The result we obtained in \eqref{7} allows us to establish the &lt;strong&gt;integral test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;integral-test&quot;&gt;Integral test&lt;/h3&gt;

&lt;p&gt;If $f(x)$ is a positive decreasing function for $x\geq1$ such that $f(n)=a_n$ for each positive integer $n$, then the series and integral
\begin{equation}
\sum_{n=1}^{\infty}a_n;\hspace{2cm}\int_{1}^{\infty}f(x)\,dx
\end{equation}
converge or diverge together.&lt;/p&gt;

&lt;p&gt;The integral test holds for any interval of the form $x\geq k$, not just for $x\geq 1$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; (&lt;em&gt;Abel’s series&lt;/em&gt;)&lt;br /&gt;
Let’s consider the convergence behavior of the series
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n\ln n}\tag{8}\label{8}
\end{equation}
By the integral test, we have that \eqref{8} diverges, because
\begin{equation}
\sum_{2}^{\infty}\frac{dx}{x\ln x}=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x\ln x}=\lim_{b\to\infty}\left(\ln\ln x\Big|_{2}^{b}\right)=\lim_{b\to\infty}\left(\ln\ln b-\ln\ln 2\right)=\infty
\end{equation}
More generally, if $p&amp;gt;0$, then
\begin{equation}
\sum_{n=2}^{\infty}\frac{1}{n(\ln n)^p}
\end{equation}
converges if $p&amp;gt;1$ and diverges if $0&amp;lt;p\leq 1$. For if $p\neq 1$, we have
\begin{align}
\int_{2}^{\infty}\frac{dx}{x(\ln x)^p}&amp;amp;=\lim_{b\to\infty}\int_{2}^{b}\frac{dx}{x(\ln x)^p} \\ &amp;amp;=\lim_{b\to\infty}\left[\dfrac{(\ln x)^{1-p}}{1-p}\Bigg|_2^b\right] \\ &amp;amp;=\lim_{b\to\infty}\left[\dfrac{(\ln b)^{1-p}-(\ln 2)^{1-p}}{1-p}\right]
\end{align}
exists if and only if $p&amp;gt;1$.&lt;/p&gt;

&lt;h3 id=&quot;euler-c&quot;&gt;Euler’s constant&lt;/h3&gt;
&lt;p&gt;From \eqref{7} we have that
\begin{equation}
0\leq a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\leq a_1
\end{equation}
Denoting $F(n)=a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx$, the above expression becomes
\begin{equation}
0\leq F(n)\leq a_1
\end{equation}
Moreover, $\{F(n)\}$ is a decreasing sequence, because
\begin{align}
F(n)-F(n+1)&amp;amp;=\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]-\left[a_1+a_2+\ldots+a_{n+1}-\int_{1}^{n+1}f(x)\,dx\right] \\ &amp;amp;=\int_{n}^{n+1}f(x)\,dx-a_{n+1}\geq 0
\end{align}
where the last step can be seen by observing the right side of &lt;strong&gt;&lt;em&gt;Figure 1&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Since any decreasing sequence of nonnegative numbers converges, we have that
\begin{equation}
L=\lim_{n\to\infty}F(n)=\lim_{n\to\infty}\left[a_1+a_2+\ldots+a_n-\int_{1}^{n}f(x)\,dx\right]\tag{9}\label{9}
\end{equation}
exists and satisfies the inequalities $0\leq L\leq a_1$.&lt;/p&gt;

&lt;p&gt;Let $a_n=\frac{1}{n}$ and $f(x)=\frac{1}{x}$, the last quantity in \eqref{9} becomes
\begin{equation}
\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)\tag{10}\label{10}
\end{equation}
since
\begin{equation}
\int_{1}^{n}\dfrac{dx}{x}=\ln x\Big|_1^n=\ln n
\end{equation}
The value of the limit \eqref{10} is called &lt;strong&gt;Euler’s constant&lt;/strong&gt; (denoted as $\gamma$).
\begin{equation}
\gamma=\lim_{n\to\infty}\left(1+\dfrac{1}{2}+\ldots+\dfrac{1}{n}-\ln n\right)
\end{equation}&lt;/p&gt;

&lt;h2 id=&quot;ratio-root&quot;&gt;The Ratio test. Root test&lt;/h2&gt;

&lt;h3 id=&quot;ratio-test&quot;&gt;Ratio test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n$ is a series of positive terms such that
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}}{a_n}=L,\tag{11}\label{11}
\end{equation}
then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;if $L&amp;lt;1$, the series &lt;em&gt;converges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L&amp;gt;1$, the series &lt;em&gt;diverges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L=1$, the test is &lt;em&gt;inconclusive&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Let $L&amp;lt;1$ and choose any number $r$ such that $L&amp;lt;r&amp;lt;1$. From \eqref{11}, we have that there exists an $n_0$ such that
\begin{align}
\dfrac{a_{n+1}}{a_n}&amp;amp;\leq r=\dfrac{r^{n+1}}{r_n},\hspace{1cm}\forall n\geq n_0 \\ \dfrac{a_{n+1}}{r^{n+1}}&amp;amp;\leq\dfrac{a_n}{r^n},\hspace{2cm}\forall n\geq n_0
\end{align}
which means that $\{\frac{a_n}{r^n}\}$ is a decreasing sequence for $n\geq n_0$; in particular, $\frac{a_n}{r^n}\leq\frac{a_{n_0}}{r^{n_0}}$ for $n\geq n_0$. Thus, if we let $K=\frac{a_{n_0}}{r^{n_0}}$, then we get
\begin{equation}
a_n\leq Kr^n,\hspace{1cm}\forall n\geq n_0\tag{12}\label{12}
\end{equation}
However, $\sum Kr^n$ converges since $r&amp;lt;1$. Hence, by the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, \eqref{12} implies that $\sum a_n$ converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When $L&amp;gt;1$, we have that $\frac{a_{n+1}}{a_n}\geq 1$, or equivalently $a_{n+1}\geq a_n$, for all $n\geq n_0$, for some constant $n_0$. That means $\neg(a_n\to 0)$ as $n\to\infty$ (since $\sum a_n$ is a series of positive terms).&lt;br /&gt;
By the &lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we know that the series diverges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Consider the $p$-series $\sum\frac{1}{n^p}$. For all values of $p$, as $n\to\infty$ we have
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^p}{(n+1)^p}=\left(\dfrac{n}{n+1}\right)^p\to 1
\end{equation}
As in the above example, we have that this series converges if $p&amp;gt;1$ and diverges if $p\leq 1$.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;root-test&quot;&gt;Root test&lt;/h3&gt;
&lt;p&gt;If $\sum a_n$ is a series of nonnegative terms such that
\begin{equation}
\lim_{n\to\infty}\sqrt[n]{a_n}=L,\tag{13}\label{13}
\end{equation}
then&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;if $L&amp;lt;1$, the series &lt;em&gt;converges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L&amp;gt;1$, the series &lt;em&gt;diverges&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;if $L=1$, the test is &lt;em&gt;inconclusive&lt;/em&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Let $L&amp;lt;1$ and $r$ is any number such that $L&amp;lt;r&amp;lt;1$. From \eqref{13}, we have that there exist $n_0$ such that
\begin{align}
\sqrt[n]{a_n}&amp;amp;\leq r&amp;lt;1,\hspace{1cm}\forall n\geq n_0 \\ a_n&amp;amp;\leq r^n&amp;gt;1,\hspace{1cm}\forall n\geq n_0
\end{align}
And since the geometric series $\sum r^n$ converges, we clearly have that $\sum a_n$ also converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;gt;1$, then $\sqrt[n]{a_n}\geq 1$ for all $n\geq n_0$, for some $n_0$, so $a_n\geq 1$ for all $n\geq n_0$. That means as $n\to\infty$, $\neg(a_n\to 0)$. Therefore, by the &lt;a href=&quot;#nth-term-test&quot;&gt;$n$-th term test&lt;/a&gt;, we have that the series diverges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For $L=1$, we provide 2 examples. One is the divergent series $\sum\frac{1}{n}$ and the other is the convergent series $\sum\frac{1}{n^2}$ (since $\sqrt[n]{n}\to 1$ as $n\to\infty$).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;extended-ratio-test&quot;&gt;The Extended Ratio tests of Raabe and Gauss&lt;/h3&gt;

&lt;h4 id=&quot;kummers-theorem&quot;&gt;Kummer’s theorem&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 2&lt;/strong&gt; (&lt;em&gt;Kummer’s&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Assume that $a_n&amp;gt;0,b_n&amp;gt;0$ and $\sum\frac{1}{b_n}$ diverges. If
\begin{equation}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)=L,\tag{14}\label{14}
\end{equation}
then $\sum a_n$ converges if $L&amp;gt;0$ and diverges if $L&amp;lt;0$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;gt;0$, then there exists $h$ such that $L&amp;gt;h&amp;gt;0$. From \eqref{14}, for some positive integer $n_0$ we have
\begin{align}
b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}&amp;amp;\geq h&amp;gt;0,\hspace{1cm}\forall n\geq n_0 \\ a_n b_n-a_{n+1}b_{n+1}&amp;amp;\geq ha_n&amp;gt;0,\hspace{1cm}\forall n\geq n_0\tag{15}\label{15}
\end{align}
Hence, $\{a_n b_n\}$ is a decreasing sequence of positive numbers for $n\geq n_0$, so $K=\lim a_n b_n$ exists.&lt;br /&gt;
Moreover, we have that
\begin{equation}
\sum_{n=n_0}^{\infty}a_nb_n-a_{n+1}b_{n+1}=a_{n_0}b_{n_0}-\lim_{n\to\infty}a_nb_n=a_{n_0}b_{n_0}-K
\end{equation}
Therefore, by \eqref{15} and the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we can conclude that $\sum ha_n$ converges, which means that $\sum a_n$ also converges.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $L&amp;lt;0$, for some positive integer $n_0$ we have
\begin{equation}
a_nb_n-a_{n+1}b_{n+1}\leq 0,\hspace{1cm}\forall n\geq n_0
\end{equation}
Hence, $\{a_nb_n\}$ is a increasing sequence of positive number for all $n\geq n_0$, for some positive integer $n_0$. This also means for all $n\geq n_0$,
\begin{align}
a_nb_n&amp;amp;\geq a_{n_0}b_{n_0} \\ a_n&amp;amp;\geq (a_{n_0}b_{n_0}).\dfrac{1}{b_n}
\end{align}
Therefore $\sum a_n$ diverges (since $\sum\frac{1}{b_n}$ diverges).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;raabes-test&quot;&gt;Raabe’s test&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 3&lt;/strong&gt; (&lt;em&gt;Raabe’s test&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n},
\end{equation}
where $A_n\to 0$, then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Take $n=b_n$ in &lt;em&gt;Kummber’s theorem&lt;/em&gt;. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;amp;=\lim\left[n-\left(1-\dfrac{A}{n}+\dfrac{A_n}{n}\right)(n+1)\right] \\ &amp;amp;=\lim\left[-1+\dfrac{A(n+1)}{n}-\dfrac{A_n(n+1)}{n}\right] \\ &amp;amp;=A-1
\end{align}
and by &lt;em&gt;Kummer’s theorem&lt;/em&gt; we have that $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Raabe’s test&lt;/em&gt; can be formulated as followed: If $a_n&amp;gt;0$ and
\begin{equation}
\lim n\left(1-\dfrac{a_{n+1}}{a_n}\right)=A,
\end{equation}
then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A&amp;lt;1$.&lt;/p&gt;

&lt;p&gt;When $A=1$ in &lt;em&gt;Raabe’s test&lt;/em&gt;, we turn to &lt;strong&gt;Gauss’s test&lt;/strong&gt;&lt;/p&gt;

&lt;h4 id=&quot;gausss-test&quot;&gt;Gauss’s test&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Theorem 4&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{A}{n}+\dfrac{A_n}{n^{1+c}},
\end{equation}
where $c&amp;gt;0$ and $A_n$ is bounded as $n\to\infty$, then $\sum a_n$ converges if $A&amp;gt;1$ and diverges if $A\leq 1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;If $A\neq 1$, the statement follows exactly from &lt;em&gt;Raabe’s test&lt;/em&gt;, since $\frac{A_n}{n^c}\to 0$ as $n\to\infty$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If $A=1$, we begin by taking $b_n=n\ln n$ in &lt;em&gt;Kummer’s theorem&lt;/em&gt;. Then
\begin{align}
\lim\left(b_n-\dfrac{a_{n+1}}{a_n}.b_{n+1}\right)&amp;amp;=\lim\left[n\ln n-\left(1-\dfrac{1}{n}+\dfrac{A_n}{n^{1+c}}\right)(n+1)\ln(n+1)\right] \\ &amp;amp;=\lim\left[n\ln n-\dfrac{n^2-1}{n}\ln(n+1)-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;amp;=\lim\left[n\ln\left(\dfrac{n}{n+1}\right)+\dfrac{\ln(n+1)}{n}-\dfrac{n+1}{n}.\dfrac{A_n\ln(n+1)}{n^c}\right] \\ &amp;amp;=-1+0-0=-1&amp;lt;0,
\end{align}
where in fourth step we use the &lt;em&gt;Stolz–Cesàro theorem&lt;/em&gt;&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;. Therefore, by &lt;em&gt;Kummer’s theorem&lt;/em&gt;, we have that the series is divergent.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem 5&lt;/strong&gt; (&lt;em&gt;Gauss’s test&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;If $a_n&amp;gt;0$ and
\begin{equation}
\dfrac{a_{n+1}}{a_n}=\dfrac{n^k+\alpha n^{k-1}+\ldots}{n^k+\beta n^{k-1}+\ldots},\tag{16}\label{16}
\end{equation}
then $\sum a_n$ converges if $\beta-\alpha&amp;gt;1$ and diverges if $\beta-\alpha\leq 1$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
If the quotient on the right of \eqref{16} is worked out by long division, we get
\begin{equation}
\dfrac{a_{n+1}}{a_n}=1-\dfrac{\beta-\alpha}{n}+\dfrac{A_n}{n^2},
\end{equation}
where $A_n$ is a quotient of the form
\begin{equation}
\dfrac{\gamma n^{k-2}+\ldots}{n^{k-2}+\ldots}
\end{equation}
and is therefore clearly bounded as $n\to\infty$. The statement now follows from &lt;strong&gt;Theorem 4&lt;/strong&gt; with $c=1$.&lt;/p&gt;

&lt;h2 id=&quot;alt-test-abs-conv&quot;&gt;The Alternating Series test. Absolute Convergence&lt;/h2&gt;
&lt;p&gt;Previously, we have been working with series of positive terms and nonnegative terms. It’s time to consider series with both positive and negative terms. The simplest are those whose terms are alternatively positive and negative.&lt;/p&gt;

&lt;h3 id=&quot;alt-series&quot;&gt;Alternating Series&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Alternating series&lt;/strong&gt; is series with the form
\begin{equation}
\sum_{n=1}^{\infty}(-1)^{n+1}a_n=a_1-a_2+a_3-a_4+\ldots,\tag{17}\label{17}
\end{equation}
where $a_n$’s are all positive numbers.&lt;/p&gt;

&lt;p&gt;From the definition of alternating series, we establish &lt;strong&gt;alternating series test&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;alt-series-test&quot;&gt;Alternating Series test&lt;/h3&gt;
&lt;p&gt;If the alternating series \eqref{17} has the property that&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;$a_1\geq a_2\geq a_3\geq\ldots$&lt;/li&gt;
  &lt;li&gt;$a_n\to 0$ as $n\to\infty$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;then $\sum a_n$ converges.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
On the one hand, we have that a typical even partial sum $s_{2n}$ can be written as
\begin{equation}
s_{2n}=(a_1-a_2)+(a_3-a_4)+\ldots+(a_{2n-1}-a_{2n}),
\end{equation}
where each expression in parentheses is nonnegative since $\{a_n\}$ is a decreasing sequence. Hence, we also have that $s_{2n}\leq s_{2n+2}$, which leads to the result that the even partial sums form an increasing sequence.&lt;/p&gt;

&lt;p&gt;Moreover, we can also display $s_{2n}$ as
\begin{equation}
s_{2n}=a_1-(a_2-a_3)-(a_4-a_5)-\ldots-(a_{2n-2}-a_{2n-1})-a_{2n},
\end{equation}
where each expression in parentheses once again is nonnegative. Thus, we have that $s_{2n}\leq a_1$, so ${s_{2n}}$ has an upper bound. Since every bounded increasing sequence converges, there exists a number $s$ such that
\begin{equation}
\lim_{n\to\infty}s_{2n}=s
\end{equation}&lt;/p&gt;

&lt;p&gt;On the other hand, the odd partial sums approach the same limit, because
\begin{align}
s_{2n+1}&amp;amp;=a_1-a_2+a_3-a_4+\ldots-a_{2n}+a_{2n+1} \\ &amp;amp;=s_{2n}+a_{2n+1}
\end{align}
and therefore
\begin{equation}
\lim_{n\to\infty}s_{2n+1}=\lim_{n\to\infty}s_{2n}+\lim_{n\to\infty}a_{2n+1}=s+0=s
\end{equation}
Since both sequence of even sums and sequence of odd partial sums converges to $s$ as $n$ tends to infinity, this shows us that $\{s_n\}$ also converges to $s$, and therefore the alternating series \eqref{17} converges to the sum $s$.&lt;/p&gt;

&lt;h3 id=&quot;abs-conv&quot;&gt;Absolute Convergence&lt;/h3&gt;
&lt;p&gt;A series $\sum a_n$ is said to be &lt;strong&gt;absolutely convergent&lt;/strong&gt; if $\sum\vert a_n\vert$ converges.&lt;/p&gt;

&lt;p&gt;These are some properties of absolute convergence.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Absolute convergence implies convergence.
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Suppose that $\sum a_n$ is an absolutely convergent series, or $\sum\vert a_n\vert$ converges. We have that
\begin{equation}
0\leq a_n+\vert a_n\vert\leq 2\vert a_n\vert
\end{equation}
And since $\sum 2\vert a_n\vert$ converges, by &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, we also have that $\sum(a_n+\vert a_n\vert)$ converges.&lt;br /&gt;
Since both $\sum\vert a_n\vert$ and $\sum(a_n+\vert a_n\vert)$ converge, so does their difference, which is $\sum a_n$.&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A convergent series that is not absolutely convergent is said to be &lt;strong&gt;conditionally convergent&lt;/strong&gt;.
    &lt;ul&gt;
      &lt;li&gt;Any conditionally convergent series can be made to converge to any given number as its sum, or even to diverge, by &lt;em&gt;suitably changing the order of its terms without changing the terms themselves&lt;/em&gt; (check out &lt;strong&gt;Theorem 8&lt;/strong&gt; to see the proof).&lt;/li&gt;
      &lt;li&gt;On the other hand, any absolutely convergent series can be rearranged in any manner without changing its convergence behavior or its sum (check out &lt;strong&gt;Theorem 7&lt;/strong&gt; to see the proof).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;abs-vs-cond&quot;&gt;Absolute vs Conditionally Convergence&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;Consider a series $\sum a_n$ and define $p_n$ and $q_n$ by
\begin{align}
p_n&amp;amp;=\dfrac{\vert a_n\vert+a_n}{2} \\ q_n&amp;amp;=\dfrac{\vert a_n\vert-a_n}{2}
\end{align}
If $\sum a_n$ converges conditionally, then both $\sum p_n$ and $\sum q_n$ diverges.&lt;br /&gt;
If $\sum a_n$ converges absolutely, then $\sum p_n$ and $\sum q_n$ both converge and the sums of these series are related by the equation&lt;/em&gt;
\begin{equation}
\sum a_n=\sum p_n-\sum q_n
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
From the formulas of $p_n$ and $q_n$, we have
\begin{align}
a_n&amp;amp;=p_n-q_n\tag{18}\label{18} \\ \vert a_n\vert&amp;amp;=p_n+q_n\tag{19}\label{19}
\end{align}&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;We begin by proving the first statement.&lt;br /&gt;
When $\sum a_n$ converges, from \eqref{18}, we have $\sum p_n$ and $\sum q_n$ both must have the same convergence behavior (i.e., converge or diverge at the same time).&lt;br /&gt;
If they both converge, then from \eqref{19}, we have that $\sum\vert a_n\vert$ converges, contrary to the hypothesis, so $\sum p_n$ and $\sum q_n$ are both divergent.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To prove the second statement, we assume that $\sum\vert a_n\vert$ converges. We have
\begin{equation}
p_n=\dfrac{\vert a_n\vert+a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which shows us that $\sum p_n$ converges. Similarly, for $q_n$, we have
\begin{equation}
q_n=\dfrac{\vert a_n\vert-a_n}{2}\leq\dfrac{2\vert a_n\vert}{2}=\vert a_n\vert
\end{equation}
which also lets us obtain that $\sum q_n$ converges.&lt;br /&gt;
Therefore
\begin{equation}
\sum p_n-\sum q_n=\sum(p_n-q_n)=\sum a_n
\end{equation}
&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Theorem 7&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If $\sum a_n$ is an absolutely convergent series with sum $s$, and if $a_n$’s are rearranged in any way to from a new series $\sum b_n$, then this new series is also absolutely convergent with sum $s$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $\sum\vert a_n\vert$ is a convergent series of nonnegative terms with sum $s$ and since the $b_n$’s are just the $a_n$’s in a different order, it follows from &lt;strong&gt;Theorem 1&lt;/strong&gt; that $\sum\vert b_n\vert$ also converges to $s$, and therefore $\sum b_n$ is absolutely convergent with sum $t$, for some positive $t$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 6&lt;/strong&gt; allows us to write
\begin{equation}
s=\sum a_n=\sum p_n-\sum q_n
\end{equation}
and
\begin{equation}
t=\sum b_n=\sum P_n-\sum Q_n
\end{equation}
where each of the series on the right is convergent and consists of nonnegative. But the $P_n$’s and $Q_n$’s are simply the $p_n$’s and $q_n$’s in a different order. Hence, by &lt;strong&gt;Theorem 1&lt;/strong&gt;, we have $\sum P_n=\sum p_n$ and $\sum Q_n=\sum q_n$. And therefore, $t=s$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 8&lt;/strong&gt; (&lt;em&gt;Riemann’s rearrangement theorem&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Let $\sum a_n$ be a conditionally convergent series. Then its terms can be rearranged to yield a convergent series whose sum is an arbitrary preassigned number, or a series that diverges to $\infty$, or a series that diverges to $-\infty$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $\sum a_n$ converges conditionally, we begin by using &lt;strong&gt;Theorem 6&lt;/strong&gt; to form the two divergent series of nonnegative terms $\sum p_n$ and $\sum q_n$.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;To prove the first statement, let $s$ be any number and construct a rearrangement of the given series as follows. Start by writing down $p$’s in order until the partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}
\end{equation}
is first $\geq s$; next we continue with $-q$’s until the total partial sum
\begin{equation}
p_1+p_2+\ldots+p_{n_1}-q_1-q_2-\ldots-q_{m_1}
\end{equation}
is first $\leq s$; then we continue with $p$’s until the total partial sum
\begin{equation}
p_1+\ldots+p_{n_1}-q_1-\ldots-q_{m_1}+p_{n_1+1}+\ldots+p_{n_2}
\end{equation}
is first $\geq s$; and so on.&lt;br /&gt;
The possibility of each of these steps is guaranteed by the divergence of $\sum p_n$ and $\sum q_n$; and the resulting rearrangement of $\sum a_n$ converges to $s$ because $p_n\to 0$ and $q_n\to 0$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In order to make the rearrangement diverge to $\infty$, it suffices to write down enough $p$’s to yield
\begin{equation}
p_1+p_2+\ldots+p_{n_1}\geq 1,
\end{equation}
then to insert $-q_1$, and then to continue with $p$’s until
\begin{equation}
p_1+\ldots+p_{n_1}-q_1+p_{n_1+1}+\ldots+p_{n_2}\geq 2,
\end{equation}
then to insert $-q_2$, and so on.&lt;br /&gt;
We can produce divergence to $-\infty$ by a similar construction.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One of the principal application of &lt;strong&gt;Theorem 7&lt;/strong&gt; relates to the &lt;em&gt;multiplication of series&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If we multiply two series
\begin{align}
\sum_{n=0}^{\infty}a_n&amp;amp;=a_0+a_1+\ldots+a_n+\ldots\tag{20}\label{20} \\ \sum_{n=0}^{\infty}b_n&amp;amp;=b_0+b_1+\ldots+b_n+\ldots\tag{21}\label{21}
\end{align}
by forming all possible product $a_i b_j$ (as in the case of finite sums), then we obtain the following doubly infinite array&lt;/p&gt;
&lt;figure&gt;
	&lt;img src=&quot;/assets/images/2021-09-06/series-mult.png&quot; alt=&quot;series multiplication&quot; width=&quot;300px&quot; height=&quot;210px&quot; style=&quot;display: block; margin-left: auto; margin-right: auto;&quot; /&gt;
	&lt;figcaption style=&quot;text-align: center;font-style: italic;&quot;&gt;&lt;b&gt;Figure 2&lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;There are various ways of arranging these products into a single infinite series, of which two are important. The first one is to group them by diagonals, as indicated in the arrows in &lt;strong&gt;Figure 2&lt;/strong&gt;:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1)+(a_0b_2+a_1b_1+a_2b_0)+\ldots\tag{22}\label{22}
\end{equation}
This series can be defined as $\sum_{n=0}^{\infty}c_n$, where
\begin{equation}
c_n=a_0b_n+a_1b_{n-1}+\ldots+a_nb_0
\end{equation}&lt;/p&gt;

&lt;p&gt;It is called the &lt;em&gt;product&lt;/em&gt; (or &lt;em&gt;Cauchy product&lt;/em&gt;) of the two series $\sum a_n$ and $\sum b_n$.&lt;/p&gt;

&lt;p&gt;The second crucial method of arranging these products into a series is by squares, as shown in &lt;strong&gt;Figure 2&lt;/strong&gt;:
\begin{equation}
a_0b_0+(a_0b_1+a_1b_1+a_1b_0)+(a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0)+\ldots\tag{23}\label{23}
\end{equation}
The advantage of this arrangement is that the $n$-th partial sum $s_n$ of \eqref{23} is given by
\begin{equation}
s_n=(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n)\tag{24}\label{24}
\end{equation}
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 9&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If the two series \eqref{20} and \eqref{21} have nonnegative terms and converges to $s$ and $t$, then their product \eqref{22} converges to $st$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
It is clear from \eqref{24} that \eqref{23} converges to $st$. Let’s denote the series \eqref{22} and \eqref{23} without parenthesis by $(22’)$ and $(23’)$.&lt;/p&gt;

&lt;p&gt;We have the series $(23’)$ of nonnegative terms still converges to $st$ because, for if $m$ is an integer such that $n^2\leq m\leq (n+1)^2$, then the $m$-th partial sum of $(23’)$ lies between $s_{n-1}$ and $s_n$, and both of these converge to $st$.&lt;/p&gt;

&lt;p&gt;By &lt;strong&gt;Theorem 7&lt;/strong&gt;, the terms of $(23’)$ can be rearranged to yield $(22’)$ without changing the sum $st$; and when parentheses are suitably inserted, we see that \eqref{8} converges to $st$.&lt;/p&gt;

&lt;p&gt;We now extend &lt;strong&gt;Theorem 9&lt;/strong&gt; to the case of absolute convergence.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Theorem 10&lt;/strong&gt;&lt;br /&gt;
&lt;em&gt;If the series $\sum_{n=0}^{\infty}a_n$ and $\sum_{n=0}^{\infty}b_n$ are absolutely convergent, with sum $s$ and $t$, then their product
\begin{multline}
\sum_{n=0}^{\infty}(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)=a_0b_0+(a_0b_1+a_1b_0)\,+ \\ (a_0b_2+a_1b_1+a_2b_0)+\ldots+(a_0b_n+a_1b_{n-1}+\ldots+a_nb_0)+\ldots\tag{25}\label{25}
\end{multline}
is absolutely convergent, with sum $st$.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
The series $\sum_{n=0}^{\infty}\vert a_n\vert$ and $\sum_{n=0}^{\infty}\vert b_n\vert$ are convergent and have nonnegative terms. So by the &lt;strong&gt;Theorem 9&lt;/strong&gt; above, their product
\begin{multline}
\vert a_0\vert\vert b_0\vert+\vert a_0\vert\vert b_1\vert+\vert a_1\vert\vert b_0\vert+\ldots+\vert a_0\vert\vert b_n\vert+\vert a_1\vert\vert b_{n-1}\vert+\ldots+\vert a_n\vert\vert b_0\vert+\ldots \\ =\vert a_0b_0\vert+\vert a_0b_1\vert+\vert a_1b_0\vert+\ldots+\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert+\ldots\tag{26}\label{26}
\end{multline}
converges, and therefore the series
\begin{equation}
a_0b_0+a_0b_1+a_1b_0+\ldots+a_0b_n+\ldots+a_nb_0+\ldots\tag{27}\label{27}
\end{equation}
is absolutely convergent. It follows from &lt;strong&gt;Theorem 7&lt;/strong&gt; that the sum of \eqref{27} will not change if we rearrange its terms and write it as
\begin{equation}
a_0b_0+a_0b_1+a_1b_1+a_1b_0+a_0b_2+a_1b_2+a_2b_2+a_2b_1+a_2b_0+\ldots\tag{28}\label{28}
\end{equation}
We now observe that the sum of the first $(n+1)^2$ terms of \eqref{28} is
\begin{equation}
(a_0+a_1+\ldots+a_n)(b_0+b_1+\ldots+b_n),
\end{equation}
so it is clear that \eqref{28}, and with it \eqref{27}, converges to $st$.&lt;/p&gt;

&lt;p&gt;Thus, \eqref{25} also converges to $st$, since \eqref{25} is retrieved by suitably inserted parentheses in \eqref{27}.&lt;/p&gt;

&lt;p&gt;Moreover, we also have
\begin{equation}
\vert a_0b_n+a_1b_{n-1}+\ldots+a_nb_0\vert\leq\vert a_0b_n\vert+\vert a_1b_{n-1}\vert+\ldots+\vert a_nb_0\vert
\end{equation}
and the series
\begin{equation}
\vert a_0b_0\vert+(\vert a_0b_1\vert+\vert a_1b_0\vert)+\ldots+(\vert a_0b_n\vert+\ldots+\vert a_nb_0\vert)+\ldots
\end{equation}
obtained from \eqref{26} by inserting parentheses. By the &lt;a href=&quot;#comparison-test&quot;&gt;comparison test&lt;/a&gt;, \eqref{25} converges absolutely.&lt;/p&gt;

&lt;p&gt;Hence, we can conclude that \eqref{25} is absolutely convergent, with sum $st$.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We have already gone through convergence tests applied only to series of positive (or nonnegative) terms. Let’s end this lengthy post with the alternating series test. ^^!&lt;/p&gt;

&lt;h2 id=&quot;dirichlets-test&quot;&gt;Dirichlet’s test&lt;/h2&gt;

&lt;h3 id=&quot;abel-part-sum&quot;&gt;Abel’s partial summation formula&lt;/h3&gt;
&lt;p&gt;Consider series $\sum_{n=1}^{\infty}a_n$, sequence $\{b_n\}$. If $s_n=a_1+a_2+\ldots+a_n$, then
\begin{equation}
a_1b_1+a_2b_2+\ldots+a_nb_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots+s_{n-1}(b_{n-1}-b_n)+s_nb_n\tag{29}\label{29}
\end{equation}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Since $a_1=s_1$ and $a_n=s_n-s_{n-1}$ for $n&amp;gt;1$, we have
\begin{align}
a_1b_1&amp;amp;=s_1b_1 \\ a_2b_2&amp;amp;=s_2b_2-s_1b_2 \\ a_3b_3&amp;amp;=s_3b_3-s_2b_3 \\ &amp;amp;\vdots \\ a_nb_n&amp;amp;=s_nb_n-s_{n-1}b_n
\end{align}
On adding these equations, and grouping suitably, we obtain \eqref{29}.&lt;/p&gt;

&lt;h3 id=&quot;d-test&quot;&gt;Dirichlet’s test&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;If the series $\sum_{n=1}^{\infty}a_n$ has bounded partial sums, and if $\{b_n\}$ is a decreasing sequence of positive numbers such that $b_n\to 0$, then the series
\begin{equation}
\sum_{n=1}^{\infty}a_nb_n=a_1b_1+a_2b_2+\ldots+a_nb_n+\ldots\tag{30}\label{30}
\end{equation}
converges&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;&lt;br /&gt;
Let $S_n=a_1b_1+a_2b_2+\ldots+a_nb_n$ denote the $n$-th partial sum of \eqref{30}, then \eqref{29} tells us that
\begin{equation}
S_n=T_n+s_nb_n,
\end{equation}
where
\begin{equation}
T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots
\end{equation}
Since ${s_n}$ is bounded there exists a positive constant $m$ such that $\vert s_n\vert\leq m,\forall n$, so $\vert s_nb_n\vert\leq mb_n$. And since $b_n\to 0$, we have that $s_nb_n\to 0$ as $n\to\infty$.&lt;/p&gt;

&lt;p&gt;Moreover, since $\{b_n\}$ is a decreasing sequence of positive numbers, we have that
\begin{equation}
\begin{aligned}
\vert s_1(b_1-b_2)\vert+\vert s_2(b_3-b_3)\vert+\ldots&amp;amp;\,+\vert s_{n-1}(b_{n-1}-b_n)\vert \\ &amp;amp;\leq m(b_1-b_2)+m(b_2-b_3)+\ldots+m(b_{n-1}-b_n) \\ &amp;amp;=m(b_1-b_n)\leq mb_1
\end{aligned}
\end{equation}
which implies that $T_n=s_1(b_1-b_2)+s_2(b_2-b_3)+\ldots$ converges absolutely, and thus, it converges to a sum $t$. Therefore
\begin{equation}
\lim_{n\to\infty}S_n=\lim_{n\to\infty}T_n+s_nb_n=\lim_{n\to\infty}T_n+\lim_{n\to\infty}s_nb_n=t+0=t
\end{equation}
which lets us conclude that the series \eqref{30} converges.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] George F.Simmons. &lt;a href=&quot;https://www.amazon.com/Calculus-Analytic-Geometry-George-Simmons/dp/0070576424&quot;&gt;Calculus With Analytic Geometry - 2nd Edition&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[2] Marian M. &lt;a href=&quot;https://www.springer.com/gp/book/9780387789323&quot;&gt;A Concrete Approach to Classical Analysis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;[3] MIT 18.01. &lt;a href=&quot;https://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/&quot;&gt;Single Variable Calculus&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;We will be going through power series in more detailed in another &lt;a href=&quot;/mathematics/calculus/2021/09/21/power-series.html&quot;&gt;post&lt;/a&gt;. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;strong&gt;Theorem&lt;/strong&gt; (&lt;em&gt;Stolz–Cesaro&lt;/em&gt;)&lt;br /&gt;
&lt;em&gt;Let $\{a_n\}$ be a sequence of real numbers and $\{b_n\}$ be a strictly monotone and divergent sequence. Then
\begin{equation}
\lim_{n\to\infty}\dfrac{a_{n+1}-a_n}{b_{n+1}-b_n}=L\hspace{1cm}(\in\left[-\infty,+\infty\right])
\end{equation}
implies
\begin{equation}
\lim_{n\to\infty}\dfrac{a_n}{b_n}=L
\end{equation}&lt;/em&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Trung H. Nguyen</name><email>trung.skipper@gmail.com</email></author><category term="mathematics" /><category term="calculus" /><category term="mathematics" /><category term="calculus" /><category term="series" /><category term="random-stuffs" /><summary type="html">A note on infinite series of constants.</summary></entry></feed>