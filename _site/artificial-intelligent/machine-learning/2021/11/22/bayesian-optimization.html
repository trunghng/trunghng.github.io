<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<style type="text/css">
  #button {
    display: inline-block;
    background-color: #A5CCF1;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    visibility: hidden;
    z-index: 1000;
  }
  #button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
  }
  #button:hover {
    cursor: pointer;
    background-color: #333;
    text-decoration: none;
  }
  #button:active {
    background-color: #555;
  }
  #button.show {
    opacity: 1;
    visibility: visible;
  }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Bayesian Optimization | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Bayesian Optimization" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Bayesian optimization" />
<meta property="og:description" content="Bayesian optimization" />
<link rel="canonical" href="http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html" />
<meta property="og:url" content="http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-22T14:46:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Bayesian Optimization" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","headline":"Bayesian Optimization","dateModified":"2021-11-22T14:46:00+07:00","datePublished":"2021-11-22T14:46:00+07:00","description":"Bayesian optimization","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html"},"url":"http://localhost:4000/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <a id="button"></a>
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Bayesian optimization">
    <h1 class="post-title p-name" itemprop="name headline">Bayesian Optimization</h1>
    <p><span>
      
        
        <a href="/tag/artificial-intelligent"><code class="highligher-rouge"><nobr>artificial-intelligent</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/machine-learning"><code class="highligher-rouge"><nobr>machine-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/gaussian-process"><code class="highligher-rouge"><nobr>gaussian-process</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/optimization-control"><code class="highligher-rouge"><nobr>optimization-control</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/probability-statistics"><code class="highligher-rouge"><nobr>probability-statistics</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/random-stuffs"><code class="highligher-rouge"><nobr>random-stuffs</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2021-11-22T14:46:00+07:00" itemprop="datePublished">
        Nov 22, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>This is simply a random post. Because I litterally had forgotten almost every details about these concepts until a friend of mine gave me a reason to lay my hands on these stuffs again.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a>
    <ul>
      <li><a href="#gauss-dist">Gaussian (Normal) Distribution</a>
        <ul>
          <li><a href="#std-norm">Standard Normal</a></li>
        </ul>
      </li>
      <li><a href="#mvn">Multivariate Normal Distribution</a>
        <ul>
          <li><a href="#bvn">Bivariate Normal</a></li>
        </ul>
      </li>
      <li><a href="#kernels">Kernels</a>
        <ul>
          <li><a href="#kernel-func">Kernel functions</a>
            <ul>
              <li><a href="#rbf-kernels">RBF kernels</a></li>
              <li><a href="#mercer-kernels">Mercer (positive definite) kernels</a></li>
              <li><a href="#lin-kernels">Linear kernels</a></li>
              <li><a href="#matern-kernels">Matern kernels</a></li>
            </ul>
          </li>
          <li><a href="#eigenfunc-kernel">Eigenfunction A nalysis of Kernels</a>
            <ul>
              <li><a href="#eigenfunc">Eigenfunctions</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#gp">Gaussian Process</a>
    <ul>
      <li><a href="#gpr">Gaussian Process Regression</a></li>
    </ul>
  </li>
  <li><a href="#bayes-opt">Bayesian Opitmization</a>
    <ul>
      <li><a href="#surrogate-model">Surrogate Model</a></li>
      <li><a href="#acquisition-func">Acquisition Functions</a></li>
      <li><a href="#opt-alg">Optimization Algorithm</a></li>
      <li><a href="#exp-imp">Expected Improvement</a></li>
      <li><a href="#implementation">Implementation</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>$\newcommand{\Var}{\mathrm{Var}}$
$\newcommand{\Cov}{\mathrm{Cov}}$</p>
<h2 id="preliminaries">Preliminaries</h2>
<p>Before diving into details, it is necessary to equip some basic concepts.</p>

<h3 id="gauss-dist">Gaussian (Normal) Distribution</h3>
<p>A random variable $X$ is said to be <strong>Gaussian</strong> or to have the <strong>Normal distribution</strong> with mean $\mu$ and variance $\sigma^2$ if its probability density function (PDF) is
\begin{equation}
f_X(x)=\dfrac{1}{\sqrt{2\pi}\sigma}\exp\left(-\dfrac{(x-\mu)^2}{2\sigma^2}\right)
\end{equation}
which we denote as $X\sim\mathcal{N}(\mu,\sigma)$</p>

<h4 id="std-normal">Standard Normal</h4>
<p>When $X$ is normally distributed with mean $\mu=0$ and variance $\sigma^2=1$, we call its distribution <em>Standard Normal</em>.
\begin{equation}
X\sim\mathcal{N}(0,1)
\end{equation}
In this case, $X$ has special notations to denote its PDF and CDF, which are
\begin{equation}
\varphi(x)=\dfrac{1}{\sqrt{2\pi}}e^{-z^2/2}
\end{equation}
\begin{equation}
\Phi(x)=\int_{-\infty}^{x}\varphi(t)\,dt=\int_{-\infty}^{x}\dfrac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt
\end{equation}
Below is some visualizations of Normal distribution.</p>

<figure>
	<img src="/assets/images/2021-11-22/normal.png" alt="normal distribution" width="900" height="380px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: 10K normally distributed data points (5K each plot) were plotted as vertical bars on x-axis. The code can be found <span><a href="https://github.com/trunghng/bayes-opt/blob/main/gauss-dist.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h3 id="mvn">Multivariate Normal Distribution</h3>
<p>A $k$-dimensional random vector $\mathbf{X}=\left(X_1,\dots,X_k\right)^\intercal$ is said to have a <strong>Multivariate Normal (MVN)</strong> distribution if every linear combination of the $X_i$ has a Normal distribution. Which means
\begin{equation}
t_1X_1+\ldots+t_kX_k
\end{equation}
is normally distributed for any choice of constants $t_1,\dots,t_k$. Distribution of $\mathbf{X}$ then can be written in the following notation
\begin{equation}
\mathbf{X}\sim\mathcal{N}(\mathbf{\mu},\mathbf{\Sigma})
\end{equation}
where
\begin{equation}
	\mathbf{\mu}=\mathbb{E}\mathbf{X}=\mathbb{E}\left(\mu_1,\ldots,\mu_k\right)^\intercal=\left(\mathbb{E}X_1,\ldots,\mathbb{E}X_k\right)^\intercal
\end{equation}
is the $k$-dimensional mean vector, and covariance matrix $\mathbf{\Sigma}\in\mathbb{R}^{k\times k}$ with
\begin{equation}
	\mathbf{\Sigma}_{ij}=\mathbb{E}\left(X_i-\mu_i\right)\left(X_j-\mu_j\right)=\Cov(X_i,X_j)
\end{equation}
We also have that $\mathbf{\Sigma}\geq 0$ (positive semi-definite matrix)<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>Thus, the PDF of an MVN is defined as
\begin{equation}
f_X(x_1,\ldots,x_k)=\dfrac{1}{(2\pi)^{k/2}\vert\mathbf{\Sigma}\vert^{1/2}}\exp\left[\dfrac{1}{2}\left(\mathbf{x}-\mathbf{\mu}\right)^\intercal\mathbf{\Sigma}^{-1}(\mathbf{x}-\mathbf{\mu})\right]
\end{equation}
With this idea, <em>Standard Normal</em> distribution in multi-dimensional case can be defined as a Gaussian with mean $\mathbf{\mu}=0$ (here $0$ is an $k$-dimensional vector) and identity covariance matrix $\mathbf{\Sigma}=\mathbf{I}_{k\times k}$.</p>

<h4 id="bvn">Bivariate Normal</h4>
<p>When the number of dimensions in $\mathbf{X}$, $k=2$, this special case of MVN is called the <strong>Bivariate Normal (BVN)</strong>. An example of an BVN is shown as following.</p>

<figure>
	<img src="/assets/images/2021-11-22/bvn.png" alt="monte carlo method" width="750" height="350px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: The PDF of $\mathcal{N}\left(\left[\begin{smallmatrix}0\\0\end{smallmatrix}\right],\left[\begin{smallmatrix}1&amp;0.5\\0.8&amp;1\end{smallmatrix}\right]\right)$. The code can be found <span><a href="https://github.com/trunghng/bayes-opt/blob/main/mvn.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h3 id="kernels">Kernels</h3>

<h4 id="kernel-func">Kernel functions</h4>
<p><strong>Kernel function</strong> is a real-valued function of two arguments
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)\in\mathbb{R},
\end{equation}
for $\textbf{x},\textbf{x}’\in\mathcal{X}$, which typically is symmetric (i.e., $\kappa(\textbf{x},\textbf{x}’)=\kappa(\textbf{x}’,\textbf{x})$), and nonnegative ($\kappa(\textbf{x},\textbf{x}’)\geq0$). These are some examples of kernel functions.</p>

<h5 id="rbf-kernels">RBF kernels</h5>
<p>Let $\mathcal{X}\subset\mathbb{R}^D$. For $\gamma&gt;0$, a <strong>Gaussian RBF kernels</strong> or a <strong>squared exponential kernel</strong> (<strong>SE kernel</strong>) $\kappa: \mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is defined by
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\exp\left(-\frac{\Vert\textbf{x}-\textbf{x}’\Vert^2}{\gamma^2}\right)
\end{equation}
for $\textbf{x},\textbf{x}’\in\mathcal{X}$.</p>

<h5 id="mercer-kernels">Mercer (positive definite) kernels</h5>
<p>If the Gram matrix, defined by
\begin{equation}
\mathbf{K}=\begin{bmatrix}\kappa(\mathbf{x}_1,\mathbf{x}_1)&amp;\ldots&amp;\kappa(\mathbf{x}_1,\mathbf{x}_N) \\&amp;\vdots&amp;\\\kappa(\mathbf{x}_N,\mathbf{x}_1)&amp;\ldots&amp;\kappa(\mathbf{x}_N,\mathbf{x}_N)\end{bmatrix},
\end{equation}
is positive definite for any set $\left\{x_i\right\}_{i=1}^N$, the kernel $\kappa$ is called a <strong>Mercer kernel</strong>, or <strong>positive definite kernel</strong>.</p>

<h5 id="lin-kernels">Linear kernels</h5>
<p>Deriving the feature vector implied by a kernel is only possible if the kernel is <em>Mercer</em>. However, deriving a kernel from a feature vector is easy. We have
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\phi\left(\textbf{x}\right)^\intercal\phi(\textbf{x}’)
\end{equation}
If $\phi(\textbf{x})=\textbf{x}$, we obtain a simple kernel called <strong>linear kernel</strong>
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\textbf{x}^\intercal\textbf{x}’
\end{equation}</p>

<h5 id="matern-kernels">Matern kernels</h5>
<p>Let $\mathcal{X}\subset\mathbb{R}^D,\nu&gt;0,\ell&gt;0$. The <strong>Matern kernel</strong> $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ is defined by
\begin{equation}
\kappa(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}r}{\ell}\right)^{\nu}K_{\nu}\left(\frac{\sqrt{2\nu}r}{\ell}\right)\tag{2}\label{2}
\end{equation}
where $\Gamma$ is the <em>gamma function</em>; $r=\Vert\textbf{x}-\textbf{x}’\Vert$ for $\textbf{x},\textbf{x}’\in\mathcal{X}$ and $K_{\nu}$ is a <em>modified Bessel function</em>.</p>

<p>As $\nu\to\infty$, \eqref{2} approaches the <strong>SE kernel</strong>
\begin{equation}
\lim_{\nu\to\infty}\kappa_\nu(r)=\exp\left(-\dfrac{r^2}{\ell}\right)
\end{equation}
where $\ell&gt;0$.</p>

<p>When $\nu$ is half-integer - i.e., $\nu=p+1/2$, for $p\geq0$, equation \eqref{2} can be reduced to a product of an exponential function and a polynomial of degree $p$, which can be written as
\begin{equation}
\kappa_{\nu=p+1/2}(r)=\exp\left(-\dfrac{\sqrt{2\nu}r}{\ell}\right)\dfrac{\Gamma(p+1)}{\Gamma(2p+1)}\sum_{i=0}^{p}\dfrac{(p+i)!}{i!(p-i)!}\left(\dfrac{\sqrt{8\nu}r}{\ell}\right)^{p-i}
\end{equation}</p>

<p>For an another special case, setting $\nu=1/2$ lets \eqref{2} become
\begin{equation}
\kappa_{\nu=1/2}=\exp\left(\dfrac{-r}{\ell}\right),
\end{equation}
which is known as <strong>Laplace</strong> or <strong>exponential kernel</strong>.</p>

<h4 id="eigenfunc-kernel">Eigenfunction Analysis of Kernels</h4>

<h5 id="eigenfunc">Eigenfunctions</h5>
<p>A function $\phi(\cdot)$ that obeys the integral equation
\begin{equation}
\int \kappa(\textbf{x},\textbf{x}’)\phi(\textbf{x})\,d\mu(\textbf{x})=\lambda\phi(\textbf{x}’),
\end{equation}
is called an <strong>eigenfunction</strong> of kernel $\kappa$ with eigenvalue $\lambda$ w.r.t <a href="/random-stuffs/measure-theory/2021/07/03/measure.html">measure</a> $\mu$.</p>

<p><strong>Mercer’s theorem</strong> allows us to express kernel $\kappa$ in terms of the eigenvalues and eigenfunctions.</p>

<p>First, let us introduce an integral operator, $T_\kappa$, which is defined as
\begin{equation}
(T_\kappa f)(\textbf{x})=\int_\mathcal{X}\kappa(\textbf{x},\textbf{x}’)f(\textbf{x}’)\,d\mu(\textbf{x}’),
\end{equation}
where $\mu$ denotes a measure.</p>

<p><strong>Theorem</strong> (<em>Mercer’s theorem</em>)<br />
<em>Let $(\mathcal{X},\mu)$ be a finite measure space and $\kappa\in L_\infty(\mathcal{X}^2,\mu^2)$ be positive definite and $T_\kappa:L_2(\mathcal{X},\mu)\to L_2(\mathcal{X},\mu)$. Let $\phi_i\in L_2(\mathcal{X,\mu})$ be the normalized eigenfunctions of $T_\kappa$ associated with the eigenvalues $\lambda_i&gt;0$. Then we have
\begin{equation}
\kappa(\textbf{x},\textbf{x}’)=\sum_{i=0}^{\infty}\lambda_i\phi_i(\textbf{x})\phi_i(\textbf{x}’)
\end{equation}
where the convergence is absolute and uniform over $\textbf{x},\textbf{x}’\in\mathcal{X}$.</em></p>

<h2 id="gp">Gaussian Process</h2>
<p>For Gaussian process, positive definite kernels serve as <em>covariance functions</em> of random function values, so they are also called <em>covariance kernels</em>.</p>

<p>Let $\mathcal{X}$ be a nonempty set, $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$ be a positive definite kernel, and some real-valued function $\mu:\mathcal{X}\to\mathbb{R}$. Then a random function $f:\mathcal{X}\to\mathbb{R}$ is said to be a <strong>Gaussian process</strong> (<strong>GP</strong>) with mean function $\mu$ and covariance kernel $\kappa$, denoted by $\mathcal{GP(\mu,\kappa)}$, if the following holds:<br />
For any finite set $X=\{\textbf{x}_1,\ldots,\textbf{x}_n\}\subset\mathcal{X}$ of any size $n\in\mathbb{N}$, the random vector $f_X\in\mathbb{R}^n$,
\begin{equation}
f_X=\left(f(\textbf{x}_1),\ldots,f(\textbf{x}_n)\right)^\intercal\sim\mathcal{N}(\mu_X,\kappa_{XX})
\end{equation}
where $\mu_X=\left(\mu(\textbf{x}_1),\ldots,\mu(\textbf{x}_n)\right)^\intercal$ is the mean vector and $\kappa_{XX}=\left(\kappa(\textbf{x}_i,\textbf{x}_j)\right)_{i,j=1}^n\in\mathbb{R}^{n\times n}$ is covariance matrix.</p>

<p>The correpondence between <strong>Gaussian process</strong> $f\sim\mathcal{GP}(\mu,\kappa)$ and pairs $(\mu,\kappa)$ of mean function $\mu$ and positive definite kernel $\kappa$ is a one-to-one since from the definition above, Gaussian process $f$ implies the existence of a mean function $\mu:\mathcal{X}\to\mathbb{R}$ and a covariance kernel $\kappa:\mathcal{X}\times\mathcal{X}\to\mathbb{R}$. And from</p>

<h3 id="gpr">Gaussian Process Regression</h3>

<h2 id="references">References</h2>
<p>[1] C. E. Rasmussen &amp; C. K. I. Williams. <a href="http://www.gaussianprocess.org/gpml/">Gaussian Processes for Machine Learning</a>, MIT Press, 2006</p>

<p>[2] Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, Bharath K. Sriperumbudur. <a href="https://arxiv.org/abs/1807.02582">Gaussian Processes and Kernel Methods: A Review on Connections and Equivalences</a></p>

<p>[3] Joseph K. Blitzstein &amp; Jessica Hwang. <a href="https://www.amazon.com/Introduction-Probability-Chapman-Statistical-Science/dp/1466575573">Introduction to Probability</a></p>

<p>[4] Kevin P. Murphy. <a href="https://probml.github.io/pml-book/book0.html">Machine Learning: A Probabilistic Perspective</a>, MIT Press, 2012</p>

<p>[5] Peter I. Frazier. <a href="https://arxiv.org/abs/1807.02811">A Tutorial on Bayesian Optimization</a></p>

<p>[6] Martin Krasser. <a href="https://krasserm.github.io/2018/03/21/bayesian-optimization/">Bayesian Optimization</a></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The definition of covariance matrix $\mathbf{\Sigma}$ can be rewritten as
\begin{equation}
\mathbf{\Sigma}=\Cov(\mathbf{X},\mathbf{X})=\Var(\mathbf{X})
\end{equation}
Let $\mathbf{z}\in\mathbb{R}^k$, we have
\begin{equation}
\Var(\mathbf{z}^\intercal\mathbf{X})=\mathbf{z}^\intercal\Var(\mathbf{X})\mathbf{z}=\mathbf{z}^\intercal\mathbf{\Sigma}\mathbf{z}
\end{equation}
And since $\Var(\mathbf{z}^\intercal\mathbf{X})\geq0$, we also have that $\mathbf{z}^\intercal\mathbf{\Sigma}\mathbf{z}\geq0$, which proves that $\mathbf{\Sigma}$ is a positive semi-definite matrix. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/artificial-intelligent/machine-learning/2021/11/22/bayesian-optimization.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  "HTML-CSS": {availableFonts: []}
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
<script type="text/javascript">
  var btn = $('#button');

  $(window).scroll(function() {
    if ($(window).scrollTop() > 300) {
      btn.addClass('show');
    } else {
      btn.removeClass('show');
    }
  });

  btn.on('click', function(e) {
    e.preventDefault();
    $('html, body').animate({scrollTop:0}, '300');
  });
</script>