<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Markov Decision Processes, Bellman equations | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Markov Decision Processes, Bellman equations" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Markov Decision Processes (MDPs), Bellman equations" />
<meta property="og:description" content="Markov Decision Processes (MDPs), Bellman equations" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-06-27T08:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Markov Decision Processes, Bellman equations" />
<script type="application/ld+json">
{"description":"Markov Decision Processes (MDPs), Bellman equations","headline":"Markov Decision Processes, Bellman equations","dateModified":"2021-06-27T08:00:00+07:00","datePublished":"2021-06-27T08:00:00+07:00","mainEntityOfPage":{"@type":"WebPage","@id":"/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"},"url":"/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html","author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Markov Decision Processes (MDPs), Bellman equations">
    <h1 class="post-title p-name" itemprop="name headline">Markov Decision Processes, Bellman equations</h1>
    <p><span>
      
        
        <a href="/tag/artificial-intelligent"><code class="highligher-rouge"><nobr>artificial-intelligent</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2021-06-27T08:00:00+07:00" itemprop="datePublished">
        Jun 27, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>You may have known or heard vaguely about a computer program called <strong>AlphaGo</strong> - the AI has beaten Lee Sedol - the winner of 18 world Go titles. One of the techniques it used is called <strong>self-play</strong> against its other instances, with <strong>Reinforcement Learning</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-rl">What is Reinforcement Learning?</a></li>
  <li><a href="#mdp">Markov Decision Processes (MDPs)</a>
    <ul>
      <li><a href="#return">Return</a></li>
      <li><a href="#policy">Policy</a></li>
      <li><a href="#value-function">Value Function</a></li>
      <li><a href="#opt-policy-opt-value-func">Optimal Policy and Optimal Value Function</a></li>
    </ul>
  </li>
  <li><a href="#bellman-equations">Bellman Equations</a>
    <ul>
      <li><a href="#bellman-backup-diagram">Bellman Backup Diagram</a></li>
      <li><a href="#bellman-optimality-equations">Bellman Optimality Equations</a></li>
      <li><a href="#backup-vq">Backup diagram for $v_*$ and $q_*$</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
</ul>

<h4 id="what-is-rl">What is Reinforcement Learning?</h4>
<p>Say, there is an unknown <strong>environment</strong> that we’re trying to put an <strong>agent</strong> on. By interacting with the <strong>agent</strong> through taking <strong>actions</strong> that gives rise to <strong>rewards</strong> continually, the <strong>agent</strong> learns a <strong>policy</strong> that maximize the cumulative <strong>rewards</strong>.<br />
<strong>Reinforcement Learning (RL)</strong>, roughly speaking, is an area of Machine Learning that describes methods aimed to learn a good strategy (called <strong>policy</strong>) for the <strong>agent</strong> from experimental trials and relative simple feedback received. With the optimal <strong>policy</strong>, the <strong>agent</strong> is capable to actively adapt to the environment to maximize future <strong>rewards</strong>.
<img src="/assets/images/robot.png" alt="RL" /></p>

<h4 id="mdp">Markov Decision Processes (MDPs)</h4>
<p><strong>Markov decision processes (MDPs)</strong> formally describe an environment for <strong>RL</strong>. And almost all <strong>RL</strong> problems can be formalised as <strong>MDPs</strong>.</p>

<p><strong>Definition (MDP)</strong><br />
A <strong>Markov Decision Process</strong> is a tuple $⟨\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma⟩$</p>
<ul>
  <li>$\mathcal{S}$ is a set of states called <em>state space</em></li>
  <li>$\mathcal{A}$ is a set of actions called <em>action space</em></li>
  <li>$\mathcal{P}$ is a state transition probability matrix<br />
  \(\mathcal{P}^a_{ss'}=P(S_{t+1}=s'|S_t=s,A_t=a)\)</li>
  <li>$\mathcal{R}$ is a reward function<br />
  \(\mathcal{R}^a_s=\mathbb{E}\left[R_{t+1}|S_t=s,A_t=a\right]\)</li>
  <li>$\gamma\in[0, 1]$ is a discount factor for future reward</li>
</ul>

<p><strong>MDP</strong> is an extension of <a href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html">Markov chain</a>. If only one action exists for each state, and all rewards are the same, an <strong>MDP</strong> reduces to a <em>Markov chain</em>. All states in <strong>MDP</strong> has <a href="/random-stuffs/probability-statistics/2021/06/19/markov-chain.html#markov-property">Markov property</a>, referring to the fact that the current state captures all relevant information from the history.
\begin{equation}
P(S_{t+1}|S_t)=P(S_{t+1}|S_1,\dots,S_t)
\end{equation}</p>

<h5 id="return">Return</h5>
<p>In the preceding section, we have said that the goal of agent is to maximize the cumulative reward in the long run. In general, we seek to maximize the <strong>expected return</strong>.</p>

<p><strong>Definition</strong> (<em>Return</em>)<br />
The <strong>return</strong> $G_t$ is the total discounted reward from t
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\dots=\sum_{k=0}^{\infty}\gamma^k R_{t+k+1},
\end{equation}
where $\gamma\in[0,1]$ is called <em>discount rate</em> (or <em>discount factor</em>).</p>

<p>The <em>discount rate</em> $\gamma$ determines the present value of future rewards: a reward received
k time steps in the future is worth only $\gamma^{k-1}$ times what it would be worth if it were received immediately. And also, it provides mathematical convenience since as $k\rightarrow\infty$ then $\gamma^k\rightarrow 0$.</p>

<h5 id="policy">Policy</h5>
<p><strong>Policy</strong>, which is denoted as $\pi$, is the behaviour function of the agent. $\pi$ is a mapping from states to probabilities of selecting each possible action. In other words, it lets us know which action to take in the current state $s$ and can be either <em>deterministic</em> or <em>stochastic</em>.</p>
<ul>
  <li><em>Deterministic policy</em>:	$\quad\pi(s)=a$</li>
  <li><em>Stochastic policy</em>: $\quad\pi(a|s)=P(A_t=a|S_t=s)$</li>
</ul>

<h5 id="value-function">Value Function</h5>
<p><strong>Value function</strong> measures <em>how good</em> a particular state is (or <em>how good</em> it is to perform a given action in a given state).</p>

<p><strong>Definition</strong> (<em>state-value function</em>)<br />
The <strong>state-value function</strong> of a state $s$ under a policy $\pi$, denoted as $v_\pi(s)$, is the expected return starting from state $s$ and following $\pi$ thereafter:
\begin{equation}
v_\pi(s)=\mathbb{E}_\pi[G_t|S_t=s]
\end{equation}</p>

<p><strong>Definition</strong> (<em>action-value function</em>)<br />
Similarly, we define the value of taking action $a$ in state $s$ under a policy $\pi$, denoted as $q_\pi(s,a)$, as the expected return starting from $s$, taking the action $a$, and thereafter following policy $\pi$:
\begin{equation}
q_\pi(s,a)=\mathbb{E}_\pi[G_t|S_t=s,A_t=a]
\end{equation}</p>

<p>Since we follow the policy $\pi$, we have that
\begin{equation}
v_\pi(s)=\sum_{a\in\mathcal{A}}q_\pi(s,a)\pi(a|s)
\end{equation}</p>

<h5 id="opt-policy-opt-value-func">Optimal Policy and Optimal Value Function</h5>
<p>For finite MDPs (finite state and action space), we can precisely define an <strong>optimal policy</strong>. <em>Value functions</em> define a partial ordering over policies. A policy $\pi$ is defined to be better than or equal to a policy $\pi’$ if its expected return is greater than or equal to that of $\pi’$ for all states. In other words,
\begin{equation}
\pi\geq\pi’\iff v_\pi(s)\geq v_{\pi’} \forall s\in\mathcal{S}
\end{equation}</p>

<p><strong>Theorem</strong> (<em>Optimal policy</em>)<br />
For any MDP, there exists an optimal policy $\pi_*$ that is better than or equal to all other policies,
\begin{equation}
\pi_*\geq\pi,\forall\pi
\end{equation}</p>

<p>The proof of the above theorem is gonna be provided in another <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html">post</a> since we need some additional tools to do that.</p>

<p>There may be more than one <strong>optimal policy</strong>, they share the same <em>state-value function</em>, called <strong>optimal state-value function</strong> though.
\begin{equation}
v_*(s)=\max_{\pi}v_\pi(s)
\end{equation}
<strong>Optimal policies</strong> also share the same <em>action-value function</em>, call <strong>optimal action-value function</strong>
\begin{equation}
q_*(s,a)=\max_{\pi}q_\pi(s,a)
\end{equation}</p>

<h4 id="bellman-equations">Bellman Equations</h4>
<p>A fundamental property of <em>value functions</em> used throughout RL is that they satisfy recursive relationships
\begin{align}
v_\pi(s)&amp;\doteq \mathbb{E}_\pi[G_t|S_t=s] \\&amp;=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s] \\&amp;=\sum_{s’,r,g’,a}p(s’,r,g’,a|s)(r+\gamma g’) \\&amp;=\sum_{a}p(a|s)\sum_{s’,r,g’}p(s’,r,g’|a,s)(r+\gamma g’) \\&amp;=\sum_{a}\pi(a|s)\sum_{s’,r,g’}p(s’,r|a,s)p(g’|s’,r,a,s)(r+\gamma g’) \\&amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\sum_{g’}p(g’|s’)(r+\gamma g’) \\&amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma\sum_{g’}p(g’|s’)g’\right] \\&amp;=\sum_{a}\pi(a|s)\sum_{s’,r}p(s’,r|a,s)\left[r+\gamma v_\pi(s’)\right],
\end{align}
where $p(s’,r|s,a)=P(S_{t+1}=s’,R_{t+1}=r|S_t=s,A_t=a)$, which defines the dynamics of the MDP. The last equation is called the <em>Bellman equation for</em> $v_\pi(s)$. It expresses a relationship between the value state $s$, $v_\pi(s)$ and the values of its successor states $s’$, $v_\pi(s’)$.</p>

<p>Similarly, we define the <em>Bellman equation for</em> $q_\pi(s,a)$
\begin{align}
q_\pi(s,a)&amp;\doteq\mathbb{E}_\pi[G_t|S_t=s,A_t=a] \\&amp;=\mathbb{E}_\pi[R_t+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\sum_{a’}\pi(a’|s’)q_\pi(s’,a’)\right]
\end{align}</p>

<h5 id="bellman-backup-diagram">Bellman Backup Diagram</h5>
<p>Backup diagram of <em>state-value function</em> and <em>action-value function</em> respectively</p>
<p float="left">
  <img src="/assets/images/state.png" width="350" />
  <img src="/assets/images/action.png" width="350" /> 
</p>

<h5 id="bellman-optimality-equations">Bellman Optimality Equations</h5>
<p>Since $v_*$ is the value function for a policy, it must satisfy the <em>Bellman equation for state-values</em>. Moreover, it is also the optimal value function, then we have
\begin{align}
v_*(s)&amp;=\max_{a\in\mathcal{A(s)}}q_{\pi_*}(s,a) \\&amp;=\max_{a}\mathbb{E}_{\pi_*}[G_t|S_t=s,A_t=a] \\&amp;=\max_{a}\mathbb{E}_{\pi_*}[R_{t+1}+\gamma G_{t+1}|S_t=s,A_t=a] \\&amp;=\max_{a}\mathbb{E}[R_{t+1}+\gamma v_*(S_{t+1})|S_t=s,A_t=a] \\&amp;=\max_{a}\sum_{s’,r}p(s’,r|s,a)[r+\gamma v_*(s’)]
\end{align}
The last two equations are two forms of the <em>Bellman optimality equation for</em> $v_*$. Similarly, we have the <em>Bellman optimality equation for</em> $q_*$
\begin{align}
q_*(s,a)&amp;=\mathbb{E}\left[R_{t+1}+\gamma\max_{a’}q_*(S_{t+1},a’)|S_t=s,A_t=a\right] \\&amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\max_{a’}q_*(s’,a’)\right]
\end{align}</p>

<h5 id="backup-vq">Backup diagram for $v_*$ and $q_*$</h5>
<p><img src="/assets/images/opt.png" alt="backup diagram for optimal value func" /></p>

<h4 id="references">References</h4>
<p>[1] Reinforcement Learning: An Introduction - Richard S. Sutton &amp; Andrew G. Barto</p>

<p>[2] <a href="https://www.davidsilver.uk/teaching/">UCL course on RL</a> - David Silver</p>

<p>[3] <a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">A (Long) Peek into Reinforcement Learning</a></p>

<p>[4] <a href="https://deepmind.com/research/case-studies/alphago-the-story-so-far">AlphaGo</a></p>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  "HTML-CSS": {availableFonts: []}
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
