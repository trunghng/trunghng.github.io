<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Optimal Policy Existence | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Optimal Policy Existence" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="the proof of the existence of optimal policy in finite markov decision processes (MDPs)" />
<meta property="og:description" content="the proof of the existence of optimal policy in finite markov decision processes (MDPs)" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-10T13:03:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Optimal Policy Existence" />
<script type="application/ld+json">
{"description":"the proof of the existence of optimal policy in finite markov decision processes (MDPs)","headline":"Optimal Policy Existence","dateModified":"2021-07-10T13:03:00+07:00","datePublished":"2021-07-10T13:03:00+07:00","mainEntityOfPage":{"@type":"WebPage","@id":"/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"},"url":"/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html","author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="the proof of the existence of optimal policy in finite markov decision processes (MDPs)">
    <h1 class="post-title p-name" itemprop="name headline">Optimal Policy Existence</h1>
    <p><span>
      
        
        <a href="/tag/artificial-intelligent"><code class="highligher-rouge"><nobr>artificial-intelligent</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/mathematics"><code class="highligher-rouge"><nobr>mathematics</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/my-rl"><code class="highligher-rouge"><nobr>my-rl</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2021-07-10T13:03:00+07:00" itemprop="datePublished">
        Jul 10, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>In the previous post about <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a>, we mentioned that there exists a policy $\pi_*$ that is better than or equal to all other policies. And now, we are here to prove it.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#norms-contractions-banach-fixed-pts">Norms, Contractions and Banach’s Fixed-point Theorem</a>
    <ul>
      <li><a href="#norms">Norms</a></li>
      <li><a href="#contractions">Contractions</a></li>
      <li><a href="#banach-fixed-pts">Banach’s Fixed-point Theorem</a></li>
    </ul>
  </li>
  <li><a href="#bellman-operator">Bellman Operator</a></li>
  <li><a href="#proof">Proof</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<p>Before catching the pokémon, we need to prepare ourselves some pokéball.</p>

<h4 id="norms-contractions-banach-fixed-pts">Norms, Contractions and Banach’s Fixed-point Theorem</h4>

<h5 id="norms">Norms</h5>
<p><strong>Definition</strong> (<em>Norm</em>)<br />
Given a vector space $\mathcal{V}\subseteq\mathbb{R}^d$, a function $f:\mathcal{V}\to\mathbb{R}^+_0$ is a <em>norm</em> if and only if</p>
<ol>
  <li>If $f(v)=0$ for some $v\in\mathcal{V}$, then $v=0$</li>
  <li>For any $\lambda\in\mathbb{R},v\in\mathcal{V},f(\lambda v)=|\lambda|v$</li>
  <li>For any $u,v\in\mathbb{R}, f(u+v)\leq f(u)+f(v)$</li>
</ol>

<p><strong>Examples</strong> (<em>Norm</em>)</p>
<ol>
  <li>$\ell^p$ norms: for $p\geq 1$,
\begin{equation}
\Vert v\Vert_p=\left(\sum_{i=1}^{d}|v_i|^p\right)^{1/p}
\end{equation}</li>
  <li>$\ell^\infty$ norms:
\begin{equation}
\Vert v\Vert_\infty=\max_{1\leq i\leq d}|v_i|
\end{equation}</li>
  <li>$\ell^{\mu,p}$: the weighted variants of these norm are defined as
\begin{equation}
\Vert v\Vert_p=\begin{cases}\left(\sum_{i=1}^{d}\frac{|v_i|^p}{w_i}\right)^{1/p}&amp;\text{if }1\leq p&lt;\infty\\ \max_{1\leq i\leq d}\frac{|v_i|}{w_i}&amp;\text{if }p=\infty\end{cases}
\end{equation}</li>
  <li>$\ell^{2,P}$: the matrix-weighted 2-norm is defined as
\begin{equation}
\Vert v\Vert^2_P=v^TPv
\end{equation}
Similarly, we can define norms over spaces of functions. For example, if $\mathcal{V}$ is the vector space of functions over domain $\mathcal{X}$ which are <em>uniformly bounded</em><sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, then
\begin{equation}
\Vert f\Vert_\infty=\sup_{x\in\mathcal{X}}\vert f(x)\vert
\end{equation}</li>
</ol>

<p><strong>Definition</strong> (<em>Convergence in norm</em>)<br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a <em>normed vector space</em><sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. Let $v_n\in\mathcal{V}$ is a sequence of vectors ($n\in\mathbb{N}$). The sequence ($v_n,n\geq 0$) is said to <em>converge to</em> $v\in\mathcal{V}$ in the norm $\Vert\cdot\Vert$, denoted as $v_n\to_{\Vert\cdot\Vert}v$ if
\begin{equation}
\lim_{n\to\infty}\Vert v_n-v\Vert=0,
\end{equation}
<br /></p>

<p><strong>Definition</strong> (<em>Cauchy sequence</em><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>)<br />
Let ($v_n;n\geq 0$) be a sequence of vectors of a normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$. Then $v_n$ is called a <em>Cauchy sequence</em> if
\begin{equation}
\lim_{n\to\infty}\sup_{m\geq n}\Vert v_n-v_m\Vert=0
\end{equation}
Normed vector spaces where all Cauchy sequences are convergent are special: we can find examples of normed vector spaces such that some of the Cauchy sequences in the vector space do not have a limit.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Completeness</em>)<br />
A normed vector space $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ is called <em>complete</em> if every Cauchy sequence in $\mathcal{V}$ is convergent in the norm of the vector space.</p>

<h5 id="contractions">Contractions</h5>
<p><strong>Definition</strong> (<em>Lipschitzian</em>) <br />
Let $\mathcal{V}=(\mathcal{V},\Vert\cdot\Vert)$ be a normed vector space. A mapping $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ is called <em>L-Lipschitz</em> if for any $u,v\in\mathcal{V}$,
\begin{equation}
\Vert\mathcal{T}u-\mathcal{T}v\Vert\leq L\Vert u-v\Vert
\end{equation}
A mapping $\mathcal{T}$ is called a <em>non-expansion</em> if it is <em>Lipschitzian</em> with $L\leq 1$. It is called a <em>contraction</em> if it is <em>Lipschitzian</em> with $L&lt;1$. In this case, $L$ is called the <em>contraction factor of</em> $\mathcal{T}$ and $\mathcal{T}$ is called an <em>L-contraction</em>.<br />
<ins>Note</ins>: If $\mathcal{T}$ is <em>Lipschitz</em>, it is also continuous in the sense that if $v_n\to_{\Vert\cdot\Vert}v$, then also $\mathcal{T}v_n\to_{\Vert\cdot\Vert}\mathcal{T}v$. This is because $\Vert\mathcal{T}v_n-\mathcal{T}v\Vert\leq L\Vert v_n-v\Vert\to 0$ as $n\to\infty$.</p>

<h5 id="banach-fixed-pts">Banach’s Fixed-point Theorem</h5>
<p><strong>Definition</strong> (<em>Banach space</em>)<br />
A complete, normed vector space is called a <em>Banach space</em>.<br />
<br /></p>

<p><strong>Definition</strong> (<em>Fixed point</em>)<br />
Let $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be some mapping. The vector $v\in\mathcal{V}$ is called a <em>fixed point of</em> $\mathcal{T}$ if $\mathcal{T}v=v$.<br />
<br /></p>

<p><strong>Theorem</strong> (<em>Banach’s fixed-point</em>)<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>    <br />
Let $\mathcal{V}$ be a Banach space and $\mathcal{T}:\mathcal{V}\to\mathcal{V}$ be a $\gamma$-contraction mapping. Then</p>
<ol>
  <li>$\mathcal{T}$ admits a <em>unique fixed point</em> $v$.</li>
  <li>For any $v_0\in\mathcal{V}$, if $v_{n+1}=\mathcal{T}v_n$, then $v_n\to_{\Vert\cdot\Vert}v$ with a <em>geometric convergence rate</em><sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup>:
\begin{equation}
\Vert v_n-v\Vert\leq\gamma^n\Vert v_0-v\Vert
\end{equation}</li>
</ol>

<h4 id="bellman-operator">Bellman Operator</h4>
<p>Previously, we defined Bellman equation for state-value function $v_\pi(s)$ as:
\begin{align}
v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right] \\\text{or}\quad v_\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_\pi(s’)\right)\tag{1}\label{1}
\end{align}
If we let
\begin{align}
\mathcal{P}^\pi_{ss’}&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{P}^a_{ss’}; \\\mathcal{R}^\pi_s&amp;=\sum_{a\in\mathcal{A}}\pi(a|s)\mathcal{R}^a_s
\end{align}
then we can rewrite \eqref{1} in another form as
\begin{equation}
v_\pi(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v_\pi(s’)\tag{2}\label{2}
\end{equation}
<br />
<strong>Definition</strong> (<em>Bellman operator</em>)<br />
We define the <em>Bellman operator</em> underlying $\pi,\mathcal{T}:\mathbb{R}^\mathcal{S}\to\mathbb{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^\pi v)(s)=\mathcal{R}^\pi_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}v(s’)
\end{equation}
<br />
With the help of $\mathcal{T}^\pi$, equation \eqref{2} can be rewrite as:
\begin{equation}
\mathcal{T}^\pi v_\pi=v_\pi\tag{3}\label{3}
\end{equation}
Similarly, we can rewrite the <em>Bellman optimality equation for</em> $v_*$
\begin{align}
v_*(s)&amp;=\max_{a\in\mathcal{A}}\sum_{s’\in\mathcal{S},r}p(s’,r|s,a)\left[r+\gamma v_*(s’)\right] \\ &amp;=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v_*(s’)\right)\tag{4}\label{4}
\end{align}
and thus, we can define the <em>Bellman optimality operator</em> $\mathcal{T}^*:\mathcal{R}^\mathcal{S}\to\mathcal{R}^\mathcal{S}$, by:
\begin{equation}
(\mathcal{T}^* v)(s)=\max_{a\in\mathcal{A}}\left(\mathcal{R}^a_s+\gamma\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}v(s’)\right)
\end{equation}
And thus, with the help of $\mathcal{T}^*$, we can rewrite the equation \eqref{4} as:
\begin{equation}
\mathcal{T}^*v_*=v_*\tag{5}\label{5}
\end{equation}
<br />
Now everything is all set, we can move on to the next step.</p>

<h4 id="proof">Proof</h4>
<ul>
  <li>Let $B(\mathcal{S})$ be the space of <em>uniformly bounded functions</em> with domain $\mathcal{S}$:
\begin{equation}
B(\mathcal{S})=\{v:\mathcal{S}\to\mathbb{R}:\Vert v\Vert_\infty&lt;+\infty\}
\end{equation}</li>
  <li>
    <p>We will view $B(\mathcal{S})$ as a normed vector space with the norm $\Vert\cdot\Vert_\infty$. It is easily seen that $(B(\mathcal{S}),\Vert\cdot\Vert_\infty)$ is complete: If ($v_n;n\geq0$) is a Cauchy sequence in it then for any $s\in\mathcal{S}$, ($v_n(s);n\geq0$) is also a Cauchy sequence over the reals. Denoting by $v(s)$ the limit of ($v_n(s)$), we can show that $\Vert v_n-v\Vert_\infty\to0$. Vaguely speaking, this holds because ($v_n;n\geq0$) is a Cauchy sequence in the norm $\Vert\cdot\Vert_\infty$  so the rate of convergence of $v_n(s)$ to $v(s)$ is independent of $s$.</p>
  </li>
  <li>Pick any stationary policy $\pi$.</li>
  <li>We have that $\mathcal{T}^\pi$ is <em>well-defined</em> since: if $u\in B(\mathcal{S})$, then also $\mathcal{T}^\pi u\in B(S)$.</li>
  <li>From equation \eqref{3}, we have that $v_\pi$ is a fixed point to $\mathcal{T}^\pi$.<br />
We also have that $\mathcal{T}^\pi$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$ since for any $u, v\in B(\mathcal{S})$,
\begin{align}
\Vert\mathcal{T}^\pi u-\mathcal{T}^\pi v\Vert_\infty&amp;=\gamma\max_{s\in\mathcal{S}}\left|\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left(u(s’)-v(s’)\right)\right| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;\leq\gamma\max_{s\in\mathcal{S}}\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}\Vert u-v\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
where the last line follows from $\sum_{s’\in\mathcal{S}}\mathcal{P}^\pi_{ss’}=1$.</li>
  <li>It follows that in order to find $v_\pi$, we can construct the sequence $v_0,\mathcal{T}^\pi v_0,(\mathcal{T}^\pi)^2 v_0,\dots$, which, by Banach’s fixed-point theorem will converge to $v_\pi$ at a geometric rate.</li>
  <li>From the definition \eqref{5} of $\mathcal{T}^*$, we have that $\mathcal{T}^*$ is well-defined.</li>
  <li>Using the fact that $\left|\max_{a\in\mathcal{A}}f(a)-\max_{a\in\mathcal{A}}g(a)\right|\leq\max_{a\in\mathcal{A}}\left|f(a)-g(a)\right|$, similarly, we have:
\begin{align}
\Vert\mathcal{T}^*u-\mathcal{T}^*v\Vert_\infty&amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\left|u(s’)-v(s’)\right| \\ &amp;\leq\gamma\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sum_{s’\in\mathcal{S}}\mathcal{P}^a_{ss’}\Vert u-v\Vert_\infty \\ &amp;=\gamma\Vert u-v\Vert_\infty,
\end{align}
which tells us that $\mathcal{T}^*$ is a $\gamma$-contraction in $\Vert\cdot\Vert_\infty$.
<br /></li>
</ul>

<p><strong>Theorem</strong><br />
Let $v$ be the fixed point of $\mathcal{T}^*$ and assume that there is policy $\pi$ which is greedy w.r.t $v:\mathcal{T}^\pi v=\mathcal{T}^* v$. Then $v=v_*$ and $\pi$ is an optimal policy.<br />
<strong><em>Proof</em></strong><br />
Pick any stationary policy $\pi$. Then $\mathcal{T}^\pi\leq\mathcal{T}^*$ in the sense that for any function $v\in B(\mathcal{S})$, $\mathcal{T}^\pi v\leq\mathcal{T}^* v$ holds ($u\leq v$ means that $u(s)\leq v(s),\forall s\in\mathcal{S}$).<br />
Hence, for all $n\geq0$,
\begin{equation}
v_\pi=\mathcal{T}^\pi v_\pi\leq\mathcal{T}^*v_\pi\leq(\mathcal{T}^*)^2 v_\pi\leq\dots\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
or
\begin{equation}
v_\pi\leq(\mathcal{T}^*)^n v_\pi
\end{equation}
Since $\mathcal{T}^*$ is a contraction, the right-hand side converges to $v$, the unique fixed point of $\mathcal{T}^*$. Thus, $v_\pi\leq v$. And since $\pi$ was arbitrary, we obtain that $v_*\leq v$.<br />
Pick a policy $\pi$ such that $\mathcal{T}^\pi v=\mathcal{T}^*v$, then $v$ is also a fixed point of $\mathcal{V}^\pi$. Since $v_\pi$ is the unique fixed point of $\mathcal{T}^\pi$, we have that $v=v_\pi$, which shows that $v_*=v$ and that $\pi$ is an optimal policy.</p>

<h4 id="references">References</h4>
<p>[1] Csaba Szepesvári. <a href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a></p>

<p>[2] A. Lazaric. <a href="http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf">Markov Decision Processes and Dynamic Programming</a></p>

<p>[3] <a href="https://ai.stackexchange.com/a/11133">What is the Bellman operator in reinforcement learning?</a></p>

<p>[4] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[5] <a href="https://en.wikipedia.org/wiki/Normed_vector_space">Normed vector space</a></p>

<h4 id="footnotes">Footnotes</h4>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A function is called <em>uniformly bounded</em> exactly when $\Vert f\Vert_\infty&lt;+\infty$. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A <em>normed vector space</em> is a vector space over the real or complex number, on which a norm is defined. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>We are gonna talk further about <em>sequences</em> in another post. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><strong><em>Proof</em></strong><br />
Pick any $v_0\in\mathcal{V}$ and define $v_n$ as in the statement of the theorem. a. We first demonstrate that $(v_n)$ converges to some vector. b. Then we will show that this vector is a fixed point to $\mathcal{T}$. c. Finally, we show that $\mathcal{T}$ has a single fixed point. Assume that $\mathcal{T}$ is a $\gamma$-contraction.<br />
a. To show that $(v_n)$ converges, it suffices  to show that $(v_n)$ is a Cauchy sequence. We have:
\begin{align}
\Vert v_{n+1}-v_n\Vert&amp;=\Vert\mathcal{T}v_{n}-\mathcal{T}v_{n-1}\Vert \\ &amp;\leq\gamma\Vert v_{n}-v_{n-1}\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_1-v_0\Vert
\end{align}
From the properties of norms, we have:
\begin{align}
\Vert v_{n+k}-v_n\Vert&amp;\leq\Vert v_{n+1}-v_n\Vert+\dots+\Vert v_{n+k}-v_{n+k-1}\Vert \\ &amp;\leq\left(\gamma^n+\dots+\gamma^{n+k-1}\right)\Vert v_1-v_0\Vert \\ &amp;=\gamma^n\dfrac{1-\gamma^{k}}{1-\gamma}\Vert v_1-v_0\Vert
\end{align}
and so
\begin{equation}
\lim_{n\to\infty}\sup_{k\geq0}\Vert v_{n+k}-v_n\Vert=0,
\end{equation}
shows us that $(v_n;n\geq0)$ is indeed a Cauchy sequence. Let $v$ be its limit.<br />
b. Recall that the definition of the sequence $(v_n;n\geq0)$
\begin{equation}
v_{n+1}=\mathcal{T}v_n
\end{equation}
Taking the limes as $n\to\infty$ of both sides, one the one hand, we get that $v_{n+1}\to _{\Vert\cdot\Vert}v$. On the other hand, $\mathcal{T}v_n\to _{\Vert\cdot\Vert}\mathcal{T}v$, since $\mathcal{T}$ is a contraction, hence it is continuous. Therefore, we must have $v=\mathcal{T}v$, which tells us that $v$ is a fixed point of $\mathcal{T}$.<br />
c. Let us assume that $v,v’$ are both fixed points of $\mathcal{T}$. Then,
\begin{align}
\Vert v-v’\Vert&amp;=\Vert\mathcal{T}v-\mathcal{v’}\Vert \\ &amp;\leq\gamma\Vert v-v’\Vert \\ \text{or}\quad(1-\gamma)\Vert v-v’\Vert&amp;\leq0
\end{align}
Thus, we must have that $\Vert v-v’\Vert=0$. Therefore, $v-v’=0$ or $v=v’$.<br />
And finally,
\begin{align}
\Vert v_n-v\Vert&amp;=\Vert\mathcal{T}v_{n-1}-\mathcal{T}v\Vert \\ &amp;\leq\gamma\Vert v_{n-1}-v\Vert \\ &amp;\quad\vdots \\ &amp;\leq\gamma^n\Vert v_0-v\Vert
\end{align} <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Also, there’s gonna be a post about <em>rate of convergence</em>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  "HTML-CSS": {availableFonts: []}
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
