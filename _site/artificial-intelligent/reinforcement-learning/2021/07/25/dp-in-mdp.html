<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Dynamic Programming Algorithms For Solving Markov Decision Processes | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Dynamic Programming Algorithms For Solving Markov Decision Processes" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Dynamic programming algorithms for solving Markov Decision Processes (MDPs)" />
<meta property="og:description" content="Dynamic programming algorithms for solving Markov Decision Processes (MDPs)" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-07-25T15:30:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Dynamic Programming Algorithms For Solving Markov Decision Processes" />
<script type="application/ld+json">
{"description":"Dynamic programming algorithms for solving Markov Decision Processes (MDPs)","headline":"Dynamic Programming Algorithms For Solving Markov Decision Processes","dateModified":"2021-07-25T15:30:00+07:00","datePublished":"2021-07-25T15:30:00+07:00","url":"/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html","author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Dynamic programming algorithms for solving Markov Decision Processes (MDPs)">
    <h1 class="post-title p-name" itemprop="name headline">Dynamic Programming Algorithms For Solving Markov Decision Processes</h1>
    <p><span>
      
        
        <a href="/tag/artificial-intelligent"><code class="highligher-rouge"><nobr>artificial-intelligent</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/dynamic-programming"><code class="highligher-rouge"><nobr>dynamic-programming</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/my-rl"><code class="highligher-rouge"><nobr>my-rl</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2021-07-25T15:30:00+07:00" itemprop="datePublished">
        Jul 25, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>In two previous posts, <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Process (MDP) and Bellman equations</strong></a> and <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html"><strong>Optimal Policy Existence</strong></a>, we have known how MDPs, Bellman equations are defined and how they work. In this post, we are going to talk about how these MDPs are solved with <strong>Dynamic Programming</strong>.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#what-is-dp">What is Dynamic Programming?</a></li>
  <li><a href="#dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</a>
    <ul>
      <li><a href="#policy-evaluation">Policy Evaluation</a></li>
      <li><a href="#policy-improvement">Policy Improvement</a></li>
      <li><a href="#policy-iteration">Policy Iteration</a></li>
      <li><a href="#value-iteration">Value Iteration</a></li>
      <li><a href="#gpi">Generalized Policy Iteration</a></li>
      <li><a href="#example">Example - Gambler’s Problem</a></li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="what-is-dp">What is Dynamic Programming?</h2>
<p><strong>Dynamic Programming (DP)</strong> is a method of simplifying a complicated problem by breaking it down into more straightforward sub-problems. Then it finds the solutions for the sub-problems, and combines them together.</p>
<figure>
	<img src="/assets/images/2021-07-25/dp.png" alt="dynamic programming" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Using Dynamic Programming to find the shortest path in graph</figcaption>
</figure>

<h2 id="dp-in-mdps">Dynamic Programming applied in Markov Decision Processes</h2>
<ul>
  <li>DP is a very general method for solving problems having two properties:
    <ul>
      <li><em>Optimal substructure</em>
  	- Principle of optimality applies.
  	- Optimal solution can be decomposed into sub-problems.</li>
      <li><em>Overlapping sub-problems</em>
  	- Sub-problems recur many times.
  	- Solutions can be cached and reused.</li>
    </ul>
  </li>
  <li>MDPs satisfy both properties since:
    <ul>
      <li>Bellman equation gives recursive decomposition.</li>
      <li>Value function stores and reuses solutions.</li>
    </ul>
  </li>
  <li>DP assumes the model is already known.</li>
</ul>

<h3 id="policy-evaluation">Policy Evaluation</h3>
<p>Recall from the definition of <a href="/artificial-intelligent/reinforcement-learning/2021/06/27/mdp-bellman-eqn.html#bellman-equations">Bellman equation</a> that, for all $s\in\mathcal{S}$,
\begin{equation}
v_\pi(s)\doteq\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]\tag{1}\label{1}
\end{equation}
If the environment’s dynamics are completely known, then \eqref{1} is a system of $\vert\mathcal{S}\vert$ linear equations in $\vert\mathcal{S}\vert$ unknowns. We can use iterative methods to solve this problem.<br />
Consider a sequence of approximate value functions $v_0,v_1,\dots$, each mapping $\mathcal{S}^+\to\mathbb{R}$. Choosing $v_0$ arbitrarily (the terminal state, if any, must be given value 0). Using Bellman equation for $v_\pi$, we have an update rule:
\begin{align}
v_{k+1}(s)&amp;\doteq\mathbb{E}_\pi\left[R_{t+1}+\gamma v_k(S_{k+1})\vert S_t=s\right] \\ &amp;=\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_k(s’)\right]
\end{align}
for all $s\in\mathcal{S}$. Thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts">Banach’s fixed points theorem</a> and as we talked in that post, we have that the sequence $\{v_k\}\to v_\pi$ as $k\to\infty$. This algorithm is called <strong>iterative policy evaluation</strong>.<br />
We have the backup diagram for this update.</p>
<figure>
	<img src="/assets/images/2021-07-25/backup-iterative-policy-evaluation.png" alt="Backup diagram for iterative policy evalution update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 2</b>: Backup diagram for Iterative policy evaluation update</figcaption>
</figure>
<p><br />
When implementing <em>iterative policy evaluation</em>, for all $s\in\mathcal{S}$, we can use:</p>
<ul>
  <li>one array to store the value functions, and update them ‘‘in-place” (<em>asynchronous DP</em>)
\begin{equation}
\color{red}{v(s)}\leftarrow\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\color{red}{v(s’)}\right]
\end{equation}</li>
  <li>two arrays in which the new value functions can be computed one by one from the old functions without the old ones being changed (<em>synchronous DP</em>)
\begin{align}
\color{red}{v_{new}(s)}&amp;\leftarrow\sum_a\pi(a|s)\sum_{s’,r}p(s’,r|s,a)\left[r+\color{red}{v_{old}(s’)}\right]\\ \color{red}{v_{old}}&amp;\leftarrow\color{red}{v_{new}}
\end{align}
Here is the pseudocode of the <em>in-place iterative policy evaluation</em>, given a policy $\pi$, for estimating $V\approx v_\pi$</li>
</ul>
<figure>
	<img src="/assets/images/2021-07-25/iterative-policy-evaluation.png" alt="iterative policy evalution pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
    <figcaption></figcaption>
</figure>
<h3 id="policy-improvement">Policy Improvement</h3>
<p>The reason why we compute the value function for a given policy $\pi$ is to find better policies. Given the computed value function $v_\pi$ for an deterministic policy $\pi$, we already know how good it is for a state $s$ to choose action $a=\pi(s)$. Now what we are considering is, in $s$, if we instead take action $a\neq\pi$, will it be better?<br />
In particular, in state $s$, selecting action $a$ and thereafter following the policy $\pi$, we have:
\begin{align}
q_\pi(s,a)&amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{2}\label{2} \\ &amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]
\end{align}
<strong>Theorem</strong> (<em>Policy improvement</em>)<br />
Let $\pi,\pi’$ be any pair of deterministic policies such that, for all $s\in\mathcal{S}$,
\begin{equation}
q_\pi(s,\pi’(s))\geq v_\pi(s)\tag{3}\label{3}
\end{equation}
Then $\pi’\geq\pi$, which means for all $s\in\mathcal{S}$, we have $v_{\pi’}(s)\geq v_\pi(s)$.</p>

<p><strong>Proof</strong><br />
Deriving \eqref{3} combined with \eqref{2}, we have<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>:
\begin{align}
v_\pi(s)&amp;\leq q_\pi(s,\pi’(s)) \\ &amp;=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi’(s)\right]\tag{by \eqref{2}} \\ &amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right] \\ &amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma q_\pi(S_{t+1},\pi’(S_{t+1}))|S_t=s\right]\tag{by \eqref{3}} \\ &amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma\mathbb{E}_{\pi’}\left[R_{t+2}+\gamma v_\pi(S_{t+2})|S_{t+1},A_{t+1}=\pi’(S_{t+1})\right]|S_t=s\right] \\ &amp;=\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 v_\pi(S_{t+2})|S_t=s\right] \\ &amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 v_\pi(S_{t+3})|S_t=s\right] \\ &amp;\quad\vdots \\ &amp;\leq\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\gamma^3 R_{t+4}+\dots|S_t=s\right] \\ &amp;=v_{\pi’}(s)
\end{align}</p>

<p>Consider the new <em>greedy policy</em>, $\pi’$, which takes the action that looks best in the short term - after one step of lookahead - according to $v_\pi$, given by
\begin{align}
\pi’(s)&amp;\doteq\arg\max_a q_\pi(s,a) \\ &amp;=\arg\max_a\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=a\right]\tag{4}\label{4} \\ &amp;=\arg\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_\pi(s’)\right]
\end{align}
By the above theorem, we have that the greedy policy is as good as, or better than, the original policy.<br />
Suppose the new greedy policy, $\pi’$, is as good as, but not better than, $\pi$. Or in other words, $v_\pi=v_{\pi’}$. And from \eqref{4}, we have for all $s\in\mathcal{S}$,
\begin{align}
v_{\pi’}(s)&amp;=\max_a\mathbb{E}\left[R_{t+1}+\gamma v_{\pi’}(S_{t+1})|S_t=s,A_t=a\right] \\ &amp;=\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_{\pi’}(s’)\right]
\end{align}
which is the Bellman optimality equation for action-value function. And therefore, $v_{\pi’}$ must be $v_*$. Hence, <em>policy improvement</em> must give us a strictly better policy except when the original one is already optimal<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>.</p>

<h3 id="policy-iteration">Policy Iteration</h3>
<p>Once we have obtained a better policy, $\pi’$, by improving a policy $\pi$ using $v_\pi$, we can repeat the same process by computing $v_{\pi’}$, and improve it to yield an even better $\pi’’$. Repeating it again and again, we get an iterative procedure to improve the policy
\begin{equation}
\pi_0\xrightarrow[]{\text{evaluation}}v_{\pi_0}\xrightarrow[]{\text{improvement}}\pi_1\xrightarrow[]{\text{evaluation}}v_{\pi_1}\xrightarrow[]{\text{improvement}}\pi_2\xrightarrow[]{\text{evaluation}}\dots\xrightarrow[]{\text{improvement}}\pi_*\xrightarrow[]{\text{evaluation}}v_*
\end{equation}
Each following policy is a strictly improved version of the previous one (unless it is already optimal). Because a finite MDP has only a finite number of policies, this process must converge to an optimal policy and optimal value function in a finite number of iterations.
This algorithm is called <strong>policy iteration</strong>. And here is the pseudocode of the policy iteration.</p>
<figure>
	<img src="/assets/images/2021-07-25/policy-iteration.png" alt="policy iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h3 id="value-iteration">Value Iteration</h3>
<p>When using <em>policy iteration</em>, each of its iterations involves policy evaluation, which requires multiple sweeps through the state set, and thus affects the computation performance.<br />
Policy evaluation step of policy iteration, in fact, can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one update of each state). This algorithm is called <strong>value iteration</strong>, which follows this update:
\begin{align}
v_{k+1}&amp;\doteq\max_a\mathbb{E}\left[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s,A_t=a\right] \\ &amp;=\max_a\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma v_k(s’)\right],
\end{align}
for all $s\in\mathcal{S}$. Once again, thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts">Banach’s fixed point theorem</a>, for an arbitrary $v_0$, we have that the sequence $\{v_k\}\to v_*$ as $k\to\infty$.<br />
We have the backup diagram for this update<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>.</p>
<figure>
	<img src="/assets/images/2021-07-25/backup-value-iteration.png" alt="Backup diagram of value iteration update" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 3</b>: Backup diagram of Value Iteration update</figcaption>
</figure>
<p><br />
And here is the pseudocode of the value iteration.</p>
<figure>
	<img src="/assets/images/2021-07-25/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h3 id="gpi">Generalized Policy Iteration</h3>
<p>The <strong>Generalized Policy Iteration (GPI)</strong> algorithm refers to the idea of combining policy evaluation and policy improvement together to improve the original policy.<br />
In GPI, the value function is repeatedly driven toward the true value of the current policy and at the same time the policy is being improved optimality with respect to its value function, as in the following diagram.</p>
<figure>
	<img src="/assets/images/2021-07-25/gpi.png" alt="GPI" width="200" height="320px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 4</b>: Generalized Policy Iteration</figcaption>
</figure>
<p><br />
Once it reaches the stationary state (when both evaluation and improvement no long produce any updates), then the current value function and policy must be optimal.<br />
The evaluation and improvement processes in GPI can be viewed as both competing and cooperating. They competing in the sense that on the one hand, making policy greedy w.r.t the value function typically makes value function incorrect for the new policy. And on the other hand, approximating the value function closer to the true value of the policy typically forces the policy is no longer to be greedy. But in the long run, they two processes cooperate to find a single joint solution: the optimal value function and an optimal policy.</p>
<figure>
	<img src="/assets/images/2021-07-25/gpi-rel.png" alt="GPI interaction" width="360" height="200px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 5</b>: Interaction between the evaluation and improvement processes in GPI</figcaption>
</figure>

<h3 id="example">Example - Gambler’s Problem</h3>
<p>This example is taken from <em>Example 4.3</em> in the <em>Reinforcement Learning: An Introduction</em> book</p>
<figure>
	<img src="/assets/images/2021-07-25/example.png" alt="example" width="500" height="500px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 6</b>: Example - Gambler's Problem</figcaption>
</figure>

<p><strong>Solution code</strong><br />
The code can be found <a href="https://github.com/trunghng/reinforcement-learning-an-introduction-imp/blob/main/chapter-4/gambler.py">here</a>.</p>

<p><button type="button" class="collapsible" id="codeP">Click to show/hide the code</button></p>
<div class="codePanel" id="codePdata">
  <p><br /></p>

  <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">GOAL</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1">#For convenience, we introduce 2 dummy states: 0 and terminal state
</span><span class="n">states</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">GOAL</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s">'terminal'</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s">'non-terminal'</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">HEAD_PROB</span> <span class="o">=</span> <span class="mf">0.4</span>
<span class="n">GAMMA</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># discount factor
</span>

<span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">theta</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">states</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">V_set</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">policy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">V_set</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">V</span><span class="p">.</span><span class="n">copy</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">GOAL</span><span class="p">]:</span>
            <span class="n">old_value</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">].</span><span class="n">copy</span><span class="p">()</span>

            <span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">GOAL</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">new_value</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
                <span class="n">next_head_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">action</span>
                <span class="n">next_tail_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">action</span>
                <span class="n">head_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'terminal'</span><span class="p">]</span> <span class="k">if</span> <span class="n">next_head_state</span> <span class="o">==</span> <span class="n">GOAL</span> <span class="k">else</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'non-terminal'</span><span class="p">]</span>
                <span class="n">tail_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'non-terminal'</span><span class="p">]</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">HEAD_PROB</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_head_state</span><span class="p">])</span> <span class="o">+</span> \
                    <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">HEAD_PROB</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tail_reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_tail_state</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">value</span> <span class="o">&gt;</span> <span class="n">new_value</span><span class="p">:</span>
                    <span class="n">new_value</span> <span class="o">=</span> <span class="n">value</span>

            <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_value</span>
            <span class="n">delta</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">delta</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="n">old_value</span> <span class="o">-</span> <span class="n">V</span><span class="p">[</span><span class="n">state</span><span class="p">]))</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Max value changed: '</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">delta</span> <span class="o">&lt;</span> <span class="n">theta</span><span class="p">:</span>
            <span class="n">V_set</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">V</span><span class="p">)</span>
            <span class="k">break</span>

    <span class="k">for</span> <span class="n">state</span> <span class="ow">in</span> <span class="n">states</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">GOAL</span><span class="p">]:</span>
        <span class="n">values</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="mi">100</span> <span class="o">-</span> <span class="n">state</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">actions</span><span class="p">:</span>
            <span class="n">next_head_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">+</span> <span class="n">action</span>
            <span class="n">next_tail_state</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">-</span> <span class="n">action</span>
            <span class="n">head_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'terminal'</span><span class="p">]</span> <span class="k">if</span> <span class="n">next_head_state</span> <span class="o">==</span> <span class="n">GOAL</span> <span class="k">else</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'non-terminal'</span><span class="p">]</span>
            <span class="n">tail_reward</span> <span class="o">=</span> <span class="n">rewards</span><span class="p">[</span><span class="s">'non-terminal'</span><span class="p">]</span>
            <span class="n">values</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">HEAD_PROB</span> <span class="o">*</span> <span class="p">(</span><span class="n">head_reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_head_state</span><span class="p">])</span> <span class="o">+</span>
                          <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">HEAD_PROB</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">tail_reward</span> <span class="o">+</span> <span class="n">GAMMA</span> <span class="o">*</span> <span class="n">V</span><span class="p">[</span><span class="n">next_tail_state</span><span class="p">]))</span>
        <span class="n">policy</span><span class="p">[</span><span class="n">state</span><span class="p">]</span> <span class="o">=</span> <span class="n">actions</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">round</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="mi">4</span><span class="p">))</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">V_set</span><span class="p">,</span> <span class="n">policy</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mf">1e-13</span>
    <span class="n">value_funcs</span><span class="p">,</span> <span class="n">optimal_policy</span> <span class="o">=</span> <span class="n">value_iteration</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">optimal_value</span> <span class="o">=</span> <span class="n">value_funcs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="n">optimal_value</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">211</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sweep</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">value_funcs</span><span class="p">):</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'sweep {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">sweep</span><span class="p">))</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Capital'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Value estimates'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">'best'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">212</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">optimal_policy</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Capital'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'Final policy (stake)'</span><span class="p">)</span>

    <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'./gambler.png'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div>  </div>
</div>
<p>And here is our results after running the code</p>
<figure>
    <img src="/assets/images/2021-07-25/gambler.png" alt="gambler" width="450" height="900px" style="display: block; margin-left: auto; margin-right: auto;" />
    <figcaption style="text-align: center;font-style: italic;"><b>Figure 7</b>: Example - Gambler's Problem Result</figcaption>
</figure>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] David Silver. <a href="https://www.davidsilver.uk/teaching/">UCL course on RL</a></p>

<p>[3] Csaba Szepesvári. <a href="https://www.amazon.com/Algorithms-Reinforcement-Synthesis-Artificial-Intelligence/dp/1608454924">Algorithms for Reinforcement Learning</a></p>

<p>[4] A. Lazaric. <a href="http://researchers.lille.inria.fr/~lazaric/Webpage/MVA-RL_Course14_files/slides-lecture-02-handout.pdf">Markov Decision Processes and Dynamic Programming</a></p>

<p>[5] <a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic Programming</a></p>

<p>[6] <a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction">Shangtong Zhang’s repo</a></p>

<p>[7] <a href="https://stats.stackexchange.com/a/258783">Policy Improvement theorem</a></p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In the third step, the expression
\begin{equation}
\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]
\end{equation}
means ‘‘the discounted expected value when starting in state $s$, choosing action according to $\pi’$ for the next time step, and following $\pi$ thereafter”. And so on for the two, or n next steps. Therefore, we have that:
\begin{equation}
\mathbb{E}_{\pi’}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s\right]=\mathbb{E}\left[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s,A_t=\pi’(s)\right]
\end{equation} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>The idea of policy improvement also extends to stochastic policies. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Value iteration can be used in conjunction with action-value function, which takes the following update:
\begin{align}
q_{k+1}(s,a)&amp;\doteq\mathbb{E}\left[R_{t+1}+\gamma\max_{a’}q_k(S_{t+1},a’)|S_t=s,A_t=a\right] \\ &amp;=\sum_{s’,r}p(s’,r|s,a)\left[r+\gamma\max_{a’}q_k(s’,a’)\right]
\end{align}
Yep, that’s right, the sequence $\{q_k\}\to q_*$ as $k\to\infty$ at a geometric rate thanks to <a href="/artificial-intelligent/reinforcement-learning/2021/07/10/optimal-policy-existence.html#banach-fixed-pts">Banach’s fixed point theorem</a>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/artificial-intelligent/reinforcement-learning/2021/07/25/dp-in-mdp.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  "HTML-CSS": {availableFonts: []}
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
