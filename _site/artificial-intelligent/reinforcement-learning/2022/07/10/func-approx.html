<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<style type="text/css">
  #button {
    display: inline-block;
    background-color: #A5CCF1;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    visibility: hidden;
    z-index: 1000;
  }
  #button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
  }
  #button:hover {
    cursor: pointer;
    background-color: #333;
    text-decoration: none;
  }
  #button:active {
    background-color: #555;
  }
  #button.show {
    opacity: 1;
    visibility: visible;
  }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Function Approximation | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Function Approximation" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Function approximation" />
<meta property="og:description" content="Function approximation" />
<link rel="canonical" href="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html" />
<meta property="og:url" content="http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-10T15:26:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Function Approximation" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","description":"Function approximation","headline":"Function Approximation","dateModified":"2022-07-10T15:26:00+07:00","datePublished":"2022-07-10T15:26:00+07:00","url":"http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <a id="button"></a>
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Function approximation">
    <h1 class="post-title p-name" itemprop="name headline">Function Approximation</h1>
    <p><span>
      
        
        <a href="/tag/artificial-intelligent"><code class="highligher-rouge"><nobr>artificial-intelligent</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/function-approximation"><code class="highligher-rouge"><nobr>function-approximation</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2022-07-10T15:26:00+07:00" itemprop="datePublished">
        Jul 10, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>

  <!-- excerpt-end -->
  <ul>
    <li><a href="#on-policy-methods">On-policy Methods</a>
      <ul>
        <li><a href="#value-func-approx">Value-function Approximation</a></li>
        <li><a href="#pred-obj">The Prediction Objective</a></li>
        <li><a href="#grad-algs">Gradient-based algorithms</a>
          <ul>
            <li><a href="#stochastic-grad">Stochastic-gradient</a></li>
            <li><a href="#on-policy-semi-grad">Semi-gradient</a></li>
          </ul>
        </li>
        <li><a href="#lin-func-approx">Linear Function Approximation</a>
          <ul>
            <li><a href="#lin-methods">Linear Methods</a></li>
            <li><a href="#feature-cons">Feature Construction</a>
              <ul>
                <li><a href="#polynomial">Polynomial Basis</a></li>
                <li><a href="#fourier">Fourier Basis</a></li>
                <li><a href="#coarse-coding">Coarse Coding</a></li>
                <li><a href="#tile-coding">Tile Coding</a></li>
                <li><a href="#rbf">Radial Basis Functions</a></li>
              </ul>
            </li>
          </ul>
        </li>
        <li><a href="#lstd">Least-Squares TD</a></li>
        <li><a href="#ep-semi-grad-control">Episodic Semi-gradient Control</a></li>
        <li><a href="#semi-grad-n-step-sarsa">Semi-gradient n-step Sarsa</a></li>
      </ul>
    </li>
    <li><a href="#off-policy-methods">Off-policy Methods</a>
      <ul>
        <li><a href="#off-policy-semi-grad">Semi-gradient</a></li>
        <li><a href="#grad-td">Gradient-TD</a></li>
        <li><a href="#em-td">Emphatic-TD</a></li>
      </ul>
    </li>
  </ul>
</blockquote>

<h2 id="on-policy-methods">On-policy Methods</h2>
<p>So far in the series, we have gone through tabular methods, which are used to solve problems with small state and action spaces. For larger spaces, rather than getting the exact solutions, we now have to approximate the value of them. To start, we begin with on-policy approximation methods.</p>

<h3 id="value-func-approx">Value-function Approximation</h3>
<p>All of the prediction methods so far have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value” (or <em>update target</em>) for that state
\begin{equation}
s\mapsto u,
\end{equation}
where $s$ is the state updated and $u$ is the update target that $s$’s estimated value is shifted toward.</p>

<p>For example,</p>
<ul>
  <li>the MC update for value prediction is: $S_t\mapsto G_t$.</li>
  <li>the TD(0) update for value prediction is: $S_t\mapsto R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)$.</li>
  <li>the $n$-step TD update is: $S_t\mapsto G_{t:t+n}$.</li>
  <li>and in the DP, policy-evaluation update, $s\mapsto\mathbb{E}\big[R_{t+1}+\gamma\hat{v}(S_{t+1},\mathbf{w}_t)\vert S_t=s\big]$, an arbitrary $s$ is updated.</li>
</ul>

<p>Each update $s\mapsto u$ can be viewed as example of the desired input-output behavior of the value function. And when the outputs are numbers, like $u$, we call the process <strong>function approximation</strong>.</p>

<h3 id="pred-obj">The Prediction Objective</h3>
<p>In constrast to tabular case, where the solution of value function could be found equal to the true value function exactly, and an update at one state did not affect the others, with function approximation, it is imposible to find the exact value function of all states. And moreover, an update at one state also affects many others.</p>

<p>Hence, it is necessary to specify a state distribution $\mu(s)\geq0,\sum_s\mu(s)=1$, representing how much we care about the error (the difference between the approximate value $\hat{v}(s,\mathbf{w})$ and the true value $v_\pi(s)$) in each state $s$. Weighting this over the state space $\mathcal{S}$ by $\mu$, we obtain a natural objective function, called the <em>Mean Squared Value Error</em>, denoted as $\overline{\text{VE}}$:
\begin{equation}
\overline{\text{VE}}(\mathbf{w})\doteq\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
The distribution $\mu(s)$ is usually chosen as the fraction of time spent in $s$ (number of time $s$ visited divived by total amount of visits). Under on-policy training this is called the <em>on-policy distribution</em>.</p>

<ul>
  <li>In continuing tasks, the on-policy distribution is the stationary distribution under $\pi$.</li>
  <li>In episodic tasks, the on-policy distribution depends on how the initial states are chosen.
    <ul>
      <li>Let $h(s)$ denote the probability that an episode begins in each state $s$, and let $\eta(s)$ denote the number of time steps spent, on average, in state $s$ in a single episode
  \begin{equation}
  \eta(s)=h(s)+\sum_\bar{s}\eta(\bar{s})\sum_a\pi(a\vert\bar{s})p(s\vert\bar{s},a),\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}
  This system of equation can be solved for the expected number of visits $\eta(s)$. The on-policy distribution is then
  \begin{equation}
  \mu(s)=\frac{\eta(s)}{\sum_{s’}\eta(s’)},\hspace{1cm}\forall s\in\mathcal{S}
  \end{equation}</li>
    </ul>
  </li>
</ul>

<h3 id="grad-algs">Gradient-based algorithms</h3>
<p>To solve the least squares problem, we are going to use a popular method, named <strong>Gradient descent</strong>.</p>

<p>Say, consider a differentiable function $J(\mathbf{w})$ of parameter vector $\mathbf{w}$.</p>

<p>The gradient of $J(\mathbf{w})$ w.r.t $\mathbf{w}$ is defined to be
\begin{equation}
\nabla_{\mathbf{w}}J(\mathbf{w})=\left(\begin{smallmatrix}\dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_1} \\ \vdots \\ \dfrac{\partial J(\mathbf{w})}{\partial\mathbf{w}_d}\end{smallmatrix}\right)
\end{equation}
The idea of Gradient descent is to minimize the objective function $J(\mathbf{w})$, we repeatly move $\mathbf{w}$ in the direction of steepest decrease of $J$, which is the direction of negative gradient $-\nabla_\mathbf{w}J(\mathbf{w})$.</p>

<p>Thus, we have the update rule of Gradient descent:
\begin{equation}
\mathbf{w}\leftarrow\mathbf{w}-\dfrac{1}{2}\alpha\nabla_\mathbf{w}J(\mathbf{w}),
\end{equation}
where $\alpha$ is a positive step-size parameter.</p>

<h4 id="stochastic-grad">Stochastic-gradient</h4>
<p>Apply gradient descent to our problem, which is we have to find the minimization of
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\sum_{s\in\mathcal{S}}\mu(s)\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
Since $\mu(s)$ is the state distribution over state space $\mathcal{S}$, we can rewrite $\overline{\text{VE}}$ as
\begin{equation}
\overline{\text{VE}}(\mathbf{w})=\mathbb{E}_{s\sim\mu}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]^2
\end{equation}
By the update we have defined earlier, in each step, we need to decrease $\mathbf{w}$ by an amount of
\begin{equation}
\Delta\mathbf{w}=-\dfrac{1}{2}\alpha\nabla_\mathbf{w}\overline{\text{VE}}(\mathbf{w})=\alpha\mathbb{E}\Big[v_\pi(s)-\hat{v}(s,\mathbf{w})\Big]\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})
\end{equation}
Using <strong>Stochastic Gradient descent (SGD)</strong>, and since the Monte Carlo target $G_t$ by definition is an unbiased estimate of $v_\pi(S_t)$ , we sample the gradient:
\begin{equation}
\Delta\mathbf{w}=\alpha(G_t-\hat{v}(S_t,\mathbf{w}))\nabla_\mathbf{w}\hat{v}(S_t,\mathbf{w})
\end{equation}
which gives us pseudocode of the algorithm:</p>
<figure>
	<img src="/assets/images/2022-07-10/sgd_mc.png" alt="SGD Monte Carlo" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h4 id="on-policy-semi-grad">Semi-gradient</h4>
<p>If instead of using MC target $G_t$, we use the bootstrapping targets such as $n$-step return $G_{t:t+n}$ or the DP target $\sum_{a,s’,r}\pi(a\vert S_t)p(s’,r\vert S_t,a)\left[r+\gamma\hat{v}(s’,\mathbf{w}_t)\right]$, which all depend on the current value of the weight vector $\mathbf{w}_t$, and then implies that they will be biased, and will not produce a true gradient-descent method.</p>

<p>Such methods are called <strong>semi-gradient</strong> since they include only a part of the gradient.</p>
<figure>
	<img src="/assets/images/2022-07-10/semi_gd.png" alt="Semi-gradient TD(0)" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"></figcaption>
</figure>

<h3 id="lin-func-approx">Linear Function Approximation</h3>
<p>One of the most crucial special cases of function approximation is that in which the approximate function, $\hat{v}(\cdot,\mathbf{w})$, is a linear function of the weight vector, $\mathbf{w}$.</p>

<p>Corresponding to every state $s$, there is a real-valued vector $\mathbf{x}(s)\doteq\left(x_1(s),x_2(s),\dots,x_d(s)\right)^\intercal$, with the same number of components with $\mathbf{w}$.</p>

<h4 id="lin-methods">Linear Methods</h4>
<p>Linear methods approximate value function by the inner product between $\mathbf{w}$ and $\mathbf{x}(s)$:
\begin{equation}
\hat{v}(s,\mathbf{w})\doteq\mathbf{w}^\intercal\mathbf{x}(s)=\sum_{i=1}^{d}w_ix_i(s)\tag{1}\label{1}
\end{equation}
The vector $\mathbf{x}(s)$ is called a <em>feature vector</em> representing state $s$, i.e., $x_i:\mathcal{S}\to\mathbb{R}$.</p>

<p>For linear methods, features are <em>basis functions</em> because they form a linear basis for the set of approximate functions. Constructing $d$-dimensional feature vectors to represent states is the same as selecting a set of $d$ basis functions.</p>

<p>From \eqref{1}, when using SGD updates with linear approximation, we have the gradient of the approximate value function w.r.t $\mathbf{w}$ is
\begin{equation}
\nabla_\mathbf{w}\hat{v}(s,\mathbf{w})=\mathbf{x}(s)
\end{equation}
Thus, with linear approximation, the SGD update can be rewrite as
\begin{equation}
\mathbf{w}_{t+1}\doteq\mathbf{w}_t+\alpha\left[G_t-\hat{v}(S_t,\mathbf{w}_t)\right]\mathbf{x}(S_t)
\end{equation}</p>

<p>In the linear case, there is only one optimum, and thus any method that is guaranteed to converge to or near a local optimum is automatically guaranteed to converge to or near the global optimum.</p>
<ul>
  <li>The gradient MC algorithm in the previous section converges to the global optimum of the $\overline{\text{VE}}$ under linear function approximation if $\alpha$ is reduced over time according to the usual conditions.</li>
  <li>The semi-gradient TD algorithm also converges under linear approximation.
    <ul>
      <li>Recall that, at each time $t$, the semi-gradient TD update is
  \begin{align}
  \mathbf{w}_{t+1}&amp;\doteq\mathbf{w}_t+\alpha\left(R_{t+1}+\gamma\mathbf{w}_t^\intercal\mathbf{x}_{t+1}-\mathbf{w}_t^\intercal\mathbf{x}_t\right)\mathbf{x}_t \\ &amp;=\mathbf{w}_t+\alpha\left(R_{t+1}\mathbf{x}_t-\mathbf{x}_t(\mathbf{x}_t-\gamma\mathbf{x}_{t+1})^\intercal\mathbf{w}_t\right),
  \end{align}
  where $\mathbf{x}_t=\mathbf{x}(S_t)$. Once the system has reached steady state, for any given $\mathbf{w}_t$, the expected next weight vector can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\mathbf{w}_t+\alpha\left(\mathbf{b}-\mathbf{A}\mathbf{w}_t\right),\tag{2}\label{2}
  \end{equation}
  where
  \begin{align}
  \mathbf{b}&amp;\doteq\mathbb{E}\left[R_{t+1}\mathbf{x}_t\right]\in\mathbb{R}^d, \\ \mathbf{A}&amp;\doteq\mathbb{E}\left[\mathbf{x}_t\left(\mathbf{x}_t-\gamma\mathbf{x}_{t+1}\right)^\intercal\right]\in\mathbb{R}^d\times\mathbb{R}^d\tag{3}\label{3}
  \end{align}
  From \eqref{2}, it is easily seen that if the system converges, it must converges to the weight vector $\mathbf{w}_{\text{TD}}$ at which
  \begin{align}
  \mathbf{b}-\mathbf{A}\mathbf{w}_{\text{TD}}&amp;=\mathbf{0} \\ \mathbf{w}_{\text{TD}}&amp;=\mathbf{A}^{-1}\mathbf{b}
  \end{align}
  This quantity, $\mathbf{w}_{\text{TD}}$, is called the <em>TD fixed point</em>. And in fact, linear semi-gradient TD(0) converges to this point.</li>
      <li><strong>Proof</strong>:<br />
  We have \eqref{2} can be written as
  \begin{equation}
  \mathbb{E}\left[\mathbf{w}_{t+1}\vert\mathbf{w}_t\right]=\left(\mathbf{I}-\alpha\mathbf{A}\right)\mathbf{w}_t+\alpha\mathbf{b}
  \end{equation}
  The idea of the proof is prove that the matrix $\mathbf{A}$ in \eqref{3} is a positive definite matrix<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>, since $\mathbf{w}_t$ will be reduced toward zero whenever $\mathbf{A}$ is positive definite.<br />
  For linear TD(0), in the continuing case with $\gamma&lt;1$, the matrix $\mathbf{A}$ can be written as
  \begin{align}
  \mathbf{A}&amp;=\sum_s\mu(s)\sum_a\pi(a\vert s)\sum_{r,s’}p(r,s’\vert s,a)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;=\sum_s\mu(s)\sum_{s’}p(s’\vert s)\mathbf{x}(s)\big(\mathbf{x}(s)-\gamma\mathbf{x}(s’)\big)^\intercal \\ &amp;=\sum_s\mu(s)\mathbf{x}(s)\Big(\mathbf{x}(s)-\gamma\sum_{s’}p(s’\vert s)\mathbf{x}(s’)\Big)^\intercal \\ &amp;=\mathbf{X}^\intercal\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})\mathbf{X},\tag{4}\label{4}
  \end{align}
  where
        <ul>
          <li>$\mu(s)$ is the stationary distribution under $\pi$;</li>
          <li>$p(s’\vert s)$ is the probability transition from $s$ to $s’$ under policy $\pi$;</li>
          <li>$\mathbf{P}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ matrix of these probabilities;</li>
          <li>$\mathbf{D}$ is the $\vert\mathcal{S}\vert\times\vert\mathcal{S}\vert$ diagonal matrix with the $\mu(s)$ on its diagonal;</li>
          <li>$\mathbf{X}$ is the $\vert\mathcal{S}\vert\times d$ matrix with $\mathbf{x}(s)$ as its row.</li>
        </ul>

        <p>Hence, it is clear that the positive definiteness of $A$ depends on the matrix $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ in \eqref{4}.</p>

        <p>To continue proving the posititive definiteness of $\mathbf{A}$, we use two lemmas:</p>
        <ul>
          <li><strong>Lemma 1</strong>: <em>A square matrix $\mathbf{A}$ is positive definite if $\mathbf{A}+\mathbf{A}^\intercal$</em> is positive definite.</li>
          <li><strong>Lemma 2</strong>: <em>If $\mathbf{A}$ is a real, symmetric, and strictly diagonally dominant matrix with positive diagonal entries, then $\mathbf{A}$ is positive definite</em>.</li>
        </ul>

        <p>With these lemmas, plus since $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ has positive diagonal entries and negative off-diagonal entries, so all we have to show is that each row sum plus the corresponding column sum is positive. The row sums are all positive because $\mathbf{P}$ is a stochastic matrix and $\gamma&lt;1$. Thus the problem remains to show that the column sums are nonnegative.</p>

        <p>Let $\mathbf{1}$ denote the column vector with all components equal to $1$ and $\boldsymbol{\mu}(s)$ denote the vectorized version of $\mu(s)$: i.e., $\boldsymbol{\mu}\in\mathbb{R}^{\vert\mathcal{S}\vert}$. Thus, $\boldsymbol{\mu}=\mathbf{P}^\intercal\boldsymbol{\mu}$ since $\mu(s)$ is the stationary distribution. We have:
  \begin{align}
  \mathbf{1}^\intercal\mathbf{D}\left(\mathbf{I}-\gamma\mathbf{P}\right)&amp;=\boldsymbol{\mu}^\intercal\left(\mathbf{I}-\gamma\mathbf{P}\right) \\ &amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal\mathbf{P} \\ &amp;=\boldsymbol{\mu}^\intercal-\gamma\boldsymbol{\mu}^\intercal \\ &amp;=\left(1-\gamma\right)\boldsymbol{\mu}^\intercal,
  \end{align}
  which implies that the column sums of $\mathbf{D}(\mathbf{I}-\gamma\mathbf{P})$ are positive.</p>
      </li>
      <li>At the TD fixed point, it has also been proven (in the continuing case) that $\overline{\text{VE}}$ is within a bounded expansion of the lowest possible error
  \begin{equation}
  \overline{\text{VE}}(\mathbf{w}_{\text{TD}})\leq\dfrac{1}{1-\gamma}\min_{\mathbf{w}}\overline{\text{VE}}(\mathbf{w})
  \end{equation}</li>
    </ul>
  </li>
</ul>

<h4 id="feature-cons">Feature Construction</h4>
<p>There are various ways to define features. The simpliest way is to use each variable directly as a basis function along with a constant function. However, most interesting function are too complex to be represented in this way.</p>

<h5 id="polynomial">Polynomial Basis</h5>

<h5 id="fourier">Fourier Basis</h5>
<p><strong>Fourier series</strong> is applied widely in Maths to approximate a periodic function<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>. For example:</p>

<figure>
	<img src="/assets/images/2022-07-10/fourier_series.gif" alt="Fourier series visualization" width="480" height="360px" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption style="text-align: center;font-style: italic;"><b>Figure 1</b>: Four partial sums (Fourier series) of lengths 1, 2, 3, and 4 terms, showing how the approximation to a square wave improves as the number of terms increases: where $f_1(\theta)=\frac{4\sin\theta}{\pi},f_2(\theta)=\frac{4\sin3\theta}{3\pi},f_3(\theta)=\frac{4\sin5\theta}{5\pi}$ and $f_4(\theta)=\frac{4\sin7\theta}{7\pi}$. The code can be found <span><a href="https://github.com/trunghng/maths-visualization/blob/main/fourier-series/fourier_series.py">here</a></span></figcaption>
</figure>
<p><br /></p>

<h5 id="coarse-coding">Coarse Coding</h5>

<h5 id="tile-coding">Tile Coding</h5>

<h5 id="rbf">Radial Basis Functions</h5>

<h3 id="lstd">Least-Squares TD</h3>

<h2 id="references">References</h2>
<p>[1] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a></p>

<p>[2] Deepmind x UCL. <a href="https://www.deepmind.com/learning-resources/reinforcement-learning-lecture-series-2021">Reinforcement Learning Lecture Series 2021</a></p>

<p>[3] Sutton, R. S. (1988). <a href="doi:10.1007/bf00115009">Learning to predict by the methods of temporal differences</a>. Machine Learning, 3(1), 9–44.</p>

<p>[4] Konidaris, G. &amp; Osentoski, S. &amp; Thomas, P.. <a href="#https://dl.acm.org/doi/10.5555/2900423.2900483">Value Function Approximation in Reinforcement Learning Using the Fourier Basis</a>. AAAI Conference on Artificial Intelligence, North America, aug. 2011.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>A $n\times n$ matrix $A$ is called <em>positive definite</em> if and only if for any non-zero vector $\mathbf{x}\in\mathbb{R}^n$, we always have
\begin{equation}
\mathbf{x}^\intercal\mathbf{A}\mathbf{x}&gt;0
\end{equation} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>A function $f$ is periodic with period $T$ if
\begin{equation}
f(x+T)=f(x),\forall x
\end{equation} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/artificial-intelligent/reinforcement-learning/2022/07/10/func-approx.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
  "HTML-CSS": {availableFonts: []}
  });
</script>
<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
<script type="text/javascript">
  var btn = $('#button');

  $(window).scroll(function() {
    if ($(window).scrollTop() > 300) {
      btn.addClass('show');
    } else {
      btn.removeClass('show');
    }
  });

  btn.on('click', function(e) {
    e.preventDefault();
    $('html, body').animate({scrollTop:0}, '300');
  });
</script>