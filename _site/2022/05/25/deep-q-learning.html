<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<style type="text/css">
  #button {
    display: inline-block;
    background-color: #A5CCF1;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    visibility: hidden;
    z-index: 1000;
  }
  #button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
  }
  #button:hover {
    cursor: pointer;
    background-color: #333;
    text-decoration: none;
  }
  #button:active {
    background-color: #555;
  }
  #button.show {
    opacity: 1;
    visibility: visible;
  }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Deep Q-learning | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.2.1" />
<meta property="og:title" content="Deep Q-learning" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Deep Q-learning and variants" />
<meta property="og:description" content="Deep Q-learning and variants" />
<link rel="canonical" href="http://localhost:4000/2022/05/25/deep-q-learning.html" />
<meta property="og:url" content="http://localhost:4000/2022/05/25/deep-q-learning.html" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-25T15:26:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Deep Q-learning" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Trung H. Nguyen"},"@type":"BlogPosting","description":"Deep Q-learning and variants","headline":"Deep Q-learning","dateModified":"2022-05-25T15:26:00+07:00","datePublished":"2022-05-25T15:26:00+07:00","url":"http://localhost:4000/2022/05/25/deep-q-learning.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/05/25/deep-q-learning.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <a id="button"></a>
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Deep Q-learning and variants">
    <h1 class="post-title p-name" itemprop="name headline">Deep Q-learning</h1>
    <p><span>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/deep-learning"><code class="highligher-rouge"><nobr>deep-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/function-approximation"><code class="highligher-rouge"><nobr>function-approximation</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/q-learning"><code class="highligher-rouge"><nobr>q-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/my-rl"><code class="highligher-rouge"><nobr>my-rl</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2022-05-25T15:26:00+07:00" itemprop="datePublished">
        May 25, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <!-- excerpt-end -->
<ul>
  <li><a href="#q-value-iter">Q-value iteration</a></li>
  <li><a href="#q-learning">Q-learning</a></li>
  <li><a href="#nn-q-learning">Neural networks with Q-learning</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="q-value-iter">Q-value iteration</h2>
<p>Recall that in the post <a href="/2021/06/27/mdp-bellman-eqn.html"><strong>Markov Decision Processes, Bellman equations</strong></a>, we have defined the <strong>state-value function</strong> for a policy $\pi$ to measure how good the state $s$ is, given as
\begin{equation}
V_\pi(s)=\sum_{a}\pi(a\vert s)\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{equation}
From the definition of $V_\pi(s)$, we have continued to define the Bellman equation for the optimal value at state $s$, denoted $V^*(s)$:
\begin{equation}
V^*(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big],\label{eq:qvi.1}
\end{equation}
which characterizes the optimal value of state $s$ in terms of the optimal values of successor state $s’$.</p>

<p>Then, with <a href="/2021/07/25/dp-in-mdp.html"><strong>Dynamic programming</strong></a>, we can solve \eqref{eq:qvi.1} by an iterative method, called <a href="/2021/07/25/dp-in-mdp.html#value-iteration"><strong>value iteration</strong></a>, given as
\begin{equation}
V_{k+1}(s)=\max_{a}\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_k(s’)\big]\hspace{1cm}\forall s\in\mathcal{S}
\end{equation}
For an arbitrary initial $V_0(s)$, the iteration, or the sequence $\{V_k\}$, will eventually converges to the optimal value function $V^*(s)$. This can be shown by applying the <a href="/2021/07/10/optimal-policy-existence.html"><strong>Banach’s fixed point theorem</strong></a>, the one we have also used to prove the existence of the optimal policy, to prove that the iteration from $V_k(s)$ to $V_{k+1}(s)$ is a contraction mapping.</p>

<p>Details for value iteration method can be seen in the following pseudocode.</p>
<figure>
	<img src="/assets/images/2022-05-25/value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<p>Remember that along with the state-value function $V_\pi(s)$, we have also defined the <strong>action-value function</strong>, or <strong>Q-values</strong> for a policy $\pi$, denoted $Q$, given by
\begin{align}
Q_\pi(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\sum_{a’}\pi(a’\vert s’)Q_\pi(s’,a’)\right] \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V_\pi(s’)\big]
\end{align}
which measures how good it is to be in state $s$ and take action $a$.</p>

<p>Analogously, we also have the Bellman equation for the optimal action-value function, given as
\begin{align}
Q^*(s,a)&amp;=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q^*(s’,a’)\right]\label{eq:qvi.2} \\ &amp;=\sum_{s’}P(s’\vert s,a)\big[R(s,a,s’)+\gamma V^*(s’)\big]\label{eq:qvi.3}
\end{align}
The optimal value $Q^*(s,a)$ gives us the expected discounted cumulative reward for executing action $a$ at state $s$ and following the optimal policy, $\pi^*$, thereafter.</p>

<p>Equation \eqref{eq:qvi.3} allows us to write
\begin{equation}
V^*(s)=\max_a Q^*(s,a)
\end{equation}
Hence, analogy to the state-value function, we can also apply Dynamic programming to develop an iterative method in order to solve \eqref{eq:qvi.2}, called <strong>Q-value iteration</strong>. The method is given by the update rule
\begin{equation}
Q_{k+1}(s,a)=\sum_{s’}P(s’\vert s,a)\left[R(s,a,s’)+\gamma\max_{a’}Q_k(s’,a’)\right]\label{eq:qvi.4}
\end{equation}
This iteration, given an initial value $Q_0(s,a)$, eventually will also converges to the optimal Q-values $Q^*(s,a)$ due to the relationship between $V$ and $Q$ as defined above. Pseudocode for Q-value iteration is given below.</p>
<figure>
	<img src="/assets/images/2022-05-25/q-value-iteration.png" alt="value iteration pseudocode" style="display: block; margin-left: auto; margin-right: auto;" />
	<figcaption></figcaption>
</figure>

<h2 id="q-learning">Q-learning</h2>
<p>The update formula \eqref{eq:qvi.4} can be rewritten as an expected update
\begin{equation}
Q_{k+1}(s,a)=\mathbb{E}_{s’\sim P(s’\vert s,a)}\left[R(s,a,s’)+\gamma\max_{a’}Q_k(s’,a’)\right]\label{eq:ql.1}
\end{equation}
It is noticeable that the above update rule requires the transition model $P(s’\vert s,a)$. And since sample mean is an unbiased estimator of the population mean, or in other words, the expectation in \eqref{eq:ql.1} can be approximated by sampling, as</p>
<ul id="number-list">
	<li>At a state, taking (sampling) action $a$, we get the next state $s'\sim P(s'\vert s,a)$.</li>
	<li>Consider the old estimate $Q_k(s,a)$.</li>
	<li>
		Consider the new sample estimate (target):
		\begin{equation}
		Q_\text{target}=R(s,a,s')+\gamma\max_{a'}Q_k(s',a')
		\end{equation}
	</li>
	<li>
		Append the new estimate into a running average to iteratively update Q-values:
		\begin{align}
		Q_{k+1}(s,a)&amp;=(1-\alpha)Q_k(s,a)+\alpha Q_\text{target} \\ &amp;=(1-\alpha)Q_k(s,a)+\alpha\left[R(s,a,s')+\gamma\max_{a'}Q_k(s',a')\right]
		\end{align}
	</li>
</ul>

<p>This update rule is in form of a stochastic process, and thus, can be <a href="#q-learning-td-convergence">proved</a> to be converged under the <a href="/2022/01/31/td-learning.html#stochastic-approx-condition">stochastic approximation conditions</a> for $\alpha$.
\begin{equation}
\sum_{t=1}^{\infty}\alpha_t(s,a)=\infty\hspace{1cm}\text{and}\hspace{1cm}\sum_{t=1}^{\infty}\alpha_t^2(s,a)&lt;\infty,
\end{equation}
for all $(s,a)\in\mathcal{S}\times\mathcal{A}$.</p>

<h2 id="nn-q-learning">Neural networks with Q-learning</h2>
<p>Recall that</p>

<h2 id="references">References</h2>
<p><span id="q-learning-td-convergence">[1] Tommi Jaakkola, Michael I. Jordan, Satinder P. Singh. <a href="https://people.eecs.berkeley.edu/~jordan/papers/AIM-1441.ps">On the Convergence of Stochastic Iterative Dynamic Programming Algorithms</a>. A.I. Memo No. 1441, 1993.</span></p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>.</p>

<p>[3] Vlad Mnih, et al. <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">Playing Atari with Deep Reinforcement Learning</a>, 2013.</p>

<p>[4] John N Tsitsiklis and Benjamin Van Roy. <a href="">An analysis of temporal-difference learning with function approximation</a>. Automatic Control, IEEE Transactions on, 42(5):674–690, 1997.</p>

<p>[5] Vlad Mnih, et al. <a href="https://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning">Human Level Control Through Deep Reinforcement Learning</a>. Nature, 2015.</p>

<p>[6] Hado van Hasselt. <a href="https://proceedings.neurips.cc/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf">Double Q-learning</a>. NIPS 2010.</p>

<p>[7] Hado van Hasselt, Arthur Guez, David Silver. <a href="https://arxiv.org/abs/1509.06461">Deep Reinforcement Learning with Double Q-learning</a>. AAAI16, 2016.</p>

<p>[8] Pieter Abbeel. <a href="https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0">Foundations of Deep RL Series</a>, 2021.</p>

<h2 id="footnotes">Footnotes</h2>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/2022/05/25/deep-q-learning.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
<script type="text/javascript">
  var btn = $('#button');

  $(window).scroll(function() {
    if ($(window).scrollTop() > 300) {
      btn.addClass('show');
    } else {
      btn.removeClass('show');
    }
  });

  btn.on('click', function(e) {
    e.preventDefault();
    $('html, body').animate({scrollTop:0}, '300');
  });
</script>