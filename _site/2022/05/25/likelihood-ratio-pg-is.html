<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<style type="text/css">
  #button {
    display: inline-block;
    background-color: #A5CCF1;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    visibility: hidden;
    z-index: 1000;
  }
  #button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
  }
  #button:hover {
    cursor: pointer;
    background-color: #333;
    text-decoration: none;
  }
  #button:active {
    background-color: #555;
  }
  #button.show {
    opacity: 1;
    visibility: visible;
  }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Likelihood Ratio Policy Gradient via Importance Sampling | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Likelihood Ratio Policy Gradient via Importance Sampling" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="how likelihood ratio policy gradient can be derived from importance sampling method" />
<meta property="og:description" content="how likelihood ratio policy gradient can be derived from importance sampling method" />
<link rel="canonical" href="http://localhost:4000/2022/05/25/likelihood-ratio-pg-is.html" />
<meta property="og:url" content="http://localhost:4000/2022/05/25/likelihood-ratio-pg-is.html" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-25T15:26:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Likelihood Ratio Policy Gradient via Importance Sampling" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Trung H. Nguyen"},"dateModified":"2022-05-25T15:26:00+07:00","datePublished":"2022-05-25T15:26:00+07:00","description":"how likelihood ratio policy gradient can be derived from importance sampling method","headline":"Likelihood Ratio Policy Gradient via Importance Sampling","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/05/25/likelihood-ratio-pg-is.html"},"url":"http://localhost:4000/2022/05/25/likelihood-ratio-pg-is.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <a id="button"></a>
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="how likelihood ratio policy gradient can be derived from importance sampling method">
    <h1 class="post-title p-name" itemprop="name headline">Likelihood Ratio Policy Gradient via Importance Sampling</h1>
    <p><span>
      
        
        <a href="/tag/reinforcement-learning"><code class="highligher-rouge"><nobr>reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/policy-gradient"><code class="highligher-rouge"><nobr>policy-gradient</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/importance-sampling"><code class="highligher-rouge"><nobr>importance-sampling</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/my-rl"><code class="highligher-rouge"><nobr>my-rl</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2022-05-25T15:26:00+07:00" itemprop="datePublished">
        May 25, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>Connection between Likelihood ratio policy gradient method and Importance sampling method.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#likelihood-ratio-pg">Likelihood Ratio Policy Gradient</a></li>
  <li><a href="#is">Importance Sampling</a></li>
  <li><a href="#likelihood-ratio-pg-is">Likelihood Ratio Policy Gradient via IS</a></li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>An infinite-horizon discounted <strong>Markov Decision Process</strong> (<strong>MDP</strong>) is defined as the tuple $(\mathcal{S},\mathcal{A},P,r,\rho_0,\gamma)$, where</p>
<ul>
  <li>$\mathcal{S}$ is a finite set of states, or <strong>state space</strong>.</li>
  <li>$\mathcal{A}$ is a finite set of actions, or <strong>action space</strong>.</li>
  <li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to\mathbb{R}$ is the <strong>transition probability distribution</strong>, i.e. $P(s,a,s’)=P(s’\vert s,a)$ denotes the probability of transitioning to state $s’$ when taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the <strong>reward function</strong>.</li>
  <li>$\rho_0:\mathcal{S}\to\mathbb{R}$ is the distribution of the initial state $s_0$.</li>
  <li>$\gamma\in(0,1)$ is the <strong>discount factor</strong>.</li>
</ul>

<p>A (stochastic) <strong>policy</strong>, denoted $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$, is a mapping from states to probabilities of selecting each possible action.</p>

<p>Let $\eta(\pi)$ denoted the expected cumulative discounted reward when starting at initial state $s_0$ and following $\pi$ thereafter
\begin{equation}
\eta(\pi)=\mathbb{E}_{s_0,a_0,\ldots}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right],
\end{equation}
where
\begin{equation}
s_0\sim\rho_0(s_0),\hspace{1cm}a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)
\end{equation}
For a policy $\pi$, to measure how good it is to be at a state, or how good it is to take action $a$ at state $s$, we use <strong>state value function</strong>, denoted $V_\pi(s)$, and <strong>action value function</strong>, referred $Q_\pi(s,a)$. In particular, these value functions are defined as the expected return
\begin{align}
V_\pi(s_t)&amp;=\mathbb{E}_{a_t,s_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right], \\ Q_\pi(s_t,a_t)&amp;=\mathbb{E}_{s_{t+1},a_{t+1},\ldots}\left[\sum_{k=0}^{\infty}\gamma^k r(s_{t+k},a_{t+k})\right],
\end{align}
where
\begin{equation}
a_t\sim\pi(a_t\vert s_t),\hspace{1cm}s_{t+1}\sim P(s_{t+1}\vert s_t,a_t)\hspace{1cm}t\geq 0
\end{equation}</p>

<h2 id="likelihood-ratio-pg">Likelihood Ratio Policy Gradient</h2>
<p>Let $H$ denote the <strong>horizon</strong> of an MDP<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. Consider <strong>likelihood ratio policy gradient</strong> problem, in which the policy $\pi_\theta$ is parameterized by a vector $\theta\in\mathbb{R}^n$. The expected return of $\pi_\theta$ is then given by
\begin{equation}
\eta(\pi_\theta)=\mathbb{E}_{P(\tau;\theta)}\left[\left.\sum_{t=0}^{H-1}\gamma^t r(s_t,a_t)\right\vert\pi_\theta\right]=\sum_{\tau}P(\tau;\theta)R(\tau),\label{eq:lrp.1}
\end{equation}
where</p>
<ul>
  <li>$P(\tau;\theta)$ is the probability distribution induced by the policy $\pi_\theta$, i.e. $s_t$</li>
  <li>$\tau=(s_0,a_0,s_1,a_1,\ldots,s_H,a_H)$ are trajectories generated by rolls out, i.e. $\tau\sim P(\tau;\theta)$.</li>
  <li>$R(\tau)$ is the discounted cumulative rewards along the trajectory $\tau$, given as
\begin{equation}
R(\tau)=\sum_{t=0}^{H-1}\gamma^t r(s_t,a_t)
\end{equation}</li>
</ul>

<p>The likelihood ratio policy gradient performs a SGA (stochastic gradient ascent) over the policy parameter space $\Theta$ to find a local optimum of $\eta(\pi_\theta)$ by taking into account the policy gradient
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\nabla_\theta\sum_{\tau}P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\nabla_\theta\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_{\tau}P(\tau;\theta)\nabla_\theta\log P(\tau;\theta)R(\tau) \\ &amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big]\label{eq:lrp.2}
\end{align}
This gradient can be approximated with empirical estimate from $m$ trajectories $\tau^{(1)},\ldots,\tau^{(m)}$ under policy $\pi_\theta$
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta P(\tau;\theta)R(\tau)\Big] \\ &amp;\approx\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\hat{g},
\end{align}
which is an unbiased estimate of the policy gradient.</p>

<p>Additionally, since
\begin{align}
\nabla_\theta\log P(\tau^{(i)};\theta)&amp;=\nabla_\theta\log\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\ &amp;=\nabla_\theta\sum_{t=0}^{H-1}\log P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})+\nabla_\theta\sum_{t=0}^{H-1}\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}) \\ &amp;=\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)}),
\end{align}
we can rewrite the policy gradient estimate in a form which no longer require the dynamics model
\begin{equation}
\hat{g}=\frac{1}{m}\sum_{i=1}^{m}\nabla_\theta\log P(\tau^{(i)};\theta)R(\tau^{(i)})=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})R(\tau^{(i)})
\end{equation}
Moreover, as
\begin{equation}
\mathbb{E}_{P(\tau\vert\theta)}\Big[\nabla_\theta\log P(\tau;\theta)\Big]=\nabla_\theta\sum_\tau P(\tau;\theta)=\nabla_\theta 1=\mathbf{0},
\end{equation}
a constant baseline in terms of $\theta$ (i.e. independent of $\theta$) $b$ can be inserted into \eqref{eq:lrp.2} to reduce the variance (where $b$ is a vector which can be optimized to minimize the variance). In particular
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{P(\tau;\theta)}\Big[\nabla_\theta\log P(\tau;\theta)(R(\tau)-b)\Big] \\ &amp;\approx\frac{1}{m}\nabla_\theta\log P(\tau^{(i)};\theta)\left(R(\tau^{(i)})-b\right) \\ &amp;=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)})-b\right)=\hat{g}
\end{align}
By separating $R(\tau^{(i)})$ into sum of discounted rewards from the past, which does not depend on the current action $a_t^{(i)}$ (and thus can be removed to reduce the variance), and sum of discounted future rewards, we can continue to decompose the estimator $\hat{g}$ as
\begin{align}
\hat{g}&amp;=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(R(\tau^{(i)}-b)\right) \\ &amp;=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\Bigg[\sum_{k=0}^{t-1}r(s_k^{(i)},a_k^{(i)})+\left(\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})\right)-b\Bigg] \\ &amp;=\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})\left(\sum_{k=t}^{H-1}r(s_k^{(i)},a_k^{(i)})-b\right)
\end{align}
These following are some possible choices of baseline $b$.</p>
<ul id="number-list">
	<li>
		<b>Average rewards.</b>
		\begin{equation}
		b=\mathbb{E}\big[R(\tau)\big]\approx\frac{1}{m}\sum_{i=1}^{m}R(\tau^{(i)})
		\end{equation}
	</li>
	<li>
		<b>Optimal baseline</b>.
		\begin{equation}
		b=\frac{\sum_{i=1}^{m}\left(\nabla_\theta\log P(\tau^{(i)};\theta)\right)^2 R(\tau^{(i)})}{\sum_{i=1}^{m}\left(\nabla_\theta\log P(\tau^{(i)};\theta)\right)^2}
		\end{equation}
	</li>
	<li>
		<b>Time-dependent baseline</b>.
		\begin{equation}
		b_t=\frac{1}{m}\sum_{i=1}^{m}\sum_{k=t}^{H-1}R(s_k^{(i)},a_k^{(i)})
		\end{equation}
	</li>
	<li>
		<b>State-dependent baseline</b>.
		\begin{equation}
		b(s_t^{(i)})=\mathbb{E}_\pi\left[\sum_{k=t}^{H-1}\gamma^k r(s_k^{(i)},a_k^{(i)})\right]=V_\pi(s_t^{(i)})
		\end{equation}
	</li>
</ul>

<h2 id="is">Importance Sampling</h2>
<p>Consider a function $f$ and a probability measure $P$. The expectation of $f$, defined by
\begin{equation}
\mathbb{E}_{P(X)}\big[f(X)\big]=\int_{x}P(x)f(x)\,dx,
\end{equation}
sometimes can be difficult to compute. We can resolve this problem by instead approximating the expectation by sampling method, as
\begin{equation}
\mathbb{E}_{P(X)}f(X)\approx\frac{1}{m}\sum_{i=1}^{m}f(x^{(i)}),
\end{equation}
where $x^{(i)}\sim P$. However, samples generated from $P$ are not always easily obtained. This is where <strong>importance sampling</strong> plays its role.</p>

<p>The idea of <strong>importance sampling</strong>, or <strong>IS</strong> is an observation that
\begin{align}
\mathbb{E}_{P(X)}\big[f(X)\big]&amp;=\int_{x}P(x)f(x)\,dx \\ &amp;=\int_{x}Q(x)\frac{P(x)}{Q(x)}f(x)\,dx \\ &amp;=\mathbb{E}_{Q(X)}\left[\frac{P(X)}{Q(X)}f(X)\right],
\end{align}
where we have assumed that $Q(x)=0\Rightarrow P(x)=0$. Hence, we can use a sample-based method on $Q$ to get estimate of the expectation. Specifically, given $x^{(i)}\sim Q$, for $i=1,\ldots,m$, we can construct an unbiased estimator
\begin{equation}
\mathbb{E}_{P(X)}\big[f(X)\big]=\mathbb{E}_{Q(X)}\left[\frac{P(X)}{Q(X)}f(X)\right]\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(x^{(i)})}{Q(x^{(i)})}f(x^{(i)})
\end{equation}</p>

<h2 id="likelihood-ratio-pg-is">Likelihood Ratio Policy Gradient via IS</h2>
<p>The IS method suggests us rewrite the expected return of policy $\pi_\theta$ as<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup>
\begin{align}
\eta(\pi_\theta)&amp;=\mathbb{E}_{\tau\sim P(\tau;\theta)}\big[R(\tau)\big] \\ &amp;=\mathbb{E}_{\tau\sim P(\tau;\theta’)}\left[\frac{P(\tau;\theta)}{P(\tau;\theta’)}R(\tau)\right]
\end{align}
Taking the gradient w.r.t $\theta$ gives us another representation of the policy gradient 
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\nabla_\theta\mathbb{E}_{\tau\sim P(\tau;\theta’)}\left[\frac{P(\tau;\theta)}{P(\tau;\theta’)}R(\tau)\right] \\ &amp;=\mathbb{E}_{\tau\sim P(\tau;\theta’)}\left[\frac{\nabla_\theta P(\tau;\theta)}{P(\tau;\theta’)}R(\tau)\right],
\end{align}
which implies that
\begin{align}
\nabla_\theta\eta(\pi_\theta)\big\vert_{\theta=\theta’}&amp;=\mathbb{E}_{\tau\sim P(\tau;\theta’)}\left[\frac{\nabla_\theta P(\tau;\theta)\big\vert_{\theta=\theta’}}{P(\tau;\theta’)}R(\tau)\right] \\ &amp;=\mathbb{E}_{\tau\sim P(\tau;\theta’)}\big[\nabla_\theta\log P(\tau;\theta)\big\vert_{\theta=\theta’}R(\tau)\big]
\end{align}</p>

<h2 id="references">References</h2>
<p>[1] Jie Tang, Pieter Abbeel. <a href="https://proceedings.neurips.cc/paper/2010/hash/35cf8659cfcb13224cbd47863a34fc58-Abstract.html">On a Connection between Importance Sampling and the Likelihood Ratio Policy Gradient</a>. NIPS 2010.</p>

<p>[2] Richard S. Sutton &amp; Andrew G. Barto. <a href="https://mitpress.mit.edu/books/reinforcement-learning-second-edition">Reinforcement Learning: An Introduction</a>. MIT press, 2018.</p>

<h2 id="footnotes">Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Any infinite-horizon discounted MDP, as defined in the preceding subsection, can be $\epsilon$-approximated by a finite horizon MDP, using a horizon
\begin{equation*}
H_\epsilon=\left\lceil\log_\gamma\left(\frac{\epsilon(1-\gamma)}{R_\text{max}}\right)\right\rceil,
\end{equation*}
where
\begin{equation*}
R_\text{max}=\max_s\big\vert R(s)\big\vert
\end{equation*} <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>We can also use importance sampling to construct an unbiased estimate of the expected return $\eta(\pi_\theta)$. In particular, the expected return of $\pi_\theta$ given in \eqref{eq:lrp.1} can be approximated by
\begin{equation*}
\eta(\pi_\theta)=\sum_{\tau\sim P}P(\tau;\theta)R(\tau)\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{Q(\tau^{(i)})}R(\tau^{(i)})
\end{equation*}
where $\tau^{(i)}\sim Q$ and we have assumed that $Q(\tau^{(i)})=0\Rightarrow P(\tau^{(i)};\theta)=0$.<br />
If we choose $Q(\tau)\doteq P(\tau;\theta’)$, this means we are approximating the expected return of $\pi_\theta$ from trajectories given according to another parameterized policy $\pi_{\theta’}$.
\begin{equation*}
\eta(\pi_\theta)\approx\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)};\theta’)}R(\tau^{(i)})
\end{equation*}
Taking the gradient of this estimator w.r.t $\theta$, we obtain an unbiased estimator for the policy gradient specified at \eqref{eq:lrp.2}
\begin{align*}
\nabla_\theta\eta(\pi_\theta)&amp;\approx\nabla_\theta\frac{1}{m}\sum_{i=1}^{m}\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)};\theta’)}R(\tau^{(i)})
\end{align*}
Similar to applying IS to <a href="/2021/08/21/monte-carlo-in-rl.html#is-off-policy"><strong>off-policy learning</strong></a>, evaluating the <strong>importance weights</strong>, or <strong>importance sampling ratio</strong> does not require a dynamics model
\begin{equation*}
\frac{P(\tau^{(i)};\theta)}{P(\tau^{(i)});\theta’}=\frac{\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_\theta(a_t^{(i)}\vert s_t^{(i)})}{\prod_{t=0}^{H-1}P(s_{t+1}^{(i)}\vert s_t^{(i)},a_t^{(i)})\pi_{\theta’}(a_t^{(i)}\vert s_t^{(i)})}=\prod_{t=0}^{H-1}\frac{\pi_\theta(a_t^{(i)}\vert s_t^{(i)})}{\pi_{\theta’}(a_t^{(i)}\vert s_t^{(i)})}
\end{equation*} <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/2022/05/25/likelihood-ratio-pg-is.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
<script type="text/javascript">
  var btn = $('#button');

  $(window).scroll(function() {
    if ($(window).scrollTop() > 300) {
      btn.addClass('show');
    } else {
      btn.removeClass('show');
    }
  });

  btn.on('click', function(e) {
    e.preventDefault();
    $('html, body').animate({scrollTop:0}, '300');
  });
</script>