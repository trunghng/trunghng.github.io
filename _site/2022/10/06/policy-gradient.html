<!DOCTYPE html>
<html lang="en">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">
<link href="https://fonts.googleapis.com/css?family=Merriweather:400,900,900i" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.min.js"></script>
<style type="text/css">
  #button {
    display: inline-block;
    background-color: #A5CCF1;
    width: 50px;
    height: 50px;
    text-align: center;
    border-radius: 4px;
    position: fixed;
    bottom: 30px;
    right: 30px;
    transition: background-color .3s, 
      opacity .5s, visibility .5s;
    opacity: 0;
    visibility: hidden;
    z-index: 1000;
  }
  #button::after {
    content: "\f077";
    font-family: FontAwesome;
    font-weight: normal;
    font-style: normal;
    font-size: 2em;
    line-height: 50px;
    color: #fff;
  }
  #button:hover {
    cursor: pointer;
    background-color: #333;
    text-decoration: none;
  }
  #button:active {
    background-color: #555;
  }
  #button.show {
    opacity: 1;
    visibility: visible;
  }
</style><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Policy Gradient | Trung’s cabin</title>
<meta name="generator" content="Jekyll v4.3.1" />
<meta property="og:title" content="Policy Gradient" />
<meta name="author" content="Trung H. Nguyen" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Policy Gradient methods" />
<meta property="og:description" content="Policy Gradient methods" />
<link rel="canonical" href="http://localhost:4000/2022/10/06/policy-gradient.html" />
<meta property="og:url" content="http://localhost:4000/2022/10/06/policy-gradient.html" />
<meta property="og:site_name" content="Trung’s cabin" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-10-06T15:26:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Policy Gradient" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Trung H. Nguyen"},"dateModified":"2022-10-06T15:26:00+07:00","datePublished":"2022-10-06T15:26:00+07:00","description":"Policy Gradient methods","headline":"Policy Gradient","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2022/10/06/policy-gradient.html"},"url":"http://localhost:4000/2022/10/06/policy-gradient.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Trung&apos;s cabin" />
    





  

<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff"></head>
<body><header class="site-header">
  <div class="snorlax"></div>

  <div class="wrapper"><a class="site-title" rel="author" href="/">Trung&#39;s cabin</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/archive/">Archive</a><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <a id="button"></a>
      <div class="wrapper">
        <style>
@import "/assets/css/default.css"
</style>
<style>
  .collapsible {
    background-color: #A5CCF1;
    color: white;
    cursor: pointer;
    padding: 5px;
    /*width: 100%;*/
    border: none;
    text-align: center;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #82B9EE;
  }

  .codePanel {
    padding: 0 18px;
    display: none;
    overflow: hidden;
    background-color: #f1f1f1;
  }
</style>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <meta name="description" content="Policy Gradient methods">
    <h1 class="post-title p-name" itemprop="name headline">Policy Gradient</h1>
    <p><span>
      
        
        <a href="/tag/deep-reinforcement-learning"><code class="highligher-rouge"><nobr>deep-reinforcement-learning</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/policy-gradient"><code class="highligher-rouge"><nobr>policy-gradient</nobr></code>&nbsp;</a>
      
        
        <a href="/tag/my-rl"><code class="highligher-rouge"><nobr>my-rl</nobr></code>&nbsp;</a>
      
    </span></p>
    <p class="post-meta"><time class="dt-published" datetime="2022-10-06T15:26:00+07:00" itemprop="datePublished">
        Oct 6, 2022
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <blockquote>
  <p>Notes on Policy gradient methods.</p>
</blockquote>

<!-- excerpt-end -->
<ul>
  <li><a href="#preliminaries">Preliminaries</a></li>
  <li><a href="#vanilla-pg">(Vanilla) Policy Gradient</a>
    <ul>
      <li><a href="#var-red">Variance reduction</a>
        <ul>
          <li><a href="#reward-to-go">Reward-to-go</a></li>
          <li><a href="#baseline">Baseline</a>
            <ul>
              <li><a href="#unbiased">Unbiased estimator</a></li>
              <li><a href="#how-baseline-red">How can a baseline reduce variance?</a></li>
              <li><a href="#baseline-types">Types of baseline</a></li>
            </ul>
          </li>
          <li><a href="#discount-factor">Discount factor</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#references">References</a></li>
  <li><a href="#footnotes">Footnotes</a></li>
</ul>

<h2 id="preliminaries">Preliminaries</h2>
<p>Consider an finite-horizon undiscounted Markov Decision Process (MDP), which is a tuple of $(\mathcal{S},\mathcal{A},P,r,\rho_0,H)$ where</p>
<ul>
  <li>$\mathcal{S}$ is the state space.</li>
  <li>$\mathcal{A}$ is the action space.</li>
  <li>$P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the transition probability, i.e. $P(s’\vert s,a)$ denotes the probability of being at state $s’$ by taking action $a$ from state $s$.</li>
  <li>$r:\mathcal{\mathcal{S}\times\mathcal{A}\times\mathcal{S}}\to\mathbb{R}$ is the reward function.</li>
  <li>$\rho_0$ is the distribution of the start state $s_0$.</li>
  <li>$H$ is the horizon time.</li>
</ul>

<p>To start an episode, the agent is given an initial state $s_0$, which is sampled from $\rho_0$, i.e. $s_0\sim\rho_0$. At each time step $t$, from state $s_t$, until reaching the terminal state, the agent takes action $a_t$, according to a policy $\pi$, where $\pi:\mathcal{S}\times\mathcal{A}\to[0,1]$, which lets agent end up at state $s_{t+1}$ due to the dynamics $P$, and is given a corresponding reward $r_t=r(s_t,a_t,s_{t+1})$. The process gives rise to a sequence, called a <strong>trajectory</strong>, defined by:
\begin{equation}
\tau=(s_0,a_0,s_1,a_1,s_2,a_2\ldots)
\end{equation}
For a policy $\pi$, let $V_\pi:\mathcal{S}\to\mathbb{R}$ denote the state value function, $Q_\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ represent the state-action value function and let $A_\pi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ be the advantage function:
\begin{align}
V_\pi(s_t)&amp;\doteq\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}\left[\sum_{k=0}^{H-1}r_{t+k}\right] \\ Q_\pi(s_t,a_t)&amp;\doteq\mathbb{E}_{s_{t+1:H-1},a_{t+1:H-1}}\left[\sum_{k=0}^{H-1}r_{t+k}\right] \\ A_\pi(s_t,a_t)&amp;\doteq Q_\pi(s_t,a_t)-V_\pi(s_t),
\end{align}
where the expectation notation $\mathbb{E}_{s_{t+1:H-1},a_{t:H-1}}$ denotes that the expected value is computed by integrated over $s_{t+1}\sim P(s_{t+1}\vert s_t,a_t),a_t\sim\pi(a_t\vert s_t)$.</p>

<p>As in <strong>DQN</strong>, here we will be working with a policy $\pi_\theta$ parameterized by a vector $\theta$.  Let $R(\tau)\doteq\sum_{t=0}^{H-1}r_t$ denote the return, or the total reward along trajectory $\tau$. Our goal is to maximize the expected return:
\begin{equation}
\eta(\pi_\theta)\doteq\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big]=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}r_t\right]\label{eq:pre.1}
\end{equation}</p>

<h2 id="vanilla-pg">(Vanilla) Policy Gradient</h2>
<p>In <strong>(vanilla) policy gradient</strong> method, we are trying to optimize the expected total reward \eqref{eq:pre.1} by repeatedly estimating the gradient
\begin{equation}
\nabla_\theta\eta(\pi_\theta)=\nabla_\theta\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big]\label{eq:vpg.1}
\end{equation}
To continue our derivation, we will be using the probability of a trajectory $\tau\sim\pi_\theta$, computed by
\begin{equation}
P(\tau;\theta)=\rho_0(s_0)\prod_{t=0}^{H-1}P(s_{t+1}\vert s_t,a_t)\pi_\theta(a_t\vert s_t),
\end{equation}
Given this definition, \eqref{eq:vpg.1} can be written in a form that does not require a dynamics model:
\begin{align}
\hspace{-0.8cm}\nabla_\theta\eta(\pi_\theta)&amp;=\nabla_\theta\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[R(\tau)\big] \\ &amp;=\nabla_\theta\sum_\tau P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau\frac{P(\tau;\theta)}{P(\tau;\theta)}\nabla_\theta P(\tau;\theta)R(\tau) \\ &amp;=\sum_\tau P(\theta;\tau)\nabla_\theta\log P(\tau;\theta)R(\tau) \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\big[\nabla_\theta\log P(\tau;\theta)R(\tau)\big] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\nabla_\theta\left(\sum_{t=0}^{H-1}\log\rho_0(s_0)+\log P(s_{t+1}\vert s_t,a_t)+\log\pi_\theta(a_t\vert s_t)\right)R(\tau)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)R(\tau)\right]
\end{align}
Since the gradient is now an expectation, we can approximate it with the empirical estimate from $m$ sample trajectories, as
\begin{equation}
\nabla_\theta\eta(\pi_\theta)=\frac{1}{m}\sum_{i=1}^{m}\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t^{(i)}\vert s_t^{(i)})R(\tau^{(i)})
\end{equation}</p>

<h3 id="var-red">Variance reduction</h3>

<h4 id="reward-to-go">Reward-to-go</h4>
<p>To reduce the variance, we first notice that the total reward along a trajectory $\tau$, $R(\tau)$, can be expressed as sum of total reward from step $t$, called <strong>reward-to-go</strong> from $t$, and total preceding rewards w.r.t $t$, which has the expected value of zero but non-zero variance. In particular, we can simplify the policy gradient to be indepedent to the reward-to-go only, as:
\begin{align}
\nabla_\theta\eta(\pi_\theta)&amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)R(\tau)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\sum_{k=0}^{t-1}r_k+\sum_{k=t}^{H-1}r_k\right)\right] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t}}\left[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=0}^{t-1}r_k\right]\nonumber \\ &amp;\hspace{2cm}+\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right] \\ &amp;\overset{\text{(i)}}{=}\sum_{t=0}^{H-1}\left(\mathbb{E}_{s_{0:t},a_{0:t-1}}\left[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]\cdot\sum_{k=0}^{t-1}r_k\right]\right)\nonumber \\ &amp;\hspace{2cm}+\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right]\label{eq:vr.1} \\ &amp;\overset{\text{(ii)}}{=}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\sum_{k=t}^{H-1}r_k\right] \\ &amp;\overset{\text{(iii)}}{=}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\hat{r}_t\right],\label{eq:vr.2}
\end{align}
where</p>
<ul>
  <li>The (i) step is due to that the total past reward is independent of the current action $t$.</li>
  <li>In the (ii) step, we have used
\begin{align}
\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]&amp;=\sum_{a_t}\pi_\theta(a_t\vert s_t)\nabla_\theta\log\pi_\theta(a_t\vert s_t) \\ &amp;=\sum_{a_t}\nabla_\theta 1=\mathbf{0}\label{eq:vr.3}
\end{align}</li>
  <li>In the (iii) step, the $\hat{r}_t\doteq\sum_{k=t}^{H-1}r_k$ is referred as the <strong>reward-to-go</strong> from step $t$.</li>
</ul>

<h4 id="baseline">Baseline</h4>
<p>It is worth remarking that we can furtherly reduce the variance of the estimator by adding an baseline, denoted $b$, as
\begin{equation}
\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\right)\right]\label{eq:vrb.1}
\end{equation}</p>

<h5 id="unbiased">Unbiased estimator</h5>
<p>First we will prove that the estimator with baseline \eqref{eq:vrb.1} is still an unbiased with \eqref{eq:vr.2}. Specifically
\begin{align}
&amp;\hspace{-1cm}\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\left(\hat{r}_t-b_t(s_{0:t},a_{0:t-1})\right)\right]\nonumber \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\hat{r}_t\right]\nonumber \\ &amp;\hspace{0.5cm}-\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\right],
\end{align}
which makes our claim follow if the latter expectation of the RHS is zero. In fact, using the logic as in \eqref{eq:vr.1} and the result \eqref{eq:vr.3}, we have
\begin{align}
&amp;\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\right]\nonumber \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)b_t(s_{0:t},a_{0:t-1})\big]\Big] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbb{E}_{a_t}\big[\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big]\cdot b_t(s_{0:t},a_{0:t-1})\Big] \\ &amp;=\sum_{t=0}^{H-1}\mathbb{E}_{s_{0:t},a_{0:t-1}}\Big[\mathbf{0}\cdot b_t(s_{0:t},a_{0:t-1})\Big]=\mathbf{0}
\end{align}</p>

<h5 id="how-baseline-red">How can a baseline reduce variance?</h5>
<p>By definition of the variance of a r.v $X$
\begin{equation}
\text{Var}(X)=\mathbb{E}\big[X^2\big]-\mathbb{E}\big[X\big]^2,
\end{equation}
combined with the claim that \eqref{eq:vrb.1} is an unbiased estimator of the policy gradient $\nabla_\theta\eta(\pi_\theta)$, and letting $b_t\doteq b_t(s_{0:t},a_{0:t-1})$ to simplify the notation, we have 
\begin{align}
\text{Var}&amp;=\text{Var}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right] \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\left(\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right)^2\right]\nonumber \\ &amp;\hspace{1.5cm}-\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right]^2 \\ &amp;=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\left[\left(\sum_{t=0}^{H-1}\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big(\hat{r}_t-b_t\big)\right)^2\right]-\big(\nabla_\theta\eta(\pi_\theta)\big)^2,
\end{align}
which suggests us that for each $t$, by finding $b_t$ that minimizes the former expectation, we can also reduce the variance $\text{Var}$.</p>

<p>Differentiating the variance w.r.t $b_t$ gives us
\begin{equation}
\frac{\partial\text{Var}}{\partial b_t}=\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\Big[\big(\nabla_\theta\log\pi_\theta(a_t\vert s_t)\big)^2(2b_t-2\hat{r}_t)\Big]
\end{equation}
Set the derivative to zero and solve for $b_t$, we obtain the optimal baseline
\begin{equation}
b_t=\frac{\mathbb{E}_{s_{0:H-1},a_{0:H-1}}\Big[\big(\nabla_\theta\pi_\theta(a_t\vert s_t)\big)^2\hat{r}_t\Big]}{\mathbb{E}_{a_t}\Big[\big(\nabla_\theta\pi_\theta(a_t\vert s_t)\big)^2\Big]}
\end{equation}</p>

<h5 id="baseline-types">Types of baseline</h5>

<h4 id="discount-factor">Discount factor</h4>

<h2 id="preferences">Preferences</h2>
<p>[1] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, Pieter Abbeel. <a href="https://arxiv.org/abs/1506.02438">High-Dimensional Continuous Control Using Generalized Advantage Estimation</a>. ICLR 2016.</p>

<h2 id="footnotes">Footnotes</h2>

  </div>
<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    
    var disqus_config = function () {
    this.page.url = '';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = ''; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-trunghng-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


<script id="dsq-count-scr" src="//https-trunghng-github-io.disqus.com/count.js" async></script>

  <a class="u-url" href="/2022/10/06/policy-gradient.html" hidden></a>
</article>
 <!-- mathjax javascript -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
    "HTML-CSS": {availableFonts: []}
  });
</script><script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
      "HTML-CSS": {availableFonts: []},
      TeX: {
        equationNumbers: { autoNumber: "AMS" },
      },
    });
  </script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = document.getElementById(this.id+"data");
      var btn = document.getElementById(this.id)
      if (content.style.display === "block") {
        content.style.display = "none";
        btn.innerText = "Click to show the code";
      } else {
        content.style.display = "block";
        btn.innerText = "Click to hide the code"
      }
    });
  }
</script>

      </div>

    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">Trung H. Nguyen</li>
          <li><a class="u-email" href="mailto:trung.skipper@gmail.com">trung.skipper@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>To document something I&#39;ve learned
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/trunghng" title="trunghng"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
<script type="text/javascript">
  var btn = $('#button');

  $(window).scroll(function() {
    if ($(window).scrollTop() > 300) {
      btn.addClass('show');
    } else {
      btn.removeClass('show');
    }
  });

  btn.on('click', function(e) {
    e.preventDefault();
    $('html, body').animate({scrollTop:0}, '300');
  });
</script>