---
title: "Maximum Entropy Reinforcement Learning via Soft Q-learning & Soft Actor-Critic"
date: 2022-12-26T13:46:09+07:00
tags: [reinforcement-learning, policy-gradient, actor-critic, my-rl]
math: true
eqn-number: true
---
> Notes on Maximum Entropy Reinforcement Learning via SQL & SAC.
<!--more-->

## Preliminaries

### Maximum Entropy Reinforcement Learning{#max-ent-rl}
Consider an infinite-horizon Markov Decision Process (MDP), defined as a tuple $(\mathcal{S},\mathcal{A},p,r)$, where
- $\mathcal{S}$ is the **state space**.
- $\mathcal{A}$ is the **action space**.
- $p:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\to[0,1]$ is the **transition probability distribution**, i.e. $p(s,a,s')=p(s'\vert s,a)$ denotes the probability of transitioning to state $s'$ when taking action $a$ from state $s$.
- $r:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ is the **reward function**, and let us denote $r_t\doteq r(S_t,A_t)$ for simplicity.

For a policy $\pi$, let $\rho_\pi(s_t)$ and $\rho_\pi(s_t,a_,t)$ denote the state and state-action marginals of the trajectory distribution induced by $\pi(a_t\vert s_t)$

Regularly, with finite-horizon MDP, our objective is to maximize the expected cumulative rewards
\begin{align}
J_\text{std}(\pi)&=\mathbb{E}\_{\tau\sim\pi}\left[\sum_{t=0}^{T-1}r(s_t,a_t)\right] \\\\ &=\sum_{t=0}^{T-1}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\big[r(s_t,a_t)\big],
\end{align}
where $\tau=s_0,a_0,s_1,a_1,\ldots$ denotes the trajectory generated by $\pi$, and $T$ represents the time-horizon.

In **Maximum Entropy Reinforcement Learning** framework, we augment the return with an entropy term
\begin{equation}
J_\text{MaxEnt}(\pi)=\sum_{t=0}^{T-1}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\big[r(s_t,a_t)+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big],
\end{equation}
where $\alpha$ is the hyper-parameter determines the relative importance of the entropy with the rewards. The corresponding optimal policy of the maximum entropy objective is thus given by
\begin{align}
\pi_\text{MaxEnt}^\*&=\underset{\pi}{\text{argmax}}J_\text{MaxEnt}(\pi) \\\\ &=\underset{\pi}{\text{argmax}}\sum_{t=0}^{T-1}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\big[r(s_t,a_t)+\alpha H\big(\pi(\cdot\vert s_t)\big)\Big]
\end{align}
To extends the problem into infinite-horizon discounted setting, we first rewrite the standard objective $J_\text{std}$ by adding a discount factor $\gamma\in(0,1)$, as
\begin{align}
J_\text{std}(\pi)&=\mathbb{E}\_{\tau\sim\pi}\left[\sum_{t=0}^{\infty}\gamma^t r(s_t,a_t)\right] \\\\ &=\sum_{t=0}^{\infty}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\big[r(s_t,a_t)\big]
\end{align}
The discounted version of the maximum entropy objective $J_\text{MaxEnt}$ looks a bit different from the non-discounted one. Specifically
\begin{equation}
J_\text{MaxEnt}=\sum_{t=0}^{\infty}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}\mathbb{E}\_{s_k\sim p,a_k\sim\pi}\Big[r(s_k,a_k)+H\big(\pi(\cdot\vert s_k)\big)\vert s_t,a_t\Big]\right],\label{eq:mer.1}
\end{equation}
which implies that the optimal policy that maximizes $J_\text{MaxEnt}$ is given as
\begin{equation}
\hspace{-0.5cm}\pi_\text{MaxEnt}^\*=\underset{\pi}{\text{argmax}}\sum_{t=0}^{\infty}\mathbb{E}\_{(s_t,a_t)\sim\rho_\pi}\left[\sum_{k=t}^{\infty}\gamma^{k-t}\mathbb{E}\_{s_k\sim p,a_k\sim\pi}\Big[r(s_k,a_k)+H\big(\pi(\cdot\vert s_k)\big)\vert s_t,a_t\Big]\right]
\end{equation}

## Soft Policy Iteration{#soft-policy-iter}
To learn the optimal maximum entropy policy $\pi_\text{MaxEnt}^\*$, we will be using the **soft policy iteration**, a corresponding version of a dynamic programming algorithm - [**policy iteration**]({{< ref "dp-in-mdp#policy-iteration" >}}), which we have used to find $\pi_\text{std}^\*$.

Recall that in the standard case, we repeats policy evaluation and policy improvement processes alternatively, analogously in maximum entropy policy framework, we correspondingly have **soft policy evaluation** and **soft policy improvement**.

### Soft Policy Evaluation{#soft-policy-eval}
In **soft policy evaluation** process, we wish to compute the value of a given policy according to the maximum entropy objective \eqref{eq:mer.1}. This can be found by an iterative method.

Specifically, starting from an arbitrary function $Q^{(0)}:\mathcal{S}\times\mathcal{A}\to\mathbb{R}$ with $\vert\mathcal{A}\vert<\infty$ and consider the **soft Bellman backup operator**, denoted $\mathcal{T}^\pi$, which is defined as
\begin{equation}
Q^{(k+1)}(s,a)\doteq\mathcal{T}^\pi Q^{(k)}(s,a)\doteq r(s,a)+\gamma\mathbb{E}\_{s'\sim\rho_\pi}\big[V^{(k)}(s')\big],\label{eq:spe.1}
\end{equation}
where
\begin{equation}
V^(k)(s)=\mathbb{E}\_{a\sim\rho_\pi}\big[Q^{(k)}(s,a)-\log\pi(a\vert s)\big]
\end{equation}
The resulting sequence $\\{Q^{(k)}\\}\_{k=0,1,\ldots}$ will converges to a fixed point called **soft Q-value**, denoted $Q_\text{soft}$, as $k\to\infty$.

**Proof**  
Let $r^\pi(s,a)\doteq r(s,a)+\mathbb{E}\_{s'\sim\rho_\pi}\big[\alpha H\big(\pi(\cdot\vert s')\big)\big]$ denote the entropy augmented reward, the Bellman backup \eqref{eq:spe.1} can rewrite as
\begin{equation}
Q^{(k+1)}(s,a)=r^\pi(s,a)+\gamma\mathbb{E}\_{(s',a')\sim\rho_\pi}\big[Q^{(k)}(s',a')\big]
\end{equation}
Since $\vert\mathcal{A}\vert<\infty$, we have that $r^\pi(s,a)$ is bounded.

Analogy to the standard policy evaluation, we will prove that the Bellman backup operator $\mathcal{T}^\pi$ is a contraction.




### Soft Policy Improvement{#soft-policy-imp}






## References
[1] Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine. [Reinforcement Learning with Deep Energy-Based Policies](https://dl.acm.org/doi/10.5555/3305381.3305521). ICML, 2017.

[2] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine. [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290). arXiv preprint arXiv:1812.05905, 2018.

[3] Brian D. Ziebart. [Modeling purposeful adaptive behavior with the principle of maximum causal entropy](https://www.cs.cmu.edu/~bziebart/publications/thesis-bziebart.pdf). PhD Thesis, Carnegie Mellon University, 2010.


## Footnotes
[^1]: To simplify notation, from now on we will let $J(\pi)\doteq J_\text{MaxEnt}$.
