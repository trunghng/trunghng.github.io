---
title: "GAN"
date: 2023-08-13 13:00:00 +0700
tags: [machine-learning, generative-model]
math: true
eqn-number: true
draft: true
---
> Notes on Generative Adversarial Networks.
<!--more-->

## Generative Adversarial Networks (GAN){#gan}
A **generative adversarial network** consists of two independent components, each of which is a neural network[^1], acting as adversaries:
- <b>Generator</b>: A generative model, denoted $G$, parameterized by $\theta_g$. The model is trained to generate fake samples as real as possible.
- <b>Discriminator</b>: A discriminative model, denoted $D$, parameterized by $\theta_d$. The model is trained to distinguish the samples generated by $G$ and the training samples. The samples come from real data are assigned correct label.

To be more precise,
<ul id='roman-list'>
	<li>
		Let $p_\text{data}$ denotes the distribution over real data (training sample), the discriminator $D$ is trained to maximize $\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\big[\log D(\mathbf{x};\theta_d)\big]$.
	</li>
	<li>
		$G$ implicitly defines a probability distribution, which we denote as $p_g$. Let $\mathbf{z}$ denote an input noise variable, $\mathbf{z}\sim p_\mathbf{z}$ , which is the input to $G$. Thus, as assigning the data generated by $G$ as incorrect label, $D$ is also trained to maximize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
	<li>
		At the same time, the generator $G$ is trained to minimize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
</ul>

Therefore, the adversarial framework can be considered as a two-player minimax game in which we are trying to optimize the value function $V(D,G)$
\begin{equation}
\min_{\theta_g}\max_{\theta_d}V(D,G)=\mathbb{E}\_{\mathbf{x}\sim p_\text{data}(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}\_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(G(\mathbf{z};\theta_g);\theta_d)\big)\big]\label{eq:gan.1}
\end{equation}
<ul id='number-list'>
	<li>
		Let us consider the optimal discriminator $D$ for any given generator $G$. For $\theta_g$ fixed, we need to maximize
		\begin{align}
		V(D,G)&=\mathbb{E}_{\mathbf{x}\sim p_\text{data}(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(g(\mathbf{z});\theta_d)\big)\big] \\ &= \int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{z}p_\mathbf{z}(\mathbf{z})\log\big(1-D(g(\mathbf{z});\theta_d))\hspace{0.1cm}d\mathbf{z} \\ &=\int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{x}p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x} \\ &=\int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)+p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x},
		\end{align}
		which, with letting $\bar{\mathbf{x}}$ denote $D(\mathbf{x};\theta_d)$, can be achieved by maximizing
		\begin{align}
		f(\bar{\mathbf{x}})&=p_\text{data}(\mathbf{x})\log\bar{\mathbf{x}}+p_g(\mathbf{x})(1-\log\bar{\mathbf{x}})
		\end{align}
		Differentiating $f$ w.r.t $\bar{\mathbf{x}}$ gives us
		\begin{equation}
		\frac{\partial f}{\partial\bar{\mathbf{x}}}=\frac{p_\text{data}(\mathbf{x})}{\bar{\mathbf{x}}}-\frac{p_g(\mathbf{x})}{1-\bar{\mathbf{x}}}
		\end{equation}
		Setting the derivative to zero, we have that the optimal discriminator $D$ is given as
		\begin{equation}
		D_G^*(\mathbf{x})=\frac{p_\text{data}(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})}
		\end{equation}
	</li>
	<li>
		When $\theta_d$ is optimal and fixed, if we define
		\begin{equation}
		C(G)\overset{\Delta}{=}\max_{\theta_d}V(G,D),
		\end{equation}
		the minimax game in \eqref{eq:gan.1} can be rewritten as minimizing
		\begin{align}
		C(G)&=\min_{\theta_g}\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\Big[\log D_G^*(\mathbf{x})\Big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\Big[\log(1-D_G^*(G(\mathbf{z})))\Big] \\ &=\min_{\theta_g}\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\Big[\log D_G^*(\mathbf{x})\Big]+\mathbb{E}_{\mathbf{x}\sim p_g}\Big[\log(1-D_G^*(G(\mathbf{z})))\Big]
		\end{align}
	</li>
</ul>


## Preferences
[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. [Generative Adversarial Nets](http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf). NIPS, 2014.

[2] 

## Footnotes
[^1]: Thus, in the trivial case, each of the components is an MLP.

