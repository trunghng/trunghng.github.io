---
title: "GAN"
date: 2023-05-01 13:00:00 +0700
tags: [machine-learning, generative-model]
math: true
eqn-number: true
draft: true
---
> Notes on Generative Adversarial Networks.
<!--more-->

## Generative Adversarial Network (GAN){#gan}
A **generative adversarial network** consists of two independent components, each of which is a neural network[^1], acting as adversaries:
- <b>Generator</b>: A generative model, denoted $G$, parameterized by $\theta_g$. The model is trained to generate fake samples as real as possible.
- <b>Discriminator</b>: A discriminative model, denoted $D$, parameterized by $\theta_d$. The model is trained to distinguish between the samples generated by $G$ and the training samples.

To be more precise,
<ul id='roman-list'>
	<li>
		Let $p_r$ denote the distribution over real data (training sample), the discriminator $D$ is trained to maximize $\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D(\mathbf{x};\theta_d)\big]$.
	</li>
	<li>
		$G$ implicitly defines a probability distribution, which we denote as $p_g$. Let $\mathbf{z}$ denote an input noise variable, $\mathbf{z}\sim p_\mathbf{z}$ , which is the input to $G$. Thus, as assigning the correct label to data generated by $G$ and the real data, $D$ is also trained to maximize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
	<li>
		At the same time, the generator $G$ is trained to minimize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
</ul>

Therefore, the adversarial framework can be considered as a two-player minimax game in which we are trying to optimize the value function $V(D,G)$
\begin{equation}
\min_{\theta_g}\max_{\theta_d}V(D,G)=\mathbb{E}\_{\mathbf{x}\sim p_r(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}\_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(G(\mathbf{z};\theta_g);\theta_d)\big)\big]\label{eq:gan.1}
\end{equation}
<ul id='number-list'>
	<li>
		Let us consider the optimal discriminator $D$ for any given generator $G$. For $\theta_g$ fixed, we need to maximize
		\begin{align}
		V(D,G)&=\mathbb{E}_{\mathbf{x}\sim p_r(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(g(\mathbf{z});\theta_d)\big)\big] \\ &= \int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{z}p_\mathbf{z}(\mathbf{z})\log\big(1-D(g(\mathbf{z});\theta_d))\hspace{0.1cm}d\mathbf{z} \\ &=\int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{x}p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x} \\ &=\int_\mathbf{x}p_r(\mathbf{x})\log D(\mathbf{x};\theta_d)+p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x},
		\end{align}
		which, with letting $\bar{\mathbf{x}}$ denote $D(\mathbf{x};\theta_d)$, can be achieved by maximizing
		\begin{align}
		f(\bar{\mathbf{x}})&=p_r(\mathbf{x})\log\bar{\mathbf{x}}+p_g(\mathbf{x})(1-\log\bar{\mathbf{x}})
		\end{align}
		Differentiating $f$ w.r.t $\bar{\mathbf{x}}$ gives us
		\begin{equation}
		\frac{\partial f}{\partial\bar{\mathbf{x}}}=\frac{p_r(\mathbf{x})}{\bar{\mathbf{x}}}-\frac{p_g(\mathbf{x})}{1-\bar{\mathbf{x}}}
		\end{equation}
		Setting the derivative to zero, we have that the optimal discriminator $D$ is given as
		\begin{equation}
		D_G^*(\mathbf{x})=\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})},
		\end{equation}
		at which $f(\bar{\mathbf{x}})$ achieves its maximum since
		\begin{equation}
		\frac{\partial^2 f}{\partial\bar{\mathbf{x}}^2}=-\frac{p_r(\mathbf{x})}{\bar{\mathbf{x}}^2}-\frac{p_g(\mathbf{x})}{(1-\bar{\mathbf{x}})^2}<0
		\end{equation}
	</li>
	<li>
		When $\theta_d$ is optimal and fixed, if we define
		\begin{equation}
		C(G)\overset{\Delta}{=}\max_{\theta_d}V(G,D)=V(G,D_G^*),
		\end{equation}
		the minimax game in \eqref{eq:gan.1} can be rewritten as minimizing
		\begin{align}
		\hspace{-0.5cm}C(G)&=\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D_G^*(G(\mathbf{z})))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{x}\sim p_g}\big[\log(1-D_G^*(\mathbf{x}))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\left(1-\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right)\right] \\ &=\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{p_r(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{p_g(\mathbf{x})}{p_r(\mathbf{x})+p_g(\mathbf{x})}\right]
		\end{align}
		We will be showing that $C(G)$ achieves its global minimum iff $p_g=p_r$.<br>
		We begin by considering the <a href=''><b>Jensen-Shannon divergence</b></a> between $p_g$ and $p_r$, denoted $D_\text{JS}$:
		\begin{align}
		\hspace{-1.5cm}D_\text{JS}(p_g\Vert p_r)&=\frac{1}{2}D_\text{KL}\left(p_g\left\Vert\frac{p_g+p_r}{2}\right.\right)+\frac{1}{2}D_\text{KL}\left(p_r\left\Vert\frac{p_g+p_r}{2}\right.\right) \\ &=\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{2 p_r(\mathbf{x})}{p_g(\mathbf{x})+p_r(\mathbf{x})}\right]+\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_r}\left[\log\frac{2 p_g(\mathbf{x})}{p_g(\mathbf{x})+p_r(\mathbf{x})}\right] \\ &=\frac{1}{2}C(G)+\log 2,
		\end{align}
		which implies that
		\begin{equation}
		C(G)=2D_\text{JS}(p_g\Vert p_r)-2\log 2
		\end{equation}
		Therefore, $C(G)$ achieves its minimum if and only if $D_\text{JS}(p_g\Vert p_r)$ reaches its minimum, which is zero, occurring when $p_g=p_r$.<br>
		It then follows when $p_g=p_r$, $C(G)$ achieves its global minimum, which is $-2\log 2$.
	</li>
</ul>

### Training GAN
We will apply the minibatch SGD method for training GAN.
<figure>
	<img src="/images/gan/gan.png" alt="GAN"/>
	<figcaption></figcaption>
</figure> 

## Wasserstein GAN{#wgan}
Wasserstein is a variation of GAN which use Wasserstein metric to measure the distance between probability distributions $p_r$ and $p_g$ instead of the Jensen-Shannon divergence.

### Different Distances
Let $\mathcal{X}$ be a compact metric set, $\Sigma$ be the set of all the Borel subsets of $\mathcal{X}$ and let $Prob(\mathcal{X})$ denote the space of probability measures[2] defined on $\mathcal{X}$. We can now define elementary distances and divergences between two probability distributions $P_r,P_g\in\text{Prob}(\mathcal{X})$
- **Total Variation (TV)** distance
\begin{equation}
\delta(P_r,P_g)=\sup_{A\in\Sigma}\big\vert P_r(A)-P_g(A)\big\vert
\end{equation}
- **Kullback-Leibler (KL)** divergence
\begin{equation}
D_\text{KL}(P_r\Vert P_g)=\int_\mathcal{X}P_r(x)\log\frac{P_r(x)}{P_g(x)}d\mu(x),
\end{equation}
where both $p_r,p_g$ are assumed to be absolutely continuous, and therefore admit densities, w.r.t a measure $\mu$[^2]. The divergence is not symmetric and infinite when there are points such that $P_g(x)=0$ and $P_r(x)>0$.
- <b id='jsd'>Jensen-Shannon (JS)</b> divergence
\begin{equation}
D_\text{JS}(P_r\Vert P_g)=\frac{1}{2}D_\text{KL}(P_r\Vert P_m)+\frac{1}{2}D_\text{KL}(P_g\Vert P_m)
\end{equation}
where $P_m$ is the mixture distribution $P_m=\frac{P_r+P_g}{2}$. The divergence is symmetric.
- **Earth Mover (EM)** distance, or **Wasserstein-1**
\begin{equation}
W(P_r,P_g)=\inf_{\theta\in\Pi(P_r,P_g)}\mathbb{E}\_{(x,y)\sim\theta}\big[\Vert x-y\Vert\big],
\end{equation}
where $\Pi(P_r,P_g)$ is the set of all joint distribution $\theta(x,y)$ whose marginal distributions are $P_r$ and $P_g$.

### Learning parallel lines
Consider probability distributions defined on $\mathbb{R}^2$:
- $P_0$ is the distribution of $(0,Y)$ where $Y\sim\text{Unif}(0,1)$.
- $P_\theta$ is the family of distribution of $(\theta,Y)$ with $Y\sim\text{Unif}(0,1)$.

Our goal is to learn to move from $\theta$ to $0$, i.e. as $\theta$ approaches $0$, the distance or divergence $d(P_0,P_\theta)$ tends to $0$. For each distance, or divergence we have
<ul id='number-list'>
	<li>
		<b>TV distance</b>. If $\theta=0$ then $P_0=P_\theta$, and thus
		\begin{equation}
		\delta(P_0,P_\theta)=\sup_{A\in\Sigma}\vert P_0(A)-P_\theta(A)\vert=0
		\end{equation}
		If $\theta\neq 0$, let $\bar{A}=\{(0,y):y\in[0,1]\}$, we have
		\begin{equation}
		\delta(P_0,P_\theta)=\sup_{A\in\Sigma}\vert P_0(A)-P_\theta(A)\vert\geq\vert P_0(\bar{A})-P_\theta(\bar{A})\vert=1
		\end{equation}
		Also, by definition we also have that $0\leq\delta(P_0,P_\theta)\leq 1$. Hence, for $\theta\neq 0,\delta(P_0,P_\theta)=1$.
		\begin{equation}
		\delta(P_0,P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ 1&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
		\end{equation}
	</li>
	<li>
		<b>KL divergence</b>. By definition, we have that
		\begin{equation}
		D_\text{KL}(P_0\Vert P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }P_0=P_\theta \\ +\infty&\hspace{1cm}\text{if }!x\in\mathbb{R}^2:P_0(x)>0,P_\theta(x)=0\end{cases}
		\end{equation}
		Therefore, we have that when $\theta\neq 0$, for any $a\in[0,1]$,
		\begin{equation}
		D_\text{KL}(P_0\Vert P_\theta)=+\infty,
		\end{equation}
		since $P_0(0,a)>0$ and $P_\theta(0,a)=0$, and
		\begin{equation}
		D_\text{KL}(P_\theta\Vert P_0)=+\infty,
		\end{equation}
		since $P_\theta(\theta,a)>0$ while $P_0(\theta,a)=0$. Therefore, it follows that
		\begin{equation}
		D_\text{KL}(P_0\Vert P_\theta)=D_\text{KL}(P_\theta\Vert P_0)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ +\infty&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
		\end{equation}
	</li>
	<li>
		<b>JS divergence</b>. It is easy to see that for $\theta=0$, we have $D_\text{JS}(P_0\Vert P_\theta)=0$. Let us consider the case that $\theta\neq 0$. Let $P_m$ denote the mixture $\frac{P_0+P_\theta}{2}$, we have
		\begin{equation}
		D_\text{JS}(P_0\Vert P_\theta)=\frac{1}{2}D_\text{KL}(P_0\Vert P_m)+\frac{1}{2}D_\text{KL}(P_\theta\Vert P_m)
		\end{equation}
		Consider the first KL divergence, we have that
		\begin{align}
		D_\text{KL}(P_0\Vert P_m)&=\int_{x}\int_{0}^{1}P_0(x,y)\log\frac{P_0(x,y)}{P_m(x,y)}\,dy\,dx \\ &\overset{\text{(i)}}{=}\int_{0}^{1}P_0(0,y)\log\frac{2P_0(0,y)}{P_0(0,y)+P_\theta(0,y)}\,dy\nonumber \\ &+\int_{0}^{1}\int_{x\neq 0}P_0(x,y)\log\frac{2P_0(x,y)}{P_0(x,y)+P_\theta(x,y)}\,dx\,dy \\ &\overset{\text{(ii)}}{=}\int_{0}^{1}P_0(0,y)\log 2\,dy+\int_{0}^{1}0\,dy \\ &=\log 2\int_{0}^{1}P_0(0,y)\,dy \\ &\overset{\text{(iii)}}{=}\log 2,
		\end{align}
		where
		<ul id='roman-list'>
			<li>
				In this step, we use the Fubini's theorem to exchange the order of integration.
			</li>
			<li>
				In this step, we use the fact that for any $y\in[0,1],P_0(0,y)>0$ while $P_\theta(0,y)=0$ and for any $x\neq 0,P_0(x,y)=0$.
			</li>
			<li>
				In this step, the expression inside the integration integrates to $1$ since $P_0$ is a probability distribution.
			</li>
		</ul>
		Similarly, we also have that
		\begin{equation}
		D_\text{KL}(P_\theta\Vert P_m)=\log 2
		\end{equation}
		Therefore, it follows that
		\begin{equation}
		D_\text{JS}(P_0\Vert P_\theta)=\begin{cases}0&\hspace{1cm}\text{if }\theta=0 \\ \log 2&\hspace{1cm}\text{if }\theta\neq 0\end{cases}
		\end{equation}
	</li>
	<li>
		<b>EM distance</b>. 
	</li>
</ul>


## Preferences
[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. [Generative Adversarial Nets](http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf). NIPS, 2014.

[2] Martin Arjovsky, Soumith Chintala, Léon Bottou. [Wasserstein GAN](https://arxiv.org/abs/1701.07875). arXiv preprint, arXiv:1701.07875, 2017.

[3]

## Footnotes
[^1]: Thus, in the trivial case, each of the components is an MLP.
[^2]: A probability distribution $P\in\text{Prob}(\mathcal{X})$ admits a density $p(x)$ w.r.t $\mu$ means for all $A\in\Sigma$
\begin{equation}
P(A)=\int_A P(x)d\mu(x),
\end{equation}
which happens iff $P$ is absolutely continuous w.r.t $\mu$, i.e. for all $A\in\Sigma$, $\mu(A)=0$ implies $P(A)=0$.

