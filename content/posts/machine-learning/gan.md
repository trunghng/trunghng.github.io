---
title: "GAN"
date: 2023-05-01 13:00:00 +0700
tags: [machine-learning, generative-model]
math: true
eqn-number: true
draft: true
---
> Notes on Generative Adversarial Networks.
<!--more-->

## Generative Adversarial Network (GAN){#gan}
A **generative adversarial network** consists of two independent components, each of which is a neural network[^1], acting as adversaries:
- <b>Generator</b>: A generative model, denoted $G$, parameterized by $\theta_g$. The model is trained to generate fake samples as real as possible.
- <b>Discriminator</b>: A discriminative model, denoted $D$, parameterized by $\theta_d$. The model is trained to distinguish between the samples generated by $G$ and the training samples.

To be more precise,
<ul id='roman-list'>
	<li>
		Let $p_\text{data}$ denote the distribution over real data (training sample), the discriminator $D$ is trained to maximize $\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\big[\log D(\mathbf{x};\theta_d)\big]$.
	</li>
	<li>
		$G$ implicitly defines a probability distribution, which we denote as $p_g$. Let $\mathbf{z}$ denote an input noise variable, $\mathbf{z}\sim p_\mathbf{z}$ , which is the input to $G$. Thus, as assigning the correct label to data generated by $G$ and the real data, $D$ is also trained to maximize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
	<li>
		At the same time, the generator $G$ is trained to minimize $\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D(G(\mathbf{z};\theta_g);\theta_d))\big]$.
	</li>
</ul>

Therefore, the adversarial framework can be considered as a two-player minimax game in which we are trying to optimize the value function $V(D,G)$
\begin{equation}
\min_{\theta_g}\max_{\theta_d}V(D,G)=\mathbb{E}\_{\mathbf{x}\sim p_\text{data}(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}\_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(G(\mathbf{z};\theta_g);\theta_d)\big)\big]\label{eq:gan.1}
\end{equation}
<ul id='number-list'>
	<li>
		Let us consider the optimal discriminator $D$ for any given generator $G$. For $\theta_g$ fixed, we need to maximize
		\begin{align}
		V(D,G)&=\mathbb{E}_{\mathbf{x}\sim p_\text{data}(\mathbf{x})}\big[\log D(\mathbf{x};\theta_d)\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}(\mathbf{z})}\big[\log\big(1-D(g(\mathbf{z});\theta_d)\big)\big] \\ &= \int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{z}p_\mathbf{z}(\mathbf{z})\log\big(1-D(g(\mathbf{z});\theta_d))\hspace{0.1cm}d\mathbf{z} \\ &=\int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)\hspace{0.1cm}d\mathbf{x}+\int_\mathbf{x}p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x} \\ &=\int_\mathbf{x}p_\text{data}(\mathbf{x})\log D(\mathbf{x};\theta_d)+p_g(\mathbf{x})(1-\log D(\mathbf{x};\theta_d))\hspace{0.1cm}d\mathbf{x},
		\end{align}
		which, with letting $\bar{\mathbf{x}}$ denote $D(\mathbf{x};\theta_d)$, can be achieved by maximizing
		\begin{align}
		f(\bar{\mathbf{x}})&=p_\text{data}(\mathbf{x})\log\bar{\mathbf{x}}+p_g(\mathbf{x})(1-\log\bar{\mathbf{x}})
		\end{align}
		Differentiating $f$ w.r.t $\bar{\mathbf{x}}$ gives us
		\begin{equation}
		\frac{\partial f}{\partial\bar{\mathbf{x}}}=\frac{p_\text{data}(\mathbf{x})}{\bar{\mathbf{x}}}-\frac{p_g(\mathbf{x})}{1-\bar{\mathbf{x}}}
		\end{equation}
		Setting the derivative to zero, we have that the optimal discriminator $D$ is given as
		\begin{equation}
		D_G^*(\mathbf{x})=\frac{p_\text{data}(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})},
		\end{equation}
		at which $f(\bar{\mathbf{x}})$ achieves its maximum since
		\begin{equation}
		\frac{\partial^2 f}{\partial\bar{\mathbf{x}}^2}=-\frac{p_\text{data}(\mathbf{x})}{\bar{\mathbf{x}}^2}-\frac{p_g(\mathbf{x})}{(1-\bar{\mathbf{x}})^2}<0
		\end{equation}
	</li>
	<li>
		When $\theta_d$ is optimal and fixed, if we define
		\begin{equation}
		C(G)\overset{\Delta}{=}\max_{\theta_d}V(G,D)=V(G,D_G^*),
		\end{equation}
		the minimax game in \eqref{eq:gan.1} can be rewritten as minimizing
		\begin{align}
		\hspace{-0.5cm}C(G)&=\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}}\big[\log(1-D_G^*(G(\mathbf{z})))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\big[\log D_G^*(\mathbf{x})\big]+\mathbb{E}_{\mathbf{x}\sim p_g}\big[\log(1-D_G^*(\mathbf{x}))\big] \\ &=\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\left[\log\frac{p_\text{data}(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\left(1-\frac{p_\text{data}(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})}\right)\right] \\ &=\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\left[\log\frac{p_\text{data}(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{p_g(\mathbf{x})}{p_\text{data}(\mathbf{x})+p_g(\mathbf{x})}\right]
		\end{align}
		We will be showing that $C(G)$ achieves its global minimum iff $p_g=p_\text{data}$.<br>
		We begin by considering the <b>Jensen-Shannon divergence</b> between $p_g$ and $p_\text{data}$, denoted $D_\text{JS}$:
		\begin{align}
		\hspace{-1.5cm}D_\text{JS}(p_g\Vert p_\text{data})&=\frac{1}{2}D_\text{KL}\left(p_g\left\Vert\frac{p_g+p_\text{data}}{2}\right.\right)+\frac{1}{2}D_\text{KL}\left(p_\text{data}\left\Vert\frac{p_g+p_\text{data}}{2}\right.\right) \\ &=\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_g}\left[\log\frac{2 p_\text{data}(\mathbf{x})}{p_g(\mathbf{x})+p_\text{data}(\mathbf{x})}\right]+\frac{1}{2}\mathbb{E}_{\mathbf{x}\sim p_\text{data}}\left[\log\frac{2 p_g(\mathbf{x})}{p_g(\mathbf{x})+p_\text{data}(\mathbf{x})}\right] \\ &=\frac{1}{2}C(G)+\log 2,
		\end{align}
		which implies that
		\begin{equation}
		C(G)=2D_\text{JS}(p_g\Vert p_\text{data})-2\log 2
		\end{equation}
		Therefore, $C(G)$ achieves its minimum if and only if $D_\text{JS}(p_g\Vert p_\text{data})$ reaches its minimum, which is zero, occurring when $p_g=p_\text{data}$.<br>
		It then follows when $p_g=p_\text{data}$, $C(G)$ achieves its global minimum, which is $-2\log 2$.
	</li>
</ul>

### Training GAN
We will apply the minibatch SGD method for training GAN.
<figure>
	<img src="/images/gan/gan.png" alt="GAN"/>
	<figcaption></figcaption>
</figure> 

## Wasserstein GAN{#wgan}
Wasserstein is a variation of GAN which use Wasserstein metric to measure the distance between $p_\text{data}$ and $p_g$ instead of the Jensen-Shannon divergence.

### Different Distances
Let $\mathcal{X}$ be a compact metric set, $\Sigma$ be the set of all the Borel subsets of $\mathcal{X}$ and let $Prob(\mathcal{X})$ denote the space of probability measures[2] defined on $\mathcal{X}$


## Preferences
[1] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. [Generative Adversarial Nets](http://papers.neurips.cc/paper/5423-generative-adversarial-nets.pdf). NIPS, 2014.

[2] Martin Arjovsky, Soumith Chintala, LÃ©on Bottou. [Wasserstein GAN](https://arxiv.org/abs/1701.07875). arXiv preprint, arXiv:1701.07875, 2017.

[3]

## Footnotes
[^1]: Thus, in the trivial case, each of the components is an MLP.

