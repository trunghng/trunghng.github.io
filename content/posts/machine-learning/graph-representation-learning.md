---
title: "Graph Representation Learning"
date: 2024-04-16T14:43:59+07:00
tags: [machine-learning, neural-network, graph-neural-network]
draft: true
math: true
eqn-number: true
hideSummary: true
---

## Node Embeddings
The goal of node embedding learning methods is to encode nodes as low-dimensional vectors that summarize their graph position and the structure of their local graph neighborhood. Or in other words, nodes are encoded so that the similarity in the latent space approximates the similarity in the original graph.

### Encoder-Decoder framework
In this framework, the graph representation learning problem is divided into steps
<ul class='number-list'>
	<li>
		An <b>encoder</b> maps each node in the original graph to a low-dimensional vector in the embedding space.
	</li>
	<li>
		Define a similarity function in the original graph.
	</li>
	<li>
		A <b>decoder</b> takes embedding vectors and use them to reconstruct information about each node's neighborhood in the original graph.
	</li>
	<li>
		Optimize the encoder and the decoder so that the similarity in the embedding space approximates the similarity in the original graph.
	</li>
</ul>

#### The Encoder
The encoder, denoted $\text{ENC}$, is a function that maps each node $v\in\mathcal{V}$ to an embedding vector $\mathbf{z}_v\in\mathbb{R}^d$.
\begin{equation}
\text{ENC}(v)=\mathbf{z}\_v
\end{equation}
In the simplest form, used in the **shallow embedding** approach, the encoder is simply an embedding-lookup
\begin{equation}
\text{ENC}(v)=\mathbf{z}\_v=\mathbf{Z}\mathbf{v},
\end{equation}
where $Z\in\mathbb{R}^{d\times\vert\mathcal{V}\vert}$ is a matrix whose each column is a node embedding and where each $\mathbf{v}\in\mathbb{R}^{\vert\mathcal{V}\vert}$ is an indicator vector (i.e., all zeroes except a one in column indicating the ID of node $v$).

#### The Decoder
The decoder, denoted $\text{DEC}$, reconstruct certain graph statistics (e.g., set of neighbors $\mathcal{N}(u)$) from the embedding $\mathbf{z}_u$ generated by $\text{ENC}$ of node $u$.

A pairwise decoder, $\text{DEC}:\mathbb{R}^d\times\mathbb{R}^d\mapsto\mathbb{R}^+$ maps each pair of embeddings $(\mathbf{z}_u,\mathbf{z}_v)$ to a similarity score, which describes the relationship between nodes $u$ and $v$.

Given this similarity score, our goal is to optimize the encoder and decoder so that the decoded similarity approximates the similarity in the original graph.
\begin{equation}
\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v)\approx\mathbf{S}(u,v),\label{eq:td.1}
\end{equation}
where $\mathbf{S}(u,v)$ is a graph-based similarity metric between nodes $u$ and $v$.

#### Model optimization
The reconstruction objective \eqref{eq:td.1} can be accomplished by minimizing an empirical reconstruction loss $\mathcal{L}$ over a set of training node pairs $\mathcal{D}$
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\ell(\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v),\mathbf{S}(u,v)),
\end{equation}
where $\ell:\mathbb{R}\times\mathbb{R}\mapsto\mathbb{R}$ is a loss function measures the difference between the decoded similarity values $\text{DEC}(\mathbf{z}_u,\mathbf{z}_v)$ and the true similarity values $\mathbf{S}(u,v)$.

### Factorization-based approaches

#### Laplacian eigenmaps
In this approach, the decoder is defined as the L2-distance between the embeddings
\begin{equation}
\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v)=\Vert\mathbf{z}\_u-\mathbf{z}\_v\Vert_2^2
\end{equation}
And the loss function is then given as
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v)\cdot\mathbf{S}(u,v)
\end{equation}

#### Inner-product methods
As suggested by their name, the decoder in these approaches is defined as the inner product
\begin{equation}
\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v)=\mathbf{z}\_u^\text{T}\mathbf{z}\_v
\end{equation}
These methods have the loss function given as
\begin{equation}
\mathcal{L}=\sum_{(u,v)\in\mathcal{D}}\Vert\text{DEC}(\mathbf{z}\_u,\mathbf{z}\_v)-\mathbf{S}(u,v)\Vert_2^2
\end{equation}
The above approaches are referred to as matrix-factorization methods, since their loss function can be minimized using factorization algorithm, such as SVD. Stacking the embeddings $\mathbf{z}_u\in\mathbb{R}^d$ into a matrix $\mathbf{Z}\in\mathbb{R}^{\vert\mathcal{V}\vert\times d}$ the reconstruction objective can be rewritten as
\begin{equation}
\mathcal{L}\approx\Vert\mathbf{Z}\mathbf{Z}^\text{T}-\mathbf{S}\Vert_2^2,
\end{equation}
where $\mathbf{S}$ is a matrix containing pairwise similarity measures.

### Random walk embeddings

## Graph Neural Networks

### Graph Convolution Networks

## References
[1] William L. Hamilton. [Graph Representation Learning](https://www.cs.mcgill.ca/~wlh/grl_book/). Morgan and Claypool, Synthesis Lectures on Artificial Intelligence and Machine Learning.

[2] Jure Leskovec. [CS224W - Machine Learning with Graphs](https://web.stanford.edu/class/cs224w/).

## Footnotes